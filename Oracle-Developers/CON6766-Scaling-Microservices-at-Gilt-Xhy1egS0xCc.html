<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CON6766   Scaling Microservices at Gilt | Coder Coacher - Coaching Coders</title><meta content="CON6766   Scaling Microservices at Gilt - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>CON6766   Scaling Microservices at Gilt</b></h2><h5 class="post__date">2015-12-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Xhy1egS0xCc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you everyone for coming today I
really really appreciate it particularly
coming in after lunch my name is eight I
am the spc spc the SVP of engineering
for for guilt I lead guilt engineering
teams in New York and in Dublin and I'm
kind of presenting today on behalf of
them effectively you know the story of
guilt we have a very sophisticated
microservices architecture I think it's
proof of existence that microservices
really really can't work I want to spend
today kind of showing you how we got
there lessons learned but also as well
what are still the challenges because
there's a number of things that we
learned along the way that we did not
expect I think that these were really
interesting challenges some of them
we've got a really really good handle on
and some of them we don't okay yes so
we're still working on them apologies in
advance as well I have been a Java
programmer for 20 years now I do not
have a code demo today okay so if you're
looking for a code demo not not this
trip okay okay so so that Sam let's talk
a little bit about it so first I realize
you know in America gilt com is a very
very well-known brand what's interesting
though is in Europe it's very where I
come from no nobody actually knows what
they are what they do right I think it's
really important to understand sort of
the complexity of gilt com ok and the
core business idea so here's the story
right what we do is we source luxury
brands entry fashion brands and then we
make those available to our members at
highly discounted prices okay so you
know these products these could be
things that are maybe like last year's
talk or excess stock or it could be
brand new inventory that we get our
hands on we do these deals with these
designers and effectively they give us
the inventory we get it in front of our
members so a lot of the time you know
one of the things we discover it is
working with these designers sometimes
they don't actually have great editorial
shots of their product and so what we do
is we take the product and in many many
cases we actually shoot that product in
our own studios okay so product comes in
we take the photograph and then we ship
back on to the to the warehouse we've
got a real warehouse you know
we're a real company right this is this
is our massive where I said in Kentucky
and effectively that's where we receive
restore we pick we pack we ship we do
the whole deal in terms of fulfillment
okay now like why am I kind of telling
you this i guess i'm telling you this
because there is no kind of
off-the-shelf software that you can buy
to run a business like guilt it's very
different in a number of different
dimensions and so the software that we
make you know both the software that you
guys see when you go to gilt com or when
you use our apps and the software that
actually runs the entire business that's
actually all being written by guilt tech
okay so pretty you know sophisticated
software one of the things I think that
makes guilt unique and to me interesting
as an engineer is this property so the
first thing is we sell every day at noon
so we get all this inventory and all of
these sales go live at the same time at
noon okay now it actually happens is at
eleven-forty-five New York time every
day we start sending out our emails and
our notifications so we send 5.5 million
emails every day to our members about a
third of those we have enough
information to personalize every single
one ok so we personalize all of the
content in terms of hey we know you like
this brand so we're actually going to
show you this brand or this sale much
further up in the email to grab your
attention ok unlike why is the same for
our notifications or push notifications
we want to make sure that if you like a
particular brand and we've got us then
we're going to send it to you ok so then
all of our customers come in hordes to
the site this is what our customers look
like a steaming herd of wildebeest you
know they're actually far more
attractive than that and far more
lucrative but it's nice and fun to think
of them looking like this what they
really look like from an engineering
perspective is this ok and this graph I
think captures what what's really
interesting I think about the guilt
business model ok it's the spike or what
we call the pulse load at noon every day
so what happens is and you can actually
see that they're on the graft is sort of
like you know a buildup in the early
more
and then just before noon we get this
this cliff okay and there's a number of
interesting properties any software that
we write has got to be enormously solid
right it's got it's got to be able to
scale and handle that change of dynamic
in a 15-minute period from you know very
very little traffic to completely off
the chart traffic okay so that's
actually you know really really
important and of course you know what
this is like an average day when we have
a really really big day with a really
really big brand that goes viral we need
to be handled we need to be able to
handle for example two three four times
our usual play at peak load so that's
what makes I think guilt very
interesting from an engineering
perspective there's another piece as
well to to this slide that I think is
interesting which is the cost of failure
is non uniform throughout the day for us
okay you know effectively the the period
of the day where we earn most of our
money is in that period between noon and
let's say two o'clock or three o'clock
okay failure in that period of the day
is enormously expensive okay failure and
the rest of the day is actually
reasonably cheap okay so it's really
really important for us that when the
traffic comes that we are up that we are
stable and that the site looks and feels
really really great ok so I'm want to
talk about how did we get to where we
are today ok so like many kind of shops
we've got this rails to riches kind of
story and effectively the idea is is you
know a small bunch of Engineers kind of
get together working on this idea called
guilt and they hack out a prototype
super fast in the programming language
that they love and that language is Ruby
right ruby is an amazing programming
language it's absolutely fantastic it's
obviously there are some things that as
Java programmers we would dislike but
overall you know Ruby is actually pretty
cool ok and so the original gilt com
site was all written on on Ruby on Rails
ok and effectively we had this
architecture right which is like
architecture 101 it's box box cylinder
box right very very simple ok we had a
big fat app server
right called I don't even I don't know
if they called it application servers
then we are basically Ruby on Rails okay
um Ruby on Rails was hitting off our our
big database which was postgres and
still is post grads in many parts we do
a lot of work with those presidents
really really fantastic for us so you
can imagine as a company in those early
days it's like well that was fine okay
we're all of our traffic both our
internal traffic to actually set up the
sales and put up the products and take
the photographs and handle the the
warehouse all that code was in that
monolithic database but also all the
code that handles every page rendering
from from gilt com like from from our
customers so so the kind of things that
you do to scale when you've got Ruby on
Rails is you can see here we've got your
ad memcache right you try your best to
to avoid hitting the database as much as
possible ok so this worked and it worked
until it didn't work okay but that I
think the the biggest kind of eye opener
for us was the sale of a brand called
christian louboutin DC these shoes went
live on gilt com the whole thing went
viral and then effectively the site went
down right and you know it's interesting
sometimes we think about that you know
the site going down is a very very bad
thing okay sometimes it's not a bad
thing because there's so much pent up
demand then that when the site comes
back up they all come back right so it's
actually kind of interesting okay
anyways so when i joined guilt in 2011
there'd been a migration to a new
technology called jav a or java
technology ok and it was a very very
interesting and I think kind of you know
pragmatic change so the business had
grown hugely in the first four years of
its existence going from a million
dollars of revenue in its first year to
four hundred million dollars in year
four okay that's just an incredible
amount of growth so with all this extra
traffic there actually was you know like
a bake-off or fistfight a knife fight
even if you will between the Ruby
engineers and Java engineers right and
it turned add I'm at the end the
decision was made at to go with Java we
simply couldn't be scary be to make it
do what we needed to do okay that is not
a slight on Ruby we love Ruby we still
we still
a lot of work in Ruby it's just I guess
a pragmatic fact okay so so what you see
when you look at kind of the
architecture at the time is you know to
to the far left you see we still have
this Ruby on Rails so we're still using
ruby on rails for our internal
development okay for our internal staff
for our internal tooling okay very fast
very easy to get stuff done you know and
in terms of the active record is just an
amazing tool in terms of productivity so
a lot of good stuff there a movie but
the core start part of the site was
actually all served using Java okay so
how did it look right well what we had
done was we'd realized at noon the last
thing you want is is traffic consumer
traffic to be hitting the database the
database isn't fast enough never will
never will be to scale at that level so
what we did was we built a service
layers for the classic kind of entities
that you would expect product service
user service cart service and these
services effectively pulled all of our
live data for the day into memory okay
so that every page hit was effectively
being served from data that was in
memory in those services and that
actually works pretty well for guilt
because you know 95% ninety-seven
percent a huge amount of our traffic
he's actually read only you know it's
getting product listings it's getting
data about what we're selling on a given
day okay so so that's the the bottom
layer there and then what you see is
this repeated kind of layer cake
two-tier kind of approach to how we
create our services or creditor our site
okay so what we had was you know the top
tier is a tier that effectively does the
page rendering the generation of the
HTML content and all of our all of our
content is server-side rendered we're
actually changing that now we're doing a
little bit more work with blind side
rendering but the bulk of what we serve
is actually a server-side rendered okay
so the idea then is you've got the top
layer and then it hits off a bottom
layer which is this content or page
assembly layer okay so the idea is is it
you know it makes a hip or the front end
makes a hit on those services to get all
of the data to construct a page okay and
for
but we've got a service they're called
page n for a page generation and it
turns you add a whole bunch of JSON and
then you use that to render your page
okay so this is the the the way that we
we looked at the world okay you see a
number of different kind of the pillars
there like there's gilt com guild City
com gilt.com / home Gil taste these were
all businesses that we were running and
experimenting in and effectively we had
a different stack to to render each
business okay now what were the problems
so a bunch of things I think that again
like when you're growing and moving fast
you make these mistakes and they accrue
over time first was that our service
layer there were very very large service
right product service is one of our
biggest right it loads up our entire
product catalog but it also has things
that are nothing to do with products it
has sales which are actually lists of
projects right it is it is other
endpoints that really aren't you know
logically coherent with the idea of a
product service the other thing as well
which drives me absolutely mental is
that these were very much loosely
constrained or loosely typed services
okay effectively with our heritage of
Ruby where you know effectively
everything's just a map give it a shot
right that's sort of that's how our Java
engineers had done a lot of the work
they were there throwing maps around and
when you throw maps around for your data
then you lose a sense of type safety and
you leave the sense of the compiler been
able to help you okay so that I think
was a problem in those early days okay
with multiple teams all focused on these
different business lines trying
desperately to get code through okay
what's interesting though I think is
that most of the teens were all handling
the channel under gilt com that's this
Swift thing and this page gin thing
right when you've got like you know 40
to 50 engineers all in contention on the
same code base that's actually a real
problem effectively this Swift app
serving gilt com became a huge
monolithic Java app and became a huge
barrier for innovation question
oh it's good question there's not Swift
the apple thing we all laughed heartily
went when Apple invented Swift we were
like those guys right no absolutely yet
this is a this is a Java framework for
rendering a service after rendering jsps
okay how we call it stuffed okay great
hidden linkages and buried business
logic so one of the big problems that we
found here was the whole idea of the
layer cake which he kept your rendering
separate from your business logic right
and the problem was was on any given day
somebody wanted to get a feature in and
they just janked it in right so they'd
kind of make up a quick link to the
database to a query and get some data
and I was the critics way to get
something done okay and these build up
and accrue over time and you lose
control over over the code base and what
it's trying to do okay and there's an
interesting for me personally an
interesting example of this so at the
time that we wrote home the guilt home
business what I was looking for what I
needed was the marketing message of the
day and this is a very very simple
concept the marketing message of the day
is something like you know free shipping
for you or ten percent off for you my
friend okay now this message is as a
service endpoint is very simple it's
like I'm going to give you a user good
or user email and you're going to give
me the message that we should show that
particular user okay so I was new to
guilt at the time and I was like okay
hold on a second so how would I get that
information okay and it turned out the
way to get that information was to go
into page gen get a rendering of the
entire front page of the site go 7 level
deep into the JSON forget everything
else and pull out that one string ok
that was just for one little string
that's probably that 80 characters long
right so I sensed that there was
something wrong here right and I
proposed in a conversation with that my
predict the great my critic you is our
CTO at the time I said that so you know
I think we got this idea I'd like to
write a service and the service is just
going to have like one endpoint right
and that endpoint is going to take use
good it's going to give you the
marketing message of the day will be
brilliant right and everybody could use
it and to be the same logic in one place
under the amazing
well everything you know and he was like
that sounds kind of crazy right so what
we're going to do is we're going to
we're going to hoist up three or four
java virtual machines you know running
this service endpoint just for for this
one thing you know I this is like purely
a crazy and stupid idea you know so we
did it okay that was the first
microservice that guilt okay and it was
amazing it was like you know we did it
we built it and were like wow it mean
it's the right thing to do ok so that
was in two thousand and early 12 oops
and effectively at that point while we
were thinking of what it meant to do
services we were actually asking that I
guess sort of the big questions you know
the big questions for us were this point
for the huge bottleneck for innovation
you know when you've got let's say six
or seven engineering teams writing on
the same code base when as well you have
a separate QA team from your engineering
teams right what you've got is a recipe
for incredibly long cycles to get
anything to production okay you got a
team we've blocked by another team and
then when they're unblocked eventually
two weeks later their feature goes to
production turns out it's broken so we
roll it back and that goes back to the
team we started on something else I mean
it's just not a happy scenario so so we
said well like how can we actually
arrange our teams so that they can
follow strategic initiatives get stuff
done I get it done super fast okay and
that's effectively where we got to today
okay in 2015 we now have approximately
250 of the micro services and apps so
our landscape has changed hugely okay
but effectively so that the world has
changed quite a bit there to highlight
some of the key things so you know north
you know towards gilt com we realized
that having everybody contention on one
web app was actually really difficult
for us and so we did is we broke up the
web application by page almost so we
have a different application that serves
our listing page or our search page or
our product detail page okay and we call
that losa lots of small apps and these
were all implemented using play
framework the other thing then as well
is is we do a lot of work now with Scala
right I'm secretly hoping that next year
we'll call this skala one and be done
with it so we have a lot of fun with
Scala and of course the general build
system that we're using is SBT we've got
a lot of tooling built into SBT as well
that helps us in terms of our continuous
deployment and use integration so that's
what the world looks like now for us
we've got much more fine-grained web
apps serving the experience and then
we've got a whole flock of services am
underneath that okay interesting Lee we
still do have you know those those big
chestnuts the core services that we had
before you know when you think about it
you know the innovation cycle like you
innovate towards something you try
something at you don't necessarily adopt
a full clean slate and say let's wipe
out everything that we've done before
right so we still have our heritage or
our kind of our legacy services that are
there and some of them by the way our
rock solid you know you think about your
architecture right and you know if it
was a city you know the micro services
are the really really new clean areas of
town world hipsters if the some of our
older services you know like our
inventory service it's just like a Roman
castle it's just rock solid built using
Java and it's just fantastic okay so
you're going to get that nyx in your
architecture okay so how did we get to
this microservices architecture so one
of the things that I think is clear to
me now is it should be said as well we
never once got into a room and said
let's actually you know get a really
really big whiteboard and draw out all
the possible services that we might need
and then that's our architecture let's
go implement it okay it simply didn't
happen that way so effectively what we
did was we put the mechanisms in place
where teams could build whatever they
felt they wanted and needed okay and
therefore the architecture emerges right
and that was actually really really
interesting so things that we did we
gave more autonomy two teams we let them
build whatever they needed to solve a
given problem for forget so for example
hey
don't have searched on on on gilt com
okay let's create a team you guys are
the search team go build whatever you
need okay likewise for personalization
right we don't have personalization on
the side we need more you guys do
whatever you want to do okay another
thing is well I think that was really
interesting for us as a social
experiment that I think is really really
paid off is the concept of voluntary
adoption at gilt when something is good
and people adopted without being told
that you know it really really is good
and it sort of becomes part of the
lingua franca part of the toolset that
we all use okay if people don't adopt us
then really it's like it's a kind of a
Darwinian force that tells you that your
idea wasn't good enough where the tool
isn't good enough so we really let folks
figure out for themselves in the large
part what are the tools that they should
be using so there's a couple of other
things there as well you know we removed
as part of giving full technical freedom
to the teams we simply said hey here's a
KPI you know we want you to change
conversion on the site okay do whatever
you want right and actually giving them
that freedom telling them what the goal
is but not actually telling them exactly
how to do us was really really great in
that in that way okay so back to tech so
if we think about the the way we adopted
services over time so this chart shows
effectively the growth of service is it
guilt there's a point of inflection that
happened in February 2012 we're at an
architecture summit in New York we
realized that we were all writing far
more Skala than we were writing Java and
that was a real interesting point
because as you can see there's a point
of inflection there as we all of a
sudden write more and more services very
very freely right now part of that
that's not saying that Scylla is
ultimately a more productive programming
language than Java it's simply saying
that when folk were given the
opportunity to program something new
right and they didn't need to code it an
existing repo they basically just
started off something themselves okay so
that was a very interesting point for us
okay so when we think about what does a
typical killed service look like and
feel like so effectively it's something
like this okay the sort of a generic
kind of architecture idea here so
basically all of our service is
practically all of our sir
services are deployed using a Java
Virtual Machine typically there's some
kind of framework that you're using to
kind of expose what are largely in
almost every case restful endpoints we
don't do soap except for integration
purposes sometimes we listen off q
technologies but largely it's all
restful calls over HTTP ok your code
goes there very interesting point is
that for me when you think of bad
microservices it's really important that
the service owns its own data so
typically each microservice has its own
data store that data store is owned only
by that service okay and that's a really
really important distinction if you have
the same data store owned by multiple
services that it means that schema
upgrades and general ownership of the
data gets very very hard to manage okay
so then you're going to have some kind
of solution for monitoring and something
for logging as well so which are
probably more interested in is but what
are the choices that we use we've got a
very very light work lightweight
framework built over jetty which we call
the guilt service framework right as a
general rule you know really sorry Larry
Ellison's right as a general rule we
like our frameworks to be as small as
possible right like we don't use I still
call it j2ee we don't use je in any way
all right with a very very simple
frameworks to get our services into play
okay its place so we have guilt service
framework one of the other things that
we found for our JSON over HTTP services
which is a very surprising result is
that we use play framework for those
play framework is actually intended for
building applications web applications
it turns out it is very very good for
building services that return JSON over
HTTP we did not expect that when we
started so programming languages people
are free to choose whatever language
date they wish generally there's Scala
and Java there we go question yeah
that's a that's a very very good call
effectively we're using a fatter stack
than we need for our play services right
that's actually very true okay typically
the way it just emerged is guys are
using a small amount of the JSON
rendering and then the general HTTP
request response flow and they're just
ignoring the rest of the stuff you're
right if play actually cut down or
restricted you know just that core piece
and that's the that's the piece that we
will be using for the services but it's
absolutely a fair point okay okay so
other other things I think that are
interesting there are in terms of the
JVM in the actual container that we use
what we're finding now more is that
people are more likely to use node J yes
we're starting our first kind of foray
into using note on the front end and
we're actually very happy with that
what's interesting there is no de peels
to a certain kind of engineer certain
kind of developer and we're kind of keen
to support them in that from monitoring
we use we use actually New Relic quite a
bit New Relic is is quite incredible at
just how completely easy it is to set it
up and have a working right so we use
that pretty much you know across the
board we also now use a cloud watch as
well because he's most most of our stuff
is now running in the cloud via for
monitoring as well I'll talk a little
bit about cave later on we've written
what we think is a very interesting
approach to monitoring and order and
i'll give you more details on that later
on okay service discovery so people have
asked me before when I've given this
talk it guilt how do you do service
discovery okay and you know service
discovery can sound like an incredibly
hard problem it turns out actually it's
very very easy it's very very
straightforward I think when you think
of service discovery at runtime you've
got a client it needs to know what's the
URL I should resolve to to
a particular service okay very very
simple pattern right so the client right
call some kind of store right which is a
map of strings to urls and says hey give
me the trolley URL it gets it back and
then it makes a restful call always to a
load balancer right everything we do
goes through a load balancer right I've
seen frameworks 24 where you do kind of
client side load balancing and we don't
do that everything goes through a load
balancer of some sort and then the load
balancer makes it called through
whatever the the services on the on the
right here so what does that look like
for us we use zookeeper zookeeper is
actually you know fantastic at providing
a single point where we can serve in a
very full tolerant way our configuration
information likewise the keeper allows
you to register for triggers or changes
on given configuration points and that's
actually very very useful if you want to
have all of your freedom services get
updated when some kind of service some
kind of configuration piece changes so
our load balancer that we use is Zeus
right through the miracle of marketing
and mergers and acquisitions it changes
its name multiple multiple times over
the last year it became stingray than
steel app and now it's called Brock ad
but if we find that to be a very very
good load balancer all of our service
traffic goes through that and
effectively what happens there is you
know while you're rolling out if you've
got four instances of a service while
you're rolling it out and you take out
one of the instances these figures out
eyes right and it redirects it says yeah
that instance is down let me redirect to
one that's live okay so having a really
smart load balancer in front of your
services it's a great way to make sure
that all of your high availability your
full tolerance and your scalability is
all met there okay so we've got all
these services what are they what are
they doing okay I I built a word map by
taking the names of all of our repos and
effectively then saying what are the
most common words surprisingly enough
service gets a big mention there we
brought all the other services names
that kind of pop up there for example
user
data min recommendation email search
mean effectively you know our
microservices stack is stunning every
part of our business okay so it's kind
of nice to kind of see the cloud forum
and to see kind of you know the kind of
the key contenders rise up okay but
that's actually kind of hard to manage
okay so we have this really funny moment
where we were like oh my god how do we
manage all these services how do we
think about them okay and one of the
guys is like I know I know let's write a
service to manage all of the services
right and you know you can appreciate
there's something gently and playfully
recursive there you're one of our one of
our senior engineer sorry our
distinguished engineers actually has put
it sound upset why don't we use a
spreadsheet right and we were all like
knocked out by the simplicity of the
concept a spreadsheet what is this
technology you know so it's chewy and
complete you know you can share you can
collaborate it's actually pretty awesome
so we used Google sheets right and it's
kind of interesting it we went from
being a hard hard problem to simply
being a list here are all the services
that we've got here's who owns the
here's what the service does here's some
notes and what we think has to happen to
the service we share that across our
whole tech team and we call that the
guilt genome project and for any of you
who were going down the road of building
a large microservices architecture
particularly if you're going to do it in
a sort of an emergent way then actually
this is a pretty good way to get a
handle on on how things work okay so one
of the things that we did recently then
and this was another very interesting
learning for me you know when you think
of a list the human mind can only hold
seven plus or minus two concepts at the
same time and we've got like you know
the latest version of the list I've got
265 services right how the hell'd you do
you go through all those and keep them
in your head at the same moment in time
so we did something very very simple we
added columns to the spreadsheet we're
where we created a very simple taxonomy
in terms of the major functional area of
the site okay the system that the
service was involved in and then a
subsystem if that made sense so we made
just three levels
okay and what we found out then I think
is actually very interesting right in
terms of the architecture being emergent
what we also saw then was an interesting
result some services and components are
really simple to set pretty simple by
that I mean you know you see the service
you worry about it come what does it do
I'm not quite sure and you go in there
and you realize it's incredibly simple
oh it's really small totally contained
you know you could rewrite it in a day
if you needed to totally fine but other
services are simply deceptive okay they
look simple but you can only understand
their context when you figure out the
constellation of services that it
interacts with okay this is something
that dance done salt and Jessica
mentioned in terms previously of saying
the complexity that was in our big
services has now actually moved out into
the white space between the services
okay so we've kind of just moved the
problem around a little bit in terms of
how we manage complexity so deceptively
simple why do I say that it turns out
that when you look at the sizes of our
services in terms of lines of code right
most of our services are pretty small
less than 2048 lines of code right
that's 2 to the 11 I think it is that's
actually great that's a great result you
know particular when you think about you
know your programming in Java or in
Scala and there's a lot of model files
and things like that that are kind of
you know boosting up or a kind of you
know bulking ad kind of the number of
lines of code that you've got but for us
most of the services are actually pretty
small with 2048 lines of code you can go
in there and you can figure out what the
service is doing in about half an hour
okay and that's been a very very nice
outcome for us so lots of these small
services likewise in terms of the source
files / service what we're seeing is
most of them are less than 32 files okay
and again that's not too bad it's easy
to get around a code base of that size
so that I think was a very very good
result okay but when we think about
complexity right the taxonomic approach
that we took the three level hierarchy
showed actually something really
really interesting okay so based on a
pivot table that I built in terms of the
number of services per area I got a
structure that allowed me to build
following architectural diagram that was
very useful to me but what I noticed was
that if you look down here at the
service constellations and I think you
can just about see it the resolution is
kind of big grainy there what we realize
is that some parts of our business have
a very large number of services related
to them okay so when I look down here
for example inventory has six services
all of our inventory management logic is
actually spread over six different
services that's a very very interesting
result okay our distribution systems
they're based or they're split up over
nine different services okay so this is
actually very interesting result for me
what it suggests is that this as an area
is either necessarily complex and we
made a good decision to break it down
into nine or ten different things or we
introduce too much complexity and it's
an area for us to go into and figure out
what do we need to change what we need
to merge we need to rewrite their to
make it a much simpler system okay so we
think alice is like this it's not rocket
science but for us it did it did
actually turn up something very very
interesting in in terms of our systems
okay so this year was very big for us
because we used to run all of our
services on bare metal in a data center
or two data centers in Phoenix and in
dulles and effectively this year and the
first quarter of the year we moved
everything entirely to the cloud okay
when I say everything that there's a
little bit there's a little bit left in
our data center to do with our data
warehouse but largely every component
that you see if you're using our app you
know or our website is going is being
rendered from from amazon okay how do
you do that you know it you know there's
a number of choices you have to make and
just to give you an idea the approach
that we took it was kind of like two
phase or two fold okay so the first
thing is effectively what we call the
lift and shift
and what we've done is we simply created
a VPC with amazon we've connected that
using a product that they may call a
Direct Connect which is you know a dual
10 gigabit line with just two
milliseconds of latency okay and
effectively what we did then was we we
we're deploying services previously to
our machines on the data center so what
we did was say well why don't we just
deploy them to machines in Amazon okay
and so where we went we had the big long
list we knew we had to do we divide up
the work between the teams and in the
period you know when everything came to
the crunch in a period of two to three
weeks we moved all of our service
infrastructure incrementally to the to
the cloud okay yeah question yeah
yeah that's right having the habitat so
the question is did our approach to
serve as discovery having a load
balancer in the middle of that help the
answer there is yes because if you think
about it you've got a service but let's
say three endpoints okay which are in
our top existing data center and what we
did was we said right well let's
actually launch another three right in
the VPC that we've got 66 right and then
once you've got sixth and then we just
simply removed the three that were in
the old data center and all the traffic
just kept on going through automatically
so all in all very straightforward you
know nobody noticed okay which was
exactly what you want okay and what's
interesting though is is that's a very
static move from from the data center to
the legacy VPC and that's actually okay
it's the quickest way to get from A to B
right we're simply deploying two
machines that are hosted somewhere else
the the next challenge then for us is
moving to what we call department
accounts for elasticity and for DevOps
so effectively you know the original
infrastructures all moved to the legacy
VPC now teams are writing new services
and they're moving the services they own
down into their team accounts okay and
if those team accounts them they have
full devops control to create whatever
services databases they need and to do
whatever elasticity or scaling that they
want to do so it's very much a move from
very much move to to DevOps for us you
know the the kind of infrastructure that
Amazon gives us makes that possible okay
some things along the way when we deploy
to Amazon one of the decisions that we
made was to deploy every service to a
single machine to a single instance of
an ami and that was a very important
decision for us what we learned in the
past was in our data centers we had many
jvms running on the same physical box
that means you don't get isolation
between services it means that from a
performance perspective it's always
possible that a service could take out
another service I am at noon ok own you
know a lot of our big major outages were
actually caused by those by their
circumstances
so effectively but we moved to Amazon we
basically said let's figure out the size
of our JVM that we need let's get it
onto a right-sized machine and Amazon
and then let's see what goes on okay the
more recent ones the more recent boxes
that were working within the team
account we're using docker to provide a
kind of a thin layer than around that
that running instance the reasoning
behind that then is that effectively the
what's in the blue layer there what's in
docker constitutes an immutable service
deployment right once you've got that
docker image that's it that's the
version number of the service and you've
confidence that that will always run
okay but what we do is we allow the a.m
is to be slightly mutable so that we can
you know upgrade or change or do things
there and that's I think something
that's worked very well for us so a gilt
we're using docker extensively but we're
really are just using core docker we're
not using any of the more fancier
orchestration features or things like
that when we go and we work with deploy
services to to the new team accounts
effectively the implementation of this
simple discovery model is slightly
different effectively we're still using
zookeeper for our for our repository but
we're using Amazon elb is a load
balancer okay so we just kind of changed
our approach there yeah we've questioned
correct yeah exactly so the client there
could be you know hey I'm the the search
web app and what I wanted to get is the
user information so they can put the
username on top of the the rendered
output so I'm hitting the user service
through some kind of load balancer and
I'm getting the response on the users
information yeah okay okay so the the
number of running instances per service
then so when we think of the number of
instances of any given service right
then it's usually tree so that's
actually ground there are some services
that are due to the extensive amount of
load and computation that they get we
have a much much larger number okay but
on average you know most of our services
are simply deployed on three instances
at a time I think this is really
interesting this way
because it shows that we're actually
living the dream very much in terms of
micro services because if you look the
bulk of the machines that were deploying
to our micro and small okay so and that
actually kind of makes sense right where
we're deploying the service is a single
service to a machine we're just using
the right size machine for for what we
need question so that's interesting
actually so deploying to AWS our costs
have come down right now it hasn't been
like that crazy you know hey we're doing
it for nothing right i mean like we got
a little bit of a benefit in terms of
cost what i think is interesting though
is more we now have a way of tying the
cost of our services and our initiatives
directly to to the teams that are
actually doing them okay so on a monthly
basis we now review our Amazon our
accounts and we spot hey personalization
you guys are spending forty percent more
than you did last month you know what's
changed right and then then you discover
oh we implements a new service and we
forgot to resize it or we left something
running you know and then you turn it
down so that kind of you know closed
loop from the people who are spending
the cash you know like having them see
the bill has actually been really great
in terms of driving down the cost so so
this is the overall kind of picture of
the entire evolution of guilt in in one
step right and it captures those major
moves right from you know large teams
all competing on a small code base to
massively kind of distributing the the
architecture and then finally then
moving moving to the cloud so so things
that we learned okay so number one we do
love microservices okay what it has
allowed us to do is to code faster we
have less lesser dependencies between
teams right I'd have to wait for another
team to get their code done to get the
whole service ash I can code whenever I
want and we used to have this thing at
guilt that you know during eleven
o'clock and three o'clock on a given day
you we're not allowed deploy anything
new okay because the risk associated
with that was just too high but when we
went to micro services
were able to easily deploy any component
right the way through the day and I was
actually one of our goals how do we make
it so that we were always deploying
software okay so without the
dependencies between teams there's lots
of initiatives that I get to work in
parallel and that's actually been very
very nice as well right so you don't
have to kind of have that bottleneck we
allow people to use you know whatever
technology language they like generally
we're still on the JDM we're still using
JVM based languages okay but the option
is always there to try something new
okay one of the things there i think is
by looking at docker as unit of
deployment that actually makes it you
know a very level playing field for
someone to deploy you know whatever they
want graceful degradation of service
it's quite possible in any given day
that some of our services might break or
we might expose a bug that we weren't
sure of right what's interesting there
though is is the core e-commerce flow is
not actually affected and that's been
really really nice parts can break will
rush to fix them but the the whole
experience will actually keep on running
okay one of the other things as well
that was really good is this idea of
disposable code we've implemented whole
features you know like from web
application to services to databases
where we did mented the feature for
example of one stage we implemented
reverse auctions on gilt com we tried it
out we found it confused and distracted
our users or users didn't like it and so
effectively we just turned it all off
okay and archive the code and that's
actually really great you know what I
mean to be able to just you know remove
code that really isn't adding to the
business it's very very nice okay also
as well I think in fairness as well we
pretty much love the cloud because what
we always wanted to do DevOps we found
that it just was very very hard to do
when we were running around data center
since we have moved to Amazon and since
we've taken on this idea of the larger
teams owning their own accounts this is
meant then that we can actually get a
lot more a lot more done and we can
really do DevOps we weren't really able
to do that before okay so lots of
goodness there lots of greatness
actually as well you can see there
low barrier of entry for new tech you
know our engineers can experiment with
you know new data stores new queuing
systems whatever they like the other
thing we do get as well which is really
important for our business is isolation
the idea that software running on these
different genes they don't actually
crossover or affect each other I was a
big problem for us okay so it's all
rainbows and unicorns okay except when
it isn't okay so there were a lot of
things that we learned along the way and
so I want to kind of share things that
were hard okay so first one of the big
problems for us that we didn't expect is
that when you've got you know 250 300
services right it's actually you need a
new approach or a new way to think about
how to do your testing you know
environments because if you think about
it like we had let's say I know five or
six different testing environments
previously and they were all running
around 2030 services and everything is
fine and all of a sudden then we're in a
situation where each environment we need
to run 250 services right that's really
really hard to maintain and keep those
environments up-to-date and what happens
then is is you're testing an environment
that's at a date you think it's all
going to work they then deploy to
production it doesn't work because the
original test environment wasn't
actually reflecting what was in
production and that's actually a real
problem okay so what we've done is we've
invested insane how do we test in
production okay what are the techniques
the teams can use to deploy code safely
to production in a way that they can
verify that everything is working but
also that you know like our customers
are not affected okay so we have a lot
of automation will use initially dark
canary code so that's where we release
our engineers can see the changes but
our customers do not then we upgrade to
a natural canary release which is where
of the four services one of the services
is running the new instance okay and
then finally then you roll out so that
kind of staged rollout can be very very
good there's also some really good ideas
from fault
works on this idea of your better to do
as much of your testing you know within
the repo itself in terms of unit testing
and then go up the pyramid okay and when
you actually get to this final stage of
deploying to production then your entry
or your testing is is smaller it's a
smaller amount of sanity testing to make
sure that everything is okay okay so
there's a you know that it's still an
area of big debate at guilt for peeps
some teams testing in production is a
no-brainer they do it all the time for
some teams right it's not okay and what
we found there was for example if you're
the order processing team right you're
testing software that actually could
send stuff to a user or you know have a
payment or an account credited that's a
very delicate area of the code and
testing in production you know it's hard
okay so effectively what we've done
there is we looked at ways to create a
minimal testing environment for those
teams and there's only two teams
actually that actually need to have a
staging environment now and that's
something that we learnt along the way
okay so i would say if you're thinking
of going down this this road please
think about how you're going to test
very important yeah so yeah so basically
we've created one kind of sandbox /
stage environment where we've hosted
only the services which is a subset
probably i think around 90 services that
those teams need to verify their code
okay and that's being a compromise over
time that we've landed on okay okay so
ownership ownership is massively
important okay ownership is important
because you know with the guys who write
the services they captured the domain
knowledge of the business and the domain
knowledge of the code that they've
written okay what you see over time is
that tech organizations change right
great people come to your organization
great people move on okay and it's very
easy then when there's all of these
services it's very easy to lose track
okay and figure out you know like who
actually owns this how does this thing
work okay and so we've actually I've got
a couple of reasonably new slides on
this
we've thought very very deeply about
ownership on the way you know an
organization should own software okay so
the way we're looking at it is this
right the teams of let's say four to
five engineers right there they're
responsible for getting their code
written and they getting their code into
production okay so they're responsible
we don't have a separate kind of site
reliability engineering team that ruins
your stuff you're responsible to run it
okay so the engineers are responsible ok
now we group those teams into teams of
teams we call them departments okay and
that's led by a director okay and that
director has got a good engineering head
on him or her and effectively that
person is accountable they need to make
sure that whatever the changes in staff
okay that the actual the main knowledge
of what these services do right and how
we run them is kept you know within the
organization I say they're accountable
okay separately then we've built this
virtual architecture board by plucking
principles and senior engineers from
various departments right and say anyway
we want you guys to get together right
and give guidance to the organization
right so those guys are great they're
not an ivory tower they're actually very
much embedded in their own departments
what they do is they give guidance to
all of the engineer is a guilt and
what's great is as an engineer you can
completely ignore what the architecture
board has told you to do right you have
that right okay but you do so at your
peril okay you know if you really want
to you know ignore the smart folk who
maybe have experienced systems on this
before then away you go okay and then
last of all techie tech exec they remain
informed write effectively so we let the
people running in Tekkit kill know
here's what's going on okay this has
been really really important for us
another thing that's been important is
classifying ownership this is a gift
that I give to you because this has
turned out to be really useful for us
okay we decided to think about software
components that we had and we said well
any given service if I own it right it's
either active ownership I'm developing
on it the code is clean it's absolutely
fantastic and feel great about it okay
or it's passive ownership passive
ownership is haven't touched it in maybe
three to six months
but we know what it does feel good can
hop in there at any time fix something
if there's a problem okay at risk
ownership is WTF why the hell have I
been given this service I don't think I
should own it I never wrote it I don't
like that code I don't want to go in
there okay we go that at risk right
what's great though is is when you
classify the services in that way and
it's a very simple classification you
end up then having you know these I call
them an ownership doughnut that's just
the way I drew the graph the ownership
doughnut can show for any department how
much risk they're carrying and how good
they feel about the software that they
hold okay but also when we think about
the amount of that risk which is in red
that means we can actually use that to
inform our technical strategy right hey
it's clear this particular team has a
lot of problems in terms of ownership
and the next three months we need to do
something about that maybe that's
rewriting a service maybe that's just
you know documenting a service you know
we need to do something about that and
this is turned out to be very very
useful for us okay so some other quick
area is deployment the when you deploy
to the cloud and you deploy your
services having great tooling around
deployment is really important we have
lots of great tooling we have too much
great tooling around deployment we've
about seven different ways to deploy
services right now guilt right and
that's because there's so much change in
this area we're kind of honing in on a
core set of AWS deployment tools that
make a lot of sense however one thing
that we have worked on as well we're
still using is something called iron
roller and this is a really really neat
tool for having this continuous
deployment okay it handles the entire
life cycle of of rolling net software to
a brand new set of nodes and Amazon
gently migrating the traffic over
keeping the old instances running so
that if you need to do a rollback you
can roll back instantly okay and that's
actually been a very very nice tools so
github com guilt on roller we open
source that recently one of the other
things i think that is super important
is lightweight api's we made a big
mistake I think about two or three years
ago made a lot of sense at the time in
terms of handwriting client API is to
handle these services
these services these clients were
written in Scala and Java and the
problem that we had there is is that the
client had dependencies and the
dependencies at dependencies and so
basically to use any client you were
pulling in a whole tree of dependencies
and that it turns out then causes lots
of problems in terms of future upgrades
and conflicts and software so what we're
actually moving to now is I guess two
things here first rest we make sure that
everybody does proper rest api's we
document them in a non programming
language right with document then using
API adopted need something similar to
swagger but then we generate clients and
the beauty of the client code that we
generate is that it's a single file and
it only has has no dependencies right it
uses whatever api's are commonly
available on the platform that you're
working on okay and that means then
you've got zero dependencies it's
actually very very nice ok the other
thing then so we've got this tool called
cave so you're deploying lots of
software regularly to production one of
the great ideas and I picked up from the
guys at near form is this idea of saying
you should look in your system for the
things that should always be true and
you should monitor those things that
should always be true and that way when
they break right well they're no longer
true then you know there is a problem in
your system this is a great idea so we
have this software that we wrote called
cave and what it allows you to do is to
create rules like this one here the
number of orders that have shipped to
the US in the last five minutes should
not be equal to 0 right or rather if it
is equal to zero that we need to raise
an alert okay so buy it by tracking
business level metrics then we know that
with any given deployment we can look
back and go hold on a second something
broke after this deployment let's find
out what's happening there good question
yet
what which does Riemann I don't know
Riemann it could be it could be so we
started this we started cave about a
year and a half ago okay and I think at
the time you know doing this kind of
time stream analysis we didn't we
couldn't see a tool that would do it
because effectively for us okay so we
wrote something ourselves but certainly
you know if there's something there now
I mean I'd be very interested to look
about it so maybe we'll chat afterwards
that'd be great okay so two of the quick
things before I run out of time I owe
explosion this is something that you
really have to watch out for if you have
services that on the given path call of
the services and those services call
other services and they call it the
services you end up in a risk of having
you know all of your code just kind of
you know fan it okay with all of these
extra calls okay and I think that we've
discovered there that you know this very
very free and open and almost an Arctic
way of writing services we've actually
left the possibility now for for lots of
services to be over cold or under cold
okay so we're looking at different
approaches now for this you know we're
looking at lambda architecture for
critical API so the idea would be that
for some of our critical calls why don't
we precompute as much of the answers as
possible store them and then it's just
an order it's an order one look up or a
constant time look up for the results
okay and that's actually a very very
nice kind of way to look at things so I
explosion last thing that you need to
watch out for is your your data team
because what happens when you explode at
all of your services and you explode a
door you decentralize all of your data
and so previously the data team was able
to join everything right and undo these
queries for your business intelligence
is your insight okay and the problem
that we have when you split up all your
services like this is the data team will
go where the hell is everything okay so
effectively we've built services to
ingest events from all of our
microservices you know so that we can
pull them into what we store them on s3
in big buckets
and then we pulled them into our data
whereas so that we can still get the
business intelligence that we need okay
I'd say it's really important you know
have a strategy or ever think about how
you want to pull the data back together
okay very important awesome so that's it
that's my by seven challenges thank you
very much for coming today I'm around
here for the next couple of days and
give any questions that are you grace
question good question so we when you
say transaction I presume you mean like
a transactional context so we avoid you
know distributed transactions like the
plague we simply don't do it so we try
and make all of our services idempotent
so that you know if we have to do a
replay from a client that hasn't got a
receipt then we do that so all of our
kind of error handling around that would
be handle at an application level I
think the experience that we've had is
simply being that we've never found a
good distributed transaction processor
that would that could be used across the
entire board so that's our case there we
move it all into the application level
code sure yeah
yeah so it's a good question so one of
the big things that we did was we moved
all of our user data from a system built
on postgrads the original system we
moved it to MongoDB ok and effectively
as part of that transition we built the
code to sink back to sink forward the
data into a Mongo but then once we made
MongoDB writable we sink that
information back so all the information
in the old postcards table was actually
always kept up to date ok so there's no
simple answer to that it's like if you
need to have the data in multiple places
you have to write code to do that too to
do the the move migration take out here
works yeah why are sir cuz I'm with
sacks with Avenue and we have will
working with on it turns out almost step
by step and what's exactly the same as
what you're doing to kill ok good plus
work with all works also yeah awesome ok
cool cope we're doing the same thing
with scaling up with an emergency system
with a bunch of microservices everything
is we're migrating yeah nice awesome
cool it was a question there yeah so
we're actually very fortunate in that
regard right now we don't have a
security issue because most of our
payload is just plain text right it's as
in its if none actually you know
insecure payload or payload that needs
to be secured ok what we're moving to is
a model where again so we have so we
have closed doors around or you know
like a closed firewall around all of our
internal traffic what we're moving to a
model is as we push api's and move them
to the front where a public can actually
call those api's we're looking into
using Amazon's API gateway to provide
token-based authorization and
authentication at that level there ok so
yeah cool take up more yeah
so it's good points so the service is
deployed as clusters generally it's a
mixture of availability and scale okay
so we took a general like top of the
head rule two is not enough three for
most services should be enough okay and
that's what we typically deploy to but
some of our services may have like 10 or
12 instances as required what we're now
moving to is like typically we over
provision which we've always done it
guilt and that's what we're currently
doing in the legacy VPC however newer
services are actually being provisioned
using auto scaling groups so for example
our cart which we recently rewrote over
DynamoDB it's usually down at two
instances during most part of the day
and then it scales up to six jury noon
so something fun that's actually very
effective for us sku sorry i sorry I
couldn't hear that sorry the store is
component
Oh what kind of database is reusing okay
so typically so we've used a number
postgres very popular right Weez RDS we
also use Voldemort historically but
we're moving away from this are the
things that we were using in Voldemort
we're now using dynamodb for okay you
still have MongoDB as well but were for
some of our services we're moving off
MongoDB I think the thing that we we
face is that whenever we implement a new
service and we're in the Amazon kind of
environment then it's it's it's usually
choice between dynamo or RDS we find
that's the case okay great I'll take
this one train service versioning as a
great question so one of the great
things about api doc is that it actually
allows you to version your services
right with a major minor update or a
major minor patch scheme so using sem
ver and what's really nice about api doc
is it will actually tell you if you if
you make an update to the service and
you upload that change it will say okay
this this is actually a minor update
because you added something but if it
sees that you've removed something or
changed something then we'll actually
say this is it will force you right to
say yeah this is a major update okay so
so typically yeah typically like that's
been a nice kind of thing to have now
but for all of our services over the
last two or three years like before api
doc we've simply you know just gone
through great pains to make sure that
they were they were backward compatible
right now always just always make it
backward compatible because in general
general you don't have control over the
number of clients who are using your
software so realistically you've got to
keep you know backwards compatibility
for as long as the clients right there
okay cool I'll take maybe yeah one more
great
and so we put our data again our
database changes as well are always
backwards compatible okay so that's been
like that's been important for us
particularly so if we look at our legacy
large Postgres database there are so
many services that are using that that
if you were to make a breaking change to
the database effectively there's no way
you could restart all whatever 30 or 40
dependent services at the same time so
as a result on those databases we've
just moved slowly to get the upgrades
being incremental okay all right cool
great okay maybe we should wrap up and
I'm like I say I'm around open four
beers or coffees or whatever so please
can I come up all right thanks very much
everyone thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>