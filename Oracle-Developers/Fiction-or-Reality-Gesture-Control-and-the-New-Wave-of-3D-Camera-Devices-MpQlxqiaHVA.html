<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Fiction or Reality? Gesture Control and the New Wave of 3-D Camera Devices | Coder Coacher - Coaching Coders</title><meta content="Fiction or Reality? Gesture Control and the New Wave of 3-D Camera Devices - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Fiction or Reality? Gesture Control and the New Wave of 3-D Camera Devices</b></h2><h5 class="post__date">2015-06-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/MpQlxqiaHVA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi welcome to the Java one here in San
Francisco to the talk fiction or reality
gesture control and the next wave of 3d
depth cameras so before we start the
talk we want to introduce ourselves so
this is thomas entras and he studied
computer science at the technical
university in Munich in Germany so we
are from Germany where the Oktoberfest
is yeah he studied computer science and
he is working as an software consultant
at TNG technology consulting and beside
his job is a lecturer at the University
of Applied Science lund choot yeah and
I'll introduced Martin Martin also
studied Computer Sciences at the
University of Applied Sciences in meza
book and after that he also started at
TNG technology consulting and in our
free time we're doing this stuff so we
are controlling real-world objects with
gesture control with bare hands so to
say yeah that's right so maybe most of
you know this picture this movie its
minority report from the year 2000 to
Tom Cruise is standing in front of a
huge wall display made of glass and he's
able to sort and organize informations
with yeah with bare hands and our
question is is this still fiction or is
it reality so this is what the talk
about us and you will see some awesome
demos tonight today address we have a
look to the agenda we want to show you
the history of 3d depth cameras and
gesture control of course we want to
show you the state-of-the-art then those
awesome demos we will control this
parrot ar.drone here in front of us with
bare hands without sensors anything only
with three eat depth cameras and of
course since this is a java conference
we want to show you some code but
pre-prepared it's not only Java code
it's also some JavaScript and some
c-sharp code and of course we want to
describe
explain that this technology behind this
stuff then we want to speak about the
areas of application and why you should
care about the stuff as a software
developer last but not least the
conclusion okay Thomas let's start okay
when we think about gesture control it
didn't start with the Minority Report
film it started in the let's say in the
year in 1993 so the Silicon Graphics
indie cam which came out and this year
was the first camera that was really
capable of doing something like gesture
control it had a resolution of vga 640
cross 480 pixels and this at a frame
rate of 30 frames per second it didn't
include a depth sensor or anything like
this is what was just a normal RGB
camera but you could do something with
it and there's an exhibition at the
german museum at watches museum in
Munchen and there you can see this
exhibition so I'm moving my head around
and at the top you can see the rest
Linkin graphic cindy cam and the skull
in the background also moves its head
like I do yeah but since this is not a
3d camera it is just a normal RGB camera
you can easily trick it because it just
uses simple algorithms or a simple
subtractive to filter so if you are
using something that looks like a head
and has the shape of the head and the
color of a head you can easily trick it
for example with a hand so you can see
the scholar also moves and it's even
better than with my head so this is not
ideal but we have it was the year
nineteen ninety-five when this was
created so let's def a bit forever yeah
Thank You Thomas now we make a huge step
in history from the year nineteen
ninety-five in the year two thousand six
and you see the Sri remote from Nintendo
it's this little handheld device and in
front of this there is an infrared
camera and it is able to recognize
infrared sources so normally you hold in
front of an infrared bar and then you
are able to identify the relative
position of this little device and with
an implemented g-sensor you are able to
gather data like the the pitch or the
role or some other things it's based on
bluetooth and that's the year two
thousand six now we make a step into
2008 you see those time frets are
getting how to say small on smaller 2008
oblong industries with their project
cheese speak maybe they are inspired of
the film of the movie Minority Report
you can see the smart dynamic software
consultant sorter sorting and organizing
information without touching anything
with bare hands and he can draw ugly
pictures on this wall to with bare hands
ok but with this broad shake there is a
little problem Thomas what is it yeah
since we want to move freely around we
don't want something like gloves on and
so the this is the year 2012 and then
the kinect for windows pc came out and
this was the first camera that could
actually detect posture defects so you
can see that i am in front of my
computer for way too long and yeah what
what was the specialty about this is it
is a general-purpose camera and with the
sdk you can track something like
skeletal data so you can track the
different bone up to i think twist 20
bones and this is detected and it can
track up to four people with this it has
a resolution in RGB mode of 1600 cross
one thousand two hundred and a quarter
VGA and depth resolution yeah and
normally this is used for playing games
so you can do the same as your character
in the game does and he
we'll follow your movements and yeah it
uses a technique called speckle pattern
which will introduce later on and with
the speckle pattern you can detect the
depth within a range of 0 dot 5 meters
to 4 meters anything else that's nearer
then this is the so-called near mode and
that's not as precise as the far mode
yeah so let's step one yeah forward
forward one year only one year this is
the leap motion who of you have ever
seen this device okay okay and it's this
little USB peripheral device and I saw
it the first time in 2012 i saw a little
promotion video and it wasn't a first
april on the first april and i thought
it's an april's fool but Harvey you
later I've just received this
development unit and yeah it wasn't a
fool so what is this device about I just
disassembled it for you without able to
be able to assemble it again so it's
this device and within the leap motion
we have three infrared diets and there
are emitting infrared light so you just
place your hand over the leap motion and
it reflects the infrared light and those
two cameras up here they can gather they
can record this and with some software
algorithms they are able to identify the
position of the hand so you can identify
behave or the translation over this
device end it is really precise it has a
accuracy of what 001 millimeters so to
get an impression how it really works we
have a visualizer here and it's
completely written in HTML 5 without
WebGL or flesh or anything like this and
I have a leap motion connected to my
laptop and I placed it here my hand over
the leap motion and now you can see when
I move my hand
it's really precise I can make a roll
gesture for example roll pitch can
change the eight and it works fright too
I would say one meter I don't know what
it doesn't feed or yards because we are
from Germany and in science we are
working head with the metric system at
science ok and now we came to the next
camera and it's the creative gesture
camera since 3d it's also from the year
2013 you see the market the summer float
adrift those devices it's a joint
venture of creative labs and in turn
creative labs just build the hardware
and into the SDK and in Germany would
say it is an eye on leading the bottom
is all the one to one translation into
English is yeah so it makes no sense for
you so I would say it's an all-rounder
you know ok why is this the case and
it's because the camera is able to
gather depth information it can
recognize gestures it can do speech
recognition and it is able to do some
kind of face recognition to identify
different people so it works with two
cameras we have a normal RGB webcam in
this case with 1000 2280 cross 720
pixels so at least 200 s 720p and we
have an infrared webcam which works with
qvga and both are based on 30 frames per
second ok we have a small visualizer for
this tool but only for the hands at this
case Thomas please hold the microphone
or just tools it's ok so please maybe
two hands okay and you can see at those
hands are recognized and if Thomas is
spreading his fingers then we can yeah
it's not that precise but the current
cameras of int'l are more accurate
okay this was 2013 now 2013 again so we
have the Maya labs variable gesture
control and it's a smaller device which
you were just attend to your attached to
your arm and you can see using your
muscular movement you are able to
control drones or entertainment systems
or whatever you can see that the
muscular movements generates different
kinds of data this is a crowdfunded
project too there are several others but
this is maybe a more important one yeah
you can do presentations on this too not
a problem okay but with this we have a
little problem again same as before we
don't want to wear anything to control
stuff so once again we'll get back to to
connect in this case the Kinect version
2 we got a development version in
December last year and we could try it
out a little bit it is an evolution
compared to the Kinect version 1 so in
this case the most important difference
is that it has a time-of-flight camera
included and it has a higher resolution
both and depth as in RGB so it has a
1080p RGB camera and some we have
resolution near vga in the RGB spectrum
and a def spectrum so what can it do
that the old one couldn't it can detect
audio beams so you can see where the
audio comes from it can also and detect
up to six bodies now and this near mode
far more distinction it's hella part so
we don't need this anymore and you can
do cool stuff like extracting the
background from the scenery and with in
the last few weeks the Kinect fusion
came out for this kinect version too
also so let's see what we can do to
visualize this one I'll just step up to
the scene I don't see if you can see me
and you see ya ok
now I move mom around a little bit and
you can see that it's really precise
yeah yeah okay and let's get to the last
one of the cameras this is once again an
evolution it's this case the successor
of the creative Jeff check em this one
is called intel realsense camera and
it's once again it has a higher
resolution both in depth as in RGB so I
am def we have a 1080p and in n no not
in that in RGB in depth we have vga
resolution now and this camera can track
more ODS DK can track more points for
the head for the hands and it has a new
mode which can scan the scenes or you
can use it for 3d scanning also yeah and
it's a pretty small device you can see
that this is the whole complete camera
and it so it can be built into laptops
and that's the interesting fact it will
be built into laptops within the next
few years and tablets and tablets also
yeah okay um now that so history is not
over but we are finished with our camera
talk and we'll get to something you can
talk about later on we'll get to the
demos so the first demo is a little game
it's called moon in Germany in America
it was called crazy chicken yeah and we
don't know if it was a hype in America
but in Germany it was a total hype back
in nineteen anyone knows the game named
crazy chicken No Wow Germany was a real
hype it was going wild was crazy yeah
you had to play it at work and
okay I'll try to play this little game
oh it's a little bit difficult to do
this yeah because Thomas is not seeing
this game right on his screen so he has
to orientate there hopefully we can do
it sue doesn't ya kicked off ok perfect
ok so now I can move my hand over the
leap motion and you can see I move my
hand and the cursor also moves so the
next thing I wanted to show you is how
to how to shoot at the birds so I'm make
this gesture and with my middle finger I
can then shoot and to reload I do the
same with my whole hand so moving it up
and down ok so i'll type in my name yeah
and Martin why by telling us is trying
to type his name from this position some
words to the moon application it's a web
application it's completely written in
HTML 5 again no word chiona flesh and he
just used it used some kind of coffee
script here which is I i would say lay
on top of JavaScript to write browser
compatible software much easier ok
Thomas M from this position I think it's
really hard to type the whole name so
just start the game please yeah I got it
with double oh yeah ok now I'll start
the game and as you can see the chickens
are flying in and I can
maybe you get out sorry I'm so
interesting to me okay okay this was a
short emo and back to you toes yeah so
now we'll get to the code and as it's a
Java conference will of course talk
about JavaScript now and since you
already saw too much java code okay and
what do we have to do to actually
control the scenery are to control the
leap motion we have to at first create a
leap controller object with some options
in this case we are enabling then we
loop over each frame and pass in a
closure and this closure is called
whenever a frame arrives and then when
the frame arrives we can for example
lock all the gestures that were recorded
then we had just have to connect and
that's about everything you can also
grab other information but we'll get to
that in the Java example that Martin
will tell us about later on so this was
the first demo now the interesting part
it's how to fly this parrot ar.drone
with bare hands so um for this we need
to interpret different types of data so
many the six degrees of freedom because
we want data about we are how to fly fly
backwards and forwards and how to roll
the drone so that we can fly to the
right and to the left and so translation
data up and down and what ever so Thomas
who started our software yeah we're
ready now okay so at first I'll explain
to you how this stuff works and then
we'll fly around because it's difficult
explaining and flying around at the same
time okay and so to start a drone and
hold my hands like this and then address
up gesture and the drone starts from now
on everything i do is relative to the
positions at stardom and now I can move
my hands forward and backward and the
drone will do the same thing and I can
also use it like a virtual airplane yoke
so these old airplane steering wheels
and so I go to the right and to the left
and the drone will do the same I can
also move up and down and I can use this
to control the off the drone and when I
want to land I to the offense down
gesture once again and the drill it and
that's about everything so okay Thomas
ride out so I have my hands and be
careful hahaha we don't want to get sued
okay now I can move to the right to the
left
now it's time to land the drone because
we don't have that much time yeah okay
perfect thank you thank you thank you
okay um so you switch this light but
yeah so let's get back to the code
because now we really have Java code but
it doesn't really look like Java code
and this is the case because we have a
native interface here so we'll use Jay
and I with a dll and until didn't
provide a native Java interface it
provided native to interface but not in
pure Java okay so and what do we have to
do to control something using the Intel
perceptual computing SDK first we'll
have to extend this pipeline thing and
then we can create such a perceptual
pipeline we then acquire frame which
basically does nothing but saying later
on we want a frame to be to be used for
our purposes and then we can create all
the objects where we want to save the
data into so we create a gesture here
and then we query the pipeline for this
gesture in this case it's any gesture so
any gesture will be recognized in this
case and you can see we pass in the
gesture object created before here so
it's a lot of reference passing here and
then we release release the frame which
actually waits for the next frame and
then we can read out the data from this
gesture and just for example print out
the label of the gesture which is an
integer constants which resembles the
gesture that was actually recognized in
this case so it's really really see
style and since we didn't want it to be
this way we implemented our own wrapper
around this and now it looks this
in our software so in this case we can
now call the factory method build
perceptual controller we can then
connect to the controller that was
created and then we can add listeners to
it so this is the listener pattern and
yeah it's Java 7 I don't pass in lambda
in this case but I could in Java 8 and
on the on detection method then M is
used for maybe printing out the
coordinates of the left hand in this
case so this is a little more concise
and you don't have to write so much
boilerplate code for it to disconnect
just call this connect once again okay
so so much about the code now let's get
on to the next step maybe you want to
tell where we where the audience can get
the software you can get it on github
using our page but we'll get to this and
the question section yeah thank you okay
another very short demo and it was one
of my first ideas using the leap motion
why the leap motion again this is due to
the fact that we want to show you some
Java code not JavaScript code with the
leap motion so my very first idea was I
want to use this leap motion since I'm a
musician to to control my synthesizer
with bare hand so I can build my own
midi controller yeah controlled with
their hand so you know what I'm theremin
is it's the predecessor of a synthesizer
it was built 1890 about I don't know
from a Russian mathematical named tiara
mean it's the complicated instrument all
over the world beside the violin and i
will start my project right now and i
have to turn the volume on so one moment
so ok I place my hand over the leap
motion and changing the heat
so these are the sounds and it sounds
like cats playing a death match game
okay this is due to the fact i'm using a
Microsoft MIDI mapper to bring my
synthesizer from Germany to hear it was
too expensive anyway let's have a look
to the code and here we used the Java
API which was delivered by leap motion
and this really looks I would say good
we have the it follows the listener
concept and what you are doing here is
we have on frame you end and it comes
with the controller object so then we
ask this controller for frame and then
we can access data about yeah do you
have some hands and if yes you can
access them why using an index and then
you can get the data your example the
the y-coordinates the pitch the role or
whatever you want so we just wanted to
show you how it works because as you see
it's really easy to use so maybe you can
develop some ideas and row it write your
own application and if you make videos
of it just send it to us okay so much
for the code now we want to have a look
how it works from the technical side
yeah and the cameras use some technology
there are two main concepts you can use
here and the first one is the parallax
concept which basically resembles the
functionality of the ice so you have two
eyes or a we have two cameras here and
so the a and B are the cameras and
there's two objects we're looking at one
is near that's see and one is far away
that's c-tick and as you can see the
father the object is away the more being
the greater the angle is so foreign
infinitely away object it's 90 degrees
for an object of this diary
ugly in the line of sight between a and
B it's zero degrees and yeah using some
basic mathematics you can then calculate
the distance between this line a B and
the Point C or C tick just using the
distance between these two and the the
angles alpha and beta which you can
determine and this is not very
complicated so let's get on to the next
slide and you can use this parallax
technique also for 2d games because you
can create the illusion of a 3d
environment in this case so if you see
the cubes moving in the background and
the cubes the we are showing in the
front the ones in the background move a
lot slower than the ones in the
foreground so if you move the background
at a slower pace than the foreground
then you can create a something that
resembles 3d and looks a lot more
natural and if you do it at the same
pace okay and that's called parallax
scrolling so if we got the position of
C&amp;amp;C tick it's easy we need we need to do
just some basic mathematics but we need
to get these positions in both images
first and there comes something called
the speckle pattern which for example
they leap motion or the Kinect version
one are using so this is a photograph of
audra penguin which who created these
beautiful images using the Kinect
version 1 and as you can see the camera
itself sends out apps you to write a
randomized point pattern that's so
called speckle pattern and then you can
do some image as some pattern
recognition on this pattern so if you
see maybe these three points here
resemble the same pattern in both images
then you can say they are at the same
point and now we got the point now we
can calculate the distance to it and
traditional computing you would do some
tracking like Hetrick
and then you've got the position of the
head but you cannot do this with general
purpose cameras because we don't know
what we are looking at so it could be a
head it could be a table could be
anything okay so and of course those
points those patterns are not visible to
us because it's infrared yeah otherwise
it would look silly right okay so now as
we already recognized that you like the
drone demo really much we just decided
what about another demo with a drone but
now with the Kinect version 2 and with
some other kinds of metaphors we are
using so-called gesture metaphors so
start your smartphones and record is it
with your video cam okay so we have a
the kinect version to hear everything
wait a minute wait a minute okay I wait
a minute a little problem with the
performance okay and I will explain how
to fly the drone bee course explaining
and flying is really bad so one moment
to start the drone imagine that I'm a
jeedai you know sorry it was just a test
yeah OMG died and i started form of this
gesture you know start the wrong and
then afterwards I'm able to control the
drone with my whole body's on my whole
body is a joystick I can move forward I
can move backward I can move to the
right and to the left isn't it really
Jedi like yeah absolutely hey maybe like
it runs read about it Jedi but that one
okay so let's get started yeah okay I
magida start the drawing or we have a
performance
let's see oh okay we have yes and a
little bit of a problem by now maybe we
can get over we will try it again oh
it's something is really strange right
now yeah okay try again oh this is a
pity okay maybe we can try it afterwards
and maybe there are some minutes left
off now and we'll try it again this is a
pretty it worked oh and our rehearsals
at work every time okay and let's
continue and maybe after words we short
okay but let's have a look to the code
Thomas and I will check it okay so um
we'll see some c-sharp code in this case
because Microsoft would never ever write
something right at java api for the
kinect so we have to stick with the
shark or javascript in this case we took
c sharp and and this whole thing
resembles a little bit the windows 8
methodology so we're opening a reader in
this case and then we create then add
the body's array and we use this
wonderful where I have a body count
which then tells us how much bodies the
kinect can actually track at the same
time and then we bind the method body
frame arrive to the frame arrive event
and this method is called every time
such a frame comes in and then we can
actually get the frame reference out of
the body frame arrived even arcs and
with this we can acquire the frame then
using the frame we can get and refresh
the body data and write it into this
body's object and then we got all the
data we need within this body's object
this is a lot boil
of boilerplate code but when you're done
with this it gets easy because then you
can easily extract things like give me
the position of my left upper shoulder
oh I don't have a lower shoulder m of my
left shoulder and stuff like this and
then you can easily extract the
information out of it okay so so much
about the code do we have before we can
try it again okay just okay try it again
hopefully work right now looks better it
looks a lot of lot better okay I'm a
jeedai right now okay and I start the
drone
and to land the drone I just have to
make a circle gesture again thank you
very much okay okay next step yeah now
we'll get back to the technique behind
this camera because we also wanted to
show you the time flight cameras and how
this works so for time of flight maybe
the next slide please thank you you can
you're actually measure the time the
light takes to get from the camera to
the target and back so in this case
margin is now a time a life particle
we're using Latin particle nature of
light now and he travels from the camera
gets reflected by the target gets back
to the camera and we're measuring the
time it took for him to get there to
actually measure this there are two
techniques so the first is the past
technique where you send out a lot of
margins at the same time and then you
detect when they arrive so there are two
gates one is opened when the emitter
opens the second is open that half of
phase whereas the first one closes and
the second closes at the full face and
then you just take it the quotient of
these values and this is directly
proportional to the distance it took the
light particles to travel this distance
yeah it's algorithmically totally easy
but mechanically it's not that easy
because you need to open up the gates
for a few nanoseconds only and then you
have to close them again and so this is
a little bit of a pity so we're try
we're trying something else so this is
the continuous wave technique now and
which uses the wave particle at the wave
nature of light we cannot demonstrate
this using Martin because it's not a
perfect wave function and but we can
send out the light in a continuous way
now so we don't have to open
closed open up close once again and so
we're sending out light from the emitter
and detecting when it gets back and then
we can use a little mathematical trick
called cross correlation to determine
the phase shift between the incoming
waves and the outgoing wait and this
phase shift is then directly
proportional to the distance the light
traveled on its way and yeah there's one
problem with it you can only go one face
because otherwise it would be the same
as going a little bit over one
centimeter or something like this yeah
and yeah and this is the major problem
with it otherwise it's really simple to
use because mechanically it's no problem
to do this and algorithmically it's also
not such a big deal ok ok so far code
and technology next thing is the areas
of application and why you should care
about this stuff as in software
developer ok maybe some of you already
have such an device in the living room
it's a so-called smart TV so let's say
you are laying in the bed you have only
one hand free and the remote controllers
on the other side of the room so you're
just able to switch the channels or turn
up and down the volume with your bare
hands isn't this a cool thing of course
it's used in the gaming scene so gaming
consoles and consumer electronics in
this case you we see the fruit ninja it
makes much fun with the leap motion to
play this game these are current areas
of application then we have the samsung
galaxy s4 it started with the s4 and
that the so-called air gestures were
implemented so this could be useful when
sitting on the toilet with your dirty
hands with you can control the
smartphone without touching it ok and
but there are studies currently going on
which are not based on
technique they are using the background
radiation of mobile phone cells to
identify some kind of gestures because
you can save battery with this because
the cameras there you they need more
much power the next thing is you can as
you already see I've seen presentations
with it so I have my leap motion here
and as you know I'm a Jedi these are not
the droids you're looking for and it
switches oh these are really not the
droids i want to show you okay it works
another application is of an us company
called gesture they'll attach to connect
to a display so the doctor or desertion
is able to scale to zoom in and zoom out
data images which is made from patient
patient data something like this and the
cool thing is there is no need to touch
a button so he remains stereo so for
medical engineering this could be really
useful and why you should care about
this stuff is the following thing it
gets integrated in laptops and tablets
and this already happened with the ulit
pocket laptops where a leap motion is
already integrated the same thing
happens with the intern and creative
labs cameras they get in built-in into
tablets and laptops too so we will reach
more and more users asking for
applications on this so this is the
reason why you should care about it but
there are those end let's get to the
dance there are some areas of
application where you shouldn't use
gesture control we'll talk about them
too and this is the case when you have
security relevant conditions so I would
propose that you should not fly an F
in using gesture control by now because
these things are not that precess at the
moment and there are outliers lighting
conditions have to be right for them to
work and yeah it's it's simply a thing
you shouldn't use whenever you have you
have security relevant conditions so
please don't do this the second thing is
Martin the second thing is when they run
efficiency suffice so this smart let's
say software consultant is trying to
control his XO sheet using gesture
control and and you see it doesn't work
that well and this is the case because
here there is something that is really
efficient like a keyboard or a mouse and
using gesture control you cannot build
something that's more efficient or more
easy to use and whenever this is the
case please don't use gesture control
just because it's cool okay okay from
current areas of application now to the
future and visions okay it's not a
vision anymore since this is an existing
prototype of units the hcd-14 genesis
concept and theriot demo you see the
smart beautiful woman and she can adjust
the volume using her bare hands and she
can also zoom in a map in the navigation
system this is a really cool thing
because in germany on the highways we
have unlimited speed that's really cool
and if you're on your Porsche driving
with 250 km/h let's say about 140 miles
per hour and it's maybe a better thing
to zoom in and zoom out a map without
searching for a knob on the
entertainment system on the navigation
system so this can give this can be
make car driving more secure while using
the navigation system the next thing
Thomas already spoke about the security
level relevant applications so at this
point in time don't use it but we don't
know what happens in the next 10 years
about when we are speaking about priests
accuracy to you accuracy sorry and there
are studies going on about remote
surgery so they're using connects and
staff to gather the gesture data to
control remote certain robot yeah so we
will see what happens and how the
science yeah we'll make it better okay
and the conclusion about all the stuff
is that this is a new trend reaching
more and more users as i already told
you but don't forget the simplicity of
the botanist unsurpassed this means
whenever you can execute an action you
can turn on on to an offer do is if it's
possible to make it with a button just
do it with a button for example when I
started the drone with my G digester
imagine I'm doing this in my car it
makes no sense it makes driving not safe
or not easier it's just better when I
just push the button ok so don't forget
it Thomas now it's your turn yeah so am
one thing that's been missing in all the
cameras we talked about today is the
haptics because when you have something
in your hand in a real word you feel
that yet that is it is there and this is
missing with all the current cameras we
have but there is also research going on
in this field yeah definitely so use
Disney research roof there bro check
Ariel you see this smart software
consultant again playing a football game
controlled by gestures and you can see
the smart device following his hands so
it's emotes little pump which pumps out
airwaves whenever you press a virtual
button i would say yeah then you will
get in touch with this air wave and have
to feed the haptics that the perceptual
haptics you will get them back somehow
and yeah you see you there are some
research going on so now we want to come
to our last slide and we are perfectly
in time are crazy okay what we want to
say keep it simple please use natural
gestures yeah for example it makes no
sense to be in the car and turn the
wiper with this gesture on I'm not sure
what it means in America but I can say
in Germany it means something like
you're an idiot and you get a fine a
ticket for this and so huh M yeah just
keep it simple and yeah that's what we
want to say so we've reached the end of
our presentation right now and we want
to thank you and we want to tell you if
you liked our demos if you liked our
showcases just with it our tech block at
parrots on Java com you can also get
some cards from us prism Scott yeah and
yeah there are more videos we do this
stuff more than we do more projects than
these two and yeah just visit us and as
we are in developing programs we have
access we're really really access to
those cameras and we are testing them
regarding accuracy and yeah and we write
some articles about it on our tech blog
so enjoy so now it's time for some
questions I think if you have some
questions
thoroughly enjoyable demo chaps are
really thoroughly enjoyed it says you
really should get the key one at the
keynote so a couple of questions
whenever you were doing your Jedi stuff
down right now so I'm just thinking in
terms of real practical uses here how
how do you see yourself disconnecting
from the UI that's wonderful something
about a button that you can take your
finger away from the button in a matter
what to do this laptops not going to do
anything but if you have an itchy nose
or you need to scratch your bottom
you're going to affect the drone at this
point game so is there a practical
application other than this sort of
academic fun to showing it in a
presentation type thing with the overall
body with the overall body yeah we're
really it's it's really difficult to
come up with with real practical
applications for this you can think
about what I thought about a few days
ago was maybe for example if you're in a
production line at BMW or whatever and
you want to stop the line for example
maybe there's not a stop the line pull
switch directly net near to you but you
can maybe wave your hands or something
like this just to do this and of course
it has to be a clear gesture it has to
be distinct from everything you normally
do but there are some areas of
application but yeah it's it's really
difficult to come up with with with real
applications that are easy to use yeah
yeah okay next question
hyah first of all congratulations thing
was a great demo great show I was
wondering uh maybe it's just me that
doesn't know much about this but how do
you really model those gestures I can I
can imagine that most of them are really
hard to recognize so is there any
standard way of doing that is it
interview a you know hard coded and in
the first cameras the gestures were hard
coded so for the leap motion for example
offer the creative gesture camp they are
hard-coded for the kinect version two
days now something called a gesture
builder which can which you can just
record the gestures and it will then use
machine learning to to recognize them
once again and what's the intel
realsense sdk the new one it will be
able to do so but but for my before the
first demo for is dead i just i just
used a 2d a gesture library it was
called dollar one I think no one it was
yeah and this one was used for EM just
pointing just drawing you and just just
with the mouse and then you can
recognize them once again yeah and in
the leap motion for example there were
different gestures already implemented
like Thomas told us it was for example
some kind of circle gestures or swiping
gestures with the hand ever different in
a minute or for the sense 3d I forgot to
show you the camera it's attached to the
laptop here so there are gestures
hard-coded like thumbs up or 1234
whatever what else making a fist making
high-five appears whatever you have you
always have to distinguish between
static gestures
like this one and dynamic chest just
like making a circle brighter and the
static gestures are the easy ones
because it just has to use a certain
post and then it can be recognized for
the dynamic justice you have to do and
record it over sometime and then find
out if it's really a the gesture you
want to you mentioned it the visual
conditions had an effect if this had
been a room full of mirror say or a
foggy day or you're wearing different
colored suits would have affected yeah
it is really difficult to fly these
drones in daylight so and normal
lighting condition is really important
for these cameras it's the same for the
connect as for the same 3d as for relief
motion but and this is really something
difficult because your skin color
changes a little bit in in the camera
image and it it just gets highlights and
and all that stuff this is really
something you have to come up with it's
not perfect at the moment yeah and this
is a problem but yeah but the software
and the technique behind the stuff is
getting better and better when I
remember the very first release of the
Intel SDK with this sense 3d it was how
to same it was a real problem if the
lightning conditions were net not
perfect yeah so but now it's who
after several releases it is much more
stable so you can you can make it better
if the software is really good you know
and the technique behind this gets
better with the predecessor
I was wondering if you have to do some
kind of financialization to say that you
are the source of the gesture the reason
I'm asking is you know what if two
people are there in front of a camera
that's one situation and then other
situation is in the future you could
have multiple objects that can send
subjects which could have different
gestures meaning something else so you
have any thoughts on that yeah I'm
smiling because we have it we already
had this issue so if you are standing if
there's someone standing behind the
connect one flying it may be the case
that he is overtaking the flight and
then he is doing it and and yeah the
kinect has body indexes so you can say
this is a person number one this is
person number two this person no matter
that's not perfect but it works pretty
well so if you're not moving out of the
picture and in again normally you'll be
the same person as before and with the
point essentially there's also an
identification that's possible so it can
identify based on your help I based on
your face it can identify you and
worthless you can then easily
distinguish between different people
okay so some other questions okay so
thank you for attending this tech talk
and we all get much fun with it yeah
thank you bye</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>