<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Streaming Solutions for Real Time Problems | Coder Coacher - Coaching Coders</title><meta content="Streaming Solutions for Real Time Problems - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Streaming Solutions for Real Time Problems</b></h2><h5 class="post__date">2017-10-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/sjT8IkWW5N8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right we're up for everyone thank
you for coming I know there are a lot of
options which you have in terms of
sessions tracks and a lot of cool stuff
going on out there so thank you for
choosing this
my name is Abhishek Abhishek Gupta I
have a product manager that Oracle
focusing on a product called Oracle
application container cloud to be
specific right now of course I'm not
going to talk about that but of course
something which I have highlighted out
there is my Twitter handle everyone can
help me but more followers I guess just
this retweet and spread the word right
I'm at 580 right now for my Twitter
account but yeah this helped me out
safe harbor statement of course the
mandatory one give you a couple of
seconds to go through this don't trust
me all right so a quick question to
everyone who here is familiar with
Apache Kafka Wow so have you been I mean
who has been doing hands-on you know
alright perfect
Kafka streams all right all right a
bunch of them okay that is okay stream
processing in general Wow nice that's
about 50% okay okay and who's first job
one is this jawan or Oracle code any any
any speakers here as well first time
speakers first time speakers oh wow I
guess I'm the only one so now you know
this is the first time for me be kind be
gentle I hope I make this worth your
time so alright yeah so before we even
dive into the nitty-gritty right so what
am I really trying to what is my goal
let me be very honest it's not to make
you a stream processing expert here
right so I'm not you know you're not
going to get out go to this session
being a being a you know ninja in stream
processing or say cough cough cough
cough streams or Redis but I will
definitely give you an overview with the
help of a practical example a use case a
demo which I have built you know which
will give you a high level you know
idea of a technical stat which you can
possibly leverage to build stream
processing applications right to handle
real-time fast data right so that's
that's it your idea here and of course
based on that goal I'll be doing a few
ones on Kafka Kafka streams Redis and a
bit of stream processing as well just to
just to get your get you going and of
course as I said I have a use case I
have a sample application for you and of
course they'll be Q&amp;amp;A time if you have a
burning question of course feel free to
raise your hand and I'll be happy to
take questions but I think if we do that
at the end it will be helpful but of
course you're free to pitch in any time
the content is here the slides are there
the the github repo I have the repo
there with all the code
questions so yeah you don't need to
worry about that okay so let me start
with the example when we talk about real
time so it was I think I think about 15
days back before I was here it was late
night I was III wanna order food so I
generally don't use food delivery apps
but that day I did and so okay I place
my order 20 minutes later I get a call
saying hey you ordered for chicken tikka
masala I think I think you don't you
know that you have an item it's not
available I'm like okay go for something
else fine that's okay that settled so
when the food was delivered to me I
asked the delivery guy hey I mean is
this a frequent issue with this app or
something you're like yes there are
customers who often face this issue
right so I went over to the app just to
check again right and I see that the
same item which was of course available
when I ordered it was it said
unavailable clearly right but then of
course I didn't see that so what I'm
trying to get at here is again I of
course don't know their back-end systems
but this is a very typical example of
not being able to handle potentially not
being able to handle real-time data
right so they possibly you know they
their app whatever back and it connects
to it connects to other other stuff
right which updates you know their their
their food status their inventory
- except in real time so that was a real
case of not know things not getting
updated there right so that's real time
for you a real example which I kind of
hit a couple of weeks back but then
there are other real time examples which
which you are all aware of I guess it's
it's Internet Internet of Things sensor
data tweets stock trades financial
transactions anything right all this
data has has good of comics right you
can you can imagine that it's going on
at a very fast rate right so what I'm
trying to get at here is is real time
data is really fast not really big data
right I'm talking about velocity and
volume here and and not necessarily say
petabytes or or say large amount of data
are lying static somewhere right that's
the idea so traditionally how these
systems handle or how are these systems
built and it real-time data how is it
handled there is one way
batch right bad solutions so you have
your data you have your events coming in
you aggregate them you probably push
them to a data warehouse and then you
have your bad jobs picking up data from
there right but what is the drawback
here obviously I mean you don't want to
see a tweet
ten minutes later right I mean who knows
that batch might be you know a
particular tweet or a bunch of tweets
might not be in that bad so I mean a few
seconds is okay but come on right so it
is essentially a static view of your
insights of your data whatever that is
in your case and then another common
approach is the typical messaging based
solution right there you have a broker a
message broker a hub and a bunch of
consumers say say they are they're
hooked up to topics they are hooked up
accuse right and then they push to
database and their applications again
which who are you know these
applications tap into this database but
what is the problem here typically and
this might be slightly different for
different brokers but typically message
brokers are designed for it you know
working in memory right so what
you say here is if there is a deluge of
if so there is a difference between the
rate of production and the rate of
consumption these messages would have to
be persisted persisted to disk and that
is where potentially there might be
problems is what I'm saying right and
let's not forget that queues and topics
generally are a consume and delete model
right so once your consumer has taken up
that message it is deleted from the
broker whether it was in memory or on
disk right so that's one drawback right
there and I'll tell you and I'll go into
the details as to why and on the
consumer side your cue or topic related
business logic right it's it's not
tailor-made for handling stream
processing so when I say streams there
are certain concepts associated with
streams itself right stream processing
so there is a concept of time there is a
concept of ordering of events etcetera
etcetera right and there is a concept of
doing distributed joins aggregations etc
all over your data think about multiple
consumer instances right so that is a
difficult problem to solve with
traditional method systems and with
traditional consumers and you know
things booking up to your topics and
queues of course this is a talk about
stream processing so stream processing
is a potential solution as I was saying
to handling real-time data something
which is unbounded in finite keeps on
flowing in think think about those
examples which I was talking about right
financial data as tweets anything stocks
write anything anything which is coming
at you at a high high velocity right and
so those are streams and stream
processing of course is something which
can help you crunch this data right
derive insights out of it and they might
be simple they might be complex it
doesn't really matter and I'm going to
show that in example of course it will
be a simple one right and generally
speaking by and large stream processing
systems are somewhere in between sim
pack systems and your typical response
and request type systems right for
example your REST API etc so they are
they're kind of they kind of sit in
between those two systems
this is the example this is the use case
which I have built in order to kind of
make this easier for you to understand
so we have a hypothetical data center
monitoring application which has three
components we have a simulated producer
which kind of simulates metrics from
different machines of course you can
hook it up to your real data centers and
everything of course we have a simulated
producer which produces these metrics
and then we have a stream processing
system which crunches write which
processes this data what is the
processing here in this case is just a
simple case of calculating cumulative
moving averages of all those you know
streams of metrics which are coming in
so one two three four five it'll-it'll
you know these are these numbers are
flowing in of course three is the
average at a particular point of time so
that is what I mean when I say crunching
statistics and of course we have a
monitoring dashboard application which
is going to give you a real-time view of
what's really going on right a graphical
view so to speak so so that's about a
quick summary of the use case and this
is the technical stack I was talking
about of course this is a high-level
overview so as I said the the matrix
producer the mock producer which we have
it's a Java C application and hosted on
the cloud and we have Kafka pacci Kafka
which in this case is the event store
moving on to the processing engine using
Kafka streams relased which is our state
store may have used a couple of data
structures just to just to keep things
simple and of course as I said there is
a JIT dashboard application which has a
server side component and a UI component
right and I'm going to deep dive deep
you know dive deeper into this of course
during the course of my talk so let's
watch it cough cough oh so historically
speaking if I know
so most of you raise their hands right
you are aware of cough that right
probably you're experts so it was
originally built at LinkedIn right and I
finally moved to an Apache you know top
level project in somewhere in 2012 in
terms of a very very high level overview
right a 50,000 foot overview this is
what kappa looks like you know it is
just like any
other messaging system right so you
could you might as well call that right
but it is not only a method thing system
and I'm going to come to that topic
right so just let let's just hold that
thought
but at a high level there is a cluster
they are producers they are consumers
and Kafka is right there in between ok
the server side of Kafka if you're
interested rate is written in scala and
the clients which are the producers and
the consumers their polyglot in the
sense that you have multiple clients
supported for for Kafka so you know
starting from Java ago know jazz.net
etcetera sector you name it right so you
have ample options there in the next few
slides I'll just go over a few I'll go
quickly I'll try to because again you
guys are familiar with Kafka but I'll go
over some of the basic tenets of Kafka
right some of the foundational blocks so
to speak so everything in Kafka starts
with the topic right so topic is just
just a bucket think of it as as a
logical named container
where all your key value stores going so
so within Kafka the messages or the
events or whatever data you send it's in
the form of a key value pair right over
the wire it's just fights Kafka doesn't
care the protocol but within your client
application of course you have to handle
that serialization and deserialization
right so that's what it is a topic
producer push two topics consumer tells
you from topics and there is a special
kind of a topic called a compacted topic
because say you're sending messages
right K 1 P 1 K 2 V 2 K 1 V V 3 again
for example right so of compact ectopic
what would it do is it's kind of more
efficient so it is only store the latest
here an you pay for that key right but
generally speaking topics store
everything right there the records are
kind of immutable so it's like a
complete snapshot which you have so
that's the notion of a compacted topic
right it's basically a trimmed-down
version so that's a topics are as simple
as that
but because topics are so simple as a
concept and Kafka as you might already
know it's famous for its performance
reliability and and distributed
capabilities partitions are the ones
which actually make it possible so
anyway anyone who has a database bag
general distributed systems backgrounds
a partition is nothing but a shard
it's a way of sharing data within a
topic right so this is what a partition
typically looks like so these numbers
which you see 0 1 2 3 etcetera
these are nothing but offsets so when a
producer or an application writes data
to a partition within a topic it goes to
a specific offset right and these are
immutable you only write you never you
know go back and change stuff right you
can only write to the end of a
particular partition right it's it's a
commit log I know if you've heard of
that term it's a commit log immutable
you can't like go back and change stuff
and on disk write soak up the stores
everything on disk everything but still
I mean of course they are there's some
magic going on to make it
high-performance still even though it
mandatory writes to disk on the disk
these partitions are simply folders
right so in this case you have three
partitions here and on the disk they
would they would translate into these
you know CPU matrix topics 0 1 2 0 1 2
being the being the partitions
replication yes so partitioning and
replication actually go hand-in-hand so
if you look closely here this is just a
single node Kafka broker it has one
topic and it and that of topic one and
that topic has four partitions right the
one in green this is a very simple case
and this particular node is the leader
for all these partitions in the topic
right very simple scenario okay one node
one topic four partitions now we scale
out right now here is where the
partition and replication they they work
hand in hand now if you notice the
partitions have been moved so now p1 and
p3 are node one and they're replicas I
don't know two as well and just notice
the replication factor highlighted over
there it's two right so you get two
copies for every partition and all the
data bitches in that partition right so
data is pushed with a partition and
there is a notion of primary as I said
and of course there'll be a notion of a
replica or a follower so leader
follower pattern here right so what
happens is that any reads or writes to
the to the capper cluster
go to the leader right and if you write
something it goes to the leader and
there's an asynchronous replication
process which happens so these are
pushed to the followers and we're
orchestrating everything and then at the
back end to keep everything sane is zoo
keeper zookeeper acts as this
distributed systems colonel so to speak
right it does so for many systems
including Hadoop hive etc right so so
that's the deal here yeah that's that's
how partitioning and and application
kind of work together hand-in-hand
producers and consumers right so if you
look here it's a very simple concept so
producer just pushes data using a client
library of your choice you just push
dinner to a cup of gesture but you have
a few options here in terms of where
your data goes so if you don't specify a
partition within your code it would so
Kafka what would it would do is it would
generate it a consistent hash of your
key say key one and then automatically
map mod it with the number of partitions
and the result which you get would be
the partition number to which your data
will go right so if so you upload this
job to Kafka secondly what you can do is
a slightly more complex because you have
to handle stuff inside you can also
specify the partition explicitly so hey
this is the partition one two three ten
whatever it is and your data would go
there right there is another option
I said it's a key value pair right data
the data model within Kafka topics it's
a simple key value pair your keys can
actually be null and in that case what
happens is that tough gorgeous round
robins load balances between all your
partitions you can have one 300 doesn't
matter but there's the load balancing
algorithm right I mean round robin
actually it's as simple as that and you
can plug in your custom partitioner and
depending upon your business logic right
so so you can plug that and within your
configuration you can mention that so
that's how producers work very simple
concept and where your data goes depends
upon how you want it to right that's
what I Ella straight it to stop
coming to consumer chat these are
personally consumer is my favorite part
within Kafka more than the cluster
itself why because this has you know
consumers have this special attribute of
being both a pub subsystem a topic based
solution as well as a Cubase solution
I'll tell you how so for a pub sub
system you have topics there are systems
which send data to this topic and
whoever has hooked on to this topic they
might be one or multiple subscribers
they all get this data right that's how
pub sub systems work this is the same
with a consumer so if you notice here
you have the cluster on top you have
consumer group a and consumer group B
just think of them as different
applications right to separate
applications which want to tap into the
data within your copper cluster right so
both consumer group a application a and
application B would get all the data
which is sent to the copper cluster as
simple as that pub sub model whereas the
Q model come into play within a queue
based model if there are multiple
listeners hooked onto your queue only
one would get the data right it's it's a
we have load balancing things if you
notice within each of these applications
or consumer groups there are multiple
instances you can spawn multiple
instances and here is the beauty right
that is how you really scale out your
consumption so what would happen is that
these multiple instances would get data
only from a unique set of partitions
within your cluster so you see p 0 P 3 P
1 P 2 these are partitions which I just
spoke about and C 1 and C 2 because
there are just two instances they would
receive from say p 0 and P 3 and C 2
will get it from another set of
partitions unique set they will never
overlap and in the case of consumer
group B since you have four partitions
there is a one-to-one mapping between
I'm sorry there are four instances and
four partitions there is a one-to-one
mapping as simple as that if you want to
say introduce a fifth consumer or a
fifth instance it would stay idle but if
something goes down a four
but if your other instance cashes it can
always you know take up that job of
processing your your your incoming data
within the cluster right so that's
that's one of the reasons why consumers
are one of my favorite things in Kafka
right it's right in between yeah again
some part of the implementation aspects
of course a viewable cloud so so for the
manage to watch across the service we
have something called event hub cloud
that's what I've used here is a very of
course I am using very basic topology
one node cluster zookeeper cup
everything is co-located we have a rest
proxy as well I'm not using that here
there is a simple UI using which you can
create your topics as well right so it
so it's all UI a bit you can of course
go to the CLI but of course and this is
the simulated reducer application which
I was talking about and of course this
is Oracle application container cloud
something which I am responsible for
yeah
so that's hosted there this is a simple
Java C application that's all it is so I
told you to hold your thought when I was
in the first you know slide where I was
talking about tougher so what is it
really
right yes and it's very good it's it's
it's a method doing system sorry simply
speaking it's a messaging system and its
core it's a distributed commit clock
like it's it's it's a rather complicated
but very efficient distributed system I
am NOT going to pretend like a computer
scientist and talk to you about logs or
something I would highly recommend this
go book by Jake reps here is one of the
initial authors of Kafka committers so
go check this book out but think about
other of thoughts of Kafka so to speak
right so it's like a reactive database
if you think right what you push into
kappa key values if you if you write a
consumer or a consumer it just sits
there right and listens to these up to
these values and CAFTA right so that way
it's a change in slight change in mental
model right so you can think of cough as
that reactive yet sharded and
distributed database KB store not did I
because I'm sorry database now you can
think of it is as a database as well but
not a traditional one where you can file
queries so there is a reset des of work
going on in ksq L which is
streaming sequel injured for Kafka it's
like a non developer thing right I mean
by non developer I mean you don't have
to write say a client code you can just
hook up using a CLI and you can do
sequel queries on your on your streaming
data right so that's the idea this is
something new check this out and Kafka
can also act as the central nervous
system within your data pipeline within
your ETL you know side of the house as
well there is another component in the
Kafka ecosystem which is Kafka correct
and then there is Kafka streams which
enables the streaming platform which
Kafka is very famous for right so you
have to stay tuned because that's what
coming up right I mentioned that
explicitly stay awake stay tuned that's
the processing part okay Kafka streams
in a signal treatment it is just a Java
library that's all it is
Kafka streams is a very simple Java
library which you can plug in to your
application for for writing for tapping
into all those data streams within Kafka
it is side to Kafka it's not a generic
stream processing library it is it is
slightly you know coupled to Casta what
are the use cases as I said you can do
it for a big data fine but but typically
stream processing applications are for
those know fast real-time velocity paste
data right so that is where you can use
them it is built on top of top I told
you and whatever it is highlighted over
here so you see scalability fault
tolerance etcetera etcetera these are
the things which I'm going to cover in
my next slide so it's so so I purposely
highlighted them one difference I want
to bring out between Kafka screams and
say other so who knows who's worked on
Sparky or maybe Lisa's aware yeah okay
Apache spark right so you need to so it
has a notion of clusters right you have
to set up that infrastructure whether it
is a standalone cluster or a yarn based
cluster whatever it is with Kafka
streams that's not the case as I said
it's an embeddable Java library just
like just like any other dependency put
it in your system doesn't really matter
where you run your application
misil's kubernetes I don't know wherever
on your laptop docker doesn't really
matter right so so that that's
the real beauty I have happen spar I
mean I'm not a fan I'm a fan of copter
Steve's obviously that's why I'm talking
here yeah I think that's that that's one
of the major differentiators because now
your stream processing applications can
actually be microservices right so so
yeah that's a sturdy and programming
style is an API is and everything else
I'm going to cover now so yeah let me go
there Haven api's so there are two kinds
of API is which kafka streams has one is
a very high level fluent dsl api and i'm
going to talk about the other one which
is a processor based API which is kind
of low level and instead of makes you go
into the details so what am I doing here
very simple bill data stream if you see
the second line I'm just tapping into an
existing Kafka topic which is server
logs topic in this case just filtering
for the logs or the line which has the
word exception and I'm pushing it to
another Kafka topic but to write the two
method it is push it to this topic
called exceptional topic right so this
is my topology I'm going to talk about
topologies in my next slide so this is
the topology using a fluent DSL Kafka
streams API the low level processor API
now I know this kind of looks daunting
scary I mean three lines of code here I
know 10 15 lines of code here but there
are places where you should be using the
processor API as well so it will give
you flexibility so if if you can imagine
if you if you just imagine this right on
the left side the fluent API if you
start hacking with the with the API you
will see that it is not that flexible it
is very simple it is powerful but the
processor API so for example think of
this you have you say you're coming from
background of some other stream
processing of applications which you
wrote right and you have to port demo to
copper and copper streams so this is
where the processor API might come
helpful right and there is there's this
synergy between them if you see they are
kind of doing the same things and I'm
trying to pick out the similarities here
right from an API perspective so this is
where I am explicitly building out their
topology right so when I say build it
out stream this topic the same thing is
done
we don't add source so I'm adding a
source I'm saying hey this scoffs our
topic is my source and then I'm adding a
processor so if you see that processor
what it does is it executes that logic
of filtration that predicate on the on
the Left of course it's a simple
predicate function it's a lambda
function there Java at lambda over here
it's more explicit right there's a if
law contains this right so that's a more
explicit way of doing this more verbose
but but still way of doing things right
and when you push to that that sink
Kafka topic if this is out how it is
right so I add I I do explicit and sink
there right and this is actually taken
care of by this this dot context this is
the processor context object there and
then you say forward it's going to push
to that topic right so this is how there
is the synergy they are trying to do the
same things but of course they are
giving you options right so that's about
API is I mentioned the word topology the
same example so the code which you write
be it the DSL API I feed the processor
API oh by the way even if you write DSL
API underneath the covers it's it's a
it's actually converted into processor
based API is and executed on the runtime
right I mean at the backend by kafka
streams or n-type okay coming back to
topology right so what does this
translate into so two things which you
which you should remember
nodes and processors and edges right
like in a graph so the circles which you
see there nodes and the lines in the
graph are really the the data are the
streams which are flowing in and there
are two special types of nodes here are
processors one is the source processor
which you just saw right the one which
accepts data from Kafka that is where
everything begins within any copper
streams based application and one is the
sink processor and this is optional you
it's not mandatory for you to put you
right into another tough core topic but
whatever you do the last you know part
of your topology would be implemented by
a single processor right so that'sthat's
the final part and if you do this is
actually as assess out if you actually
do if you have darker streams object if
you see here tougher streams if you do
this out
on that it it'll actually helpfully spit
out this this this this log right this
chunk of off string where you can
actually visualize your topology right
so in this case if you see it's the same
thing it is talking about the stream the
stream source it's the same topics of a
logic server logs topic is highlighted
and so on and so forth right so this is
just a very developer friendly way of
visualizing your topology or within your
code itself mainly for debugging of
course you won't like spit out these
things in production but yeah it's there
if you need it how about scalability
right so I mentioned I will be talking
about scalability fault tolerance
etcetera so let's get started on the
scalability part so you have a topic
again simple example for partitions one
single topic one single Kafka gesture
you have a stream processing application
instance so here's your code right you
have already written your your business
logic using either the DSL API processor
API you have threads within your
application of course whether it is
explicit explicit or not within Java
it's explicit right and there's
something called stream partitions and
stream tasks these are very these are
two important concepts to understand
right so a stream partition maps one to
one with a partition in Kafka right and
stream tasks are the ones which are
executed in threads so a thread can
execute one or more tasks it doesn't
matter right so stream tasks really
consume this data from stream partitions
and do that churning right that
processing right so so again one-to-one
mapping between stream partitions and
your Kafka topic partitions stream tasks
which take up this data from stream
partitions and threads execute them
right they I mean one or more tasks it
doesn't really matter when you scale out
when you move to your second instance of
your application how is it scaled so if
you see I mean this is not like a
physical movement of the task right
because conceptually there is Kafka
there right there right globally so this
new instance what it does it just
becomes another
consumer to these partitions so
internally I think I forgot to mention
this in the beginning in the top streams
101 slide that pretty kafka streams is
that abstraction library because of
which you actually don't have to write
explicit consumer or producer code to
say consume from a Kafka topic and push
to a Kafka topic right so that's all
taken care of all you all you are
exposed to is is the producer and I'm
sorry the DSL and the processor API
right so there there are different
concepts right so I hope that make sense
so when you scale out the the new
instance just access just under the
consumer which starts tapping into the
remaining partitions to you you keep
scaling out right up until four
instances you will have like one to one
mapping and if you scale up further then
the lie idle under some some some
instance crashes right so that's a very
simple and you know overview of how you
would typically scale scale across the
stream application or or even a general
consumer application right there's this
notion of scale out and and and scaling
up so what I showed you just now scale
out so you have one instance you go to
one two three four five you scale out
but there is a notion of scaling up as
well now how do you do that I I spoke
about threads so these tasks are
actually executed by threads within your
application so within the API and I've
highlighted this there is a
configuration this is this is the
default one actually so when you say
streams constant num stream threads
conflict one is the default so the kafka
streams library which will only spawn
one thread and you can you can have five
ten fifteen doesn't really matter you
can customize this right so that's the
concept of scaling up I'm not talking
about memory I'm not talking about
increasing the heap size etc in terms of
purely I'm talking in terms of thread
and number of instances so there are
options here and your maximum
parallelism the number of instances
which you can have is of course
logically the number of Kafka partitions
which you have and the number of threads
for instance so if you have say 50
partitions and you are running one
thread per application you can have 50
instances if you are running five
threads per application you can have a
maximum of ten ten plus five 50 it's as
simple as that simple
on a sidenote right I actually I raised
this you know sort of a JIRA on on the
only only Kafka project because I think
it would be cool if if if someone can
actually specify explicitly what which
thread pool I want to work with right so
Kafka doesn't the caucus teams library
doesn't give me an option right now so
if I were to say hey this is the
executor service or that thread pool
which I want you to tap or you know use
the threads from I think it would be
useful but especially for managed
environments like let's say a Java EE
runtime specifically right so if you're
aware of Joey 70 the concurrency
utilities specification I that was from
that angle so I just want to mention it
I think it would be cool good to have
this ah okay another important notion or
concept in stream processing is the one
off State right so so you can have a
stateless way of doing things right so
when you have your data coming in there
is no correlation probably right with
you know between one event and the next
right so that way your processing logic
can be stateless but what if it has
staked and in my application the
application which I'm going to show you
it's it's it's a cumulative average so
of course I have to have some state that
right so how is it handled within Akaka
streams application right so then so
Kafka streams actually gives you this
notion of a very a plate embedded
database within your application
instance itself so you can make use of
it right it can be in memory so there
are multiple implementations of this one
is the in-memory one and the other one
is is a rocks DB persistent rocks DB
database rocks TPS by Facebook by the
way they change their licenses and
everything so it was kind of messy I
think it was a couple of weeks or a
month back so anyway that's rocks DB is
user the implementation for the
persistent data store and and and there
is a notion of a custom store as well so
you can write some custom
implementations of interfaces to your
own data stores right so you can do that
as well and there is a concept of
interactive queries
now these databases itself they are
embedded so they are specific to your
own application instances how do you tap
into that data if you if you want to say
tap into your you know whatever data you
have crunched if you want to take a look
at that how do you do that so there is a
there is a feature of interactive
queries and I'm going to talk about that
right so that is how you you kind of
extract data how do you do that coming
up simple set up a couple of application
instances the local state stores
remember these state stores are not
global these are local to your embedded
within your instance now there is a
configuration within these streams
metadata API and I'm going to show you a
code snippet I know it can be hard to
digest so so using a combination of the
application dot server config and the
streams metadata API what you can
actually say that hey here is an
endpoint for my specific to my
application instance in this case I put
this as machine one a DAT machine to
8080 right so you can specify you can
tell your application through the
configuration your Kafka stream the
application and write a custom RPC layer
on top of it now this can be anything my
example is a REST API this can be any
layer right you can use any protocol
whatever it is that you want to and then
an external application or multiple
external applications can tap into this
this this stream of there's this local
data store which you have within your
individual instances now you give it a
global global you know place to go to
write for for any other external
application so this is custom
development but what Kafka streams gif
gives you is is on the right the
application taught server config and
these streams metadata API the custom
RBC layer of course it's custom you
build it onto on top of khakha streams
and this is an example write a code
snippets just just to this should give
you some context in this case it's a
REST API see you see the one which I
have highlighted right on top there
props property is it's a configuration
so I say hey this is the rest endpoint
right this this is this is my host :
port which I'm configuring and then I
start up this this embedded Jersey
container right and then this Dreamz
metadata API can be used to fetch the
metadata for pretty much all the
instances and I can go ahead and I can
go and ask it Hey so for this particular
key in this case it's machine one I'm
assuming it's not in that local state
store so to what it'll do that that that
query endpoint if you give that give
that URL it will go over to another
instance and tap into the state store of
that that a local state store of that
particular toughest reims instance right
so that's the beauty you have some time
work there development work there but
but it's there I mean you can do it if
you want to it so that's that's
interactive queries for you what about
whole torrents of course so couple of
things against stateless applications it
is relatively simple so you have say two
instances one of them goes down you
bring up another and it will keep on
consuming from from the from the Kafka
cluster right but in case of stateful
applications in that case there is local
store within your individual application
instances right so if one of them
crashes that state is lost how is it
taken care of right so if you notice the
transition here what happens that if you
notice on the left on top kakÃ¡ there is
this compacted topic right I I mentioned
about compacted topic if you're
listening in the beginning so we're
we're only the latest value of that key
is stored right so what happens is that
within Kafka streams application it
automatically created some internal
topics now you should not be mucking
around with those topics you don't
create them Kafka streams application
automatically does this it creates these
compacted topics within Kafka so that
any application instance can go pick up
or replay from that particular topic
right we play the stream of events and
release
or the state in case your application
instance crashes this is precisely what
has happened right so if you see app app
instance one it s K 1 V 1 K 2 V V 2 the
same thing is there in that compacted
topic as well and that's precisely why
when this application instance crashes
the second application instances and
this is all automatic of course built
into the cough cough cough collect after
streams library it will pull up or
replay that data from that internal
compacted topic I was talking about and
have that ready within its own local
store right so these highlighted parts
are I've just ruined it on the only
local state source that that's all they
are right and here is a snippet right so
this is actually done by default so that
enable logging right so you can actually
say and hey keep on pushing these local
states to add data to this compacted
cough core topic you can disable it as
well of course then you won't get this
fault tolerance right so that's the idea
and it is compacted because it will save
space it is more efficient right so k1
so there might be like thousands of
entries against the key k1 it will only
store the latest value because that's
what is required for it to you know
switch over to say when there is a then
your instance caches right you are only
interested in the latest value that's
how it is hope that made sense moving on
the implementation part ok this is a
java application the Kafka streams
library is used here the Java the Java
client of course the Java based Kafka
streams library and in I because I have
spoken about scale out how would you be
able to do that on application container
cloud for example right so the topic has
5 partitions the CPU metrics topic which
I just showed you so you can have either
one instance of your Kafka streams
application or the matrix processor
application in this case or you can have
a maximum of five right which can tap
into each of these partitions from the
CPU metrics topic and and scaling up and
scaling out is pretty easy that's what
I've tried to highlight there of course
alright my product I should talk about
it at least a little bit I hope you
don't mind so that's how it is right
it's
simple as scaling up and scaling down
elastically let's switch over to Redis
and I'm not sure how am i doing on time
ah sorry anyway anyway he'll Albert I'll
friend go little faster here there so
radius in this case is the state store
right and let me just give you a quick
overview of today's rate so you might be
ready says very very famous I mean very
very popular with developers right it
stands for remote dictionary service
Redis right it is a data structure
server right it is not it is it is a key
value store a traditional key value
store but the keys are simple straying
binary cap to a limit of hundred twelve
MB but the special thing about redness
is that values can be anything they can
be data structures from simple string
lists sets sorted sets etc check time
and the more fancier ones the hyper log
log geospatial go check that out right
so that's that's really the USP of ready
it's a data structure server
it's a Cavey store and it's in memory
with durable persistence you can you can
configure it to push to discuss well
right so those two are the key USPS or
selling points of Redis the data
structures are just highlighted to
because these are the ones which I have
used it in in the example sorted sets
I really have sets on steroids right
traditional set so you you have this you
have you you have a criteria for for
sorting right that that basis for
sorting within in this case it's it's
the it's the CPU utilization in the
example and I'm going to show that right
so that is sorted set for you there are
a bunch of operations you can of course
go check them out I am NOT going to dive
into them and lists internally it's it's
a linked list right so so the operations
at the head and tail are constant of one
and and yeah these are the couple of
data structures which I thought I would
highlight I have also I mean indeed the
example uses Ultraman l range Elrond is
basically you know you you trim the I am
you take a subset of the list etc trim
is you can if you want to gap your list
to a specific size I have done that and
I'll show you a real quick
wrapping up with Redis so I couldn't
cover a lot what about R it is because
again and this is a lot in there but
some of the good stuff which you should
definitely check out is Sentinel
Sentinel is is this in itself is a
complex distributed system for
monitoring your radius clusters you have
a synchronous master say the application
you have a concept of sharding which is
catered to by Redis cluster you have lua
scripting transactions pipelining
everything pub/sub yeah you have a lot
of things built in right and the typical
it's like a Swiss Army knife so to speak
in this case in our example in the
example in the sample application it
actually is used for a leaderboard sort
of a use case but again it can act as a
cache cache is very popular job queue
messaging pub/sub right I mentioned that
yeah so it's very versatile in that
sense and of course it has a lot of
client libraries just like this like
Kafka has I assume this this I had put
in the slide because I assume will have
a lot of questions as to because I spoke
about Kafka streams the embedded state
store and then on the other hand I'm
moving to Redis as the state store why
is that so it's actually about horses
for courses
it's about options in this case I am
reducing Redis because I want to model
that live leaderboard I'm going to
leverage that sorted set capability on
the server side which Redis gives me of
course it's a simple one but that's the
idea right but you can combine them you
know the use the embedded state stores
use your custom state store use use
anything then is etc right and by the
way so Oracle application container
cloud the product product it has an
inbuilt cache as well so I I haven't
block up there if you want to check it
out as to how your stream processing
applications if deployed an Oracle
application container cloud can use that
distributed cache which it gives in
order to store your state right
of course this is specific to the
product but if you're interested go
check it out you should like it where is
ready set up in this case in the in the
Oracle cloud ecosystem so it's very
simple if you see this there is there is
a notion of the Oracle cloud marketplace
you can pull out a bit Namie image that
is what I have done
and and that's pretty much it that's
what is running if you see at the bottom
right and yeah it is just to be n you
can log into it
check out the start stop here it is in
students etc right so just to be M on
the club Oracle cloud in this case right
and there's nothing different from from
say ec2 yeah it's just that it is much
more simpler because you pull it off
from bitNami right from the Oracle cloud
marketplace so it's just there for you
the last part of our monitoring
application the dashboard right so as I
said it's a combination of a Java EE
application and and and and uh and UI
component which is which is built with
Oracle jet these are the specifications
of the Java if you're if you're
interested I I just put them up here and
of course it is an Oracle application
container cloud right so this is a
high-level overview of the streaming
based solution again mapping one-to-one
with the individual cloud services I
thought I'll explicitly before I dive
into the demo the demo is coming up
right so a particular container cloud
application container cloud with the
simulated producer pushes data to Kafka
which is Oracle event up cloud in this
case Kafka streams application Java C on
a protein or cloud as the data pushes
them to Redis on Oracle cloud
infrastructure just to BM again
and finally the dashboard application
which is a Java EE application which
gives you that entire view time for the
demo all right let's see so I can
actually so let me open up this
dashboard application
well let me check it in stop that or
what am I looking at something else
I might be looking at something else
hold on yeah so this is the yeah I was
gonna hear something else so this is the
application contained in the cloud UI
and a bunch of bunch of applications
here this far is the dashboard
application waiting for this to come up
so if you notice so the so on your on
your extreme left over there is a live
leaderboard of all the machines as per
their CPU usage right of course this is
this is simulated data and on your right
is the data for a specific machine right
and now I moved on to machine seven now
if you see this is static right now and
there is a reason it is because my
simulated producer is I don't think it's
running let me do that so this is my
producer application I have an ax
standpoint they're just just just to
make sure just to kick-start that that
you know okay
Kafka producer is up and running
hopefully we should ah immediate action
so you saw that changing so this is just
so so what is happening right now this
data is being pulled in real time from
Redis okay so there is a server side
component which is polling Redis and
exposing data for the leaderboard as
well as those individual machines or SSE
Channel server sent events okay I'll
show you this data within Redis right
now let me so this is ready so close now
yes this is my instance I'll refresh it
just to make sure right so this is the
Redis instance I was talking about on a
VM so okay so this leaderboard which you
see right now this is this data
structure this is a solid set the CPU
leaderboard this audit set which I
mentioned this is that one right and
machine one up until 10 these are the
individual machine values right
so yeah value these are the CPU metrics
which are being assimilated or produced
so that's how it is if you fish on to
other other machine
the the front-end application is going
to hook up to that server sent events
SSE channel and that's how you should
see that in a second yeah so this is
continuously changing right because I
have my producer application on if I
stop it
it will keep its mouth shut as well
right yeah so that was it for the demo
relatively short yeah yeah I think some
resources appeared for you if you if if
I was able to spark any interest in
checking out application container cloud
you can do that actually go down right
to the bottom first sign up for Oracle
cloud trials and then you can go to a
container cloud there are some sessions
I'm going to just go to the next screen
as well check us out on Oracle depths we
have a blog up there on medium and
follow us on Twitter the Oracle devs
community yeah and if you're interested
you can check out these sessions these
are from our team a bunch of stuff on
cloud native applications infrastructure
s-code hopefully you should like it and
one on blockchain that should be
interesting enough hey I think that's
pretty much it from my side if you have
any questions I am more than having to
take them up thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>