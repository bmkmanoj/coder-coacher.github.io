<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Simplified and Fast Fraud Detection | Coder Coacher - Coaching Coders</title><meta content="Simplified and Fast Fraud Detection - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Simplified and Fast Fraud Detection</b></h2><h5 class="post__date">2017-10-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/M3zaeB71RRA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so good morning everyone nice to see so
many people here so early in the day if
you have a laptop with you you'll be
able to follow along I've got a hold of
this scripted demo up on a website it's
publicly available you'll be able to
follow me along as I do the coding there
is a lot of code in here so hopefully I
checked at the back already
it is gonna be slightly difficult to see
so you might try and come forward a bit
I have posted the slides on the content
catalog but for some reason I when I go
to the content catalog I can't see it
but I have uploaded them so hopefully
during this session they will magically
appear and you you'll be able to
download them okay hopefully fingers
crossed you're all in the right room
this morning I'm going to look at how to
use sequel pattern matching to do fraud
detection Who am I I'm Keith Laker I am
part of the product management team
Oracle I look after a lot of the
analytics sequel features and sequel
pattern matching as well now fortunately
I have two people who are real experts
in this stuff sitting at the front here
I have Leigh from our development team
who's responsible for just about all the
coding that goes into this that runs in
the backend and andy wachowski here
whose hair chief architect so if you've
got any questions just stick your hand
up I'm happy to take interruptions
during the session and I have two people
experts here who can help me answer on
questions that you
we have this is sort of part of a huge
set of data warehousing sessions that we
have we have an online guide that you
can pull down now see there if you've
got an Apple device
it's an iBook it's got a lot of
interactive iBook pop-ups and things
they'll help you if you're stuck on an
android older technology you can use PDF
guides unfortunately you lose all the
great features that are part of an iBook
but you still get all the content at the
end of this conference I will publish
another version of this with links to
all of the presentations that were
allowed to publish so you can pick up
this one guide if you do data
warehousing is part of your job role or
big data and this this will be a useful
resource for you after the conference as
well as during ok that's me that's why
right blog that's where I tweet you have
any questions and we'd love to hear you
from you if you're using pattern
matching maybe if you're using pattern
matching without Oracle technology and
you would like to move it over to our
technology there are features that you'd
like to see us put into the then please
let me know feel free to contact me
as I said if you've got a laptop or a
very big tablet I think this will run on
the tablet and you've got an internet
connection you will need an ATM account
hopefully you have one of those lots
equal da oracle.com that's where we can
now run the demo from you don't need to
type anything a live sequel site will
cut and paste off Dover for you and then
you just hit run and you can see the
code running you can see the output so
you don't need any sequel skills if
sequel is new to you
it's just cut and paste and click we go
so for those inside in the count how
many of you are familiar with sequel
good ok that's good how many of you had
developers DBAs how many of you are on
12c
okay looks good sir this feature that we
cannot look at so they came out in 12
see if you haven't got access to 12c or
you want to look at any of the other 12c
features that we've had you go to life
sequel to Oracle comm you'll see as I
show you later you'll get access to a
12c release 2 environment where you can
try out pretty much anything and so if
you want to just test out features new
functions and you don't have access to a
system in your data center or in your
development center you can use their
free service and you can play to your
heart's content
what you'll get is on this site live
sequel to Oracle comm quits sequel
scratchpad
if you like it's a free service and we
don't charge you anything it's not often
we give you something free of charge but
this one is free it is running in our
cloud it is Google searchable so if you
want to content that we published up
there will come up in your google search
and you can save your scripts as you
start to write your own scripts to test
things you can save them you can share
them with other people you can build
your own tutorial very very useful
feature to have and going forward you'll
see in the documentation where you had
examples sitting in the documentation
will then we're looking to integrate
into this service so you'll be able to
read about feature leap over till I see
one actually test it and see it running
there's lots and lots of stuff up there
if anybody hears PL sequel developer
there's there's lots and lots of PL
sequel examples up there there's lots
and lots of DBA scripts and things like
that or to use and it's all free of
charge so what I'm going to do today for
those of you that have got laptops or
big tablets you can click along with me
I'll show you how to get to the demo in
a minute but I'm sort of going to cover
five areas I'm gonna go into the essence
of sequin pattern matching and then look
at the specific syntax of the maps
recognized clause which runs the pattern
matching inside the Oracle database then
we're going to do three demos
one is just searching for some money
laundering issues so we've got a small
data set for you some basic transactions
and we're going to find what we call
interesting transactions in that data
set we're going to use some JSON
features that we added in 12-2 to pipe
the transactions in and we're going to
interrogate those transactions in JSON
and convert them to sequel so we can
then analyze them and then we're going
to take that data set and come back as
normally happens I'm sure in your
projects you get this you deliver your
project to your business users and
within a couple of days they come back
and say yeah that was great but we've
got new requirements for you now can you
change everything and look for this so
we're going to show you quite how simple
it is to extend an application in the
JSON space without impacting your
current demo your current working
environment and we're going to add some
new business requirement so we're going
to change what we're looking for but
we're not going to break the original
demo the original demo is still going to
run one of the big advantages that we
see for using sequel is that you get
access to a very very broad network of
analytical features so what we're going
to do here in the last demo is we're
going to combine the sort of fraud
detection that we've done with max
recognized and enhance it a little by
adding spatial analytics in there so we
bring together a couple of different
types of analytics that you can combine
they're very easy to do what I call
sequel mashups take different types of
features bring them all together get a
good query out at the end of it and then
we'll wrap up hopefully I can do that
all within about an hour
okay any questions so far
everybody good okay so let's let's start
who is doing pattern matching today not
necessarily with sequel but with with
any tool any form of pattern matching
okay sorry
yeah that'll do it works there there are
an awful lot of ways to do pattern
matching today there are a lot of
languages out there that allow you to
search for patterns with varying levels
of complexity that you can define now
since 12c release one we've had sequel
pattern matching inside the oracle
database so this is part of that and c
2016 standard so you'll see this across
a number of other database platforms not
necessarily just oracle this is an ansi
standard requirement for 2016 what it
allows us to do if you're used to
pattern matching already this will be
fairly familiar we are going to search
through a sequence or a stream of rows
we're going to look for an event or
series events that you define and we're
going to look across row boundaries so
is anybody here used analytic functions
windowing functions okay so you're
familiar with those so you know that
within a boundary you know we can we've
got lag so we can go back we can look
forward with lead we can go up and down
there that boundary what we do with
pattern matching is that we can go
backwards and forwards wherever we want
and we can also go across columns so we
can go backwards forwards we can go
right looking over to the left we can
pretty much pick out any value that we
want to do our analysis and similar to
analytic functions and we can do rolling
totals we can pick up final totals
there's an awful lot of processing that
we can do if you're used to analytic
functions a lot of the keywords that
we're going to use to describe a pattern
to help you build a pattern query I can
all look very similar so as I said this
is part of the ANSI standard so it's not
something that Oracle's put in that's
can unlock you in this is part of it the
ANSI component it is fairly
straightforward to set up as I say if
you're used to analytic functions you'll
be used to partitioning and ordering
your data so the idea is that we need to
make the pattern visible to our sequel
query because we're going to go through
the rows we want to be able to go
backwards and forwards and we need to be
able to discover the pattern we then
define what it is that we're going to
look for so we need to be able to tell
the system this is what you need to find
and we do that using regular expressions
so if you're used to other languages for
pattern matching regular expressions is
the way that we do it and we have
pattern variables that tell us how
initially what are we looking for what
frequency where do we expect to find it
and what are the conditions what does
what does each variable mean in terms of
what we're looking for and then the last
step is okay what information do you
want returned if you actually find the
pattern that you're searching for and we
have a lot of features built in there
that will help you analyze discover how
your pattern is being applied as well as
giving you the capability to give a lot
of useful information back to your
business users okay so let's look at the
syntax four simple steps define your
partition buckets set up the pattern
define the measures one point we haven't
mentioned yet is the output how much
information do you want back do you want
a summary or do you want very detailed
information I'm going to go through all
of these and step by step so the first
part we need to get the data grouped and
ordered so that as we go through it and
we're going to go through it
sequentially even though we can step
backwards we can step forwards we're
going to progress through the rows
sequentially that makes that pattern
visible now these are optional you don't
have to use partition by you don't have
to use order by you can leave those out
be warned you can get a query that runs
without them is it going to be
deterministic it might be it might not
be so it's always useful to think about
how your data is ordered do not rely on
the fact that if you query that table
you know a select star from table it
comes back beautifully ordered
everything is in in the right place for
you to then just do a match recognize
and
and right as little code as possible
it's always useful if nothing else for
the person that's going to come after
you
he's got to try and understand what
you've done partition my order by now we
start to look at how we define what it
is that we're looking for so here I am
going to look for a pattern of four
components X W sorry X Y W and Z I have
plus next to each one which is an
important thing we'll come to that in a
bit
that's the sequence that I want to look
for it in X followed by W for a solid
followed by Y thought about W followed
by Z sorry I am completely jet-lagged
and distraught because there's no coffee
here till 10 o'clock so I am struggling
a bit but I will do my best so what does
the plus mean it means I'm going to look
for one or more matches so for each of
those patterns XY w Z I'm gonna look for
one or more matches okay now I can I
have any one of these and we're going to
use some of these different notations in
the demo then we're going to build there
is and I'll give you a link to something
at the end there is a lot more that we
can do here this is very very powerful
feature I am going to gently bring you
in to how to use the basic elements of
this if you want to read a lot more
information about these notations and
without wanting to scare you all
completely the concept of greedy versus
reluctant quantifiers for those of you
that have done pattern matching that's
probably a familiar term
there's a blog post that I've done it's
all linked in the PowerPoint I've
uploaded to the demo site you can read a
bit more information around greedy vs.
reluctant okay so what exactly does X Y
W and Z mean because we haven't
what I've said is I'm going to search
for one or more of these things but I
haven't actually said one it's going to
be the define part of our syntax gives
us the option to say exactly what each
of those letters means and it's a
boolean representation that you have to
put together so here I'm going to look
at my and see if I can do this oh yeah
so I'm gonna look at my balance my
account balance and compare it to I've
got a new keyword here previous which
you probably won't have seen before it's
kind of similar to lag but it's a bit
more powerful and I'm going to compare
it to the previous balance so I'm just
going to make sure that my current
balance is less than my previous balance
if that situation is true I have matched
X I'm good I can now move on I've got my
1 I can now move on to searching for y
and see if I can find a y next stage
okay I found a match what is it that I
want to know about that match and
there's all sorts of different pieces of
information that I can pull out as I
find a match I can pull out data about
the previous match I can do the next
match I can find the first instance of X
so if I've got don't forget I said I
want an X and I want one on one so I
might get five X's so if I want to get a
timestamp for X which time stamp do I
want do I want the first one doing
last one so I have keywords here
previous/next first and last to help me
determine which pieces of information
I'm going to pull out so you can see
here I've got my keyword which is
measures and I'm gonna pull out the
first time stamp for X and I'll label
that as first X and I'm gonna go from my
last time stamp which is going to come
from matching Z or Z and I'll call that
last C well get into all of this as we
go on as I get to the code we do have
two very very special measures that are
built-in so if you're a developer or a
DBA or even a business user who's going
to write this the key question is how do
I know that this thing's working I might
get an answer back how do I know it's
right how do I know that I've gone
through my data set and I found the
right rows so what you typically you
want to know is how many matches did I
find and which of those four variables I
had X Y W and Z which ones were applied
to each right so I can then go back and
visually check that I found everything
so we have two if you like debugging
tools classifier and match number that
will help you understand how much
recognize sequel pattern matching is
being applied match number or tell you
within each partition how many matches
I've got and within each partition it
resets to 1 so as you go through each
partition you'll have 1 2 3 4 5 6 so
classifier or just tell you for each
write whether it's an X Y the W or Z so
these are very very useful don't just
assume if you run your match recognizer
and it gives you a result that
everything's great
you've got an answer back make sure that
you've actually got the right variables
being applied to the right rows the last
one and I'm gonna go into too much
detail on this because there's a lot of
information here is okay how much data
do I want to return I can ask the
systems to give me one rate per match or
I can ask it to give me all rows per
match and then we have some more
keywords and we'll look at some of the
keywords in the in the live demo the
next part this okay I found a match
where do you want me to start searching
again you may have overlapping patterns
so for example it's the example that
we've got on the blog post over here we
have an overlapping pattern so I need to
be able to finish looking having found X
Y W and Z I find that actually the
pattern that I'm next looking for could
start with where Y finished so I need to
be able to go back over the rows that
I've already processed and then start
moving forward again looking for my next
pattern and we allow you to do that to
go backwards as well as skipping
forwards
okay that's the full syntax some of
these are optional
as I said partition by and order by are
optional you can leave them out
don't call me if you get the wrong
results or if you run it once and then
you run it again then you get a
completely different set of results
don't call me if you stick those in and
get the different results then call me
but so always make sure you use those I
think this should have actually been
mandatory but that's just me one row all
rows per match we'll look at those and
you'll see in the demo how we get on
pattern mandatory obviously and define
their measures is not mandatory and in
some cases you can run a query and get
will run it will process it will find
matches for you if you don't give us a
measures clause you actually won't get
any rows back so your your your pattern
will have been found but we won't tell
you anything about it which is quite
handy again if you don't put measures
clause in it you don't get any answers
back don't call me put a measures
closing okay that's the basic syntax any
questions everybody happy okay we are
going to do a demo or I'm going to do a
demo if you've got your laptop with you
you can do a demo as well if you go into
lycée called oracle comm yes
ah that is a very good question sir
thank you yes those of you that know how
Oracle database licensing works
partitioning is a costed feature which
you need to license this is not linked
to partitioning so you don't need to
have the partitioning option enabled
this is just an internal way for us to
chunk up the data transparently within
the sequel engine it does not rely on
the partition by Clause thank you that
was a that's a very good point okay if
you if you log in to live sequel go to
the home type Java one you should see
that that should be there it should be
live
I hope its life and you can then follow
on you can type with me everybody got
that everybody do that okay
if you want to do it later you still go
to live sequel to Oracle com still type
in Java 1 you can run this anytime you
want if you want to do this back at your
hotel room this evening perfect I
definitely recommend other things to do
but you are you are definitely free to
run this over breakfast tomorrow morning
whatever you want to do it will be there
I will probably enhance this as I go
along
so probably later tonight when I get
back I might add small things based on
questions etc that you you give me but
you can run this any day you want any
time of the day and I'm tempted to say
it's up 24 hours a day but who knows
there may be times when we're gonna get
down to patch it and do maintenance but
it should be up pretty much 24 hours a
day so if you don't if you don't want to
click click click click click what
during this you can you can do this
later ok we want to look for suspicious
money transfers what does that mean we
want to find three or more small money
transfers what's a small money transfer
it's less than 2 K all three of those
transfers must occur within 30 days then
gonna look having found those three I'm
now interested in has there been a large
transfer of more than a million within
10 days of the last small transfer could
you do that in sequel just standard
sequel you've got analytic functions
you've got sequel model clause you could
use windows you've got lang bleeds
you've got a whole series of functions I
guarantee you you can do this using
standard sequel it's going to be a lot
of code it's going to be an awful lot of
code could you do this in Java
absolutely you could definitely do this
in Java you can pretty much do this in
any pattern matching language
the objective my objective here is to
show you how simple it is to do it in
sequel but most importantly how simple
it is to then take changes to those
requirements so I haven't got this the
next stage is somebody's going to come
back to us and say yeah that's great but
we need it changing it's got to do more
and then go on again yeah that's great
but I need it to do even more it's the
simplicity of the way that you can
interact with this that's the important
part ok so what do we want what I want
to pull out is the account the date of
the first small transfer the date of the
large transfer and the amount of the
large transfer okay does that make sense
everybody happy yes that's a very good
question does it matter which direction
the money is going in this instance I as
you'll see let me go to the next slide
so we have at the moment all I'm going
to collect is the user ID so this is the
account owner what they're up to
depositing and what the amount was
if I had and you can see here it's
actually all job so I've only got have
only on one person in reality you've got
more than just job you know i marry bill
Keith so on and so forth that's where
our partition by Clause comes in
so that then allows me to chop it up by
each account and within each account I
can treat each one in isolation and go
through the transactions okay the next
question might also be following on from
what you're saying is does it matter who
that money goes to so what if it's a
transfer for a standing order or what if
it's a transfer to their partner could
we identify that that's going to be the
business change that's going to come
back to us in the second half for this
okay who's the money going to because we
need to understand that as well but for
this part just going to keep it
relatively simple I got one user who's
called John doing a whole series of
transactions your notice here that this
is in JSON format so this is this is not
an Oracle standard rows/columns table
this is going to be in JSON format and I
want it in that format so I need to
transform it into that format so I can
now scan through it and that's going to
be part of the process that we're going
to go through so here is my data you'll
see as we scan through the data that
I've got small three small transfers
first one on the second the last one on
the 20th they're all under 2k so those
are the ones that I'm looking for then
I've got my big one
a million dollars on the 27th and that's
got to be within ten days so we're
actually within seven days so we know we
know we're going to find at least one
match in this little data set so how do
we get the JSON data in how do we
convert it to something that we can use
and then how do we write our pattern
matching routine okay first thing we got
to do we need to create my table I'm
going to call it chase on transactions
has anybody tried using the JSON
features in 12c yeah okay some of you
okay
so what we can do is those of you that
are used to defining our Oracle tables I
guess that's pretty much all of you I am
going to create a if you like a document
it's a club and I'm going to put a check
constraint on it and it's this thing
here valid JSON so as I try and push
data into this it's gonna check that
this is actually valid JSON and the
check is my document trans doc is in
JSON format okay does that make sense so
it's got to be a JSON file that the
system recognizes Jason key value pairs
properly formatted properly structured
if it doesn't it's going to get kicked
out but assuming that you've gone nicely
formatted table a nicely formatted
document we can put it in now most of
your JSON data is going to be in a file
correct it's gonna be a log file
somewhere so you don't want to have to
oops you don't want to have to convert
this into the following insert into by
jason transaction tables values so I'm
actually taken the JSON structure and I
just created an insert statement
ordinarily in your situation outside of
lifecycle you will just create an
external table everybody familiar with
external tables yes okay you can create
an external table over that JSON file
and just reading yes 12:1 yes 12.1 we
put some more features in 12 - to do
more JSON validation and what you get
made it easier to pull out and manage
Jason time value pairs I think we added
the option that you could update as well
in place in 12 - but I'm guessing most
of the time you're just going to create
an external table one of the limitations
of live sequel is that we can't create
an external table we don't have access
to the file system so in this case I've
had to convert it to insert statements
but please don't think that you've got
to do this with your JSON files you
don't you just create an external table
over it and it will just read the data
as needed yes sir
in the head nodes the CPU notes you have
space there is storage space there that
you can put fast on there for us to
access external tables or you just mount
that as an access point from somewhere
else or if you've got dbfs enabled but
dbfs works on Exadata
I think it does you can you can you can
mount a dbfs mount and put the follow
I'm also going forward you will be able
to use something a little more
interesting you can also leave the file
on Hadoop if you're using big data to
click your JSON files and then we have a
connection tool called Big Data sequel
which is part of our cloud offering as
well but you can install this on premise
next to your Big Data appliance and that
will also create and in it simplifies
the table creation but it will read JSON
files then on to exadata directly from
say our Big Data or appliance or your
Hadoop cluster and we'll simplify the
process for setting up that external
table I'll add some more information
about how to actually set up the JSON
structures if it's an external table so
I hadn't thought to put that in there
because we can't do that on my sequel
but I'll do that and then upload the
presentation again yes
that's a good question MongoDB
can I access JSON files through that if
you've got access to our Big Data sequel
which is a cost adoption unless you're
on the cloud I think that now supports
Mongo so you can using big data sequel
make the transparent transformation from
Mongo or our no sequel database or
Cassandra as well I think and make that
visible directly inside the database as
a table M rows okay
so I've loaded I've inserted we'll go
through this me I've inserted all those
JSON transactions I've converted them to
insert statements what does it look like
I can just do a select star from my
table and guess what I get JSON data so
it just looks like each row of my JSON
data okay isn't it still not
particularly useful to us from a
relational point of view but I can get
on what I can do is take my JSON
transaction table setting out and I can
say remember that I set up a document ID
as the club I just reference my club my
key value pair identifier has give it a
column name and going through that that
gives me my time ID which links up to
the JSON key value pair okay laughs in
the top right hand corner let so it's
probably a bit smaller than Mike I
apologize and give it a column alias so
on and so forth
they voila
I've converted a JSON document into rows
and columns I now have a sequel
relational table that I complain with I
can work with I can now do pattern
matching on what you will notice is that
I've got time stamps and I have text
data and I have amount numeric data note
everything we returned from the JSON
environment is varchar' to say if you'll
see as we go forward if you want to do
computations if you want to work with
the time and dates you've got to be
aware that you may need to do some
transformations okay let's quickly see
if I can flick over to this okay I don't
know whether you're going to be able to
see this hopefully you can let's see if
I can okay so this is live sequel we go
in here and we just type better knowing
this one hopefully this works I'm
guessing the network's gonna be quite
slow and I just want cheat orioles there
it is okay so it looks as if
we're system is live which is good so
this is broken down into modules here's
my worksheet you may if you're starting
it may look like that if you're
following along you can you can get rid
of this thing here on the side by just
clicking up there give us a bit more
real estate here's the text just to give
you a introduction to the lab and then
here's all the lab points so these are
all the bits that we've done and if
you're ready we can just whack that in
run I'm hoping the network's not gonna
be too slow table creators we're good
okay so there's my JSON table I'm now
gonna do this so there's my JSON data
we're just gonna run that in and you'll
see one row one row one right so we're
pretty good okay as I said bear in mind
that if you've got JSON files you're
going to want to create an external
table you don't want to go to the pain
of converting this to insert statements
but because I mean life sequel I can't
do external Tainos yes there was a
question over there somewhere
how do we work with multiple searches
going on simultaneously that's an
interesting question okay based on
available memory that we have we will
chunk the data up we're not going to
process every single partition in one go
bearing in mind each partition is
discrete so we can process each
partition in isolation so we can run in
parallel using parallel execution we
have resource manager which can
constrain the amount of resources that
we can pull through and we're allowed to
access we have temp tablespace there are
various other optimizations that you can
do such as pre sorting your data that
will make sure that this runs as
efficiently and as fast as possible does
that make sense so you don't have to
it's not the way just kind of pick up
the whole table and throw it in and
start processing it we will take the
partitions and then work through the
partitions with the resources that are
made available to that query yeah we're
not going to run it as a big block it's
it's similar to any other parallel
execution that we would do is that the
question or are you worried about the
conversion of this
that's correct
it is it is a permanent table
you you're both going to access the same
table or you're both going to access
different tables if it's the same table
then assuming that you're your prosit
you're roughly right if you're running
both the same query then we then enter
into the buffer cache and and data will
be reusable oh I see right I've got you
right right right right right I have yes
sorry I completely misunderstood you so
the question is what happens in live
sequel yes okay so what happens in live
sequel what happens if you're running
this do use do you see what I'm seeing
yes can you access my data no you're
you're in complete isolation I have no
access to your data so you can change
your data you could just go in and you
know make that instead of 17 make it 15
for January 2015 I would not be able to
see your data so I'm in complete
isolation here yeah does that make sense
okay my apologies I wasn't quite
following them
okay so the question is if this is a
large file a large JSON file should you
load it into the database given that
you're going to have to do
transformations on it to convert to
dates numbers etc it depends it depends
on how fast you want it to run so there
is going to be minimal overhead for
doing those transformations from bar
chart to you to which ever numbers that
you want most of the time I would say if
it's an external table then you should
be good to go you're probably not going
to have to worry about putting it into a
staging table and certainly if you're
running this on big data c-calm say
you've got some of the transformations
that we have a lot of the conversion
processing will actually get done on the
Hadoop cluster so if you've got big data
sequel one of the advantages Big Data
seek ways that we can offload a lot of
this transformation movement from one
data type to an Oracle data type we can
push that to the other cluster and we
can we can then do it directly on this
when the data arrives in exadata or
whatever your platform is it's already
in the right Oracle formats but there
isn't a huge amount of overhead here to
do this and I think if you go to one of
the demo booths
in the demo grounds I have the number
here I can give it to you later there is
a specific JSON booth and they'll be
able to give you some guidance on best
practice for doing conversion when to
when to do it performance figures that
they've got for this for doing all this
processing
I got they've got some quite good demos
in there yes
ah very good question can I build
indexes on the JSON column yes you can
and okay hunger to throw away the script
and well we'll get to the end here
you'll see you can also is anybody here
licensed the in-memory option okay in 12
- you can put JSON documents columns or
key values from your JSON document into
the in-memory store as well so if you
really really really do need real-time
well near real-time super fast
processing on specific elements of your
document for pattern matching you can
put it into the in-memory key store in
memory store as a JSON column and then
process it that way as well so we
support JSON in memory yes let me come
back because I can't hear you
can you create an external table in a
different database yes yes yes you can
do that and then either materialize the
results through DB link and connect from
your other system to that however you
want to do it yes so you can definitely
do offload the processing to another
database if you're lucky enough to have
two exudate errs you could always do one
processing on one Exadata and your
pattern matching on the other however
you want to mix and match it okay so we
did this bit so let's have a quick look
and so if I just select from that JSON
table there's my there's my JSON
document and the nice thing is life
sequel I can download it as a CSV as
well at the end of if I want to okay so
how do I get from that to something
that's useful so here I've got my
chasing transaction which I prefixed
suffixed with J that's my details
transaction doc that's my club in my
table that's my key value pair and I can
run that now and voila I've now got a
JSON document transformed into bar chart
to format okay
so to convert it I might want to do for
example to date give it a date mask to
number to bring it on I'm also going to
filter it so I'm not interested in the
deposits I'm only interested in the
transfers I don't care about withdrawals
I just want transfers and I'm going to
order it by user ID and date so now I
just have the transfers that have gone
on okay and you'll notice we have now
got actually a full time stamp although
you can't see it here okay just to keep
things simple I'm going to create a view
let me just do this bit I'll show you
why in a minute so I'm going to create a
view so now I can do all I've got to do
if I want to reference this is do select
star from that okay
so I've now got the information that I
need
right okay question yes correct
okay so the question is in reality when
you folks are doing it it's not going to
be a simple one table you're gonna have
lots of tables so you don't have to join
get customer ID customer reference
product IDs someone and so forth how do
you do that okay you neither create of
you to keep your source code simple that
also decouples your pattern matching
from the data delivery side it also
makes your code a lot simpler you don't
have to I'll show you in a minute when
we run this that you can actually
reference you can put this select
statement within the match recognized
okay so let's do a very very simple
match recognized so what have I got I'm
gonna select my user ID that's my view
name that we just created
down here match recognize pattern so I'm
gonna look for if you remember I use
this the requirement was three small
transfers less than 2k occurring within
30 days and then one large transfer so
I'm looking for three instances of acts
one instance of Y which is what this is
so this says you remember before we had
X plus which was one or more here I'm
saying I need to have at least three
instances of X and one of Y
okay so if I've got two transfers it's
no good I got a half three and must find
three and what is X&amp;amp;Y mean x and y mean
X is as I go through as a test each row
so I'm going to go through check each
row I need to know that the amount is
here we go here is less than 2000
if the amounts less than 2000 I'm good
I'm going to capture it and I like that
right if it's more than 2000 I ignore it
it gets rejected but I have to find at
least three rows consecutively
consecutively that have a balance or a
transfer of less than 2,000 pounds
within those rows the first time that I
find one and the last time that I find
one it's got to be less than 30 days so
I've got very very tight window here
assuming that I match this I find three
small transactions within 30 days so I
can have our other transactions going on
in between but the three small ones have
got to be within 30 days I'm then going
to look for a big one that's going to be
Y in which case only want one of them
and that has to be the time ID for that
big transaction - the last small
transaction that I find my transaction
number three of the small ones - that
time has got to be less than 10
okay everybody get that
so the first one I got to go through my
transactions if I find a value and it's
2,000 or less
sorry less than 2000 as long as it is
between the first time and the value
that I found is less than 30 days it's
good
it seemed I'll keep it in okay so I'll
go through a match as soon as I'm
outside a 30-day window I'm done I'm out
of it but if I can get all three
transactions then within 30 days I'm
good I'm going to keep it and then then
I can look for the big one
and make sure it's within ten days at
the last small yes
yeah yeah because I've gone through okay
I found three transactions that are less
than 2,000 pounds and within sort of
$2,000 and within 30 days of each other
okay so I now I've got a block of rows
that I have kept what I need to know is
what was the date of that last small
transaction that block of rows and last
X tells it to find the rows that it
matched for eggs and pull out the last
time ID that it had for X now we're not
case-sensitive here yeah so I can if you
want I can do that
yeah okay right okay first yes correct
yes can I look for both at the same time
yes so I can look for a sari and I can
look for X or Y there's various
permutations that I can do on how these
things occur and let's say I had a third
element to it that was the the transfer
had to be overseas as well I could look
for X or Y and the transfer had to be
overseas so there's there's all sorts of
different permutations where we allow
you to define how the pattern is grouped
how we search where we search when we've
finished with one second we move on to
the next set someone I'm just gonna I'm
just trying to keep this quite simple
this stage it's not you're not convinced
okay next question yes
yes so not good yes yes
can you I suppose the question is can
you double count if you've got
overlapping boundaries then yes so now
at this time this is a very interesting
point this is a very interesting
question what happens if you've got
boundaries that are the same so let's
say let me change this I get the
question okay so what if I did this
so the first three could be less than or
equal to sorry less than or equal to
2000 and I want those who in the 30-day
period and then I want Y where it's
greater than or equal to now I've got
the possibility that x and y could both
be in play at the same time what do I do
this is where we get into greedy versus
reluctant quantifiers how greedy do I
want X to be compared to why or how
reluctant do I want Y to be 2x and then
I can say if you've got a situation
where both are in play I want you to
make X dominant or I want you to make
wide dominant okay
yes so other questions
I know that's a very interesting
question can we can we define them into
any order we feel like yes yeah that
makes sense
I'd I've just never tried doing actually
just never occurred to me
it's like X right do that first yes they
must be so if you do x and y you have to
define them x and y and so on for it so
you have to define them in order okay
okay so why that's yeah somebody's this
is good
why is there no partition by or order by
clause you are about to find out okay
yes
okay that's a good question so
performance wise should I is there
anything I need to do to make sure that
I get the very best performance okay
that is a huge question what I'm going
to do is I'm going to cop out and say
that I have an excellent blog post that
takes you through those people who are
used to explain plans does everybody use
explain plans to test the sequel yeah if
you go into the explain plans I'm not
going to do it here you will see new
keywords clicking several new keywords
and we're going to cover them here there
is a presentation if I get time I'll
show you where it is if not if you go to
the analytics equal page on OTN there is
a link on there to a deep dive
presentation which is probably way more
than you need to know about patent
matching right at this point in time but
it will take you through everything
about optimizations where we can
eliminate thoughts for example how to
apply predicates where do they get
applied do predicates impact how much
data gets fed into match recognizer so
on and so forth but I'm not planning to
cover that here because it's like way
too deep okay any other question yes how
many patterns can you define as many as
you want
yes as long as you have to bring the
data set together to present it to match
recognize so this is where we get into
all the joins and pulling data in okay
so you but you can define as many many
matches as you want there are some
interesting implications of doing that
so I want to go into that I'm going to
point you to the deep dive because we
can get into those of you that have used
other pattern matching languages people
familiar with the concept in pattern
matching a backtracking some yeah okay
you you can get into the issue of
backtracking which is a whole topic in
itself and I can I can point you out
presentation that will give you
information on backtracking okay so we
have we have done this we've got our
basic query so let's run it and guess
what and work columns are correct
everything's right correct it's not
defined
we haven't what's missing here somebody
already said I can't really very much
partition by somebody over here
partition by order bar is not there the
other thing it's missing is measures so
what I've done is is is ripped out all
of the things that you don't need to
worry about or you think you don't need
to worry about so we have defaults all
over the place to try and make this as
easy to use as possible
sometimes those defaults can get you
into trouble
because you can miss out on important
key points
and we've failed to tell the
pattern-matching
what it is that we want out what
happened what do I want to know when
I've found a pattern so user ID doesn't
exist doesn't know about it we fed it in
because you know we're fed in because we
have that that's the view we created so
if I do let's get rid of that hopefully
this works there you go
it doesn't this doesn't exist it's there
but if I do that
well it doesn't exist okay
so we will get an error so let's see if
we can rectify that so rather than ask
for user ID
I want everything how do we get
everything out sequel select start
that's what you normally do if you want
to get all your columns back right
select star table contains no user
visible columns you may not seen that
before in the past probably one of the
few times when select star is not going
to work for you so we need to use some
of the defaults that we have so let's
move on okay so now I've added measures
okay I want to see user ID and amount
that's one that no data found
interesting so that's because we don't
have the right order so the data is
going into if we do let's get rid of
that let's have a look at our dataset so
here's our data set I got second I got
27th of Jan 20th of June 10th second
effects all over the place you're never
going to get data that's ordered so when
we run through the pattern matching I
can get it back
stick that back into there what I need
to do is order it I need to make sure I
use an order by so now haha result I
have two matches I've actually found two
matches that work but what happens if
rolled out of it let's add some more
data so you know in the past we just had
one user ID well I've put some more user
IDs in here so let's run that okay so
now we've got a new data set and let's
run that Oh remember the first one when
we did that before we had John John now
now all of a sudden I've got two matches
and there I've got two different names
now partition by yes so that's the next
step let's whack partition boy
okay so now now I've got my two
transactions for John which is what I
want and I got my transaction for me so
the partition boy and the order by a
very important very important leave them
out at your peril
your query will work it will run but it
may not give you the answer you're
thinking so make sure you do this make
sure you work out what your pattern is
and how you're going to identify it how
you need to carve up your data and order
it to make sure that you find the right
pattern okay okay so now let's run that
one and now I can get more information
out so before we just got three rows
back and forth at what we said was I
stick that one backing I haven't
actually told my pattern matching
program what type of output I want now
we can either give you the default is
I'll give you one row per match that's
the only information I'm going to give
you select style so I want I've just
give me everything you've got and I get
three rows I get my two columns which
are identified by my measures and I also
get the partition by column free of
charge
I can get more information about how my
pattern matching because I really don't
know if that's right how do how do I
actually know that that's right it's
kind of difficult to tell really because
I've only got three rows back if I
switch to a different output so
underneath the measures clause you can
tell us okay give us every single row
that you find there are other
permutations on this and you can find
those on my blog there's there's
different syntaxes that we can add to
this and if I run this now you'll see
select star same query all I've added is
all rows per match now I've got more
information coming back now I know the
user ID the time ID the amount the
amount of the small transfer the last
small transfer and the event ID and the
amount so one row I can change this to
one row per match and we're back to the
other one so from if you were a business
user you probably want you probably just
need to know okay which the account IDs
though that we're causing problems
that's all you're really interesting
what was the big amount if I'm actually
wanting to know has this thing actually
worked correct have I picked up each of
the transactions that I need to for
debugging purposes I might actually want
to do all rows per match just so as I
can pull out the data I can even do this
is where I always is it a is it with
unmatched rows or including
bear with me this may not work hey there
we go I can also do give me all of the
rows that you matched with the unmatched
rows and you'll notice the unmatch rows
I have playing values where I couldn't
pull them out so if you really want to
test it you pull out the full dataset
and you check to make sure that it works
okay
yes right to the back so it's just going
to pull out the last amount that it
finds okay what I should do here this
this is again where we get into
ambiguity so what if I want if I
actually want they might do that makes
it completely unambiguous and this is
quite important how you set these up how
you write this code is quite important
there are lots of defaults it can infer
a lot of information about the content
that you put into here so for example I
haven't specified amount it'll make a
best guess and think you want the last
value and pull it out be very very clear
the person who has to come along the
second time after you to try and do more
things is going to be really pleased if
they know that you actually men last why
don't amount rather than first ex-da
male yeah if you just say amount which
one do you want are you referring to X
are you referring to Y I do you want the
first value the last value what is it
that you want so be very very explicit
about what it is that you're after
yes
correct
okay so the question is when we were
doing queries before it kicked it out
and said there's no user ID now
magically I still don't have user ID as
a measure where is it finding user ID it
infers it from here the partition by
Clause as you change from one row to all
rows we will infer more information and
give you more information back by
default so things like you'll notice
here not only have I not mentioned the
user ID I haven't mentioned time ID
either or event ID or the amount field
but I've inferred those from the source
table I've got the time ID and the user
ID from the partition where in the order
by closes and all rows will then Hoover
up the other parts there they're not
mentioned anywhere in the it just infers
oh it's in the source table you're
asking for all rows I'm going to give
you all the data that I want because
only because I did select staff so we'll
give you as much information as we can
about the source table and the
information that we found that you've
defined in the measures Clause if I
switch that to one row per match at
which point obviously I can't do unmet
rows it all changes the amount of
information we can infer becomes a lot
more reduced because
what eventid am I getting for what time
period am i going for I have no idea
because you've condensed it I've gone
from 30 odd rows to 3 so this is very
little I can just has to be the measures
clause
okay does that make sense yes yeah as
long as my transactions occur within 30
days it's okay I can also say I want
three or more so it's got to be three
but it could be more I could say I want
3 up to five so if I put its this bit
here three or more oops three to five
now or I could do just three say three
or three you can set this up to
determine do I want three or more do I
just one three two one three two five
you've got the flexibility here within
that pattern clause to give us the
information that you want
yeah yes yes okay yes okay let me say if
I prove that oh that's the next example
thank you okay
how do I know let's go let's go back
again stick that one in okay run all
rows by match how do I know which is X
how do I know which is why how do I know
in this data set how many matches I've
got okay debugging tools hopefully
everyone was more awake than I was when
I was doing this
god I need some coffee I really do need
coffee max number and classifier your
two best friends when you're doing
pattern matching from a testing
perspective I am Not sure I can think of
any reason why you would leave these in
if you're going to then hand this query
off to a business user and say keep
running this or hand it off to some sort
of event processing system but they're
there you can use them for whatever you
want so let's run okay so
how do I do this max number MN 1 1 1 1
big transfer ok remember I only asked
for one big transfer so at that point
I'm done
I finished I've got a match 3 &amp;amp; 1
I'm done so that's my first match now I
can carry on I get into April and I can
start we'll start the next match xxx I
found 3 oh I found four
the key thing is who I found actually
found quite a few do you've got quite a
few X's but they're all within the
specified time period because I've got X
3 I need at least 3 after that I don't
care how many else you find as long as
it's within 30 days ok so that gets me
to I've got X X X X Y done oh now I
start the second partition and I reset
my platen my match number counter to 1
again so we are when you hit the return
and execute the command we're running
these partitions in parallel so they're
being processed in parallel each
partition so we can default each of the
match numbers to 1 in every single
partition so as soon as you hit a new
partition your match numbers going to
resync to 1 and then increment from
there onwards okay I've got half an hour
left so I am going to
doot-doot-doot okay I'm going to switch
over to the new reconnaissance is more
here you can play with these afterwards
when you go back to your hotel room this
evening you can magically play with live
sequel
okay so let's because this bits quite
interesting so what we want to do is do
okay so this is all the code that goes
with it okay add new requirements okay
so the business groups come back too so
fantastic that's exactly what we wanted
but what we really needed to work out
was do these transfers go to the same
account where do they go and the total
of all those transfers has got to be
less than 20k so we're looking for
people testing the system doing lots and
lots of little transactions what number
in okay so we've got you and you file
we've added the transfer ID okay we're
gonna modify the JSON file here's the
new JSON file go off kill yourself give
us the results as soon as you're ready
so we can input the new column we need
to go through our dataset again we are
looking for three small transactions got
to be less than 20k and the big one has
got to be within ten days and we want to
make sure that they're not going to the
same account so how do we do that so we
can keep most of the testing for X is
pretty much the same so the amount is
less than 2k so we're still looking for
these small transactions
and I just need to check that the
previous transfer ID does not equal the
current transfer ID done so I know then
every single transaction that I come
through is going to a different account
so I've got at least two accounts in
play and we still want it to be less
than 30 okay and then why huh
this is where it gets interesting we've
added a new test on white so I find the
big one but the sum of eggs of all of
the little transactions that I've
discovered must be less than 20k yeah
fairly simple okay yes
yes
okay so the question is could all
transfer IDs be different can we ensure
that all the transfer IDs are different
yes there is a way to do that given the
time I'm gonna try and move on because
there there's some important points here
so I'm just going to quickly go into
this because we've got new requirements
so I'm just going to insert this slot so
these are all my new transactions so
I've now got my transfer ID different
accounts going in okay hopefully you can
see that Alan Bob Tim John blah blah
blah okay new data set if I do this
sometimes this happens for some reason
okay so there's my JSON data still works
same table same table and so remember in
the first section before we had all
these new requirements my business users
we had a view I've got new data in my
JSON file haha my view still works so
that still runs so I've added new key
value pairs into my JSON document I
could still carry on running my old
sequel pattern matching
so let's find one let's drop that into
there there you go still runs all my old
code still runs even though I've changed
the JSON document structure everything
still works everybody happy with that
that's kind of yeah that's quite
important that you can modify the
structure I can add new key value pair
elements in there but everything is
still gonna work
I can still do all my selects my old
select still work etc so go down here
let's change the view
so I'm now gonna pick up the transfer ID
as I need that piece of information and
then
okay here we go so if I come far enough
down there we've done that one
so that's proving that the query still
works what I need is that one so our
original code still works here's the new
one so I'm looking for the previous
transfer ID I just want to make sure
that it's not the same as the other one
I'm not testing for uniqueness in this
case across all the transparent IDs I'm
just trying to do it simple
can I test this and I need to make sure
that my X amounts run so let's run there
don't
Oh what did I do wrong
yes where did I do it I've lost more
space in it you created okay thank you
this is a problem with no coffee okay
there it is so I now have I've
identified two transactions ooh
racket and I can do that I don't know
what I have no idea how it suddenly
started doing that but there we go mmm
Wow very good so I got two transactions
cracked you're gonna tell me that an
hour ago
and you people at the back could have
seen something okay so there it is so
we've taken new requirements we've
changed the key value pairs our original
code still runs we can still select from
our old view it still runs we can still
run the old pattern matching code we can
change the view we can still run the
same pattern matching code we can then
just simply change the pattern matching
code and incorporate our new
requirements so you'll see here
try this again see if it works oh yes so
you can see I've got first oh my god
like that's too much I can't see it you
know I've actually put in the Sun how
much is the Sun of X amounts that gets
me the total so I can check that it is
less than 20k each time and I've got
first time ID from why I pick up the
first time ID and I pick up the last
time I do
let's supposing that I wanted the third
the second time ID and if I tell you
there is not a function that says second
time ID so I haven't got a first a last
and a second how do you think you can do
it I did discover this by accident I
have to confess I'm not sure it's in our
documentation yet
tamo - don't call them ambiguously okay
so we'll make that second small so they
don't have to put that in hey second
small end small so you can pull them out
or I could make that three I think I can
also do it the other way around
so I can do I think I think this works
last two takes you back one from the
last that makes sense
so Nate so if you want if you've if your
ex was a a long range you can we allow
you to poke out find any value you want
yes
sorry say that again it's very difficult
to hear him he'll don't know why I'm not
sure I get the question I know and I can
I can make that three or four that and
then that's the same as that potentially
assuming that I've only found three but
if I say okay because I think one of
them we had five so I could say six
question then is what happens that's a
very good question
nothing
so if you if you poke for the wrong
value something doesn't exist or just
returned all okay so the last bit okay
am i doing good okay
what is the advantage what we think what
I think the advantage for you of doing
this in sequel is that yes you can do
pattern matching that's that's pretty
good in itself it's very easy to make
changes hopefully you can see that but
it also means that you can incorporate
either as the input into match recognize
or use the results to feed into other
things like time series analysis or
feeding it into a machine learning
algorithm or spatial or graph or using
it for forecasting the sequel model
clause so you've got a rich set of
sequel analytics that's available to you
you can bring all of that into pattern
matching or you can use pattern matching
to feed other things so what I've got
here for you and for those of you that
want to do this I'm going to run through
this code back at your hotel there is a
workshop for you to do at the end of
this you can thank me later so what we
want to do here I'm going to use roughly
the same scenario we're going to look
for purchase patterns for an account
credit card what I want to do here is
look at the distance between
transactions with anybody here in 2015
some of you any of you go to Andy
Mendelssohn's database 12c keynote No
okay we have a live video this so it's
up on my site where we actually did this
and used spatial and pattern matching
together so you can look at the
transactions that are coming in and then
you can see how far apart they are
distance wise
you could grab the IP address of
whatever the machine was that was
requesting the information get X&amp;amp;Y
coordinates for that you now have a
spatial geometry that we can then
compare and you could say okay here's
transaction 1
here's transaction 2 how far apart are
they geographically could you physically
have got from point A to point B in the
time between those transactions so you
know if it's 15 minutes and it's 10,000
miles apart chances are that's probably
not possible it's probably a fraud that
cards been scammed it's a fairly
simplistic idea okay but it gives you
the idea that you can mix and match
analytics so we have is anybody here
using the spatial features okay two of
you very very good very very easy to
understand at least that's what I found
really when I came to this completely
hold there are some basic elements that
you need to understand sdo geometry to
get a spatial data type a point which is
longitude or latitude and then we're
going to take those pieces of
information and create a calculation so
all I have to do again is add this is
very good
and my X&amp;amp;Y positions longitude and
latitude and I've gone for a slightly
different JSON file this time and then
if I wanted to load the data and convert
it so rather than having to take the
data and read it in JSON format and
convert it as part of the process
I could actually convert it upfront and
just read it indirectly I need this
additional column which looks kind of
weird it's of an SDO geometry type
so rather than varchar' - or rather than
date its SDO geometry type it's
expecting X&amp;amp;Y longitude and latitude
coordinates
so then I have to put the data in so I
might have you know whatever it is
insert into my values but sto geometry
and then I get these weird numbers ok
2001 8307 what does that mean
2001 indicates on just a 2-dimensional
longitude and latitude because obviously
spatial I can be three-dimensional so
I'm just after longitude and latitude
the 8307 tells it that it is actually
longitude and latitude so it gives it a
precise position on the globe and sdo
point type is the combination of
longitude and latitude and the last
values now because we're dealing in two
dimensions not three a null mount at the
end is required for two-dimensional data
points so now I can compute the distance
so I can do sto geometry SDO underscore
distance I pick out my values so the
first value x and y and then subtract it
all out and I say to do why so then
subtract I think I've got that there's a
minus sign missing there after the last
no oh no that's right sto geometry the
first set of values s do X s do y I'm
gonna have to give up on that liberties
that's too much without any coffee let's
try that so I want the first set of x
and y and then I want the value from the
previous transaction which I've set up
and if those of you that are used to
using lag function you can
use the leg function to go back and pull
up the previous value and again there's
various points there so I could do a
fairly simple something like that take
the difference in time that the
difference in the distance that I've got
divide it by the time difference and if
it's more than if it's less than C it's
fine it's probably doable so you can
start to mix and match analytics so now
we've got sequel patent matching under
the covers running spatial analytics to
work out distance differences so I was
going to do a live demo but we've had so
many questions and what I've done is I
have left a gap in the online schedule
that's up on live sequel and it all it
says it is this is one for you to try so
you can try writing your own pattern
matching sto geometry query when you're
back at your hotel room now somebody
asked about indexing and enhancing the
query capabilities of JSON with the
in-memory feature you can load JSON
columns up into the end memory columnist
or if you want to going forward we are
looking at I think you have to check
with the JSON guys on the demo booth but
there's also additional features that
we're doing around JSON access points to
be able to speed up pricings such as
JSON indexing okay so yes I have to mmm
get moving okay so that's what we've
done we've gone through the syntax of
max recognize you've introduced yourself
to how to work with JSON data types
you've looked at geospatial data types
apparently is new to a lot of people
that's good how to search a data set for
a pattern using geospatial analytics and
just for information purposes how you
can push up data into the in-memory you
can't test the in-memory option on life
sequel they said I don't think you can
because it's not it's not running on
live sequel so you just have to take my
word for it unless you have in memory at
your place so there there are a lot of
like the videos of this geospatial demo
are all up on our homepage on OTN
there's a whole series of scripts and
stuff linked on life sequel that you can
go through there's a whole series of
bits and pieces on my on my blog taking
you through the whole introduction and
then going deeper and deeper and deeper
and deeper into pattern matching for
example greedy versus reluctant
quantifies why do you need to worry
about them it goes into that sort of
level there is by the time I hopefully
get back to the UK a lot of the
information here is in this book so what
do you have an iPad or a Mac computer no
crashing windows no in which case I have
to put it in PDF format for you which is
a shame because you'll lose a lot of the
interactions that go on with iBook but
it will go through a lot of the content
that I've covered here including a lot
more information so go back
for after birth they should work
so if you get the iBook you get you get
the free video that goes with it and
then we have a whole series of chapters
so an introduction to pattern matching
there's an overview here we go
so we'll take you through writing your
first at matching clause like we've done
here using the built in measures so if
you want to get more information about
having use match number and classifying
they'll all be in their patterns and
predicates and we were talking about how
do I push data in how do I get data
round hanaway optimize the explained
plan how do i optimize the performance a
lot of that is covered in here and then
there's more information linking to
what's going to be in volume 2 which
will be out later next year empty rows
can I get an empty match it's an
interesting question if you if any of
you done pattern matching other
languages empty matches interesting
concepts state machines and backtracking
who's done state machines at University
College yeah if you thought you could
have put all those text books away no
you can get them all out again state
machines that is the fundamental basis
of pattern matching we if you want to
know what we're doing under the covers
how this all runs we build state
machines that's that's the code that
lays put together it builds a state
machine and runs a state machine if you
want to understand that concept that
will be in part two of this book and
then we get into the concept of
backtracking what happens if I get
remember we had four parameters X Y W
and Z what happens if I've matched X
I've matched why I've matched W and
don't match Z now I have to go back to W
and see if I can move forward again in a
different way to match Z and if I don't
have to go back to Y and then go forward
this is all the concept of backtracking
it's maybe a lot more information than
you need to worry about skipping and
then finally running a query explain
plans what are you going to see in the
explain plan how do you interpret a
splain and explain plan from match
recognized and so on
it's all in this but given the the lack
of the sad lack of Apple people here and
I'll put it out as a PDF as well and it
will be on the analytic sequel homepage
so I'll put it on here I'll put a link
so if you just have a look or if you
follow my Twitter account you'll get it
that said Burke that's alright that's it
I think oh seven minutes to go yes sorry
that one okay okay
I'm guessing everybody needs as much
coffee as I do thank you very much for
your attention have a good conference</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>