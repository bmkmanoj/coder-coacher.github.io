<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Move Data Between Apache Hadoop and Oracle Database for Customer 360 Degree Analytics | Coder Coacher - Coaching Coders</title><meta content="Move Data Between Apache Hadoop and Oracle Database for Customer 360 Degree Analytics - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Move Data Between Apache Hadoop and Oracle Database for Customer 360 Degree Analytics</b></h2><h5 class="post__date">2017-10-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/IvGd_AxLm08" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to work of open-world Java one
and this Oracle code session we thank
you for coming to this early morning
session and I hope you have a great
conference and if you were here
yesterday I hope you had a chance to
listen to the lots of interesting
sessions that were on and listen to
Larry's keynote so with me today so I am
mainly anomaly I'm a product manager
with the big data and database
organization at Oracle and with me or
Krishna who is a developer in the same
organization and Shalva who is a sales
consultant working with big data so I
think what you will find we hope as kind
of a spectrum of views from summary
lecturer Holbox
very closely with customers all the time
and really understands what customers
are looking for and me as a product
manager and Krishna working in
development so we hope to give you like
a spectrum of views as we go through
this session it is a two-hour session
and it's designed to be a tutorial we
plan to kind of break give lots of
breaks in between where we can have some
discussion and so on and I really hope
that we we have an engaged and lively
discussion so I am going to be talking
about some features which are not yet in
production so we have the standard safe
harbor statement so we've broken up this
presentation into kind of five different
sections and before going into that I
should just talk a little bit about why
we find this session why we thought this
session would be interesting we are
living in a time when analytics is
really cool I mean it's that this
tremendous the number of amount of
things you can do one because there is a
lot of data available
that wasn't available before and the
computer and the memory costs and so on
have really made lots of things possible
you can munch through a lot of data and
get very interesting insights into data
to just give a sampling of what our
customers are doing our customers we
have banking customers who are able to
munch through more data than they did
before and get better insights into the
creditworthiness of someone applying for
credit they are also able to look at
relationships between that person and
other people to to identify fraud and so
on we have media companies getting a
complete view of their customer by
looking at everything that the customer
does looking at what is in their
transactional database and also looking
at what they are doing on the web when
they browse through certain their
websites and they're able to determine
whether or not this customer is going to
stop subscription of a particular
service one of our telecom customers are
able to determine churn better by
looking at where what groups the
subscribers belong to and what people
they're connected to and see whether
they're likely to switch providers
because they are connected to people who
are also switching providers so we're
able to get all of these new insights
and it's really only in the last few
years so there are lots of sessions at
this conference that have have really
nice graphs and visuals showing what
things can be done in this session we
are kind of going we are not looking at
having fancy graphs instead we want to
show what are the tools and techniques
that are needed to create these graphs
because data is going to be on different
systems how do you move them to the
right place so you can do the analytics
and then have this nice-looking graphs
that give you insights into your data so
that's kind of the focus of this session
so data a moment I mean tends to be kind
of unglamorous and we like to call it
plumbing it's kind of essential it's not
it doesn't look cool but it's it's
essential because without moving data to
the right place
you won't be able to do the analytics
that you need to do so with that let me
come back to the sections we have here
today so I'm going to just have a kind
of an introduction into the problem
space kind of going a little deeper into
what I was just talking about and then
we have a section on streaming data
because that is really getting to be
very interesting to many of our
customers I know the session is titled
moving data between Hadoop and database
but we really began to feel that
streaming they including streaming data
also what really make this session
richer so we're also going to see how
you can move data from a streaming
source into your Hadoop platform and
then onto your database so that's the
first section and then we'll go into
moving data how you can move data
between Hadoop and Oracle database in a
fast and secure way and then finally
there might be times when you don't want
to actually move the data I just want to
query in place you want to be in one
system and you want to access data from
another system and we'll see a couple of
technologies of how that can be done so
I would like to have a show of hands how
many of you are familiar with Hadoop or
have used Hadoop in your organization's
okay thank you so it's a good number so
I don't need to go into an introduction
to Hadoop which is good so I'll just
very quickly go over this slide so
Hadoop is a distributed computing
platform it's great at doing some tasks
specifically when you can have nodes
working in parallel operating on a
section of the data it's shared nothing
architecture so you know the nodes it
works really well when not--don't
do not need to communicate with each
other but are just working with a subset
of the data on that node the nodes in a
Hadoop cluster share the Hadoop
distributed file system that is designed
primarily
for the Hadoop cluster and we'll be
looking a lot of what we'll be looking a
lot of the tools and technologies we are
looking at today will be working with
the Hadoop file system
it's Linux based but it's it's meant to
support a distributed computing
infrastructure hive is a sequel layer on
data in Hadoop so if you think of data
files in Hadoop on this Hadoop file
system if you want to this query the
data across across these files and easy
way to do it is through hive so hive is
not a database in the sensitive contrary
don't inserts and there's no disaster
recovery and so on built in but it gives
you a sequel layer on top of on top of
Hadoop so we can query Hadoop data with
a language that might be familiar to you
that is SQL and we find this
particularly used by Oracle database
customers because they are familiar with
sequel they have the same standard
sequel and they that they can use to
query data in Hadoop there are databases
in Hadoop like HBase and and to some
extent Impala and the data sources but
what we'll be focusing in this session
mostly in hive where we find that the
most heavily used in our customer base
Kafka is a technology which is getting
increasingly popular I think I saw a
tweet about that there are 19 different
sessions on Kafka at this Oracle
conference it's a scalable platform for
managing streaming data or managing this
firehose of data if you're having data
coming in at a very high speed how do
you make sure you're managing their data
and providing an API for applications to
use the data that is in a Kafka cluster
so one of the things we've been looking
at here is to see how Kafka data can be
queried by or a sequel query running in
Oracle database and finally object store
so objects store is a model to its
historic architecture that manages data
as objects it does not have
hierarchical structure of traditional
systems in the Oracle cloud
infrastructure object store is
increasingly a a way to share data
between different cloud instances so
that that also plays into this when
we're moving data so I wanted to kind of
highlight that as well so when we're
talking about 360-degree analytics and
we're talking about data coming from
different sources here are some of these
sources that we commonly see when
someone is trying to design an
architecture around looking all of their
data at all of their data there is
structured data in the database that's a
transactional data the point-of-sale
data your customer data your customer
addresses all the data that you want to
store reliably and you want to the kind
of data video you're writing queries
where you're this saying I want to go
and get a specific record very quickly
like I want to say find me some
information about Melly in the database
it can do those kinds of point queries
very very quickly and efficiently
because you have indexes and all of that
then we have the Hana left in the middle
there we have the Hadoop file system
which shown itself to be really good at
managing unstructured or semi structured
data
examples are log files from your web
servers JSON data that might be coming
in from devices a social media files
electrical fields popular are popularly
used for doing some kinds of analysis so
these are data types that you don't
necessarily want to store in your
database and you might not want to store
it forever store them for a long time
but you want to store you want to do
some analysis on the data do some get
some insights from the data and then
maybe preserve a subset of the data
preserve the results of the analysis for
later years and then on the left here is
an Internet of Things or streaming data
data captured from devices that is again
very of great interest nowadays so we
have so these are these tend to be
different systems and the database is
managed has his own architecture Hadoop
as his own architecture Internet of
Things systems have their own
architecture so when you are doing
360-degree analytics of all your data
you really want to combine all of your
data together and analyze the data
together and that's where you have the
challenges of moving data so each of the
systems also have their own tools for
doing analytics and analysis I've just
given a few examples here for example
our Internet of Things cloud service can
combine can connect to our bi cloud
service Hadoop has hive and Impala which
is another sequel weight other way to
use sequel or Hadoop it also has some
spark spark is very popular in machine
learning these days so if you want it's
very very common use cases to use pork
to analyze data that is in your Hadoop
to get insights into your data on the
database idea of Co sequel bi tools we
have support for the ARM statistical
language and also we have a machine
learning dashboard that's going to be
available with the data warehouse in
cloud service so each of each system has
its own preferred tool and when we want
to access data from any source it the
question becomes where do you want to
really run your analytics if you're
running analytics from that data source
of course is straightforward there will
be access mechanisms for moving that
data but if you want to let's say you're
doing some work with some
internet-of-things data IOT data and you
want to kind of link it up with data
coming in a Hadoop file system you need
to have a way for that compute engine to
connect to the Hadoop file system what
is more likely so you're doing some
analysis in Hadoop and you have some log
files in Hadoop and you want to link it
up with data coming in from an IOT
service then you want to do some you
want to ask you need a way to access the
data in a secure and efficient manner
because the security models are
different with the different systems you
want to be able to kind of we won't be
able to do that successfully and if your
compute engine is hadoop you might want
to acts you might want to access data
from the database
copy some important data from the
database into your Hadoop cluster and
then do your analysis in the in the
Hadoop cluster or you could say that
node my database is where my skillset
lies that's what I'm going to use for my
analysis so I want to access from the
database all of this data and there's
other sources so all of these different
connections part of our of interest so
when you're doing unified analytics and
looking at all of your data together the
question to ask is really where do you
where is your compute engine and there's
no clear answer to that I think it
really depends on your use case like if
you want to look at what we call the hot
data in the database
you would do the analytics in the
database if you're looking at more
historical data you're doing more like
vashti oriented processing you would
then probably look at hadoop as your
compute engine so it depends on it
depends on a use case but where do you
want to do the analytics and depending
on that you want to move your data so if
your database is your compute engine
then you might want to move data from
your streaming sources into your
database and from your Hadoop sources
into your database and today we will be
seeing about how you can move data from
streaming sources to HDFS and from there
to the database if your compute engine
is in the Hadoop cluster then again you
want to pull in the streaming data from
the from the streaming sources and then
pull in database data from the database
so that you can run your analysis in an
Hadoop environment so what we're going
to talk about in this session is as I
kind of mentioned earlier we're going to
talk about streaming sources and
connecting them to to the Hadoop cluster
we're going to look at two streaming
sources one is the Iowa T cloud service
and all the demos and code snippet we'll
be showing here all of them work with or
cloud instances and the demos we're
actually going to be showing or on live
cloud instances I hope the network holds
out and things work fine so we're going
the streaming piece there we're going to
show with the Iowa T cloud service and
the kafka we're going to show with the
event hub cloud service which I think I
have in the next slide here yes so these
are the different cloud services that
we're going to be using the IOT cloud
service that Kafka is going to be took
off even have cloud service is a service
to manage kafka data and the Big Data
cloud service for the Hadoop side of
things and then the Oracle database
cloud service and we're relevant we will
be using the object store so some of
these use cases and some of the demos we
will walk through will actually move
data through the object store so we will
be going through that as well so before
we just dive into the first demo which i
think is the IOT cloud service i just
wanted to kind of talk through some of
the data movement and access challenges
that we see after some time of working
with customers one is the parsing of
formats each of these different systems
have their own format for their own
reasons the database is an optimized
format it makes it easy and faster run
database queries Hadoop has their own
formers that they've developed in the
last few years part K being a very
popular format for doing that 4k is a
highly compact format that is used for
compression as well so you have a lot of
data in the in your Hadoop file system
4k seems to be a preferred way of
storing that data
it's a columnar format so to understand
the 4k data you need to parse the 4k
format and so if you let's say you're
running this from the database you're
running your analytics in your database
your database application needs to
understand 4k data and Krishna here by
the way is the resident expert in 4k
format it's just done a lot of work on
that or sees another similar format
coming from a different segment of the
Hadoop community JSON is very popular in
these types of applications and then
mapping types so the database is a
strongly type system
Hadoop is much more loosely typed for
example you would show if you're storing
time or date people tend to just use the
string datatype on Hadoop rather than
using the time stamp which Hadoop does
have our hive has but they'll just use a
string data type in Hadoop so if you're
loading data from Hadoop into the
database you really want to kind of take
advantage of the fact that Oracle
database has the date type and you can
do lots of functions on that date type
so you want to map your string datatype
here into the date datatype in the
database and for that you need to have
the right format this is almost a single
common issue that we people run into
that they don't specify the right format
while mapping their date fields so that
the date is not interpreted correctly so
they're also rejected while loading and
so on and you don't want to lose
precision and you if you have
milliseconds in your data you want to
keep those milliseconds when you're
loading into the database as well and
there are things to because there are
different systems that Oracle database
has its own default date formats Java
applications have their own default date
formats all of these need to be mapped
together and we have the tools to help
you do these types of things and
performance of course everyone it's
always interested in performance and the
question that's particularly interesting
to me that comes up within customer says
I'm ingesting data at this high rate
from all of these sources can I load it
into the database at that speed so a
telecom customer was ingesting data at
around I think it was one 400 GB an hour
it is not a very high speed but they
have some network constraints so they
needed to load into the database at that
same speed otherwise you need to worry
about buffering and so on right so they
were they were not doing much unheard of
they have just using Hadoop as a staging
area getting in data at a rate of 40 B
an hour and then they wanted to push it
into the database without any lat time
so that was kind of their SLA can I get
all the data into the database before I
have new data coming in so these are
issues that people work with and of
course Diagnostics I'm getting all of
these 400 GB rows what is the guarantee
that these rows really low
do I care if some girls were lost how
easily can I go find out where my
rejected roles are and those of you you
worked with loading and database I'm
sure you you know the challenges with
this and we are putting into place and
this is one of the things in a road map
on the cloud especially how can we do
Diagnostics to make it really easy for
the user to find out instantly how
successful their load was and finally
what skills do my teams have we find
that the top we're going to walk through
a few load tools today and we find that
people tend to pick the tools that match
the skills that they have which makes
sense so if it's a database group trying
to pull in data from multiple sources
their preferred tool and language would
be sequel if it's a Hadoop group wanting
to pull in data from different sources
of push data to other sources they
preferred tool or preferred platform
Sidhu
and they want to run Hadoop jobs to to
move the data and of course Park is
increasingly popular so that influences
the tools and techniques you would
choose when you move data between these
different systems so with that I'm going
to I think there's now at our first demo
how do we stream data into hive an SD FS
just a recap we're going to be looking
at the IOT cloud service and kafka data
from the event hub cloud service or
service so I should pause here for any
questions before we get started with
this section yeah
yeah the slides will be available online
the slides will be available online and
I have included as much as possible code
snippets as screenshots because I don't
have a real good way to upload scripts
so I want to make sure that this is
available I am actually also I think
I'll make this the code snippets
available as a blog post and one of her
blogs and the blog address is coming in
later in the session so I'll make sure
to make all of this available
so so I think the value-add with Hadoop
is so Kafka does a good job with
managing real time streams if you're
having log of lots of log files that you
want to persist and go back to later
that's one reason to look at Hadoop as
well so you do some processing and a
Kafka then you move some of that data
into Hadoop the other thing is the
sequel layers and cough car still
immature the sequel and sequel is really
the a very popular language for
accessing data and doing further
processing and they want to do that they
can you have hive an Impala and spark
sequel on the Hadoop side that being
said I think finally it is a question of
choice I would say Hadoop technology is
mature and Kafka's it's comparatively
newer technology so you might have a
head of system up and running then
there's no reason to destroy it and this
amount of Kafka instead you would
connect cough card to your existing
system that's the way I would look at it
but if you're designing a system from
scratch you could look at what Kafka
offers and then maybe you can say but I
think how do persuade more a lot more
features I think I don't think at this
stage we can say cough a kind of
everything that had of does I think that
would be my opinion yeah
yes so there are two cloud services the
there is the Big Data cloud service
which is the engineered system and that
uses cloud era so it's when I say
engineered system is the Oracle Big Data
appliance which you might have heard of
though it's the Oracle Big Data
appliance in the cloud we have another
service called Big Data cloud service
compute edition which is incidentally
what I am using for the live demo today
just because I had access easy access to
it the Big Data cloud service compute
edition is directly sourcing from Apache
Hadoop and Hortonworks I think it
started off with Hortonworks and out
directly sourcing from Apache Hadoop so
that is not clutter abased
so yeah you would store so you store the
part you can store the parking files in
in object store correct but then where
would you do the processing do you want
to do the access them from the database
and you want to access them from you
want you
right right that yeah yeah okay I say I
understand your question now so the
question is whether we want to use hive
and which is more file based and more
bat oriented rather than spark which is
more real time and spark is definitely
the language of choice for machine
learning type of analysis you're right
some people are moving into spark well
again if you're processing lots of
historical data I think the way to do it
is hive an Impala
I think spark is really good for like
more interactive kind of querying so I'm
doing some data analytics and I'm asking
some questions of my data and then I
want to go back and feed the results
into something else
I think spark does well in those
situations but if I'm going to focus if
I'm going to do some historical
processing on five years worth of data I
think still hive and Impala are I think
better tools for that we ourselves so
one of the tools that were using right
when you will see later that we're
loading from Hadoop into the database
our tool is based on the old Java jar
files that you were talking about it
goes through it goes through all the
data on the Hadoop side it does some
processing organizes the data and then
loads it into the database we really
looked closely it should be more to
spark should we move this re-implement
the whole thing in spark and we felt for
the use case that we had which was
loading large amounts of data fast into
the database the hive Impala kind of
technology worked better if we didn't
get or in other words we didn't see any
gain with spark because there's a lot of
data you have to write a lot of data to
disk anyway if you're going to write an
sitting it another way if you're going
to write a lot of data to disk there is
not much of a difference it only makes
sense you were able to do everything in
memory to your spark so that's other way
to look at it yeah right so all this
that is there - yeah
so that let's now move on to the Iowa
cloud service demo thank you good
morning my name is Silva a principal
within the sales consulting team I work
mostly close to the customer and then
see the services are provides from the
customer's viewpoint as well and today I
will be focusing more on IOT cloud
service and the integration of IOT cloud
service with Big Data and also the main
focus I when we talk about IOT is there
are various streams or various areas
various industries you can look at it
one is in the production manufacturing a
lot of fields but today in particular I
will be focusing on smart cities because
that is one of the popular areas that
has come up in the recent times and I've
worked very closely in the APAC region
and in India recently Indian markets a
lot of smart cities have been declared
by our prime minister 1:50 to be
specific and it's coming up in a very
big way and it's a big market for this
right now as we speak and when we talk
to the customers from the smart city
perspective Ellie was talking about
large amount of data that is coming in
and that needs to be handled that needs
to be processed and Hadoop is the way to
go
big data is the way to go now what kind
of data is coming in that's another
question that way that comes into
picture we have so many kinds of devices
that comes into picture when we talk
about smart cities for example we had a
partner who is more into lighting so
they have built something like a smart
street lamp so which has some predictive
maintenance you know capabilities where
it says okay I need a maintenance it's
going to tell by itself that I need a
maintenance and also automatic metering
automated metering and all these things
happens within the lam
post we have partners who are into
traffic management one partner I can
take a name is map my India who have who
are working with companies like uber and
Ola these are the taxi taxi companies
and they are streaming a lot of data as
well from the location or perspective
look a lot of location data similarly in
the smart city situation we see if you
go out further one more level and
control the city everything in one from
one place for like a command center
that's also a concept that is very
popular in your Asian markets right now
you talk about disaster management if
there is a fire how are you going to
deal with it so there will be sensors
that is talking about that will transmit
when there is a fire in a hospital when
there is a fire in a laboratory when
there is a fire in school so the way we
deal with this is different right how I
deal with this fire in school is
different from a laboratory is different
from a movie theater is different from
you know hospitals I mean another set of
data or another kind of data that we get
is we have water storages in the city
right to supply drinking water to the
people and for I don't know here in
India we have two kinds of water that is
being supplied to homes one is the
drinking water another one is not
drinking water you use it for the
purposes so when you have drinking water
there is a certain level of quality that
needs to be maintained in the area in
various tanks when the when these
parameters change there is an action
that is required to be taken to you know
make sure the parameters are at the
right levels so this is another kind of
data we are getting and also we have
recently you might have read in the
newspapers about floods that happen in
Mumbai floods that happened in others
couple of cities because of heavy rain
there are water levels and indicators
water level sensors placed at various
parts of the cities and they talk they
see the rising level of the water on the
streets and with the past data from the
rain meteorology departments
if this rain continues for the next two
days there is a possibility of flood so
we should be able to predict that
beforehand so that we can make efforts
to evacuate the people or you know keep
any disaster management teams on the
toes to helps such a situation and our
you know minimize the casualty and
recently I was talking to a customer and
they wanted to it's a city of Mumbai
actually where there is a high level of
pollution you know because of traffic
because of industries so they wanted to
monitor carbon monoxide levels and
various are the levels in the air
basically check the air quality at
various parts of the city so you know in
a city like Mumbai today you can have
more possibilities of how you capture
this data one mechanism we can
straightaway think of is place sensors
in different parts of the city capture
the data from those sensors that's one
thing but if you really look at it those
sensors are sitting in one place so the
readings that I get these from you know
discrete locations that I have
predefined so when we were talking to
the government they came up and they
said okay how about we use the hawkers
you know these guys they move around the
city to sell various kinds of products
probably it's not true in America
anymore but in India and you know Asian
countries it is still possible so we
attach the sensors they're very kind of
a band and they keep moving then we get
a better view of air quality in a
particular region when we capture the
data from those moving sensors and I'll
be used to different kinds of people not
just the hawkers we'll all the people's
people who actually move around the city
and capture the data now having said
that now we have a idea of what kind of
data that we could be capturing now all
these devices they talk to our IOT cloud
service so we have the service where you
can virtualize the device you know like
digital twin but you call and you can
secure these devices how do we secure
this device
can I switch now when we talk about
scenario that I just explained to you
the you know the impact of not being
secured is huge just imagine somebody
can hack into my water storage you know
tweak the parameters or you can you can
you know send wrong data and the impact
is huge of such a compromise so there
should be a possibility to secure these
devices and make sure the data that is
coming is secure and only legitimate
data is coming in to my cloud service
that's possible here wherein we define
the service you know I have 100 devices
placed on the city and we will you know
create it we know fit here and then we
will make sure it is secured how do we
make it secure I will just show it to
you very briefly you know whenever you
have devices that are place here when
you register a new device and say ok I'm
going to receive data from here there is
something called a provisioning file so
the more when you register this you
download this file you can imagine this
as a private public key combination this
is going to sit on the device that's
going to talk to me talk to my Hydra
cloud service so the data exchange
happens using this so that way it is
secure and the legitimate data is coming
true now there are some inbuilt
out-of-the-box features available here
wherein I'll be able to send this data
directly to Hadoop or Oracle database
there are some predefined integrations
possible and also it is able to
communicate with any system that offers
any REST API now once we do this we can
define the integration that I want to do
you can define your own applications and
then you know define the integrations
you want so it is as simple as that you
create a new application for your city
smart city and then you go to the
integration and you start creating
integrations with the systems that you
want for instance now these are the
possible integrations first one is an
enterprise application it can be any
application custom applications that you
have defined if it is able to
communicate like this you will be able
to integrate so the moment you are able
to expose ready api's you are able to
integrate with it similarly this is
another interesting one where bi cloud
service what do we do with all the data
that we get here some of them are
actionable some of them I need for
future analysis or doing prediction so
you will be able to pump the data into
bi cloud service with no additional
coding required you just configure and
it works similarly you have it for
mobile cloud service JD Edwards and
storage cloud service and Big Data cloud
service this is what we with Emily was
talking about so you will be able to
just with the click of few buttons
integrate the data directly into this
now I just choose the Big Data cloud
service and then just fill in these
parameters and it will be able to pump
the data just to save time I have we
have it defined here already so it
generates a unique ID for this
integration and this is the name name
for the integration and here you just
define the connection parameters where
is this service located that is a first
parameter so that is the URL to the
storage next one is identity domain in
which this provisioning is done and this
will be the name of the container where
the data from the devices or whatever I
am going to give out is going to be
stored it'll be in JSON format and these
are the credentials that we'll be using
now this is the integration but where
does this get the data from that is
something I define here as streams so in
this case what we have done is we have a
traffic light data for this demo purpose
what you have done is we have defined a
traffic light and the data is coming
from the traffic lights will be streamed
into this integration so anything that
comes out of the traffic lights like
signal colors
the intensity of the light and alig no
electricity how much electricity it is
getting so these kind of parameters
anything that you want that will be I
know pushed into this integration and
you can just verify here all the
messages that were received previously
and of course once you give all this you
can just verify the connectivity to see
if your integration is okay now it was
something with the connection yeah okay
now what we do is to mimic this traffic
light that I showed you we have defined
a simulator here because we are not
having a we are not able to set up a
traffic light here we have a see movie
have defined a simulator here where in
this will mimic a traffic light okay so
now this is going to generate data like
how much electricity it is having and
what is the right color it is switching
between red and green and you can see
the amount of lumens that's the
intensity of light so it is generating
some data okay I think we have enough
data now it has generated for some time
now I'll stop this now here you can see
since the last synchronization it has
received 23 messages that is what J we
just generated so there are two
possibilities one to synchronize this
one is you can define every one hour
every 30 30 minutes you want to
synchronize automatically just enable
this and choose the interval that is
available in this list otherwise the
other option is to you know you can just
manually synchronize in this demo we
will use this so we'll say synchronize
and the data is pushed into the Big Data
credentials that we just give here that
cloud Sarris Big Data cloud service okay
now the synchronization is
you'll be able to see that here yeah so
here like Mellie was explaining like
Mellie was explaining hi is one of the
ways to easily query the data from the
you know Hadoop file system we are using
five here and this is a high beam
interface and I just shoot a query
wherein I want all the records that has
a light lumen less than one one four
zero to me this is a matter of concern
but because any traffic light that has a
value of less than 1 1 4 0 is going for
maintenance so if the light is dimming
down so there is a maintenance
requirement so that is the meaning of
this so this is the data that it was
that was transmitted the ID you see is
the message ID every message that is
received has a unique ID attached to it
and the light lumens is the intensity of
light so anything that needs maintenance
will be able to see it here so this is
what I wanted to show it is as simple as
that and ok there is another part to it
where there are client libraries
required to pump the data from the
devices into IOT cloud service that's a
different session by itself Oracle of
words rather IOT cloud service offers
mainly two mechanisms one is using the
client libraries
Varian you can write Java programs to
pump the data into IOT cloud service
another mechanism is IOT cloud service
offers api's that you can just invoke
and then transmit the data from your
devices this is a mainly two mechanisms
and we have implemented it using
Raspberry Pi no dempsey you are
Raspberry Pi and esp8266 already know
this kind of microcontrollers your
devices connected to these
microcontrollers and Raspberry Pi of
course to IOT cloud service any
questions
IOT cloud service sends it in the form
of Jason and Jason yeah yes a tween flat
files in Jason exactly we should show
you that actually which so if you look
at so this is the hive table definition
that we used to to run the query that's
that's the hive table that shall watch
this query so you can see here that the
JSON this is so we're using the JSON
serialize and deserialize ER that was
available with the hadoop distribution
we were using so that the hive external
table can interpret the JSON data and
query it as Shalvis query and iot-cloud
service also has a stream Explorer
within within the box so you will be
able to do a analytics within IOT cloud
service as well yes
sir I think the question is then he put
it in a message queue or did you just
how did how did you sink what I think
what shall what did was he he showed
that they even do a manual sync and an
automatic saying he can either schedule
it to say every five seconds write it to
the object store and then on to the
Hadoop file system or what he disordered
the demo was he actually said
synchronized now which was when the
messages which were queued up were
written to object store and then they
had the file system didn't say a
question and also iot-cloud service has
a no sequel database inside
we're in it will store all the messages
you can decide to you know how long you
want to retain that that's something
that you can configure so it has no
sequel database where all the messages
are persistent yeah
from the moon no sequel database I just
mentioned so my ID cloud service has a
no sequel database in the clouds where
it is storing all the data all the
messages that is coming in and there is
a parameter you can configure how far
how long you want to retain the data
within the cloud service okay it will
have a limited storage you cannot you
know store it for a long time that's the
reason you need to push it to a person
in a permanent location big data yeah
and you know the other possibilities I
showed was big BIA that is a bi cloud
service which will now be or plan
analytics cloud service yeah we showed
the Hadoop file system it can be other
sources as well yeah we can put it in
other places as well but what we showed
was moving it to the Hadoop file system
and we are moving it remoted wire object
store in this case so we put the data in
object store and then there is a way to
more data from object stored into the
Hadoop file system the cloudera
distribution comes with something called
this CP which is like a parallel mo of
data from object store into the Hadoop
file system but we have found it's not
fast enough so Oracle has developed its
own Oracle DCP which was the tool
basically being used correct in yes
in this demo wave mode it a nice toaster
process yes what is shipping now is this
two-step process but future roadmap is
to move more direct and you see in the
Kafka case we will be doing it directly
we are not we won't be doing the staging
the next demo is a tough dilemma that
will not involve staging
for the IOT cloud service it is the
meter closure its meter cloud service
you bite for a month or no it's the time
time based
in this case so what I guess no I think
they're all individual files we are not
merging the files themselves and you can
have individual files and the same hype
query will be able to query the data as
long as they're all in the same location
we see at the bottom of this slide here
the bottom of the slide you see that's a
location in the Hadoop distributed file
system will be queried yeah right so
what this hive definition does says say
read JSON files in that location that's
basically what this hive table is doing
so okay now moving on to Kafka so cough
cough we're going to use the event hub
cloud service which manages a Kafka data
in a cluster and connect that to the big
data cloud service and this we will see
is done without staging we have here a
hive table a hive table is going to be
querying Kafka data it is going to be
using a storage handler that we defined
so in the previous example we were using
the JSON serialize and deserialize so
that comes with the heads of
distribution typically we but what we're
going to do now in Kafka is an Oracle
developed hive storage handler so hi has
an extensible mechanism where you can
extend it to look at new data sources so
we look at our own hive storage handler
to directly read from a Kafka's room
without this staging so I think enough
people know something but Kafka but let
me just highlight a couple of terms here
for those who are not familiar with
Kafka so Kafka basically categorizes
data by topics you can have a stream of
messages that is a topic like traffic
data where you see the same data and
this demos well so traffic data could
all be a topic and within a topic they
have further categorizations to kind of
balance do some load balancing while
managing the data so Kafka says concept
of Kafka broke
and you have different Kafka brokers
handling subsets of data within a topic
each of the subsets is called a
partition the most important field we
feel is the timestamp so you have a
timestamp that is associated with each
message that has arrived so you can
query based on that timestamp what we
have today
it's we don't index in the timestamp so
you're just looking at all the
timestamps and querying messages by
looking at all the data but we are
working on a timestamp index that will
make it more efficient to query data by
timestamp so the reason I wanted to put
these terms up here is to show what
happens with our Kafka table so like I
said this is something that's developed
by Oracle and you can see here the Kafka
30 that we have that was developed by us
using the hive storage handler mechanism
and the hive table that is created has
all of these these columns automatically
it has the name of the topic it has a
value of the partition ID for each
message it has the value of the data
value itself and then offset and
timestamp and so on so in addition to
the data that is in the value in this
case this is just a simple example so
you have the value this is a string but
in the demo we'll see the value is
actually a full JSON message because
it's a similar or a full message of data
coming in from a traffic light so in
addition to the data that you have you
will have all of these additional
columns in your hive table the timestamp
and the topic and the partition ID and
so on and one reason to do this is to
you to use this Kafka 30 is to directly
query this data from an Oracle database
so the Big Data sequel which I'll again
touch upon later in the session Big Data
sequel is a way to query data and hadoop
from an Oracle database you have an
Oracle sequel query that maps to a hive
table and can query data in the database
through hive so now that we have created
we can create a hive table or kafka
streams Oracle big
The Seeker can be used to run an Oracle
sequel directly on Kafka streams wire hi
so you can actually direct you can
process streaming data using Oracle
sequel in the database which is I think
pretty cool so what we did to set up
this demo is so this is from the event
hub cloud service that we set up we set
up a Kafka topic and the thing we had to
note was the public IP address because
this is used by hive for the hive table
to know where it's reading the Kafka
data from so we had to note the public
IP and then what we have created for
this demo is a hive table that looks
somewhat like this the hive table
specifies the 30 that it's using and the
stories handler and a few properties
that have to be defined the name of the
topic and the IP address and port that
we're reading from so you have the event
of service putting in a firehose of data
that you can read from the big data
cloud service why are this hive table
you can see here that I also have Avro
right there right above the IP address
because we're specifying the schema of
this data and our format which is a good
way to do this when you're reading data
through hive because the data that is
coming is a we are not here using JSON
to interpret the schema we're saying use
this Avro schema to interpret this data
so that you can understand what the
different fields are so this type of
schema registration would need to be
done for the type of data that you're
reading so depending on what device or
what topic you're reading from you can
create a schema that will then tell you
how to interpret the data and that
schema is what we're providing here so
with that let me go to the demo
so what I have here is I'm going to I'm
going to the start a this is a simulator
again so I'm just sending it a bunch of
understanding in a bunch of Kafka
messages right there in this other
window and then I'm going to kind of
query run this very similar query to
what shalwar was running and I think
they have more data here so you can see
this more data coming or so there's
twenty thousand five hundred rows here
that were retrieved by this hive table
from the Kafka stream if we run this
query again there will be more data and
the reason I was twenty-two thousand
rows so between the two runs of that
query two thousand more messages were
generated by the simulator I have
running in the background so you can see
here kind of the live nature of district
so hive is able to query this data live
just as data is getting is being
generated so and again we're getting the
ID and the luminescence information to
do maintenance and so on like shell was
saying so now going back to so now that
we can query this in hi the next step is
to query this in the database and that
is how you would do this I don't have
access to a live Big Data sequel
instance I had to put a screenshot but
later on or we are planning on using a
VirtualBox and Krishna will show how you
can created this table from a database
using sequel developer but this is what
the table schema looks like now that we
have the hive table there hive table I
had called the hive table traffic Kafka
now I have the high table I can create
an oracle sequel table on that table and
query this table directly from that from
the oracle sequel so just like i was
able to query with hive the live
messages as they were coming in i can
query the live messages that are coming
in with oracle sequel
any questions
right that's a good question so the
question is what software do you need on
the Oracle database side to run a query
like this so Oracle Big Data sequel is a
separately licensed product on Prem and
it's a separately licensed service on
the cloud so you would need the Oracle
Big Data service if you were doing this
on a cloud that's a good question
currently Oracle Big Data clouds at
Oracle Big Data sequel works with
databases on Linux but you raise an
interesting question because we do have
of course a lot of customers have
databases running on IBM and database 91
hp-ux so we have a lighter version of
Oracle Big Data sequel called Oracle
sequel connector and that works with
databases on these other platforms like
IBM and hp-ux and Microsoft Windows on
the roadmap is to make this kafka
handler available with that sequel as
well but that's on the roadmap Oracle
Big Data sequel with all its rich
functionality is only available on
database running on Linux and Linux and
Solaris actually Linux and Solaris
well Oracle Enterprise Linux and Red Hat
or supported I don't believe cent OS is
supported it had a double check on that
I think or claim to price index and Red
Hat it would work you just need the
Oracle Big Data sequel license for that
correct yes it's this yeah it's just
yeah is this license differently but I
mean there is an installed Oracle Big
Data sequel has to be installed so
there's an hole so and it because a lot
of the Engstrom happens on the database
side there are some pieces on the Hadoop
side but lot of the installers on the
database side though there's a separate
installed tool it comes with with the
big data licenses that you typically
have so it's called mammoth then it's a
separate install service it's a surprise
you just need to put the bits in the
right place and then it's it's not a
complex internal service but you a tool
is useful the manager this big did a
sequel is 12 specific but the other
lightweight version I was talking about
which will do some of this some of this
functionality that is actually available
on 11 so we can talk after actually if I
can give more details on versions and
all of that any other questions
yeah correct
you will have to create another Oracle
table to point to that table the hive
table right so you can have like you can
have like a thousand five tables and you
can have a thousand Oracle tables each
pointing to one of the thousand five
tables so you can think of a review so
it's really an external table so you can
see here organization external right
here so it's an Oracle external table so
we create some each external table maps
to one hive table so you map an external
table to five tables and you create as
many as you need you can create
thousands of tables I think our clothes
our collection table also can't read
from multiple multiple files okay okay
that we shall take offline the external
table question I take offline later not
in his current implementation but that
is on the roadmap there on the roadmap
is to put a jar file on the database
that you can use directly to query and
not go through hive the reason we went
too high was to make it available with
both stuff but I increasingly we're
getting that question yeah so yeah
you can have an you can have a hive
table and multiple topics it doesn't
have to be one term but it so depends on
how we want to logically organize your
data yes you just had to have multiple
tables querying that that topic yes so
you can have multiple topics in one hive
table let me talk about high for a
second you can have multiple topics in
one hive table or you can have multiple
hive tables looking at the same topic
and for each hive table you can have a
database table and it will work
accordingly so it depends on how we want
to logically organize your topics and
your data
so is it possible to directly feed
Oracle messages into Oracle in a basin
right so there are ways to do that there
are Kafka itself or I think some other
products you mentioned memc equal the
other products like confluent and so on
which have JDBC connectivity to the
database so you could just use JDBC to
say I want to connect Kafka streams to
Oracle sequel but I don't know how real
time that would be and I we have seen I
mean JDBC can tend to be like a
bottleneck so I will try but it's
possible but I we want to make this as
like as efficient as possible so we want
to have our own native mechanism to
query from database directly to Kafka so
that is on the roadmap the data is
stored in the Kafka side correct it's
only queried while this external table
from sicko yes that is another way to do
it correct the Oracle GoldenGate is
another way to do it that's something
I'm less familiar with but that is
definitely another way to read Kafka
data and push it into the database so
this is more high and sequel specific
where you're kind of not I think Golden
Gate pushes the data into into the
database so we are not here moving the
data actually in this case we're really
not moving the data right we're kind of
querying it live by this Oracle external
table so I think that's the difference
so it would depend on your use case okay
I think I'm going to move on to the next
part yeah
yeah
right yeah in in this case we are not
moving the data into the database we're
having a high point to the location of
the the IP address and the port and so
on and that hive table is accessed by
Big Data sequel so it's going through
the hive layer to pulled it or not
looking at files it's going through the
hive layer to read data that is coming
in from the Kafka C or the network yes
yeah so all of these services in the
cloud all these services would have to
be connected which is one of the reasons
I couldn't show this lie I could not get
a big data single cloud service
connected to all of these things okay
now I think I really should move on to
the next part of this presentation so
now I want to talk about moving data
between Hadoop and the database and this
is now this is not going to be so much
on streaming but more than you are
actually having data staged in Hadoop
and you have data staged in the database
and now we want to figure out how to
move data between the two so there are
two products I'm going to be focusing on
here one we call oracle order for hadoop
and then second copy to herd them so
moving data in both directions
i have data in Hadoop system I want to
move that into the database I have data
in database tables I want to move that
rather some moving data in both
directions so I wanted to kind of give
the context of this whole the challenges
in kind of doing this well and and the
thought that went into our designing
these products so parallel processing
Hadoop is an inherently parallel
processing environment it's if you want
to more data fast from Hadoop into the
database you want to be really you want
to take advantage of the fact that
Hadoop is parallel
so if Hadoop has 18 nodes and it can run
like 256 tasks that will more data you
want to be able to take advantage of the
toss that remote data so what happens on
the database side the database also has
partitioning but we look at it a little
differently one you have data
partitioning you can have a database
table that is partitioned
and the partition data can for example
the popular partitioning example is I
want to partition by month so the data
for January goes in one partition and
February goes in one partition and so on
so when I'm creating that database table
that is partitioned and I'm querying the
month of January
I don't read any other data blocks I
only read the January day partition data
blocks and that gives lots of efficiency
so that's one way to think of
parallelism and of course pattern query
execution as well Oracle queries can be
broken down into multiple tasks that are
querying reading data blocks
simultaneously and querying so what we
set out to do when we designed this
product is to see how we can kind of
leverage the parallelism on the Hadoop
side and the parallelism on the database
side so that we can have high speed data
moment between the two so that when we
come to Hadoop and database we begin to
really look at large volumes of data
that you want to move in a performant
manner between the two so we really paid
attention to how we can leverage
parallelism on both sides so first
moving from the Hadoop side into the
data base there are two ways of doing
this one is to run a Hadoop job and this
is where we're using the old jar files
that you're referring to it's not based
on SPARC it's actually based on
MapReduce because we find that that
still works well for loading large
volumes of data so you can run something
in the Hadoop cluster connect to a
database and push data in parallel so
what happens here is you're running a
Hadoop job that is pushing data into the
database the second option is to now run
something on the Oracle database side
and kind of pull data in so run a sequel
query and pulled it again just like you
would pull data again from an external
table actually if you have an Oracle
external table and you're pulling data
in you can do that using the second
option that this tool provides and again
this goes back to looking at which is
your tool of choice if you have you want
to push from Hadoop you have Hadoop
skills you have Java skills input from a
dupe you have sequel skills or even DB 8
- if you're on the DB 8
when you want to pull data you don't
have access to the Hadoop cluster then
you will run on the database and pull
data in from Hadoop and you can do stage
loading and so on yeah so it really it's
question of where the work is done the
question was which method is more
efficient in the first option more work
is being done on the Hadoop side so
Hadoop is going to process the data
convert data into an oracle binary
format and then push data actually also
pre partitions the data so you can
simultaneously in parallel push data
into multiple partitions or sequel if
you're running from or per sequel the
work is being done by Oracle database
are using CPU cycles in the database the
CPU cycle data if you're using Oracle
Exadata or something right and having a
degree of parallelism of 256 that's a
really powerful system it'll be faster
than pushing data from Hadoop but you're
using cycles in the database so we would
recommend that you use stuff on Hadoop
but it depends on how fast you want to
get the depends on a bunch of factors
how you design your system so Orca
loader for Hadoop can load data from
many different formats JSON log files
and we do a lot of work through hi so
you have the JSON flat files and you
want to load these files into database
what we would do is we put a hive table
on top of it and hive table can use the
JSON sanity to parse the data so we can
load individual columns into the
database you can of course load the JSON
itself into the database and use the
database JSON processing to process your
data or but more likely than not people
like to parse that on the Hadoop side
and then you can load all of these data
into tow and also we can load Park a
data sequence files compressed data all
of that we understand a whole range of
format and you Hadoop side that we can
then push into the database it can be
used both with sequel developer sequel
developer as a popular tool something
like toad that we have an integration
with sequel developer to load data from
hadoop into the database I have some
screenshots to show that we also have a
sea
a command-line interpreter to execute
commands for loading and I'll be demoing
some of the CLI here today yeah yes it
is a tool we have developed from the
ground up
so remote operating systems you support
on the cloud LSI oh sorry on the Hadoop
side on the database side any database
because we are just making a JDBC
connection or a direct path connection
so database platform doesn't matter for
this term that's a Big Data sequel
product correct this tool does not have
the restriction yeah the second option
like I said loading from the database we
have a sequel query pulling in data so
you have an external table that
something looks like this and it's
pulling in data this is available on
premise today it's planned for cloud
deployment and when we deploy this on
the cloud we are likely to go through
objects store because we have various
ways so I think what one implementation
would be that you move data to object
store and then you pull that into your
your database so that's to summarize
some key features here for this
particular tool it's a balanced load in
the sense that the key thing when you're
using parallelism is you want to make
sure that all nodes are doing equal work
otherwise you're losing the point of
parallelism will end up having one or
two dots doing most of the work you want
all the nodes to do work so we have some
inbuilt features to do this balancing so
that we can load at a very high speed or
documented performance benchmark is if
you have the network bandwidth for it 15
terabytes an hour more often than not
the bandwidth is the limiting factor and
it converts like I was saying earlier
converts to the Oracle binary format on
the padieu platform itself so you can
really save database CPU and we can load
into any kind of database table
in memory comfort stable security is is
challenging because the two security
models are very different the database
has his own security model Hadoop has
its own security model its Kerberos and
for example if you're using the CLI and
you want to access Hadoop that is
carburized it's a Kerberos cluster you
need the the OS user running your or
shall the the client the CLI to do the
movement has to have the Kerberos ticket
and all of that so it's kind of I would
say it's not non-trivial to set up we're
looking to make that more a smoother to
use but we are of course paying a lot of
attention to how we can do this securely
so security is a really important part
of how we design this Oracle this tool
this tool is free if you're subscribed
to the cloud any big data cloud service
in the cloud will have it on premises a
separate license I think it's much
faster yeah it's over three times or
four times faster and it supported by
Oracle so you should use well okay
technically technically it's three to
four times faster second we allow a lot
of partitioning schemes scope is limited
in the partitioning schemes it supports
we can load from any kind of partition
table and we really understand the
Oracle format so we're able to leverage
that and and real try any partitioning
thing is okay
the third advantage is offloading the
CPU processing we really do the
partitioning pre partitioning everything
on the date on the Hadoop side and then
push in the database scoop uses more
database cycles than then we do yeah
yes you should be able to do that I have
not had a customer do that but I think
that that really instinct I think that
should be possible I don't see any
reason why that would not be possible I
think that should be possible it is
equal running on and against an Oracle
external table yes you don't need that
no we have C don't need that so I'll
come to that we address the difference
between regular sequel and Oracle Big
Data sequel let me address that later
and I said if I don't remind me
so I'm going to stop briefly about
copying to Hadoop and then I'll go into
the demo so when you're copying to
Hadoop also there are again two methods
one similar idea you can use a Hadoop
job to kind of pull data from the
database into into your Hadoop cluster
and then creates a hive table on top of
it or you can use oracle sequel to kind
of export the data into Hadoop so in
actually exporting two data into Hadoop
there are multiple things that happens
behind the scenes your first exporting
the data into a local file system then
using Hadoop tools to move or the data
into your Hadoop system into a Hadoop
file system and then creating a hive
table but when you're using either the
sequel developer CL UI or the command
line CLI which I will be showing shortly
it's all done behind the scenes so you
don't have to worry about that's
different steps but it's useful to know
what happens behind the scenes so this
is option one which is basically pulling
data from the database in parallel so
it's exactly like oracle order for
hadoop it works in the reverse direction
and the what it writes on to the Hadoop
file system is either Oracle data pump
files Oracle data pump files are the
Oracle binary format the reason to leave
the data in Oracle binary format is if
you wanted to access it back from the
database if you wanted to leave it in
Hadoop and then query with Oracle Big
Data sequel or this other 'display 'none
in a sequel there are some advantages to
living data in Oracle data
format more likely than not you will
want we find people want to move it to
the 4k format because the reason they're
moving data out of the database is to do
some analysis in Hadoop so they want to
move the data into 4k or or C or some
other format that their Hadoop
applications understand so you have the
option of writing out data into any one
of these formats and I can use a sequel
develop button to CLI that I will be
showing shortly and in the option to
like I said we are using oracle sequel
to push data and then move those files
to hadoop using Hadoop tools but again
all of this happens behind the scenes
I'm this since the source client if a
tutorial session I wanted to explain how
this is done but when you're using the
tools all of this happens behind the
scenes
correct we could correct exactly right
you're exactly on the database it-it's
created a data pump files and the Hadoop
site is converted to porque files are
accessible to hive and that is the cord
that we have written
that's a Java code that we write to do
that conversion from data pump to porque
so summarizing the key features of copy
to Hadoop it's a fast secure way of
moving database or table partitions a
common use cases I want to move all data
in Y partition into Hadoop and I think
we already talked about the Oracle
binary format and interesting advantage
we notice with Oracle data pump mrs.
Parker or I should say difference Oracle
data pump files keep the data assets so
if you have floating point or some data
if you want the precision to be exact
you might want to keep the data assets
and data pump files and you can query
the data pump files in hive there is no
need to convert it into 4k we have we
provided 30 to be able to read data from
files in hive and it has the exact same
precision when you convert to 4k we lose
some of this precision because Oracle
number doesn't always translate exactly
into inter big into on the Hadoop side
so that's one thing to keep in mind for
data formats sorry can you speak up now
this will work with the Apache Hadoop
this works is everything
in the data pom format now then we
provide a 30 so that hive can query
those data pom files so you can create a
hive table on data from files so hive
can queried it up on file so on any
hadoop cluster that should work if the
data pound files are encrypted right I
can't remember whether we support
encryption with copy to Hadoop we don't
think so right we don't support I don't
think we support encryption so that's
something I will take us a desired
feature so in your case that will be
really sure you think that if you're not
if you're moving data from files without
encryption you think that might be a
problem let's talk after and then we can
look into that a little bit ok let's
let's talk after I would like to
understand use case better yeah
this is not real time this is not for
that you had to use something like
Golden Gate to replicate every
transaction in the database onto
something that's Golden Gate this is
more batch high speed processing third
thing I speed movement type things sorry
there's a question in the back yeah
sorry could you speak up I don't believe
we support that in the 30 so you can use
obviously you can copy the data right
you can copy the data with the
transparent data encryption all of that
but if you want to use it on the Hadoop
side you need to have high or Impala or
SPARC understand the data and I think
that's the piece that is missing I don't
think we have that today we I don't
think we have I don't think that hives
30 can read encrypted data I know we
cannot read compressed data for sure and
that's one on a road map
I shouldn't let's talk about encryption
and what you use cases for hive to read
it because you want to move it when you
so I think our idea also was there after
you move the data you're using it in
hive it's a different world I mean we
assume you're not it's that you're using
different kinds of encryption you're not
using Oracle's encryption but let's talk
about a use case what a question here
somewhere ok I'm going to move on to the
demo I think we're kind of covered a lot
of these topics I'm just going to move
on to the the CLI so what we have here
to help move this data are we have the
sequel developer UI and we also have
this what we call Oracle shell for
Hadoop loaders which is a command line
interface produce recall sorry for
enhances a shell and it connects to
Hadoop hive Oracle database in the batch
shell so we had when our first early
versions of this products when you when
we said oh you're going to use option 1
we would say you have to run a Hadoop
job so you say Hadoop jar and here's
your class in the air pushing data
you're pointing to data files if you
using option 2 we would say ok you have
to go to the oracle external table and
do this this and this but these are kind
of different technologies and we found
that this is really not user friendly at
all and it
it was just complex and laborious so we
said okay we're going to define the
shell that can connect to different
systems and give you uniform syntax for
data moment so I have the same syntax of
whether I'm moving from the database say
Hadoop to the database or database to
Hadoop I'm going to be using the same
syntax I get set my parallelism on
either side in the same way because
parallelism and Hadoop is different from
parallelism on the database but I can
set it the same way I can set my date
formats and time formats the same way so
we giving your uniform CLI so you can
set your formats the same way so instead
of logging into your Hadoop system or
logging into your database system and
setting your parameters accordingly with
it basically also major pain so that is
a really cool I think interpreter it can
run anywhere and it can run on any node
that has access to these things it has
to have access to it has to have access
to a hive client because it's connecting
to hive and doing some processing there
or when it's moving data it needs to
connect to hive it has to have a dupe
client it has to have the ability to
have a JDBC connection to the database
and then it has to have a sequel client
to talk to the database so it has to how
all of this client good this is really
functioning as a client for both Hadoop
and the database so to have all of these
things in one place that commonplace
with people we find people use this as a
like and a Hadoop client because I had a
client or a Hadoop edge node already has
these clients installed the one missing
thing would be the sequel client which
you can install you can the Oracle
database secure client is very easy to
install so you can any node that has all
of these clients you can use to query
you can use to use this tool the ocean
tool and then you can kind of move data
between these two systems so what I'm
going to show in the demo here is I'm
going to show what we do so when we have
the sequel client and we want to connect
to the database we want to be able to
create what we call a sequel resource so
or she'll has the notion of a resource
that you can be successful
we're providing here is you you're
telling it the the connection string
right we're saying this is a connection
string that you need to use to connect
to the database if you want to run
sequel queries on the database this is
the sequel resource that you use
similarly here's the JDBC resource that
you use so these tools that I was
talking about Oracle loader for Hadoop
and copy to head dub require a JDBC
connection to the database so we're
saying here's how you create a JDBC
resource and you can see the syntax is
very similar it creates sequel +
resource or create JDBC resource it's
really nice - easy to use and then we
have the hive resource so we have this
concept of resources you have either a
database resource to connect to the
database or you have a hive resource to
connect to hive and these are the
resources you're going to be using to
point to data when you move you're going
to say okay here's the hive table I want
to move to here and and so on and when
you start up or shell you have some
basic defaults resources that are
available like hives zero connects to
the default hive database when hive you
can have multiple databases the basic
database is default and then you can
create your own databases within hive
and you can then create additional hive
resources to connect to those database
so you get the default resources when
you start up a shell so I think with
that I'm going to move to the demo and
yeah it is it can be downloaded from OTN
again the licensing it works very much
with the data load products so like I
said if you're using the Oracle cloud
service this client will be available
with it if you are using on-prem then
when you license the loader tool this
will be available with that so it goes
to the licensed product it's not
licensed separately but but the
licensing is different on from on the
cloud so it goes with that it's called
aura it's called Oracle shell for Hadoop
loaders we just call it ocean for short
look for OHS H and towards the end I
have a nice there's a nice blog post
written by one of our architects
it goes in the great land and that
should have also more information on
things like that yeah okay so on prim
licensed oracle big data connectors yeah
that's on prime on the cloud it's
available with the cloud services your
license
it's either available now or will be
available depending on which cloud
service but it will be available for
free with the cloud big data cloud
service your license so let me go in
here and so I have here the set
resources file that I'm going to be
using to set some of these resources and
I'm going to set the number of reduce
tasks which is the parallelism on the
hadoop side and that's the syntax for
doing that and I'm going to be setting
up some directories which you need if
you're using Oracle sequel and then I'm
going to be setting up the reach the
sequel resources that I was talking
about the sequel resource the JDBC
resource and so on so let me now start
up push off and let me run this and now
it's asking because when it when it's
creating the sequel resource it's asking
me for the username password for the
database schema that I'm connecting to
and similarly for the JDBC schema that
I'm connecting to so now I have an auto
shell client ready to go with all of
these resources created so if you want
to look at this hi movie demo reso that
I can do for example this is the hi
movie demo is a resource I I created to
connect to the movie demo database in
hire and if I'm going to query it from a
shell the syntax is just to add a
percentage to the resource name we just
tells it to interpret this as a resource
and then I decided
a person days and then I'm just going to
say show tables so we can look at the
tables that we have in hive from the
social client so here are the two tables
I have that I'm going to be used for
loading in this demo and I'm able to
look at them from the from the ocean
client now if I want to now look at the
database that I'm going to be loading
into and this database I can now use
this the sequel plus resource that I
have sequel result and now I'm going to
say select T name from tab which is how
I can look at the tables that this is
the sequel syntax that runs the database
so what this client is doing is it's
taking the sequel statement and just
passing it to the database earlier on it
took the it just took the show table
statement and this passed it to the hive
server so here now now from the same
Rochelle client I am connecting to the
database encoding the database I'm
connecting to hi and I'm connecting to
hi this is what we have done to make it
simpler to kind of work with these two
systems at the same time how one client
that accesses all of these both of these
both these systems so now that we've
seen how and if you wanted to do bash
you can just do let's say I can run a
bash command from my or shell point
that's this convenience right like many
clients have that so now if I want to
load data I'm going to copy this I'm
going to use the Oracle loader for
Hadoop tool to move data from so what
the syntax is doing here is it's saying
lord oracle table movie ratings from
hive table movie ratings I'm moving a
hive table movie ratings to an Oracle
table movie ratings and I'm just going
to hit return here and this is going to
go ahead and start loading data so while
this is loading it we're going to load
okay when it's loading let me also talk
about this so we noticed this yesterday
and with that well we let's talk about
it because there's kind of an
interesting feature so I said earlier
that
the loader balances the load across the
different tasks we have on the Hadoop
side and he went remember that in when I
was setting up the ocean resources I set
up for tasks but I don't have the data
for for tasks but this is synthetic data
I just generated it and I did generate
it so that is evenly spread across the
partitions in the table so they're all
going into one partition so I'm getting
this nice warning message from the ocean
client saying oh three of your tasks are
not doing any work though you have set
of four tasks three of your tasks are
not doing any work and only the fourth a
thing is doing one so now this Lord has
completed I can either go to the
database cloud service and now check the
Lord so now I'm on the database cloud
service so I see that thirty-nine
thousand rows are being loaded here or
actually I could have done this from a
shell itself I could have said this so
this is accessing the movie ratings
table in the database oops what did it
like oh I get the wrong stream in
percentage
so I'm now querying the database table
from from the ocean client I could also
like I was doing earlier check the hive
table now this is the same table same
name but on the hive side so I'm
checking the number of rows on the high
side and taking the number of rows on
the on the database side just to make
sure my data loaded correctly and so on
so this we find is a very interesting
feature the ability to do all of this
right in in one in one client so it
helps in Diagnostics it helps in
actually running commands is easy to do
it air helps in scripting because it's
all done with the same client so you can
see here that on the hive side so when I
mean I'm using the hive resource I'm
checking the table and the hive side I
was using the sequel resource to check
the table and on the database slide and
you can see the same number of rows are
loaded
so if I wanted to now just use option
two so this is using option one I'm
making a direct connection from Hadoop
to the database and pushing data if I
wanted to do option two for example I
said using direct path I would use using
X tab so that's all everything else
remains the same and I'm just saying
this is a different option that you're
using now I don't have the setup so I'm
not going to run this but I just wanted
to show syntactically how simple it is
to just use choose different options and
if you're used to products before you
realize that this was not this simple at
all but client is really designed to
make specifying these tasks easier so so
let's me go back to the presentation
here so we just saw that the first we
just ran the first command which is
loading from JDBC into hit Ohio using
direct path and like I say we just had
to change it to X tab to the exact same
Lord
but using Oracle sequel but I know we're
saying earlier that oh you you might
want to use option 2 if you have Oracle
sequel skills and want to pull data from
the database and you can still do that
those if those API survey labelled too
you can go into the database and pull
the data but if you want to reduce the
ocean CLI this is one way to do it
similarly now you want to copy data from
the database to head up let me actually
go and and start off that job as well so
that's the how do I know now that I have
loaded the data into movie ratings I'm
going to copy that back into hive
so this is now the syntax for copying
data from the database into Hadoop so
I'm bringing the exact same data so I'm
looking at the database looking at the
movie ratings table and moving that back
into hive but I'm just giving it a
different name I'm calling it movie
ratings dB so the data that I just
loaded I'm going to be moving it I'm
moving it back I make I'm copying it
back basically so this is doing the
reverse now it's making parallel
connections to the database and then
loading that data and then creating a
hive table on on top of the data so
while this is loading let's go back
what's that a question sorry in this
case I'm doing a poll so I'm making
connections to the database and then
reading the data by partition yeah the
tool exactly the tool has just has to be
located on any any client and it doesn't
matter whether whichever system is
pulling at pushing it doesn't matter
when you are using the client if you
Debbie allow set by all means we find
DBS don't like you sometimes but yes you
can put it on your database server you
just need your hive and other clients
right that's the thing you need which is
often not there on the database side
correct and I even had the client yes
you ever need that now all the all the
data types supported by the products
will be supported yes yes okay so we
don't support complex data types we
don't support objects we don't support
user defined data types but all the
simple data types are supported clobb is
supported longer supported to long isn't
deprecated anything supported yeah long
supported yeah
it's separately it comes license with
the connectors so the versioning is
different from the database versioning
you can use it as leverage II you can
use it 11g database yes so it's a
license by the yes on Prem when you're
licensed Oracle Big Data connectors
which is what will include this product
it is licensed by course on the Hadoop
side yeah Oracle Big Data connectors
it's licensed by course and they had
upside on cloud is included with your
cloud licensing well yes on Prem on the
other side so okay actually I should go
back I think the Lord probably has
finished by now right here okay so you
can see here that I was able to copy
data so I forgot to go back and look at
I'm going to look at the new hive table
that I created it should have the same
thirty-nine thousand seven hundred and
sixteen rows that were just copied over
from the database so you can see here
the syntax is meant to be intuitive and
and like with this discussing it can
this run on the client so well that is
running let me go back to the
presentation and here you can see the
once again you can use direct copy or
you can you staged the two different
options very finish the demo so it's a
couple of notes on performance so I was
talking earlier about parallelism on the
Hadoop side parallelism on the database
side so set reduced asks is how you set
the parallelism on the Hadoop side said
do P is how you said the do P on the
database side so again you do that all
within uniform uniform CLI and your load
performance is really only bounded by
network bandwidth so when we measured or
a 40 gig InfiniBand connection
connecting Hadoop to Oracle Exadata we
were able to get a load speed of 15
terabytes an hour but you'll find that
your network is what is bounding this
and the nice thing you can control if
you're using accelerator let
say you're loading into accelerator
that's your target you can go up to like
a deal of P of 256 you on either side
and it will work fine and that's what
you were set here yeah
a lot in the park a file yes it is
possible I didn't yeah so we don't have
this I don't have the specific syntax we
don't yet have the specific intent but
that is we are planning that for the
next release too directly going to park
a file with the CLI we can do that with
sequel develop for today with sequel
developer UI we can just go to 4k files
today with the CLI we just haven't added
that syntax to the language so you can
manually do it and it actually I'll let
me take that back you can do that today
it will just be you just need to specify
to ocean commands you do the first ocean
command to copy the data and the second
ocean command to write the hive query to
convert it into porque so the short
answer to your question is yes the
direct syntax to do that in one step
it's coming but yeah if you want to more
data base later and a pocket today there
are like any number of ways to do it
with this tool straight and those pesky
date and time fields which are kind of a
really trip up a lot of our customers
date format is the date format on the
database side the database is looking to
interpret the data date mask is on they
on the Java side or the Hadoop side and
it gives you a more interactive way to
play around with this also you set the
date format you try something listen
works at it again try something it's
it's kind of convenient to see how that
works so we have this whole series of
blogs very recently written by our
architect who designed the OSHA alciel I
I should have put up that URL which is
more visible so I don't know whether you
can see it but if you google they get a
connectors blog you'll find it and
there's a whole series Bob is a great
writer he makes it fun interesting
humorous you can just go through the
whole set of block series and this is
sequel developer I think some of you or
asking for sequel developer so in sequel
developer you can see here I've made a
hive connection on the left here I have
a hive connection and then when I click
on the stables icon here I can see copy
to Hadoop listed as an option so I can
then select copy to Hadoop
and here I can select the destinations I
can say do I want my data formatted with
Park a data pump or C or I can so I'm
selecting the the database source - I'm
saying I'm connecting to the Oracle
database movie demo schema and this is
the data table that I'm reading from and
this is the final format that I wanted
to be in so you can do this all through
a sequel developer UI very conveniently
and the CLI is catching up with that
particular syntax yeah it's in sequel
developer 4.2 and there are a couple of
things you need to set up so if you look
at single developer documentation there
should be a way to say set up this
things and then you'll see it you just
re-enable Hadoop connectors then you
will see it in four hour - yeah
you can lose this on your local laptop
so scheduling is something that is on a
road map how to say so today you can use
you can use suppose to see a line you
can script things so that you want to
say I want to load every hour or every
half an hour or something with a sequal
developer today on the thing it's it
this is really meant to be more manual
but again let's talk offline I want to
understand your scheduling you squeeze
right right so you don't have to use a
tool and you can use the command-line
interpreter as well I'm just giving this
as one option of how this can be done no
we don't have an integration with or
Enterprise Manager now I see oh I see
where you're going with it known right
now we don't have an enterprise license
yet about the command-line interpreter
yes if you want a schedule that's how we
do it right okay I see now your question
yes you would go with the he would go
with the command line for schedule a
correct correct correct so for
scheduling large jobs continuously you
would go with the command line CLI Oh
she'll CLI so the last section is query
in place we kind of refer to a couple of
examples but let's look at that and we
might just have time for a quick demo on
that
so Oracle data source full of what you
had the begin is licensed with the
Oracle Big Data connectors it is a way
to run a query on hive that talks to the
database and so earlier also we're
having high tree database data but I was
actually moving the data right we are
moving the data from the database to
Hadoop and then creating creating a hive
table on it and querying it what this
allows you to do is leave the data in
place in the database and query it from
hive so it's really useful if you have a
small amount of data I just have my
customer table I just want to get the
customer IDs that I want to link with
something here I'm not moving a whole
bunch of data I'm not doing a scheduling
job I just need to quickly look at some
data and get it then you can just use
this product which will say oh I can so
we have a hive storage handler for
Oracle database tables so you can run a
query in hive that is pointing to a
database table so it generates again it
generates data base splits and it
translates the hive sequel into Oracle
sequel so that you can then read you can
do some kind of predicate push down into
the database and so the database does
some of the execution and sends the
resulting matching rows too high so you
can actually run a query in hive and
query data in the database without
moving the data so all this while I was
talking about moving data it is all
about how you can move data fast
large amounts of data fast this is now
I'm just looking if you don't want to
move the data typically if you just had
a small set data set here's how you
would just do it with the Oracle data
source for Apache Hadoop and here's just
a syntax that that if you want to create
an external table that you that this is
a hive external table now
this is a hive external table using the
storage handler that is specially
designed to query tables in the Oracle
database and finally Oracle Big Data
sequel there are lots of sessions here
and Oracle Big Data sequel which is why
I kind of I'm not spending a lot of time
on this but I just wanted to include it
for completeness sake so what Oracle Big
Data sequel does is it you can see here
on the left it allows you to query the
Oracle database that we know and Hadoop
you can see here and also kafka which we
saw earlier and no sequel database which
we didn't see in this session but there
are ways of seeing I'm sure the other
sessions has covered this so Oracle Big
Data sequel allows you to be in the
Oracle database world and query any of
these different data sources by creating
external tables that point to these
different data sources so so that's what
our cryptic data sequel is it's really
powerful technology and what it also
is able to do is able to evaluate
predicates in Hadoop so there are
actually small servers running in the
Hadoop nodes so you can push down your
predicate in your sequel query so that's
executed on the Hadoop nodes and only
relevant data is passed back to Oracle
Big Data sequel so this is once again if
you don't want to move data I just want
to query data in place you can use
Oracle Big Data sequel to do this yeah
it's just a next I'll show the same
thing it's just an external table where
you are specifying you are pointing to
the hive table the syntax has to point
to hive table that's what they mean yeah
but as any other every other sequel
syntax is the same you just have to pass
the parameters is the hive table
location the Hadoop cluster name and
things like that so right here actually
it's a syntax so this is what we mean by
enable table so you have an external
table and it says oracle hive so your
that means you're going to be using the
oracle hive driver that is available in
the external table to point to a table
in hive that's what it mean they'll also
have an oracle HDFS driver as well to
query files that are in hadoop if you
don't want to go through hive it's just
that if either you're accessing a hive
table or accessing HDFS files no it you
just might have some data and files
right like when we thought the IOT cloud
service we can just you might just
choose to leave the JSON files as files
and this quite the file sir actually I
think it's just I don't think any
particular advantage or difference I
would actually recommend everyone create
a hive table it makes I think it makes
life easier you just have a hive table
hive takes care of a lot of things but
if you choose not to create a hive table
you can access files directly with this
external table mechanism and there are
lots of stuff on the demo ground as well
so so this is we already saw that this
table definition querying the earlier
traffic data where she came from the
kafka streams we were able to query the
traffic data by pointing to the hive
table that was pointing too high so I'm
going to skip over that and look at this
hype table which is this are this big
data sequel table which is going to
query the hive table movie ratings that
was in hive so I was when I was using
the loader tool I was actually loading
the movie ratings table into Oracle
database but I think with big data
sequel if I wanted to query the movie
ratings higher table without moving the
data I could do that I could just point
my Oracle external table to the movie
ratings table and there's query that
table and I will have access to the data
correct yeah hard yes
yeah yeah yes yes you can't create
indexes because the data is external the
data has to be internal to the database
to create to this but even though it
doesn't have indexes it has like I said
you can do predicates pushed also what
it does is it will take your predicate
in your Oracle sequel query here and
actually executed in Hadoop nodes and
Hadoop is a distributed system it's a
parallel system so it's going to execute
that predicate on every node to locally
access data on that node so the
filtering will happen at a per node
level so you get good performance by
that mechanism oh okay that is a danger
of the live Devorah I don't have first
of all and I don't know where this cloud
instance is somewhere that's one problem
the second problem is I don't I I have
not put all the tuning in place right so
and there's a constant query Quentin
alloys are not the best ones to do it I
should have done a more meaningful query
which would have been faster but for
next session I will keep that in mind
and do it in a more meaningful way but
now this can be this can be very very
fast so they can they have done lots of
tests there are lots of benchmarks
available and Big Data sequel to show
how fast they are they are very close to
Oracle native queries they are not same
as native queries because it is data is
outside remember that but they're very
close to Oracle native queries yeah
so the execution the question is whether
the execution is in the big data side or
the hive side so the predicates are
evaluated on the hive side so when you
say select star from table where user
equals Mele that where users equals Mele
that predicate is going to be executed
on the hive side so if you a question on
the hind side whether you're using yeah
no no the hive can be this regular hive
it can have any database the hive at the
meta store can be whatever it is there
will be my sequel water we don't need
you don't need an Oracle database on the
Hadoop side but the predicate is
executed by cord that is running on
Hadoop nodes and that's what you get
when you install the Big Data sequel
product it will install the code
necessary to run the predicate
evaluation called on the Hadoop nodes so
can you can you speak up I'm not able to
hear or maybe this mic works can you
actually come and speak into this mic
thank you
Oh opacity no I don't have I don't have
performance yeah I would increase in you
just I would encourage you to stop by
the Big Data overview demo pod which is
in Moscone West in this building
downstairs they have lots of experts and
big did a sequel who can go into more
details on this and compare them with
different products I don't have that
information with me so so okay we talked
a lot about what's in the roadmap right
diagnostic scheduling as you were
talking about and more automation more
integration so I think that kind of
brings me we're very close to the end
but they wanted us take a show a quick
Big Data sequel query on the VM but
maybe those who are interested can come
by and we can take a look at I think
you're really interested in the sequel
so let's just open it up to general Q&amp;amp;A
now yeah yeah well yeah let's just open
up to general Q&amp;amp;A and we can look at Big
Data sequel yeah okay
right
in my experience Hadoop I won't say
Hadoop performs better than the database
I would say the different kind of use
case if you have like 20 terabytes of
just regular data relation structured
data I think your database is really
probably your best option for querying
the data yeah it should still it social
work fine I mean I think it's so if
you're looking at equal and hardware ID
you're looking at an eight-core system
on the database side eight core system
with a Hadoop side I think it had a
database with all ways you could write
write write write write write you write
there's not much licensing cost so this
at least lesser licensing cost but then
you will have the cost of you need
someone to maintain that system and
manage the system and so on so that will
be a I mean there will be a cost there I
think it won't be right so the other way
to look at it also is right whether
you're going to be doing what kind of
queries are you going to be running if
you're going to be running point queries
when you say select star from where user
equals malloc
I would say that the database is here
better option the least that that would
be my feeling from looking at all the
stuff that people have been doing with
what these systems and you could give it
an 8 core node and it and the cloud is
not that expensive
as we saw from Larry yesterday right
right right so there was this great
desire right to said oh yeah Hadoop is
cheaper and Hadoop will do listen but I
think the database is just way more
mature so this is really hard so I see
them as complementary so I would say
true to me the breakpoint is really the
nature of the data I have a lot of JSON
files I have a lot of unstructured data
then I need Hadoop to work with my
database I have a lot of strings
relational data I'm just going to use
the database and look at other ways to
save costs maybe using the cloud that's
the way I would look at it other
questions
oh you mean for the copying data to
create the data pump files yes you
internally you're creating an external
table to do that but you don't have to
if you are using the CLI that will take
care of everything
yeah no no you cannot you cannot I
should have mentioned that you cannot
use the data form files cannot be
exported using the export import tool
you have to use our tool to create the
data poem files because there's a slight
difference in the data pump files okay
I'm getting the time we're out of time
signal so I'll take this last question
and then we'll stop I'll upload to the
session wherever with you even download
from open world it sites yeah so I
typically upload internally okay I'm
going to wind up I typically upload
internally to a place where I'm as a
speaker I'm supposed to upload I think
you should be accessible to you if you
log in to that later so why don't I uh I
I don't upload this to the blog I'll
upload this to our blog this Google big
data connectors blog or if you want to
give me a business card sign value I'll
mail you a copy I am happy to take your
business cards and mail you a copy of
the presentation so okay thanks everyone
and we'll be around or outside for more
questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>