<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Turning Relational Database Tables into Spark Datasources | Coder Coacher - Coaching Coders</title><meta content="Turning Relational Database Tables into Spark Datasources - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Turning Relational Database Tables into Spark Datasources</b></h2><h5 class="post__date">2017-12-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/C_HKg-h0uHI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Oh
good morning good afternoon
welcome to Oracle code online
my name is Kauai Samantha I will be
presenting turning relational database
tables into spark data sources
our disclaimer I might be talking about
some features not yet available are
planned for the future so this is the
Oracle standard screen okay my bio I do
product management at Oracle
predominantly on gather around the
database um we also work on integration
with Big Data frameworks Hadoop spa link
on JavaScript with the database using
national either from client GDK JRE or
the g vm embedded inside the database
itself i graduate from the programming
Institute of Paris six inches here so
French accent and I think it differently
than it addressed throughout the book so
you can lead you follow me on Twitter
Linkedin and also a blog okay here is
the agenda briefly talk about the
requirements why are we doing this then
an overview of apache spark then how
would you turn a relational table into a
spark data source what is provided out
of the box from apache spark and why are
our DBMS vendors or DBMS vendors coming
up with their own um data source you
know which which kind of optimization
are they providing in addition to the
standard Apache getting this interface
and then I'll show you some demo of our
ongoing implementation
okay so why are we doing this why would
you consider that well the problem is
you have your marketing person or sales
person telling you asking you to give
them a query which will tell them which
of their products got everything of four
stars or higher on social media in the
last quarter so to answer this question
you need master data which is
predominantly in our DDMS this is where
you store your product your customers
those are
cross
education
Oh enterprise-wide
the data that you can use across several
applications so those are usually stored
in our DNA
then when you are waiting on social
media big data normally usually store in
HDFS no sequel
viviana scene of some other storages so
you need to combine data from both sides
in order to answer that specific
question okay
a party spark a brief overview of apache
spark so the concept the main concept of
apache spark is it processes data in
memory this is different from Hadoop
which processes they got from history
and the other key notion is the RTD it's
evident and distributed data or whatever
it's a fault current abstraction for
in-memory data sharing you know the the
problem with Hadoop was you will not
allow sharing data across tasks here we
can share the RDP across task because
it's in memory
but it is immutable you cannot change or
update an RDD you can transform an RDD
into another our video on when you do
several transformation you end up with a
graph on line it on this graph can be
reconstructed if there is a failure we
can look at the different transformation
from your slack program or spark
application and we can redo the same
transformation remember there is no data
change so you will get exactly the same
data or the same value the other notion
is data friend beta frame conceptually
equivalent to the notion of relational
table consequently we can use query you
can use spark sequel queries or the data
frames as you will do with relational
table
then there is the data source API which
is sort of a low-level API which
provides support for different type of
data formal education Avro pocket and
relational
travels through JDBC interface okay then
you have sparks equal you know which is
the query very framework
which operates on top of the data frame
and here is the Apache spark
architecture what you see here is
become you have the different storages
you have a DDMS for the JDBC interface
you have the Jason you have the MySQL at
the HDFS so those come out of the box
you might define your own extension to
the data source API if you have a
different data format which is not
supported by this out-of-the-box
interface okay on top of the data source
are you have
from API forget this Paco you have the
data frame API
so the beta friend API will give you a
pseudo relational sort of presentation
of the data and you can use the query
dispatch engine to query that beta
friendly interface so at the top you
have different languages even those
spark itself is written in scala it
allows you to write programs in Scala in
Java in title in Oh
and you can see that those can interfere
or interface with spark Seco or spark
streaming
or machine learning library or grass
extension etcetera feature so this is an
overview of the SPARC architecture
some data points on spark okay so
because spa operates on in memory it
runs faster than Hadoop
okay I stir in memory on 10 times faster
on disk in case all the data are not
loading or cannot be loaded in memory as
sparks also can access reduce but even
in that case it can operate 10 times
faster than I do are in terms of speed
you know it can serve hundred terabytes
of data free time fashion and hadoop
mapreduce using only one tenth of the
hardware
flagship
the largest known spark cluster has
8,000 notes
probably more as we speak but that one
data point and thousands of
organizations are using sparking
production so it is well adopted it is
widespread its overtaking adult because
hadoop mapreduce cannot address the new
processing requirement okay but before
we go into that how Apache spark works
the first step is you need to create a
dataset from your data source then you
apply parallel operations to it okay so
that's the how it works now all your
operation all work everything you do
Express as either transformation meaning
create a new oddity or create new RDD
from an existing oddity or action so you
can call operations on the oddity
so all those transformation create what
is called a directed acyclic graph it
means it only goes into one direction it
does not look back goes on leading to
one two actions like it trees you start
from the root and you go into the
branches of the tree
so every spark program
well absol you get back on text which is
the main entry point in our Katie I then
you need to create an RDD initial RDD
from external data and then transform
the existing RDD just created into new
oddities by using operations or
transformation like filter we will see
more transformations to do the
presentation if you want to you can ask
expect to persist rdd's you know in case
you want to reuse them at the present
remember those are in memory construct
so you can persist them anytime you want
then on the RTD you can launch action
such as count first etc etc and that's
where you click of parallel computation
okay so now how a spark application
works
and this is an example here we have
think I borrowed this example from from
someone from the internet and this
example is in Python you can see here
that we are creating lines are busy or
Hadoop RDD from the storage from the
HDFS so this is the HDFS and we are
creating the Hadoop poverty which is
here from text file then from the Hadoop
RDD we can create filtered LD be called
errors here so errors are light which is
the initial oddity Hadoop oddity and
when we apply the filter start with
errors or any blind
starting with error will produce what we
call in the field field LGD okay and
then we can apply another operation like
map on that
we want to map
fear of population so we want messages
okay so the separation within those
lines will get attacked and then that
defines recipes so those are not
oddities or messages and then you can
decide to persist the messages so that's
very basic or way how Spock works and
here is how the whole thing works so
notice that here there is no node data
access you know you the SPARC driver
will analyze your application your
application code okay it will figure out
the oddities that you want to create
initial oddities and then some
transformation from there it will find
the graph with direct acyclic graph
remember this is the line it of all the
transformations that you have performed
that you want to perform in your own in
your application so from this graph the
dag scheduler which is the scheduler
which from the graph construct and the
tasks you know how the tasks will be
scheduled and you see here this is the
scheduler so this is the tas schedule
the task schedule will now be dispatch
to work kernels okay food the cluster
manager big cluster manager or task
scheduler within the cluster manager
will dispatch those tasks to different
workers so worker can basically be one
or two guys with interested so this is
actually where data actions will happen
okay so first we X align I'm in the
spark framework with X I mean
the application
in the crown the transformation the
dykes Kabila will turn them into tasks
giving the task setting will be
dispatched and the task will run will
operate ok
sparks training okay so spark
the difference between spark and Hadoop
is that spark the car engine is MicroBot
is based on micro-budget okay so it's
not a full streaming engine it's based
on micro budget now the trend in Big
Data is to process data as it enters the
system you know the businesses are
moving from reactive where you analyze
after the fact to proactive interaction
where you want to analyze when things
are happening for example you want to
find out if somebody is a terrorist
when they are boarding the plane you
know before the flight takeoff you want
to find out if somebody is a dangerous
person
oh boy I'm sorry to take that negative
example but it gives you an idea of the
sensitivity that you want to find out
before you know it's too late to react a
proactive interaction so you need to
process data as it enters the system
okay this is more important you know
Donny Noah
analyzing the fact that after so that's
why we need new processing framework
versus making it such as spark think the
try box back at core is back first then
they added box training which is a
straining extension that you need to use
on top of the spam correspond on what
the sparks training framework does it
takes trimming data turns them into
micro batches that can be consumed by
the spark energy
okay this is a quiz time so if you need
to remember one thing from streaming
data this is if you know those are the
concept you need to remember from
streaming processing okay so the notion
of low latency first of all the notion
of window okay so to analyze streaming
data you analyze a fragment or a window
of the streaming data you know you
cannot analyze data so streaming data
when the flow continues to come you know
the explaining out of water faucet so it
so you need to slice I take a fragment
and analyze the file
okay so key notions low latency meaning
sub second windowing different type of
window you can have six window sliding
window session window which is more
specific to the session so the side is
not well-defined
timestamp different times you know
different time you know when they say
even time is entirely different from the
injection timing mediation times the
time where this event in term system
which is different from the processing
time which is the time where even has
been processed by the system and the
example I like to take a Star Wars movie
where the sequence of the movies are
different from the event in the history
certain you good
there is a notion of watermark which is
when we consider a window that you've
been arriving consider that done so then
it can be garbage collected at the
present tracks processing on it some
frameworks support in order processing
that obvious but some other can support
out of order processing which is hard
which is punctuation its segmentation of
the string into couples we can have some
operators to control three girls went to
me to write the output of the
computation it could be based on
watermark it could be based on given
time could be based on processing time a
teacher then you have the notion of
accumulation disjoint overlapping result
observed for the same window so you
analyze the same window different timer
you can have overlapping results
deliver delivery guarantee there is an
ocean there you have at least once
exactly one or in to end exactly once
that means all the component because
when you processing you have different
components and end-to-end exactly ones
mean all the pieces all the components
will
Ashi we guarantee that at the end there
will be exactly one result on their
prioritization back pressure is the way
to regulate the flow when it overwhelmed
the processing mechanism so you probably
want to ask the provider
the producer to slow down a little bit
all you have the mechanism to prefer
event while in processing the current
even you can back you can buffer
incoming events so you can get back
those events when you're done with the
one you're processing so all those are
things you can find are not fine in
different frameworks you know stream
processing frameworks for example you
look at flink it's different from spa
and thinks about some stuff you
know so there are differences so
depending on what exactly we're looking
for bills are the concept you want to
you want to watch okay now to the core
of the session of the presentation
turning on relational database table
into spark data source this is the spark
seeker library you know it's a it's a
basin I mean based it it provides
several API and services so the data
source API you can see here on the right
this is the actor low-level at a lower
level we have the data source API and we
have the JDBC interface as part of the
data source appear we also have other
interfaces I'm just showing JDBC here
because we're talking about turning
relational database table into Hadoop or
a spark pages of sorry so you have the
basis then you have the data trend API
remember we operate on data's and a
different produces it's sort of tabular
data it
organized into nine columns similar to
relational table and then you can apply
sickle operation on top of the day
difference so you have this equal by h
qm here but you can also have
domain-specific query mechanism
completely out of the scope of this
session this foxy could also provide a
sickle service which is based on height
but this is not the topic of this
session okay so the SPARC framework
out-of-the-box furnishes a JDBC
interface
so why our vendor providing their own
specific connectors or data sources well
because the spark JDBC interface
supports predicate push them and basic
partition it does not support other
optimization or other way to access a
database in a parallel way and in a
efficient way for example I know what to
pull all data from all partitions
if the where Clause does not involve all
partition that's called partition
pruning for example so that's why you
will see different database vendor not
just a behemoth but also the new sequel
database vendor they furnish their own
specific connector right so this is just
to give you an idea how you create a
frame you know so a data frame if it
didn't be CR they need plus schema
the RVD does not have a schema metadata
so the beta friend is more complete
because with husband era data of the
skin and this is how you would create a
JDBC beta frame I mean I name it didn't
is data frame it it's got it just the
data from so you made it however you
want but you can see here we get the
secret context and then we'll really
with JDBC Pharma then you have to
provide a lot of the name of the table
Exeter Exeter
it's more than that I'm just giving you
a very summary how you will create
basically you can see here this is using
the fluent notation different
programming style
this is the equivalent in Java okay um
you can see here this is how so
on table we could come up with some JDBC
RVD over here isn't it the daily decent
action
okay now I'm gonna spend some time
explaining what are the optimization but
different vendors are providing
so
performance scalability and security
optimization
look it so
the optimizations are in the areas of
providing custom partitionless or
splitters will explain what this means
and remember here I'm very cautious in
using potential optimization because
what I'm talking to you about is not yet
released it's not a product yet so
everything I say I happen or might not
happen so I need to be crucial as well
as in putin's reputation okay so custom
partition our speakers partition tuning
this is the way if a query
be satisfied by only accessing one
partition even though the query crime
generates several partition we will only
access a specific package that's called
partition pruning it's well known in the
relational database and other database
console faster dvb-c type conversion I
will explain one business some collision
properties you know you can provide like
fed size to do arrow fetching for
example when connection caching I will
explain what it is then also support
some strong authentication
authentication encryption and integrity
those are security organization
so partition
this is the mechanism
controls tunnel access to the our DBMS
table so the whole idea of I do for
spark of lingo all those big data
framework is parallel axis okay is one
path can accomplish something in 60
minutes if I put 60 tasks
we'll accomplish the same amount of work
in one minute and if I put 3300 tasks
they will accomplish the same work in
one second I mean that at eye level the
principal has way parallel access is
very important okay so practitioner as
the mechanism which controls how the
parallelism will work okay so the same
table is logically partition it's it's
not we're not talking here but the but
the table partition doesn't find in
relational database you know a
relational database table can be
partitioned but that's not what we're
talking about here we are talking about
the extract mechanism called the
partition er which allows you to have
parallel access to each table even if
the table is not physically partitioned
within the database okay so a single
table with no partition can be accessed
in the parallel way by several ABC
carnations so how we do that is the it's
the topic of this section okay so what
you see here in the picture is that we
have several tasks you know in work
kernel to several worker node on they're
accessing the table one table in
parallel okay that's where it's shown
okay so how does it work so Oracle has
already produced and released a product
called Oracle date sauce for Hadoop on
we have
define the split you know how we use the
split or partition in the spark
vocabulary we have defined those in this
document so if you look at this document
you know you will see the definition of
the split and these are the same you
know we have the single splitter which
means there is no parallelism the whole
table is treated as one unit so only one
task will be accessing that table for
one single TDC connection
that's the default then you have a Rose
clicker will split around you to split
our partition one table based on will
count for example if your table has 1
million roll and you want to split in 10
you will set reciprocal 200 thousand so
it's going to create 10 logical
partitioning of that table the other
mechanism we use is block splitter so
instead of rows we will be split based
on blood count and we also have the
partition splitter which split based on
the physical partitioning of the table
so if the database fare as if the table
the database table has been already
partition you know physically in the
database we economist Oh sheet one split
to Kate partition so it's gonna be one
to one so those are the partitioner that
we we will be supporting that we already
have are we reducing or using the same
mechanism the same of course we do have
something called custom partitioning
where the end user will use its own
sequel query to do the partitioning
belanger talking about this here okay so
this is how we will create a data frame
you can see that in invoke this function
you specify several jar file you know
this will be the job representing the
product but I am not showing it here
and then we have the rid you know the
beta string we're creating a data stream
here by using you know the Oracle data
source for spunk for example I'm just
making this up and you can see that in
through the options we will provide the
URL you know the delivery URL to connect
to the database we provide the driver
the name of the table the database table
the schema is an imposter
if you are using the username password
authentication but if you're using
caporals or more advanced application
this is the place where you will specify
those authentication properties we are
specifying the partition type here to
block splitter and we specifying how
many partitions action we want and then
we say the load when you invoke the load
function it does not load it just
creates a framework you know it does not
actually stretch the rows you just
define the framework
okay um I was referring to some
operation on the data frame you can see
here we got the load we had to show when
you invoke show that's where Rosa tries
you can do count you can print the
schema or you can filter filter in the
transformation you can do an action I'd
first give me the first rolling back bit
different now you can apply a query you
can select the employee ID the job title
on the data frame and we can't show
apply this show which you can apply a
filter so this will create a new IVD
under the cover on you showing the data
or you can apply another query and
should bring this and this accessing I
don't know duplicate those lines right
now okay so some other optimizations are
JDBC type conversion in our
implementation for example for some data
types we will only provide the directly
provider role which if it died from the
database to spark or hadoo so we we're
not doing the intermediate conversion
from sequel type into data type or
scholar type etc we're currently going
from sequel to spark you know spark data
type also refer to a catalyst data types
and in some older tied you know such a
time or times then we really provide the
byte representation of second or days in
epoch and we're just giving this robot
to respond so now we speed up the
conversion with there will be no
intermediate conversion from sequel to
JDBC on them to spark I always can allow
you to set all the JDBC commercial
properties you know for example you want
to specify
are researching
statement caching all sort of things
connection caching this is a mechanism
where we avoid closing a connection this
is not connection pooling this is
Commission cache which means we cache
the existing connection this is
something you will see in PHP as well so
when you invoke the clause we do not
close the connection we keep it on the
reason is let's say you end up with 100
click partition
okay so you specify how you want to
create a data frame you specify the
splitter and we end up with 100 but we
don't have hundred simultaneous
connections to the database
your DBA a database administrator will
not allow you to fire 100 concurrent
connections to each database
I'm just giving an example it could be
more than that than just giving an
example that in case the number of
active connection is lower is fewer than
the number of split you have to do it in
several run you know you do round one
you use 50 condition or whatever number
your DBA allows you to use and then the
rest remaining number of split with the
process again by reusing the same
condition that's why we don't close the
connection that's why you can catch none
we use book authentication but we can
support cables reduce user name password
and some other mechanism all the
encryption of integrate is supported by
Oracle JDBC are supported the high
availability in this case Oracle
database has rack real application
cluster architecture where if a node is
down I mean in instances down another
instance might be up and will continue
accessing the same data base so JDBC
supports that and this will also be
available to any data source we provide
for Hadoop or spa and also runtime load
balancing of connection requests you
know for example in the cluster
environment we will
check the condition witches
direct it to the lease loaded instance
you know two instances are several
interested accessing the same database
that the runtime balancing mechanism
allow you to go to the least loaded
if not you would just be around rubbing
you know you get to the one I'm going to
the legs and there is no load balancing
control okay so that's the summary of I
will go back onto the split mechanism
and during the demo okay so I will show
you now some the movement will wrap up
okay today most this I have VirtualBox
and in the little box this little box in
Basel Oracle DBA
big data appliance which is a Hadoop or
spark cluster so the BBA light is the
VirtualBox image
mr. box appliance you know detail box
image that you can use
with VirtualBox software so this
appliance you know appliance is the term
they use individual box columns this
appliance of all sort of software it has
everything you need to go on spark again
it has high it has the Oracle database
so it has everything and this is
available or the Oracle technology
Network which has now been renamed into
the Oracle developer network something
like that anyway it has everything so I
have my demo here and you can see that I
have several script I have different
script let me show you the table
creation so this file
is creating the table employee data okay
this way I'll show you this discreet
okay so let's see okay here we have
okay this one is create bonus hi this is
the high table okay this is hi we are
loading the data from a CSV CSE beta 5
you know so this is just to create a
bonus where you have the employee ID and
the bonus with nobody sees bodies and
this is in height this is an Oracle
database table the name of the table is
employee data on this table is
physically partition and the partition K
is the Ferrari so based on the salary
range we have five partition okay
and this is another table on Oracle
which will contain the report you know
when we do our play a bonus report we
will start with out here
okay so I'm here okay I'll show you
different mechanism soon
okay so I'll show you um predicate push
down
I'll show you partition on pruning
partition pruning and join so for demos
okay so what is in predicate predicates
for snack syndicate push down as you can
see invoke this function with all the
jar files
you know rigidity feature the you see
puja
the audio sauce jar and then eating box
is scarified so what is in this codify
this kind of file this is where we
create the beta frame you see the data
frame here and this is where we specify
you know all the implementation details
and options you know you can see here we
simplify in the URL we specify the
driver the name of the table this the
username password so now we create the
data frame and then we have the basis a
different filter which is applying this
transformation
this transformation on the data frame so
we select first name last and reply I
did and we are specifying this workload
which is the pimple and then I will
explain we will have an explain to
explain what is the different planning
on how this will be processed and we
will show the resort levels ok so if I
say dedicate push down
okay so if we invoke spark shell
and then it will
invoke the content of the predicate
pushdown Scala
which has all the details I've shown you
so in this example we are just showing
how predicate pushdown works so the work
clothes you know the employee ID equals
some value will be pushed down into the
Oracle database in other world we are
not gonna pull all data to spark no we
will only get to spark one role and here
we are using a single speaker so if you
look at there are a lot of debug
information here including the physical
plan on you can see that we are pushing
the predicate predicate is push down
push the filter is in pain ID equal
three nine three three two okay so this
is the employee is Christopher Harris
case they don't play with that ID so
this is just to show you how predicates
push down with work okay now let me show
you partition not funny partition on
pruning means we are not going to prune
partition okay so you partition pruning
not scar
the shell script we just consume this
gala file so I'm not showing we shall
buy it to say Mass this year in working
this caliper and we can see here we are
creating the data front bodies and in
the password above this is important we
need to specify what is the partition
type on the partition type is partition
splitter okay and there is a max
partition you know to contain P to
control the box you don't want to
generate too many partitions so if we
find out that we will generate too many
we control we check with marks partition
and managed to keep the number of
partitions below max partition okay so
we could initial data frame and we apply
the filter this is different filter this
is a different workers you know we can
see here that this is the Omega I'm
showing the wrong I'm showing the wrong
non foolish sorry non pruning okay
okay lunch warning we can see that the
option is option partition splitter and
the workload is the same as in the
predicate push none example it's the
same we're closed
so it's the same query if you want but
the difference is we are using partition
splitter instead of single speaker are
we using partition splitter because we
know the table is partitioned okay but
this is the same way so let's see what
happens when we execute
partition on pruning that Sh
okay so what happens on the Dakota is
the the software that we develop you
know we introspect the table we look at
the partition type that you specified
and we'll generate as many click or
partition based on what you have
specified in this partitioning time so
that's how we can generate parallel
actions for LEDs and we will see that in
this case because we specify a partition
type it will map
this flips to the database partition so
the database table has 5
partitions you know we remember the the
creation of the table and you can see
here that we are generating five so five
speeds have been generated by the
project in relation and the Oracle DVD
CR DB we push down the predicate the
predicate is the workload and you can
see here that we retrieve the same
employee mr. Harris okay so this gives
you predicates push down and the
partition splitting you know but it's
non pruning now let's look at partition
pruning
artists including we are using the
salary in the we're close here and
remember the table is split based on
salary so partisan pruning means if my
query can be satisfied by one or two
partitions I don't need to access the
other partitions so okay so let's
execute this okay I can show you the
shaft and you can see the shell is just
invoking this function with all the jar
file and this caliper so if I come here
like in book partition pruning that Sh
so the same operation will happen it
with excellent the specification that
you put in the scanning in the options
and generate you know based on the table
then as many split as necessary so you
will see the difference between
partition on pruning and partition
pruning if the difference is the number
of generation tape spirit how many split
will be generated how many partition
with the action
in the physical table okay so it's going
on and we have 65 partitions Victor
and this is the were closed and we can
see number of scripts generated one why
because this were closed the workload
here can be satisfied with one partition
which is in fact the second if I if I
show you let me show you again you
create employee data which is the one
which create the Oracle database table
you see here so everything
less than
is not this this one less than 8,000
okay so this is the part the partition
that will be used to satisfy this
specific query okay so that shows you
how partition split work now the last
thing I want to show you can work at a
stage
okay join that Sh in this example we are
going to join data in Oracle database
with data in height remember employee
bonus is a table on this is the script
to create that table and this is the CSV
file to populate that table so the first
phase we will populate the table on
second stage we will connect to the
Oracle database to produce the joint and
then change the data so do join dog
scholar this is the example which I
started the session with explaining the
need to join data from outside okay I'm
taking this very a dummy and basic
example but it's to prove the concept
that we can join data from different
size so this is the data from here okay
we're using the block partition and then
we reduce the that table we operate data
Oracle you know which is the data friend
that we got and then now we apply
another query okay so we look at this we
have the oricon played it so it's man
last name and then we have the employee
polish table which is a low per table to
spark it this is the high table we can
Roca okay so we will select those and we
will join based on employee ID so the
umpire ID involved the oracle table
under punish the table if those are the
same way and the bonus if the Saran is
higher than 7000 and the pony see higher
than 7000 then retrieve all those rows
so that's what we gonna do
okay as a to join that Sh
okay so the speed you are seeing here is
because I'm running dieter box on my
laptop and I have the data build I have
sparked I have everything running and my
laptop is only 16 gig so it takes some
time but this is not what you will see
in real action and we let you have your
powerful spark cluster and you can fire
you know waiver hundred thousand of
course and you can try it in Kingston if
we go back can come back quickly but
also the rows in the table here we are
just using a toy cable but in real that
you can have millions or billions of
rows so this is just for demo purposes
so what's happening it's going to create
a high table populate the right table
then it's gonna retrieve data it's gonna
retrieve data because we push down some
predicate to be or Athan so it's pushing
down the predicate and then it will
retrieve some data and join with the
local data so remember the query
coordinator is still on spark so spark
is driving everything right but I get I
think it will treat complete success
rate so the predicate is push down it
will retrieve salaries higher than 7000
and then it will join locally with rose
and where the bonus is higher than 7000
okay the were close that would be the
same the employee ID same employer idea
above Seidman and we retrieve the people
rich in their bodies intelligence it
okay so that put the concept that's what
I wanted to show you okay so to Martha
this is it
so I just show you you know how you can
turn a relational table into the spa
painter so thank you very much bye
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>