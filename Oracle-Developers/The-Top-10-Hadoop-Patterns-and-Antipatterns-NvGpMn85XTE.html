<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Top 10 Hadoop Patterns and Antipatterns | Coder Coacher - Coaching Coders</title><meta content="The Top 10 Hadoop Patterns and Antipatterns - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>The Top 10 Hadoop Patterns and Antipatterns</b></h2><h5 class="post__date">2015-06-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/NvGpMn85XTE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I will welcome everyone thank you very
much for attending my talk today I'm
going to be talking about Hadoop our
teams and anti-patterns Rosen started
with 10 thinking that was a nice for a
number but 10 was going to take about an
hour and a half and I figured that was
gonna be too long for you guys and from
me so I trimmed it down to seven so my
name is Alex homes i'm a software
engineer I've been specializing in
Hadoop now for the last six years can
you mookie as this beggar okay all right
okay so by that yes my name is Alex
homes um I've been a soft engineer now
for a few years I've been specializing
Hadoop four by six of six years and have
a book a second edition is actually
literally to press this week and the
javelin bookstore has got some copies
out there so I'm having worked in her
group I get asked a but you know the
same kind of questions over and over
again there's a lot of you know if
you're not familiar with Hadoop there's
a lot of lack of clarity around what I
do is I get asked you know is Hadoop and
no sequel system and as a batch only and
what can the problems can easy to solve
what kind of promises are not good at
solving and that's really the goal of my
talk here so you know when i'm looking
at learning a new system I mean what I
try and do is I try and look at
successful examples of whatever that
thing is weathers architecture or some
soft ramble building and I try and dig
down and can look at the patterns that
were successfully apply to result in in
whatever the final entity is and what I
find even more instructive action i
think is something we don't do a lot of
in our industry is look at failed
examples and really understand why did
things fail and that's really the goal
of my talky i'm going to present seven
real world things that are commonly you
know presented in Hadoop and what I'm
going to do is I'm going to be many ways
by which each problem can be solved this
is going to interactive so you guys
argue me sitting there and just kind of
letting me talk and talk I'm going to
ask you to vote on you know a number of
problems for each number of solutions
here and then I'm going to give you my
feedback as to what I think they can
make the best the best and practice here
is
so here's our agenda I've kind of broken
them into three sets of categories the
first one is connect atta integration
and some patterns around how to deal
with data and HDFS and the second we're
going to bump a party and look at some
computational pattern so we're going to
look at my produce and stream processing
and some fun stuff like that and then
the last one is going to be an admin on
Hadoop so before I get started to hear
em is familiar with with Hadoop at least
at a higher level understands okay
that's a pretty good number who he is
actually using an introduction right now
okay so that's that's pretty good so I'm
this is gonna be one minute and then
we're going to dive into the interior
and the limited content here but at the
at the basic level I'm Hadoop em and
early years was based off of some papers
like Google published about ten years
ago and there they described a
distributed file system and Hadoop's
equivalent of that is called HDFS and
that's where we now part ways a little
bit so up until about a year ago the
only way you could do work and hadoop
mapreduce which is kind of a bad space
to em system and that was again based on
the paper that Google had published
Hadoop's and being you know trying to
evolve am so it's kind of it's kind of
moved my produced upper tier and just
now come up with this thing called yarn
which is what for yet another reason ago
she ater it's basically a
general-purpose schedule much like you
have on my regular operating system and
it doesn't care about what can a
platoons applications are running on the
cluster you just talk to the schedule to
ask for work it manages resources of the
cluster and then kind of gives you the
permission to schedule application
specific tasks are trying to run so
combined they can be logically thought
off as being a kernel for big data so
generalized platform where you have
storage and generalized compute
capabilities and really was the more
interesting problem but Hadoop is really
they can like the rich applications I
set on top of that and like I said my
produced used to be the only we could do
what now it's just a yet another M
Hadoop application and now you can have
graph processing and sequel now you you
could do graph processing sequel on
MapReduce but had to be done within a
framework and its clergy and they're
looking like non-native hacks I had to
be done to make them work now
these applications can up now being
migrated to be full-fledged yarn
applications so let's dive into things
here let's start off by looking at data
ingest the process of getting data into
head tube so imagine your boss comes
along to you and says you know we've got
all this great data at sitting Oracle we
now want to get that into Hadoop right
we want to use analytics something this
is now where you guys get to vote on
what you think the right approach here
is going to be so one up one approach
one solution would be to build your own
etl solution here right if you do that
you can customize any way you want you
can tweak it you can build that you can
refine it to exactly the they connect
needs that you have for your solution
the other option here is you know maybe
use off-the-shelf kind of solution so
we're going to ease into here so who
thinks option number one just raise your
hands if you think option number one is
a way to go in terms of building m data
in jest okay so few people and then who
thinks option number two using
off-the-shelf okay interesting well
that's indeed the way that I would do
things and and it is tempting to build
you know by engineers we love building
stuff right it's tempting to want to
build your own ETR system it sounds like
fun we know how to get data out of
databases we know how to get data into
Hadoop we just got a little bit in the
middle we go to build I mean what can be
complicated about that right it seems
simple enough but when you get entered
it's actually complicated you got to
deal with potentially big database
tables may be multiple tables you you
probably want to paralyze the data in
jest and you probably want to run them
on multiple hosts you've got fault
tolerance and now you're building a
full-fledged distributed system and you
probably didn't want to do that when you
started ironed and and why did that one
is a great tool either called scoop and
it does just that runs as a MapReduce
job in Hadoop it can be used to move
data from a database to Hadoop and vice
versa so now deploy scoop you know sit
back video hacker news and you know and
don't think about all these things I've
already been solved but you know our
bosses are always coming back to us more
work and once you do that he's probably
going to come back and say okay so that
same data that we want that we moved
into Hadoop I want you to be able to
calculate aggregates in real time using
that data so you're like okay that
sounds doable I'm you know heard of
storm and I've heard of em linkedin
Samson and heard of sparks streaming
these all can like these generalized
stream processing frameworks but how do
I get data into that and that's where
the ? is so I mean you've kind of solved
one and data integration between a
database in Hadoop but now another one I
was sprung up and you're like well how
am I going to solve that and it's really
indicative of a bigger problem we have
in our industry here is the fact that
we've got multiple part in a persistent
systems at the top there's we've got em
relational databases we've got no sequel
and you see code we've got Hadoop data
warehouses and at the bottom we've got a
can applications are all hungry for data
and the question is you know ideally we
have I can a very interconnected and set
up where we have this point to point
solutions but I certainly don't want to
support building a custom et our even
off-the-shelf solution for each one of
these lines I don't think you do either
so how do we solve this problem and and
the solution or one solution is using
Kafka who here has out of Kafka I'm kind
of curious okay good I'm so Kafka it
came out of linked and they had this
exact same problem they had to move data
from a my data buyers into Hadoop and
then at some point down a line from her
dupe into a key value store and they
realized the need for generic can I data
integration and platform and that's what
Kafka is it's a pub sub log based system
and my isolates it produces from the
consumers and it's got some really
interesting and in the performance is
pretty spectacular on it and it's got
some interesting properties that are can
it work very well for consumer for Kafka
not caring about who the consumers are
and the consumers having a lot of
control over connect delivery guarantees
and these kind of things so basically
the takeaway for this pattern is you
know avoiding it yourself and because
the spoiler had been solved and scoop is
a great tool but if you find yourself in
this position where you know that the
same data set needs to be moved into
multiple locations then i would consider
taking a cold hard look at Kafka books i
think that's a great solution for some
of the problems that we have all right
so let's change gears a little bit and
look at em files and HTF face so imagine
you are Google and you need to write
amrap web crawlers are going to download
tent from the internet and ends / system
and HDFS alright so there's going to a
few options here asked to the solution
here so one approach would be so the
content for each URL gets saved as a
single file in HDFS the second option is
that your buffer the buffer the rights
and coral a cement to records around you
know one Meg files and then the file the
final option is buffer and also call
this but make them bigger files so who
thinks writing a separate file for URL
is probably the way to go with em HDFS
anyone I guess I'm the only one then so
who thinks buffering and writing
coalesce records to 11 Meg emphasis a
way to go ok a few people ok and then
who thinks is going to be you know
greater equal to 256 meg would be the
recommended ok majority of people ok
good well indeed maybe these are too
easy for you guys but yes indeed it's
you know you want to have the files as
big as possible and and the reason for
this is that HDFS a storage system and
Hadoop is master slave based so and the
way it works is that the Masters
actually cash in memory all the data
about the filesystem all the files and
the blocks of these files occupy get
cached in memory and when you look at
the numbers it's about six on averaged
around 600 bytes in memory now that
doesn't sound like a lot but if you're
developing a web crawler type
application you're potentially dealing a
billions of em URLs a building your else
is going to consumer by 60 gigs of RAM
that's a lot of a lot of memory a lot of
heat pressure creating them and then I
miss your face you know if you look at
the internals on average it's around 256
Meg's as the block stays in HDFS and you
know whether or not you write a file
with one bite or 256 Meg's it consumes
them at the same amount of memory in the
name node and HDFS so it's it's really
to a benefit to connect pack these files
as because you can so the faller is
again really the fact that you're
creating as he pressure you'll note it
you'll know you have this problem
because HDFS will slow slow down over
time and it's really an auntie parking
HDFS was really designed to deal with
these large files so and when we're
looking at solutions em coalescing as
it's kind of a no-brainer here and we've
already had this problem and I've had
this problem in the past and there's
some compaction tools odor that can help
you kind of cool ése files already
exists there and HDFS Federation is a
way by which you can actually have split
your namespace and HT fashion to connect
discrete parts that get connect em like
it stored by different name nodes like
that's a way of Connect spreading the
memory load across different servers and
then and there's a Hadoop vendor out
there called map our butt and they've
got kind of highly optimized and
distributed storage they don't use HDFS
above the own connect native file system
and they don't they claim not to have
this problem so it's worth looking into
all rights our last item here for data
is going to be data organization so
imagine you need to consume the Twitter
firehorse right it's a pretty pretty big
fire hose need to get a data onto head
tube so we've already learned that you
know we won't be storing large files
we've already done man so let's look at
how you want to store the data in Hadoop
so there's a couple of options here
first option would be petitioning a data
by deigned the second option would be
just writing all the record since I
single directory because Hadoop is you
know scalable and it can easily handle
any connected data volumes and a third
option is a single a service directly
doesn't support large data volumes this
limitation and having HDFS that are
directly can only support one terabyte
you need to connect structure your
rights are cordless you do you don't hit
this limitation so who thinks em you
should just partition your data by date
ok and then who thinks we should write
all the records into a single directory
because HDFS is scaled well anyway I
hear all these great things about and it
can handle any kind of data volume you
can stick in our directory and then who
thinks is gonna be the final option the
single data kind support and a more than
a terabyte ok well it turns out that and
partition your data by date now my sound
a little odd but in the fact of the
matter is you know we've had decades of
data warehouse research and Hadoop
doesn't obviate the need for that
research tool for that these kind of
best practices being applied and the
fact that matters full data scans are
expensive and Hadoop
every think about what's involved you've
got a lot of disk networking cpu iola's
being utilized every time you're reading
the entire data set and then when you're
dealing in multi-tenancy environments
we've got multiple data scientists and
production floors all these in the same
data set that's a lot of unnecessary
connect reads are occurring over the
whole data set and and oftentimes you're
just interested in a small piece of that
data so you know partitioning your data
if ever you're dealing was dating Hadoop
even if another outside it seems like a
small amount of data and I strongly
recommend come up with some sort of
partitioning scheme now how do you
forget how the petitioner data well
understanding house can be read is kind
of a big step here and if you don't know
then talk to your users about how
they're going to user data this is you
know I can a very basic and pray here
and if they don't know and you don't
know and you know you're like how do I
petition it then obviously at least do
it by deigned and and when I say date I
mean and and make it obvious and nobody
lay out your data so it's you know you
want to lay a data in a way by which you
know as a reader of that directory name
it's clear to me what's in their data
tweets d equals that's kind of economic
sense so the takeaway here is that in a
full data scans I expensive even n
connect these thousand no two clusters
that we can have and just talk to users
be engaged with your your connect and
your internal community around how the
data is going to get used and and try
and partition in a way that's going to
can i optimize further for the users
here alright so let's bump up our to you
here and can i go from the data to the
computation so let's talk a little about
MapReduce so imagine you have a one
terabyte file now on today's enterprise
rotating hard drives if you sequential
ereader a file from beginning to end
it's going to take around 74 minutes
which is you know decently long so
clearly the way to connect optimizer is
to take that file in and split up into
these discrete chunks and then basically
have different threads or processes I
can read each chunk and that's you know
you can bring that time down to a
fraction of what it is and that's
basically what MapReduce does and when
you save our one terabyte file and HDFS
it's actually chunking it into these
blogs yeah I mentioned at 256 meg
earlier
it puts East 256 meg block onto a
different node in the cluster and then
when you run a MapReduce job and you
indicate that this one terabyte file is
your input MapReduce figures out what
nodes these blocks exist on and then
runs these map tasks local to these
nodes and this is called data locality
and Hadoop and the kind of things you do
in your map task January is you're going
to do filtering you can do projections
unit transformations and then you have
this magical connect step that occurs
between the map and reduce job so my
producers key value pairs and the
shuffle is connect the magical part
entity in the middle here and what it
does is it takes each map I put key it
does a harsh mod over the number of
reduces that you have and then the
basically siphons that records to that
specific reducer so yeah you have the
straight whereby all the records with
the same map I put key get sent to the
same register and then the shuffle also
sorts all our data by key and then
presents you that data with one
invocation to your reducer with an
iterable over all the values so that's
basically how my produce works and then
the reducer so the shuffle is giving
this kind of unique join ability to join
and aggregate your data and then you
reduce you can do the aggregation
typically that data gets written back at
it to HDFS so now i'm going to throw
some MapReduce code at you with a little
data set and and I'm going to ask you a
little quiz about what you think the
expected output is going to be so we're
going to design little program a load
program that's kind of stupid for
MapReduce a data set is going to ask Eve
I repaired the key is a name of fruit
and the value is a cost of the fruit and
the goal of the algorithm here's to
output for each unique and fruit the
lowest cost so for this data set is
clearly the number one so here's a
mapper the key in value will get a
supplied in text form and it is
essentially call an identity map we're
not really doing much of anything other
than but emitting the same key that the
name of the fruit and we're just
converting the value into an integer and
supplying that as a value as but as
simple as you can get when it comes to
my opera the reduces a little more
computer but is now a whole lot more
complicated so we again remember that
what's going to happen is you're going
to get a witches are called once for
each for all the instances of benign
in our data set and all the values are
going to be supplied in the iterable and
what we're going to do here is we're
calculating the minimum value so we're
going to store we're going to store but
we think there's a minimum value
initialize it to integer max value we're
going to iterate through all our values
and if it's less than and the current
cash value we're going to set it and
then at the very end once we're done
iterating we're going to output the name
of the fruit and the minimum value
pretty basic stuff here all right so
here's our input data set this is a
order in which provides you're going to
get streamed to that reducer and you're
going to vote on what you think the
output of this up MapReduce operation is
going to be so one option is as integer
the max value which is a the first value
probably initialize them in value to
another option is seven or one or two
that the connect three options with
revising it in this data set here so who
thinks that M the output of this program
is going to be integer max value N 1 ok
and then who thinks the values gonna be
seven maximum value ok who thinks the
output is going to be one the minimum
value ok and who thinks of I is going to
be too ok so you may be surprised to
know that I put of this is 2y seems
bizarre well here's a deal so you may
have noticed we're not dealing with
strings and integers we're dealing with
text and in tradable now strings and
integers in Hadoop are immutable you
can't change them and the reason Hadoop
doesn't use them is because it wants to
cut down on the patient on the garbage
collector and right and again because of
fact we're dealing with MapReduce you're
dealing with millions minds of Records
you know creating new objects for every
single record is expensive so what
Hadoop does is it creates text and
writable their mutual data structures
beneath the scenes what's happening it
looks in your iterator that you are
trading over different and dry tables
but beneath the scenes there's only one
intractable reference and what Hadoop is
doing beneath the scenes is setting the
underlying primitive to be the latest
value so the first time I gets cold is
going to have a value of one and then
seven and two you're just drawing a
reference so the value within reference
be
changed underneath you so by the very
end the last time you've been you're
going through the edge variable value is
2 and a Sandeep I guess outputted in
your function here so this is a classic
can I gotcha I bumped into there's so
many times I can't tell you I even
bumped into this year and I should know
better right i mean so so really the fix
here is to don't use a don't store the
reference just extract the primitive
encapsulated type the spill entrant and
store that instead so the takeaway here
is that MapReduce does that to cut down
on GC GC is very expensive and these
kind of batch processing systems don't
store the reference and like I said I
make this mistake all the time so don't
feel bad when you make it too okay so
let's look at data exploration so this
is a kind of new field as it exploded
recently in Hadoop em again because of
yarn and some of the facilities it's
kind of opened up toes so now your boss
comes to your data scientist or data
analyst and your boss comes to you and
says hey you know the number of and
disengage users on our website is kind
of gone up no can you research into it
and look into why and I want and results
today so like okay yeah we've got you
know user account information we've got
connect user activity and events we've
got the connect em social connections
they maybe have and and really am the
kind of activities you want to perform
as a data scientist if you want the
results back today you're going to want
in a low latency queries you can be able
you won't be able to work rapidly in a
shell and maybe do some iterative
processing machine learning and logistic
regression all that kind of stuff so
what do you think is the best tool for
for this kind of for this kind of role
and it could be a MapReduce it could be
sequel on Hadoop or could be something
else so who thinks you know you should
use MapReduce to do to these kind of
things okay whoo thanks sequel on Hadoop
use sequel for this okay and another
tool okay alright so it turns out that
there's actually I'm kind of cheating
here but both of these B and C are a
viable options to end and an asthma you
know can i refresh my memory why
MapReduce isn't a good fit here so it's
not low latency as a badge system it can
take minutes fab
MapReduce job to even start it's not
even done anything it's just kind of
been bringing up processes the other
thing I'm about my producers have got
this thing called a replicated disc
barrier what's that well that's
basically the reducer outputs get
written to HDFS HDFS is a replicated
file system so not only is reduced to
writing output to local disk but two
replicas of the ipod being streamed over
the network so that's high that's heiio
and really when you're looking at things
like you know graph processing iterative
processing we've got multiple jobs in
the chain together that's really
expensive and it's going to take you you
know our players complete my produce
doesn't have a shell either so it's kind
of useless for I basically need to write
you're right my MapReduce job and you
got a runner and then kind of look at
the out person kind of there's no
interactive shell to work with and and
then finally it's verbose I mean it's
going to take you the whole day to even
write the code to do a basic function
never mind anything else so my produce
isn't really a good fit for the kind of
work we're trying to do here and and the
sequel really is I mean I've and there's
a few systems on sequel where that you
can pick from I mean hive has been a
mainstay from from the other days of
Hadoop and traditionally used to be
MapReduce based and but now it's been
rewritten to amuse a general generalized
graph processing engine and beneath the
scenes and so it's becoming you know a
lot more interactive em Impala which
came as a cloud era and one of the
Hadoop vendors is designed from the
ground up to be a native distributed
file system dissipated em database it is
super responsive I mean it's amazing a
quick you bring up and power shell and
you run queries over you know multi
terabytes of data and results come back
and in seconds it's really impressive so
really my recommendations here is hive
and Impala I would recommend both
actually hive is a more mature and power
still kind of am a little green there's
some aggregation functions you can't do
an Impala that you can do in hive so
generally I you know I use both and both
these systems together for for my sequel
needs and now um spark is a really
exciting new development who here has
heard a spark I'm really curious to see
okay so I said
general purpose cluster computing system
and one of the basic and execute is on
Hadoop and is that the really compelling
thing but spark is it's got this really
high-level descriptive functional API
it's got it's a kind of natively n
scholar but it's also got my produced
using lambdas in Java 8 end is up
there's a pie spark as well for Python
users which data scientists were like um
but I love the API it's kind of very
descriptive is simple and the neat thing
is in the spark is not only I can I
generalized execution engine but you can
use it for stream processing and graph
processing machine learning and use the
same code across all these different
types of functions this is really the
first time we've seen a system allows
you to write code ones and execute
across different kind of processing
paradigms so Miss Parker is pretty new i
think is going to be i think in you know
23 years time it's going to be you know
the backwaters way that we do big data
processing so i would keep an eye out or
not and and you know one of the things i
kind of want to draw your attention to
the spark is a the neat thing here it's
got at the very end here you were doing
i connect a simple mmm filtering
operation but that cash statement at the
end there basically tells spark to cash
that distributed data set in memory
across all the nodes we've got this your
spark processes running and then you can
basically iterate over that data set
really quickly this is like the first
time we've had like in-memory processing
available I'm on a Hadoop platform and
it's really exciting I think it's going
to really open up and the kind of work
that we can do in her loop and logistic
regression which is a complex saying
this is an example I'm just going to
throw em up and but this is just shows
you how simple can we do to that pretty
complex em function em in parallel or
ver distributed cluster so whether
takeaways here are the fact that in a
MapReduce isn't a great fit for you know
was the only way so it was it used to be
how we used to do graph processing into
the processing and Hadoop there's no
need for that and we've got a spark and
and other tools now that can fit in
philly for the need there so let's look
at stream processing so imagine that
you're working for the New York Times
and you're being asked to develop a
little portlet which shows em your user
the top 10 trend
news articles are happening right now so
how do you go about building that well
one way would be to collect the
clickstream data that's occurring in
your application move that into Kafka
that we talked about earlier and then we
can use a stream processing system like
storm to aggregate mi data so we do a
grenade with basically group on the URLs
and have this sliding when you know ten
minute maybe window chunks where we
count the number of clicks we see for
each URL and then we write that out
periodic 22 anos ecosystem like a space
both these things can be running in
Hadoop and then your application can
like basically hit and use that news no
escucho system to bring up the top 10
trending results and display in your web
amp so you build this is running in
production everything's looking great
and and you're happy with that but QA
come back to you and say hey there's a
bug and your aggregation logic you know
like well crap how am I going to solve
that I've got like you know a year's
worth of a month's worth of these
connect ten minute top 10 windows how am
I going to go back and fix that so
there's a couple of options here you
could use batch processing to do this as
one option or you could replay the data
I mean °f / Perez in your Kafka cluster
you could replay that data back into
storm and recalculate it and and and you
be done so who thinks using a batch
processing would be the waiter will be
ready to go here and one okay and what
about replaying back data back to storm
I mean Kafka okay so I mean as it turns
out this is not you know a binary you
know one way is better than the other
way and this is pretty subjective is
there's a good amount of contention and
the mechanic business around which is
the right way of doing this my personal
opinion is batch processing is the way
to fix this and and the way this would
work is that we've got the tea at the
top of got the real time processing
happening the top and you know the same
data that we're writing into the storm i
connect teeing off into mhm TFS using
commas which is a linkedin tool allows
you to do that and once it's in HDFS you
know we can use it for data science and
all other kind of good stuff that we
talked about but then we can fix bugs
and the real time processing to you in
MapReduce or spark or ever computational
engineer want to use and the
to data out to a space now the reason I
think this is better than replaying data
from Kafka is that you oftentimes hit
scenarios where it is a bug in your data
so if there's only bugs in your code I
think it's perfectly reasonable to only
use calf can storm and replay data and
design your pipeline that way but I mean
often encountered in my experience times
where there's been bugging data and when
there's a bug in data really the only
way to fix that is that the is that the
system of record level and if you use
Hadoop as your system of level you can
basically fix your data there reprocess
it push out to your no sequel store and
now you're now able to fix data problems
in your bug and your data as well as
your application logic so they're kind
of takeaways here all that you know and
these stream processing systems they
sound really connect fun and and and
Roebuck within they are but in reality
is bugs everywhere and you need to you
know as you're thinking of I think it's
thinking through the design of your
solution you need to think through how
am I going to fix these bugs and deal
with these kind of scenarios this is
called lambda architecture by the way
and Nathan Mars kind of coined that term
and he's got a big called big data and
he's got pretty interesting and blog as
well where he writes the boat and this
in more detail okay so a final one here
is Hadoop admin so here's here's Bob you
know we've all got a barber now in our
office and Bob's greater systems admin
and you know who's been connect and
repurposed to now become a Hadoop admin
in our organization he's been working
with her Duke for about a year now and
he manages number of small clusters and
your organization so you know your
organization's kind of built and num has
decided to kind of build that Hadoop in
a big way it's going to even have a
thousand or cluster it's gonna be
supporting critical features using this
cluster what should you do from a Hadoop
admin perspective I mean one option is
use Bob I mean Bob's been working with
her dupe for a year he's an essay he's
got all the skills he needs to do that
admin the other option is getting
certified as a who dip admin to fill in
any gaps you might have in his knowledge
I'm option three here would be building
your own
dev ops team and then the final option
will be getting a support contract from
a Hadoop vendor so who thinks we should
use Bob who likes bomb not many people
like Bob okay and who thinks Bob should
get certified as a Hadoop admin okay
let's definitely some love Bob here not
much was a little bit and who thinks
building a dev ops team is a way to go
home ok and then who thinks getting a
support contract is is the right
solution here ok so you guys mean pretty
on a bowl with all these so indeed and
building a dev ops team as a way to go
and and the reason for that is that not
only do you have this traditional SDA
you know jobs i mean disks fail all the
time in these big clusters and hardware
software fails but Hadoop is complex you
know it's it's still cutting it it's
been out for you know six or seven years
now but it's still bleeding edge and
when he compared to like data warehouses
I've been around for decades things go
wrong things will bump in the night if
you need if F is a part of your critical
infrastructure you need someone who can
look at the code correlate you know the
cord with connect em exceptions in your
and your logs be able to patch the code
open Giro's do all these kind of things
that s is typically unable to do so you
know I'm the takeaway here really is if
you're doing mishel mission-critical
Hadoop and it seems intuitive that the
only thing you should probably need a
tech vendor support but vendor support
typically you know they operate their
own schedules and if you find a critical
bug who knows when they're gonna fix sad
I mean they'll tell you in the sales
call that you know they'll fix it the
next day but you just don't know you
kind of add that mercy so what you
really need is one or two people who
know some Hadoop internals I can look at
the log traces and just and pat your
code so basically in conclusion and you
know Hadoop you know is a powerful tool
the one thing I think is really novel
about Hadoop that we're going to see
more and more fizz that really helps
sale solve this data integration for
remember that picture I show you with
all the different kind of systems and
the interconnected lines if all these
systems are running Hadoop HDFS becomes
your data integration here that's how
you exchange data and solve a lot of the
macy etl work we need to do who do it's
great but it is you know it's still
bleeding edge and
got to look at 4am some of the
anti-patterns I've discussed you and
make sure you've got good support and
you know forms are great and make sure
you get you know some training the
vendors do great training and and I said
basically so am i've got a shameless
plug 1pm today I'm sending my book the
first five people in line get a free
copy so am you know if you might pick up
my book please please be there and
thanks a lot I appreciate your time any
questions a little early so I have time
for questions Barney yes
a VN tradable oh man yeah that's a stump
the chump question right there I can't
REM off the top of my head I'm afraid
sorry yes sir that's a great question i
would say if M Hadoop is the family
place we push data that kafka is
probably all you need because Kafka
actually as off like a month ago
actually can work with flume as well so
you can appreciate a Kafka and then use
pull out of flume and use the existing
flume way of getting data into Hadoop
and but if you're looking at feeding
data you know into other systems and
like stream processing and other
applications using Kafka then you know
you may not need flu may be overkill
nights now and maybe film doesn't need
to exist in your in your architecture
picture at that point so it's a tough
one I mean Kafka's a fully-fledged rich
system way of getting data into her dupe
and I'm sorry I'm flume is flume Islamic
sure has a lot more connect ways where
you can connect weaker Kafka is pretty
new still so for example you can't do if
you want to do transformations and your
data pipeline you got to use slim
because Kafka won't let you do that so
it's it's really a function of you know
how many pieces of data going to and you
know do you need some of the risks
capabilities that you get from flume
like in line transformations yes sir
yep so and they also basically Clara and
and more recently hortonworks and
emotional but map are they all provide
am now spark at least in beta form as
part of the M connect the contracts now
so you'll get that same support with
them and data Briggs is the organization
that came out of em Berkeley who created
sparks and they are also providing first
class level support for spark storm is
Parker well they have overlap so storm
is only a storm is a stream process
stream processing system as all it does
spark has stream processing but it also
has you know iterative processing and
memories sparks like a whole compute
ecosystem in over a Selfridge got these
kind of disagree activities where
extreme processing is one of them
basically storm is a little more mature
now I would say if I think spark is
catching up so if you're starting out if
you don't need to run code right now in
production I would say look up look up
spark if you need to run code right now
in production you might want to weigh
storm russ's spark for for that fend
both both do both do stream processing
yeah yes sir
that's a great question yeah I mean it
looks really interesting i think they've
done a nice job abstracting away and
look pretty complex you're on api that
exists there i I think spring is
probably one of the more compelling ways
of writing yarn applications right now
yeah else okay all right well thank you
everyone</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>