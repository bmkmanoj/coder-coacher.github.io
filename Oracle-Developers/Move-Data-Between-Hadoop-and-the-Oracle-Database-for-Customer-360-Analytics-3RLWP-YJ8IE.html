<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Move Data Between Hadoop and the Oracle Database for Customer 360 Analytics | Coder Coacher - Coaching Coders</title><meta content="Move Data Between Hadoop and the Oracle Database for Customer 360 Analytics - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Move Data Between Hadoop and the Oracle Database for Customer 360 Analytics</b></h2><h5 class="post__date">2017-08-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/3RLWP-YJ8IE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">notionally for customer 360 but in
reality it could be these techniques are
applicable to a lot of different use
cases little introduction my name is
Jeff Richmond I work on the in a cloud
enterprise architect team based in
London doesn't sound like I'm based here
but I am originally this presentation
was going to be delivered by a product
manager and unfortunately I think there
was some date confusion and she ended up
in Berlin instead of London so here I am
instead so that's my I get out clause so
before we get into the detail I wanted
to talk a little bit about the big
picture of our big what we call our big
data management system we have the data
warehouse on one side on the data
reservoir on the other I think it's very
common to have data in many places in
the enterprise these days we see a lot
of customers you know wanting to
leverage data that's in their data
warehouses and their databases and use
them use that data with their data in
their reservoir which is typically
Hadoop and no sequel type of stores and
we have various technologies that play
in both spaces certainly on the data
warehouse side we obviously have the
database itself and the various options
that allow data warehousing like
in-memory and like a tendency and we
have industry standard data models you
choose to use them we can do advanced
analytics in the database we even have
spatial and graph options XML JSON it's
quite a over the years it's become a
very flexible platform shall we say and
then of course in the reservoir side we
ourselves don't have a distribution of
Hadoop but we partner with cloud era and
we also partner with Hortonworks
in some cases and we even have our own
distribution in a sense of that we take
the standard Apache distribution and
tweak it a little bit for our purposes
we also have a set of products that help
us on the Hadoop side to add more value
we have a big data sequel which allows
us to query between the two platforms
we're going to talk a lot about that
today and I'm
time permits I will do a short demo of
Big Data sequel you can see how we can
query data between the two platforms
without moving data around we also have
a distribution of R which is based on
the open source R we just add packages
on top to extend RS capabilities in the
Big Data space unfortunately no time to
talk in detail about that but it's quite
interesting we also have our technology
in the database as well so it's a you
know we you can play in both spaces with
with R and if we have Big Data spatial
and graph capabilities on Hadoop as well
not dependent upon to dupe but certainly
part of that space and then we have a
bunch of tools and techniques that allow
us to move data between those platforms
and that's really what we're going to
talk about we have for change data
capture out of databases we have a
product but called GoldenGate that
captures database change and delivers it
either to other databases or to a Hadoop
stack it can deliver to kafka q it can
deliver to HDFS it could deliver to
HBase pretty much whatever you want to
do and we have streaming analytics it on
top of that so you can be doing
analytics in real time on the data
that's flowing into both hadoop into
your data warehouse so really it's a
combination of the two platforms and
there's some technology that we bundled
together called big data connectors that
help you move data between the two
platforms and then we have other
technology that allows you to query in
place so really it's about data locality
you either physically to copy the data
from Hadoop to the database if that's
the direction that makes the most sense
or you go from the database to Hadoop if
that makes sense
or you leave the data in place and you
query from the database or you query
from Hadoop and we have tools that allow
all of those combinations so it really
just depends on what makes the most
sense for giving use case so let's talk
about moving data first where we're
actually going to physicalize the data
from one platform to another so we have
the ability to use Hadoop to take data
create a Hadoop job and deliver it in a
hyper highly performant way to the
database so
that's a product called Oracle loader
for Hadoop and it really it does create
a MapReduce job that splits the data up
into pieces as it exists as it already
is split on HDFS in parallel loads data
to the database so it's extremely high
performance much higher than you would
get with products like scoop for example
which is the open source or alternative
we also balance the splits so that you
don't end up with one MapReduce job
running much longer than the rest of the
MapReduce jobs so you can get a
predictable load time the other thing
that we do is we convert data types to
Oracle data types as part of that job so
we're converting from native hive data
types if you will into native Oracle
data types which obviously is is
required if you're going to load it to
the database yeah we also have the
ability using oracle sequel to reach out
to hadoop and create an internal table
from the hadoop side so if you have a
hive table and you want to instantiate
it you can use a sequel command to
create a table select data from the hive
table and instantiate it locally in the
database so that's going from the hadoop
side to the database side if you want to
move that data copy that data we go the
other way as well we have a utility that
allows you to again using Hadoop to
bring data from the database to Hadoop
but it's driven from the Hadoop side you
know so we call that copy to Hadoop and
allows you to take data from the
database instantiate it on Hadoop in
what we call data pump format so I know
if you're familiar with oracle databases
but data pump is a is a file format that
we use or heavily for import and export
and backups yeah so the nice thing about
that is is that you're retaining the
data types and you can query that using
again oracle sequel once you've moved
the data in data pump format you don't
have to use data pump format but it is
it can be quite useful the other thing
you can do is you can literally detach a
tablespace or a partition of a
tablespace copy it to Hadoop and the
database will simply see that as another
partition
of the partition table it will manage it
as just another partition they optimized
is fully aware of that and does
partition pruning against those
partitions so it really gives you an
active archive capability hmm the other
thing we can do is you know we can use
hive to to query the data and spark will
come on to that in a second Oh move to
the next one so again if we're moving or
querying data between the two
environments one of the options is
rather than move you can query in place
and we've already sort of mentioned how
you can do that with Big Data sequel you
can also do it with a product that we
call data source for Hadoop Oracle data
source for Hadoop and what that does is
it allows you to from hive or from spark
to see Oracle simply as another data
source so in real time it goes up to the
database and brings the data over I'll
show you a slide about that in a second
all of these techniques of course can be
done through command line we provide
utility called ohs h oracle hadoop shell
handler helper sorry which allows you to
interact with both environments and
issue commands that allow you to load
and push data back and forth so you
don't have to write MapReduce code
yourself you don't have to go to API you
can do it with utility we also have
support for this in sequel developer so
if you're familiar with sequel
developers standard tool for querying
databases then we can use sequel
developer to do this movement of data
back and forth and to query back and
forth so this is a slightly deeper
description of what I just mentioned
around Kopp this is a oracle loader for
Hadoop you're identifying partitions in
hive you're transforming them to Oracle
data types you're using whatever
serializer D serializer you need to use
you can substitute your own so if you if
you have a special 30 you can use your
own 30 we split it up into parallel
streams and we then copy the data over
a very high-performance way now the
degree of parallelism is usually
determined by the number of reducers
that you specify on the for the Hadoop
job but you can set your own parallelism
you can also use the parallelism that's
set on the table side on the Oracle side
of the table the one thing you have to
be somewhat cautious of is that if you
have a lot of reducers you know do you
have enough query slaves on the database
side to support the level of throughput
that could be coming from your Hadoop
cluster to your Oracle database you
could overwhelm the database if you're
not careful you know the obviously the
best performance you're going to get is
if you have a table that matches the
same number so it has the same number of
partitions as you do query slaves so if
I have a table and I've chunked it up
into 128 partitions maybe hash
partitions or range and I then set up
the job to use 128 reducers you're going
to get a very good impedance match
between the number of slaves and the and
the number of partitions on the database
side so it the advantage of this is it
uses very little database CPU it uses
mostly the power of the CPU cores that
you have on the Hadoop side to do this
movement of data so if you're
constrained on that in that sense we
don't you know we have databases that we
don't want to overload this is this
could be a good option for bringing data
to the database without without slamming
it too hard there's a couple options
there too you can do JDBC or you can do
direct path loads depending on your
environment and how you want to do it
this is the other technique that I
mentioned earlier where we use either
Big Data sequel in this case this
diagram is showing you or we can use
what's called the oracle sequel adapter
for hadoop sequel connector for a new OS
CH what's happening here is that we are
deploy our own engine if you will on
each of the Hadoop data nodes and that
engine uses our our
a smart scan technology from our Exadata
line of engineered systems to
interrogate the data as it lives on
Hadoop filter it project the rows do any
transformations and then bring the data
to the database yeah so this is doing it
in a query time this isn't something now
you can you can do it such that your
persistent persistent to an external
table effectively so you say I'm going
to take the data I've defined an
external table that says that's my
source and I'm going to create an
internal table in the database and it
doesn't say create you know create table
or insert into table as select select
the data over and insert in or you don't
have to do that you can just do a select
statement if you're just interested in
actual analytic query so I did want to
touch on Oracle Big Data sequel because
it's an important tool in our bag of
tricks really so what is the gate a
sequel I think one of the challenges
that we've seen is that you know we
obviously have the Oracle database that
runs a very you know rich variety of
sequel that's been around 35 years
roughly and then we have Hadoop hive
type sequel on the right-hand side and
we have no sequel type of database of
data stores and they each have their own
unique kind of interface some of them
you know have their own api's that that
are unique to those platforms so it's
hard to get a single view of data that
lives in all those places so we created
Big Data sequel to allow you to issue
oracle sequel across all of those
platforms simultaneously in one query so
we can support currying interactively
any database that supports a hadoop
storage handler hive store Chandler so
not only can we talk to things like HDFS
but we can talk to our own no sequel
database we can talk to HBase MongoDB
Cassandra accumulo pretty much any data
store that has a high storage handler we
can talk to that performance will vary
depending on how good the hive storage
handler is certainly HDFS natively gives
you the best performance without
question in terms of the Hadoop side of
the equation
we support creating HDFS as I already
mentioned we use hive as a mated data
store so we we don't use hive and a
query sense we don't issue hql this is
not MapReduce this is live interactive
sequel but hive of course has a very
rich made a data definition of all the
tables under management typically and so
we tie into that excuse me in terms of
file formats we support of course you
know CVS type but we support parquet and
Avro and Orci orc and you know depending
on your analytic query requirements you
know each of those formats will deliver
benefits and some you know drawbacks but
we support all that so how does it work
very quickly if we have this select
statement that you see on the left there
we're selecting some columns from two
tables movie log and customer customers
in the database movie log is on the
Hadoop cluster we have a couple of
filter requirements well really only one
in this case where we just say we're
interested in the movie genre being
comedy and we're going to join between
the two with customer ID so what happens
is that we go out and we talk to HDFS
the name node and we say where are the
files
that are composed this table movie log
what is the data structure and how what
degree of parallelism should we have to
be using here and then the you know the
combination of HTM node gives us the
locations and the hive made of star
gives us the structure that we need you
know we then go out and we do parallel
reads using our Big Data sequel server
code running on each of the data nodes
so we scan the rows that are relevant so
we're only interested in rows we're
drawn or equals comedy so it's going to
filter out anything that's not fitting
that predicate we project the columns
out we do any data type conversions we
also parse any JSON or XML on the Hadoop
side so we're using again the CPU
available there to to help in the query
performance and finally when we've done
all that we we've reduced the number of
blocks if you will of data that need to
come back to the database we bring the
data back and we join it we do the join
then and we apply any database
dirty policies to the data at that point
that's an interesting point because it
allows you to do some very sophisticated
application of security policies on data
on Hadoop that are hard to do if not
impossible on Hadoop right now so we can
secure the data more securely if you use
this as an access mechanism then you can
do on native Hadoop this is a slight
technical but this is what happens as we
read the data off of disk and then we we
have a record reader and we have a
surgery that DC realises for us we
translate those bytes into something
that the Oracle database understands and
then we apply smart scan which I've
already mentioned and the bikes flow up
through the stack and eventually what
gets passed back to the database is just
those database blocks that are relevant
to the query now we also have storage
indexes that live in memory to improve
performance so all the HDFS blocks that
have been touched read that is will have
an index block in memory so we can
automatically prune which HDFS blocks we
want to read based on the query
predicates so in this case we want to
find all ratings with them with movies
where a movie ID of 1109 if I look at my
index I can see that that column on that
table for those blocks could have values
in block b1 because it falls between a
thousand and one sixteen hundred and
nine but it couldn't possibly be in
block two because the min and max range
are outside of the 1109 so we can
immediately dramatically reduce the
number of HTF s blocks that we have to
read we also do predicate push down I
won't go into too much detail here but
basically we're trying to push as much
down to the as far down the stack as we
can go so that allows you to do earliest
possible filtering so I don't know if
you're familiar with park' but the it
has an internal data structure with
metadata defined at the top of the file
and it will tell you which which columns
are where they appear in the file so
again it can skip read and dramatically
reduce the amount of i/o you would have
to do anything that the
the storage handler in this case parquet
can't do we will do with smart scan
above that now so there's some bits that
we can push down into parquet and
there's some bits that we do in smart
scan to deliver the final result we also
do that with hive partition pruning and
with HBase sub scans so I was just a
very quick primer on Big Data sequel I
hope to get to a demo at some point on
that if we have time because this is a
code event and you should see some code
right so just to summarize what we've
said already about going from Hadoop to
the Oracle database we can use a Hadoop
job to do that in particular our product
called Oracle loader for Hadoop the
advantages are that it's using the
Hadoop CPUs minimal use of database CPU
we can pipeline it with other Hadoop
jobs you know if you want to create a
pipeline of activity we can do batch and
we can do continuous streaming as well
and then of course this can connect to
to an Oracle database running on any OS
platform it doesn't have to be a
particular operating system in terms of
another way of doing what we describe
with Big Data sequel the advantage of
using Big Data sequel is that it uses
both sides of the equation it uses
Hadoop CPUs and it uses Oracle Database
CPUs and the nice thing about it is that
you're using standard sequel and all the
tools that support standard sequel so if
you're using a bi tool any bi tool will
be able to query data through the Oracle
database that lives on Hadoop so the
nice thing about that is it's completely
transparent to end-users they don't
actually know whether the data is living
on in the database natively or whether
it's living in a dupe so you can hide
that completely from the user they have
no idea and of course the performance is
very very good real-time access to data
on both platforms the limitation here is
that it is Linux at the moment although
we are adding support for for Solaris if
there are any Solaris customers out
there going the other way we want to
take data from the database to Hadoop
again we've already mentioned we can do
this with copy to Hadoop and we can
deploy that with data
pump file format which I mentioned
already again highly parallelized
process if you want very high speed we
can also do this with with hive ql where
we can literally see the Oracle database
simply as another hive external table so
we can literally in real time go out to
the database and query it from PI of QL
or from spark SQL as well SPARQL so
that's really powerful because you don't
actually have to physically take data
from one environment and put it into the
other it stays in place
you simply query at query times get what
you want and it's quite high performance
generally as fast as the database can
provide that data to you we can get it
back to hive and spark and of course
spark being primarily in memory even
better if the volumes are not
egregiously high so in summary what
we're talking about here is two
approaches copy to Hadoop again it it
uses a minimum amount of database CPU
most of the works being done on the
Hadoop side you can store it in data
pump format if you you know if you want
an identical copy of what's in the
database and it's it's also great for
archiving and we can query it with with
oracle SQL and then what i just
mentioned a hive ql approach where we
access the tables in the database in
real time so i've gone through a few
things there and it gets a little more
confusing because of the way we
productize all s but if you want to
physically copy data we have a set of
products and if you want to query date
in place we have a set of products to do
that so the you're going from hadoop to
the database you can use oracle loader
for Hadoop or you can use Oracle Big
Data sequel we also have that other
driver I mentions a sequel connector for
Hadoop and you'd say well what what the
heck's the difference between or sequel
connector fer do an Oracle Big Data
sequel what the fundamental difference
is is that Big Data sequel is deploying
our engine onto the Hadoop cluster with
Oracle sequel connector for Hadoop it's
just using this native streaming
interface
the Hadoop provides you and so you get
relatively minimal level of performance
right if you want a high performance
solution you want to go with Big Data
sequel low performance sequel connector
for Hadoop yeah and there's a price
difference of course database to Hadoop
again we've already mentioned we have
that Oracle copy to dupe utility I
mentioned you can take tablespace
partitions and copy them over as well
and I don't think I mentioned Golden
Gate well I did from a change data
capture perspective capturing those
database changes and delivering them to
the Big Data Platform in so that's
anything that's going on in any
relational database whether that's
Oracle db2 sequel server mainframe
Teradata
we capture it and then deliver it to
Hadoop using Golden Gate tor to the
database if you if you choose query and
data in place is again we can use either
from the data if you're driving the
query from the database side we use
either the sequel connector for Hadoop
lower cheaper Big Data sequel faster
Google more expensive from the Hadoop
side we have that utility where you can
read within commit hive QL and spark SQL
directly Oracle tables as a data source
yeah so we bundle those some of those
things together into what we call Big
Data connectors so from a marketing
perspective there's a little bit of
trickery going on there Analytics wise I
just wanted to mention you okay now that
we've got this data in either place well
what what are some of our utilities that
help you work with the data we do Oracle
our advanced analytics for Hadoop which
I mentioned already we also have Oracle
X query for Hadoop so if you have a lot
of XML we have a an engine that runs on
the Hadoop side that does XPath X query
type of transformations very powerful it
also allows you to do XML extend hive to
handle xml stored in hive and i'll show
you that in the demo we also have a
visualization tool called big data
discovery which is a faceted search
interactive
discovery tool rank data wrangling sort
of citizen data scientist type tool to
help your power users navigate data in
the Big Data environment the database
side we also have the equivalent of our
called Oracle our enterprise we also
have the more classic version of Oracle
data mining which is done with purely
with sequel in the database we also have
native XML and JSON support in the
database I don't know if everyone's
aware of that but it's certainly
interesting spatial and graph where we
can do RDF graphs and property graphs we
have a visual analytics tool as well to
help you visualize interactive
visualization again so just kind of an
overview of analytic type tooling we
have available I want talk a little bit
about cloud because it's all about cloud
now so we have a very rich platform for
cloud application development now which
you starting maybe to get a sense of
today we have application container
cloud service Java cloud service we have
a forthcoming functions cloud service
which is serverless computing similar to
lambda AWS lambda if you're familiar
with that we have a mobile and chatbots
cloud service and we support lots of
various SDKs visual development we have
application builder and mobile
application builder process automation
and chat bot builder common services to
support that kind of application
development continuous delivery ie our
developer cloud we have a container
cloud we have a log analytics cloud to
do analytics of logs obviously we have a
Casca oriented service called event hub
cloud so basically Kafka in the cloud
and you just create topics and throw
millions of messages at it and away you
go
messaging cloud API management which is
quite interesting actually identity
cloud service to support
idaite identity across all of our cloud
services underneath that we have data
services and integration services so we
have a database in the cloud we have my
sequel we have no sequel in the cloud we
also have big data in the cloud and
that's why this slide is here because we
wanted to talk about the big data cloud
services integration services as well
and we'll talk about that and of course
we have I as underneath to support all
of that all of those above cloud
services and we can do cloud a customer
as well so we can deploy a subscription
basis hardware into your data center and
provide you all of these public cloud
services but privately within your own
data center but I want to come back to
Big Data what do we have we have an
on-premises offering called the Big Data
appliance we also have Big Data cloud
service I don't have time to go through
describing everything about it but I
think one of the interesting notions of
an appliance is that it's easier to
manage then your your own Hadoop cluster
so rather than trying to maintain and
run a Hadoop cluster which is painful we
we have a utility that helps you install
and patch that entire stack so when
there's a new revision of the cloud
error distribution for Hadoop you can
simply run the what's called mammoth and
it will upgrade the entire appliance for
you everything from Linux patches Java
patches CDH patches it's all handled for
you and of course thoroughly tested you
don't have to worry about installing
version six point three point nine point
seven point two point one point six of
coogee beau and it's going to break
something over there and it's all
handled right from a cloud perspective
we have two cloud services I've already
mentioned Big Data cloud service which
runs on that that appliance architecture
and we also have a computer dishin which
is fully managed Apache Hadoop and spark
in the cloud it's it's a similar
distribution to Hortonworks whereas our
other cloud services CDH cloud era the
difference here is that it's a highly
elastic scalable storage and compute
independently so rather than having
local disks attached to local compute
which is what the Big Data appliance
does compute edition
virtualizes all that so you can scale
your your cpu requirement completely
independently of your storage and you
can scale massively thousands of nodes
we also provide a zeppelin notebook
interface to help you navigate that
stack and what we will do is we will add
best-of-breed technology to that stack
for you things like Alexio and all flash
block storage and vme all that stuff
bare-metal it's all coming to computer
edition and it will just continue to
become more and more powerful but we
will decide you know what the best
debride is you don't have to worry about
oh should i deploy you know ignite or
should it be some other you know
technology and of course it will
integrate with all of our other cloud
services very quickly we also have a
bare-metal cloud service partnering with
a company called cueball so they
actually deploy hadoop environments and
we are one of the environments that they
can deploy to now so they can deploy
whatever stack you want really Hadoop
stack to our very metal cloud service I
won't go into the details but obviously
bare metal gives you the best possible
performance in a cloud environment that
you can get finally I just wanted to
talk about how do we access object
storage because object storage is
becoming you know very important when
you're talking about being able to use a
variety of cloud services against the
common data store the rather than having
your data locked up in specific
technology bundles object storage seems
to be the lingua franca where everyone
goes okay that's going to be how I'm
going to access data rather than you
know pushing it around so much so we
have an object storage cloud service
that is based on Swift and you can use
the Swift API to access objects stored
there and all of our cloud services have
interfaces to talk to our object storage
service so my event hub cloud service
can see it my Big Data cloud service can
see it my analytics cloud service can
see that object storage
so it's a way of sort of having a common
pool data reservoir data pool whatever
body of water metaphor you want to use
and make that accessible to lots of
different cloud services and there it's
also available via bare metal what you
tend to do is you will swing if you want
you know high performance you probably
swing from object storage up to you know
nvme storage and run some processing and
then swing back to us we just bring the
result back to object storage that's why
we sort of see the universe progressing
so I covered a lot of stuff there time
is it is 12 in so I have 15 minutes I
want to show you a little demo if I may
it's not the most exciting demo in the
universe because it's SQL right but hey
it's better than
right so I just want to make sure this
is reset so I'm ready to go
you sit everyone see that okay yes
so first thing I want to point out is
that I mentioned earlier we use hive for
our sort of our understanding of the
data set and the structure of the hive
table that you're going to be querying
so from the database I have a live view
into the hive meta store that's what
these queries are meant to show you is
that I can go out and say well from the
database what are all the hive tables
that I can actually see by the way this
is all running on my laptop in a VM
called Big Data Lite which is a VM that
Oracle provides with a lot of our
tooling inside the VM so it's running a
Hadoop cluster it's running a Big Data
sequel it's running the database it's
running the no sequel database so it's
got a lot in it and you can download and
play with it yourself if you want to
anyway that query I just ran went out
and looked interactively at the hive
made a store and came back and said
here's all the tables I've made a store
knows about who owns them what the table
name is is a partitioned etc so I can go
in now and select from another view that
we have which says ok for a given table
in this case called movie what are all
the column names what are the hive
column types and what are the defaults
Oracle column types so it does a mapping
a default mapping for you and you can
change that if you want to but
automatically it will do that conversion
based on what you see here so how do I
actually then query a hive table so this
statement here creates that table
creates an external table that's why you
see organization external in this case
it's a movie log tables a log of clicks
on a movie rental website in this case
the click is relatively unstructured
it's JSON in fact so it is semi
structured but we don't really know what
it looks like yet so we're just going to
give it a varchar' to 4,000 now I could
automatically create that table without
having to give it those definitions
because it knows what's there anyway but
in this case I've explicitly said what
it is and this is this is data that's
stored on HDFS so that's the HDFS path
to the directory for the files that
compose the movie log table so that's
it's that fast because all I'm doing is
creating a metadata definition yeah and
now I'm going up and saying ok
give me some rows back and it's going
out to Hadoop and pulling the rows back
to the database
so what you see there is one column
click the click column it's a big string
I can do the same thing now if I wanted
to do it from hive directly so let me
just do that drop that definition create
a new definition and instead it has a
type of oracle hive instead of oracle
HDFS and again it's still querying the
same underlying data right but you'll
see it's a big string it's a big JSON
string yeah so we have native JSON
support in the database now so I can
extend my normal sequel syntax and reach
into the JSON structures and pull back
columns of data rather than one big
string so I've taken that string and
decomposed it into the sub subtypes in
JSON yeah so I can do things now this
combines two tables the movie in the
movie log table movies in the database
movie log is that external table I just
defined this is going to join them
together and do a gross calculation and
compare it to the clique rating so I'm
taking data from two places combining it
in real time and that's given me the
gross for all of those top titles and
what the average click rating was yeah
this data is a little bit old now as you
can tell by the type movie titles I can
do the same thing if it is if it's in a
different format so that was in JSON
format I may have data in Avro format
but I can do that because I can simply
specify what you know how to handle
different types with different Surtees
yeah so like for now I've just created
the table in Avro and I can then query
that table in this case give me a give
me everything from that table where the
rating is greater than four yeah I can
also do that in our know sequel database
Tamara I'm n shinned how anything that
has a high storage handler we can query
well I have a no sequel database that
has a table recommendation and those
columns in it and I can go and I can
I can go and create an external table on
that and select from it
likewise I can mention I mentioned XML
earlier so I can go and I can define in
hive an external table called in this
case customer XML you notice here I've
got I've named my own serializer D
serializer so we provide an XML
serializer D serializer
it's part of Oracle xquery so I've told
hive use this sir D to map this XML to
these columns and you can see in the
table properties of the hive table how
I've specified the XPath of everything
that I want and relate it to specific
columns it's quite interesting and these
X paths that I'm showing you here are
very simplistic you can do arbitrarily
complex XPath expressions and all of
that gets parsed on the Hadoop side so
I'm just going to run that in hive to
create my hive external table over some
files of XML yep and then I'm going to
create my oracle external table that
points at the hive external table sorry
let me go back to over here
there's my external table again I've
just said type Oracle hive so it knows
and then I can do things you know just
simple query that grouped by income
level from that XML table and I can show
you that XML table over here we want
just just prove that I'm not there's no
trickery going on here he says where did
I create that then hit to get the name
here's where the demo goes pear-shaped
well you have to trust me it's there the
other thing that I mentioned earlier is
you we can automatically create tables
so you know how I've been doing the DDL
myself but I can actually call a PL
sequel procedure called create external
DDL for hive and you just give it some
parameters what cluster do you want to
connect to what what database name and
hive what hive table name and you know
what table name you want to call you
don't even have to set a specified table
name it'll it'll substitute it'll use
the same name if you want and I can then
say okay go and run that and actually
execute the definition of that table so
I have to provide even less information
than I would in pure DDL so the other
thing I wanted to show you because I'm
nearly out of time is how you can do
redaction on-the-fly so I don't know if
you're familiar with it but in Oracle we
can implement Virtual Private database
policies and we can redact data on the
fly in that policy so that if a given
user falls in a certain group a security
group or they have a certain IP address
or it's a certain time of day or
whatever kind of security policy you
want to create as part of that we can
say I want you to redact data on that
the user can actually see so in this
case on the customer table in the
database we have a redaction policy that
says don't show customer IDs just blank
it out with nines
so under underneath that of course
persisted is the real data but at query
time we're redacted likewise don't show
me the whole last name show me the first
two characters and then use asterisks to
blank out the rest of the data yeah so I
have on this policy that's applied for
this given user I can apply those
policies to data stored in hive and in
those sequel databases so this bit of
code here is the PL sequel that sets up
redaction policies against tables that
live on Hadoop yeah and in the no sequel
database and on that XML file that I
just created
so if I run that it will go out and
establish those security policies and
then if I query the data you can
immediately see its redacting data even
though it's coming from Hadoop can't see
the customer ID likewise from the no
sequel database you won't see customer
ID and I think with XML I didn't I
didn't redact the customer ID but I
redacted a customer name yeah but what's
interesting here is I'm going to run a
query now crab which joins all three of
those tables together so the one in the
no sequel database the XML file and the
file on HDFS on all of them will have
that redaction policy applied so I have
a with clause this is this is building
up on a recency frequency and monetary
query you know quite straightforward
thing you want to run what I'm doing it
across three different data sources in
real time I'm selecting from movie sales
there's the customer XML table I'm using
an end tile function to bring me back
you know five buckets in terms of sales
just you know see who falls into each
bucket and standard group buy and then
I'm looking at the click data and I'm
doing a couple of N tile functions over
that to determine how frequently the
customer visited us and what was their
latest visit and then this bit I simply
join the three the the two pieces
together and do a bit of math and a bit
of filtering where I only want customers
that are high value and who haven't
visited recently and I should get back a
result which gives me RFM across the
customer base if the bear in mind this
is running on a VM right so I have to
give it a little bit of latitude so
redacted data across those three data
sources and there that concludes my
demonstration questions
that's right right under my glass yeah
yes I'd not off the top of my head but
if you give me your name I'll drop your
line with with our performance benchmark
well I don't know if we've done direct
comparisons but the so there's there's
two different things going on right so
the scoop bit uses the utility that I
mentioned call Oracle loader for Hadoop
yeah but drill is more of a is more like
Big Data sequel right where it's
reaching up to different kinds of data
sources and the the challenge the drill
has is that it relies on the technology
of each thing that it's querying so it's
one of the challenges of data Federation
versus data franchising which is what we
call Big Data sequel because we're
deploying our part of our engine on the
node each data node of the Hadoop
cluster we can deliver much higher
performance than typically than tools
like drill can deliver your mileage may
vary
caveat caveat caveat right but by and
large we are faster because we are
deploying a self-similar engine on to
that stack rather than relying on you
know the underlying technology and that
is the caveat when I say we can talk to
anything that has a hive storage handler
as soon as you move out of the HDFS
realm you then become dependent on how
good the hive storage handler is which
is the same problem that drill has right
so you know if you want the best highest
performance possible and you deploy our
engine and use data on HDFS with hives
metadata on top it's pretty hard to beat
that from a performance perspective
particularly when you add all of those
features like storage indexing on there
and predicate push down if we have some
benchmarking we can share with you I
don't know if we have it specific
against drill though but yeah
anything else as you breathe as you be
yes can you actually predefined no you
can hydrate if you want by selecting
columns and tables or running a
hydration query and then it'll see it
and then go okay no and it will create
the in memory index that way but there's
no automatic hydration if you will you
have to push it a little bit to do what
you want it to do but I mean it'll do it
all on its own but when you first start
up of course it has no data because it's
in memory yeah
but not necessarily every column right
because you might have a table that you
know has 100 columns but most people
only ever queried five or ten of them so
it'll only build storage indexes for the
five or ten that get queried yeah so
it's quite efficient actually okay well
I hope you found it somewhat
illuminating thanks very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>