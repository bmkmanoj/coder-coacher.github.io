<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>From a Certain Point of View: Eye Tracking with Java(FX) | Coder Coacher - Coaching Coders</title><meta content="From a Certain Point of View: Eye Tracking with Java(FX) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>From a Certain Point of View: Eye Tracking with Java(FX)</b></h2><h5 class="post__date">2015-06-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/wVyTGXOniIY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so being an oracle employee we have to
put up this particular slide which
basically says that we're not committing
to deliver any particular product on a
particular date I pretty sure we're safe
in terms of we're not creating a
particular product around this but
you've had time to read that so we will
move on now in terms of the structure of
the presentation what I want to do is
start with a little bit of background in
terms of how we interact with machines
how we interact with computers because
the idea of using eye-tracking is
obviously closely related to that so
it's worth looking at some of the
different ideas before we get to the
idea of eye tracking then we talk about
the general principles of my tracking
how it works what you can use it for the
general ideas and so on and then we'll
dig into the details of the I tried
device and the Java API that's been made
available for that so you can see how
you actually create a simple piece of
code in terms of getting the information
from the eye tracking and then
integrating that into your application
once we've done that I'm going to hand
over Garrett who is going to show some
really good demos of eye tracking in
action so we'll start with human machine
interaction and it's interesting from a
historical perspective to look at how we
interact with machines and this really
is how it all started the typewriter it
was a way that we can actually use our
fingers and communicate by typing things
so obviously we could create a piece of
paper we could use piece of paper and we
could actually type things on that paper
and one of the things that's interesting
about this is the fact that you have a
particular layout of the keyboard and
some people actually believe that the
layout of the keyboard was designed to
slow people down because when the
keyboard was first developed the people
got so good at typing that sometimes the
the hammers that were involved in a
mechanical typewriter would actually get
stuck together but that's actually not
true the design of the keyboard is
designed to make it easy to type quickly
so that's part of the idea is that is
actually helping to make it easier for
you to interact with the machine and
obviously if you look at how it moved on
in time we have progressed we now have
nice shiny black plastic keyboards which
have lots more keys on them and they
have function keys and merit keys and
things like that and then Along Came our
good friend the mouse now we had a
different way of interacting with the
machine where we could move something
around on the table in two dimensions
and we had actually control cursor on
the screen and we could move that around
in order to control the user interface
and by using the buttons on the mouse we
could actually select things and
depending on how many buttons you've got
on your mouse you can do all sorts of
different things it's interesting that
for me personally because I'm quite
interested in how we interact with
machines and user interfaces that there
are certain companies like Apple who
insist on believing that you only need
one button on a mouse now I don't know
about you but I've got more than one
finger and I'm you know pretty dexterous
with more than one finger so I do find a
little bit frustrating sometimes that
why can't we just have more than one
button on a mouse it makes sense and
clearly the the Xerox PARC Mouse
developed into something there was a lot
more ergonomic in its shape and as I say
some of them do actually have more than
one button they have a scroll wheel
things like that so you can do all sorts
of things in terms of controlling the
interface again if we skip forward a bit
what we've seen now is that things like
multi-touch have become much more
popular and so Microsoft came out with
the surface the original surface rather
than the one that they've kind of
rebranded as their tablet and this was
the idea of having you know touch
sensitive screen with a projector
underneath and then you could put things
on it you could touch it in different
places and you could swing photos round
and expand them and do all those sorts
of nice things and really the massive
popularity of that has been promoted
through things like the iPad the iphone
now Android all those types of devices
so we're very familiar with the idea of
using multiple fingers that the cuckoo
there isn't there now the other thing
that's had a really huge impact on how
we interact with machines is gaming
because gaming requires quite complex
interaction with the machine you know if
you're doing word processing or
spreadsheets things like that the
interaction is fairly simple in terms of
moving to a particular cell or typing in
a particular place that kind of thing
but gaming requires a lot more complex
interaction so we've had all sorts of
things now anybody recognize this yes
the Atari joystick this is this is a
great idea because it allows you to move
things around and you know have a fire
button but that was clearly quite a
simple idea and we went a little bit
further with that so then Nintendo came
along and they added new buttons and the
ability to sort of control things at
your ways what do we have next oh yes
the dance pad so this was a different
way of interacting with things so rather
than using in hands you'd use your feet
so we're getting different types of
interaction then we have a very nice
sort of Buck Rogers looking data glove
where you could actually interact things
that you could mount the the buttons on
there and have the idea of actually
controlling things with your fingers and
that's kind of led really to what we are
very familiar with in terms of the game
controller where you've got these two
thumb joysticks and lots of buttons and
if you see young people now I can't
really classify myself as a young person
anymore if you see young people now
they're very very dexterous in terms of
the movement that they can get with
their thumbs and the fingers in terms of
controlling games and taking that even
further what that's really led to is a
movement away from using just our hands
to interact with them sheep because
gestures have become much more common
and so Nintendo led the way with this
with the week so the idea that you could
actually
the game in a much more natural way so
if you're going to bowl in the game you
actually move your arm back you hold the
controller release the button and so
it's much more natural in terms of how
you interact with the game rather than
having to use this kind of controller
and move the joysticks in particular
ways to control that the part of the
game different companies have taken
something different approach so Sony had
this idea of sticking our kind of
glowing ball on the end of one of the
controllers so they could track it with
a camera and that way they could see
where the controller was relative to the
game because the thing with the the
Nintendo is that although it has very
complex accelerometer and gyroscope akin
formation it won't give you an absolute
position so it's only a relative
position and Sony wanted more of an
absolute position so they tracked the
end of the controller using a camera and
of course that led to the Kinect which
does give you a lot more kind of
absolute position of the person because
they use infrared they have all sorts of
processing of the images that are coming
from that and that enables them to track
all the different parts of the body so
you can see our movement you can see leg
movement and as we know that can lead to
some complex interactions in terms of
the game and some of the things we've
been looking at more recently is taking
that kind of technology but moving it
back into the idea of user interactions
again so anybody here familiar with the
leap motion there so the leap motion is
like a little infrared box in fact it's
very similar in sort of look to this
it's just a bit smaller but what it does
is allows you to track the position of
your hands in space so it's kind of like
a mouse but you don't actually have to
hold anything and you have very fine
resolution in terms of the movement that
you can get which enables you again to
interacting in a different and
potentially more natural way with the
user interface although I have to say
that when you start getting into this is
holding your hands in space above the
thing is actually not a very natural
thing to do we tend to I put them on
or you know interact with some kind of
object and then there's another company
family labs who have come up with a
thing called the Myo anybody seen the
Mayo this is really interesting bit of
technology because what it does it
basically have an arm band that goes
around your arm and rather than using
the approach that the Nintendo does in
terms of having solar ometer and
measuring the the movement of the band
what it does actually have that but it
also measures the electrical activity in
your muscles so you can actually do hand
gestures and it can detect what the hand
gesture is based on the muscles that are
moving in your forearm so that you don't
actually have to be anywhere near the
device and you can just make hand
movements and then they can be picked up
as gestures into the application haven't
had a chance to play with that myself
yet but I know one of my colleagues has
got one and it's something that does
look very interesting in terms of how we
can interact with machines so all of
these things so far are about well with
the exception of the dance pad I suppose
all of these things so far are about
using our hands to interact with the
system and that's very natural because
we do use in our hands a lot in fact we
have opposable thumbs and that kind of
thing in terms of evolution has meant
that our hands are very much the primary
focus for how are we going to interact
with things but what about if we don't
want to use our hands to interact with
the system what kind of things can we do
well one of the things that you can do
is you can actually have a system that
monitors your brain activity and so
again there's another company called
NeuroSky who I've actually had some
experience working with and they produce
this little to the headband thing that
attaches to so you've got a sense that
mounts on the your forehead you've got
another one that clips onto your ear and
what that allows them to do is to
monitor the
EEG activity in your brain with the idea
that you can then use that to control
and interface does anybody actually
tried that no okay I have to say that
it's one of those things where you think
wow I could just think about something
and it would happen it doesn't quite
work like that because you think about
something and you get this kind of
variation in that the values but it's
very difficult to actually control a
specific thing with your brain using
this particular system so I think we're
still quite a long way from
thought-controlled activity I know there
are other companies investigating this
which really kind of leads us to what
we're talking about now which is using
your eyes because clearly we use our
eyes a lot in terms of receiving
information we use our eyes a lot in
terms of what we're actually looking at
so there's a lot of potential there for
how we can interact with the user
interface and how we could actually
bring that information into the system
that we're working with so let's talk
about I tracking there and let's talk
about the details of this and what's
actually involved in doing this so this
is what wikipedia says about eye
tracking I tracking is the process of
measuring either the point of gays or
the motion of I relative to the head
that's quite important because there are
really two distinct things here there's
either thing where you're looking at
what are you actually focusing your eyes
on so the point on the display that
you're looking at but there's also the
idea of what the motion is relative to
to your head and that's quite
significant because a lot of extra
information can come from that in terms
of what we can do with user interfaces
and so what you find is that from a sort
of physiological perspective eighty
percent of the the information stimuli
that goes to our brain comes from the
eye so that means there's a lot of
information going into the eye
that we can potentially work with that's
really important from the point of view
of what we're going to do in system and
then there's another thing being a
little bit more poetical which is that
the eyes are the windows to the soul now
you might think well why is he saying
that in a technical presentation but
it's actually there's there's a real
element of truth here in terms of what
we can do with our system because yes
you know the eyes are the window onto
the soul but from a psychological
perspective that's actually quite true
so if we look at somebody who is looking
at you and you ask them a question you
can actually tell quite a lot from what
that person or how there's that person's
eyes move when they answer the question
so if you're a right-handed person and
so what I'm going to talk about now is
for a right-handed person if you're a
left-handed person these things are
reversed but for a right-handed person
if I were to ask you a question like
imagine a purple elephant chances are
your eyes are actually going to move to
the right and up because that is that we
typically do as humans if we're visually
constructing something in our mind so
we're imagining something that we
haven't seen before if I were to ask you
a question such as can you remember the
color of your first house what you would
do then is you're more than likely to
look to the left and up because
psychologically and physiologically what
you do is you tend to do that when
you're remembering something that is
visual so you're trying to remember a
particular colors and you've seen that's
what your eyes are likely to do
similarly if you're talking about sounds
if I try to get you to imagine this out
like what's the highest sound you can
possibly here your eyes are likely to
move just to the right rather than the
left and more in line rather than up or
down same thing if you want to remember
a particular
so if I said can you remember the sound
of your mother's voice your eyes are
going to move to the to the left and
then for other things if it's not
something that's visual if it's not
something that's a sound if I ask you to
remember the smell of like a campfire
what your eyes are tending to do art
move to the right and down and then the
last one which is the moving to the left
and down is if you're talking to
yourself in your mind so if you're
having an eternal discussion that's what
your eyes are going to do so that the
upshot of this is that if you are
somebody a question and you look at
their eyes depending on which way their
eyes go you're going to tell roughly
it's not at a holler but you've got some
kind of idea what that person is telling
you from the point of view of whether
they're lying or not because if their
eyes tend to move more to the right then
they're constructing something in their
mind it's not a remembered thing it's
constructed thing therefore it's more
likely to be a lie than if they're
remembering something so this is
something that we can use in terms of
user interface because if you ask people
questions and you look at where their
eyes are moving when they respond to
those questions that can you can
actually get information and kind of
include that into the user interface so
where could we use eye tracking what
sort of applications could I tracking be
applied to well there are a number of
different places we could use that one
of them is the idea of video
conferencing and I think I find it's
quite strange because I use like Skype
quite a lot on my laptop and what you
find is that you've got the skype window
on your display and you can see the
person in front of you and so you're
looking at them but of course the camera
is at the top of the frame so when they
see you they say you can looking down
rather than looking at look so you think
you're looking at them but because of
where the camera is you're actually
looking below so you're not getting that
normal
I contact that you would get if that
person really was in front of you what
you can do is if you can track where the
person is looking using technology like
this you can actually kind of fake the
movement of the eye so you can kind of
like fake the video as it's coming
through and kind of make it appear but
the person that you were talking to is
actually looking directly at you even
though the camera is separated from the
video feed that you're looking at so
that there's one potential use of eye
tracking another is where we have very
mission critical safety systems so if
you look at something like air traffic
control there's lots of information
being presented in front of the person
who's doing the air traffic control and
it may be that they're looking at a
particular plane and they're making
decisions about that plane but they've
made a mistake in terms of what they're
looking at so by using the eye tracking
we can actually correlate the
information between what the person is
looking at and the decisions they're
making so we can kind of double check
things and sort of say you know this is
flight I know 42 are you really sure
that you're talking flight 42 because
you're looking at flight 89 and so if
there's there's errors like that that
come up you can use this kind of
technology to double check things and
then alert the user to the fact they're
not actually looking at what they think
they might be looking at because they're
making a mistake so again that's
something that could be very useful in
terms of using this technology
flipping it around in terms of not just
controlling the user interface what
about if we wanted to monitor what
somebody's looking at so if you're
developing an application which is a
visual application you may want to see
how the person interacts with that game
in terms of what they're looking at so
what are the things that attract their
attention more than others and you can
produce heat maps of where the person is
looking so you track their eyes and
their gaze over time and then you can
kind of generate a heat map and
garrett's going to show us an example of
that later on so you can see where
they're looking at when they're playing
the game and that way you know what
things are working what things are not
attracting their attention and taking
that sort of one step further we can
also apply to things like advertising
and products so you can put a set of
products in front of somebody in a
picture get them to look at it and you
can use the heat map to determine which
ones are more visually appealing and if
you take you know big sample people you
can figure out which things are going to
get the attention more than others and
if there's particular colors that people
like or particular patterns you can use
those in your products to draw people's
attention to them in a sort of subtle
way because you monitored what it is
that people tend to like and so you can
design your products and the packaging
to attract people's attention and of
course we can you know do the good old
Tony Stark kind of thing the Iron Man
approach where if you're in a situation
where you're not going to be able to use
a mouse or a keyboard because you're
flying through space or whatever then it
could be very useful to have the ability
to look at specific things and control
interface directly using that kind of
approach
so in terms of the eye tracking what
actually can you do from more of a sort
of technical perspective what sort of
values are you able to measure well the
first thing you can do is you can
actually determine whether the user is
actually looking at the screen or
looking off somewhere else so if you've
got somebody doing a training course and
you know it's one of these mandatory
training courses that you have to do
then you might want to have a system
that tracks to make sure the person is
actually paying attention to what's
being presented on the screen rather
than to the drifting off and looking at
at the window or doing something else so
that kind of thing can be very useful in
terms of being able to determine are
they looking at the screen or are they
looking somewhere else is the user
reading or is the user just scanning
what's on the screen so again you can
you can look at where the user is gazing
but also what speed that gaze is moving
across a particular piece of text and
that way you can determine are they
actually paying enough attention to be
able to read that text or are they
moving so quickly that they're just
scanning you know every other word or
every third word or something like that
and that will give you more information
about whether they're really interacting
with what you're presenting to them you
can get things like the intensity of a
user's games and this kind of comes back
to the heat map that we saw a couple of
slides ago where you can see whether
somebody is focusing on a particular
point on the screen for a long time and
like I say that kind of information can
be very useful in terms of determining
what things are more appealing visually
to a user than other things and then you
can even take that further and you can
actually determine whether somebody is
searching for something in particular
because they may be moving around the
screen a lot until they find something
that they're looking for and then they
focus on that particular thing so
there's all sorts of different ideas and
different types of information that we
can get from this
now there are different types of eye
tracking technology there's really kind
of three different technologies that you
can apply to this one is attached to the
eye one is for measuring electrical
activity and the other is an optical
tracking system so if you look at the
three different types of system as I say
there's ones where you can get literally
like contact lenses that you can put
into your eye and they have sensors
built into them that can measure
activity and they can use like a
magnetic field so as they move through
the magnetic field you can detect
movement by the fact that you can induce
an electronic electric current in the
coil or whatever and so it allows you to
get very high precision in terms of
where somebody is looking because you're
actually tracking exactly where their
eyeball is and so you're not reliant on
them looking at a particular direction
anywhere that they look you will be able
to determine that information it does
allow for measurement in all different
directions so it gives us not just a
high precision but also high degree of
accuracy in terms of the different axes
as well the drawback to this is it's
very expensive it's also kind of a bit
of a hassle because if you need to wear
contact lenses to actually enable you to
determine this information it's not
something that you're going to be able
to put in front of a lot of different
users electrical potential measurement
is more about sort of measuring like the
muscle movement in your eyes so or the
muscles around your eyes so you can
actually do things like this with sleep
research because it monitors the muscle
movement even when your eyes are closed
and that's a bit different from the
other systems because you're not just
measuring the movement of the eyes when
the owners open you also measuring it
when the eye is closed doesn't have as
much accuracy because of course what
you're doing is monitoring muscle
movement and you have to translate that
muscle movement into the the real
direction that the person is looking at
and so the position is not really that
good for that kind of thing and again
these types of systems are expensive as
well so in the third one is optical
tracking which is what we're going to
use in the examples we're going to see
in a few minutes and what that does is
it uses infrared in the same way of
things like to connect things like the
leap motion due to track where you're
looking so it doesn't require you to
wear anything it doesn't require to put
contact lenses in sensors or anything
like that so you can put a sensor in
front of the screen and track people's
eye movement without having any extra
kind of stuff for that works well in
terms of both fast and slow I'd movement
and works well when you're looking at a
particular thing was if you're looking
in different direction that's not going
to work and the really key thing here is
it is a cheap solution so if you're
looking for some kind of eye tracking
where the type of application can use
this kind of thing then it's it's a very
good solution from a cost perspective so
how does optical I tracking work well
it's actually quite straightforward what
you do is you have your eye tracker on
the right and then you've got your eye
on the left now if you look at your eye
you have an eyeball so clue is in the
name it's like a sphere and if you look
at the structure of you I you've got the
lens on the front and then you've got
the retina at the back which collects
the light in order to turn that into the
information which is when your brain
interprets so the eye tracker basically
sets up an array of infrared LEDs which
shine out from the tracker because it's
infrared our eyes don't respond to that
frequency of light so we don't actually
see it if you look at it through like an
infrared nightscope or even some digital
cameras you would see it but in terms of
you is you won't see it normally and
then what the infrared does is it
basically just produces a field of
infrared light that goes
to your eye and then your I will reflect
it back so you have a sensor mounted in
the eye tracker it knows which direction
the infrared is pointing out it looks
for reflected sources of light and it
can use that to track the position of
your eyes and the reason this works is
because your eyes are very reflective
and you may have seen things like this
before it's called red eye so if you
take photographs of people and use a
flash you tend to get this red eye
effect and that's because the retina on
the back of your eye is very reflective
and that means that for infrared light
as well we get a very good reflection so
you do get to very bright points of
light which makes it relatively easy to
track that information using sensors
because they do actually show up very
well so that's the ideas behind optical
light tracking so it's let's talk a
little bit about the I tried this device
that's been created to do this cheaply
so this is the eye tribe like I say it
looks kind of like a little much little
bar which has got some infrared LEDs in
it and you can put it basically in front
of a screen or you can put it in front
of your laptop or something like that in
order to track the position of your eye
from a technical perspective sampling
rate is over 30 Hertz or 60 Hertz so
it's very good in terms of frequency you
can certainly get plenty of information
about where the user is looking and
respond very quickly to changes in their
view you're not looking at low sampling
rates here accuracy is pretty good i
mean it's about half to one degree and
if you're looking at something you know
half to one degree will give you a
reasonable level of accuracy over the
kinds of distances that this system
works oh distances are somewhere in the
region of 45 75 centimeters which i
guess is about in older you in old money
sort of about a foot and a half to two
and a half feet I guess
something like that so it works very
well if you're sitting in front of a
display tracking area is a reasonable
size certainly if you're you're sitting
in front of the laptop or a display it's
going to be able to track somebody who's
not moving around a lot if you're
running around then it won't be able to
track you but if you're sitting in front
of the display it won't be an issue
screen size is supported up all the way
up to 24 inches so work with really big
displays because if you look at the sort
of field of view it's not that huge so
it won't work with really big displays
they have support for number of
different bindings of the C++ there's C
sharp and there's Java and it does use
USB 3 I have to say that the power
requirements for this are quite high I
need not empower acquaintance so much
the the speed and the amount of data
that comes through is quite high when I
had it connected to my machine and I was
doing some development work on here i
also have an external mouse which i have
plugged in and that uses a little dongle
that plugs into the USB i noticed that
my maps was was kind of not really
responding as well as I wanted it to and
I finally discovered it was because the
USB 3 port was being hammered so hard by
the data from the eye tracker but it was
interfering with the amount of
information coming from the mouse so
just something to bear in mind if you're
going to use this that you might not
want to have other things connected to a
USB port that actually also going to
generate a lot of information as well in
terms of the architecture and how the
the software works it basically uses I
like a server so you have to start up
the server first and that's the thing
that actually interacts through the USB
port to the device reads all the
information processes it figures out the
coordinates of where you're looking
what's going on and that does all the
hard work for you so there's the hard
work hardware side of things there's a
tracker server that runs and then
there's a network that you can connect
sushi you can actually have a network
connection to a different machine if you
wanted to all of that is handled
internally so that initially
is all of the the framework and the
devices and all that kind of stuff is
handled by that what you then do is you
have the different wrappings or
different bindings that sit on top of
that depending on whether you want to
use Java or some other language but of
course we all want to use Java so we
just use the Java binding for that and
then the API interacts directly with the
server to get the information and
provide that to your application so that
all appears in terms of the information
so let's talk a bit about Java and I
tracking and see actually how you use
this and from a coding point of view
it's actually fairly straightforward so
essentially you first of all make sure
that you include the appropriate java
package which is the Commodore I tribe
client that's easy that's pretty
straightforward and then what you need
to do is you need to have some piece of
code which implements the eye gaze
listener now clearly you can actually
implement it directly in your code or
you can use a lambda expression and do
it using a simpler way if you wanted to
but in terms of implementing the
interface what you've got to do is
basically implement that interface and
then you've got to initialize the eye
tracker so from an initialization point
of view first thing you have to start up
the server because if there's no server
there's nothing to talk to you so you
start up a server separately and then
when you want to initialize things you
basically talk to the wonderfully named
of gays manager you talk to the gays
manager use a factory method and you get
the instance of the gays manager which
talks to the server and then you
activate your connection to that and
place that you've got to pass two
parameters to that first is the
diversion number that you're going to
use in terms of interaction and the
second is the way that you want to
interact so there's both push and a pull
way of working so you can either let the
system give you events or you can poll
separately so it depends on whether you
want to
use a push or pull model having done
that you then add your listener to the
the gays manager to say this is the
listener that I want to be notified of
events when actions actually happen so
as I say the frequency of events is over
30 Hertz or 60 Hertz so literally 30 or
60 times a second your code will be
notified there's some information about
where the person is looking and in terms
of implementing that interface there is
just one method that you need to to deal
with which is on gays update and that
will provide you with some gays data and
then within that what you can do is you
can extract from that the information
about where the person is looking so you
get coordinates of their their gaze on
this screen and use that in whatever way
you want to and in terms of stopping
system because you have to disconnect
from it so you rue the listener and then
you deactivate the connection so that
you can end your application and and get
actually well stop your application it's
not the eye tracker so what data you get
so we've got our listeners set up we've
got our gaze data coming in what are we
actually getting well you get a number
of frames so you've got the frames of
data that are coming basically like a
packet and from point of view of the
information there you get really for
different things you've got the tracking
state which gives you the information
about whether somebody is in the field
of view so whether somebody is actually
looking at the system or not you've also
got the fixation which is looking at
whether somebody is you know fixated on
a particular part of the screen which
can help you to determine how much
they're moving around the screen if you
don't want to look specifically the
coordinates and then you've got both the
gays coordinates and the pupil
coordinates and this kind of relates
back to the idea of having in effect
two different things because you can say
okay what are they looking at on the
screen but also where are they relative
to the screen so it's like the two
different ends of the way that you're
looking at the screen and so you get
those as x and y coordinates the one
thing if you're using the the gays
coordinates on the screen is clearly you
have to calibrate it so you put the
sensor in front of the screen and then
you basically come up with a calibration
screen and it says look at the dots in
each of the corners we've got the dot in
the middle and then it uses that
information to figure out where the
screen is relative to the sensor that's
actually detecting the information and
that way when you get the information
you get the coordinates based on the
screen and so it's actually reasonably
accurate there's only one slight
drawback with this which is actually the
amount of data that you get and
sometimes it can be problematic because
it's so much like you're flooded with
data and so you need to kind of slow
things down a little bit in terms of
your application so it's 60 Hertz
there's a lot of data coming in and what
you can do to get around that is
basically use some kind of timer in this
case we use the animation timer to put a
pause in so that you're only getting
information once every so often into
your application and that way you don't
kind of thrash the processor just
dealing with the only tracking data and
your application doesn't get a chance to
do anything else so just to kind of
before we move on to the demos just to
kind of give you some conclusions and
places to live more information my
tracking does provide a very different
way of interacting with your system
because you don't use your hands you
don't use your body to interact with it
it's just where you're looking which
allows you interact with the user
interface using that to control the user
interface is actually quite hard because
you you've got to teach yourself in
effect to stare at particular places on
the screen and you find when you start
using this that your eyes move around
more than you think they do in terms of
looking at the screen you might think
you're staring at particular spot on the
screen and as Garrett will show you in a
minute it's actually not the case that
you're staring at one particular point
on the screen your eyes tend to move
around quite a lot many potential
applications so not just in terms of
like advertising checking out what
colors and what patterns are appealing
to people but also you know for people
who are physically challenged if you've
got the problems with using your hands
then using eye-tracking can be very good
from the point of view viewing allowing
lots people to use different systems as
I said reducing potential problems in
critical systems double-checking what
people are actually doing in terms of
interacting with like air traffic
control and those sorts of things are
all very good uses for this if you're
interested in finding out more about the
eye tribe you can only go to www I tried
calm and there's also a developer part
of that site which is dev I tried calm /
Java so that is basically the slides
part of this so now i'm going to hand
over to garrett's who's going to give us
some demonstrations can use ok let's let
me try like this I will work it up and
down the pike so the biggest problem in
doing the eye tracking stuff is
calibrating the system there is a
calibrating software there but the
biggest problem here for example I I
don't get really good calibration values
because you have to
better ah because the the biggest
problem is like I said the calibration
you have to keep your eye your eye
straight to the points and don't move
the head and then the light should be in
the same condition as in the way you use
it later on when you use the application
which is a little bit hard and then you
have reflections from other lights so
that makes it tricky to set it up
correctly and I tried it during the top
yeah now it's really you you have to set
up the tracker and then it has to figure
out where's your head related to the
screen and then you have to watch it
spots and then you calibrate your
position now to the screen so but as
soon as you move like this then it
doesn't work anymore so that means you
really have to figure out where is the
right position and maybe I have it open
so I can show you oh it stopped working
bright just yeah it's it works kind of
most of the times every know ah here we
go you see it's really can you see that
okay you can see that it can detect if I
close the eye and if it's green it's
good if it's red if I'd move like that
then it's gone so you really don't have
to move and that's the hard part so this
is their application so I try to do that
in in Java let's see if I get it running
yes well that's just that you can say if
you would like to use nine points for
for calibration or 12 or 32 so it's just
make it more accurate but that's all so
it's a let's see okay as you can see
this is really the simplest one but you
see the when I try to focus on the mouse
pointer it's really hard
it's hot you see it that's what they say
its fixation but even if you think you
look exactly at the spot your ice cores
always like this they are jittering this
is normal it's it's not yeah so this is
but this is really a problem and you
come to real applications so the next
one that I created was and that is the
most important thing right it's you
think about it you're here I tracking
then it's about images looking at images
see what happens so this will now
collect data for 30 seconds when I look
at this image which is a really nice car
and I really love it so now let's see
what I'm most interested in you can
guess what it would be so this is a it's
a little bit half of me to talk and
watch it this don't move to my eyes to
you sorry but we will see how it works
out so it's five seconds let me see and
I don't collect continuous data I just
do like peace ah you see here is the
that's my heat map yeah it's yeah a
little bit accurate let's name it like
this because I you never know if it's
now in the calibration correctly so but
it works for the heat map that's okay
that's the most the stuff that most of
the people know right having some images
and i can tell you i know i know i'm
male yeah I met this test with some
images that i can't show here but it is
it's really interesting alright it's
really interesting to see right but if
someone talks about eye tracking you
directly think what on it's interesting
but i tried that and it really is okay
so what that is something is this i'm
also interested in race car so that i
use that it's better so that's the easy
part so then you heard all that rumors
about this flat UI thing coming up last
year so i thought it might be a good
idea let's see what happens if i take a
look at two images they should show the
same but on the right side you see it's
flat UI on the left side is the more
fancy former i can't release pellets
core morphic design so it's a
it's interesting to see but if you take
a look at these images you will figure
out that the left one is so much more
attractive to the eye and to you search
for details on that image and the right
one our that's totally clear and that is
a good example where flat you a flat UI
really make sense because it's a you
focus on that on the facts not on the
details right you see I took a look at
this image the left one I'm really
interested in all the details in the
background interview it keeps it takes
you away from the real thing right it
was just an on switch but you keep
looking on that thing because it looks
so great but then if you have a whole
app with all these things on it then you
can imagine that flat UI I mainly is
more focused on the part that you would
like to do right so it makes sense
sometimes also the old style makes sense
but it it's interesting to see that
stuff well then next one would be I
haven't have seven demo so I have to go
a little bit quick through the demos
because just 10 minutes left i created
the heat map demo i hope it works yeah
so i have the mouse i can now do
something on my and this will directly
go to the to the heat map data so i can
type something and then i can see what
happens if i type it it's might not be
really calibrated correctly you see all
the spots a little bit to be no heart it
was really hard to get that really
working correctly because i collect the
data from the eye tracker and parallel i
also use the mall stuff so but you can
use that to keep an eye on how users use
your interface what are they doing when
they use it in what way are they looking
at so if you have for example at some
images on the upper right corner you
will see people tend to take a look at
the images and if you have big letters
on top they firstly will go to these big
letters and and check this so that's
really interesting and then I also of
course you can move to other types of
sports and for example if you can use
big sports you can have different for
the heat map different color schemes
that I implemented it makes it it
depends on the use case it's really
that's really useful for analyzing you
eyes though it's really and you can also
use that true for mouse clicks or
whatever events you would like to
collect but with the eye that's really
interesting approach to see where people
looking at when working with your
software and it's just an overlay I just
over lated over the application and then
it tracks the data in and shows it there
alright so do you see it so yeah it's
interesting let me try to focus on one
spot it's more left doesn't really work
ok so but you get the idea right so you
can really track your eye during the and
it's maybe a little bit 22 right let's
make it like this better to see the
interface so it's really fascinating to
see that if you if you work with the
application where where you're actually
looking at so that that's nice that's
nice playground but when i started with
playing around with that thing the
really most interesting part was
figuring out can I control the software
only by my eyes and what does it need to
do that so you can imagine I don't want
to use my hands I just would like to sit
there think about the information kiosk
somewhere so how can you do that how
would you select something just with
your eyes you can blink with the ice
that would be possible but I figured out
that is really sensitive and it's really
hard to figure out was it really
blinking with the eyes or was it just ah
just to take your short look away and
then suddenly something happens so I'd
create it a different way of doing that
it's let me show you it's like that you
see the point where i'm looking at and
then it asks you questions if i take a
look at this thing long enough it
selects it right so that's the way how
it is so now i can if it's too short oh
this job are or what's that a cotton in
oh yeah I'm not interested our Java
that's good and I'd take a look at Java
and if it's very clearly calibrated then
it should stay in the yep so you can do
things like that but to be honest I
didn't really had the idea I found it on
the web that was a designer who created
a control like that and just showed
images and so I got the idea and then
created to control that is able to do
that self and that it's really it's fun
to see that but even more fun that leads
me to the last day more than we might
have some time for for questions is
therefore I have to show you something I
found it on the web there was I was
thinking by myself how can i use it for
really something useful and i found a
little question le about let me see it
was about I what kind of race driver are
you write it must be related to
motorcycling or race driving something
like that so it's here let me show you
the image first so then you get on here
it's this one so there are have been
some guys who put up this sheet right so
it's always yes no questions which makes
it perfect for this kind of control
because I always can go to the left or
to the right and select something so i
implemented that thing and yeah let's
try my usually my idea was to let Simon
do with this stuff because he's also a
petrol head so but i will now do it
because it's roughly calibrated so lazy
it's not too late yeah so are you a good
driver mmm shuai hard to get it right oh
you're more on sure
let's see oh you see that's it's really
clearly ah so I should do a really cross
in the sbarro that's that's what he
recommended me to do and I did you saw
that there's a lot of different things
and it will restart in 10 seconds of 15
I isono can try again and if you have
who wishes can come up later on and then
we can can try it so it's really fun and
I also implemented a third demo which is
a map stem or just to give you an idea
but this is quite unstable we can try
that and this is now what I do is I crap
my I coordinates where I'm looking at
and I control the mouse pointer of the
software by using a javafx robot class
and you see here you see it zooms it
shouldn't do that now I can look
something and somewhere and then I use
my left eye blink and it zooms in ok
let's try to so it's now I can also use
pan and go to some point and then I can
go to let's do try new york or something
oh that's hot
ah damn it are we getting close okay
getting there oh the internet is for
good the problem at my place was the
internet was so slow that loading the
map was really a mess okay so you see
you can do stuff like that also only
with using the eyes and I use now one
key but you can imagine if you know the
leap motion controller and it could be
totally touchless you can use gestures
and your eyes in combination to do and
you see it's really sensitive when
blinking so it goes like oh and the
biggest problem when I started that was
I control the mouse pointer right i
forgot to switch that off so i can't
control the mouse anymore I couldn't
stop the software running so I couldn't
do anything so I did that now it works
so yeah this is what I have to show so
we have four minutes left for questions
if there are any so I don't know yes no
I want to do is I just collect the last
15 days for the coordinates and just do
a linear average so that's very simple
they may use a different approach and
there you saw the calibration software
which was really stable so they you can
do that of course you can implement all
that stuff yeah yeah yeah let me show
you this one to one more time because oh
sorry and the question was if if you can
more smoothing out the coordinates
rights because it was jittering in my
approach but if you take a look I can
open their original one just takes a
second to pick it up so here you see
it's really smooth so you can do that
and if you open their calibration
software it's the screen then oh you
can't see the I can switch it on I
didn't do that so that's the way it
works right you just have to follow
these coordinate look at the points and
then they collect the gays position and
then they calibrate it but it can be
really smooth it's up to you because you
get really the raw coordinates and that
means even sometimes the eyes are
jumping around and this thing will
recognize but it's really hot you have
to filter them and they have some kind
of smooth coordinates already but even
this ones are chittering so it's true
and this is normal you can't really do
it without any chittering at all so then
it gets slow following the eye and yes
yes the question is if it's really my
eye if it's triggering or if it's
something related to the output so no
it's really your eyes that's how your
eyes work so you really go like that you
don't recognize that but that's the way
it works they are really there says I
read a lot of studies about that when we
did this session and that's really
interesting there is something from the
1970s where someone really tracked all
that and then you see it's really like
chittering around all over the place and
what is it it's a carding yeah right
that's that's a work you guys do because
yeah it's good scoring in your eyes
actually have to do it because of the
fact that the retina loses the the image
so in order to keep the image fresh on
your retinal you actually have to move
your eyes around so they're changes the
view slightly and your brain filters so
your brain is filtering that out so you
get a stable image but of course what
you then need to do is to filter that
out on the software in order to remove
the the jitter that's happening yes
continuous calibration or something yeah
yeah so the question was if I can
improve the calibration somehow sure you
can so you can I use just the given
calibration to it but you can calibrate
it by yourself so there you can
implement a routine to calibrate it and
then you can take into account the
position of the head and all that and
you can do that continuously sure yeah
sure yeah it's possible it's definitely
possible any yes over there
to question what is this glasses of
defect declaration anyway
no but it works with glasses
it's kind of general calibration so the
question was if it's a user specific
thing the calibration know if it's if it
works in general so it should work
because the difference between the I
distance for different people is it's
really small so it should work in a you
know yeah in a good way but to make it
really perfect you should calibrate for
yourself the most important thing when
doing the calibration is the position in
front of the screen so because if you go
a little bit number on the back or in
the front and it makes a huge difference
so this is the that's the only thing you
have to take care about right any other
questions oh yeah right here well I
don't I don't know to be honest I don't
know for sure but infrared is used
everywhere so if there would be any
damage we would know it's not it's not
really New it's not a new technology
that's known for years how to do that so
that now it's just cheap to produce
that's the difference so I think it I
hope it's nothing it's not dangerous so
there was one question in the back yet
we're drawn in the background you mean
well then this is no standalone
application they just have the
calibration software so you can write
your buy yourself something that stands
in the background and tracks that and
then don't have to implement it in your
code sure it's possible it's just the
second program running that's possible
yes ask question yeah so the question
was if it could go faster than 60 Hertz
you know it count then no not this tool
but there are other tools from Toby
which is also quite popular I think they
have also tools that can go faster than
this but let me tell you 60 Hertz it's
quite fast you get a lot of data out of
that thing and the hard part is to
analyze it filter it in real time so I
think you won't get it in 16
milliseconds and we got the signal that
we should close the session Oh so thanks
for attending</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>