<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CON3538   Apache Lucene for Java EE Developers | Coder Coacher - Coaching Coders</title><meta content="CON3538   Apache Lucene for Java EE Developers - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>CON3538   Apache Lucene for Java EE Developers</b></h2><h5 class="post__date">2015-12-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8aS0eUuXdXU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome everyone very pleased to see so
much interesting leucine and I really
hope you get out of here knowing taking
a lot out of this so just to quickly
introduce myself I work for a dad where
am I principal software engineer I'm
Dutch an Italian but I'm living in
London and in terms of team organization
I am employed on the hibernate team so I
work full-time on hibernate I am leading
the hibernate search project which is
essentially integration for the scene to
make sure that everybody can use you
seen in a nice comfortable way from
patterns that you are very familiar with
and I also work on hybrid am of course
I've been redesigning the query parser
the HQ l language recently and work on
hibernate ogm which is the no sequel
integration for hibernate so you can
store things in MongoDB in Cassandra
we're playing with Neil for je and many
more I also contributed to infinite span
which is a memory grid and the cash and
what I do there I'm mostly interested in
the routine integrations again so I
started contributing to infinite span
because i needed these components like
there was like a hole in the
functionality that they could achieve
from the same point of view this was way
beyond the working for reddit actually
it was pushing the limits of loosening
cloud environments like eight years ago
and things could be much easier if I had
some additional pieces so I also help
with white fly now because that's well
what we're hibernate is mainly
integrated with and J groups because
it's again it's one of the core
components of clustering both in white
fly and in hybrid search itself but also
in a Finnish pan and of course I
contribute to apache Lucene since i am
participating in this community since 10
years approximately now mostly as a user
but you know I've been in this community
for so long that I am I would say a good
friend of many contributors and
commuters of there so what are we going
to talk about
a quick introduction on a leucine itself
sorry oh sorry okay so we are going to
have a quick introduction on leucine and
show exactly what it can do for you what
kind of problems it solves and then I'm
going to show you how this is integrated
with hibernate or with JP I applications
in general but not only we're going to
see how this interacts with infinite
span how you use these components from a
white fly point of view and I'm going to
show a bit of what my experience has
been in using listening cloud
environments and how this I mean I'm not
actually going to show what i did but
more like what is brought in terms of
technologies that we are having now
within whitefly and infinite span and
we're going also to spend a bit of time
on yeah yeah and we're also going to
spend a bit of time to see what we are
currently working on and maybe you're
even interested in trying out you know
the latest experimental things and all
what we're doing is fully open source
it's fully developed in community which
means even like the brainstorming and
all our meetings are on public platforms
and actually most of the people
contributing on these projects are not
employed like myself on reddit so it
really is like welcoming for everybody
if you have a specific problems or ideas
I really love to her about that so let's
just starting from the really really
basics of what leucine is for like you
might have heard this problem in these
days here like you don't remember where
the next talk is that you want to go to
and so assuming didn't have the printed
paper and your online and you search for
it you are not interacting with the
search system using a primary key right
you would have to remember so what's the
the code of the internal code of the
database of the session that you're
interested it this is something that for
from point of view of usable
he would have been a completely
pointless life it would all have been
hurting around randomly and not finding
any room at all so of course this is
something that successful companies know
since a while so the primary interface
the point in which you communicate in
website what you want to is usually a
single input of text where you will type
something and then it's really up to the
system to understand what you are
looking for and what do we have in
standards to achieve this so from a Java
Enterprise developer or you know general
the person who interacts with the sequel
systems you'll have the like operator
which is some ticking you know it's it's
the closest thing that you can do to
implement a query like this right the
problem is it is it is of course
extremely limited so let's see you know
you know in a very silly example
actually to how do how would you do a
query on the Wikipedia with the like
operator assuming let's say the
Wikipedia is a public service in which
you can search for information and let's
assume that it's exposes you as an SQL
console to you know go ahead you can
search for any article you want in here
so you can start typing some things you
might need to have the schema just to
know what what to type right but you can
start trying something like this so
which pages to have this content but
what do I have to put there as a
parameter like in my query I would need
to write exactly the full article right
so that's if I already know what I'm
looking for then why I'm searching for
that I already have it right so of
course this is even too silly like this
is completely pointless but you could
you know maybe give me the title and
they will give you the article starts to
get a bit more interesting so we could
we could teach children at school like
you have to memorize exactly all these
titles so that when you were looking for
the article you need to remember exactly
how this is typed and you know which
order which capitalization and which
spacing is being used because otherwise
you will never find information
you know in your life as an adult
professional you will not be able to
access this anymore so you can be a bit
more helpful you can make a query like
this you can start saying okay well
we're going to look not for the full
title but you know just give me a couple
of words which need to be in there and
we are not going to disregard at least
like a capitalization so let's lower
case everything and store things over
case in the database this starts getting
to something that you might have seen
actually in real world in many
applications and this is exactly how I
have been starting this journey like we
were maintaining a system which has you
know at some point you your manager gets
your requirements like okay we need the
search box there okay now with this
yokai that done essentially you know
probably just with the title but then
people start complaining it's not
finding anything or now you can then
improve them with the second version but
really the problem is the the overhead
of your database is extremely high and
still you have lots of problems like the
information which is coming out of there
is really not what people are expecting
and they are all thinking like your
system is really bad like why is it when
i'm using google i find you know
everything like it's on topic and i'm
very lucky in a way i'm feeling like it
most of the times it works so in a
system like amazon building a query like
that might be the difference between
being a successful company and being a
company which cannot absolutely compete
in the market because people searching
for these things will not find the
things that are looking but then they go
to somewhere else and other case is
right there on the front page you know
where i can find it and of course
companies like Google day they would
never have the success they had if they
were implementing this over a SQL engine
hood so that's where we're getting out
lucene is not an SQL engine its its data
mining it's a index management tool
where the index is a space with vector
information we
so it's vector space where you have a
statistical information over the
frequency of terms which are your corpus
and well this is just a screenshot i was
searching before ok let's search you
seen you know what twitter is using well
it's using the seam of course and linked
elena's using leucine and i could go on
with pages and pages of companies who
are using the scene behind the scenes
and probably all the tools that you're
using with your daily their job as
developers are also using the scene like
java ides to do auto completion of your
code they're using you seen behind the
scenes it's of course it's not a
relational database in there so well i'm
including these successful companies
right but really how widespread is this
problem do you can you think of like
successful companies which are
interacting directly with human input
which don't have this problem like there
is a it really is about human
interaction that's what I'm getting at
as long as you're integrating different
systems and there is a convention you
know you're translating these IDs mean
these to my other system and these so uh
pour rester integration points between
your different services they will talk
in a language that they know about so
they can match with the keys or the
foreign keys or the representation they
very specific identification of
information what what you're talking
about but here it is about human
interaction and the problem really is
that the humans are expecting you to
guess what they want but they don't know
it like if we go back to the first
example of Wikipedia sorry
if you have to guess what the user is
asking you that the problem really is
that the user doesn't know it yet so
because otherwise we are back at this
initial problem that you're asking him
to give you the content that you are
supposed to give to him right so if
you're on Amazon you're searching for a
specific problems you don't really know
exactly what you're looking for you
might be looking for again know when you
book to read do you already know the
title sometimes you do but some other
times you are just interested in a
subject or a category so yeah you
definitely don't want your users to have
to type everything and that you have to
guess we have been there the next point
is you're there is a strong expectation
that you deliver the the response
immediately right there is no patience
anymore in this world you could have 15
years ago maybe it was would have been
acceptable to have your system you know
think maybe three seconds and maybe that
was the tolerance nowadays you have to
be milliseconds you did otherwise people
start thinking your site is loaded it
it's not really working and they will go
somewhere else but the most important of
all is really the relevance relevance is
a technical term in this field it's
about computing exactly how much a
specific fragment of information that
you have available in your system so it
is a score which matches how relevant
this specific information is for this
specific query and you are having to
rank all the information that you're
having to then identify what's the top
relevance and the subset of information
that you're going to show to to the end
user so if you're using google your use
it with this now you type something lets
you type hibernate and then you see
there in well 20 million results we just
computed 0 to 27 seconds that's the time
it takes to load the page and what
you're looking for is right on the top
with no second choice is approximately
there you don't even have to go I'd like
second third page I remember when I was
young in studying and I tu we're
browsing for multiple pages in sir
Systems it wasn't that advanced to
immediately guess what you want now how
do you get is integrated in your own
applications you have to consider
several aspects of the problem the first
is the words that user is giving you as
a hint of what is what he wants they are
never accurate okay he doesn't really
know you have to remember he doesn't
really know what he's looking so you
have the approximate work matches he
might have overheard at the bar that
this technology is really cool but he
doesn't really know how to write it so
you might need like a phonetic
representation you have to convert to
the input in a frenetic representation
and find within your data set what
concepts or which wich other systems
might have the same similar sound and
these encodings they're all available in
the scene we're going to see how you how
you express the knee that I want to use
a functionality like this and stemming
is extremely useful now let's see a
sample like this you have to deal with
typos of course people will junk in
something quickly on the keyboard and
have no pages to go back and you know
fix it Oh proper grammar when I'm typing
something in Google and of course
synonyms abbreviations and technical
language here with technical language
and specializations this is where it
gets most interesting for you if you're
using google the it's a it's a very
general purpose tool right it's meant to
search like anything that we know of one
which is on internet so almost anything
but if you're building your own system
and let's say you are in like in a
medical field you have specific acronyms
which will have a specific meaning and
so you can build up your own mapping of
equivalence of terms of abbreviations or
of see no means and that's all under
your control when you're building your
own engine let's get started with the
basics of like an example we have this I
heard of this book it's about improving
to run by written by some scoped and I
don't remember a surname so this is the
input the user is
entering in a system because he wants to
buy this book he does not remember
anything else it was you know we suggest
you on an evening at from a friend now
what you have to start looking at here
well of course like operator owner is
not going to bring you anywhere so we
start to tokenize this the tokenization
is the first phase of the analogies of
an input it means splitting in this up
it's very simple let's extract tokens
out of here and each of these tokens are
then successfully they are in success in
their analyzed so we have a sequence of
tokens in the structure and each token
is going to be looked at with additional
filtering if we take the default
analyzers and you seen this is what you
get out you get for tokens out of this
sentence and they are how in proof run
Scott no not no to lower case here so
some particles disappeared here that's
part of the filtering process in which
the defaults that i'm using here they
are traded for english language and they
know that some particles of language
they are so common that they don't
really give you a hint like if if you
are going to on a website and say oh i
weld those articles which have on in
there that's not helpful right so the
engine will immediately discard that
noise it's is going to just use more
memory more disk space on indexes so
that's out of it and then it starts to
identify the other parts but like
improve is this is a standard so improve
is the stem of improve improve and you
could I could have written improving
their what's you know that look about
improving my run no that would have both
matches with the improves them so we
know that this is the concept that we
are looking for this is essentially our
internal identifier for the concept
which is they looked at and then the
same for run and Scott is lower case
because the defaults lower case
everything just so you know and then we
go to the scoring phase which is the
nice part so the second
up here is we'll go through all of our
corpus and see which of these have these
same for announced for keywords but each
of them will have a different score
where the score is more relevant when
the keyword is more rare in your corpus
so let's say I had a million books but
only two of them are written by Scott
that means that Scott is the key word
here which is the most value because it
narrows down the possible results at the
most for and yeah of course maybe you
know both of them have been written
about how and things but maybe one only
on run and so you get these two results
and one which is running on top that's
that's how scoring works that is the
very most basic concepts you can
customize everything about this by the
way but so let's step back a second and
let's go technical about what's Lucy
nice so it is of course an open source
project it's hosted at Apache the Apache
community is very strong and especially
the apache Lucene community is extremely
vibrant you go and look at the logs of
how many committers there are how many
comets are done in a year this is by far
the most the most active project in the
Apache community that might be
surprising because one reason for me to
do this talk here is it's it's very
strange to see this lucene world to be
thriving extremely successful and I'm a
bit like in a bridge I live in this Java
Enterprise world where most people do
not talk about loosing and in racine
world though they don't care anything
about what happens in the Java
Enterprise walls so there is a not much
overlap and they really hope that we
start to interrupting more so just so
you know it's extremely active and and
used everywhere it's primarily written
in Java but our ports in many other
languages and they have an extremely
impressive test suite but if you have
been following a bit like all of the
bars that happen at like when Java 7
came out at the very beginning like the
first two releases were some critical
bulbs which were reported immediately by
you the scene community
they are they're basically ranking on
top of home you know our popularity list
of who's finding most bugs in the JDK
they're always like first because their
test to it is so exceptional it's so
wide they it's run with so many
permutations and so many different
services that you know as soon as the
regression hits there is some point you
seen somewhere that will highlight a
regression and so when regression
happens the people start looking at it
so deeply that we actually find real
issues its systems so I'm very impressed
by the by the project as a job
enterprise developer what what should we
look at so we know you're I expect
you're all familiar with the java
persistence api or or hibernate we but
they look very much like you're too
modest but leucine is such matter a
better tool to solve these problems we
have you know the example is it before
the like you can do that with the
criteria you can do it which hkl but
it's not going to solve any of your
problems if you read a full text query
it's really a full text engine that you
want to use and how to express this and
how do we integrate is in the in java
enterprise platform so ideally what you
want in your own application framework
what you want is your express changes
that you are doing on your domain model
and you know like when you're using the
java persistence api you make some
changes and that you committed
transaction and hibernate or your other
implementers of JP they will replicate
the changes that you've done on your in
memory model that will replicate that on
the storage engine which is usually the
relational database so what we are doing
here is we take care of synchronizing
the state not just with the database but
also with leucine index so you don't
have to deal with writing on the scene
index and why is that a really good
thing well because writing on the
routine index has many many strange
peculiarities first off we can only have
one writer open ever
the same time on our index so you have
to synchronize between them well with
this thread size but then you start
entering in you know multi-threaded
coding and the performance of a single
right is also a point of concern it is
an index it can be large the compaction
the you know there are several aspects
of optimizations that need to happen
here that you might not want to deal
quite much with it what you really care
for is be able to express the query in
the most richest way and having most of
the control in how you run the query and
how things get indexes that's also
something that you want to control but
you don't really want to have to manage
this second arrow and saying okay I'm
storing something let's update these
tables here and what do you do do we do
it after the comma transaction are you
going to do before are you going to
right onto the scene index which is not
transactional by the way so you you
start needing to have to keep in mind
all these little tricks and that's where
we are helping with a bit how do we help
it well that's introducing the hybrid
and search project I admit here it's
very deeply integrated with you have an
identity manager so we listen for events
from the core of hibernate and based on
the events we know everything what
you're doing on your data set and we
know when you're about to commit when
you're after commit which if an error
happened if you're rolling back for
whatever is happening with your data
that's something that we can intersect
reactron and we can make sure that the
indexes are kept in sync on top of that
sorry went ahead o is yeah okay it's a
notation driven so on top of your model
you have additional notations to express
what you want to index it so that you
can run a full text queries on top of
that and what I really push for always
is you will never take away any advanced
capabilities from loosing itself so I
love you seen so much that okay I like
this idea of integrating the GPA and
integrating with your life cycle of your
your domain so that you don't you don't
have the you know the least amount of
problems in taking advantage of it
we expose all of it in terms of pure
leucine api's so you can grab leucine
index reader natively and run the most
crazy queries or access directly the
structure which is underneath if you
don't want to run a query but you just
want to access like a frequency of terms
for example it's great to look at
frequencies of you know how often
something was stored which has these
keywords in there so like if you want to
build a very quick like a tag cloud or
something you just it's a query that you
run it's not even a query you have you
have the frequencies in there in the
index it's just a look up by primary key
the IE is the is the key of the map
essentially so it's extremely efficient
to run this kind of operations we do
transparent in the state organization
and we integrate with the transactions
there are actually different options in
how it integrates with transactions
because in many cases you don't want to
have your wall systems transaction to
fail because the index writer failed on
one of your disks for example in some
cases the indexing is something that
okay you know if I don't get to update
the index it means that the searches
will not reflect this last data for
until you can compensate for it we give
you hook for compensating in a you know
second time or you know just jmx
operations to build the index at runtime
so by default it's not transactional
it's a post transactional listener but
if you're using the JMS integration to
store the events of indexing which is
also an option then we also can handle
this as a two-phase commit transaction
it's so very new featurette and of
course we give you options to reveal the
index from scratch so you don't have to
really make a full backup plan or maybe
you want if it's really huge right but
if you have a reasonable data set let's
say a database with a couple let's say
20
30 gigabytes of data in there it should
not take days to rebuild the index so
there is a powerful machine dexer which
uses parallel scanning of your tables so
as long as you can pump all the data
quickly could take you know depends on
your tuning three minutes 20 minutes
maybe in a couple of hours is very
complex but you can always rebuild the
index out of your database this is also
extremely handy when you're developing
because you are changing what exactly
needs to be index it for me from your
data set and your data set is probably a
bit smaller and test so when you're
ready deploy you have it you know to
build the index and then run the queries
on top of that and yeah we do of course
integrate with a failover and clustering
I don't really care which application
server you're using if you're using
application server at all or if you're
running on standalone spring everything
is fine what we do though is if you're
running it in white fly it's probably
easier because you're we're using the
same technologies which are included in
white fly so it might be a bit more
familiar with the concept going already
mentioned error handling you get a
synchronous whirl synchronous on
tification some errors but since the
rights actually happen in background
threads that might be a synchronous on
the point of view of replication so how
do you start with this in terms of
configuration nothing you just have to
have to dependency and so if you're
taking the very latest version now this
is I've been at search this is the ID in
maiden take 550 final it's the latest
stable release and take hibernate in
this case I'm using hybrid density
manager so we can use a jpa 503 final
release this morning and you're all set
you are ready to start using it what you
need to do is opt-in specifically which
entities you want to start using with
these new super capabilities so let's
take an entity the the simple sent idea
could think of as an ID one useful
property string name
let's make this an actor so this is like
a movie star right we were storing it
it's a JP entity here and now we want to
start indexing on this well you make it
an index at the entity and now it's
being looked at by the search engine the
search engine is already running because
it was found on glass pot and hibernate
orem looks up for extension points you
can also write your own extension points
hibernate orem will look them and wire
them up in its own event listener engine
and and then you have to specify you
know again you opt-in which fields you
want in exit feel this again it's a term
technical term from the scene world
every in the document document has a set
of fields where index oh they are
identified by field name the field name
is by default the property name that you
can override it and you can override
many many different options on how
exactly want is to be processed but this
is the simplest way and we also deal
with relations relations becomes really
tricky because in losing world
everything is flat so there are no
relations in there it's not a relational
database you have to keep that in mind
when you're storing so if i had a movie
which has multiple actors playing in
there and we make this index to we make
it an index identity with an ID and the
title and it is a it has multiple actors
and we mark this within the set embedded
this means that the properties which our
index earth in the actors in there will
include all the recursion recursion the
the fields which are in there they will
include it in the same document of the
movie itself so you end up with a
structure like this let's say we have
two actors harrison ford and kirsten
dunst and there are two movies there is
a melancholia and there is the force
awakens now in the document in the index
structure that you're getting here since
we market actor as having that actor was
a very simple entity which has one field
index set so it has an ID and it has a
name in there then in the names you'll
have to index the authority act on
but for the movies we have this relation
with a moment in too many so we have a
single title and an ID but then in the
field actor names will have multiple
value store in there now this is where
it starts getting really different and
crazy compared to a relational database
like in a field there is not a single
value it's a space for values it's
unordered it's not defined in which
order you going to be able to find this
up but the thing is you can match things
like which movies do have you know
Harrison Ford playing in it well there
are no this one there is one of them in
the set right and that's what the data
structure of you seen underneath it
really provides you of course these
terms will be you know broken down
tokenized lower case depending on your
options how do they run a query on top
of that well you pick which field names
you're targeting you create a loose in
query we're going to see later how you
would find those inquiries but the main
point here to me is that you look at the
type this is an org apostle you seen
search query it's not a jpa type it's
not a hybrid type it's not something
custom that we enforce you to use so you
can use any of the I think 200 different
libraries in deducing community we to
build different queries with different
power and any of those whose inquiries
you wrap it in full text query like that
so sorry first you get the full text
entitymanager and not search dot get
full text entitymanager we pass it a
normal entitymanager the return will be
an entity manager which has additional
super powers it will have some
additional methods which allow you to
run full text queries like this create
full text query you pass it loose in
query you say which entities you're
targeting and then this query here this
is not an hour gap a solution query this
is a jpa query jquery with additional so
this is extends the jpoa query again
you'll have all the pagination methods
that you're used to jpa get result list
but you'll also have some additional
metals like get results eyes so
indicates of get results eyes here what
do we get here so we are getting no more
than 100 results we are budgeting from
zero getting the result list so here we
get a list of up to 100 movies of
managed identities these are
transactional managed identities loaded
from database so they're entirely safe
you can make changes on them they will
be managed by your GPA context and as
soon as you change these changes are
again they are transactionally stored in
the database and apply it onto the scene
index but also you can try to see how
many results were there in total you
remember the google page at the
beginning it shows us 10 results but it
tells us oh yeah by the way it was out
of 20 million so here you can have 20
million but you're not really loading 20
million entities from your database
right because you're not going to show
them to the page anyway so let's let's
see this I have a very simple demo by
actually tweak this demo to be so simple
that I'm not using any dependency
injection any other framework it's not
running an application server but it's
using JPI so if we start having lookup
dependencies so that I'm not cheating
yes this isn't get up and the links are
on my last slide so but she'll dots so I
have hibernate search for em and while
i'm defining some properties ok this is
the hybrid version from yesterday so
it's outdated and hibernate corhagen
identity manager oh ok i had to add a
transaction manager but that's it and
then j unit and h 2 12 database in
memory to test it then we go to an
entity now in the example here let's say
that we are going to implement Twitter
and we want to service no sorry not
reimplement Twitter but a service which
you listen to tweets and we store tweets
and then we are going to do like a game
and people can vote for technologies and
you want to count the keywords which are
mentioned in these tweets and you want
search among them right so the first
thing is we define an entity called twit
we give it an ID it's a generated value
I don't care who it's generated and we
give it a message and we mark it with
field so this is going to be in deck set
and we also want to store who send it
and here we start seeing some of the
additional properties for indexing so
yes I want this index set this is
default but Annalise no that is not
default so here the message will be
analyzed which means tokenized splits do
all the processing of language specific
properties but in this case it's I'm
telling you don't bother analyzing it
that's like one keyboard and you have to
store it exactly like it is right that's
the main difference and we can also
store a time style I'm marking it with
numeric field but that's actually
explicit when it's a number it's going
to be stored in the index in
representation which is more suited for
numbers that gets a great performance on
range queries and that's it that's our
entity now on top here here we have
declarative description of really the
guts of the technology this is part of
the biggest entry point of this what
we're doing is we are defining an
analyzer by name oh sorry here I'm
defining analyzer by name i'm going to
say so when i'm referring to an analyzer
called english what we mean is an
analyzer which works like this so the
tokenizer we're using the standard
organizer and there you have Java doc
for heat and it's going to tell you well
if I open it this is basically split on
white spaces ignore the white space it
doesn't matter if it's new line tab or
anything is just a split and that's my
tokenization but then I want to filter
these tokens with these additional rules
and here we define a chain of filtering
and there are a huge amount of different
filters available zinc
unity from which can can do lots of
interesting magic from more than 400
different native languages are supported
here to do quite clever language
processing that you cannot really invent
or your room but in this case we are
keeping it very simple so it's doing a
ski folder filter which means if there
are strange accents get right sorry if
there are strange accents get rid of
them we just want the basic ask a letter
and then we're going to lowercase them
and you remember when I said let's get
rid of some tokens which don't really
matter for my text we're going to use a
stock filter and I want to give the list
of properties of terms which need to be
ignored i am going to give that
explicitly and so it just happens that I
have a file here which is a property
which just list the things that I want
them to ignore when in exam okay this is
a this is an example but you can
download these files for every language
and that's it I sign should we then we
can encode the service i'm not using an
injection framework so the best harness
is it needs to set this essentially but
i just want to show you how it looks
like to use this system so you get to
query builder and that you say on on a
keyword unfilled message that depends on
the entity matching by keyword let's
call a bit create a query so this here
this is my loose in query or gap a scene
and from my full text entitymanager can
you make that a full text query and we
return that so this is the query itself
and the tests will will will use this
and then you can have messages by and
sender and all tweets sorted by time
that I mean it's pretty simple to get
used but the point is again you can use
any of the new scenes done or queries
the point here is you don't have to care
about writing the thing
which is really tricky now how am i
using this sorry oh the persistence.xml
is interesting to have a look at because
really this is a very standard
persistence.xml you see there is nothing
special except the hybrid search line
here so all the properties of vibrant
search have these prefix it's saying for
all the indexes the directory provider I
want to use is the ramp on the in-memory
so don't store things on the disk
because this is a unit test the service
test what I'm doing in a test let's
start from the prepared data and enlarge
it to did so i create the persistence
the entitymanager factory and then from
the entitymanager factor they get an
entity manager we begin a transaction
will show you later well what
essentially this is a helper to do that
from the jboss transaction manager which
is not really meant to run in standalone
i'm assuming that normal you would have
your own framework managing transactions
so this is not too interesting but what
you do then is you rub the entity
manager and get the full text
entitymanager out of it since it's a
test and making sure that the index is
wipe it clean purge all this is one of
the full text special queries sorry
special operations of managing the index
flash that and then we're going to store
some tests entities sorry some tweets
let's say these tweets have been written
by so a smart marketing guys say well
I've never heard of tools but he
mentions it right and the other guys are
we love hibernates and well thanks and
we wouldn't vote for tools I mean do it
they are just mentioning technologies
but what we want here is counting how
many times the specific technology is
invoked and how do we do that we're not
going to use the like of course so the
test here is
running on the service that we wrote
before we get the food text
entitymanager we set it on the service
because again made there is no
frameworks you're doing anything and
then we do you remember the first
methods it takes a parameter which is
what we are looking for and we assert
that there are two results out of that
and we can take them the list but then
the thing is we can also write rules in
a very weird way and since of the
processing that has been specified on
the on the top of the class this is
going to be recognized again as rules
because well accents we said ignore it
and we said also lower case everything
and so if it was written like this or
lower case it doesn't really matter it
still is the same thing this is too but
ok the the example is extremely simple
it's much more interesting when you have
millions of things towards because what
you're searching is what's you got to
get on top and that's how you are tuning
this system so let's back to some more
interesting parts we have seen this just
to so to recap on the part the jump
let's say the special capabilities of
the framework the first is you define in
declarative way how this language
processing needs to happen but then we
do filtering and here it's really gets
really interesting so you can have a
declarative way you'll specify or
specifying filters which stack on top of
your queries so let's say you have you
always want to programmatically make
sure that things which minors shouldn't
see are always applied on top of queries
you can do that programmatically so you
can enforce additional restrictions on
things like of queries which are being
defined by end users or maybe like
public users and they can take
parameters so you can stack these up
doing multiple filters it's you don't
have to edit the query appending strings
on top of it you just Add and enable
multiple filters and here we are doing
ok we are not going to show anything
which is you know not allowed for minors
and we want the special offers of this
day and the only the things which are in
stock today at this location
so these are parameters which we are
passing to a filter if you define and
the filter is nothing else than a
container which wraps an additional
query so why are you why is it nice to
use filters the first thing is the
declarative you know expression of it
but the best part is really how they are
handled in the internal system so what
happens is the query is run on top of
the full documentation or the full index
set you have the results and then
filters are applied on top of it but the
thing is the filters are used across the
lifecycle of your system as long they
can they can be cash it and as long as
the segments of the index are still
valid the segments of filters are
connected to the lifecycle of these
segments so you don't have to deal with
how these segments in the index
structure are mutating when we apply all
the changes but these things are used so
these are very compact bit set
representations they actually live of
hip so it's not even taking memory of
your jvm but it's boosting the
performance of the world system
significantly so if you can break down
your queries in a part which is you know
user provided and then the additional
filters which you want to stack on top
of it you can get absolutely off some
performance by the way there are
customers of Reddit which do really
crazy things with this and among them
they don't use it just for full text
they used it to just increase the
performance of the databases so
everything which the database would
struggle to do they push that to the
scene because then they have these
additional capabilities to you know
apply it set restrictions on top of
queries and they don't even have to
really hit a database for most of the
complex operations another feature is
faceting so in a fascinating case you
have faceting is is this structure here
on the left side the capability that
most shops have that you are searching
for something and you get the list with
the most relevant things on top but
while you're getting the list you also
get the categorization with some more
vector information here like how
frequent it is in different categories
you break down and we have an API which
basically allows you to give return both
of the listing in one go you get also
different classification different
and finally we have the more like this
if you have an entity which looks like
very similar to what you are looking for
you can pass that into a query builder
or more like this kind of query you can
tell them which fields you're interested
in having it compare you pass the
original entity and you create a query
out of that note that this is again this
is a loose inquiry so then you can use
composition you can do like an boolean
operation on top of these and then stack
additional filters on top of the end
result spatial filtering as I mentioned
before it can encode in numbers in a
very efficient way numbers can be
represented for range queries and rage
queries are super efficient again now
all these very super nice efficiency is
very cool but remember no relations at
all right so but as long as you can
stack these restrictions together and
say you can say well i'm looking for an
Italian restaurant and I think the name
was like this you overheard the name and
so you can going to do phonetic spelling
and it wanted to be like in this area
all of these restrictions are things
that you can be very easily map on top
of you seen how do we run this on widely
so we've seen a simple example in
standalone GPA I'm going to show you a
similar example but running this other
example is in Gaeta but it's not any
separate repository this is actually a
union integration tests with inhabited
search yourself we run integration tests
with conceding here with JMS with spring
with the different to SGI containers
with whatever now the most trivial
integration tests are actually rotates
for this because normally you want to
test for some very specific thing here I
want you just need to show how to run
the basic things so it's an argelian
test arquillian is what we use to run
tests within a container so fully
deployed in a running application server
in this case and we need to define
deployment archive so this is the
shrink-wrap dsl we specify this is going
to be a web archive with this name
and in the web archive you have to put
these three classes this is my project
we have an entity and HB and another
helper class which will see you soon and
we have beans XML because i'm using CDI
and the persistence.xml what's the
content of the persistence.xml well i
like the dsl so if defined it here i'm
going to use the default data source i'm
going to say to hibernate create the
tables and drop the tables because it's
a unit test and i'm going to say to are
going to search keep the index in memory
because again because it's a unit test
that's all I'm needing in whitefly you
don't need any dependencies because they
are that are already integrated with
worth right time you don't need to
enable anything else because it will see
your entities are market with index set
okay so you are going to need to see
here it is and you're going to need type
in a search and these things are going
to be available for you in class but we
inject the teg b which is nothing else
that the service facade you know just
does a couple of things and has a full
text query in here so it can search
members by name and it can register
members and what we do here is register
a new member we verify it has been
stored and then we released another
member and we verify we can search for
him by name and then we care if I that
if I am searching for somebody else I'm
not finding just you know the opposite
and this is all there is it's almost
boring so you have no excuses to not use
this technology sorry something is wrong
oh I did not prepare my environment
there's no point
ok
what happens in my maven I have a rule
which before the integration test starts
download whitefly unzip it and have it
there when I'm running it from my IDE
this isn't run so my world fly wasn't
there so I'm killing this now because it
already unpack at work right here and
now I should be able to run it from my
ID area yeah see it's booting here
started whitefly and wow it's fast so
it's already deploying somewhere these
are the beans database started and that
was the test flying by and it shut down
the application server so really that
solder is the tests were green that's it
let's get back to slides because I'm
very late and there was so much still to
say now we're getting now this is how
you use it and all the benefits you get
but there is a big catch so now we have
these awesome index but you have to
manage it where do we store it you have
to store it by default you have to store
it on file system but that is sometimes
a big surprise because people are
especially in operations their hand they
are used to have you know an application
server which is mostly stateless you can
get rid of it and then you have
databases that's where they have to care
about you know that that's the important
data and most of the times the specs of
your hardware will also be in terms of
my data you know the i/o the hard drives
of my vacation servers would not be that
high and in cloud environments did my
results will become having some some
questions how having a storing all of it
sorry I said all this now the the
interesting part becomes that we can
plug in multiple different kinds of what
we call like backends back end is where
we send the indexing operations to so we
generate is in these operations we need
need to happen they can be stored in JMS
queue so that they can be delegated to
different servers or they can be applied
right away in the same server that's the
leucine back end and then we have
another obstruction which is called the
directory this is attribution
obstruction the directory represents the
storage engine so where we store things
the defaults in the scene are two we
have the
directory store the memory and the file
system directory which stores on disk we
come with an additional director which
is the infinite pond director infinite
Spanish this in-memory data grid so you
can store things within the data grid
but getting back to the basic so if you
have any cluster environment the problem
is like I said at the beginning you can
only have one writer ever at the same
time for specific index which means if
you have a multiple server node even if
you are having of shared file systems
you have to have a coordination between
servers because only one of them can
write on it if you have multiple writers
the in debt will get corrupted so what
you would do in these cases you might
have a shared file system and you can do
regular copies and you can use our JMS
queue which is the back end sending
messages to one node and the one node is
supplying all the things now JMS can
have h a failover so you can have
multiples here to make sure that the
single master is not going away but the
drawback here this is an asynchronous
approach so the index it will never be
updated you know within milliseconds
from your right you will you need to
wait a bit from having applied to change
on the transaction being able to search
for this sting operation now there are
some alternatives for that one is well
nuts let's store they change directory
in Finnish band so that you don't have
to have shared file systems and copies
between shark directories but we can
finish pan you have a real-time
replication problem even if you have
real-time application you still need to
delegate to a single worker now we also
have jacobs back and on top of JMS and
both of them can be configured to be
synchronous so if you really need this
part can be synchronized in this part
here is real time so the latency becomes
extremely low it's not currently up and
in the same transaction of course but
that's expensive I would think about it
if you really need real time
with some more goods about that very
small integral introduction to infinite
span so its infinite 18 at this point
it's an in-memory k register or its
apache license I need to really speed up
and let's go straight to the interesting
bits so it is a je cache implementation
but also it is integrated with in white
fly so while fly already uses in
finished plan to do sessional
replication to do distributed caching
for hibernate and so the good part is if
you are even if you're having a cluster
running you're already using and finish
panel and you have already configured
this for the specific cloud environment
that you're running so there isn't much
more needed to actually have it to store
the index as well and yeah it does
include the same technology that we have
seen so you cannot show you now how to
index a.j.p entities but actually you
can also use infini span directly and
let me just skip to that so if you take
all of infinite Pandey API it looks like
a map and you can sorry yeah it's like
it's later so you can index it and where
is my example now oh sorry i missed the
underlying things i'm making a mess of
this
I just explained by words you can
annotate the Fazio objects which are
storing in grid with the same property
and index it and they will be index at
the same way so you don't even have to
use jpa to take advantage of leucine
into an integrated to convince plan and
again it has the same capabilities but
on top of that you can also use the
leucine directory in another system that
you're using the scene for so if you
don't want to use I banette search you
don't want to use in finished plan you
just want to use leucine itself you can
still use the leucine directory to store
the index in the cloud or in the data
grid in your in-memory container that's
how it looks like it doesn't depend on
any of these dependencies its separate
projects right and so let's spend my
last two minutes on this there was a
google summer of code project this
summer Martin Brown a german student was
sponsored by google to work with us and
what he did was a database trigger
baited trigger system so we don't have
to listen to hibernate notifications
percent which means now you can also we
can also intercept changes that you're
doing in native sequel and you can also
insert changes which have been done by
open japi or eclipselink so this work is
still being integrated in hibernate
search but the hope really is that you
get these additional options and also if
you're if you really have to use it when
you care in eclipselink sorry for you
but at least you don't have to miss out
on all of these of the features here and
that's where this is living and finally
a word on elastic search and solar these
are based on the same leucine technology
that we have seen so far so all of these
powerful things they are exposed by
elastic sir and solar that these are
like service which expose again just
plain you seen and they give a nice
packaging around it and a REST API on
top of it so this is our getting now
very popular in terms of you know
microservices you want to split these
things up in different systems and so if
you have a large indexes maybe it's
worth looking at these and what we are
doing and working on now is having
higher net search not just push directly
to all seen in embedded mode which then
can be clustered but we can push
directly to apache solr or elasticsearch
there are is it's been working on right
it's not available yet but there are
branches which are experimental and if
you really want to help joining us the I
would really appreciate having people
you know playing with those and
experiment even if they are not really
finished yet these are some more things
which have been working on infinite
plane is getting much more performance
improvements and we're going to work on
dueling white friends forum but
generally we really would love to hear
more of what you're doing with it like
if you are using it for something please
let me know I love that and we also love
contributions so if you find something
which doesn't work or if maybe if it's
just a typed in documentation quite
absolutely take it then legit and there
are lots of other users besides myself
and my team working on it which
participate a lot so we it's a very open
community and fully open source of
course thank you very much i hope i can
take some questions let me show all
right thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>