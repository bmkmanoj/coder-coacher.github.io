<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Use Apache Kafka to Add Streaming Analytics to Your Application | Coder Coacher - Coaching Coders</title><meta content="Use Apache Kafka to Add Streaming Analytics to Your Application - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Use Apache Kafka to Add Streaming Analytics to Your Application</b></h2><h5 class="post__date">2018-04-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/49uCqZI61hY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thanks everyone for joining us here my
name is Stuart Bryson this is Bjorn
roost we're gonna be talking about Kafka
actually the title on the slides is a
little bit different we're gonna talk
about adding streaming analytics to a
really to a legacy application or one
that is relational database based so mad
props to the OTN and the Oracle well
actually the developer program at Oracle
for for sponsoring myself as an ace
director the dev champions and it's a
dev champion and an ace director so just
a lot of props to Oracle for continuing
to have events like this so that we can
speak at it and for sponsoring us at
these events so my name is Stuart Bryson
I'm the owner and co-founder of a
company called red pill analytics were
in the analytics data warehousing big
data space
we're about 30 consultants mostly in the
US but a couple of people in Brazil and
a person in the UK there's some of my
social media avenues if you're
interested in following me be really
honored if you did that yeah the name of
my company is a reference from the
Matrix movie I see some gray hair out
there so some people about my age might
get that reference you only get to name
a company once so you might as well have
fun with it but we do sort of put that
idea of choosing to see data differently
in our logo and our material so we do we
do think we sort of effect a paradigm
shift and the way people think about
analytics and and really making that
move from traditional data warehousing
to more modern architectures this is
Bjork whoops I got my own clicker here
I'm a grown up I've been working as a
database consultant for about 15 years
now kind of transitioning from being a
pure Oracle database person into the
DevOps space and the Big Data space I've
actually kind of done with DevOps now I
think but certainly exploring dev or Big
Data mixes up
there is that I was a company called
Pythian it's a global company
headquartered in in Ottawa and we have
employees all over the world and we
started out as a managed service company
providing managed services for
relational databases but our our motto
isn't it's not on here it's love your
data and we do everything data so we do
big data small data relational no sequel
machine learning artificial intelligence
all of these are topics that we touched
on and I really enjoy working there as
it gives me a great exposure to all of
these cool technologies other than just
relational databases the agenda for
today's we we do want to tell you about
Kafka and Casey Kerr but we want to walk
it through a use case and the use case
that we love we set up is taking data
from a relational data source and
analyzing it processing it in real time
using event stream processing using case
equal and then displaying that
information in a dashboard and you'll
learn a little bit of a Kafka and case
equal and and just stream processing on
the way as we get there this is this is
the the very big picture is we have a
relational data source and this is an in
my simpler case it's a my sequel
database and this could be your legacy
application it could just be your
application that it's doing whatever
it's doing today but what you want from
this is and this is something that our
customers are asking for is I want
real-time analytics from this if it's a
and this example is a sales example we
sell things and I want to know in real
time how many how many items I've sold
in the last in the last time period and
I want to start by just showing you the
end product and then we'll go through
all of the components and the end
product is running here this is this is
actually the end product this is say a
dashboard in Griffin and you can see it
updates as we go so we see these dots
updating and all of these are sales for
a certain product category or a certain
product at a certain time so we see wine
salt olives and protocol lumpy and we
get to what lumpy is in a little bit and
this is graph on ax which is a
visualization platform that runs on top
of a lot of the elastic stack or on top
of the last assert so the data is
actually flowing from your legacy
application which is relational in this
case my sequel flowing into elastic
elastic search and
we're using Griffin on top of it too and
all that obviously is flowing through
Kafka and the and the the one quite
exciting thing about this demo is I'm
inserting rolls into my my sequel table
here or my relational table and as I'm
doing that we can see that this this
orange line for the lumpy product which
I just inserted it's now being updated
in real time and we see that where there
was a big sale for that coming in so
that's that's kind of what we want to
show we start with a my sequel database
or relational database and we get to
this streaming dashboard yeah and it's
also decoupled from anything else you
might already be pulling from that
relational database you might already be
going to a data warehouse or some sort
of reporting instance Kafka is really
easy to sort of add to anything you
already have going on and one of the
points I like to make about Kafka's as
you know I've been working for coffee
with Kafka for about four or five years
now and this is pretty much the initial
state of of what people talked about
with Kafka right it is you have some
data set some data source and Kafka is a
great thing to sort of stick in the
middle of a process to decouple the sort
of the downstream consumption from the
upstream ingestion I think I got that
right and and it's great for that right
we're gonna talk more about that as we
go but what Kay sequel adds is actually
this this new element okay which is you
don't just have to subscribe to Kafka
raw data that's great if you're doing
machine learning if you're doing any
sort of data science or data analysis
raw data is fantastic you you you
absolutely want to be looking at looking
at that but we no longer have to just
subscribe to raw data in Kafka what we
can do is we can actually curate and
process the data right there inside of
Kafka so that your downstream
applications can consume either the raw
data or the curated data or both right
so now we have the capability inside of
Kafka both through what's called single
message transforms but also through
what's called K sequel and K sequel
actually sits on top of what's called
Kafka streams which is a full Java
framework for writing your own sort of
streaming applications similar to
something like spark so all of that
sitting
right there in the same cluster and
that's what we're really excited to talk
to you about today and this kind of fits
into the big data space even though it
might not be that big but if you if you
google big data and what is actually a
big data how do people define big data
Wikipedia and some other sources talk
about the three V's of big data and
that's volume and that's quite obvious
like big volume that makes sense but
there's two other aspects together that
are quite important one is variety you
might just have a variety of data but it
might not just come from a relational
data source it come might come from IOT
devices it might come from locks and
click streams and then the the kind of
the newest aspect to Big Data to me at
least this velocity is I want to have
very very quick insight to to my data
and a lot of the traditional Big Data
solutions cover volume and variety but
they're not that great in velocity if
you want inside of your data quickly
then spinning up a new cluster and
running a query there might not might
not be what you hit you're after because
that usually takes a while and although
Kafka is a part of all the major Hadoop
distributions right a lot of people
think it it is sort of necessarily a
part of the Hadoop ecosystem it
certainly grew up there but there are a
lot of full fledged production district
Kafka where there's no you know there's
no hint of Hadoop anywhere around right
so you don't necessarily have to think
about Kafka yes it's sort of its
distributed it works very similar to
Hadoop and in the way that it scales but
you don't have to have a Hadoop
ecosystem Kafka's certainly sits by
itself in that way and this is the
definition of what patchy Kafka calls
himself they don't say what they are
they say what they do and what this
product does in their mind is helps
people build pipelines and be fault
tolerant make it fast and already runs
in production in many many places and
this is a slide that we grabbed from
confluence marketing slides so dumpling
is a company that supports calf calf got
service open source and confluent has a
service offering around this and their
claim - - why you would want calf guy is
that without calf girl your life might
look like this you already have systems
talking to each other and sending
messages or events between one source
and it target that's nothing new but you
end up with some spaghetti code because
you end up writing these connectors one
by one and just
plumbing each
one together and the end of the
spaghetti and their claim is if you
adopt Kafka is the one and only source
of truth for all of your communication
streaming messaging services or needs
then your life might be a little bit
more organized because you have get rid
of the spaghetti coat you just have
everything talks to just this one
streaming platform yeah regardless of
what sort of downstream system you're
using to analyze data you as a sort of
as a point just go to go to Kafka to get
it and if you have any sort of data that
might be valuable to downstream systems
or downstream analysts as sort of out
just a you put it into Kafka and you
decouple that but you also make all data
available sort of just as a point for
your for your data architecture cool so
what would Kafka to me is and there's a
few things that are that are not very
unique but then there's one or two that
I think are unique so first of all it's
a clustered message bus but that is not
something new we've had message buses
and systems that can transfer messages
from A to B for it for a while
one thing that Kafka does different than
most of these other systems is it also
stores your data for however long you
configure it to be so it uses the local
storage on the cluster nodes that you
set up and it replicates it for that but
one node goes down you still have access
to your data but you can configure to
store and keep your data for seven days
for a month for a year for a ever long
you forever a lot of systems are doing
every base for keeping keeping data in
Kafka forever and the nice thing about
that is it allows you to dynamically add
consumers to your topics or to your
streams at a later point in time and
just rican soom all the events that have
taken place since the beginning of time
or since the week or a day ago and that
allows you this flexibility of you don't
have to know who your consumers are when
your produce to it to to Kafka can just
write to it now you might not even have
a single consumer but if you provide
applications that want to consume things
you can just do it and look at all data
Kafka is also really amazing that's one
of the reasons why I think many people
are using it if it has a ton of support
for other systems so it can connect
Kafka to pretty much anything that can
read or write data and you don't have to
write these things yourself that's
that's really nice and then the last
thing I'd Kafka does that I think no
other system does is it also allows you
to process the data right there on your
message bus on the same cluster using
the same resources
and transform your messages as they come
in
yeah we used to have sort of uh you know
Kafka would feed something else right
spark some other cluster well you you
know don't you don't necessarily need
these multiple clusters now right so so
Kafka allows you to do your processing
right there in the same cluster so
remove the number of clusters you have I
guess you'd say and what what really
makes the reason that you know Kafka is
often sort of before all these
processing frameworks is that you don't
necessarily ensue Murs of your your data
and Kafka manages which record they call
it uses the terminology and offset which
offset each downstream consumer used
through what's called consumer groups
has actually seen so you might have
multiple downstream consumers all
reading in real time as data streamed in
for batch or streaming data that's
coming into a what's called a cough
Kafka topic which is sort of like a
table all the while that data is still
being dumped in there right so what's
really magic about this is that you
don't have to configure ahead of time a
consumer group you just use a unique
consumer name when you make a call to
Kafka Kafka starts managing that the
offsets for that consumer group you can
reset or sort of reset your offset back
to the beginning so it makes things like
reloading your data warehouse reloading
downstream systems very very easy you
don't have to write double code right
we've done that a lot and what's our
historical load code versus our
incremental load code etc you don't have
to do that anymore you just go tell
Kafka I want to reload this downstream
system that's another reason that that a
lot of companies are now keeping data in
Kafka sort of indefinitely right because
of that use case becomes very very
simple to reload downstream systems
new skates so why do people use calf cam
you're covered most of them so it's
either - link or decouple systems from
each other and it can be used and people
use it to drive micro service for
example so you might have something that
produces events like a new user signing
up and then any other micro service can
subscribe to this and fire something
often in the cloud you do this for
service functions or cloud functions and
if you're not using the cloud you can
still you write your own things that
consume these things and then we see
this being used for real-time analytics
that's the use case we have today and
that's that's the - you see as I see
mostly its need coupling things - micro
services or to other services and for
real-time analytics more and more of the
applications that we deliver today are
sort of data driven data is a major
element and the micro services
architecture is wonderful but how do
these applications all share data do
they all have their own version of the
data is it a shared databases at
separate databases etc you can sort of
eliminate a lot of that a lot of those
problems in an analytics driven micro
services architecture if all of them
just get their data from Kafka via rest
bio bio whatever by sequel whatever you
know sort of however you want to go at
it so so it sort of smooths that micro
services architecture when it when it
when they are data-driven and if you
wanna get started with Kafka I mean we
talk about clusters and nobody today
wants to install and run an operating
cluster one easy way to get started with
a Kafka cluster that you don't have to
build yourself is to use the Oracle
cloud and they have a service called the
event hub cloud service which you can
use to spin up a kind of managed if
kefka cluster for you so that you don't
have to worry about adding nodes and
deploying software and all of that so
that can you could take care of that
with them without the Oracle was
actually first for Kafka in the cloud
they were the first to deliver that
there's others now but they were the
first point about yeah yeah so the
lights are really bright so I can't
really necessarily see but raise your
hand if you're sort of from a database
background maybe an Oracle database yeah
so I like to sort of make the point of
what Kafka tries to be using an
architecture that a lot of people are
familiar with this is an Oracle database
and there are a series of tables and
this is relational databases
what we call state in the stream
processing world right so so a table in
your database is not necessarily
designed to to record or to keep track
of all of the changes that have gone on
to that database over time it really is
just designed to give you state and we
look at relational databases and that's
sort of what we think of but truly in
the Oracle database those tables are not
really the source of truth what is the
source of truth is a couple of things
usually the redo log you might even put
the undo which we'll talk about a bit in
that category so if your database
crashes before data is written to disk
and comes back up it's the redo logs
that sort of help it maintain that state
right so the redo logs are actually the
gold copy of your day to the source of
truth if you really want to think about
it for events for the events how did the
table get to the state it's in now what
kafka so this is an example using my
company so up up above is sort of the
state of the table but below it is the
events and when we start thinking about
stream processing what we really want
and what Kafka tries to deliver are the
events there's been a lot of techniques
for trying to get this this has been
valuable for years in data warehousing
right if you want to if you want to
track anything like like price over time
it's really important to track not just
when a product was sold at what price
but how the price changed leading up to
it to a cell for instance and we've
often tried to get events in the past
and Kafka is really good at delivering
this so I just want as you sort of think
about the rest of this talk state is
something we've thought about for a long
time but but really stream processing is
more interested in advanced down below
so what Kafka really wants to be is the
redo logs for your entire organization
right so instead of trying to think
about you know what is the source of
events for all these different
downstream apps and how do they capture
change just put it in one place
and subscribe to it from one place and
build data however and consume data
however you want to consume it
downstream
whatever tools and techniques make sense
but use Kafka sorta as the event
processing hub cool and so that's that's
the insurer out of the way and now we
want to look at the actual code that we
billed to for the for the small demand
that we have so the the setup starts
with a relational database and I used my
sequel mostly because I started to run
this on my laptop and flying up an
Oracle database just to get some sequel
going it's a lot of effort and my secret
was just yeah we run the whole thing
right sorry you don't run the whole
thing in docker which is just a lot
easier for my signals a lot easier yep
oh yeah and also I've got this the code
and all this stuff on github so you want
to try the demo out you can just go to
the admin and a single docker image so
you can just spin up the container and
play around with it right so but but
this is what we have in this demo we
have a table called orders and i just
insert random orders into it and they
have a product name a price that i sold
for a user ID and then the time when you
order this so that's that's what's my
setup that's that's my application is a
script that randomly inserts these
things but this could be any any
application right it's just a table
relational database so how do we get
data from relational database into Kafka
that's the first that we have to solve
and not just state right we actually
want to get the events if we can right
so the first thing you look at is is
Kafka Connect Kafka Connect is a
framework that sits on top of Kafka is
also open source and it handles a lot of
things for you so Kafka itself doesn't
care about what types of messages it
handles and the interface it's hard to
cough get surface I think there's really
only a Java API and kapha connect sits
on top of that and provides the number
of things the schema registry for Avro
data and a lot of pre-made and
pre-written connectors for all kinds of
data source yeah really anything that's
that's the least bit popular now as
either a source or a target for data
there is a Kafka connect
connector for it so there's a really
this stuff this is one percent this is
does not do justice to the number there
was a Kafka connect plugin delivered
while we've been sitting in here
honestly I mean they're really coming
out very fast my demo is that this is
lumpy and this is
math cover I used to carry around with
me to conferences until I really kill me
bother to explain to TSA people why I
have this donkey pinata with him but the
example of these lumpy so now you know
what lumpy is so lumpy has a diary and
lamp is quite upset that he still has to
be inaudible where it's still snowy on
the ground and and he hates the snow and
he puts us into his diary this is a
simple example of a data model or a
table where we just insert data right we
never update something in this in this
table and we just insert new data and
this is what the simplest connector from
databases to kafka users
it's called kafka connect JDBC if we run
against your database you specify a
table or whole schema and if we remember
the offset if we remember that ID number
16 was the largest it process and then
it will wake up a few seconds later will
run like very busy saying select staff
from me from diary where I did greater
than 18 it is and it's got a lot of
options for combining that with
timestamps there's other options besides
just the the incrementing ID yep but it
really only helps for inserts but you
can't really do can't we do much on in
terms of updates and deletes that's the
conflict so that's all the code that you
have to write it's not even close it's
just a config file that you have to
write specifying your connect string
your table and then the the motive how
we want to pull your data in this case
look at the ID column and take those in
incrementing so that's all you have to
write and you start up a connector on
your Kafka cluster and it will pull the
data from a database problem of this is
again you don't fetch updates you don't
fetch deletes and it kind of runs it
pulls it periodically pulls yeah you
don't necessarily get all events right
you get you know how often do you do you
wake up and go check for new records you
don't necessarily get all the updates
between those those two time frames
right and so and lumpy has an inventory
and so he keeps track off he's got a
name tag he's got a bunch of friends and
he takes selfies and this is a different
kind of table because this table we
don't always answer it we update like if
you takes another selfie or makes more
friends we update a row and this is
something that the Kafka connect JDBC
would not be able to pick up because the
ID hasn't changed and the crabby of
select star from inventory where ID
greater than five would return any new
so this would be inventors cuff lumpy
making friends would be an event that
Kafka would not be able to pick up in
this case one solution to this is you
could add a column if you have a column
for a last modified time stamp that's
great you can use that but if you don't
you probably don't want to modify your
whole database schema to add these
columns that's a lot of a lot of
nastiness you could make it even more
nasty by adding temporal validity
whether it from data two and then you
have to code some logic into your code
and still what is valid then there's a
there's a better solution and I just
want to go this very quickly it's I call
this poor man's change data capture I'm
using Oracle flashback if you're lucky
enough to be running on an Oracle
database there's a feature called
flashback Ferry and you can use that to
read the undo information to get
information about how data changed so
the syntax of that is you say select
star from table versions between and
then min value max value to give you all
the history that Oracle has and the
output of that would look like this so
for this inventory table we now get
these pseudo columns of operation and we
can see eyes for insert and you for
updates and we see the time as when
these were valid and if you put this
query into the calf connect JDBC we can
now pick up these changes right so a lot
of people think that flashback is just
for querying your database at a
particular point time it is that but
this versions between syntax gives you
the ability to see all the versions as
long as the that still exists in your
undo and we have what's called well
you're getting to it right the flashback
archive I do yeah okay well yeah so the
way the way this works is Oracle stores
old versions of their data or old roles
in what's called undo anyway it has to
be the for read consistency if a
long-running transaction or a
long-running sequel and somebody else
reads this before it's committed they
actually read from undo so Oracle has
been doing this forever
yeah so there's no overhead for using
this solution it's just that this
flashback very exposes this data in this
in this syntax and then they added a new
feature called flashback data archives
where you can specify a table that this
undo data for a certain table should be
stored for herever long you want to keep
it so undo you usually keep undo for 10
minutes 15 minutes 20 minutes it's the
range of minutes might be hours but you
don't keep it forever but you can
specify that single tables you actually
want to keep your flashback data
and then it gets written to and the
querying you using undo doesn't matter
you you use the same flashback
terminology you same flashbacks equal
statements if the data is in the undo
space it'll grab it if it's in the data
archive it'll grab it so you don't have
to change your code just this is
something it just mines the undo for the
tables that you've specified stores it
in the data archive longer it's sort of
like a regular table in a regular table
space actually and then the language you
use doesn't have to change and it's if
you if you go away from this target you
say Kafka is not for me there at least
this is a good trick to have in your
toolbox for if you ever get in a
situation of I need to get change data
capture cheap and easy and quick and
then this is a good way and thence
aren't just for stream processing you
should be trying to get events for any
sort of analytics it's important that
you get on all your changes and this is
how the my config my connect JDBC config
looks like with this I basically have
the same config file and instead of just
specifying a table I specify a query and
that's just my query and then it'll pick
up the changes that was Kafka connect
JDBC but Stewart already mentioned that
your database already has all the
changes check you don't have to pull
them because Oracle or any database
really stores changes in what Oracle's
redo logs and then called bin locks in
my sequel and why the head locks and
Postgres and transaction locks and
sequel servers I know nothing about so
we already have this data so we already
write all these events we can use them
right we can just read and parse these
real locks or bin locks and parse them
and turn them into events because they
basically are events whenever we update
a hundred rows or just add one update
that's on updates we actually track a
hundred events in a real book and we can
mind this and encloses it and one
product does this other thing Golden
Gate for for Oracle is DB visit this is
just how they do it by T you parse DVD
locks they stored in an intermediate
format they called P locks here yeah and
and then you read them and you write
them out to Kafka and that's again using
Kafka Connect and then you can do
whatever else you can do a cough connect
you can use sing
to Hadoop elasticsearch like in this
example or anything else and both of our
companies our DB visit partners just
full disclosure and there's there's a
lot of products out there so any
database really has a product to do this
yeah a lot of them are free that of them
are some of them aren't like Golden Gate
obviously isn't but it has a ton of
flexibility in it I use my sequel in
this case and there's there's one that
I'm using for for my secrets called DB z
'm that's another patchy project that's
sponsored by Red Hat and the B's iam
tries to be the most general or most
supported Kafka producer for databases
so they do support Mongo puskÃ¡s and my
secret at this time and they are looking
to add support for Oracle and other
database you can think about them sort
of as the open source Golden Gate for
open source databases that's fair enough
so I used to beat him for this so I had
to download this connector and configure
this connector the configuration I don't
even show it it's it's we just a few
lines it's just pointing in at the
direction of your directory of a pin
locks and that's almost all you do the
output and excuse me that this big this
is the output for one event
so remember I had this orders table and
this is the output for one of them you
don't have to read all of that but you
can see it's this is a bro so or this is
actually JSON it turns into a for later
this is Jason with some nested elements
showing the actual data so that's all
the stuff on the right and then there's
metadata on on the bottom and that's
that's that's what the B's iam rights to
Kafka this is basically one Kafka
message this is too much for me I don't
need all the metadata I don't care what
the database ID was and at what time in
the database time this was created
because I have the order time in my data
so I need to flatten this JSON out to
just to just the bold part that I
actually need and I could do this
further down the line using Kay sequin
actually in the first iteration of this
demo I did and then I discovered that
there's the actual tool for this called
single message transform which you
specify right in the connector config so
in the connector country you can specify
certain pre-configured transformations
that can work on a single message at a
time so they can't look at other
messages and group and transform and
count we'll get to that with case equal
yeah but what you can do is you can
transfer messages
how's housekeeping type things you might
transformations going into Kafka right
you certainly don't want to transform
the raw data is valuable but if there
are things like just simply and uh
nesting things cleaning up things then
go ahead and do it on the way in so my
first my first transform is to just
filter out everything that's not the the
after the after element so that gets rid
of all the stuff on the bottom this is
still nested so now I need to flatten it
that's my next transform and now it's
flat and now it looks nice and this is
something I can almost use the one of
the problems it flattened the column ID
- after underscore ID so I renamed that
that's another thing a message transform
so this is actually what I want so now
this becomes my Kafka messages and they
get written into CAF can if we have time
for the demo we'll actually look at the
raw stream of these takes events as they
come in so this is now being written to
Kafka the the JSON the bottom or this
still goes yeah anyway things is that
for already yeah this gets written
notice the datatypes right that's why
it's written - Kafka and Kafka that is
one of these messages per every row that
I insert or every database event that
that there is so now we have four data
in Kafka so we have a my sequel demo
random data inserting we configured it
easy um so we have the events in Kafka
but they're still just single events
like we can't make much sense out of
them we could of course just process
right into a downstream system like that
say Hadoop and then run analytics on
them and what's the point if we could
just run data based sequel queries
against my sequel and so if you remember
that diagram from earlier it's that loop
that we can do now that curation loop
from inside of Kafka yeah so usually we
do see weird one ETL would extract the
data and load it into a data warehouse -
to do analytics on it but this is
usually done in batches and we can only
do so many so many per day or so many
per hour that's what we usually do but
hot topics right now are this I want my
data really fast I want real-time
analytics and real-time might still mean
five minutes of delay for a lot of you
that's still close enough to real-time
but also we get a lot of requests for I
want my data in real time vendors are
addressing this but things like
in-memory columnstore that's what Oracle
is doing to allow you to do report
on your OTP systems other things that
people are asking for us machine
learning AI that's something hot and
then the other thing that we what we do
in this demo doesn't show it but I had a
demo not too is other data type other
data sources like click streams IOT
devices like if you have IOT devices
that for example measure every car in
highway as it goes through and it's
speed you probably don't want to write
that data to a relational database to
then make sense of it because what you
want to see is you want to find out when
was the average speed in a single lane
slow because that's an indicator for a
traffic jam but you don't want that
information after your daily ETL because
it's no good to me to know that
yesterday's traffic was bad I want this
information really really fast
same with clickstream or transaction
data payment transactions you don't want
to detect fraud a day after yes happen
if you wanted it detected as it comes in
and so that's that's dream processing
and that's again that's not really new
there have been frameworks for the
stream processing processing event
streams as they happen for quite a while
so this is these are some of the
frameworks Apache beam is the other one
that I really like
storm flink are the or the older ones
spark streaming is is something that
they added to spark so all these things
exist the the common thing that they
have in common is you have to write
mostly java code to actually do your
analytics so you start with sequel and
then in the middle you have to write
java programs and compile them and
upload them to some framework to
actually do your analytics on your data
and that's where a lot of people
struggle because a lot of DBA of data
people are not great at java including
myself so what people ask for a sequel
give me sequel I really want sequel I
know sequel sequence awesome secret is
here to rule the world
and plus we don't have to do another
cluster so there's too bad there's two
major values here one is you get it in
sequel but also we don't have to spin up
another cluster all those things that
one had on the previous slide or
additional it some of them were
cloud-based and that's nice easy but
they do require additional
infrastructure right so just do it all
right there inside of Kafka which is
nice right so and that's that's at this
price this part is called case equal NSA
product that's also open source and and
free to use
yeah and I mean it is GA now and it's GA
now this lights are a bit old but it's
it's available now when we did this talk
the first time it was in beta but it's
GA now it works and Kafka does these two
things aid allows you to do the stream
processing and we get to what that
actually means in a bit and it allows
you to do it on the same class you and
her like you do spark or spark streaming
for that you have your Kafka cluster to
possess Kafka and Kafka streams to spark
and spark persists it and then spark
runs it and you have your separate spark
cluster that's a lot of clusters and lot
of machines to run and keep running if
all you want to do is do a little bit of
real-time analytics on some data because
now we run two clusters with at least
three nodes each that's a lot of
infrastructure so what's the special
deal about stream processing why stream
processing different from what we will
be all but we all know and what we know
is what I call bounded data sets we have
data set that has a start hasn't an
ending and it's it's finite and it's
it's consistent like a I know that all
the data in my database is usually
consistent with respect to each other
and I know that whatever I carry now is
it's actually now estate there's no late
arriving events and bound the data sets
and that makes that kind of easy if you
think about this imagine writing a
sequel statement that runs forever right
that's what stream processing is it's a
sequel statement that continues to run
as new events come in so when you start
thinking about how do you join datasets
that aren't consistent forever right
data is coming in we have to talk about
what do we do with these new records and
these old records as they sort of fade
out that's that's what is this whole
concept of unbounded a single statement
that runs forever sort of and and you
get things like late arriving data data
it's not in sync with each other and
data that doesn't have a beginning like
you you might keep data in your cupcake
caster forever but you might not and you
might not be able to really count or
something you would have to actually
count all elements in something that
doesn't have a start and an end that's
that's kind of hard to do so there's
three things that we might want to do
with unbound datasets the first one is
really easy I've already showed how we
do is is sing a message ransom the only
need to make one message look different
like flatten it or minutes do a little
bit to it yeah
just touching a single message and we
can easily do that like whenever a
message comes in just take this one
message and do whatever processing you
have to do to it and every framework can
easily do this this is easy and this is
just a reminder that was my my
flattening and division or in singing
mr. transform the the harder thing and
this is showing the example on a
boundary as it is counting things so if
I want to count the number of yellow or
purple elements
that's aggregating things together right
and that's that's quite simple we do
this on bounded datasets all the time
but on an unbounded data set how do you
do this how do you count something that
doesn't have a start or an end and the
solution is is really bad geocache
windows but it's not that kind of
Windows it's these kinds of windows so
you take your unbounded data set and you
overlay that with some kind of Windows
and now each window has a start and an
end and we can easily do things like
counting things or building averages and
other things based on each of these
windows and if you look at the different
windowing techniques of your ins going
to walk through there's no right or
wrong here there's what's right for you
do I want this you know how do I want to
slice this data into windows how do I
want to process it you start thinking
about fraud detection in those sorts of
use cases it's sort of up to you but
there's a lot of options right so this
looks simply enough this looks like oh
this is easy just do this and then
you're done it's not quite that easy
because you have to deal with things
like late arriving events there's no
guarantee that your events arrive in
order and the don't arrive late so what
if at 8:17 an event arrives an order
arrives that was made at 8:02 then my
system has to be smart enough to
actually put this thing into the window
that this element belongs to and then
recalculate whatever the group I or the
idea grid of that window was so that's
what makes this already a little bit
harder this window concept is called a
tumbling window it's not just case
equaling well that's what most of these
tools use and tumbling windows means
they're fixed in size so they're in my
case five minutes each and they do not
overlap so every element goes into
exactly one window my demo doesn't use
is my demo uses what's called hopping
windows and eventually I just have to be
not lazy and fill out the rest of the
the things so hopping windows have a
fixed size but they're allowed to
overlap
in my case I created one new window like
in my demo I create one you can know
every 15 seconds and every window runs
for a minute and what that gives me for
my demo is it gives me four windows that
update at any given time it just makes
my real-time dashboard look a bit more
interesting because otherwise I would
really only have one data point that
shifts in my in my dashboard
that's hopping and then there's the last
session type that or window type that KC
with supports and that's so-called
session windows where you can start you
don't have a fixed size you just start a
session whenever something happens and
then keep it open until a timeout occurs
so in this case I set my time up to five
minutes and I said five minutes after
the last yellow way then just received I
want to close this window and start a
new one that's useful for transactions
and click extremes and trying to figure
out what what is a user session right
when these things aren't literally
trapped so I should have an extra slide
in here that shows case equals so case
people has a command-line tool they
actually union between case equals CLI
to just case equal with with the GA and
it looks like C pop lasso it looks like
any other command-line sequel client and
you can run describe on things you can
run this on things and you can create
tables so in this case I create a table
on an existing topic that I have on an
existing stream and all I do is I say
create table as select stuff the sum of
the price of the sum of the amount from
my waters stream that's a stream in
unbound dataset use a window a hopping
window size 60 seconds advance every 15
seconds and group by product from the
auricle world what what this looks like
from like the Oracle worlds more like a
materialized view that constantly
updates right so what's happening when
he issues the create table statement in
Kafka is it creates another topic to
hold that data and spins up the Kafka
streams processes to perpetually keep
that table up-to-date with data right so
this is not just creating a table with
no data and this is actually creating
the data and the processing and spinning
it up to curate that data for you it's
really nice and so what happens now is
we have the source stream which is one
event per order and now every time one
of these comes in we update the window
or the windows because this could be for
it should be for that this event belongs
to and then the output is another Kafka
topic another Kafka stream with this out
potato that's that's no windowed so this
is how that looks like and if I would
run this in my calf gap will just keep
running we just keep running anytime
you're an issue a statement in case
equal it doesn't really give you an end
it just just teach you to use the limit
across yeah um yeah and you see on the
left that's the window start time and
you see the product and then you see
these the counter you can see that the
count decreases because the the last
olives window this one does really just
has been open for a few seconds and then
these have been open for a minute so
that's why they have the large numbers
in them these are the functions you can
use aggregate counts um there's no
average in here you have to calculate
yourself and you think about that
Everage on an unbound dataset is kind of
hard because what you do is you look at
the next event or the next the next the
event master comes in and you would have
to kind of calculate a new average from
the old average plus that an average
just really hard to do max amount is
simple because you just increment that
and yeah some is simple because you just
add that right but there's no average
that's I assume it's coming right I
don't think so okay but because you just
build yourself I mean that's the point
here yeah all right so we talked about
singing message transforms and
aggregates and there's a third time of
type of transfer that you can do that
that's with the the last the last
frontier and that's joining two data
sources together in unbound datasets so
let's say you want to in my example I
have users in my in my click in my order
data but I don't have the user name or
the user city and what if I want to
actually aggregate my joint a my my
orders by the city where somebody lives
in then I would have to join my order
state which only has a user ID to a
user's table that could be a different
data source altogether my users table
cooks clear coming from a different
relation database or from another
warehouse or anything really and joining
these two together in real time that's
the as the last thing you want to do on
on streams and this is currently only
somewhat supported in case sequel what
you can do is you can join a stream to a
table and the table is basically
a materialization of state kind of
replicates stating that you have to
define a key and then for each key don't
keeps the last so you saw the yarns
create table statement in case equals
also what's called create stream so so
Kafka tries to sort of play both sides
of the coin really in in state and
events right so when you create a table
in case equal it's asking you sort of
how do you you know what's what's the
unique key or the business key or what
is the unique identifier across events
when you say create stream it's not
asking that it just all data goes in
there not just the latest record so
create table kind of gives you the
latest record yep so and that's how you
would do this and case equals so again
say create table so I create my table
users I consume that from it cough cap
topic and I just put they take the key
as a user ID so that's my table and then
I join this table to my stream of orders
to to give me the in this case user
level so my users have a platinum gold
server level think so I joined these
together and then I can group them by by
that one minute and something left let's
just go with what we talked about today
so we we've started with this big
picture thing and I showed you the
dashboard we talked about well my secret
that wasn't hard to do but we talked
about the museum how to get data from
the museum into Kafka then we talked
about transforming your data in Kafka on
the same cluster and writing out a topic
that has to transform information in
this case the grouping by-product and
potentially the joining of a second data
source to this so at a time for the demo
my apologies well short the end result
we've showed the end result yes we saw
that an elastic because right now most
analytic tools cannot very Kafka
directly so we use graph honor to
displays in graph on R or Cabana or none
of these really today can read Kafka
streams directly I think that will be
coming in the next couple of months or
years Big Data Sync okay so for now we
have to buffer things in elasticsearch
just to have a way to get to it more
resources that's excellent blogs about
about cafeteria ranges to getting start
with Kafka or seeing other use cases and
and then the last thing that I yeah on
the bottom you see you just get up
and then Bjorn lost as my username and
you can find the code for the demo here
and Deker and the instructions on how to
run it how to get start with the docker
container it's dr1 and then the
container image name and you're up and
running and you can test the entire
thing runs in a single container sorry
that we ran a little long will take
maybe no probably don't have time for
questions my apologies well take one
okay ten seconds anybody's got a
question they can squeeze in in 10
seconds yep
do you mean it's not really it's not
really or orchestrated right I mean when
you say create table it spins up the
table and starts populating it right so
then you can you can create another
table based on that if you want and
another table based on that or multiple
streams but that wasn't the credit
question was the order of events can you
maintain the order I'm sorry and so
Kappa does not give you a guarantee that
the events will be ordered because they
might actually come from two different
CAF connotes what you can do is if your
ordering is by the order time for
example you can just stitch the data
back together on that but by its nature
Kafka does not guarantee you that data
will be in order
so you might get you might produce in
order and neatly but when you read the
data back in Kafka or when you produce
it somewhere else it might be in a
slightly different order so you don't
like their guarantee but it does you
know track the the the time it goes into
the topic that's sort of a sort of a
standard function of Kafka yep yep
so the IOT device probably has its time
but for the event and then you'll have
the time that that Kafka knows about it
more when it came into the topic and you
just have to figure out what which of
those what combination of those two
things works for you and what you can do
is you can under covers the way of calf
guarding those things is in what they
call partitions and if you ride to one
partition at least it reading from that
partition will be in order and how was
written to that partition but as you
system scales you might want to store
things in more than this one partition
and then that's when you get into the
awkwardness of you might get events out
of sync slightly so thanks very much for
attending we appreciate it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>