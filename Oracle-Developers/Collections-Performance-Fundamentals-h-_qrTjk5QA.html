<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Collections Performance Fundamentals | Coder Coacher - Coaching Coders</title><meta content="Collections Performance Fundamentals - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Collections Performance Fundamentals</b></h2><h5 class="post__date">2015-06-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/h-_qrTjk5QA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good afternoon everyone I'm Mike do we
go I work on the core libraries team at
Oracle this is the introductory session
on Joe some aspects of Java performance
using collections as a vehicle to talk
about some of the key things in building
concurrent performance with Java
presenting today with my coworker and
colleague Chris Haggerty and I'll let
him introduce myself I'll first mention
though that the slides will be posted on
my Twitter handle here so you don't need
to take notes about what you see on the
screen maybe some things that we say but
the slides at least will be available
online shortly thanks thanks Mike
so my name is Chris Haggerty I work in
the same team called ivories team and
I'm Mike along with Mike I also work for
Oracle I had okay just to move Mike
so we work for Oracle's so we've got the
standard disclaimer which you can pretty
much skip over so as Mike said this is
an introductory session so we're gonna
start off as slow so I'm going to take
the easy bit at the start Mike is
hopefully going to dive into more of the
details as it goes true so we're going
to be talking about some of the issues
with concurrent performance we're going
to be using some examples from the
collections API very simple
implementations but the use of
collections here is really just
incidental they work very well for for
the examples to demonstrate the issues
we want to talk about so in the
beginning a long time ago programs are
very simple there was just a single
thread so the program flow it was was
quite quite easy and straightforward
he started off your thread and did some
work and then it ended and if you wanted
to analyze performance you really just
looked at the algorithm that's trying to
that you're trying to implement that's
that's the process in your your work and
you you looked at D and then to CPU
cycles you were using so it's pretty
straightforward to try and analyze your
performance while it may not have been
very easy to fix performance issues and
to make performance better it was kind
of more understandable so skipping
forward to more modern times as you're
probably hearing throughout the week
with lambda and streams API and things
like that program flow is much more
complicated it's parallel and frequently
is it's concurrent but in essence you
still have a number of threads they
start they do some work and then they
finish if you're analyzing performance
though you're probably more interested
in the transactions per sec
a true put the latency to response times
and also the utilization of your
hardware so as Hardware gets more
advanced and more beefy you obviously
want to take advantage of that so you're
going to be interested in is your easier
program using as much of the hardware
that's available to it as possible so
here is a very simple example using
ArrayList it's an implementation of the
add method so you're trying to add an
element to the ArrayList it's not the
implementation that's in in the JDK but
it's just further simplified so we can
we can talk about somebody the issues
when you're running this kind of code
concurrently so here the work that's
been done by the method I guess can be
split into four individual steps first
you read the size of your list and you
write the elements that you want to add
you increment the size and then you
write the size back to the list it's all
quite straightforward very nice very
easy to understand it's probably quite
common for most people so this works
quite well but clearly it's not going to
scale it's limited to running within a
single thread there are different ways
to get better performance out of a
method like that and you could run it
sequentially in multiple instances and
sometimes that's that may be the right
thing to do but you know in other cases
it's not again your your hardware is
evolving you've got multiple CPUs like
everybody wants to use them but as you
start adding threads you need to start
worrying about the coordination and the
interaction between these threads and
make sure that your still your program
is functioning correctly
so as I said I mentioned earlier the
simplest model is a sequential model
where you've just got a single tread
it's performing a single task it's in
blissful isolation it doesn't care what
happens outside of itself it's not
interacting with anything so it's it can
run runaway no problem there's also a
parallel mode where you've got multiple
threads performing multiple tasks and
similar to the sequential mode these
treads are running in isolation they're
not they are they may not be interacting
with each other so there's there's no
problems you're not worrying about
coordination or issues between the
treads and then probably the more
complex mode is in current mode where
you've got multiple treads collaborating
on a single task and now you're into all
the sorts of issues that threads need to
coordinate and with each other make sure
that you've got no contention and that
your program still operates as you would
expect it to so we go back to our simple
add method the source code hasn't
changed here from what what we seen
earlier but here now we have we have two
threads that are trying to add two
different elements to the same list so
the steps are the same just the same
four steps you read the size you try to
write the element that you want to add
you increment the size and the right to
size back to the list so you would hope
after after this that you will have two
new elements in your list and your size
will be incremented by two well this
this is not always the case because the
two threads are racing the DICOM is
non-deterministic you may end up with
just a single or most likely end of what
is just a single element being written
to the list you may end up with two
elements being written sorry one element
being written but decides being
incremented by two you just don't know
so clearly this is not this is not going
to work
so this graphic is basically
demonstrating the issue the treasurer
are the trains represent the treads boat
steaming ahead trying to get their work
done and all of a sudden you don't get
the outcome that you desire so I
mentioned that Desai's may only get
incremented by by one when you've got
two threads that are trying to add
elements so to resolve that problem we
could add volatile to size this will
ensure that boat treads seen up-to-date
value for the size you're not going to
see a stale value but adding volatile is
not going to solve our problem because
we still have the issue of concurrent
updates so while the size may be
maintained correctly we will not
guarantee that the elements will get
pushed into the list so it's a pretty
obvious thing to do we just add the
synchronized keywords so the code stays
exactly the same just a method
declarations has changed you've added
their synchronised keyword and this
change is to flow somewhat we've added
two new steps one before and one after
you can see when you enter the other
methods the first thing it does is it
gets the lock on the list and it
continues as before it reads the size
adds the element increments the size and
writes decides back to the list and
finally when you exit the method you
released a lock on the list so this
seems to give us the behavior we were
looking for I'm reliably informed that a
guy named Ike stir a Norwegian guy came
up with or invented the canal locks so
this is kind of equivalent to what our
synchronized is giving us only one tread
can enter the lock at a time it can stay
in the lock for a certain period of time
and then it exits just like the boats
going through to the canal lock
if you've got any questions on this side
like we'll have to answer them so here
is our method again but we've added an
extra tread that's also trying to add to
the list we can see the the lower part
of the slide on the right hand side the
another treads tries to add an element
as the first tread is already in in
progress of adding its element
so clearly the second tread gets gets
blocked and most wait until the first
read completes so this is giving us our
sequential access through the to the add
method so this is the seems to be what
we want so this slide is really just
representing we have a number of threads
not just two clearly each each add is
not overlapping with with another add
and we're getting the behavior we want
to see the sequential type behavior I
guess what's not shown here as we've
seen in the last slide say each add may
actually try to be try to happen on the
threads before a previous previous one
is completed so it'll get blocked out
and wait just as as we've seen in the
last slide so I mentioned the possible
use volatile for the size fields earlier
on and now we've kind of moved to this
synchronized model do we do we still
need to do half sized bean volatile well
the answer is no we don't
and in fact keeping besides volatile can
actually reduce the performance in this
case so the reason we don't need sized
mean volatile is because the
synchronized block our method now
ensures that any writes that happen
within us are visible to other threads
so that gives us the visibility that we
were looking for that we try to achieve
with a volatile earlier and the reason
the volatile could actually slow this
down is because each time the true
through the loop you're trying to clear
the
the size variable will have to be reread
because it may be updated by by by a
different thread so if you make size
non-volatile it means that the compiler
can extract size from the loop as it
knows it can't be can't be modified
outside of this method so synchronize
sum has got kind of a bad rep over the
last number well it always had bad
reputation as somewhat hurting
performance but I guess in this case to
ensure the correctness of the behavior
of the the program we need to to have
this coordination between threads and
clearly there's going to be some some
kind of overhead to that coordination
but in fact you know in modern VMs
synchronized a lot of work has been done
and synchronized and it's not actually
as heavy as people may think so there's
a number of optimizations listed here
I'm not going through them all so some
interesting ones are the case quite
quite commonly locks our own contended
and in that case the VM can optimize
away a lot of the the heart or the the
low-level hard hard locking mechanisms
by just updating a value in the the
instance header to to to the thread that
actually owns the lock and leave the
heart more heavyweight locking
mechanisms to when the lock actually
really gets contended so when you get
more than you've got several threads
trying to get in at the same time locks
typically are only held for very short
periods of time as well so spin spin
lock technique can work very well where
you don't if you find a lock is held
rather than again instantiating and
creating the heavyweight low-level
locking mechanism to tread may spin for
a certain period of time in the
expectation that the lock will actually
become available and I thought can be
quicker than
they say using the harder locking
mechanism in the VM so this is where
things get a little more complicated
cylinder template so we've started
looking at the an ArrayList than just
the add operation so now we're gonna
switch over to looking at a queue and
both the operations of adding to the
queue and also removing to the queue so
the the put method looks very similar to
the add method that we had had defined
for ArrayList but this queue operation
in addition to just adding elements to
the list as some additional behavior and
in the case of this put method that
additional behavior is is that if the
queue is full it waits until there's
space available in the queue to stick in
its element it rather the alternative
would be oh there's no space I'll just
fail and in fact
Java's queues provide both alternatives
you can either wait to wait around until
there's some space for you to put the
element you've got into the queue if the
queue is in fact bounded or you can fail
immediately or if the queue is
configured to be unbounded then you
never end up having to wait because it
just adds it on to the end of a linked
list a key point here is that even
though this is a synchronized method
when you wait the you release the lock
temporarily so that anybody else who
wants to do things on that object can
continue to lock the object and perform
their behavior if the lock wasn't
released and we decided to wait because
the queue was full well obviously no one
could ever remove an item from the queue
which wouldn't really help anybody so
the opposite side of the put operation
is the take operation and in this case
we're blocking as well but we're
blocking on a different situation in
this case we're blocking
whenever the queue is actually empty so
we'll wait around until some item
appears and when that item finally
appears we'll remove it from the the
array of elements we'll remove the first
element in the list it will always be
the in this case the the zeroth element
and we look at the operations you know
they're like the arraylist ones that
happen to be very you know ordered each
thread takes his turn via the
synchronization and in fact the only
difference we need this slide and the
one for ADD is I changed some of the
plots to or some of the ads to puts and
some of them to takes in in this setup
though we've got two kinds of waiting
now previously we had waiting for
contention meaning that one thread was
doing some operation on the queue and
another thread tried to do some other
operation the second thread coming in or
any number of other threads coming in
would have to wait around until the lock
was available to perform their operation
that's referred to as contention waiting
to acquire the lock and this is the
overhead that comes with concurrent
programming is that multiple threads are
attempting to at the lock and while
they're waiting around for that to get
their turn at the lock they don't get
any work done other than waiting and of
course it's very important to measure
this if you've got 1000 threads
contending on a single lock most of
those threads are going to spend all of
their time waiting to get access to the
lock and very little time actually doing
any work that's going to of course be a
serious problem in the performance of an
application the other kind of waiting
that we have here is what was called
starvation waiting something about the
resource is unavailable in this case
it's either free space we're awaiting
the condition that some free space is
available in the queue or we're waiting
for an element to come available
this is sometimes entirely natural if
you have a web server up and running
well there's not always requests in the
middle of the night maybe threads just
sit around and wait for people to wake
up I know the threads waking up but
actual people to make requests to wake
up sometimes the starvation can also be
caused by contention and we'll look at a
bit into that later this is also
important to measure as well and the
reason it's important to measure is to
determine what the cause of the
starvation is some starvation conditions
are going to be natural just there
aren't requests available but other ones
there may be a root cause to the
starvation condition that you have to
diagnose and adapt the program so that
you avoid that starvation condition so
here's here's a situation that we have
in this queue and it shows the
starvation situation so we have a thread
beginning a take operation but in this
particular queue there aren't actually
any elements available so it has to
sleep and wait around until the first
element appears in the queue put there
by some other thread and the same thing
happens on this other thread where it's
waiting for the second foot to to take
place that's just normal natural
starvation of the of the consumer
threads we have a different problem here
in that we have one thread putting
elements into the queue and other
threads taking them out but we if you
look at this closely you'll notice
something odd here this thread began
waiting for elements in the queue and
then this thread began at some point
later on but this one got the second
element so why didn't this thread which
was waiting longer get the element with
this we went that was added to the queue
when it was added well it turns out that
normal Java locks
don't worry too much about fairness so
if there are multiple threads waiting
the operating system or Java doesn't
make much effort to decide who was
waiting the longest it regards that oh
you know some anybody who's waiting is
good enough I'll just hand it off to
whichever one I choose to and you know
if there's enough throughput eventually
every thread will get some resources and
I don't need to worry about making sure
that the resources are handled are
handed out fairly to other to each
individual thread sometimes that matters
or in most cases if you have I have
enough throughput the operating system
is fair enough that it doesn't actually
matter every thread eventually gets some
elements in other cases depending on the
type of application it may actually
matter in order to provide fairness so
we'll look at one mechanism that you can
have in order to provide fairness the
normal Java synchronization doesn't
provide fairness if you want to do
fairness you have to go to an explicit
locking object there are multiple kinds
of locking objects available they all
implement the lock interface in this
case we're looking at the lock object
without particular regard to what
particular a flavor of lock it is it's
you know of course the beauty of Java
interfaces is that we have an interface
contract for lock and know how a lock
behaves what the exact implementation of
the lock is is kind of irrelevant to our
actual example so different than the
synchronized version of our take method
we now have an explicit lock step this
is the equivalent of entering this incra
nice block one of the things to standard
with explicit locks is that we have a
try finally block around all of the code
that actually happens inside or while
we're holding the lock the reason for
this is that if this block exits for any
reason we want to make sure that we
release the lock unlock it so that some
other thread can continue to use or can
can start accessing the lock if we
didn't have this try finally and for
example this thread
an out of memory error or somebody who
decided to kill it in Java con RJ
console or something like that it's
possible that the lock would not be
released so having the finally block is
is quite important to ensuring that the
lock is made available to other threads
when you're done with it it's good to
just get in the habit of always putting
a try finally around every usage of a
lock the the inner part is kind of the
same where we're waiting for well the
size is zero we await entries to we wait
an event whenever something happens on
the queue now we'll be woken up up by
this signal and we'll check to see if
the size is nonzero when the size is
finally non zero we'll move on to remove
the element from the queue just like the
weight method that we our weight method
that we had in the previous synchronized
example the signal Dada weight sleeps
the thread and temporarily releases the
lock in this case so that we wake up
other threads who may be interested in
the condition where they're trying to
stick elements into the queue we also
when we've removed an element signal
that conditions so that the threads do
get woken up and notified and finally we
just return the result again you know
our goal in this event was to in it to
show lock fairness and this is exact
this is how we actually create the lock
with fairness the default which is very
similar to the behavior synchronized
with reentrant lock is is what you is
what you would create in the first case
but if you care about fairness you can
provide a boolean true using a
difference lightly different constructor
to to get that fairness and
with fairness the longest waiter
generally goes next
so it'll do complete round-robin among
the threads who are waiting in the order
that they've been waiting for them
having this fairness on the locks is not
free in most cases that that random
behavior of who goes next is entirely
adequate if you actually if you require
that the lock actually keep a list of
what the order is that does have some
costs so and if you would care about
fairness you have to you have to be very
sure that you care about fairness in
order to to request it so if we do have
fairness we have a bit of a problem here
so if we have the first thread put two
elements and then another thread stick
another element in we have a four thread
coming in to put a fourth element while
our little array only has room for three
elements so this fourth thread starts
waiting for the condition that there
becomes space in the queue only at this
point does our fourth thread which is
trying to take an element from the queue
get an opportunity to execute well it
was waiting all along and it was very
interested in the situation of their
elements in the queue this trying to be
fair to every thread and being evenly
balanced about which thread gets to go
next
produce a situation that we had to wait
until the queue was absolutely full
before we got the opportunity to have a
thread which was taking elements from
the queue get its turn even though they
were all waiting the same amount of time
it'll be nice to be able to bias that so
that we can wake the appropriate thread
rather than just some random thread and
in fact you know as we've previously
described we in fact have two conditions
here in this queue one is that there's
space available which some threads
wanting to do puts are going to be
interested in and another condition of
elements available which only the
threads that are doing take operations
are going to be interested in
so we recast our take method again so
that rather than just looking for a
single signal that's used for for waking
up other threads we now have split it
into two halves
so a taking thread and awaits on the
condition not empty and when it actually
removes an element from a thread it
signals on the condition not full so if
we look at the opposite side of the put
method we see that in fact we've got it
now reversed a put method is what
awaiting not full and signaling not
empty so these two coupled together
signalling on opposite conditions are
going to give us a better behavior in
that they'll wake up the threads that
are waiting on their opposite condition
rather than everybody just contending
over the same signal and just waking up
a random thread and in fact we see that
with this different kind of signalling
we have a better behavior here for the
take operation so take gets in ahead of
the additional put we haven't talked a
lot about the cue sizing our little
example used a very small fixed size cue
of three elements the characteristic so
your your design decisions in queueing
are actually pretty important and
they're very dependent upon your
application it matters what your actual
goals are as we kind of mentioned is
sometimes natural for consumers to not
be busy because the cue is in fact empty
if you're looking at sizing a cue or
configuring a cue you have to be very
mindful of what particular
characteristics you're trying to achieve
if for example you're just distributing
work among consumers it might be
reasonable to have a bounded queue that
has just enough space for each consumer
so basically it would mean that for
every thread which is actually
taking work from that queue there would
be an opportunity to have an element
waiting for it when it when it went to
get another element it's if you're just
if your goal is to flatten out bursty
inputs so that sometimes elements which
are sticking things in the queue put a
lot of elements in and then don't put
more elements in for a long time you're
just trying to swallow big bubbles of of
data coming into a queue it might make
sense to have a nun completely unbounded
queue but if you have an unbounded you
you have to be careful that the
consumers of the elements from that
queue actually get around to doing all
the work if you have an unbounded queue
and the consumers can never catch up
well eventually you'll just fill all of
memory with requests that aren't being
serviced so look carefully in your
choice of queues and how the queues are
configured and the size is configured
also look at the performance or monitor
the performance of the queue and make
sure that the queue has the utilization
you're expecting if producers are
contending heavily are waiting lots of
time in order to stick their elements
into the queue depending what your goal
is that may or may not be reasonable but
you need to check in order to make sure
it has the behavior you expect so one of
the things we did in the example is
update the size field so in addition to
sticking elements into the array or or
queue we're also updating the size field
of how many elements there are that
operation is pretty lightweight if that
was the only thing we were doing
updating the size field using a
synchronized lock would probably be be
like using a 20-pound hammer to pound
finishing nails it would just seem like
we're doing way too much in terms of
handling the tension for the tiny
operation that we had to do of
incrementing the size a size counter so
there's actually an alternative
available called atomic opera
Asians and what atomic operations have
as their concept is is that I wish to do
some very small operation and all I need
to know is whether the operation
succeeds or fails and if it succeed if
it doesn't succeed I'll just try again
if it succeeds then I'll continue on my
merry way and the expectation is is that
if contention is reasonable on the
atomic variable then most of the time my
simple request to update the variable
will succeed and I won't have to repeat
my attempts and I won't have had to go
through all the trouble of a
synchronized block so we'll we'll look
at an example of what would be
equivalent to updating the size field of
that ArrayList and we're going to use
the atomic integer class and our little
method here is increment the size field
and return whatever the new value is so
we enter the block and again as I
mentioned the behavior of Atomics is is
that we try to do the operation and if
it didn't succeed we need to continue to
try so we have a do-while loop where we
get the current size decide that we're
going to increment it so next equals
curve plus one and then we're going to
call compare and set which is that we
say we expect that the value of the size
field is this value occur and if it is
then update it to next if it turns out
that somebody else is snuck in ahead of
us and the and the value of the size
field is no longer what we expect then
this operation would fail and we would
just try the whole thing again
eventually we'll succeed that's where
the the little title here is you know
insanity is the expectation that if you
repeat the same thing you'll get a
different result well in atomic
operations you in fact count on that
behavior you try the same thing over and
over again and eventually you will
succeed as it turns out this increment
in get method
is already available on atomic integer
we didn't actually have to write this
method ourselves the and its
implementation on atomic integer is in
fact very similar to exactly what we
have here and and there are some other
convenience methods on atomic integer
that are similar like you can add to an
atomic integer etc you don't have to
code all all the simple operations
yourself if you wanted to do something
more complicated that can't be conceived
of by the the authors of atomic integer
like you want to multiply it by three
they don't provide a method that does
multiplication by three of the current
value so if you needed to do that you'd
have to do something using just this
compare and set loop go ahead I changed
my example and forgot that sorry about
that so this would need to be int and in
fact the atomic integer classes return
type is is int so we haven't yet figured
out where we're going to use volatile
keyword that we were promised before
you'll see in a lot of older code a
particular idiom that does use volatile
and what you'll see is something like
this which is synchronization to update
a variable so that you can be sure that
no other thread has the opportunity to
interleave operations so that the values
written to update size are coherent but
size is read via volatile this is
actually what was very commonly used
before atomic integer was added to the
JDK you'll see this particular idiom in
a lot of older code so if you see this
particular style where reads are coming
through a unsynchronized method of a
volatile field and writes are going
through a synchronized method you can
replace that with atomic integer
operations and in fact
you'll see much better performance I
think that if I remember correctly the
random number generators were switched
from using this particular idiom to
using atomic integer and it was about a
30 percent performance improvement in
most cases and when the contention when
there's very heavy contention it's going
to be even higher than than using
synchronized so we've mostly talked
about concurrent things here and threads
interacting and we had that initial
model that Chris showed of sequential
parallel concurrent well the problem
with concurrent is is that all of the
threads are working upon the same task
and the price you're paying for that is
coordination among the threads the
parallel situation where you had every
thread working on its own task was
pretty great because threads had to only
worry about what they were required to
do they didn't have to coordinate with
any other thread well as it turns out
there's a relatively or for some types
of problems there's an easy way out of
having to deal with the concurrent part
which is to divide up the task
so rather than considering the task be
performed as a single task if you can
decompose the task into multiple
subtasks you can in fact hand off those
subtasks to individual threads and you
get behavior that is a lot more like the
parallel operation if for example you
wanted to add up a whole lot of numbers
you could do a single atomic integer and
have many threads trying to update that
single atomic integer variable as they
add up numbers from an array and you
know each thread would have to keep
track of which are there would be a
single counter about which numbers the
next to be added to the accumulator and
there would be a different atomic
integer for the accumulated value a
better approach is to say you
the first hundred thousand numbers you
do the second hundred thousand numbers
and when you're done I'll just combine
your results that's a much better way to
handle that particular problem of adding
numbers than it is to try and go fully
concurrent on the on the problem and in
many cases it doesn't even matter that
the problem is fully decomposable
obviously something like adding up
numbers the model of what each
individual thread does are pretty much
the same if the problem was something
more complicated like processing the
image processing some kind of image data
if the source images were different than
each thread might take a different
amount of time so it's not necessary
that the exact division of the problem
be perfect but the fact that you're
going you're able to divide up the
problem into subtasks is usually a big
step forward in and of itself so there
is an alternative to doing concurrent
operation which provides easy access to
the parallelism of current systems being
added in java 8 this is the lam the
streams API that you've probably been
hearing about for many types of problems
that are in fact decomposable the stream
2 api provides a much easier mechanism
for doing that than was available in the
past and it gives you that parallel
execution that was in many cases in the
past so hard to achieve I know in the
keynotes Brian gets showed an example of
using the fork/join process framework
that was introduced in Java 7 to add up
numbers and it was about two pages of
boilerplate which was all about mostly
about court setting up the coordination
of the threads using the fork/join
framework and very little of it was
about the actual adding up now
members well the lambda streams API
frees you from having to handle all
those coordination aspects all you have
to provide is the business logic and the
streams API handles all the coordination
aspects of it so for example you know
got a simple piece down here the only
pieces that that you as a app developer
have to provide are the logic elements
such that I only care about students
that are enrolled and the thing that I
want to get the highest of is the
students grades and the thing I'm
interested in is the max maximum grade
all of the other parts of this in terms
of coordination of how which students to
hand out to what threads the the adding
up of the subtotals so that if each
thread process is ten thousand students
the coordination of saying okay my Vout
the value from this thread is is for the
value from that thread that's five ok
the value that's closer to the max is
five and if you've got 50 threads you
have to you know merge all those values
together to come up with a final result
the framework handles all of that so it
makes the move to parallelism a lot
easier and completely nearly completely
frees you from all of the coordination
aspects one thing that is important to
note is is that it doesn't entirely free
you from the coordination aspects in
that parallelism in the Java eight
streams API is explicit so we in fact
had to say parallel stream here we could
have said stream and it wouldn't have
been done in parallel the reason that we
require you to specify explicitly that
you want your operation done in parallel
is is that when you request it you're
providing an explicit assertion that in
fact there is no coordination between
the threads and they don't interfere
with each other you're you're signing
the non-interference pact between the
threads that are going to be used to
perform your operation if for example
you're enrolled is enrolled method did
something evil that updated some global
state that might not produce
reliable results if in fact it's
executed from multiple threads
simultaneously and that's the kind of
behavior that you want to avoid in the
parallel execution situation so when you
request parallel streams you're
basically saying I have no interference
between the threads that are going to be
used to perform my operation some
resources that are available for
continuing with looking at Java
performance and looking at concurrency
you know we mentioned the of course the
Java streams API that's coming in Java 8
I think that that's going to make things
a lot easier in most situations for
getting very parallel programs without
having to worry about a lot of that
overhead measurement is an absolutely
key aspect of anything to do with
performance you know you can optimize
the heck out of a single method but if
the utilization your system says we're
and contention results in each thread
only spending one percent of his time
working in 99 percent of its time
waiting on a contended lock then the
degree to which you optimize that
particular method is almost meaningless
so having good measurement tools such as
Java Java flight recorder which is now
part of 7u 40 it's a free for for
development use up until your app ships
you can use it completely for free
Oracle charges a license for commercial
deployments of Java flight recorder
Mission Control for 4 production
environments some book resources that
are very good to look at further into
the concurrency issues if you choose not
to use the streams API and want to be a
bit of a masochist and work on handling
concurrency problems yourself Brian
guesses Java concurrency and practices
you know very much consider the top book
not to be surpassed about Java
concurrency
and there's been a number of Java
performance books over the years and
many of them had a tendency to get dated
because they were tied to particular
hotspot versions or provided advice
about you know only specific products so
Charlie hunt came out with a book at the
end of 2011 looking at Java performance
it's much less specific to a single VM
version it's not kind of frozen in time
like many of the other Java performance
books have been instead it gives you you
know better conceptual modeling tools
for how to understand the performance
model of your application and what to be
looking for so the characteristics of
what to be looking for and and how to
understand program performance are the
key tools that it's providing and those
tools are you know if you have something
good like Java flight recorder to
actually deliver you data about what's
going on the tools that the Charlie hunt
book gives you are going to help you
interpret those that data and come up
with solutions for changing the behavior
program to fix performance problems so
we can we can take some questions now go
ahead waves back
I'm sorry I can't hear you can you hear
Chris
oh you're asking about performance of
discreet locks versus synchronization we
can get another question closer in so we
can actually actually hear it sorry
about that
so you started with a packed house</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>