<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Portable Cloud Applications with Java EE | Coder Coacher - Coaching Coders</title><meta content="Portable Cloud Applications with Java EE - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Portable Cloud Applications with Java EE</b></h2><h5 class="post__date">2016-09-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/nCqVSf5v37s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay we'll go ahead and get started so I
wanted to just quickly say we presented
on the programming model yesterday for
those of you who were here and this is
sort of a continuation in some sense but
not necessarily from the same talk so
we're here to talk about portable cloud
applications using Java EE so the focus
is around proposals for java ee 9 so I'm
regime or Danny and I have Josh here and
Joe DePaul and we'll be talking through
the packaging and deploying and running
the applications in the cloud mostly
today so this is the standard safe
harbor I'm not going to spend time on
this but what I do want to point out
like I did yesterday for those of y'all
go here these are initial proposals or
thoughts that we have towards 79 it will
go through the standard java community
process of you know going going to the
community process of going off the specs
and what will be standardized is is
something that will be determined by the
expert groups so we'll do a quick recap
of what we talked about yesterday in
terms of programming model and then
we'll go straight into the packaging and
provisioning part of it we have a small
demo with a proof of concept that we've
done and that's how we'll wrap it up
basically so you know we want to
acknowledge that programming the
development style is changing with cloud
and microservices becoming more of the
focus so we want to make sure that Java
EE continues to be relevant for
developers and provides the necessary
infrastructure needed for in a standard
fashion for people to be able to use and
develop their applications and run in
the cloud using the new style of
programming so over the past few years
the weight of the whale applications are
written has changed very rapidly instead
of having you know
a couple of war files or ear files with
your servlets in each of these package
these applications are now broken down
into many small services each package
independently and deployed independently
and with the focus on solving certain
business needs itself it also helped in
basically having a continuous delivery
in the cloud and basically developing
and deploying in a rapid fashion and a
lot of these applications are now
becoming basically a distributed
application with lot of remote calls
that are being done asynchronously one
thing that we've seen in the in the
industry is that with this model while
it's easy to develop and control your
small applications it becomes a little
hard sometimes to deploy and manage all
these small micro services what we used
to be potentially two or three files now
two or three tier files are now broken
down and spread across maybe 10 20 micro
services or maybe even more in some
cases so in terms of the programming
model right what are some of the trends
that we are seeing that each of these
small micro services that are written
they are interacting each other by
making remote calls using rest and JSON
and that results in a lot of basically
remote calls that having made people
aren't using the Global's or shared
transaction typically in this in this
new world and they rely on even the data
to be eventually consistent so across
all the services if there are any if
there is any shared objects that are
that are there between multiple services
reactor style of programming is also
becoming very popular so instead of
having synchronous method cause you have
asynchronous non-blocking method calls
which are dealing with mostly as a
stream of events or data that you're
getting so with all the remote calls and
asynchronous in place what we have what
we realize and recognize is that you
need to have resiliency in place for the
applications that are being developed so
we are also considering adding things
like support adding support for things
like the popular patterns for resiliency
where you have health check circuit
breakers and
bulkhead patterns so that it makes it
possible for these applications to
recover and or fail gracefully when you
have a lot of these remote calls which
are failing due to ethernet for clayton
sees our services not being available we
went through this yesterday but we'll
just recap here so as part of Java EE
nine some of the areas of focus
basically are two base enhance their
programming model where you have we have
we want to add the capability to have
reactive support have support for
eventing in the platform of course we
continue to enhance the existing rest
and HTTP to support that is already
there in the platform on the packaging
front we want to be able to package your
applications into these micro services
with just enough runtime and that's
going to be the most the focus of a lot
of talk to the airport packaging and
running and how you can package may be
more than one of these micro services so
that you can deploy the whole
application as a whole in in the cloud
in in the new style of programming
you're also seeing that you know the as
opposed to using the traditional RDBMS
is people are more starting to move more
towards no sequel style based databases
like the keystore key value store or
document based databases in in this
environment a configuration basically is
also an area of concern where we want to
externalize the configuration from the
service so that the services when
they're brought up in the cloud they can
be essentially stateless and ephemeral
and the statist a are stored in a
different here all together when you
start running in the cloud when you
start running applications in the cloud
you want to make sure you get the
maximum throughput front from the
environment that you're using so if your
application is basically a sad style of
an application you may want to add
capabilities for multi-tenancy as well
and so for that we want to see for those
type of applications we want to see if
you can optionally add capabilities to
get tendency related information in an
application so you can do the right
switching for the tenant based on the
incoming requests I already just upon
resiliency we want to add support for
things like circuit breaker patterns
bulk
and health checks on the security front
we also recognize that you know people
are moving more to the new OAuth and
open ID connect as as authentic Asian
mechanisms so in the platform we want to
make sure we add support for the for
these for these standards as we as we
basically move forward this is basically
a recap of the what we think the
architecture would look like you have
the typical hypervisor with a container
like darker on top of which most people
these days are trying to deploying their
applications in the cloud and what you
see there is what we are saying we would
like to propose in the of 89 timeframe
for the run as the runtime for your
application development using using
microservices style in the cloud and
then you have see you can see that there
are other other infrastructure pieces on
the left which span across all the
entire architecture things like
reliability security and management and
orchestration and on the right hand side
what you see is the services that the
typical cloud provider would provide and
their applications could then leverage
from from the cloud provider so that
sort of recap soda too we started
talking about yesterday I'm now going to
hand it over to Josh to cover in detail
more about the application portability
select extra chief so even if we have
all these great standards for writing
our applications in a more micro
services based architecture things like
eventing and reactive getting the state
and the configuration out of the
application make it more cloud friendly
that's still not enough in order to
truly make our cloud application
portable from one vendor to another
we've looked around at some of the other
vendors that have mature support for
micro services based applications and
they do things very similarly but it's
all implemented slightly differently
what this means is that if you develop
an application for example in Amazon AWS
it's not so easy to just take that
application without modifying it and
take it to a coop
or maybe a marathon measles and set up
so we'll be looking at how we can take
some of the common solutions that the
other vendors have come up with and see
if we can examine those first standards
so we're going to be focusing on how we
get a truly portable application you
might think that if we have a
distributed style microservice
application its distributed in nature so
is the cloud so we should just be able
to push it up to the cloud and
everything will work right well that's
it's not that simple there's other
concerns that we have in the runtime
things like how does my application
define the resources that are required
in order to run things like how do I
influence where my application might run
within the club there's I'll get to a
little bit more details about the
thermal placement later but we may want
to influence it to get these services
closer together together for low latency
there's also the concern of service
discovery for those that are familiar
with microservices all of your services
are distributed throughout the cloud and
you might need to find other back-end
services and communicate with them so
this is also an issue and the vendors
implemented differently in each
situation we also once we get all of our
services up and running and
communicating we also want to make sure
that they stay up that's one of the
promises of micro services is the high
availability and the faster delivery
with with versioning and having
stateless ephemeral services being
replaced easily while I give the talk
there's there will be quite a bit of
infrastructure that i'll be covering
that is common cloud infrastructure it's
not something that we plan on trying to
produce a standard around this is just
something that we acknowledge is is
fairly common out there and that we're
examining what in that space that we can
with that we can in fact standards so I
have a key at the bottom you can refer
to for later slides in order to address
these portability issues i'll walk
through the typical life cycle of a
micro service basically after you've
written your business logic what happens
next well you'll want to package it up
you want to produce some kind of
artifact that can be delivered to one of
your ear cloud vendors
and then you'll want to be able to
provision it within that cloud system on
to some set of machines virtual or
physical and you need to describe where
it runs what resources it needs that
kind of information and then as I
touched on before with service discovery
you want to be able to basically connect
these services together how do they talk
to each other how do they talk to
services that the cloud vendor might
supply like databases of service cash as
a service config as a service those are
those concerns and I talked about
availability now that you have your
application up and running in cluster
talk to each other how do you keep it
that way how do you make sure that it
has the highest amount of throughput
possible even with expected outages and
then last but certainly not least you
want to plan on versioning what kind of
there are varying levels of support out
there for efficiently rolling out
different implementation versions of
your individual services within the
complex application graph and we want to
see how can we best take advantage of
those strategies with specific
information around the services
themselves first for packaging we want
to rely on the java SE 9 modularity
capabilities in order to produce just
enough runtime for your for your package
we imagine that there will be some kind
of cloud artifact that can take
advantage of the key java ee java ee
api's and get packaged up in a way
that's that's light and can be portable
from environment to environment whether
it's an Oracle cloud or perhaps a red
hat or IBM cloud anybody that supports
the Java EE a Java he heat 9 runtime
excuse me so going into further in that
if you have such a artifact than you can
imagine the platform has to be
intelligent about what it does with such
an artifact what what happens when you
kind of quote unquote for lack of better
word deploy it to the environment in
that case we imagine that the artifact
will have certain annotations or
metadata that can be useful that the
platform can
inspect and use for things like auto
wiring how do you connect to your
configuration as a service eventing
logging the other things that the that
the environment might provide and that
kind of goes hand in hand with using the
modularity we imagine that the platform
could take advantage of the mob I a c9
modularity and utilize things like Jay
link in order to get just the right size
runtime once you have provisioned your
your module so to speak into the
environment that can then widdle things
down to get just the package that it
needs for the runtime also we should
examine where things have gone with in
the cloud environment with
virtualization and technology advances
today we have we've gone from simple
virtualization zhan the machines so now
de facto containers and we'd have to
mention that the docker should be one of
those containers it's more opaque than
what we imagined in a cloud archive a
cloud archive could contain things
internal to the application that the
platform can inspect whereas in
containers a lot more opaque we should
still be able to support that in in a
cloud platform wiring microservices but
it certainly presents a disadvantage in
according to comparing to the the
archive that could be inspected by the
platform so next up is provisioning so
we have some kind of a cloud archive
some artifact and we need to describe to
the cloud environment where this archive
is going to run what kind of resources
it needs and maybe influence the
placement so in a typical microservices
supporting environment it's abstract you
don't know where the services run you
don't know what kind of machines are on
if their virtual or physical what ends
up happening when you when you provision
them is they just run somewhere within
this abstract environment so that's
important to keep in mind as we talk
about how these things get provisioned
the user really doesn't know which
machines are on and what the machines
capability is so we have looked around
and found that a lot of the vendors have
very similar sets of information
described either with JSON or llamo
or annotations or properties and they're
all very common attributes things like
the service name specific version maybe
it's an API version or a limitation
version there's also resources like CPUs
memory may be shared disk space or
resource dependencies around that we
also have seen that there's a that
there's some support for grouping that's
their varied but all these are very
caught or very similar in in the
different environments they're just
specified differently so we see that
these are these are kind of a common
mature set of information that's worth
looking into for standards you look a
first example it's a marathon example
mais Dulce marathon at another example
from Cooper Nettie's both of these
examples provide the basic rudimentary
information as I mentioned to before the
the services typically get distributed
throughout the environment you don't
know where they're running so if you
look at the the left hand side your
typical cloud distribution you have your
services running anywhere within say a
region you don't know necessarily that
they're close to each other they could
be in entirely different data centers
within that region and if you happen to
have two services that use a lot of
communication a lot of network traffic
then you may want to influence that to
get lower latency in order to solve that
issue one of the things that we've seen
if you compare it on the right is
service grouping you look at some of the
examples there's a tight grouping where
you can actually co-locate your services
together on the same host in order to
reduce that network latency so we've
looked at some of the strategies that
are out there again today again this is
kind of common support that's already
available in different platforms there's
the notion of tight groups you look at
Cooper Nettie's they have pods and in
that case the services are tightly
co-located they share a host if you
scale them you scale the entire group
together and new instances of that group
will form a new group they don't get
added to the group the benefits as I
mentioned you can get low latency you
can share disk space if your service
requires that you can also reduce
network traffic overall in comparison
that we also see the ability to have
quite kind of
loose group where you can tag your
applications and kind of categorize them
together in that case you can imagine
that this could be used as an affinity
level to get requests low latency but
maybe not necessarily require the the
same host the collocation of those when
you scale them you scale them at the
service level so you get kind of the
benefits of my career services where you
can horizontally scale individual
components when you add more services to
them they add a new group they don't get
or they added their excuse me they're
included in the same group as I
mentioned before kind of categorization
the benefits of this is as I mentioned
you can get some low latency you could
imagine some health and performance
monitoring as a specific category and
log consolidation so here's some
examples that are available I guess
again kuber nutty's in Marathon both of
them provide kubin energies as the pot
as I mentioned before which is a tight
group marathon is also kind of a tight
group but we found that you can kind of
get away with scaling the services
individually so it's it's a little bit
it's a little bit scale skewing more
towards the loose group now that we
could provide some basic information and
tell the cloud where to run these
services then the next step is to make
sure that they can talk to each other
make sure that they can make take
advantage of the vendor specific
services here's a kind of an example of
what what the high level architecture
would look like when you get these
services into the into the cloud this is
something that again is common AWS and
Cooper Nettie's and marathon all support
this kind of architecture where you once
you deploy your services there's some
kind of single entry way a gateway that
that allows for multiple client types to
connect and aggregate responses through
your back-end and it provides a kind of
common services cross-cutting concerns
like logging analytics kind of response
caching type features and then somewhere
in the environment there is a service
registry and a load balancer that
understands that service registry
whether it's a full load balancer
whether it's a client doing the load
bouncing and then your services as I
kind of had it mentioned before they're
kind of a frame Merle there's they're
out in the cloud somewhere and there
they're largely found through the
service registry and they utilize the
service registry themselves both to
register themselves and discover other
services that they depend on so kind of
looking at that service registry problem
itself for those who are just kind of
learning about microservices when your
service gets into the cloud obviously it
might depend on things like a Redis cash
or another back-end a compute service
and it needs to be able to find that the
endpoints are dynamically assigned
they're constantly changing as services
come down and go up so the request to
those services need to be able to
dynamically find the backend services
their dependencies they also need to be
load balanced there are a couple
patterns that the various vendors have
used one is a Netflix pattern very
popular with spring cloud as well they
use netflix eureka and ribbon it's the
client-side service discovery and how
this works is that the client itself
embeds some library or package that is
able to talk to the service registry
there you use either Eureka zookeeper
console in order to find these services
the client then caches that and then the
client is able to either specifically
route the request itself through some
some of its own implementation of a load
balancer so to speak or it can utilize
some of the library functions on order
to automatically load balance in this
way the client is kind of tied to the
service registry and there's pros and
cons to this approach as well as the
other approach the other approach is
more of a server side to service
discovery you see this in amazon's AWS
elastic load balancer also with marathon
lb this is where the the router can find
out the information directly from the
service registry and typically the
service client which could be in this
case it could be the API gateway or
another one of your front-end services
that need to talk to a dependent service
that will get proxy to the
the router load balancer some way which
will then get automatically load
balanced in a common way to the backend
services this kind of decouples it but
it does require that the platform
supplies this the the application can't
write this themselves can't provide it
themselves this impacts the application
you can't write a portable application
as you see some of the examples of
actually doing a connection in Marathon
load balancer you might use a common
service port you might send all of your
requests to localhost 800 1 and that is
supposed to be a specific service and
that can strange the service ports you
have in other examples you'll see a
namespace and a naming solution and you
write your application in that way to
send it to that specific service in the
Netflix and spring framework you see
that they use a discovery client to get
the number of instances and in this case
the client performs the actual request
with one of those instances so none of
those models provide the application
developer a common way of sending their
requests that is portable we think that
this should have a standard the platform
should supply some kind of standard way
we think it should be related to a
service name maybe a version or
namespace to help qualify where the
request should go how that's implemented
whether it's the client side or the
server side model it doesn't matter as
long as there's a standard way that the
application developer can write their
application just a little bit more about
the information that we think can go
along with this again metadata that can
either be annotations within the app or
supplied when the application gets
provisioned things like the service name
are critical for this you can also
imagine a compatibility version Cooper
nutties uses an API version the API
version helps determine whether the
request should go to version wanna
version 2 and then you can also include
an implementation version for keeping
track of your services we also think
that it might be valuable to look into
something about dependencies the
explicitly declaring which services you
depend on and maybe what version it is
so that we can later introspective and
have a nice application graph
your parties could do this or it could
just be even be used for versioning
later on and some of the methods that
will get into later we also think that
there's a method that you can this is
kind of common to declaring whether the
service should be externally exposed to
the API gateway or whether it's simply a
back-end service that should be internal
it shouldn't be exposed all those is
Rajeev mentioned we think should go
through the standard community process
and get reviewed for the expert group
this is one I mentioned with the
dependency information we imagine that
it could be very useful in order to help
manage your application which services
are dependent on version one which
services are dependent on version 3 how
do I know whether it's safe to fully
deprecated or remove a certain version
from my catalogue how do I know if I can
can shut all of those instances down or
if I need to keep them running this can
all be kind of introspective and it
helps if the information is provided
along with the service so that either
the platform or maybe like a third party
tool could examine that information and
provide some of these some of this
information for the you for the
application developer the other thing
that I mentioned before is once you
deploy it then you want to take
advantage of some of the cloud services
that are available things like
configuration eventing all of that
should be we've talked about some of the
standard api's that a user might use to
get the config but how does the
application get automatically wired to
get the configuration service instance
those things we think that with specific
annotations or or metadata one is
provisioned could help make that
available to the application so that
it's portable you just right at the one
time and drop it into different
environments and you get the service
that's available there here's an example
of one one example that we think might
be a good way to have the application
declare the event service and that would
get automatically injected or Auto wired
runtime based on the fact that it
declares the event service on the right
hand side is the config service that
would get inspected when it's
provisioned and the implementation
version of the or
it would get automatically wire to the
application so that it's available at
runtime now that we have our
applications hopefully talking to each
other and making available the vendor
services we want to make sure that they
stay available there are a couple
proposals in Java EE eight that we hope
to build on and ants and make sure that
they're fully supportive of the micro
services architecture and that's around
health checks and performance checks so
typically I've mentioned the state list
of femoral services you guys are
probably for those of our aware of
microservices you're probably very
familiar with the pets versus cattle
metaphor so in this case all of these
will be the cattle we don't care if one
starts to behave badly or starts to get
sick it will be replaced in that case
it'll come back in could be a different
location how do we support that how does
the application tell the cloud platform
that it might be sick but it's not
working in that case health checks this
is pretty common as well supported by
all the platforms or AWS Cooper Nettie's
marathon they all have three primary
ways of help providing health checks the
application declares the application
declares whether it can receive an HTTP
request and it can respond with some
response code between 200 and 300 99 or
if the application requires that you
check on its health with a specific
script execution or processing vocation
within the container there's also a tcp
port check so we think that all those
should be pretty standard and we should
examine that for either an annotation
within the application or some kind of
metadata that goes along with the
application so the application can
declare those points to the platform
this is a little bit of detail stepping
back now into Java EE eight we wanted to
socialize some of this information to
kind of get feedback because it is a
kind of a new one in it and it's just
getting proposed now so we would like to
have some kind of an EE specific
proposal around maybe a REST API this is
similar to what
the EE 9 microservices information that
I just mentioned something where you you
can declare you're bootstrapping end
point where you can find all the
application specific health information
it could be new annotations or
descriptors to help specify those there
could be a health excuse me a helper
class to help with the health report
structure it could be a deep nested
structure that has scoping supplied
around which parts of the application
are unhealthy and the the
implementations could make use of those
annotations and descriptors in order to
map the at the end points to actually
URLs so as I mentioned there's a idea of
scope because with the e8 is it is more
of an application type environment app
server type environment or you'll have
not only applications you'll have you'll
have your app server itself might have
partitions and all of those have
different nested layers of health checks
that you might want to check on you
might want to manage within the
environment there's also specific
security requirements that could be
dictated by the implementation for
example some some health checks could
require a security authorization in
order to check that the state another
another part of the health checks would
be metrics and performance monitoring
and this will also apply to EE 9 and
more of a micro services model in this
case we want to define something similar
a REST API or way to declare an end
points to collect specific information
about the application it could be custom
metrics and similar annotations and
descriptors to help specify those custom
metric endpoints and similar helper
classes the the SPI should allow the
vendors to provide the specific
implementations for some of the
in-process agents so that we can so we
can harvest the metric collections into
a centralized service and make the
decisions about whether the service
might need
tuning or attention again the security
requirements would be dictated by the
implementation so getting back to micro
services based architecture the way that
Mike reverses provide high availability
is also through horizontal scaling and a
lot of the environments provide
horizontal scaling in either a
declarative manner or also automatic
they usually determine the automatic
scaling requirements based on CPU
consumption memory consumption requests
routing information in the load balancer
how often your service is getting hit
and they also supply some custom metrics
and this is interesting whether it would
be something that would build on the ee8
metrics collection or if it would be
something more like Amazon's AWS where
it's a service where the application
pushes its custom metrics to be consumed
later on they also make use of
predictive scheduling so you can imagine
on cyber monday you might want to
predict that you're going to have extra
requests to your specific catalog
services and scale them appropriately
beforehand this allows for if you if you
look at the way they've implemented this
it's a it's a dynamic service that that
declares policies about how these
specific metrics should be utilized in
order to automatically scale so if you
look at the policy based scaling in this
case they have policies that if for
example if all of your instances report
75% utilization or higher for a specific
amount of time then you might want to
scale up similarly you might want to
scale back if the utilization is under a
certain percentage for a certain number
of instances or for a certain amount of
time and there's also maximum and
minimum bounds on the number of
instances all this is is capable if the
application can provide the metrics or
excuse me if the application can provide
its health information to the cloud
platform
finally for versioning so versioning
should be planned on upfront even if you
have a great single purpose microservice
it's going to require some minor changes
if it's successfully you want to add
features so the two styles of are the
two main areas that you have to worry
about everybody's promised that micro
services are simple to version simple to
upgrade and that may be true if
everything's backward compatible but
what if things are not compatible what
if the API changes what if the data
that's exchange or the data that's
manipulated changes in a specific way so
you have maybe as you're rolling this
out you of five instances that are using
an old format of the data and you have
one instance that wants to start
changing the data to a new version so
all of that should be considered and
what it means for how that gets rolled
out in the environment in the backward
compatible example again this is fairly
simple and there's a couple of different
ways that platforms typically allow for
the raw there's either just a rolling
replacement or there's more of a
blue-green model where you can slowly
route the traffic and so and some of the
environment support I think marathon
supports Bluegreen out-of-the-box which
is which is very convenient for
application developers so going and
looking at in cap incompatible upgrades
again so in this case you're really
going to need to keep things side by
side if you look at like a Netflix
example they have services that still
support very old api's and the reason
for that is that you can imagine your TV
doesn't get upgraded very often it still
requires the old service in order to
connect and get information from netflix
so in that case they have to leave
things side by side in that way we do
need to differentiate between the
versions in some way whether its
compatibility version or you're just
going to name it a whole new name so the
one of the interesting things as you
look at the auto scaling is as more and
more services attempt to connect to the
new version of the micro service that
version naturally gets scaled out
whereas less and less instances hit the
old version that instance will scale
back so it's kind of a nice it's very
similar to blue green even though
it's not technically blue-green so if
you look at the actual request traffic
in this case it's a little bit busy but
it essentially shows that client one is
going to get routed to version one and
it's based off of the information that
it supplies when it makes the request of
maybe it provides the name and the
version and then the request load
balancer will have to know that
information with the service registry
same thing with client to for backwards
compatible as I mentioned there's
there's a couple of different ways of
doing this essentially as you roll it
out the new version will get placed in
and the old version will get phased out
it's just a matter of whether you're
going to do like a rolling replacement
and as you provide and provision a new
service you don't want to utilize the
resources so you take one of the old
ones away or you can do Bluegreen where
it's a little bit more gradual in this
case you see the backwards compatible
requests both neither the client neither
of the clients require that this the API
version be v1 maybe the Micra service
declares that it can support both
version 1 and version 2 and in this case
then the request can get routed to the
the exact same service regardless of the
client version and this is a kind of
example for those that aren't familiar
with kind of the blue green model as I
mentioned the you would add the new
micro service and slowly over time you'd
start to shift some of the traffic to
make sure that it's healthy and then
it's responding okay and as you get more
and more comfortable than you'd shift
more and more of the traffic to it until
finally the old microservices not
receiving any traffic and then it's safe
to remove it for compatible requests
again this is this is the the rolling
replacement as I mentioned you might not
want to consume extra resources in that
case then as you introduce the new
service you're one of your instances of
your old service goes away and until you
repeat this process until all the old
services are gone those are things that
the platform can provide given that it
has enough information to make those
decisions within the routing
so in kind of summary of all of these
specific areas that we think that the
application can be made portable there
should be specific service metadata that
kind of declares how the application
gets provisioned what resources it
requires within the environment and
especially the versioning information
that helps with the routing and
discovery there's there's also you can
imagine some handy information with the
dependencies that we should look at and
again service grouping although it may
not be the most common scenario it is
one that's supported and it does come up
from time to time service discovery as I
as I mentioned with the application
being able to write its request in a
portal my lanter we would like to
utilize some kind of consistent pattern
that all of the vendors support in some
way whether it be through a specific
client library or specific interception
of the request and and all the cloud
services should be easy to find it
should be either Auto injected or some
kind of standard configuration that
allows the application to easily find
those and make use of the standard api's
and the availability the application
should be able to provide its health
check information and possibly some
custom service performance metrics that
the that the policies can then make use
of for auto scaling at this point I'm
going to turn it over to Joe and he's
going to introduce the demo thanks Josh
ok so today we're going to show a simple
demo that illustrates some of the ideas
that Josh mentioned as all start out
orienting you with this diagram so you
see on the left hand side we have a
collection of artifacts that that make
up the micro services for an application
and what we've done we've taken the
spark lines application which is a
jersey sample it's a simple example
where you pass the application or
collection
of numbers and it produces a bar chart
or a little chart for you so we've taken
that application we decomposed it into
three microservices so as you see on the
left we have the web app which is the
the front end for the sparklines app we
have the compute layer which is the
resources the rest resources for the
application and then we've added a cash
using hazel cast as well also on the
left you see JSON file containing some
metadata on how to deploy this
application so again what we're doing
here just illustrating some of the
concepts that that josh described and
then we'll take these artifacts will
upload them to our cloud provider which
is Duke's cloud service and then we'll
deploy them on a marathon framework
running on an ASOS cluster that's
running back an Oracle and when we do
this you'll see an example of the
grouping idea of at Josh mentioned so as
you see on the right hand side we've
taken these three microservices we've
put them into one group we've said that
group has strong affinity so those three
microservices will be located on the
same machine and when we scale out that
that group it will scale the entire
group and as you also see there was only
one micro service in that group that is
registered with an external load
balancer right the other two micro
services are just for internal use only
all right so if that will go to the demo
okay so here you have Dukes cloud
service you'll see there's three tabs
across the top one for uploading the
service jars one for uploading the
service definitions that JSON file and
then the final one for doing the
deployment so we've already uploaded a
number of jars here you'll see the three
that are for sparklines in the list
there in this case these are fat jars so
as josh mansion there's other
possibilities here depending on the
direction we want to go these could
contain java SE nine modules but in this
demo they are fat jars in fact the two
of them are running glassfish embedded
and the third as I mentioned is hazel
count so we've also uploaded the service
service definition for spark lines so
you'll see that here the spark lines
metadata JSON file and I'll click on
that and we'll just take a few seconds
to go through this so if you've worked
with Cooper Nettie's pods this will look
similar to that again these are the
instructions given to the cloud provider
so that it knows how to deploy and place
these micro services so you'll see that
we have the application with spark lines
we have an API version we stated that
the affinity for this collection of
micro services is strong again this is
just you know an example of what is
possible we have a context route
specified that's used for marathons
health check to make sure that the
application group is lively and then we
have our list of services so the first
one is our web app and of course we
indicate the jar file that's to be
executed the Java command to run to
execute that jar you can provide
environment variables although we're not
making use of that in this demo and then
ports so in this case this is the web
app so we want this registered with the
load balancer so in this case we've
indicated our service port that our
application will be listening
and then we've indicated that we want a
type external which means we want this
registered with an external load
balancer so that we can get at it
finally we've we mentioned that this
micro service depends on the spark lines
resource microservice a second one and
this helps us with some of our internal
coordination when we use docker composed
to link these micro services and so we
have two more entries here one each for
the other two microservices the only
thing I'll call out here on sparkline
resource is that on its ports definition
again it's running in its own docker
container so it's also listening on a
DAT but in this case the type is
internal which means that this micro
service will not be registered with the
external load balancer okay so that just
gives you it's just an illustrative
example of what could be possible as you
provide some of this metadata to do the
deployment all right so now we have our
jars uploaded we have our metadata
uploaded and then we're going to deploy
it so as I mentioned this is going to
run on our marathon infrastructure back
in Oracle which has been a little
temperamental so I have my fingers
crossed here so I'll select sparklines
which is that sparklines definition that
I uploaded that references the three
jars and i'll click provision so this is
now registered the application with
marathon and you know a clicker refresh
here so now we see that the sparklines
application has been registered with
merit with marathon with those three
microservices and we've taken that
metadata that JSON file and we
translated into the marathon python file
needed to configure marathon we've also
converted some of that information to a
yamo file that's used by docker compose
and all that information is now staged
but the application isn't yet running so
to get it ready and I need to
Dale one instance so we'll start by
scaling one instance so like I said at
this point the application has started
to provision we see marathon sees that
the application has started but that
single instance isn't live via and
that's why we're waiting for that
raining 0 of 1 to switch to running 1 of
1 and there it is so now we have one
instance of that sparklines group
running and remember that consisted of
three microservices so the URL published
here is for that front end web app
that's registered with the load balancer
so this URL that you really can't see at
the top there is going to our load
balancer that is fronting our marathon
installation so I've clicked on that URL
and this is now running sparklines and
what we're showing here and it's going
to be a little small but we have six
charts that were generated using random
data and if you remember we have a
friend and that serving the JSP page we
have the compute layer that's computing
the images and we have the cash where we
can cash the images so if you could see
you would see that all of these are
running on the same host right because
we deployed that one group with strong
affinity and they're all running on the
same host and if you come up close you
can see that here on the diagram also
down at the bottom we have a hostname
that rendered the JSP page ok that's
great so we have one instance of spark
lines running now I can go I can scale
it to two so this is again scaling that
group to run two instances of that group
and you'll see now the status of
switched to renting one of two so now
we're waiting for the second group to be
fully up and live
I know the more I click doesn't mean it
goes faster right all right now we have
you can see running two of two which
means that second group has come up so
now in theory if I bring up a new
private window
I'm going to hit the load balancer again
and this is going so this is hitting
that same application from another user
then this should render these images
from the second instance that was
brought up and now you'll see if you
could see up here that will look at the
bottom of the JSP page and yes this is a
different host name than the previous
one and all these instances of spark
lines were generated from that same host
so again it's I know it's not super
clear from this but this is
demonstrating the affinity we said we're
if I hit the web front-end on one host
I'm that front end is using the micro
services also provisioned on that host
and if I hit the web friend in on the
second host again and getting that
affinity and we did that to demonstrate
this concept of an affinity group where
if these micro services require very hot
they're very chatty high bandwidth you
want them co-located in with that that's
pretty much it for the demo consumers
were you going to summarize Rajiv
go back to the beginning this is a good
summary
taken
okay so thanks Josh and Joe I think the
demo was great Joe so we just wanted to
summarize you know between the two talks
that we've done with yesterday and today
we want to make sure that Java EE is
being is being enhanced right we want to
make it the platform of choice for
developers even in the new environment
one of the things that again it gives
you a guarantee in terms of what we can
give is the portability of applications
you don't want developers to be confused
at all the choices that they have we
want to give them the ability to
portably write their applications and
run across multiple vendors you know if
the goal basically of Java EE has always
been to give you the standards around
developing applications for the
enterprise and we want to keep that goal
in mind as we as you take java ee
platform ahead of course we want to work
at existing solutions and vendors so you
know that we know we acknowledge that a
lot of existing solutions in today in
the cloud so we would like to encourage
those people to participate in in the
JCP where possible to help standardize
basically the commonly face problems
that developers are seeing in this new
in this new world so to that extent we
also have a survey going on like I said
these are all very initial proposals we
would like to get feedback from not only
their partners and customers that also
from the community members as to what
they think is we should that we should
be focusing on as we go through this
process of evolving java ee in the
jetway e9 time frame so i encourage
everyone to take the survey and of
course there are other the other
channels always available for
participating this in the spec leads
expert group meaningless are open you
can participate in the JCP and and
contribute that way to there are other
sessions in this conference around java
ee these are some of the ones that are
remaining some some sessions happened
yesterday some there are some more
remaining throughout the week so i
encourage people to go listen to the
talks and
provide feedback at the next level of
details as well and I think we have time
for a few questions so if people have
questions which we can try to take them
here and answer them I will also be
around after the talk if people want to
come up and talk
going once going twice okay thank you
very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>