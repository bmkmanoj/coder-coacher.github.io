<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Thinking in Parallel | Coder Coacher - Coaching Coders</title><meta content="Thinking in Parallel - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Thinking in Parallel</b></h2><h5 class="post__date">2016-09-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/iDplU7mOocU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello good afternoon welcome thanks for
coming my name is Stuart marks I work on
the core libraries team in the JDK group
at Oracle and this is with me on podium
brian gets also at oracle i'll be
covering a few topics in the first half
of this presentation then about half way
through I will turn it over to Brian so
you all came here to learn about
thinking in parallel our key message for
you today is that the best way to think
about parallelism is not to think about
parallelism instead what we want you to
do is think more about the fundamentals
of your problem and your program and how
that affects the the way you write the
program and then in turn almost as a
side effect effects parallelism so what
I'm going to do is take two trivial
examples and run through them and do a
lot of commentary and deep dive into
those examples and and really see what's
going on and how that affects
parallelism all right here's the first
example
and it's pretty trivial convert an array
of strings to uppercase I'm going to
show two ways to do that one using a
conventional iterative for loop and
another using streams alright so here's
the code to convert an array of strings
to uppercase using a for loop pretty
simple
allocate the result array have a for
loop that runs through each index
assigns the result into the right
position in the result array and then
returns the result pretty darn trivial
so how does this work what actually
happens and you can all envision this
we've all written code like this what is
what what happens is you start off with
your input array and then this converts
the first one and then it converts the
second one and then it converts the next
one and the next one and you can you can
see what's going on here it does them
one at a time in left-to-right order so
let's look at the code again and examine
what parts of it are essential
we have a computation of to uppercase
which is just a method call but that's
that's the real computation that's going
on in this that's where the actual work
is and then we have a for loop that says
it wants to say we're gonna do this for
every element and in the array but what
it really says is start at zero and then
proceed to the next and then proceed to
the next and proceed to the next until
you get to the end so it doesn't
actually it doesn't actually say quite
what we want it to do but that's the
effect we get and in fact this is so
idiomatic when you say oh I want to do
something for every element in the array
you just write this out I mean you know
there's there's IDE macros that do this
for you because this is something we do
all the time so as I said this but this
this brings us right into a mode where
our processing is sequential and it's
left to right and and this is entirely
I'm saying I'll call it accidental to
the the tasks that we were trying to
accomplish so when we're upper casing
every string in the array we can just
you know conceptually we can just say
that we want to uppercase every string
in the array we don't say okay here's
what I want you to do start on this
element and proceed from left to right
one at a time so it's very important
that each it's for a problem like this
it's very important to note that each
computation is entirely independent of
all the others right you have your input
array of your output array and the
uppercase version of the element over
here doesn't matter whether that happens
you know it's not related to it doesn't
depend on the upper case computation of
the one over here
it can happen before or after at the
same time that's the nature of the
problem the way we set it up yet we've
written the for loop that processes them
one at a time one after the other why is
that that's basically the only thing a
for loop can do is do things one at a
time in a particular order now things
get a little bit better with the
enhanced for loop so that says do this
for every element instead of stepping
through in
using AI but it still precedes one at a
time in a particular order left-to-right
so it's a little bit better but not much
better all right let's look at the
streams code a raised dot stream of
input so we're taking the entire input
array mapping it through the to
uppercase method and sending the output
to an array and returning that so notice
I didn't talk about individual elements
anymore talked about the entire array so
we have the input array in the output
array and they can all happen in any
order left-to-right right-to-left some
random order that we don't care about
all at once we don't care and notice the
code did not say anything that made us
care or told the computer to care about
what order was that's all the the actual
ordering the actual means by which this
computation is performed is handled
entirely and hide inside the streams
library and so perhaps a better way to
think about this is instead of talking
about the individual elements of the
array is to say we want to take the
entire array and upper case all of them
say all of them instead of this element
this element this element this element
this element so that's how we talk about
entire arrays or lists or aggregations
using the stream library okay so two
examples of a trivial problem which one
of those is better so you know you can
imagine going on some internet forum and
saying oh here look at this look at this
cool streams code isn't that great it's
it's new and cool it's more compact it's
more functional functional programming
everybody loves functional programming
that's why streams is better and then
you get the the contrarians
they say well yeah the stream stuff but
you know there's overhead you created a
bunch of objects it's not necessary you
could just write a for loop it's much
more efficient because jits know how to
optimize for loops it's much more
straightforward programmers and already
know what that means so this stream
stuff that's just the distraction that
entire argument is wrongheaded now each
of those individual statements yeah you
could argue about whether they're true
and there's some
this stuff but really that's that's the
entire the argument is entirely at the
wrong level so we claim that the streams
version is better not because of those
those detailed ideas but because we are
now writing our program at a higher
level of abstraction and so we're not
talking about individual computations
the streams program rights allows us to
write a computation in such a way that
we can see ah this computation for an
element of this array is independent of
this others we're saying do them all at
once we don't care how you do them
there's less accidental complexity we
don't have to worry about oh is it less
than length or less than or equal to
length incrementing variables with none
of that in there it's an aggregate
operation we are implicitly performing
an operation on every element of the
array instead of perhaps a sub range and
what this does is it lets us focus on
the results we want to get out of the
computation instead of the mechanics of
okay let's get an index increment
through it test it okay get the next
element stored into the sort into the
result so we're we're operating in a
much higher level of abstraction so
that's why we think the streams code is
better all right so why hasn't he said
anything about parallelism well so the
point of this talk is to say we're not
going to say ok here make your code
parallel contort it this way so it runs
in parallel and that's why it's better
now actually we're saying exactly the
opposite take your code rethink it make
it better and make it have these useful
properties like higher level of
abstraction compy independence of the
different computations talking about
abstraction aggregations aggregate
operations instead of individual
elements that will make your code better
and it'll also make it parallelizable
alright let's move on to the next
example so I'm calling it a less trivial
example here it's still pretty simple
the code the the problem statement and
the code solutions both fit on a slide
so anything anything that's small enough
is pretty trivial so the the problem
statement is we want to take a list and
split it at locations determined by the
application of a predicate and what we
want the result to be is the sub lists
around those split points so let's look
at an example so we have a B hash c hash
d e and the predicate is let's split
where each element is equal to the hash
symbol and so the result would be a sub
list of a be a sub list of C and then a
sub list consisting of de okay this is
pretty simple right so this is this came
to my attention because I Brian and I
have done some spent fair amount of time
on Stack Overflow over the past couple
years after Java 8 was launched and so
this came up and it was it was it was
pretty interesting because it's like oh
that's an interesting problem you know I
saw that there I follow the Java 8 tag
and I looked at it and I said oh okay I
think think it would do a pretty good
way to do this using streams but there
were a bunch of other answers and they
were really complex they're terrible
and not only that they were wrong so so
maybe this may be this may be this
problem is not as trivial as I thought
it was so I think what you should do is
go look at this question on Stack
Overflow find my answer and upload it
because I need to stay ahead of Brian
but but it's an interesting exercise
them I don't have time to go through all
of the other wrong answers I mean it's
interesting and this is a different kind
of failure analysis thing air of oh this
guy got it wrong this way this guy got
it wrong this way this guy missed an
edge case there's a bunch of that there
it's kind of it's it's quite a
digression from this but it's an
interesting object lesson because this
really is not at all a complex problem
but people just went off into the weeds
it's pretty amazing all right so let me
tell you how to think about this problem
so before I dive into the code I think
it's useful to to take a step back and
look at it all right so here's our input
array now oh I forgot to mention the the
problem statement talked about sub lists
and in Java the list dot sub list method
takes indexes as arguments and so you
can take an index take a sub list from
an index to an index and then you get a
view into the original list so that's
what we want to get here so the sub list
method takes indexes into the original
list so we're operating on we need to
think about the indexes into it so let's
number all of the elements okay so now
we want to split at the locations of the
hash signs so let's you know I put bars
there where the splits are and so we can
see that where those bars are are the
edges of the sub lists and there are
interior sub lists so the only sub list
that's interior is bounded on both sides
by the split points so that's that's the
sub list consisting only of C but
they're sub lists on the outside and
they're sort of open they don't have any
outer edges yet so what we need to do is
we need to add them so we'll ads will
synthesize split points at the outer
edges so that's before the beginning of
the array and after the end of the array
at least conceptually and so you just
extend the index numbering that way and
so now these bars are where the split
points are and the sub lists are between
each split point so notice we have one
of these fence posts things where we
have one fewer sub list than split
points and so what we do
is for every split point except last we
can compute the sub list from starting
from one split point to the next split
point and then there's a little bit of
+1 in there because the the split point
occurs before the beginning of the sub
list and then the end of the sub list
actually is the same index as the split
point because sub the sub list is just
an artifact of the API the sub list API
takes a half-open range that it takes a
range that is half open on the end so
you have to do a little bit of Plus Ones
here and there but it's not complicated
you look at this diagram you say ah I
can immediately derive all of the sub
lists I can based based on the split
points all right so now let's write the
sequential for loop code that does this
start off by writing a method signature
we have a split method that takes a list
of T input predicate of T and then
returns a list of lists of T I've left
off the wild cards because they just
clutter things up so what does that look
like in a for loop all right so we
create first create a result array
because we know we need one so we're
gonna add things to it we have a start
variable that's well how can you
describe it's kind of the start of the
sub list we're currently working on it's
the index point of the start of the sub
list we're working on now we step
through each element of the array and
apply the predicate to it and once we
found a predicate that matches ok so we
found a sub list so we have to take that
sub we we adjust the indexes what we I
don't know do we just the indexes we
have a start and a current point so
that's one of the sub lists that we're
interested in so we put that into the
result array and then we have to do
something to to reset our current state
so that we can prepare for the next sub
list we're looking for and so that's
what start start equals cur plus one
does and then oh that's interesting at
the end you have to add this extra sub
list just trust me you have to write so
so and this is telling because when I
wrote this originally
I forgot and I wrote some unit tests for
this and it's like why is this oh oh
that's right there's that half-open sub
list at the end I guess I just have to
add it to the end right so I believe
this code is correct I have a bunch of
unit tests that that that work you know
over it and it's you know it's fine but
let's talk about this code and what's
going on the real meat of the code is
for every element in the array we're
applying the predicate and then at
certain points we're creating sub lists
and adding it but then there's this
other stuff right so we're doing our
usual loop indexing stuff and then and
then we're saying here I have some some
pointing out some some other things
going on in the code well first we had
to sorry just restart at the top again
first we we had to create our repository
of results and it's like okay well
that's kind of the thing we have to do
because we know we're gonna add to it we
do some loop it you are usual loop
indexing stuff and then when a predicate
matches we add the sub list and then we
have to do this this this the sub list
resetting thing and it happens to be
that you set the start variable to cur
plus 1 why is it plus 1 well if you look
at the diagram you can say oh okay
because you know plus 1 comes from from
this that's why it has to be there and
then of course as I mentioned you have
to finish off by adding that last one so
there's there's a lot of weird things
going on in this version of the code
that you might have forgotten you might
like I said I forgot the I forgot to add
that that tail sub list at the end you
might forget a plus 1 even though you
might be looking at a diagram you might
say okay I'm just going to write a sub
list here it's like how come it's not
working oh that's right plus 1 over here
ok all this stuff right so so there's a
bunch of a bunch of accidental
complexity going on here so let's let's
talk about this so I kind of alluded to
this before you know the plus ones and
stuff does this handle all the corner
cases how do we know well you could you
could write down some loopin variants
and then derive them and make sure you
know step through each loop and make
sure your
variants are still preserved and then
you know and that's that's actually
pretty hard it's weird that that last
sub list is treated differently from all
the others
why is that the case you just kind of
have to do it that way
now here's an interesting one I showed
you those diagrams that laid out I think
fairly clearly how to solve this problem
then I wrote the code now look at the
code can you if you didn't have that
diagram could you figure out what that
code was doing you know I think the way
I would have to do it is peace through
the code and actually redraw the diagram
based on what that was so I submit that
means this code is hard to understand
now here's the real killer is that we've
introduced a data dependency between
each iteration of the loop so we set up
some state where we have the start
variable we're adding things to it each
time around the loop we're modifying
that state so every iteration of the
loop depends on the previous iteration
and that means you have to do them one
at a time left to right why is that we
said at the outset that determining the
split points is independent of all the
others so the fact is we wrote a for
loop that created this data dependency
between the loop iterations that is
entirely irrelevant to the actual
problem so what we should do is rethink
the problem so if we take a streams
approach think about things at a higher
level operating on all the elements at
once instead of one at a time we're
interested in loop in sorry not loop
indexes we're interested in sub list
indexes because that's what sub list
uses so instead of operating over the
array elements any of you who are used
to streams are probably pretty pretty
pretty familiar with operating directly
on lists or array elements you can say
list dot stream you get a stream of the
elements here we're going to do
something slightly different we are
going to operate on a stream
of indexes into an array so we use in
stream dot range to generate that list
of index or that the stream of indexes
and like I mentioned earlier the
computation of each split point and
therefore the sub list edges is
independent of the computation of every
other split point and so we we can write
that in a very straightforward way
looking at every individual location
instead of trying to have instead of
trying to write a loop that depends on
the state we left behind from the
previous loop all right so let's look at
how to do that again okay so here's this
is the same diagram so again so you have
the indexes find the split points add
the exterior split points and then
compute the the sub lists from the
locations of those split points and so
let's write some code that basically
does exactly that this three steps
that's exactly what I what I said a
moment ago so let's write that code so
let's find the split points we take the
stream of indexes and filter it by
calling the predicate on every element
and send the results to an array so this
temporay is our list of interior split
points that's it there's no loops all
right we got that part all right they
say you should never apologize so I'm
sorry for apologizing this is terrible
god it's really lousy in Java that to
add things to an array you basically
have to copy it so it's it's kind of
gross code but it's pretty simple take
the take the array of Interiors from the
points copy it into an array to larger
at the right place and put values at
each end
now there's an there's a there's a nicer
way to do this which is hard to explain
and it's a bit of a trick but
conceptually it does exactly the same
thing so we took the interior split
points we took the array of interior
split points and added the exterior ones
that's all this does
now that we have all of the split points
we're going to stream over them again
and then map each one to the right sub
list and you see a few pluses and minus
plus ones in these places all this does
is say at each split point okay I'll do
it from your point of view at each split
point the stub list starts here you go
to the next split point and that's the
end of the sub list and so given two
adjacent split points we have derived
our sub list location and so we've
ranged over those indexes map table
takes that the index and converts it
into an object which is the result of
calling sub list then we just collect
that to a list and we're done all right
now here's the trick so instead of doing
the array cop nasty array copying and
stuff basically there's a trick to
expand the index bounds and then modify
the predicate so that the the ones at
the end automatically fall through so
basically we can compute all of the
split points using the first stream once
we have all of the split points we can
compute the sub lists from those split
points two steps and you can prove by
induction that this works for all the
edge cases which of these is better
I think the streams one is better again
not because it's new cool shorter
functional programming because we're
talking at a higher level of abstraction
notice how my language changed we took
all the array elements applied the
predix to them now we're talking about
split points
now we're derive sub lists from them
we're not saying loop through each
element one at a time left to right
modify some state remember where we left
off last that none of that we're not
talking about one at a time left right
anymore
take the array and we're doing aggregate
operations on it and so what this does
is it preserves the independence of all
those operations right this key
characteristic is that at any point in
the array the computation of whether
this is or is not a split
is independent of all the others and
we're preserving that because we're not
we're not relying on previous state that
a previous computation left behind and
also what's nice about this is once we
set it up this way every one of those
cases is treated uniformly we don't have
this weird special case of oh don't
forget that sub list hanging off at the
end and we don't have to worry about
loop mechanics or anything like that or
so one of the concepts here and it's
it's an interesting technique which is
to stream over indexes instead of
streaming over elements and so there's a
certain class of problems that can be
solved this way and so if you if you're
working with streams and you're
streaming over elements and you find
yourself fighting the streams API
somehow you know try try streaming over
the indexes instead of the elements so
that's a useful technique that finds
applicability in many places not
everywhere
certainly not Universal but nothing is
Universal anyway so it's a it's a useful
technique in the bag of tricks but the
main thing is this whole notion of
aggregate operations instead of
individual operations independence of of
computations from each other and
preserving that independence instead of
introducing accidental data dependencies
oh and by the way now that we've
rethought our problem and recast it in
these terms it runs perfectly in
parallel now should you run it in
parallel that's for my colleague Brian
to discuss all right so Stewart talked
about how to set the problem up so that
it is amenable to running in parallel
and even if you're never gonna run in
parallel Stewart made a number of good
arguments about why setting the problem
up in this way just makes your code
better but it doesn't address the
question should I run in parallel at all
and you know one of the things about the
streams library is it makes it really
easy to turn any calculation parallel
that doesn't mean that your calculation
will run faster if you go in parallel so
what I want to talk about is how to
analyze the problem to get a sense of
whether there's a potential
for parallel speed-up what I generally
advise people people ask like well
should I just use parallel streams all
over the place
generally what we advise people is start
out with sequential streams very often
it's perfectly fast enough if not then
you have to do some analysis and
measurement to determine whether there
is a pinch or parallel speed-up and in
that case then you can go parallel but
generally start start sequential but
let's talk about what parallelism is
parallelism is using more resources to
get to the answer faster you've got you
know four cores eight cores on your
machine and you you decide to break the
problem up and solve it in chunks on
different cores you're always going to
be doing more work in a parallel
solution you're using more CPUs you're
probably using more memory you're doing
work to break the problem up and and and
combine the results and so it's really a
trade-off of burning more cycles burning
more memory in the hopes of getting to
the answer faster in terms of wall clock
time so parallelism is strictly an
optimization you can still solve the
problem sequentially if you want if
there aren't you know if there's only
one processor that's that's your only
option and you know so whether we should
go there or not is you know strictly a
question of is it actually an
optimization so parallelism is only
useful if it gets you the answer faster
sometimes it doesn't sometimes the
parallel computation is slower so that's
really terrible trade-off right you're
using more resources using more energy
using more memory and slower so we want
to avoid that so like I said we have a
number of tools for determining whether
it is going to be effective analysis
measurement I'm just going to be talking
about analysis there's a whole separate
story that measurement but it's actually
the case that once you have a reasonable
model for how this works
you can look at a stream pipeline and
get a very good intuition of whether you
have a chance of getting a parallel
speed-up or not so we use this word
speed-up is a measurement of parallel
effectiveness
basically how much faster or slower does
the parallel version run than the
sequential version that's called
speed-up okay so the parallel
computation like I said is always ends
up doing more work than the sequential
version because how could it be
otherwise it still has to solve the
problem if there were somehow a more
efficient way to solve the problem you
could also do that sequentially and then
it has to do some extra work it has to
do all of the work of managing the
parallel solution divide the problem
into subproblems
schedule the subproblems on on different
CPUs wait for them to be finished
combine the results so that's more work
than the sequential version so the way
to think about it is the parallel
version always starts out behind the
sequential version and you're hoping to
make it up in volume right so that by
you know breaking it up and having
multiple workers work on the problem at
some scale hopefully you know that that
wins but just like in any you know
ordinary situation if you have a small
number of tasks to do it may be more
work to divide it up among luckily
people and to do it yourself but if you
have a large number of tasks to do it's
more likely that you'll have a chance of
getting done faster if you split the
pile in two and give it to your office
mate so there are a couple of criteria
we need to have in place to know that
we're gonna succeed in getting a
parallel speed-up first of all the
problem itself has to be paralyzed Abul
and you know Stewart talked about some
of the characteristics of what makes a
problem parallelizable you need to have
an implementation that brings out that
parallelism you need to have a
reasonable parallel framework and the
good news there is we have the fork
drawn library you know in in Java and
the streams lie built on top of that and
you need to have enough data if you
don't have enough data you're gonna
spend all your time coordinating tasks
and not enough time actually doing the
work in parallel that gives you the
speed-up that you're looking for all
right so these are some of the criteria
you need to have so if we take a simple
problem this is a problem that paralyzes
really nicely adding up numbers one
thread right think about the data flow
dependency graph that you get
both intrinsically from you know how you
would like to solve the problem and also
that's implied by the code that you
write so if you write you know the for
loop version you you know where you know
you work left to right through the array
like Stewart was talking about you end
up with a dataflow dependency graph that
looks like this right you're not gonna
do the second addition until you've done
the first one you're not going to the
third addition until you do the second
one and the problem itself is
intrinsically amenable to parallelism
but we wrote it in a way that it was not
amenable to parallelism and we ended up
with this lousy dataflow dependency
graph and that means we're not gonna get
any parallelism out of this computation
what we'd like is to get a dataflow
dependency graph that looks like this so
that you know one guy can be adding the
first two elements another guy can be
adding the second two elements and then
when when both of those results are
available they can be added together etc
so one of the ways to spot a lousy
parallel algorithm is when you store it
with an accumulator and you initialize
it to zero that's like big red warning
sign that you're doing something that is
effectively sequential and you're gonna
have to do a lot of work to undo that so
as you know Stewart said the best way to
think about parallelism is not to think
about it there there's a number of old
habits that we have to unlearn and we
have to stop thinking about things at
the level of individual elements and
accumulating them and think about things
at a higher level so under the hood this
is what you know that this is how the
streams library if you run a stream in
parallel is attacking a parallel problem
it's divide and conquer and this is the
standard tool that we have for for
executing algorithms in parallel you
partition the input into sections oh and
you want those sections to be something
where you can operate independently on
each section doesn't matter which one
happens first etc and you recursively
decompose the problem until the the
sections are small enough that it makes
sense to operate sequentially so if you
have you know a million elements you
know to to add up you're going to divide
it
if you have half a million elements you
divide in half have you know again half
a million elements you keep doing a
quarter millennium you keep doing this
until the sections are so small that it
doesn't make sense to keep dividing it
makes sense to say oh I've got 50
numbers here now I'm gonna loop through
them add them up right because when you
have small amounts of data the
sequential solution is all as often
faster so you're gonna recursively
subdivide the problem until the the sub
parts are small enough to solve
sequentially you're gonna solve them
sequentially hopefully you know an in
parallel and then as you proceed up the
computation tree
you're gonna combine the elements so
you're gonna combine the partial
solutions until you get to the top of
tree and you have a complete solution
right so all parallel algorithms
effectively looks like look like this
you say is the problem already small if
so I'm done otherwise divide it into two
halves solve them hopefully concurrently
and then combine the solutions and it
depends what your problem is if your
problem is adding up a bunch of numbers
so you're gonna have two partial sums
combining the results is adding the two
partial sums together to get a sum of
the the two segments and if you have a
computation tree that's been divided
seven times then you're gonna do that
you know you're gonna do that combining
going up the up the tree you know seven
low levels until you get the sum over
the whole the whole amount so this is
our standard trick the nice thing here
is there's almost no coordination
there's no synchronization here because
since we're partitioning the problem
it's effectively you work on that half
I'll work on this half and we don't bump
into each other and so we don't need to
synchronize we're each operating on
different data okay so what are the nice
things about this approach well it's
simple and it matches pretty well to
certain kinds of data structures that
are already defined by our recursion
like trees we don't have any shared
mutable state great no race conditions
no locking we can just partition the
data and each each worker works on the
part they like and the intermediate
results generally tend to live on the
stack so
you know there's not there's not
necessarily a lot of allocation going on
in order to make this efficient you want
to get to the point where you're working
in parallel as quickly as possible if
you have if you have let's say 32 cores
for every cycle before you start forking
off work and using more than one core
you can think of that as throwing away
31 cycles of work because you have 31
processors that are are sitting idle
waiting for work to do so in an
efficient parallel algorithm you want to
start dividing the problem quickly so
that multiple cores can start working
and you know when we recursively divide
the problem each core you know you have
one core it has the whole thing it
slices off half some other core picks
that up now you have two cores working
they each slice it now you have four
cores working eventually everybody gets
something to work on pretty quickly one
of the cool things here is that the
decomposition is dynamic no there was no
nothing in the code that said how many
processors I have and you know the
runtime can't incorporate you know
knowledge of how many you know core
counts we have and what the what the
length of the you know the run queues
are and things like that to determine
whether it makes more sense to split or
two to work sequentially but the the way
I express the problem in that recursive
decomposition is portable across core
counts that works just as well with one
core as it does with a hundred so that's
kind of nice that's an accidental detail
that you don't have to think about all
right so going to are summing an array
how do we want this to work we have an
array that we want to sum in parallel so
we want to divide the array in half and
we're actually not going to copy the
elements we're just gonna say you work
on elements one through a thousand you
work on elements a thousand and one
through 2,000 and we're gonna continue
to subdivide it until we get to the
chunks that are small enough that they
make sense to operate sequentially and
what we want to do is compute these
partial sums and work our way back up
the tree and it doesn't really matter in
order we do them right and you know we
we add up partial sums into larger
partial sums into larger partial sums
and we get the answer that we that we
expect okay so I told you I was going to
talk about performance so let's talk
about performance under what conditions
is doing that in parallel going to be
faster than doing it sequentially well
there's a couple of considerations the
first one is how expensive is it to
split the source if it's expensive to
split the source that's introducing a
lot of additional work similarly how
expensive is it to manage tasks handoff
chunks of works to other other you know
processors you don't have a lot of
control over that but we do have a
pretty efficient library for doing that
on the other end of the computation how
expensive is it to combine those results
in the example of summing an array
combining two partial results was just
adding two numbers so that was really
cheap but sometimes combining the
partial results might involve merging a
sash or concatenating strings and that
can be expensive and then the the
elephant in the room is locality so the
reality is most computers today are
memory bandwidth bound that when what
when you when you dereference a pointer
you're likely to cache miss and you're
likely to have to wait a fairly long
time for the memory subsystem to pull
yeah to give you the data that you want
to work on if your memory bound you may
have lots of threads all just sitting
around there waiting for cache misses to
be serviced and there's no computation
going on in parallel there's a lot of
waiting going on in parallel but there
isn't necessarily computation going on
in parallel and so you know in order to
get a speed-up you you generally want
for the individual tasks to be able to
take advantage of memory locality and
you know this is one of those things
that it's it's it's harder to see that
in the code than it is to see combining
costs and decomposition costs but it's
probably the most significant factor and
if any of these are unattractive this is
going to steal away from your
speed up okay so streams are very cool
now they allow you to express a
computation that might be parallel might
not be the sequential and the parallel
expression of that computation are very
similar that's nice and they're pretty
efficient they generally if you have
like a filter MapReduce pipeline it's
going to get fused into a single pass on
the data whether you're running in
sequential or in parallel single
sequential passed in parallel fast but
they're not magic parallelism dust just
because you get C Z to go parallel
doesn't necessarily mean that you'll get
a benefit out of it so we still have to
answer all you know go through that
checklist and say how easily splittable
is the source how expensive is the
result combination
what kind of locality does the
computation get the short answer with
locality is array based sources good you
know sources within directions like hash
sets not so good so the the the best the
best parallel speed-up that we're gonna
get is out of computations where our
source is an array it's easily
splittable and you get good locality
working through it
if you have very pointer rich data
structures like graphs and unless the
operations that you're doing on each
element are very expensive you're likely
to spend most your time waiting for data
from the cache subsystem whether you're
running on one core or or 20 so I talked
about making it up in volume that I mean
obviously if you have one element in
your array no amount of parallelism it
can make it go faster right and you can
make the same argument for small numbers
you know two three ten you know
generally you need a lot of data in
order to make up for the cost of
splitting the task scheduling subtasks
waiting for subtasks to finish and
combining them reasonably simple model
is you know for estimating this we call
the enqueue model and it's the number of
data items you've got and your input q
is the amount of work per item doesn't
really matter how you measure q this is
this is more of a qualitative thing but
in general I mean if you think of you
know reasonable measure for Q is
how many arithmetic operations am i
doing you want the product of N and Q to
be large tens of thousands the trouble
is if you go searching for examples of
you know parallel streams the examples
all are like do trivial small amounts of
computation and then people wonder why
aren't I getting a speed-up right if
you're doing expensive tasks if you're
doing you know fact you know factoring
large numbers you're doing computing md5
hashes and things like that you can get
reasonable parallel speed-up you know
even if you don't have good split
ability and all that but if you're just
doing silly things like adding up
numbers everything else kind of has to
go perfectly and very often when we
start experimenting
you know we write a program that does
something trivial we run it in parallel
we you know we benchmark it and we say
gee this parallel stream stuff isn't
very good so the more work you're doing
per element the more likely you are to
get the benefit of parallelism okay so
let's go through this those those items
source splitting this is this is a you
know the reality is some sources split
better than others
arrays split really nicely linked lists
let really terribly other data
structures somewhere in the middle so
there's the cost of computing the split
ah
there's how evenly do things split array
is very easily split in half linked
lists the only way to split them is
first and rest then you split them again
you get first to the rest rest to the
rest and and keep going right so you get
terrible evenness and also it's nice if
you know in advance what the split is
gonna be because that allows you to
optimize way some copies so again arrays
they split perfectly they split cheaply
you just you know calculate the midpoint
and you're done and they split
predictably linked lists have none of
these properties you know things like
hash maps and tree tree maps are kind of
in the middle they split okay for some
amount of time but they don't have as
much predictability and eventually like
hash mapped
you know once you get down to the level
of one bucket generally you have a
linked list and it behaves like a linked
list from there if you're using
generators like in streamed range
a good analogy is an iterative generator
like iterate where you have an initial
value and a function that you keep
applying to that value that splits like
a linked list if you have something like
in stream range like Stewart we're
showing that splits like an array so if
you're using you know whether you're
using a data structure or using a
generator some of them split better than
others and then that turns into a big
consideration in a parallel efficiency
similarly locality like I said that's
the kind of the elephant in the room
parallelism were wins only when we can
keep the processors busy doing computing
not doing waiting and in order to keep
them busy doing computing we have to
keep them fed with data and the way to
keep them fed with data is to be opera
you know it is to not be spending all
their time waiting on a cache miss and
so if you have an array and you slice it
up into segments where everybody's
working on some segment in the array
arrays exhibit great locality because
you know you reference the first element
of the array it's already pulled the
first end of them into the cache line
and you know by the time you've looked
at the second and third the prefetch has
kicked in and it's come you know already
pulling the next one elements it you
know into cache so you spend more time
doing computation and less time waiting
for data so array based you know in
America you know problems tend to
paralyze really well and you know
quick-quick benchmark that I did don't
put a lot of stock in it but it kind of
illustrates the point the speed-up that
we get with summing up an array events
versus summing up an array of boxed
integers I you the this is all speed-up
relative to the you know the thousand
element case speeds up almost perfectly
with Ince
with boxed integers the the the add of
the gate performance is terrible because
you're spending all your time cash
missing and you still don't you know
ever get that you know the the the good
speed ups that you're looking for
because you're spending all your time
waiting for data so to illustrate this
this is the memory layout you get with
an array of integers where the array of
integers
is an object each element of the array
is a reference to another object every
time you traverse one of those pointer
those arrows there's the potential for a
cache miss there where as an array
events they're all laid out sequentially
in memory bam-bam-bam-bam you walk down
the list and you're not you're much more
likely for the data to be in l1 cache
and and and be able to I act on it
immediately okay
a few other considerations with with
streams some operations on stream
pipelines are sensitive to what we call
encounter order encounter order is the
order implied by the source you have a
list first element second element third
element that's the encounter order some
sources don't have an encounter order
like a hash set doesn't have an
encounter order but you know arrays do
lists do some operations there semantics
are tied to the encounter order like
limit if I say limit of 10 that doesn't
mean any 10 it means the first 10 the
first and according to the account
encounter order
similarly fine first means fine the
first according to the encounter order
these operations have less exploitable
parallelism because even if you've
divided the the input up and everybody's
working on different chunks the guys who
are working on the the far end they may
get to you know they may get to some
intermediate result but they can't
proceed until the guys at the beginning
of the account you know beginning of the
range under the encounter order you know
have have been processed and so as a
result you tend to get much more speed
ups with operations that are
intrinsically tied to encounter order
the thing to remember here is sometimes
the encounter order is there but it's
not relevant to the solution to your
problem in which case you can say take
the stream and turn it into an unordered
stream by calling this unordered method
and all that does is return an
equivalent stream that has no encounter
order and very often you know this
doesn't affect whether you get the
answer you want or not and the encounter
sensitive operations are all all have
optimized versions for when they're
operating on an unordered stream so for
example
on an order stream limit of a hundred
means the first hundred elements on an
unordered stream that means any hundred
elements because there is no first
hundred elements and sometimes any
hundred elements will do and if that's
the case by removing the constraint of
working working within the encountered
order you can get better parallelism so
this is one of those things where you
can look at a stream pipeline and
immediately see that this is a risk to
parallelism that you've got an order
sensitive operation you can ask yourself
is this important to my answer or not
similarly if I replace fine first with
find any will I be happy with the answer
sometimes you are in which case you've
gotten rid of the the encounter orders
dependency alright let's talk about
merging when I decompose the problem I'm
going to be merging partial results how
expensive is that merge step I didn't
have to do that in a squint reversion I
only have to do that in the parallel
version if I'm adding up numbers the
merge step has just add two numbers
that's cheap if if I'm doing grouping by
where the result is a hashmap this is
really expensive merging to hashmaps
involves iterating through all the keys
of one of them and sticking them in the
other one and then you go up the tree
one level and you do it again and you go
up the tree one level and you do it
again this can be exam insanely
expensive
similarly if I'm concatenated a string
you know I may be able to do all the
compute the parts of the string in
parallel and then as I move up the tree
i'm concatenating longer and longer and
longer segments and when I get to that
last segment where emerging two big
segments into one segment I'm doing that
sequentially because there's only two
pieces to merge and this is often a
limiting you know a limiting factor in
the parallel efficiency so again this is
something that you can look at the the
stream pipeline and say is my merged
operation sheep or is it expensive and
that's gonna give you very quick and
intuition of whether parallelism is
gonna help you or not so you know in a
pipeline like this where you're just
collecting numbers to a set going
parallel you know will have a
significant slowdown it's not not just
isn't faster it's slower it's much
slower
accuracy a factor of 10 slower so you
know but fortunately this is something
that you can look at the code and and
get a pretty good idea of whether this
is gonna be effective and kind of to
illustrate what's going on right if you
have a bunch of sets that you want to
merge right so you merge those into a
set and then you take these two small
sets and you you know either create a
new set or you merge the the right one
into the left set and you know the the
same thing going up the tree eventually
you're merging two big sets into one big
set and you're doing it sequentially and
that's often a limiting factor okay so
the summary is any of these following
factors can conspire to steal away from
your speed up I don't have enough data
I'm not doing enough work per element I
have too many in directions in my data
source and so I'm spending all my time
cash missing it's too expensive to split
my source it's too expensive to combine
the results or my pipeline is somehow
sensitive to encounter order if you can
avoid all of these pitfalls and there
are plenty of stream pipelines that can
avoid all these pitfalls then you may
get a pretty decent speed up you know
from parallelism if you're on the wrong
side of one or more of these you're
probably not going to get much of a
speed-up in parallelism the more work
you're doing per element the more you
can tolerate not being perfect on some
of the other some of the other items so
the quick summary here is streams are
cool we love streams parallelism is cool
parallel streams are cool but let's not
get carried away
parallelism is an optimization and
parallel streams are not always faster
but we can build a pretty reasonable
intuition looking at a problem to say
you know am I going to get a parallels
and benefit out of this now I'll make
the usual the warnings I make when
talking about optimization optimization
is fun but it's not always an activity
that creates business value sometimes
we're just entertaining ourselves and
our employers may not appreciate us
spending our time at work entertaining
ourselves
so before optimizing you probably should
have certain things in place you should
have performance requirements if you
don't have performance requirements your
code is by definition fast enough
stop optimizing if you have performance
requirements before you start optimizing
you should have performance measurements
otherwise you're just sort of like
randomly tinkering with stuff and if
your measurements say you meet your
requirements
stop optimizing if you have requirements
and you have good measurements by all
means go ahead and optimize until you
meet the requirements and this might be
one of the ways to get there all right I
think that covers covers us we're at
about 50 minutes side we have a little
room for questions all right who's got
questions some people trying to sneak
out quietly we see you questions someone
has a question so they want to get beer
before yeah I know
ah there's a hand back right hand up
over there
yeah so the quick the question is this
is really just about compute bound tasks
right that's the question and and and
the answer is yes that's pretty much it
this this library was really designed
for data parallelism over compute
intensive tasks there is a complementary
you know form of parallelism where you
might want to be running your iOS in
parallel you may be able to get some of
that out of this framework but you
probably won't because what the the
heuristics for tuning whether to split
or not basically make assumptions that I
don't want to split down to the level of
one of one element I want to split down
to the level of ten fifty a hundred
elements in which case you know if
you're like fetching a hundred URLs and
you want to do so in parallel but we're
only splitting down two chunks of 100
elements you're still not going to get
parallelism out of that so the the the
tuning assumptions that are built into
this framework are really geared towards
compute intensive data parallelism you
had a question over here
yeah so the question is all right that
measurement things sounded cool can you
tell us some good places to go to learn
about that so performance measurement on
you know it in the face of dynamic
compilation and you know things
happening on different threads pretty
hard so there's a lot of sources that
can you know mess up you know mess up
your numbers and you end up with numbers
that maybe look good but don't actually
say anything the best micro benchmarking
framework that we know of is the jmh
micro benchmarking harness it does a
really nice job at eliminating a number
of sources of of noise in benchmarks and
there's relatively lengthy you know
tutorial on how to use it lots of sample
programs that give you an idea of some
of the pitfalls you run into when
benchmarking so that's a really good
place to start
so I don't think I heard the question no
okay so what about application servers
this is a good question so application
servers are in control of the you know
they're on their own threads right they
don't let you go ahead and create
threads and similarly there's a lot of
things going on in application servers
that are tied to thread contacts like
security context transaction context etc
so in in general application servers are
likely to disable parallelism here
because the kinds of computations that
you might be running are not necessarily
just the straight computer intensive
computations but might rely on container
services and the cost of propagating
those execution contexts to the you know
to the execution threads would overwhelm
the benefit you would get out of
parallelism so you know in like a Java
EE application container where you have
a lot of calculations that are tied to
thread specific contexts it's not a
particularly good fit so so in your in
your slide of all the things that can
rob you of parallel speed-up maintaining
the application service context is yet
another thing yeah and there's a there's
a definite measurable and significant
cost to doing that so I don't know if
the I don't know if anybody's doing any
work on that though I mean application
containers doing stuff in parallel I
mean it's sort of conceptually you can
think of a container could you know
manage the thread pools or something
like that but that's just I don't think
anybody's working on that that I know of
and and the application containers tend
to be more interested in the problem
that the gentleman the back asked about
which is running i/o and parallel
writing computation in parallel
so the question is basically how is the
parallelism of the stream library
implemented yeah so you know I had a
sketch of the divide-and-conquer where
there was this spot in the middle where
it says do these things concurrently and
it didn't say at all how that actually
happens the way that actually happens is
we use the fork/join library so the way
that actually happens is we use the
fork/join library going back a long way
there we go divide and conquer
yeah so in Java 7 we added a library for
parallel decomposition it is fairly hard
to use on its own which is one of the
reasons why we built the streams library
on top of it and so behind the scenes
what we're doing is we're dividing the
problem into and then we're using
fork/join to say you know fork this task
off wait for the result and and combine
them so there is a built-in common
thread pool that that the the library
creates that's by default size to be the
number of cores you have and parallel
computations are executed in that thread
pool question over here it so the
question is if I'm running in an
application server am i dead in the
water and tap and the answer is and I
don't mean to dodge your question the
answer is it depends what your
application server has done to you right
so some application servers may do
nothing in which case you're fine some
application servers may decide I'm going
to I'm gonna lock down the common pool
so it has only one thread I'm you know
that's what your container does to you
you know and I don't have a lot of day
I mean I've talked to a lot of container
maintainer z-- and they've asked me what
what are my options and I've laid out
the options and then they went and did
something and they never came back and
said what they did so your mileage may
vary I think we have time for one more
question yeah how about you sir yeah so
the question is we know when you're
doing concurrent calculations very often
the cost of a context switch overwhelms
the cost of just doing everything
yourself and so is this a problem with
parallelism and the answer is yes when I
talk about you know one of the costs is
dispatching tasks that means I'm gonna
put something on a queue and some other
thread is gonna read from that queue and
it takes a certain amount of time for
that handoff to happen if I have enough
data if if there's enough work to do
then I can overcome the fact that it
took me however many milliseconds to
hand that task off but if I don't have
that much data to work with or the or
the work that I'm doing on each data
element is not that big it might very
well be easier to do that sequentially
and so that's why the divide and conquer
you know say it has its heuristic you
know am I better off doing this
sequentially for the problem is small
enough I am better off doing it
sequentially because I don't have to
coordinate between multiple entities and
unfortunately you know there's no magic
determination of how small should n be
how you know or even had a measure Q at
all and so you have to apply some you
know some of your own analysis but if
you have large amounts of data or you're
doing significant work on each data
element you will be able to overcome
that it's just a question of at what
point and how do I make that decision I
think I think that's it we're good thank
you very much thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>