<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Ask TOM Office Hours: Sharding April 23 2018 | Coder Coacher - Coaching Coders</title><meta content="Ask TOM Office Hours: Sharding April 23 2018 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Ask TOM Office Hours: Sharding April 23 2018</b></h2><h5 class="post__date">2018-05-03</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/KGdcPGgv_r0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right good morning everyone my name
is Mukesh Petula this is the ask home
office our session on the shampoo which
is a brand new Oracle database feature
that has been released as part of 12 T
last year right so what we will do is
today we gonna do a short overview of
the particular feature and then we'll
spend a lot of time at the end for
questions and answers if you have any we
can discuss that as well my once again I
sent out my name is Mukesh I've been
with Oracle for almost six years now
I've been the product manager for global
data services as well as alga shining
prior to joining Oracle I was a regular
customer like yourself
I work at Intel and other companies but
primarily were to an Oracle high B&amp;amp;C
technologies okay so what we'll talk
about today from the gender perspective
we'll introduce you to our coach our
team we'll take a look at you know what
does it take for you to deploy start
agents and how do you create a shot at
schema and then once you create the
shard schema how do you do routing from
your clients to the right charge so that
you can execute your transactions all
right and we'll talk about other
lifecycle aspects of the char database
and we'll touch upon the benchmark
results of the scalability study that we
have done we found I'm going to showcase
those results and and we'll take a look
at the competitive aspects at the end
and I do have a customers slides
customer success stories in the interest
of time I may not be able to cover that
today but I certainly would include in
this deck so you can take a look at it
so now what is oracle shadi it's
basically a geo distributed multi model
oracle database okay
in its simplest form a shard database
involves horizontal partitioning of data
across a pool of Oracle databases okay
so let's say in this example I have one
large database and it has numerous
partitions regular table partitioning
right when you
the database what you're essentially
doing is basically or distributing these
partitions across a pool of databases in
this example partitions ABCD on shard 1
we have judged and Sharpton's hobbitses
and so you're distributing your
partitions and of course they're
replicated for high availability I'm not
showing that that aspect in this picture
so now I said they are distributed
across multiple charge charts are
nothing but regular Oracle databases
right but the important thing is these
are these shots run on separate servers
this could be physical servers or this
could be VMs or this could be extra data
for the platter but there is mouth
shapes to rates that binds these shots
together and there's no cluster way that
by three shots together in essence it's
a loosely coupled system built on
showing up thing hardware architecture
platform okay now how do one distribute
your data or partition a data across
these shots oh I get you have multiple
mechanisms multiple charting methods
that is supported in its simplest form
it's consistent hash-based so you get
linear scale they also support composite
shot very nichols assigned some set of
shards for one geographic location some
set of charge similar than you have with
education and within each ear you could
still have linear scalability using
consistent damage those are the two
methods consistent hash base as well as
composite sharding and in 18c we're also
introducing list base and range page
shot as well so this is at a high level
what shard charting me is right now as a
customer long why would I be interested
in child the first benefit is the alias
Kimball for my current business
requirements for this particular
application I could start with few
shards
let's say in this example I have ten
shots and that's good enough for my
scalability requirements my database
size and my number of concrete runs and
Konkan usage etc right
but if own business is doing great my
application is to support more users
larger database and more transactions
all I would do is add more shots to this
pool and I would get cleaner scalable so
basically it's elastic enough
so you add shots the automatically
rebalance data and you get linear
scalability and you would add online
fashion there's no downtime that you
take to add and incrementally scale your
database that's number one number two is
extreme availability if you step back
and see today in a single database
architecture in a non-scientist
architecture all your data is in one
gigantic database and if that database
undergoes a service or switch over a
failure what your data guard instance
right all the users undergo a brownout
during that role change at all times
issue but because in the Charter
database your data is distributed across
multiple shards let's say one of the
shots is undergoing a failover to switch
over so nine of the shots are completely
unaffected they are supporting your
business and the applications are
running only this one particular shot
where you have one tenth of your data is
undergoing a trauma so it's available in
a different way it's not zero or one
it's an online the third one is
geographic distribution this is
important for a global customer where
where you want to store European data in
European descent or Asian alienation
it's like that's one use case now the
use cases you want to bring your data
closer to your users the date
approximately aspect we support that as
well with child so in summary basically
linear scalable the extreme event within
geography distribution are the three key
benefits in addition to Melia now what
I'm going to do in this slide is quickly
touch upon the components of a shot at a
pace and we'll talk about what are the
key features of that okay so if you look
at the diagram here at the bottom most
here I have distributed the shot here a
bunch of shots where the data is
distributed as well as replicated with
Alisha
and I'll talk about how replication is
done in now that's shard that's the shot
here and at the top you have the
application here okay the application in
the application tier you've passed a
shard in key using the api's that we
created that we provide and I'll show
the examples are known as well you pass
a key and then that's how you check out
a connection from the pool and the
connection poles are smart enough to get
you routed to the right chart so that
you could execute transactions and I'll
talk also about how connection pools had
the intelligence to understand the shark
topology so that they can check out the
correction that I'd shot can we gonna
cover that as part of the routing
section so you have depth here you have
the connection poles here and then in
the middle what you have is the shard
management and rotation okay the two
components of the shard management here
one is the shard catalog
the other one is short director short
catalog think of this is the regular
Oracle database that purchase the
metadata of the shadowgate like what how
I did what the Sharks I have we are the
located
what's the replication technology here
you say all that information is
persisted in the catalog in addition to
the metadata short catalog also hosts
the goal scheme of your application so
let's say you have your application as
charlie tables duplicated tables etc
those are the the actual DDL is kept in
the catalog in a dictionary table right
later on the shard directors in
connection with the catalog will make
sure those those details are propagated
to all the shots so your goal schema is
kept in the casual relation to the gold
schema chart catalog also assumes the
role of a proxy router we talked about
what a proxy adulteries when we cover
the routing aspects
so that's catalogs and chart directors
as I said they're instrumentally routing
in management aspect so
for their aerial propagation shadrach
let's play a big role and also they
published the notification to the
collection polls whether it's a runtime
load balancing metrics or whether it's
the service failover metrics right
all that real-time events are published
by char directors to the Commission
votes that some connection olds know in
the little time you know which hard to
connect to as such so they're
instrumental in the routing and the
minus 50
okay now let's talk about the features I
said they're up to thousand shots in the
first release the creation of the date
the charts and the replication is
completely automated
you have declarative syntax that will
show the examples that you specify hey
here are my shots in my notes that you
want to create these shots on here are
the regions that you want to create the
shots you specify for the declarative
syntax and the framework automatically
builds the shot at it it's little
occasionally included whether its data
car or Golden Gate okay like I said
multiple charting methods consistent
hashing composite and in the agency you
have linear oh sorry you have the range
and the list base Charlotte centralized
schema maintenance the beauty of
Charlotte database is you're not
following a propriety interface you're
not adopting a preferred interface yeah
it's regular sequel answer seeking that
has been enhanced in support of a
distributed environment so it's a each
other Oracle database the transition to
Charlotte a this is very very easy
because it's all native sequence attacks
and I'll show examples of how Charter
tables are created using sequence
attacks and duplicated edges as well but
they're all created from one central
location is Catala right so you manage
your schema from one location you don't
have to go to each and every shark to
add tables or modifications so routing
right we created we talked about the
deployment we talked about the schema
aspect and then now the routing right it
a general estimate it now how does the
client know which are to go to okay so
we
two ways of doing it intellect arty and
proxy our direct outing is what you can
use when you want to have when you
wanted seen massive linear skills right
so as part of the electoral thing you
specify a key K could be account ID
email ID ID customer ID etc you pass
your the value of the key and then once
as you pass it using the ApS that we
provide connection calls will allow you
to get to the right chart which has the
data pertinent to that particular tree
once you correctly the shark you execute
you all the transactions within the
scope of that shot I can regulate or
convey base will execute you can do
insert update delete whatever the case
may be on that particular shark for that
particular once that's done you check in
the connection you know and that's very
fast and scalable right so that's
director proxy routing is one when you
cannot pass a shot in key as perfect
connection checkup like the sambar some
modual some workload in your application
that you can't pass a key on one reads
them second business maybe you
intentionally don't want to pass a key
because you want to do aggregations
maybe you're running ripples and then
you need to go to a multiple shots right
not say you're doing sales aggregations
then you need to go to all the shots to
get to that data then you do what's
called as proxy dog proxy Roderick
happens by connecting to the catalog
catalogs other name is also a
coordinator proxy dog so you correct to
the catalog and you execute a sequel
unchanged as is you can run your massive
simple quilt on this catalog and the
catalog as I said acts as a coordinator
it parses a sequel decomposes it pushes
the sequel down to the shots queries get
exited the shard level and then the data
is brought back and then if there's any
ordering or sorting that means to happen
it'll happen one last time and send back
to the right
advantage of proxy routing is this you
don't have to write ETL or change to
move data from a char a database to
another data warehouse and run your
sequel queries there you could never
adjuster it in the stock
you know pile by taking advantage of
this coordinator and run your sequence
to generate your shipments so director
thing is what even used for you all
sticky transactions Roxie routing is
what you can use for you reports etc I
also talked about the online scale out
so for incremental scalability you
simply add charge you getting is
scalable you can also scale scale it if
you have allocated too many shots maybe
if you want to relinquish some you can
take them off as well charting is
supported both for on-prem as well as
the cloud right you could even have an
hydrate deeper limit some shots in the
cloud some chance in the only
thunderstruck and then the sharding
framework gives you full support for the
lifecycle whether it's if you want to
bring balance you can explicitly do a
rebalance and field if your chunk of
data is too big you can spread a charm
right all those aspects are natively
supported by sharp edges we'll talk
about those later now I talked about an
application for higher input let's talk
about this the default high availability
is statical so for each and every shard
you have but by as part of the default
deployment we enable there you got it
faster trainable
right so if it shard is down the
automatically failover - its Stan line
that's data card we also support Golden
Gate so in this example have three shots
I have a chunk of data shown in
different colors like in this case of
gray I have the chunk is in chart 2 and
its copies in shot like well the red is
in shard 3 and it's copies in channel 4
and so forth
right so the bad and then it be
automatically enable
chunk level active active application
using : game with Auto CDR the content
detection and resolution so if a shot
goes down no big deal we have these
copies of the chunks or the other shots
and we automatically route you to those
thoughts and so we
retards you have high availability bacon
for your data so your data is available
within these three shots versus in data
guard you need have a dedicated standby
for each charge and some customers were
expressing just in enabling rack at the
shard level so in this example I have a
tool or draft cluster for each shot so
we could if you if you want to have an
additional load level skill you can even
believe that you have that opportunity
that - along with char any questions so
far
okay so we talked about the concepts the
advantages etcetera right now let's talk
about and how do I create a shiny lips I
talked I mentioned that there is some
dignity of syntax that we created that
you can leverage to deploy Charlotte
vehicles so now in this example I'm
showing few commands that will allow you
to deploy Shange tips my again my
intention is not to bore you with the
syntax and all these parameters but
allow you to appreciate how simple it is
to specify your topology and make one
command called deploy - which does the
automation for the creation of the
shadow games so you say create chart
catalog that means you're building this
catalog and you specify which database
that you want to catalog to be on that's
it's wrong and then you say add yes and
DSM is nothing but a global service
manager the marketing name for that it's
short director right so behind the
scenes it's the GSM listener that is
being created that acts as a shortcut
for all you care it's a regular Orca
listener that's been beefed up that's
been empowered for a distributed wonder
so you create a pool of G essence in
this example I have to but you could
have up to five years of Smith's change
five directors in the stadium right so
you define your GS
you specify a list of ports and later on
in your application connectivity you're
going to use these listeners instead of
his can listen Charlotte Davis listener
given a loosely these SH are directors
listen points in the taro States after
the DSS are created you specify the
credentials of your Oracle OS user and
password okay and then you create these
logical groupings a shard group chart
with one and the childhood to so sharp
blue one is a set of charts that are
deployed as primary and childhood two
could be a set of charger for this
channel you'll just specify these shots
in this group are to be deployed as
private these shots in this shot groups
are deployed as ten bucks right during
the deployment phase and then you say
I'd invite a lot you're registering
these is servers with the cache mode and
then you say create shot right you're
basically here you're specifying hey I
want a shard that should belong to the
shard Group one that means it should be
deployed as a primary and here is a
horse that you want to create the shower
on and then you specify Prevention's
what you did until now with these steps
it's basically a specific amount you've
specified the default in this case of
data guard is specified the hosts using
a create shark man right and then
created the shark group the logical
grouping etc and then once this
specification is done you create your on
the deployed man all of these are done
in an interface called GDS CTL
command-line utility you can also do
this for Enterprise Manager so you don't
the deploy which does the automation of
creation of the chart databases
setting up the replication and if it's
dating are it will set up the broker
with the password field work with the
observer enable right the whole nine
yards is completely intuitive you just
specify what chance I'm going to have
and what application technology that I'm
windows after the database are created I
would say add service I will create a
global service in this example all to be
rewrite service around climate that
means I will run this service on all the
shots whose current role is Frankie
right and this is the service I'm gonna
music in my tea in the scenes along with
being shard director mister
now Nagesh yes quick interruption we've
got a question for max I assume I need
standard Oracle licenses for each node
and the Chardon configuration what other
licenses will I need absolutely that's a
good question
so charting is included as part of your
active Data Guard or Golden Gate or
Oracle I'm sorry as long as you have one
of these highlighted ability options
charting is covered the other important
thing is this have the Enterprise
Edition databases so it's plus one of
the high availability options
interactively record or or Kodak or go
to there is no additional separate
license for for Charlie ok so now we
talked about the deployment now the next
important thing is is the data modeling
and the application constrictions
shouting is very very close to the
application developers so the mounts on
a database dealing is drive it right the
creation of the database is setting up
the whole nine yards is done but it was
but shouting yes DBS do all the good
stuff of deploying the database etc but
once the deployment is done there is a
handshake that needs to happen with the
with the application developers as well
as octaves right that drives the success
of a application of the Chateauguay mix
and we'll talk about what those are
right to get the best scalability French
on a database there are certain design
application design and data modeling
considerations so what are those first
thing is the the application should be
char amenable right so to do aspects to
that one is that certain tables must be
nominated as a Charlotte table family
and certain tables are normally has
duplicate tables so let me talk about
what those are so in this example I have
a schema which
which has customers as a parent and then
at the root as well as the child or
version Manhattan's grandchild table and
other products tables okay so the
important thing is these tables the
customers orders and line items must be
a traditional okay
that means if my charting Keys customer
ID in this one I need to have customer
ID in the orders as well as the customer
ID in the nine items as well I'll show
you an example of how it would look
- okay and in the reason why they need
to be a coop additional is so that we
can store related data together that
means a set of customers their orders
and they line items those related
petitions are always kept in one and
only one shot right the advantage being
if an application comes in with a
certain key let's say maybe it's
customer ID you will be rest assured
that you've got to go only to one and
only one shard to get Mary's profile
information many so large main line
items all within the scope of one chart
because we have a competition and store
the related partitions in a given sure
okay and that's our scalability but you
want to have co-locate relatedly right
so that when you do joins and indicated
constraints checks you're always doing
within the scope of one shot so you're
not globally doing these checks that
would be you know inhibitor to a linear
scale now I guess another question for
max does include reference to partition
tables or just the key have to be
physically present in each table right
so the key has to be present in each
table that is the requirement at least
in this first release we expect that key
to be there in all the child tables and
then you would you would call out the
reference petition and this would be
very apparent by ratio with the syntax
how do you create a row table and child
table and I'll walk you through that
the the other important thing is for
that given application for their
requirements of that application you
need to identify the best shouting
method when it becomes in hash would it
be composite or would it be list or
range where the list of kings are mapped
a shard or a range of keys on my pizza
it all depends on the application like
levels and what's the right starting
button and what's the right sharding key
that need to be closed during the design
time right once you design it and then
deploy shot a database with this
particular matter and this particular
key it's not easy to change it unless
you read all your shots so it's
important aspect that means to be closed
with the application developer what's
the method was and the sharding key must
be the leading called modulation right
most of the cases the primary key is a
shutter key but if it's not the shadi
key has to be the leading column of the
plan and for tables that you cannot
place the sharding let's see in this
example I can't place a customer ID in
the products table what we allow you to
do is there is a mechanism with which
you can duplicate this particular
products table and all the charge right
so the products table let's say you'll
have all the list of all the products I
mean that information that particular
table is duplicated on the charts so
that in your transactions if you're
joining customer ID customer tables
orange light events and products again
you're joining lives in the scope of one
shop your transactions are not doing
distributed transactions and typically
these products are you know they're not
late right
kind of a use case it's mostly type
thank you loaded periodically let's have
once in a day or once in a while and
then you want that particular data to be
duplicated on all the shots so those are
the data modelling considerations that
we expect now here is an example that I
said how I'm going to create the
Charlotte tables of the duplicated tips
and duplicated I take the same example
of customers orders and line items have
some sample data here mary jo and peter
color-coded and then I have many
soldiers in red Joel in orange
now when I shot them I want to be I want
to co-locate them right so I had to make
sure many swords line items and her
profile information is always kept
little muncha ensuring one record
example I could have millions and
billions of records right but so
basically what i'm doing here is i'm
co-located in two traits a data together
likewise john his order should lie that
was stolen short peter in charge so on
and so forth right and what about my
products the duplicated table we make
the copies of them on all the shots
using read only materialized views every
30 seconds this data which is this
products table which is created on the
catalog database in synchronize on all
the charts using read-only which allows
user okay so now mainly comes in she
will be routed to this particular shot
chart one and when she performs
transactions all her data is with him is
one shot that's how you get to you get
linear scalability you could have
millions and billions of customers at
the max for each customer you would only
go to each charge so before I show the
syntax there is one important concept
that I won't touch upon and this is very
critical as fresh alligators here's the
concept called chunk chunk in its
simplest form is a tablespace in a
simplest form is a tablespace that
contains related partitions of tables so
partition P one of customers partition
people of orders partition P love line
items those three partitions are placed
in one table space and that table space
is called
okay now a DBA can create these three
partitions on three different table
spaces as well in that case those the
table spaces the table space that's
hosting customers p1 the table space
that's awesome orders p1 and the
template that's hosting neither does p1
those see table spaces are logically
back to what's called as much so in
simplest form something that'll take
space but in cases like this where you
want to have separate table spaces for
separate partitions those three table
spaces are mapped to a chunk chunk is
our unit of data move it for example
when I add charge we rebalance now how
do we rebalance we rebalance we move
data at the level of this chunk right so
a chunk is moved from one shot to the
other as part of rebalancing so when you
move it you can again be rest assured
that the related data is always kept in
that chunk and you're moving them with
at the level of the chunk right if not
in a state where you will leave this
customer people in charge one or as
people in short the etc not in that
state you're always eating well together
right so chunk is like giving up data
movement which is this plays a big role
as part of the balancing when you add
charge that we move shots etc okay now
there is another interesting one that it
explained what's called tablespace that
when you create a let's say you stand
back and in a nonchalant database when
you create a partition table as a DBA
what I would do is I would form I create
the partition table and this not my
columns my data tides and I specify my
partition strategy like whether its list
arranged or hash etcetera and then I
list all partitions
partition be 1 2 3 4 etc and then I map
a tablespace to each partition
typically for for good practice we
mapped a tablespace 4 inch partitions
all right and we take the same exact
notion here as well into the chart
database but then in a shara database
you will have thousands and tens of
thousands of partitions right so now how
would a DBA
go and create is 10,000 tablespaces what
we did is we automated that so with one
command called create tablespace set
Adva gets to create new - table spaces
and what's their number so in this
example have shot 1 2 &amp;amp; 3 the default
number of table spaces the default
number of chunks for shard is hundred
and twenty that's the default kept set
120 table spaces in shard 1 1 2 again
chart open 2003 so altogether over these
three charts we create 360 table spaces
right maybe just one come out okay so
the prerequisite is done the tablespace
creation is that now I would go into the
creation of a surety tape so I did the
create tablespace set as a DBA and I
have 360 table spaces so now I will use
the new syntax called create Charlotte
table customer and if you look at it the
syntax is very similar to what you've
been used to in a partition table your
columns data types constraints etc and
define your primary key at this time
around for this strategy you'll say
partition PI you say consistent hash
this is the algorithm that we use for
you to get linear scalability for over
the distributor in logic and the
sharding key which is the customer lines
and then I also specify the TV space set
which is nothing but I just the one that
I discovered the previous solution the
360 table spaces right
the last thing also note is the
partition sort oh I'm telling the system
Hey create 360 partitions I already have
360 table spaces create 360 partitions
map each partition to the table space so
that complete definition of a char the
table across the shards is completely
automated during the simple syntax write
something that you all means to be
useful except for few causes called char
the table and and and the algorithm for
for consistent hashing so this is my
room table what about my child's table
in this example and showing the orders
as I mentioned I need to have my
customer ID that needs to be according
Cordelia eyes for this particular
application in design
I have customer ID in my child table and
I have family to me and the foreign key
that reference to the customer and I say
partition by reference right what I'm
doing here is exactly the equipment
issue if I have 360 partitions here I
will have 360 partitions at the orders
table and that's how we know what are
the and these partitions are created on
exactly the same customer ID 360
partitions based on customer ID since
its position based on customer ID here
as well and that's how we know what
partitions are or need to be co-located
and placed in a given chunk so and like
when you have line items table say it
follows the exact same scenario use
partition whatever's there as well so to
answer a question that you posed max yes
you need to have customer ID that needs
to be DWIs
basically the order needs to be dealer
was with the customer ID and also you
would use partition by reference so that
they can co locate related data and what
about my child table right we have a
sink
for that as well can you duplicate a
table when I do this particular but I
execute this particular syntax I'm gonna
create this table on the catalogue right
that's where I load my data and then
every 30 seconds they read only
materialized views will get refreshed on
all the shots right so the data would be
updated on the shelves I'll stop here I
know we talked about quite a bit on this
I won't see if there's any questions I
can answer
there's no question you want so though I
think one thing that will come -
sometimes - hey um and do I have do I
have it do you have a single point of
failure anywhere in this random
variables let's talk about that I
mentioned shot group one shot group to
write one set for primaries one set for
standbys and if something goes down we
automatically fail over at the shard
level not at the group level right this
particular shot goes down by failover to
its standby right so repeater of time I
could have some set of shots primary in
Chandra form some set of shots at
standby since I'll go to a city but the
important thing is you have high
availability for you data whereas data
guard of : get your projector shard
catalog needs to be protected with them
actively regard as 10 bar right so you
have protection for that as well and
then Shawn director you want to have a
pool of sharpness in each region right
I'm showing - you could have a bare
minimum one - haha I mean don't yell the
best practices for a tree so that the
few Corrections
so this basically you have full of
characters that are doing the routing as
well as the management of your shattered
database so you have high availability
as well as scalable to make Tim using
the pool and then of course you have the
connection
okay so bottom line is there's no single
point of failure but that's catalogue
but it's a chart here or addi tion
director here comes the most important
one we talked about the deployment we
talked about the schema requirements we
talked about how we create a shadow
schema and they duplicated tables etc
the next step is for the applications to
get to the right chart for a given key
so what we have done is we have enhanced
cue api's whether it's OCI
but it's only bit out there but it's UCP
right and if you have if you have third
party Tomcat it's a tar or WebLogic it's
tough you can still use it as long as we
use UCP as a data source behind what I'm
going to do in this example is I'm going
to show you and what are the API calls
that you would do to build a sharing key
and pass the charting key to get to
leash like shot so here on the call so I
create sharding cable that in UCP is the
call that I'm gonna do I'm gonna build
this key form a rock step the value of
that particular sorry I'm gonna build it
I'm gonna check out a connection using
the key that I just built using the
great connection below right from the
connection cool perspective
I built it and now I'm gonna check out a
connection using this key that I just
did okay these are the two things that
you would do from your application to
get to the right shop okay so now let's
walk through what happens when you check
on a connection that's okay that the two
phases but I'm going to talk about the
initialize pull initialization case
first and then I'll talk about the run
time case well let's say the database
the shots are up and the pools are just
initialized right so now the customer ID
1 2 3 is passed to the application so on
the correction pool now got the key now
it'll go to the Charlotte and the
Charlotte understands the shark apology
map looks up the map and knows that hey
this particular key 1 2 3 is in shot -
I'm going to redirect that connection so
that from then onwards this connection
is made on to that particular
all transactions are made with Muskoka
the chant once the transactions are done
you check in the connection and and this
is the process that happens when foolish
spoons are initialized on the very first
connections to these charts right so
that is an important enhance when we
have done on the very first connection
to this particular shard connection
fools are empowered to extract the
topology information from this
particular shark into their memory once
the topology information the information
such as what are the chunks that reside
on this chart what are the ranges of
sharding keys that are mapped to those
channels that information that metadata
right is brought into the memory of the
collection pool the application zone and
it does so on the very first collection
to be charged or a period of time the
connection points will have a memory
structure which looks like this hey what
are the chunks in this chart data phase
what are the ranges of sharding keys
that are mapped in the chunks and what
are the charge that these chunks are
deciding on right this array this
structure is in the memory of the
Croatian
so once the pools are initialized
whereas when a new collection request
comes in the connection pools know the
topology the topology structure so it
knows and it goes directly to the shard
for a given key completely bypassing
each character so at runtime the
connection will score directly to the
shards days or the key that you'd
provide completely bypassing this a
direct and that's how we are very fast
and scalable right so you may wonder
what if our new shots how does the
collection pool slow or if I remove a
shard how does the collection pools no
cash from the shard it does play an
important role okay the anytime there
are structural changes configuration
changes that happen in the shot or
whether you will be located a chunk
whether you added a new shot on those
kind of a structural changes or the
configuration changes chart directors
publish fan notification the first
application notification we are the OEMs
Channel the Oracle notification cells
like so that sells the correction poles
update their memory structure let the
new information about get information
from this chart directors and this one
happens in front I'm a realtor so that's
how they know Chili's are big topologies
and that's it many got moved to another
short they will automatically route you
to that particular shop ok so this is
the fast and direct routing for to get
the best scalable think this is one in
what you want to do you want to build a
key and pass a key go to the right shot
executed her last you could have
thousands of shots millions of customers
and you collapse petabytes of data
across these shots
you still get maybe a scale of good
alright and I'll show you the baseball
presents in a bit
so that's democratic right where you can
use it for multiple transactions now for
reporting workload or for workloads that
cannot pass a key you would do what's
called as proxy dot this is column is
for multi short plays and you want to go
across multiple shots to get your code
insane security so a game important
thing is both with direct routing and
with proxy routing then is zero change
to your sequel court your corn is
untouched you change same exact simple
and then you played played here right
the reporting in this example that
reporting for that so then you run this
query you it has groupers and roll ups
etc you are going to the coordinator
first right which is the chart catalog
it acts as a proxy outer the coordinator
parses sequel and rewrites the queries
so that the most of the processing is it
is done at the shot left okay so
basically these shards become compute
nodes in this example right and then
once the queries are executed here that
it is grown back sorry the dailies
brought back to the coordinator and if
there's any ordering role upsets at all
that need to happen on this data that's
brought from the from these shards
it'll happen here one last time and then
it is centrally applications and the
important thing is we also support short
pruning like for example in this example
it's a scatter gather because I don't
have weight loss but let's say I have a
weight loss weight Sharky in there are
certain list of shark is at strong we
know that hey these are the shark is I
think you have to go after and we know
that what charge these keys are and you
only go to those shots and we prune the
rest right just like how you
petition pruning is irregular
petitioning world in the Charlotte
database ball you have sharp cooling as
well right you don't always do scatter
gather across all thousand shots you
only specifically surgically go to those
shots that have these keys and finish
your execution and you get back right so
that's again an improvised metric and it
all happens because we have optimizer
here I have a distributed optimizer that
will do that does all this tricks but so
we covered they're talking cover proxy
routing now that's another question that
will come up for demons actually nagesh
there's another question that just came
up on the chat yeah thanks for multi
shared queries you previously mentioned
duplicated tables does database resident
code like PL sequel also get
automatically replicated yes yes if you
do have PL sequel code that you want to
be present on all these shots you can go
to the catalog execute them there and
the directors will make sure that those
are all automatically propagated and
duplicated tables will be present on all
the shots as well as part of a material
as we refresh so DMS
sorry table that certain restrictions
with respect to DMS I just want to call
it out here in this first release the
DML statements such as update this table
set column this predicate where sharding
equals constant right
this when when I run this it can only
affect one of only one shot I can't have
you know a massive update that will
spawn and run across multiple shots
right so that's important restriction
that we won't know it's just one or only
one shot but that being said in my
transaction I could have multiple single
shot earbuds in this example in my
transaction here I could have insert
that effects when you shot one or an
update in this example a fixed shot to
dealing that can affect shot they have
all these thing in my transaction and
then I could commit it right in this
case I'm making multiple single shot
earmuffs
what happens is the coordinator will act
as a well now we'll start a distributed
transaction DeLuca's conduct I clicked
all these three events
alright the life cycle aspects DBA can
explicitly move which hung from one
chart to the other if they believe that
hey this particular chunk is very active
and very busy in this shot and it's
thrashing this particular maybe I'll
take it to an Amish are that's not
overloaded or maybe to a powerful server
I could explicitly move a child from one
shot to the other with simple genius
it'll come on
if I jump becomes too big I could even
split it charge a little and then you
remove the top of the chunk turn of the
show thank you can you have those
controls but a new shard is added chunks
are automatically rebalance and we use
traditional are male incremental back
and transportable tablespace technology
to relocate the chunky balanced the
chopped question to avoid shard spanning
DML and before I run say an update can I
get a list of what shards would be
affected
what shots will be affected
oh there is a way to find out what
shards is what charting keys are back to
what charts yes you could
the digital table to identify what
charge will be event yes that's one way
there are other ways to go to the
execution plan and then see that as well
so we do have the execution time that
will define every child it's going after
so when I when I had a sharp I talked
about the Armin incremental backup and
transportable in space technology that
are leveraged to rebalance to move
chunks at all any time when you split a
chunk move a chunk and I shall remove a
shard
I mentioned that the correction pools
get notified we are the oldest channel
Oracle notification service so the
correction they basically application
has two choices that hey you can
reconnect or access the data lead only
fashion right and that's with data guard
but he's Golden Gate because you have
multiple read writable chunks the moment
you are in the state you can always go
to another copy and and because shards
are regular code database all the best
practices from back in the offender
story all whole good whether its charge
or the cache block and if you want to
catch on the shots you can go to one
location to the catalog and run Omaha
trip and you can specify do you want to
run it in parallel fashion or in a
rolling fashion yes like i said 'i'm
also supports monitoring and management
of shadow database along with the
command line interface continues CTL
there is a sequel develop integration as
well so you could create your charlotte
tables you can build your schema using
sequel developer last couple of slides I
want touch upon the benchmark reasons
it's an interesting exercise that we
have done to understand you know what's
the scalability aspects of accommodation
what we did is we we acquired few shots
200 of them on the Oracle cloud we have
100 charts and availability domain one
in Phoenix and hundred in the are
they're able to the weight and we are
using active
as part of the default sorry
and then each chart is very high-powered
that 36 cords vital gigs of ram etc
right
many high-powered servers at the shard
level Google wants to understand as I
add charge to this pool I'm going to
linearly scale by application or not
that's one and the second thing is
what's the availability the extreme
availability that I tell to the box
right now you know am I seeing that are
you exhibiting that or not okay so what
we did is we took the swing bench or
entry application they ported it to be
shot amenable basically I just in the
schema to have the key to be there
another child grandchild tables and
followed the design conditions that we
touched upon ability a part of the stack
we basically did those constellations
apart it to be assumed each other engine
application and you started the Shonan
database 0 25 50 and we added up to 200
charts and we were able to skim the
application transactions million right
and this includes both read/write as
well as really work ok that he drive was
working on the families and laid only is
what you want be sandbox so now let's
take a look at those numbers the elastic
scale have to do on the charts we kept
adding shots up to 200 as we kept adding
the shots they were able to linearly
scale your transactions that out of 11
million transactions we were able to
execute per second 0.5 were lead right
0.5 million were late right and 6.5 were
read-only that already understand but so
basically all your shards well in
Stanmore prime and they're all
participating in the production workflow
in this example and this is no surprise
to us and this is no surprise to the
current production customers who have
seen similar behavior as well as long as
you get the right application as long as
you make those design considerations you
are you can see these kind of numbers
right in this kind of scalable change no
matter linear scaling the aspect the
other one is the highly available to the
X availability right so now what we did
is we intentionally
down one shot out of this hundred data
got configurations I'd have a primary
hundred standbys we in general is shot
down one of the primaries and what we
observe is there one hundred percent
availability for the light that person
of the application and that one shard
that is brought down underwent as I'm a
goner
15 seconds failover time in the Oracle
cloud to its timeline right and this is
the 12.2 you talking about extreme
availability from Oracle perspective if
your application is available from my
dead personal application from my
provider personal users it's just one
person users data of pain over by the
background and this is the data of
course very few years Golden Gate even
that's eliminate you as well so the last
couple of sites and somebody what is
shouting basically what charting it is
is this best of no sequel investor fork
right mostly comes from great they
brought him a linear scale routine using
commodity servers extreme availability
and automatic deployment etc and as
you've seen with charting as well you do
have automatic data with data
distribution you have built in the
application setup you've got linear
scalability and then of course the more
than this it's the these shouting
letters it's not just the consistent
hash we support that like most loss
equals to its the list space the range
weights the composite higher dedicating
circles are the showers for different
regions and then the centralized
management and most important is a
latency you're not undergoing you know
adopting to it provides interface it's
same sequel that you've been used to
with enhance intense like you can all
that but you're not sacrificing passive
you don't sacrifice any consistency not
sacrifice to get transactions and then
you get all your traditional strengths
of alone connecticut's but that security
was compression
whether it's a multi modality relational
JSON there also put it by Charlotte
database okay so you get the best of
both worlds in disk so if somebody what
we talked about is charting it's a
platform for you to Bill
and distributed scale our database right
what we leveraged is time-tested
technologies that you always had and
paint this architecture we had
partitioning where distribution using
application technologies we con located
later data right and we did routing with
a local net listeners they've been there
forever they have been built up for but
Elizabeth environment now and then the
lifecycle management right all the time
does the technologist we always had have
been put together and people the great
solution for in civil environment and
the important thing is ideal for
applications where the data access
patent is primarily we are charting P
prime alleviate sorry that's when you
get the best of fish or the database if
your name is know if your applications
always going to do you know proxy not in
workload maybe that's not the right one
for a push hard at least for this
release you're in this in this example
because long as the data access pattern
means we are charting key you get the
best out of or charity space we've got a
scalable team you can't extreme
availability and then you get the global
nature distribution licensing I talked
about it right thur on Prem it's you
need Enterprise Edition plus one of the
high will the dumps either activity card
or going into rash it's inflated it and
if you want to do it on the cloud you
need to have extreme pulse collards
before we open up for Q&amp;amp;A I just want to
touch on this particular side which
talks about how do you can get way this
collateral there is one for ot and
Oracle technology Network there's a
portal that we have for shouting I would
strongly encourage you to go there as
well
go to our couch re I do write blogs in
my blog site please go check it out
that's one other place to locate and
then do followers or interest media both
for having Ellen as well as mine my
profile as well I'll stop here and then
see if you have any questions and then
hopefully I'll be announced at this time
okay hopefully most of your questions
have been answered if you still do have
questions deeply a spring to reach out
to us we would be glad to help you on
that as well as if you're if you're
planning to do a proof of concept do
include us do let me know I'll make sure
that you get the right help and if
you're stuck for anything actually I can
definitely see you in the right
direction the important thing is if you
are doing POC do check out this
particular website the go to Oracle
charting which has all the collateral
where there's documentation webcasts
cookbooks we can walk you through you
know how do i how do you go ahead and
deploy shadow database how do you create
a sample chart schema how do you do
routing all those examples are nicely
covered in the two books that I place it
in this petroleum sorry I stole mine can
you take a look at that as well</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>