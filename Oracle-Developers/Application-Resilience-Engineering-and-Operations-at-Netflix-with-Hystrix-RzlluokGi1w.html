<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Application Resilience Engineering and Operations at Netflix with Hystrix | Coder Coacher - Coaching Coders</title><meta content="Application Resilience Engineering and Operations at Netflix with Hystrix - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Application Resilience Engineering and Operations at Netflix with Hystrix</b></h2><h5 class="post__date">2015-06-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/RzlluokGi1w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so with this presentation today I hope
to be able to leave you with on both
collection principles that we've learned
on my team at Netflix that you can go
and apply but also show you some of the
tools that we have built in open source
I had to put hystrix in the front we got
such a cool logo be ashamed not to put
it up there for the designers and
Netflix did it for me
so I am going to be using hysterics
today to demonstrate the principles but
whatever technology stock you are using
the principles would apply equally so
also if you anyone has questions during
it to clarify something go ahead and ask
if it's something that will take too
long I'll punt it till later or if I'm
going to answer it later I'll let you
know but feel free if we're a small
enough crowd that you can just raise
your hand and I'll manage so I'm in
Netflix we do movies and TV you should
probably know that being in the u.s. we
are now in a lot of places around the
world and one of the fun parts of
Netflix right now is we we're growing
and so it gives us lots of fun
engineering challenges to deal with and
we make up a lot of the downstream
Internet traffic to the last mile we
account for about a third of it there's
a whole other group with the Netflix
that does some other really fun stuff
they're building built out their own CDN
to because of that number and the API
team which I am on that's been our
growth over the last couple of years and
the team I'm on sits we're basically the
the facade that sits between all the
Netflix internal service oriented
architecture and then the external
facing the Internet all the devices on
the on the outside and there's over a
thousand different devices that connect
to us broken up into dozens of different
types of them and there's two major edge
services in Netflix the one that does
all the the actual video streaming
negotiation licenses DRM those types of
things and connects it to the DRM sorry
the CDN servers to get the actual video
bits that's not the team or architecture
I'm going to be talking about today the
other half of it which is the part that
all the devices talk to for the
discovery piece everything that drives
the you is now
we actually are working on merging those
two closer and closer together it just
makes sense as we evolve our API but all
the examples I'm going to give today are
from the the a pure API portion which is
the discovery side how do you drive the
the discovery process of these our
interface and so these are just some of
the UI's one of them is for iPad the
other one I think is for ps3 if I
remember correctly in TV base UIs and so
the the Netflix API sits in basically
some people describe as an hourglass or
the fan and fan out
very typical it's the facade that all
the the devices connect to a all
environments prior to Netflix that I
worked in you typically have like your
browsers and like a mobile and a desktop
type situation Netflix is a little bit
unique and they have all these different
devices
and so that started to change and evolve
where we where we took the API I'll talk
a little bit about that today just in in
the context of fault tolerance the real
story though is everything that happens
behind so underneath that line the the
netflix api line underneath that are
dozens of different back-end services we
have somewhere in the neighborhood of
200 ish commands that represent the
functionality that we reach out into in
that service-oriented architecture that
architecture is spread across three
different amazon regions 100% of our
infrastructure is hosted by AWS and we
have deployments in u.s. east u.s. west
and EU west and within each of those
regions we have we used three
availability zones from amazon and
within each of those availability zones
we have a large number of auto scaling
groups or clusters and within each of
those clusters we have tens to hundreds
of machines and at any given point time
machines can be failing
now the first level of fault tolerance
is fairly straightforward a machine dies
our discovery system and the load
balancers to automatically detect that
remove them and launch new ones and
that's fairly straightforward machine
level failure is something that for a
long time now Netflix is not concerned
itself with it all that's as far as
we're concerned that's a solved problem
in our
structure it's such a common thing that
you may have heard of the chaos monkey
and it just goes around and kills
instances randomly just to make sure
everyone is truly resilient to that type
of a thing and so that's the extent of
what I'm going to talk about as far as
instance failure today the type of
failure I'm going to talk about are
going to be within the application so
outside of the application you've got
these instances or aSG's like those
different kind of infrastructural
components fail the infrastructure is
pretty good that you know with that
within Amazon or how we use it and
dealing with it and if you are
interested in some of those aspects come
and chat with me after and I can point
into some other slide decks from folks
at Netflix who have published more about
that subject when I joined the the
Netflix API team though a couple years
ago well we had some problems with our
with our app it was actually pretty easy
for it to fall over and one of the ways
that that would happen very often is you
had one of the many underlying services
go bad and by go bad that could be that
it was failing quickly actually
returning errors but more often than not
was because some latency occurred
somewhere and latency never happens in
like you know a 5% increase it's always
an order or two of magnitude when it
happens
and what that does is it wipes out the
entire cluster in seconds at scale and
so we'd have a you know a thousand
machines all running talking into
thousands more and any one of these
routes these dependencies a piece of
functionality the more popular ones that
are used by a lot of our incoming
traffic within seconds at that rate we
would saturate every thread and every
connection and we use Tomcat in this
case and they would just fall over dead
and the whole thing would just be toast
all because one system underneath us
went latent and this is a fairly common
experience and one of the VPS of
engineering put up this math once and I
I took it to heart and you know it's one
thing to say that your system has 99.99%
uptime but when you start multiplying
that by the number of systems
that you may have within a company it
actually isn't as good as you might
think
because you start multiplying that
together and you get
and lots and lots of failures still even
with four 9s on each of the independent
systems and reality is always worse than
this you claim that you have 99.99 on
something well over what time period all
it takes is that one month and they need
it and then you're toast right and it
all it takes is one of those systems
going down for four hours and if your
entire infrastructure is brittle enough
that that latency or failure can
propagate across and just in in cascade
across the infrastructure what we would
see is that the API level at the top all
the failure within the company
eventually hit us and so we started down
a path of how could we improve that how
can we change that story and some of the
constraints that we had on it so you
understand some of the engineering
decisions that I make that we made that
I'll share with you a little bit later
is speed of iteration is a big deal at
Netflix so we know that we're not a bank
we're not a nuclear facility or anything
like that so we're allowed to be a
little bit more nimble we're allowed to
fail a little bit more often as long as
we can recover quickly we're not going
to kill anybody we're not going to cause
people to lose money and so that's one
of the things from the top-down we're
just we know that we're optimizing for
speed how quickly can we iterate on the
user experience and so whatever
decisions we made about fault tolerance
had to take that into account that meant
that we cannot swing the other direction
of saying let's try and lock everything
down there's actually other reasons why
I don't think that that's a very good
idea but that wasn't an option we
couldn't say we're gonna harden this
stuff and really test it before we push
it out we had to be able to be deploying
every couple hours if we wanted to the
next thing was that within Netflix the
part of that speed of iteration is that
teams when they expose functionality
they deliver a client library that
everybody consumes so this is a slight
variant on service-oriented architecture
where in a pure service-oriented
architecture you put up your restful web
services or whatever protocol you choose
and then everyone could consume that
build their own clients against it and
you're actually it's better for
decoupling we have accepted that a small
coupling between the systems by sharing
client libraries because it allows one
team to deliver
for that and then everybody very quickly
adopt the change rather than potentially
a dozen teams all having to reinvent
that same client but it does mean that
there you get it's much easier to
propagate a bug and it's also you couple
the systems a little bit tighter because
the object models are now flowing across
and so that was another constraint we
had to account for the last one is that
we had to account for the fact that
we're a mixed environment we're polyglot
on the JVM and most of our stuff is on
the JVM but even outside the JVM we have
some other technology stacks running as
well but most of our production system
is Java based but we had to account for
groovy and Scala and closure on as well
as Java so if we go back to what that
failure state looked like what we needed
to do is figure out a way to handle that
one piece of failure and isolate it to
itself but it wasn't just that the
network layer there's a lot of other
things that go in partly because we have
those client libraries and a lot of the
decisions that we might might make to
optimize things you end up with the
logic of when the argument comes in it's
all the pre-processing before I do a
network call and then you do the network
call sorry you serialize the data which
is a whole piece of fun and then you
perform the network request and then you
get the request back and then you're
processing it doing validation logic and
other types of things maybe you're
transforming it into some other format
so everywhere along this path some of it
running some of it client-side some of
it server-side some of it network all of
it can fail and we have to basically
treat all of it as untrusted and this is
what happens when you don't isolate all
of that stuff this is a failure that we
had years ago where one system went bad
and it's the down the ball on the stack
trace is showing it creating a socket
and it's just sitting there hanging
waiting on those socket connections and
so this completely wiped out our our
system so any of you years ago when you
were trying to use Netflix that one
night if any of you were that you
couldn't it was toast
and the
the worst part about it was is that it
was really quite a trivial thing that if
we theater said stop attempting to do
that and just degrade we really didn't
need to be down and then the next bad
part is that this kind of a failure when
it saturates every resource in the
system it has a really hard time
recovering from it and in practice we
just had to recycle all the boxes and
that is never a fast way of recovering
so that was not what we wanted so
instead we we pursued bulk adding so
bulk adding Michael Nygaard and his book
release it and and other systems talk
about this a lot it's not a novel
concept ships have been doing it for a
long time even Titanic cab them they
didn't work so well in their case
because they weren't you know maybe they
didn't have as many as they needed but
this is not a novel idea but we weren't
using them and we hadn't implemented and
so we we needed to be able to create
bulkheads around each of these different
dependencies and treat every network
interaction as an untrusted thing
basically anything that would touch the
network we had to treat it like it's
that iceberg that can come and kill us
which we had been seeing over and over
and over again so when we can bulk at it
what that should allow us to do and this
certs become one of the engineering
constraints it's easy to say go isolate
the thing but what does that mean for us
what that meant for us is we needed to
be able to insert ourselves in between
the the interaction so that when failure
occurred we could then do something
about it
we could control how many resources it
was consuming and we could also control
the response back so that as that
dependency failed the response back to
the user waiting on it we could start to
make decisions about what to do and we
could degrade that user request rather
than completely fail it so some of the
different ways that we can do bulk at
there's a few different ways but there
was two that we pursued the first one is
we wrap around this thing a semaphore
tribal semaphore if you don't try it
it's not very hopefully to block
everything in a different way but a
tribal semaphore where what we're doing
is we're restricting the amount of
concurrency concurrent requests to any
given underlying system and it's amazing
how
very simple thing completely changes how
your system behaves when it's failing
and so what this does is it says - this
back-end system I know that normally I
should only have three or four
concurrent requests open
let's give it ten and because 10 is
still I'm just using random numbers for
you all let's say that your Tomcat
system has 200 incoming requests that it
can handle you don't want any one system
to be able to take up 50% of that or
100% of that so let's say this back-end
system you're getting give it 10 so you
could potentially lose 10 of your 200 so
in this case when you start hitting that
10 you just start rejecting everything
after that and so everything just starts
bouncing off of it and so you shed load
if that back-end system has gone bad one
step further from that is you can use
thread pools for them and with the
thread pools you sized them to the same
way that you would the tribal semaphore
is but now you also get the added
benefit of being able to walk away and
time out something that is latent and so
here we get two opportunities to protect
ourselves one is on the concurrent
throughput we're able to reject when we
saturate that and then secondly we're
able to clean out the underlying calls
with the timeouts on them so this left
us with basically this idea of the
various points where we could where we
could control it so the first one is the
obvious network level reading connect
timeouts that was the the first level of
protection that we had for our system
the next one up we could use these
triable semaphores to limit the amount
of concurrency to the backend and this
doesn't this is the entire client
execution not just the the network call
but anything going on in the in the
client the funniest story that I've got
of a client that went bad is that the
client itself and the backend service is
ok but they had erroneously done
something with the XML deserialization
somebody was still using XML and it was
trying to validate the the xst or
whatever and was making a network call
outside of our entire infrastructure to
some like site out on the net some
and that service went down and
completely destroyed that client that's
one of the funnier ones so semaphores
would allow us to reject the concurrency
coming in and then also the network
level with the read and connect timeouts
thread pools give us a little bit more
they give us both the ability to reject
when we hit the concurrency but also the
thread timeouts so that if for those ten
that do get in they're not going to be
they're pinned as long as the network
holds it because for better or for worse
even with Network timeouts we've seen
some rather interesting issues where the
network time house doesn't matter how
they're configured they still don't
actually trigger we've seen some very
odd behavior like in Amazon's with for
at Amazon apparently for security
reasons they change some of the TCP
behavior with like this and this goes
well beyond my understanding of TCP but
it doesn't actually send back the acts
all the time and so that we get these
TCP requests that would just sit out
there for like two three minutes until
the Linux kernel level would kill it and
so those types of things it doesn't
matter what we're setting down there the
timeout it was the only reason we were
protected from that was because we had
that the next level of timeouts and
concurrency constraints so these four
three tools plus the fourth one I'll
talk about are the tools that we put
into our toolkit to be able to go and
start to figure out a solution for for
our problems so we used triable
semaphores for anything that's trusted
and we really don't trust that many and
so we we use this very little we use it
in a few places where something is
almost always an in-memory lookup with
the potential to sometimes go out over
the network we'll use it sometimes there
we use separate threads from basically
everything else so within the Netflix
API stack we have over 45 thread pools
running isolating around 200 different
command pieces of functionality and for
everyone who's out there going how on
earth do you handle that overhead I'll
get to that later
aggressive timeouts so that we can when
something is happening we can clean it
out we can clean out the churn when
something is bad
and then the circuit breaker is the
release valve so I'm gonna walk you
through the flow of this of what the
workflow looks like so we create one of
these command objects we use the command
pattern for it and there are three ways
you can execute it you can execute it
synchronously asynchronously via future
or asynchronously with a callback via an
observable the observable stuff I'm not
going to really talk about much today
but that is we leverage the RX Java
project for that behavior and allows you
to compose and transform on all those
things in a completely non blocking way
all three of these mechanisms though
under the covers they ask two questions
first is the circuit open and do you
actually have resources for me basically
am i being limited or throttled if both
of those things are positive we're
allowed to run and so the run method
that lives within the command is
executed and this is this basically
whatever you would have written without
the the bulkhead around it so this is
your client execution I'll show some
sample code later that demonstrates that
if it's successful the success is
returned just as it always would have
been put within a feedback loop that's
just returning back the metrics that
feeds into the circuit breaker so that
it is just tracking the health of the
system all failure types on the other
hand all go through the same path and
this is pretty important for us we
doesn't matter how the failure occurs we
want to be able to intercept it and
control it and so all four of those
failure types all route through the
first thing they do is they go to a
fallback method and if that fallback is
implemented and it returns successfully
then the user is calling this would see
it as if it was a successful response if
the fallback itself fails then we just
throw the air up and if you don't have a
fallback implemented then you just fail
fast
so I'm going to walk through a little
bit now just how you do some of the
sizing on this stuff and I'll come back
to concrete examples later with some
code to help you understand what I just
showed you a little bit better I want to
now dig into some of the just tuning
aspects is those are some of the most
common questions I get so one of the
first is like how on earth these sighs
these thread bowls well if you were to
take a dependency such as this one that
has a median latency of 200 milliseconds
99th of 299 fifth of 300 milliseconds we
get end up with a thread pull size of
about 10 and it's very simple you take
the 30 requests per second and you
multiply it by the 200 millisecond 99th
percentile you end up with about six
concurrent threads that you need to
handle that now obviously this is
assuming that you have a fairly constant
rate bursty something that's bursty is
different we handle bursts though
typically by wanting to batch those we
we never want to size our system such to
allow very large bursts because then
what we end up doing is we're all we're
doing is we're allocating that many
resources that can fail later and so we
we prefer to migrate anything that is of
a bursty behavior to a batching system
instead and so we then get the six and
then a point order of magnitude is all
it matters well we just round up and we
all of our settings are at like five ten
fifteen and in our system that's the
biggest is 20 and so we have five ten
fifteen and twenty that's about it and
it keeps it simple that way because
order of magnitude is all that matters
here because what we're using this for
is constraints and as long as we're
bigger than what the normal traffic flow
needs is not going to impact normal
throughput and we're only ever going to
hit those limits when we're hitting a
problem a note on queuing so by default
we use synchronous queue so that there
is no queuing as you're handing off to
the next thread there I've seen it's
interesting when you look at link
blocking queue or synchronous queue
they're just slight nuance differences
in how they behave in different systems
so we support both if you are using link
blocking queue don't think that queuing
is cheap or free be
everything you put in a queue is
actually worse than putting into the
thread pool because it's not making any
progress and it will block just the same
way and so if you have a thread pool of
ten and then a queue of a hundred it's
the equivalent of having a thread pool
of 110 with that many of your your
incoming requests are all going to pile
up on it so generally our preference is
no queues at all and but if you are
going to use link walking queue we found
that 0 or 1 doesn't really work you need
somewhere in the 5 to 10 but what that
means is that if you have 10 in your
thread pool and 10 in your queue you
actually are saying you are potentially
going to have up to 20 of your incoming
user threads blocked at the network
level what this might look like is if
you know that you're not your 9999 5th
is 300 milliseconds this example I'm
being very optimistic I'm what I'm
saying here is there's no reason why I
should ever hit that 99 fifth so I'm
gonna start chopping it off early and a
common case in in scenarios like that is
will set the read time out fast on the
first one time it out and then let the
second one take longer and then the
histor it's time out around it allow for
a request a time out a retry and let the
whole thing run the defaults though we
we set the the network timeouts just
high we just let them set high and they
use only the historic level timeout for
it and that hasn't there's no technical
reason for that at all it's purely the
human operation side once you get
thousands of servers and like a hundred
different systems and hundreds of
different clients trying to manage all
those becomes just a nightmare and they
get set and no one ever remembers why
they're set that way and so in general
we said our timeouts higher and rely
upon the history concurrency restraint
constraints instead and then the
historic timeouts are much more visible
in all of our systems and then they will
interrupt the thread underneath you when
they timeout so what's the cost of these
threads the so these are some actual
graphs from our production system at
some point to represent what price we're
paying for you for this extra level of
indirection from the requesting thread
off to the ex-king thread and then back
and so at the median we're actually not
paying any cost here and this is a
fairly high velocity circuit this is one
one thread pool size 210 that's doing 60
RPS on a single box so each of each box
in our cluster be doing 60 RPS for this
guy and at median latency of two so it's
also very low latency one on the left
side here is the amount of time it takes
the executing thread underneath to take
on the right is the time that it the the
observed time from the outside on the
tomcat thread waiting for that to return
its response back so we can track the
diff whatever we're paying in thread
scheduling and handoff at the ninetieth
we start to see a cost we're paying a
three millisecond cost at the 90th
percentile and at a 99th in this one
we're paying nine milliseconds you'll
see that however as we jumped from 2 up
to 29 with 28 on the actual thread time
that the cost is far higher at the
network level that we're paying there
that cost the nine milliseconds though
we judge well worth it for the
protection we get in this case another
example is 75 this one's doing 75 RPS
and the again it's a fairly low latency
one so the meeting it's a one
millisecond Network call being made and
so for 50 percent of our calls were not
having seeing any cost
the 90th percentile we're seeing a two
millisecond cost again at the 99th
percentile we're seeing two milliseconds
and so these costs are things that we
we're aware of them and if you don't
want them you can use some ofourse so
semaphore is this is one that we do that
is 5000 RPS per instance because it's
almost all in memory and in this case
there it it's negligible cost the the
the compared and set that it's doing
under the covers does have some cost
it's not free but effectively it's free
for what we're doing so I'm going to go
through any questions now before I start
to get into use cases yeah
it depends on if you're using hue future
or observe so if you if you execute the
thing directly then it will just block
and wait if you instead queue it up and
get a future back it it will continue
asynchronously until whatever point you
dereference that future future yet so
ultimately just depends on what
programming model you're using in in
ours we have gotten we are doing funny
things with Tomcat now is we're getting
ready to migrate away from that - like
Nettie or something the front end so we
actually have changed our entire top end
of the stat to be asynchronous using rx
and so we do everything in a
non-blocking way and be into preventing
the tomcat thread from running off and
returning an empty egg response we pin
it on a countdown latch waiting for all
the work to be done and then we release
it when we're done just because we
haven't but we're not quite ready to
flip everything to a fully non-blocking
model but within about six months we
should be fully non-blocking at that
front end but it Ultimates whatever you
do with the tomcat thread there's one
over here this one this was over a year
ago so I can't remember but that's not
very it's pretty typical yeah when you
start looking at systems at the
granularity like this it's for the first
couple months after we started looking
at this granularity it was like our
systems were broken
it's like no that's just the jitter
that's always been there you'd never
look at is when you're looking at like
averages of a minute all this stuff goes
away and so we started tracking
everything out the second level and
that's a fairly normal occurrence so I'd
have to who knows in that particular
example could be but when you're
aggregating like a thousand machines
together garbage collection just become
just noise yep I will get to that for
you I think I'll answer it now just in
case I don't actually so the circuit
breaker is really simple we purposely
kept it really basic all it does is
there it's I think there's three
variables it looks at its looking at a
ten-second rolling window and derp
that time period there's a minimum
threshold of request to have to come
through it that we've set at 20
arbitrarily and that if you have more
than 20 requests during that time and
over 50% of them failed again that's
just the default that we arbitrarily set
and it works fine for us all
configurable at that point it closes the
circuit trips the circuit open sorry on
that single box and then every 5 seconds
it will try another request it will
allow one request through it and if
anyone know if it succeeds it closes the
circuit we've looked at like should we
make it more advanced like wait until
you have like a certain number or
successful backlot stuff but we decided
that simplicity was more important in
this case especially because at least
it's scale when you have a thousand
servers all doing this I'll show you
some images later the circuits are
tripping back and forth across the fleet
and so you're kind of getting the it's
at the macro level that you see the
adapt the the adaptive behavior
happening if I was running only one or
two machines I might want a more
sophisticated circuit breaker alright so
the very this absolute simplest command
implementation is this one totally
useless I would never have histories if
this is it but that's the simplest one
you have a run method you execute it
returns and you're done
so anything that would be the client
logic you'd normally have you just stick
it inside the run method so fail-fast
is the default behavior if you don't
have fallback logic you're just going to
fail fast that fail fast happens if run
blows up if it goes late and you hit the
timeout if you hit your concurrency
limit it see you reject or if your
circuit is tripped all of them will do
that will they'll just fail fast and
throw an exception if there is a
fallback though then the easiest way of
doing a fallback is you fail silently
then failing silent is like the way
basically you're turning off that
functionality this only works if your UI
is known to behave that way but failing
silent is a reasonable option if you
have something that you just want to say
when this happens turn it off stop doing
it returning the oil return new option
collections empty lists whatever works
well for you
and we use this in quite a few places
and so you would have behavior on a UI
that went if that back-end system goes
away that UI functionally will just be
hidden yes yes yes we do and I'll talk
about the implications of that in a
little bit
static fallback is another approach we
take where you want to get the real data
but if you don't get it there's some
other behavior I could do the
stereotypical example of Netflix is get
your bookmark so if you've ever watched
a show on Netflix left half way through
or picked up on a different device or
come back later the way that it knows
how to come back and start playing where
you left off is we're we're heart
beating the whole time and then we have
that bookmarked position well when you
come back and play that again we fetch
the bookmark and give it back to the UI
well if we can't get your bookmark it'd
be pretty silly for us to make it so
that you can't use Netflix at all which
is actually how it used to work at one
point that if we couldn't get your
bookmark sorry you can't play that even
though you could start at the beginning
and then fast-forward up to where you
were
we're just gonna completely blow up well
a much better behavior is I can't get
the bookmark so let's just return
default of zero and the user can figure
this one out and they can off stupid
Netflix didn't work this time and go
forward because I know how it works
everyone someone when I see that I'll
just back out restart and then a minute
this time we'll get my bookmark yeah so
stubbed fallback is the next one that
works well is this is one where you've
got parts of the data but you failed to
get the rest of it and so we're able to
create basically it's like a decorator
of the bits and pieces of it and we stub
parts of it and get the real data for
other parts of it so it's just a more
it's a more complicated version of the
previous one and so like here the
failure you know the name alrighty so I
can pass it and I'll give you some some
better examples a bit later with some
real code but works really well for
hello world
fall back via the network so this is one
where if you're gonna if your fallback
needs to do a network call this is
starting to go to your question you do
not want to put inside your fallback
logic another network call because then
that's going to expose you on that
tomcat thread now to protect ourselves
if someone really does something silly
and does that we also do have a
semaphore limit just by default on the
get fallback so that we just across the
system limited to a concurrency limit of
10 so if someone does put Network logic
in there at most we're gonna allow 10
threads to pile up on it but preferably
you'll route through another history
command which then protects the fallback
as well so if you wanted to go off to
another network to get it and so a
common use case of this is if we can't
get the the most fresh data we'll hit an
old cache and but it's sitting in a
needy cache somewhere and then
preferably at that point if you can that
fallback also has a fallback which then
is using something local where you try
and get the real data that doesn't work
try and get something stale or not as
personalized but it's still more
personalized and then the last one is
just give me some global static fallback
or fail silently and so this tiered
approach is using some of our more
complicated ones for example the grid of
movies that we show our first thing that
we do is we go to a system that tries to
get you your personalized list if we
can't get that we fall back to another
one which tries to get a fallback free a
cluster of users with similar tastes as
you if that fails then we just go here's
just some movies go try and watch one of
them
and so it's better than just having the
big old error screen at least you'll get
like the Avengers or something cuz it'll
be like one of the popular ones and so
we probably try and do that tiered
approach we try not to we're still not
perfect yet but that's the idea yes well
fault-tolerant is a fool's errand in its
just if you can't ever like so once you
get into systems of the scale of Netflix
it based there into complex systems
world and so if you start to a study on
complex systems theory the definition of
complex instead of complicated is that
no human or machine can actually
understand everything that's going on in
it and so the the idea of actually
understanding where all failure can
occur is impossible and failure will
emerge just from the interactions of how
the systems are and so we don't even
pretend that we're not gonna fail we
just we're gonna fail yes
yes huh the idea though is that we
hopefully we never actually get to that
point that it's long enough to affect
people's viewing habits but yes the CDN
is fine the CDN is pretty great so I'm
not concerned about it handling it so if
we have a very simple just client like
this this is a blocking call to a client
and this is typical of the type of code
that would take down Netflix a couple
years ago so I want to go get that
bookmark and it freely is not that
important it's not important enough to
fail the entire user experience there's
only like three things that are and I
think they're all related to like
encryption
somewhere along the line everything else
we should be able to degrade in some way
so I want to throw that inside that run
method and this is what the actual
command can look like in this case and
so the way that you get the arguments to
it or via the command power and they're
passed into the constructor and yes
there's boilerplate here we have chosen
to accept that I know that there are
people working the open source one who
want to use annotations and things like
that
we have consciously chosen not to do
that because we want it in your face and
clear and simple and not magical we have
chosen to not have any magic here
because the last thing you want when
you're dealing with an outage is magic
that no one understands and so we want
it so that it's so clear that when
you're stepping through that stack trace
and you want to know what's going on
that you know exactly what line and what
source code and all that stuff so that's
why we've gone this route you can choose
whatever idioms you want so you pass in
the user a movie and then you created
this object and then nothing happens to
it until you actually execute cue it or
observe it and at that point then it
would run it would run in and get that
user movie object and go do its thing
the only config that we force upon
someone to put in there is to define
what group it is and this was a decision
we made I'm sure that there will be
someone who did this
but we decided that we wanted to at
least be able to know how to group these
things and that's because we've got
hundreds of these and so we want people
to at least tell us what classification
is this and we use it by teams so the
different teams within the company they
have their their names there is a more
advanced set of things you can do so
that that team name or group you can
also give it the exact name of the
command we automatically just derived
that from whatever the class name is by
default that you can inject it yourself
the thread pool by default is using the
command group key but again you can get
more nuance with that you might have one
group and then two thread pools like
read and write some teams do that
because the right system is very
different than their reads and so they
want to isolate them differently and
then there's a whole bunch of different
properties that you can override the
most common one those they want to set
the default timeout in there we hardly
ever do this because it just defaults
2,000 milliseconds and then we use our
dynamic properties that are distributed
across or all of our servers we just do
that all using our external system so
all the properties are is dynamic and
change in real time but they can be done
like this as well and then whatever you
set in real time just overrides that but
this is what it normally looks like the
next two interesting parts you would be
implementing are the fallback logic and
so in this case that's what the bookmark
one was that's the thing that makes it
so that I don't kill your user
experience when I just can't move you in
twenty minutes into your show and that's
all it takes and then the cache key what
this is all about is it's very very easy
in a complex app when you have dozens of
different developers all writing things
that they'll all just go fetch the thing
and like trying to get them all to
coordinate on a caching strategy is just
insane and so instead what this does is
within a user request it just maintains
a very short-lived cache if you use this
option to dedupe calls and it does it in
the correct way of catching it before
the network calls happen so that instead
of how I've seen a lot of cache is
implemented two calls come in they both
do the network call then they come back
and it throws
melanne it's like well that wasn't very
helpful and so this one here will cache
the actual future of the network call
and then when the network call comes
back then you both get the response and
so it's a very simple way for the
hundreds of different commands within
our system to dee doop calls which has
actually become a very common occurrence
when you have lots and lots and lots and
lots of developers all writing different
paths through the code on the cache
piece it it's just it's very basically
it's using threat it's a thread local
cache it's a little bit more complicated
than X we can have dozens of thread so
it has to transfer state between threads
so it's a request
it's a request context but think of it
as a thread local cache and that if I on
a single tomcat user request I have two
different code paths that both fetched
in this case bookmark only one of them
for the same argument would go over the
network whereas it used to be that if
two people wrote independent code paths
they would both do the network call yeah
we're not attempting to do across
machine stuff there is absolutely no
reason for us to do that that's we have
UV cached in those things for that yeah
37 million users you can't catch all
that data on a box that's why now what
this is is because so to reduce the
number of Network calls coming in from
the front end or there are web service
calls that become coarser and coarser
and coarser grained and so that means
that because we want to do as much on
the server side as we can before you
ship it off to a device so it means that
we could be making 20 Network calls for
one device request yeah yep and so like
get user for example and everyone asks
for get user like a million times it's
like just fetch the thing once yeah
bookmark you normally shouldn't be
requesting that multiple times but
there's a lot of them like get user
that's very easy to end up with a lot of
calls for it another example
get movies this is like getting the grid
a very simplified version and get the
grid of movies so you go out and get the
movies and your fallback has a normal
cash thing and the fallback here is you
use one of the another command that goes
to do the default and this is the one
that would go to the the network based
system that has like the next level down
of personalized fallback then within
that command itself it goes off and
doesn't network home in it if it fails
then it gets a local snapshot and this
is when you end up with the Avengers on
everyone's screen well ah so now I'm
going to show you is the user command
because this one has a much more
interesting fallback scenario so by
default if I want to fetch a user what
on earth do I do here one like I can't
fail silently on this and I'm just going
to like open the floodgates and so this
one what we do is we actually change the
input arguments and we accept in in the
real one we actually just passed in the
entire HTTP request object in my example
here I'm just passing in the request
cookies because we know that we've got
the the user credentials that came in
and we've put in into those into those
cookies just enough of the important
information that we need to if we can't
get the full user metadata from that
system of like a hundred things on it we
can at least get the three or four that
we need for us to access all the other
systems and so by doing this what we're
able to do is at least if we've gotten
authenticated cookies in the front door
I may not be able to get all your
preferences and metadata and all those
things so I might not get every the the
user experience might be just slightly
different but instead of it completely
failing I can now create a stubbed user
object that generates a user from those
things I got from the tokens and then
defaults for the rest so that I can
continue on this was the single biggest
impact on the availability of our system
doing this prior to this whenever the
user object failed there was nothing we
could do the entire Netflix ecosystem
would fall over and doing this allowed
it so that we could degrade very
gracefully and when the user services
went down
they've become much more resilient sense
but that you it was actually in the
transition period from data center to
cloud when we were finishing that off
there was this weird connection between
the data center and the system always
went down and until it this allowed us
to completely weather those it could be
down for two hours and if if it needed
to be and the rest of Netflix would keep
working and so this was a one of the
more interesting examples of how fall
backs work you have a question
so so it it becomes a lot more
complicated than probably I can talk the
efforts but we do track on every single
system the the metrics of what was in
like basically the failure of states of
everything and then from the API level
down to the device we actually report
the latency in the time at what failure
scenario occurred for every device that
they hit and we use that for trace level
debugging when we're trying to track
down something that's going on on our
device and I can talk to you more
afterwards if you want very simply
here's the three different execution
models you can execute synchronously you
get back type T you can cue it up you
get a future of T you observe it you get
an observable of T those are the three
ways that we execute we are moving more
and more to the third one as we move
more towards that model but all three of
them work so you can use it depending
upon how your programming model is so we
got all that stuff the code is great we
think it's all happy this is when a lot
of our learnings came in the operations
side operations is the big second half
of this whole system that is actually
what has made it so successful is the
tooling and the insights around what's
going on and also the recognition of the
human factor and all this stuff so this
is one one of our views of historical
metrics on a command and up on the top
left there you're seeing a rate of about
40,000 RPS at peak for a single command
we zoom in to logarithmic view on it we
can start to see the chatter at the
bottom of in this case it's timeouts and
thread pull rejections so around 40,000
successes with the time on rejection per
second is kind of what those numbers
mean what we can see here are these
spikes happening obviously there are
some systemic thing ticking along
somewhere that's causing this to happen
probably what this is actually is waves
of autoscale events because of when when
they're happening here it says we're
scaling up and we ought to scale up as
new machines are coming online
they're always jittery for the first
minute or two
warming up all their network connections
and so we see these bursts of errors
just as each scale that happens well we
can see in the next graph is right in
lockstep the fallbacks happening with it
so you have a suspect's of fallbacks
happening with the spikes of errors down
here we track our Layton sees we
actually track every 5th percentile zero
through hundredth there's 23 of them in
arcs we do 99 999 fifth as well and on
this graph I'm only showing five of them
and we also track the exception thrown 7
4 permits in use so the concurrency
request cashing so we can actually see
how much how many duplicate calls are
being made so we see how effective that
is and the calling thread light and say
CC the outside view in yes we built our
own because we blew up the one that we
were using before there is intention on
open sourcing at some point it's a big
beast to get out though so there's they
haven't given me a timeline on that it
was a fork of the whatever that are deep
whatever the that thing might just
escape me in the con party yes whatever
yeah it's not that one maybe I'll come
to me later
but it long ago we forked off of that
and start customizing for our own and
then a couple years ago we outgrew that
and they've rebuilt another one that is
architected to handle the tens of
billions of metrics a day that we shove
into it so no that's not even close to
No
no we're not no it wasn't no they aren't
anywhere from 2 to 20 tons yep so from
the historical view the historical was
nice but what we found is when we were
operating it just wasn't good enough
well when everything's healthy
historical is great but when you're
actually dealing with the production
event either an outage and alert or
you're pushing new code or whatever we
need to have a much faster turnaround on
decisions because it used to be that
you're ticking along at one data point a
minute that's like 2 or 3 minutes late
and so you do anything and you want to
see at least a few data points to see
you're five six seven minutes behind you
flip a property somewhere and then like
seven minutes later like I have four
data points yeah we're good or not and
we were really struggling with that and
it would really increase the mean time
to discovery and recovery in production
operations so we built this instead and
this is one second latency of all the
metrics from the system and so this is
being aggregated from all the servers at
the API level this particular snapshot
was only 379 boxes and I'll take you
through what this does for us so a given
bulkhead tells us that this the size and
color of the circle is telling us the
volume of relative volume of traffic and
health the gradient through green yellow
orange red for the error percentage the
sparkline is telling us the last two
minutes of traffic volume the hosts
reporting so you just have a sense of is
this actually reporting to me how many
instances I expect the last minute
latency percentiles whether or not the
circuit is open or closed request rate
and then the error percentage of the
last ten seconds and this becomes an
artificial metric that we can judge the
health of any one of our dependencies on
and each of those individual numbers are
the successes short circuited thread
timeouts thread pull rejections and
failure exceptions underneath so when
something does go bad we can see what
type it is
all of our metrics are captured in a
ten-second rolling window we can change
the size of it for us 10 seconds is
plenty is quite a long time and so that
10-second window is just ticking along
it one second increments the lat
and the last bucket drops off a new
bucket comes in and so all the numbers
that were spitting out are just from
that rolling window the we pump the the
metrics through a system called turbine
we're actually looking to replace with a
new one that we're building in turbine
basically holds open connections to all
the servers with just a constant stream
of data coming down aggregates them
together and then spits that out we also
have we've had this one for a long time
in Netflix but it's low latency
configuration changes so when you have
these two together what it allows us to
do is you see something and that data is
seconds old you make a change within
seconds you see it propagate across and
then the the feedback loop of making and
seeing changes in how they behave on the
system are now measured in seconds
instead of minutes one of the more
interesting ways we test this is we
audit things we simulate failure and
production so one of the most useful
ones is latency simulation and so we'll
take if the tool it's called latency
monkey keeping with the whole simian
theme of that group within Netflix but
this is an example I remember this one
vividly because it was just like the
perfect use case I was sitting right
next to the guy who's flipping the
switch and I'm like okay we're injecting
latency now and so we saw this guy light
up and so it's circuit is completely
tripped it's all it's all in fallback
mode but Apple TV are dependent we also
track of our incoming routes in the same
way and so I can see inbound and
outbound on the same dashboard and in
this particular one it was the Apple TV
device actually we broke it like bad and
so within seconds I see Apple TV shoot
to the top completely 100% failure I
turned the guy I'm like we got to turn
this off now so he flips it off and
seconds later everything's back
and so this is an example of where
having just the halving of the code it
by itself was not enough having insights
and tooling around it was equally
important so that when we were
exercising how that code was behaving we
realized that there was something about
the appletv implementation that was not
handling our fallback state correctly it
didn't affect any other devices but that
one we fixed the the issue in the Apple
TV and then two weeks later redid this
exercise and we were able to simulate
the latency that the fallbacks did their
job and then all of our clients handle
it yeah test doesn't work so here's
another example this is a real
production one where the latency spiked
from 125 milliseconds to 1500 plus for
that same time period we see the
fallbacks spike up to 5,000 per second
and during that time period the
exceptions thrown around one and so we
can see here that the fall backs are
handling and we're not throwing
exceptions to our users another thing
that we use hystrix for is part of our
continuous build pipeline and it's not
for the runtime behavior it's for the
the metrics out of it and so when we
every time we're preparing to launch new
code we run a canary with it and we run
it for a period of time and it
milestones along the way we're taking a
snapshot of the metrics and comparing
them against each other so that we can
analyze the difference in both error
rates and performance metrics across all
of our systems we also squeeze tests
well we're take one box out and then
incrementally load up traffic on it
until we hit its breaking point and see
how they behave differently and then we
actually used that data we feed that
into our auto scaling so that we can
change our auto scaling per logic for
every push based upon what our psi box
can handle before it falls over we also
have another cluster that we call a
coalmine like carrot canary in the
coalmine type thing the difference
between this one in the normal canary is
this one is a carry that runs for the
entire lifetime of the
that deployment the reason why we do
that is because we have lots of ways
that you can dynamically change the
behavior of an app fast properties a/b
tests all these different things that
can open up new code paths or someone
else could deploy a new server-side code
that changes things and so what this
guy's job is is he's just sitting there
watching all network traffic fly by and
he's watching to see if it's being
isolated by hystrix or not and if all of
a sudden a new code path opens up that
starts making network traffic that is
not within a historic bulkhead it starts
waving its arms around and calls our
attention to it so that we can see that
someone just changed a property
somewhere that opened up a code path
that is making a network call that is
unprotected and it helps us to find
those things in production before they
become problems and so in effect
everything that yeah this is sorry this
is this is an example showing one item
that's not bulkheaded and that's what
the coalmine is doing is tracking those
down so one failure happens this is kind
of what it looks like in this case the
social service failed and the
interesting thing here is that you see
that the circuit it wasn't a hundred
percent failure and so in these kind of
scenarios you can start to see the
cluster just adapt itself it's just or
organically and it's kind of interesting
and watches it just kind of ebbs and
flows back and forth because the circuit
will trip on some that releases the
pressure so that others start to succeed
and then it kind of like pushes over a
limit again and so those ones will trip
and then the other ones close back up
and so just go to the circus trip and
open across the cluster is back and
forth this is another one that failed
was it a twenty percent failure rate and
you can see their failure rate spike on
the historical metrics as well and you
can see the fall backs kick in on the
historical so I guess the the principles
that that we have really learned to
adopt or to stop pretending that we
understand what's going on when you have
thousands of servers all doing billions
of network calls a day and code
constantly changing there's always some
deployment going on at some system
somewhere
you just can't pretend to know what's
going on and so we have to start to
think of these as complex systems I've
heard some of the books on this on the
subject they talked about like a Boeing
747 now when it's sitting on the ground
in the warehouse it's a complicated
system an engineer somewhere actually
engineer every single component in that
thing and they knew how to pull it all
together as soon as you turn it on and
take off it is now a complex system that
you have no idea how it's going to
behave because a you have software
running on the thing now and B you have
got all the the the influence of the air
and the environment and the human
operating it and the study of airline
crashes or aircraft crashes are quite
interesting in that space nASA has
similar things it's just now that our
industry is starting to look at this and
start to say I think that actually
starts to apply to us so we've done a
lot in the last couple years of starting
to look at those and realize that we
have to look at our systems in similar
ways the principle of isolating
relationships is a big deal this has
been the single biggest thing that we've
done that has improved our resilience to
failure and that doesn't mean that we
try and eliminate failure we do that as
much as we can obviously we want to
write good code and good systems and all
those types of things but it doesn't
matter how good you write your system
something out of your control will cause
it to fail or just bugs that exist in
all of our code and then the last bit is
that auditing your resilience to failure
and the human operational side are key
most of our failures nowadays are not
because of the the types of things where
one system goes down most of our
failures now are because of human error
there was an interesting one earlier
this year where it was a developer
pushed a property change and targeted
the wrong cluster they thought they were
targeting the canary cluster and instead
targeted the production cluster that in
and of itself wasn't a huge deal except
that it caused a huge latency spike on
that machine on that cluster because it
was taking pumping traffic through a
very Compu to intensive code path that
in turn triggered all of our alerts and
we would have been okay
if we had just reverted it easily but
then human operator knee-jerk reacted
and instead of letting the system do its
job and shedload open the floodgates and
let it saturate all the resources and
then it became instead of a two minute
outage II became like a 30 minute outage
and so we're now starting to spend a lot
more of our time figuring out how do we
make the Machine human interaction
better so the humans trust what the
system is doing and the system can give
data back so the human operators have a
hope in actually operating it correctly
when things do go bad I hope this has
helped there's a lot more information of
these various links and I'll post these
slides on my Twitter account and on up
on speaker deck thanks for coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>