<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How Not to Measure Latency | Coder Coacher - Coaching Coders</title><meta content="How Not to Measure Latency - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How Not to Measure Latency</b></h2><h5 class="post__date">2015-06-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/DxF077s081Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so this is a subject I've actually been
developing a talk for for almost a year
now I think close to a year and and even
though the material talks about the same
subject I keep gathering more and more
information for it and hopefully better
ways to explain it as you can tell from
the subject this is really an attempt to
teach you about the mistakes I've made
and that I've seen other people make I'm
pretty sure all of you know exactly how
to measure stuff right but I'm going to
tell you what other people do wrong
sometimes okay so sometimes it could
prevent you from doing something wrong
there I'm not sure how the feedback
system works in Java one this is the
first session that I've been to because
you know that's actually like this so if
it makes any sense or matters that's the
session ID up there and obviously should
vote for this being a really good
session I mean do this now because
afterwards you might change your mind
but this is what we're going to try and
cover roughly this is what I'm planning
to do you know talk about some
background about latency behaviors and
how people think about them talk about
the use of what I call statistics to
describe them and we'll go through some
examples there and I'll get into some
philosophical questions of why it is we
measure latency what it is we should be
looking for and what matters and what
matters changes or differs by what you
care about we'll get into a problem that
I see everywhere I look which I go
coordinated a mission and it's starting
to be pretty well documented and its
really in your way if you're measuring
latency and we'll talk about some useful
tools both for measuring layton Caesar
for trying to overcome coordinate
intermission in your measurements and
last but not least I'll use those tools
to do a little bit of bragging so bear
with me I'll break it down a little bit
in the middle too so just a little about
me I'm the CTO and one of the cofounders
of Azul systems at azul you know I work
on all kinds of things but finally
the thing I'm pretty well known for is
working on garbage collection and
interesting different ways of doing
garbage collection I've created a couple
of real garbage collectors but this is
some evidence of me working on garbage
collection all the way back in 2004 that
is an actual trash compactor and it's
got fragments falling out the back for
those of you though but I've also worked
on other things everything from helping
design CPUs and instruction sets to
drivers to operating system kernel to
virtual machines of all kinds two
routers and switches and firewalls and
telco systems that manage millions of
subscribers with java based servers we
had to build on her own because there
were no app servers in 1998 you could
use and all the way to here I've been
working on java virtual machines for a
decade now I'm also one of the members
of the JCP Executive Committee something
i usually don't raise too often but
since we're running for election this
year um please vote for me okay a little
about Azul we make scalable virtual
machines that's the only thing we do and
we're fairly focused on it we've had the
approach of doing whatever it takes to
build those Scobell virtual machines for
a while and whatever it takes meant some
interesting crazy things including
building our own Hardware our own CPUs
and on and who connects and a lot of
really cool stuff in there we built
three generations of this stuff which
what you're looking at is an 864 way
symmetric multiprocessor 50 course 54
cores on each chip full mesh
interconnect pretty cool stuff but this
is what we use to design about four and
a half almost five years ago and we
looked at the Intel and AMD roadmaps we
said you know they're getting pretty
good and we don't have to build our own
hardware anymore so we spent the time
hmm somebody's talking over there so
some we spent the time to convert what
we did to pure software to to x86 Linux
environments and that's what zing
zing is our current product line it
basically it runs on x86 servers
commodity servers from any of the
vendors on top of Linux platforms we're
pretty well known for low latency and
and the ability to run any size he from
a gigabyte to a lot in four very
consistent and resilient execution but
enough about us let's talk about common
fallacies that lead to this talk so
these are things that people know are
wrong probably but intuitively assume
and how they measure things and I like
to highlight them the first one is that
computers and applications run
continuously that unless something
really bad happens this thing won't stop
and that's the opposite of true computer
stop all the time they stop for
interrupts they stop for power savings
they stop the context which they stock
because the hypervisor took away your
cycles they thought they stopped because
of garbage collection they stopped
because of I don't know what but they
stopped non-stop and it's important to
understand that that's a normal behavior
that happens all the time it's not rare
it's not something you can assume won't
happen if you tune right there's some a
level of stoppage between sub
microsecond levels to multi second
levels that exists very normally in
these systems the other one is that
response time can be measured basically
by taking work and dividing it by time
and this is a very common thing I run
into so that's another belief that
there's some sort of a normal
distribution this is what I call a
wishful thinking assumption that there's
an assumption that because we'd like to
be able to model what behavior like is
in our mind that the world needs to
behave that way or probably does behave
that when there's some nice kind of
statistical distribution like we learned
in school and that there's a way to
describe latency using those tools and
then there's an implicit assumption that
glitches hiccups stoppages little things
that are rare they only happen once in a
while don't really affect your results
if you don't do well there the rest of
the results reflects
thing about your system that might teach
you something every one of these things
is wrong and demonstrably wrong I'll
hopefully be able to demonstrate these
to you but let's talk a little about how
people have looked at latency and
response time for a while and this is a
good example of wishful thinking this
graph is about actually I'm not sure if
it's 40 or 45 years old but it's from a
son IBM kick server manual okay it's
it's old and it shows how modeling
response as a function of load works
right so we all intuitively know that
this kind of works and it does kind of
work there's there's some response time
to your system and as you increase the
load it's still ok and then at some
point a strange system starts working
harder and harder in a temple and it
just gets painful enough and we know
there's some wall that you can't do more
than but but what do we mean when we say
you know response time are we talking
about the average or the max or the 99th
percentile and are those things really a
function of load again there is probably
some effective load on that but there's
an implicit assumption and drawing this
graph that that is the primary if that
is the thing that primarily affects
behavior so let's look at that with some
data and by the way some of these graphs
I just went to the internet and I type
you know Google kind of response time
graphs and I grab what people had and
then I I stole it and commented on it I
do credit them at the bottom yeah so
this is another graph i went and just
looked up and this is an actual you know
test under j varying load of an actual
system and there's some interesting
things to see here and by the way i
don't actually know what the system does
or how it works or anything i'm just
looking at the output this is an average
response time in a period of time that's
a very common thing you'll see plot it
but notice that this light blue line
going from the bottom left to the top
right that's the low that's user count
growing through the test
this black line with the spikes that's
the response time averaged over time and
these things are clearly not a function
of load there's a lot of them at the low
load they're as big as more they're
bigger than some of the times that the
high load clearly something other than
node is affecting our time and well you
could look at the bottom of the black
graph and say that seems to follow the
load because it does it grows that's
clearly not the dominant effect on time
right load is affecting time but
something else is affecting it a lot
more time is affecting time so for
example there's probably some periodic
work that's going on here are some noise
or something and and this is very common
when you look at systems that that
gather up things that need to be done
later where you eventually have to pay
the piper to reindex your database to
flush your buffers to garbage collect
whatever it is that you're doing that
happens once in a while that tends to
look like this I call these things
hiccups you can call them by any other
name I've been calling them hiccups for
a while and hiccups are just a part of
life a response time and latency here's
another graph with some hiccups in it so
anybody want to guess how I got these
hiccups so this is me going to the
keyboard and pressing ctrl Z counting to
10 and letting it go you could get
hiccups in lots of different ways the
real point in what I'm saying here is
you probably want to see what the system
behaves like including these not
assuming they don't exist so how do
people deal with hiccups well let's
actually look at some real world one
example before we stop into that this is
measured with it or I'll talk about
later it's called Jacob and it's from an
actual low latency trading system so
most human applications we would love a
behavior like this because that line up
there is 27 milliseconds but the thing
to note here is the 99th percent of this
system happened to be 60 microseconds
just pretty good trading some trading
folks life to do much better than that
but that
a very good number and that's the 99th
percentile and if we just reported that
not even the average we could go home
happy right but this red line represents
the maximum in NH five-second interval
and as you can see there are a lot of
these maximums that happen and there are
so much bigger than the mean or the
average or the 99th percentile they are
thirty thousand percent bigger I like to
point out that people often talk about
jitter when they talk about latency this
is not jitter calling this jitter is
like calling cardiac arrest missing a
heartbeat you did miss a heartbeat
there's just no more heartbeats after
that or for a long time right the gap
here by the way is that ratio between
missing a heartbeat and 10 minutes of
brain-dead activity so when you have
those ratios you're talking about
something that's not jitter that's not
just noise that's not a little bit of
distribution of your stuff and this is
the real world hiccups happen so to
understand the key behavior of them is
that it's been said that things are very
multimodal in the world we usually don't
have a nice spread of noise affecting
our stuff we just have multiple modes of
operations and the multiple boats of
operations usually look like a complete
shift from working in one way to another
way and then working back in other way
usually most systems have a good mode
that's that common mode the mean mode
the median mode the 90th percentile mode
but then they have a bad mode or a
little bit of bad mode and then I have a
much worse mode and if you're unlucky
there are three or four of these modes
but they exist most software systems
have at least two or three modes to them
and understanding that that's how the
world really works is a first step in
understanding on how to measure it so
what do people do about this hiccups
exist they're all over the place and
they're strongly multimodal they're the
opposite of standard distribution so
what they what you usually do about it
is ignore it
okay I mean you don't like that picture
it's hard to describe so let's not let's
not plot those pics and max let's just
do the mean let's just do the the 99th
percentile and go home let's not talk
about the dirty laundry so let me give
you instructions on how to do that okay
you take a shovel and then you measure
the standard deviation of the shovel
because that that is what standard
deviation is really good for if you
really want to hide from your data then
take your data set and lunge it down
into exactly two numbers a mean and a
standard deviation and by doing that you
express you what you wish the world
looked like and try and paint that
wishful world with a little bit of data
you happen to have around and then you
describe that world to everybody else
and yourself five minutes after you've
stopped looking at those bad clutches
and at that point you believe there's no
problem and you could go home now this
is not just a philosophical discussion
I've actually had this personal
experience where I often when I talk to
our customers and I talk about their
system behaviors because we help solve
glitches and latency problems I try and
see what data they already have and what
the requirements are so I was talking to
our customers said I have an average of
this and I've got a 98 percentile done
95th percentile that night and I like to
ask if they have more data but not to
directly so I asked if they can have if
they have any other percentiles can you
give me the ninety-nine percent though I
usually ask this because if they can
that usually means they have a log of
all the results and they can sort it and
process it and they've got good somewhat
good data so guy says yeah I can get
that for you goes away for 20 seconds
comes back with the 99th percentile now
at this point I know he's got all the
results and I could go analyze them so I
asked for the results you know can I
have the log file you look that he said
I don't have a log file last how did you
get in your 99th percentile I said well
I have the average and I have the
standard deviation and I know that three
standard deviations from the average I
have the 99.7 percentile that's pretty
close and that's how he got it right now
that's applying wishful thinking instead
of measuring
so that's not how that's how dr. measure
so what you can do a little behind it be
a little better than that as a step in
the right direction is to try and
measure and actually look at the data
across the wide spectrum this is a good
example of how to do that I like to plot
percentiles not as a data point or a
data point by this entire spectrum look
at the entire percentile range not only
will the numbers tell you what's going
on in the number of places and you can
compare them to actual requirements if
you look at at this kind of graph but
the shape is very informative and as we
go through the rest of the slides you'll
see examples wherein the shape itself
you can detect the problem this by the
way is a good shape for a three-mode
system it's got a good mode it's got a
somewhat bad mode and it's got a really
bad mode and it's a fairly good shape
because it's smooth real-mode shifts on
a graph like this look like smooth
transitions no spikes no bends no
corners if the dark one is usually
there's a methodology problem in
measuring and I'll give you examples so
let's talk about these requirements for
a second and why is it that we measure
things what do we need to actually
measure in what what do we want to
report on so latency basically tells us
how thing how long something took and we
want to know that but when you ask
somebody what they want latency to
behave like they say well I want it to
be fast and when they say well a cry
every time or sometimes they say yeah
every time later when you say i want
requirements what you're actually asking
is kind of what are you willing to pay
for what are you willing to invest your
five developers in can I you know tell
me what you actually need not what you
wish the world looked like and and you
need to decide what the behavior looks
like is saying I wanted to always be the
same maybe practical if you're willing
to spend the money on it but it's
usually not especially not in software
so usually you want to define some set
of requirements and they should reflect
the business needs not some technical
wishes and there's usually a simple
pass/fail does it do what I needed or
does
not do what I needed does it do it when
I'm carrying the load or not do it when
I'm carrying the load carrying a load
without actually meeting requirements is
not very useful for business different
applications have very very different
needs so you want to capture the needs
the actual application and business has
and we'll go through some examples here
and once you know what the needs are
then you want to make sure your
measurements your experiments are
actually measuring those metrics not
some other metrics trying to project
what the requirement metrics would look
like because that's what standard
deviation is all about if there's one
thing you walk away from here by the way
with is never ever ever ever ever use
standard deviation with latency in the
same page or if it is there please erase
it you know if somebody else put it or
at least hide your eyes from it so
measurements should actually provide the
data that you need to determine whether
or not your requirements are met other
kind of measurements are may be fun
scientific experiments but they're
usually rat holes so let's go through
some actual examples of applications
different motivations that have very
different needs for latency I like to
classify them and the first one I use as
a classification is the Olympics this is
the ring the bell first I need to win
application very common in simple
trading simple trading that probably
doesn't exist anymore but this is the
first guy there got it and nobody else
matters right and I'm assuming the
Olympics with only gold medals okay
silver and bronze don't count so your
goal here is to win some gold medals and
in order to win some gold medals you
need to be faster than everybody else in
some racist that's the actual goal
that's what we'll actually do the
business need it's okay to be slower in
some races it's okay to even not run in
some races it doesn't matter as long as
you win some or you meet your goal and
there are various strategies you could
do to try and win a gold medal other
than actually being really really good
that's basically a requirement you can
say hey I'm really good at some things
I'll focus just on those races and I'll
try and beat everybody else or you could
say I have
I've trained in a lot of things I've got
a lot of stamina I'll try I'll plane 11
races hope to win three or two or one
and spread your like around these are
strategies right again look at trading
and you'll see those applied so that's
one category it's it's an interesting
category but usually in a fairly narrow
sector right not common to see this in a
web store okay if you're not trying to
win that one customer trying to win a
lot of customers another example is
pacemakers these are classic hard
real-time applications and we can
understand their motivations very
intuitively okay in a hard pacer your
goal is to keep your heart beating all
the time okay it has to be better than X
and it doesn't you don't get any scores
for Aggie for for being even better than
X right I don't need my heart beating
three hundred times a second it doesn't
help me that you can now it's very
important to understand that telling
somebody that ninety-nine point nine
percent of the time their heart will
work right is very very you know it's
not very assuring okay it's probably not
what I would want in one so you know all
these other metrics don't matter there
is exactly one metric that matters and
that is the worst case and the max and
the barrier okay nothing else matters
and you better be reliable on that
that's what hard real-time is about by
the way real-time true in real time is
not about speed it's about slow and
steady now steady being the keyboard
okay now let's try and mix some real
time with a little more aggressive
behaviors low latency trading which is a
world we live and breathe in right now
it is a common example of a soft
real-time I really don't like the tone
of the name soft real-time because real
time is real time and I spent five years
spending hard time in real time but but
there is such a category as a lot of
people calling low latency trading is a
good example of that where you're trying
to be pretty fast you want to win some
stuff but consistency also matters okay
because there's a cost to inconsistency
in consistency
in low latency trading means risk most
rating systems these days do not amount
to oh look there's a good price there
let's grab it they're usually something
like look if I buy this and I sell that
and I move this over there right now
there's an opportunity to make a little
money right now however if I grab a
position in this and I fail to release
the position here I might lose a lot of
money so it's not just how fast but how
consistently fast and whether you're
making a bet that you could do three
things in an hour window of time not one
and you don't want to do any if you
can't do all three and unfortunately the
market is not atomic so you're all about
speed and consistency of speed and how
fast the market shifts so that's these
are typical things I typically want to
be within 200 microseconds 20 micro
second pick your number and I can't
afford things bigger than X you know 20
millisecond 30 million second is typical
here and in that world you basically say
you know I want my typical behavior to
be a certain number typical being median
or average and and I want you know
really good max is really good 99.99
percentile kind of numbers okay that's a
typical soft real-time behavior and then
we have things like interactive
applications I call those squishy
real-time people are squishy so here
your goal is to keep people happy and it
doesn't necessarily be need to be truly
happy you just want to keep them from
complaining and and you know that
usually means good snappy behavior most
of the time what snappy is changes over
time right today's applications are much
more interactive to ten years ago or
what's acceptable is different but it's
okay to sometimes not really work well
it's okay to sometimes delay sometimes
glitch once in a while some number is
okay you can't go to the business and
say I need a worst case of 30
milliseconds they get the money for that
if you're ticking with humans it's just
not not a not a true business goal for
that so typically you would describe it
as some sort of a graded percentile
Irv you'll have I want this percentile
to be better than X I want a higher
percentile to be better than Y and I
want to maximum that's no worse than
something else and you need to be
reasonable here you know I know you
don't like a 10 second pause but maybe
it's ok to have that once a day you need
to ask whether or not it's ok I don't
know the answer it depends on your
business so you have to remember by the
way when you create these requirements
for human applications that a human
interacts with your systems a lot one
web page is often tens of connections
and requests one session is usually
hundreds which means that you're 99.9
percentile is finally exposed to about
one in three users so when you ask the
business you got to be really careful
who you're talking about I you you
talked about ninety-nine point nine
percent of users or ninety nine percent
point nine percent of images that I've
sent right now okay so we talked about
types of applications and motivations
for them let's talk about establishing
the actual requirement and this is a
hard thing I've been doing it with
customers for a while that so I know how
hard it is and it's usually hard within
any business you want to know what's
actually needed and nobody will actually
tell you so how do you get this
information out of them since they
haven't thought it through necessarily
sometimes they have because it's written
in a contract but most of the time it's
hard to eat get out especially for a new
system so that I've written down some
interview questions here or walk through
an interview and I'll hopefully people
can read this little text and i'll walk
through what I've done and how this
works and i highly recommend that you
guys use this to extract internally what
the actual needs of the system are the
first thing is okay so what do you need
your latency to behave like what's your
actual requirement and the typical thing
is I want it fast I needed to be you
know 20 millisecond on the average or
200 millisecond average pick whatever
they say there and usually my response
to that is when they stood average right
there I know there's a big problem
because I've never met a business with a
bist that actually has a need for an
average number the only time you have a
need for an average number is because
you've committed that average
number to another business that didn't
have a need for an average number or
maybe they have a contract with somebody
but that's the only time with this
actually makes sense because it's really
hard to get people say well what do you
mean is I mean I'm doing a million
things is it okay to spread like this
right you know an average really makes
no sense but usually when people say
this they mean typical they mean I like
this a median and average ER in people's
minds similar though numerically they're
very different so I'll say okay
typically you want that but you know
what's your worst case what can you take
for worst-case and the usual answer is I
don't know we haven't thought that
through we don't care so you need this
number because this is probably the
number one most important number you
need to understand in order to
characterize latency what is actually
needed what is the worst case and when
they don't tell you here's a technique
for getting it out of them I do this a
lot now I say you know so it's okay for
the system to stop for five hours every
once in a while right and they all
immediately say no right unless they're
running Hadoop but usually they'll say
no okay and at that point I say okay let
me write this down customer said worse
case no worse than five hours okay I'm
taking I'm playing it back to them and
at that point they'll go aggressive and
they'll say no no no no no no no I want
that worst case to never be worse than
100 milliseconds and at this point you
need to reverse the negotiation
direction because they will do this
they'll go very aggressive I want it all
the time I want it perfect and this is
where you say come on this is a human
response time system you don't need this
what do you really need and they'll
think about it will come up with an
actual reasonable thing like two seconds
or five seconds or whatever number they
actually need right at this point when
you extracted this number from them
you're not done unfortunately because
all you have is a typical and in a max
and no idea what the shape between it
needs to be like you need that some some
level of that so now you ask them you
know so how often is it okay to be you
know less than you know what was it so
okay that you do nothing worse than two
seconds and then you say
how often is it okay to be some other
number right like one second and at this
point they usually get angry because
this is like I didn't want to talk about
I wouldn't want to think about it now I
think I've already answered the question
and and you need to explain that you
said okay we said that it will never be
worse than two seconds but that means I
can be 1.9 second 5,000 times a day are
you okay with that and at that point
usually they'll say okay let me think
about this and start thinking of of a
bunch of other percentiles and levels
and you will end up with something that
looks somewhat like that this should be
your goal for the interview okay
extracting i would say at least two
steps hopefully three from whoever
you're interviewing a typical a max and
at least one percentile point that
reading matters in the middle but
probably more than one and i will note
that if that doesn't have numbers on the
right side of the decimal you're kidding
yourself 90 percentile it's so a
favorite example of mine is spec
benchmarks they usually use a 90
percentile requirement with no max
requirement and i just turn around and
ask most of the people who use those
benchmarks does that mean you're okay
with ten percent of your application
transactions actually failing is that
okay most of them will say no but that's
what the requirements said if you say
ninety percent oh so you want to get the
max you want to get some good percentile
numbers on top of it and i'll show you
how important that is in a second they
need to understand latency doesn't live
in a vacuum it does depend to some
degree unload and you probably remember
this picture i put up front looking at
load and capacity you know load-bearing
capacity there is a relationship between
what you're loading a system with and
what latency will happen because every
system has some limit and if you try and
ask it to do more than it can everything
will wait in line if you're lucky and
probably even worse but if you have a
graph like this what is the capacity for
the system and that depends on who you
are the marketing benchmark is going to
say look how much we can carry
we did this many transactions forget
about that latency we didn't talk about
that but look my TPS is so right that's
most benchmarks ok now the sysadmin is
going to worry about when users are
going to complain right so you're going
to say that's unacceptable that's where
they will actually pick up the phone and
if they pick up the phone too often I
won't get a bonus so I need to sit here
that's what Headroom is all about it's
about getting the SIS adminius ok but
that's that's good because that means
he's aligned with the actual business
needs of not getting users the complaint
now the actual capacity of the system is
somewhere around here this is the
sustainable throughput that the system
can handle without the users complaining
how much it can handle while the users
complain is completely irrelevant to the
business ok and so you when you're
actually trying to figure out system
capacity you are doing it in London see
context or when you're measuring latency
it's within some throughput context
usually because you know what an idle
system has really good latency and one
that does one transaction an hour
probably has pretty good latency again
unless you're running Hadoop but but you
need to understand the context here and
to define what i mean by sustainable
Layton's the three put that is the
throughput that you could achieve while
safely with some headroom sustaining
your service levels completely not
partially but you pass requirements not
for two minutes a day but for the entire
day now this is an example of
unsustainable throughput I mean the
makers of this car will tell you how
fast it can run they don't tell you how
fast it can run without hitting a light
pole and say how fast it can run but
what you actually want to measure is how
fast you can drive this thing without
hitting light poles that's what the
requirement well should be so that's
unsustainable three point now let's look
at a few ways to measure and compare
things for that so this is the format I
often used to display latency and
throughput against each other
this is multiple scenarios the
percentile curve for each completely
plotted and each one of these is just
scenario ABCD and you choose what you
want to understand out of that right and
usually you can slap a requirement on it
so for example this is an example of
comparing different systems different
configuration at different throughputs
against the requirement that red line is
the pass/fail criteria you need to be
five milliseconds or better ninety
percent at a time this is a messaging
system you need to be ten milliseconds
or better ninety-nine point nine percent
of time and you shall never pass the 20
millisecond mark in my cool low latency
messaging system ok that's the
requirement now notice that you can pass
it or you can feel it depending on for
example how much load you put on it or
maybe what product you use to carry that
load this is a good example of how we
try to depict value another way to look
at it not in low latency is a human
squishy response time this is a portal
against an SLA how much can I run
without cracking through that SLA and
that's as far as we could run in that
specific portal system we tune the heap
we tune the number of users and you can
see that we're just about to crack the
requirements but pass if we push more
users it cracks if we give it a bigger
heap it cracks if we give it a smaller
heap it cracks that's that's after
tuning and then you know obviously I use
this to compare and brag this is what
zing looks like on the same thing same
system same hardware same everything but
you know carrying about 17 times as many
users in a sustained basis because I
don't crack through the response time
notice that both systems would have been
able to carry those 800 users but one of
them does not carry it in a sustained
basis and pass requirements and the way
we do this is by cheating we we use you
know the memory in the machine 50
gigabyte heaps to sustain 3.2 gigabytes
per second of allocation right here but
the fact that it's possible to do it is
what we're measuring and how much can
you carry on this is the question
okay so we talked about backgrounds and
philosophy and some examples of plotting
things let's talk about coordinated a
mission this is a problem that is
unfortunately very common at this point
you might understand the joke I think it
exists in 99 percent of the cases I look
at and I call it an accidental
conspiracy because nobody actually means
to do it but everybody does so what this
Gordon intermission looked like this is
an example of how to do it but you have
seen it in measurement for monitoring
for low generation for everything
usually it goes something like this you
have a measurement system maybe you
bought a load generator in the mo
generator issues request to a system and
measures the response time or maybe
you're monitoring within a system
recording what each transaction did it
took and you collect statistics on that
and then later you report on those
statistics and you know what is the
problem with that the problem is that
most serial operations that go request
wait for response record requests wait
for response record do not do the next
request until the response comes back
and that works fine as long as the
response doesn't take come back after
you were supposed to issue the next
request according to whatever plan you
were supposed to do or what the real
world tried to do now at that point when
the request comes back later and you
didn't issued a request you skipped a
little time right and it's a little bit
come on I mean it's not going to happen
a lot most of the time you know it's not
going to happen in a few times but a
very small percent of the time you know
you're going to see these glitches right
how bad can this get so let's walk
through a hypothetical scenario to
explain how bad it can get and then I'll
show you some actual examples from the
real world now this is how you measure
percentiles in a system and I'll go
through the naive measurement technique
which is what most low generators do
imagine a system that's perfect it's
serving a hundred some things a second
every single one of them comes back and
precisely one millisecond it's probably
concurrent or it doesn't it doesn't
really matter but it's always good
except that I go and I stop it 100
seconds then I stop it for a hundred
seconds I just completely stall it I hit
ctrl Z ok let's not even care about why
it's stopped now let's describe to our
boss to our customer to ourselves what
the latency characteristics of the
system if the system is in a way that we
probably will all agree on so let's
let's do a little bit of simple math ok
on the left-hand side we have a hundred
seconds with one millisecond everything
average max everything right so the
average on the left-hand side is 1
milliseconds now on the right side we
have a hundred seconds of doing
absolutely nothing if I came in at a
random point during the 100 seconds
it'll average 50 right if I come at the
beginning of a wait a hundred seconds if
I come in towards an almost nothing in
the average it'll be 50 the overall
average across the 200 seconds is pretty
close to 25 seconds right if I randomly
went in there knocked in the door and
said please give me an answer it'll on
the average take 25 seconds to do this
and probably would all agree that that I
mean it's a terrible system but we're
just describing what it behaves like
here are some other numbers that are
easy to intuitively agree on the 50
percentile is still pretty good it's
still 1 million seconds and then it gets
worse did that 75 percentile is about 50
seconds and the 99.99 percentile is
pretty dang close to 100 seconds ok
everybody good with that let's measure
this system with jmeter grindr
loadrunner your favorite thing in your
software and let's go through the simple
technique right so we measure we issue
requests we measure responses we do this
again and again on the left-hand side we
got 10,000 responses each one took
1,000,000 second on the right hand side
we issued a request that took 100 second
we recorded it then we're back in
business let's do math these are the
stats that come out of that data they're
a little different than reality they're
a little different than how a human
would describe the same system but this
is what a
normal naive loadrunner joinder jmeter
whatever it is will do to you if you
measured this system which I highly
recommend you do okay so you're talking
about a ninety-nine point nine percent
of one millisecond because that is the
data there's 10,000 good stuff and
there's one bad thing that's ninety-nine
point nine nine percent okay and what
went wrong here the problem is we were
supposed to measure the same number of
results on either side per second I'm
supposed to measure the same thing and
it's okay to drop things here and there
it's okay to say I only measure half on
the left and half on the right but it's
not okay to only measure the good stuff
or to measure the bad stuff only once in
ten thousand times and the good stuff
thats whats whys data and in reality
that's what happened here coordinate
admission is but only these numbers
would have given you exactly what you
expected coordinate admission is
basically the equivalent of somebody
coming in and rubbing out all the bad
numbers in the log file and then doing
stats on the remaining data so the stats
that you get it doesn't matter if it's
standard deviation or average or 99
percentile to all wrong because the data
is wrong unfortunately this happens all
over the place how many of you think it
happens to you Wow pretty convincing huh
okay so let's gives examples we actually
hired an intern for the summer who did
work both to document this in jmeter and
to fix it we have a fix for jmeter that
hopefully will it's up on github now but
not well documented will do stuff over
time but this is an example of jmeter
with a hypothetical system just like
this we built a proxy that simply
stopped every once in a while for fifty
percent at a time and this is what
jmeter said it behaves like in this is
what jmeter said it behaves like after
we corrected jmeter we can correct the
log files or we could actually run
jmeter interactively and corrected it's
a nice effect but notice that we saw
this same exact characterization and
grinder in an HP loadrunner and pretty
much every load generator we tested
there's a couple out there that I know
of that don't make this mistake twitter
I forget the name of this really cool
load generator that Twitter is built
that's completely asynchronous the trick
is don't wait for the response if you're
going to issue the requests issue them
when you were going to issue them don't
wait and then you don't get this thing
so by the way the words coordinate
admission mean well a mission is throw
away some results which is ok but doing
it coordinated only you know
coordinating with the system that you're
measuring is a really bad thing because
then you've swayed all the data and
everything else is just wrong here's
some other examples how many of you know
the why CSB benchmark or run things like
Cassandra or memcache okay so this is
typically how men cash is benchmarked
and the Y CSP benchmark has coordinated
a mission in it now I've looked in the
code to see it but there's some easy
ways to see it in actual data sometimes
sometimes you're lucky enough to see
such a bad skewed that you know it's
wrong so let's highlight how this is a
test that is single threaded okay one
thread is issuing requests really really
fast usually it'll go in the thousands
or tens of thousands a second so nice
and fast then you know in this test we
saw a 26-point something second max
knowing that this is a single threaded
test we know that the test was
completely stalled for that amount of
time now that amount of time happens to
represent one point two nine percent of
the entire test type in with that I
could do math that says if I take one
percent off of that and left with 0
point two nine percent the 99 percentile
can't be better than that and that is
about five seconds in this number notice
that the benchmark reports say 99
percentile of five milliseconds it is a
thousand times often it's reported
result because of coordinated emission
now if you use this to capacity plan
your mem cache or your Cassandra setup
understand what you're doing you're
taking garbage data and making
conclusions based on it and that's a sad
thing by the way there are ways to
correct this so
another example not quite proof but hi
reason for suspicion this is a
world-record result in a spec J
enterprise 2010 benchmark it does not
have a max requirement it does not have
a standard deviation or other
requirement it basically has a 90th
percentile requirement but they report a
lot of numbers and as you can see here
they have a required 90th percentile on
the right and what this benchmark
actually did and it was doing like nine
thousand transactions per second and
gets database is really cool now another
column they happen to report that that
is not actually required but showed up
in the report it's up on the website is
the max time now this this is a one hour
test one our timing test and almost
every measured result here at a
five-minute pause in it this is a
published passing world-record benchmark
that shows how big a system can carry
how many transactions when you buy them
okay was there a question there now just
shocked now that represents only eight
point four percent of the total running
time so it is theoretically possible
that that 90 percentile in that max can
coexist in the same test it's unlikely
but it's still possible I haven't really
proven here but it's a good reason to
suspect there's a problem when Max is so
far away from the rest you've got a
problem offered another thing to note is
remember that thing with standard
deviation so this next time is 760 two
standard deviations away from the mean
just for example if your if you resolve
these seven standard deviations away
from the mean it's probably got eight or
nine 9s and it's percentile if it's 700
standard deviations there would have
been millions of bag Big Bang events
before this would be possible in a
standard distribution this is proof that
the distribution is not standard here's
some real world things not benchmarks
this is an actual customer measurement
where the requirement was for the
ninety-nine ninety-nine point nine
percent have to be better than a second
what you see at the top is the measured
data out of jmeter in this case and
remember when I said the shape tells you
a lot the top graph has jagged edges
it's got vertical spikes that's usually
a telltale sign of coordinated emission
this is why looking at the shape of the
graph and plotting the graph is very
useful so at the bottom you have that
thing corrected corrected for
coordinator mission and at the
requirement point there was a 7x
difference and a reported number so what
they thought was passing was badly fin
and here's another one this is only
three or four weeks old this is a high
frequency market data system which
measured Max's and knew there was a
problem in this case they knew they had
a problem we just had to figure out how
to correct it because this was not a
load generator this was the real world
so we didn't have a plan we didn't know
what the intervals were supposed to be
but you could see that they're scored
into the mission and the result just by
looking at the shape and in this case
while they know there's this really big
macs it only affects you know the eight
plus nine so maybe it's okay right well
they knew it's not okay when we sat down
and we corrected it and applied the
right math and techniques to it and one
of the tools i'll tell you about does
this we used it here HDR histogram this
is the corrected data it's smooth and
its way over to the left now why do I
care about this remember we sell them
jvm that eliminates Big hiccups and on
this system that is saying and this is
the other JVM now depending on how you
measure there's a 2500 x difference
between where you think the value kicks
in you see that the max was eliminated
but you know if it's only one and a
billion numbers maybe i don't care as
much when it's one in a thousand numbers
you care a little more etc okay
so when you're measuring whatever you're
measuring test your measurement system
basic physics basic labs calibrate and
the best way to test it is to give it a
hypothetical system you know the
behavior of hit ctrl Z on the keyboard
count how long it was truly stopped for
and see what your monitoring system
you're measuring system your benchmark
system tells your system did if it does
not describe the stall you did nothing
else it describes is correct and nothing
else that describes could be trusted
when you get it to be sane or somewhat
sane and describing actual known
behavior you can then try and project
from that and unfortunately a lot of
systems have this you know people
blindly follow the system don't waste
your time analyzing anything until
you've achieved that sanity and you know
we already talked about standard
deviation don't ever use it for anything
it's embarrassing once you know what it
means now always always measure the max
it's the one number that's harder than
anything else to cheat on and to
accidentally miss you can get
coordinated emission in many ways but
hiding a maximum value is hard and your
maximum value is usually what your
number one indicator that something
weird is going on so be suspicious look
at how what it tells you how far away it
is from other things and then measure a
lot of percentiles as many as you can
here are some tools okay I need to start
speeding up here to make sure we don't
crack the next 18 year Instagram is a is
a basically a Java library I put up on
github it is public domain as open
source as open source gets and do
whatever you want with it just don't
don't blame me it lets you produce
graphs like this and the library itself
doesn't produce the graph it produces
the data that you can plot like this
with the fidelity and the resolution
needed in to plot a graph like this you
need I call this HDR Instagram because
it's a high dynamic range histogram you
need both a good dynamic range of
of value and and percentiles and and the
rest and good accuracy at the same time
accuracy not meant in resolution
necessarily but in relative terms i use
number of decimal points you care about
in HDR histogram some background about
it its main goal was to allow me to
collect data to support good legacy
characterization it's good for any kind
of data that you need to histogram
latency as just a good example of data
like that and you need a quote wide
range because we're interested in the
99.99 percentile and we want a good
fidelity of numbers in that range to not
just course big buckets the existing
alternatives when i was playing with
this was to use regular histograms that
are either linearly bucketed or
logarithmically bucketed or arbitrarily
bucketed and I don't know why other
people haven't seemed to do this before
I honestly I didn't think this was
really that special when I built it but
so far I haven't seen any other
histogram shape the data itself like
that and the traditional histograms have
their limitations a linear instagram has
the resolution you want but a very
limited dynamic range if you want to
cover from a microsecond to an hour with
one microsecond resolution you'll need
3.6 billion linear buckets and that's
not that practical for most people a
logarithmic histogram usually has a very
wide dynamic range but very very bad
accuracy you can't tell the difference
between half an hour in an hour in the
scale okay and then the arbitrary ones
are just arbitrary you have to know what
the scenario is before you test it which
makes it really really hard to design
the histogram to begin with so HDR
histogram is really kind of a coverage
both things it covers a configurable
dynamic range you tell it what the value
range you want to cover is and then you
tell it how many decimal points you care
about their I want to cover from you
know and a microsecond to an hour and I
want three decimal points of resolution
of accuracy a resolution is it is a
different actual
meaning but anywhere through that I want
three there's no I don't care about six
decimal points I don't care about a
microsecond resolution at the hour point
but I do care about a microsecond up to
a millisecond a millisecond up to a
second a second up two thousand seconds
I care about that and the reason we care
like that is when we do graphs we can't
really tell the difference between you
know when it's point one percent apart
but we we can't tell if it's a 2x thing
so it has built-in optional you can tell
it please correct coordinate admission
on this number which is a very powerful
thing and Sally produce the correction
graphs for you so when you know what the
expected interval between request is you
give it a request it will automatically
generate all the missing things for you
if that's what you told it to do that
lets you do interesting things like
compare corrected and uncorrected or use
the corrected when you need to it's open
source as I said internally it is fixed
spoke costs both in time and space so a
fixed size structure with a fixed cost
operation will log as many Layton sees
as you want to throw at it and maintain
the accuracy you asked on a drilling
results if that's what you need it's
built a lot like a floating point number
it's got linear sub buckets with in
logarithmic super buckets you can think
of it as a min to sign and exponent in a
floating point number but a really big
one and typically to cover say you know
an hour range at a microsecond
resolution with three decimal points
that'll be about 180 k of data at two
decimal points it'll be about 20 k of
data that's not very expensive and it's
fixed size etc I'm I'm going to skip
through the details of the bucketing the
sources up on github if you guys want it
and there's a link at the end so bottom
line is you can do a lot of nice things
with it it provides multiple types of
iterators you can iterate the data
linearly logarithmically by percentile
and it'll produce output that looks like
this in text file if you really want to
I mean you can write your own code for
that but there's just a method that will
do this for you in that output is what I
put into exomes and just directly plot
the graphs you saw so far so if you log
in this thing you can get the right
fidelity of data and there you know with
only about a hundred or so points you
can get a graph that smooth and these
points are kind of you know sort of the
the turtle approach you know for the
first half we do every ten percent for
the next half we do every five percent
for the next half we do every two and a
half percent and we run this down until
there's no more data so there's always
you get to night but you'll get as many
9s as the data has and and you don't
have to throw out you know a million
results to get that smoothness so that's
the graph you get at the other end J
hiccup is another tool J hiccup lets you
basically see the discontinuities of
running java applications or java
virtual machines and it uses HDR
histogram internally to record pickups
and then plot them you'll recognize the
format i think but it plots the bad
things over time as well as by
percentile now the way it generally
works is simple almost silly and in i'll
explain in a second but you basically
added either as a java agent or is a
rapper script or you can even point well
this is not up on the web baby but i'm
about to send what the next year is
where you could grunge a hiccup and
point it to a running process and get
the logs for it but basically all it
does is add a background thread to your
application that takes every millisecond
and sees how long before you know i went
to sleep for a millisecond how long
before I woke up and it's a pretty safe
bet that if a process that if I thread
that went to sleep for a milli second
woke up 200 milliseconds later a lot of
other threads were affected by that
hiccup as well so it's recording the
fact that a thread observed the
discontinuity in that tells you about
the rest of your system and it's open
source and public domain as well so to
understand what this shows you this is
hiccups overtime these are pickups these
are actually maximum measured in a
five-second interval by default that you
can play with what it means this is a
cups by percentiles and you've seen
enough of these shapes so so far to see
what that is hopefully
now I die bilder store a reason I like
to be honest I told you I'll do a little
braggy and this is Charles Nutter saying
what you know we built it for right and
it's true it's so that I could show off
and show things so this is an example of
that on the left hand side you have
oracle running the CMS collector with a
1 gigabyte thing and a gigabyte heap and
on the right hand side you see using
running the same load and can you see
how much prettier zing is well it's hard
to see from where you are maybe let me
highlight the scale and then normalize
the scale and at this point I basically
I could say it's hard to show in a graph
how good you are when you only have a
thousand pixels and you're more than
that in goodness right there is
literally that the ratio here now this
is not just for squishy human
applications it's also all the way down
to the soft real-time and in trading
level applications exactly the same
visual trick is here you saw that graph
before regular hotspot base JVMs zing
next to it and normalized and you know
it's pretty easy to explain with that
graph why somebody might one thing if
they care about those glitches and
hiccups and they want to get rid of them
quickly without a PhD in tuning so I
said I'll brag a little I've got three
minutes to do that in the same is the
product that we make it you know it's
why I like to measure latency cos were
really good at it and zing is really a
JVM for x86 Linux platforms it
completely eliminates not just slightly
improves or pushes a little better or
twenty percent better than last year but
total eradication of GC as an issue for
all enterprise applications I know that
sounds like a big deal but we have a
concurrent collector that is concurrent
everything concurrent marking concurrent
compacting concurrent weak references
concurrent deflation concurred
everything and as a result we just don't
stall your jvm or your application or
cause any long hike ups for anything
that has to do
garbage and we've actually spent the
time to make sure we don't stall for
anything else either because once you
take care of the garbage are other
things like you know je viens might
stall for it's got a very wide operating
range it can do anything for a gigabyte
heap to hundreds of gigabytes and people
use it across that range depending on
what they want to sometimes it'll be you
know because they just care about lane
see sometimes they need size and they
can't afford the latency that comes with
it the key thing is that decouples
responsiveness from scale scale does not
affect responsiveness you could be big
you could be small it doesn't affect the
jitter and certainly you could be faster
you could be slow it doesn't affect the
jitter not the garbage collection in je
v MJ directories what's it good for it's
pretty much any server based application
if you're running on Linux and if using
more than about three hundred megabytes
of life set you probably are seeing the
behavior that we eliminate whether or
not you care about the behavior is up to
you but it roughly that size that's the
crossover point of it's going to be
visible and different when you look at
graphs and we'll probably just deliver
better metrics where it shines reading
is low latency machine-to-machine
messaging stuff human response times you
know if you've got a web store on Black
Friday and you really don't like the
missed revenue to your competitor we do
a lot of business there online clothing
is a good example and then large data
sets that are truly in memory in-memory
analytics risk analysis and financial
computations things like ad placement
analysis SAS software all kinds of stuff
like that so that's thing now on to just
take aways well first of all standard
deviation is really bad everybody got
that and I know people might argue with
you but I've never ever seen a software
system that has a normal distribution
for latency idle Linux does not have a
normal distribution for latency
so the software that runs on it won't
won't have it either if you haven't
stated your percentages and max's you
probably haven't stated your
requirements and your measurements are
probably not useful towards a goal
measuring throughput without latency is
also useless so always always measure
latency and document the state that it's
under the mistakes that you can run into
light cord into the mission can
completely change the meaning of all
your numbers and a little bit can affect
quite a button so in examples I give you
a real world stuff point one percent of
data was affected but sometimes that
point one shifts the averages by
importers of magnitude or than
percentiles by orders of magnitude ok
and unfortunately the percent house is
what the business actually needs now Jay
hiccup in HDR histograms are useful and
you'll see links to them or just you
know Google for them and then you know
design jvm is school or at least I like
to think so so that wraps it up just a
couple of notes on other sessions that
I'm doing there's a session
understanding garbage collection it's an
educational sesa John CC with a little
bit of bragging at the end kind of like
today and I've been doing it a lot it
seems to be very very popular I mean I'm
this is going to be the twenty sixth
time I've delivered that talks is pretty
pretty clean by now help but it's on
Wednesday and then we have a JCP next
panel discussion today and I know a lot
of you think the JCP is sort of boring
stuff and honestly I don't blame you but
JCP next is about the future of the JCP
rules the IP flow the rules on who gets
to contribute who gets to vote what
happens to things like TC Kaizen and and
you guys can affect it because there is
an election going on ok so please come
if you can and if you can please read up
on it and vote
so and last one is you know please visit
the zoo boost if you can so I think I
ran two minutes overtime how are we on
gaps and QA and other stuff we have to
clear the room okay I'll take questions
outside from anybody who is interested
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>