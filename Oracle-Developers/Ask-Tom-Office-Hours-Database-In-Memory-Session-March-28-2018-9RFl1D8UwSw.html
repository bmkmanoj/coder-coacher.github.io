<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Ask Tom Office Hours: Database In Memory Session: March 28, 2018 | Coder Coacher - Coaching Coders</title><meta content="Ask Tom Office Hours: Database In Memory Session: March 28, 2018 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Ask Tom Office Hours: Database In Memory Session: March 28, 2018</b></h2><h5 class="post__date">2018-04-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/9RFl1D8UwSw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">and then so the topic today for database
in memory office hours are why
columnstore scans are faster than rows
store scans
again we have Shashank shaven who's a
vice-president of in memory technologies
and basically one of the lead
development managers for database
in-memory myself I'm a database of
memory product manager then we have
Gregg Crispin also on the line today
he's going to help moderate the
questions and make sure we try to get
all of the questions that we can insert
so the agenda for today is basically to
go through go through a few slides kind
of cover why columnstore scans are
faster than roast or scans I have a
short demo for you and then we're going
to turn it loose for questions so I
apologize for the late start and the
technical difficulties with zoom here
but we will go ahead and get started
safe harbor statement database in memory
just to review from our last session in
case you're you're not aware database in
memory provides columnar formatted data
in addition to the traditional romantic
data so that's kind of where we get into
the question of why is this columnar
formatted data that's in this new in
memory column store as part of database
of memory why is that so much faster
than the traditional Oracle row store
data it's in the buffer cache which you
might be more familiar with and so we
are going to try to cover that the gist
of the of the two formats is that I like
to tell people that we could have called
this the analytic data database option
because it's really about analytics
columnar formatted data is very very
good for fast scan analytics will tell
you why versus a row store which is very
good for both second
Oh LTP and so database in-memory it's a
pure in-memory format we you don't have
to populate the entire database in the
column store you get to pick and choose
the objects you want to populate and the
data is then formatted in columnar
format we have a couple of questions
that we'll get to that cover things like
what about DML and we D ml is kept
transactionally and and simultaneously
in sync with the row store we do that
similarly to the way we keep indexes
up-to-date but it's very very much or
very much faster because we don't
generate any undo or redo because it's
not sure in memory format oops so let's
get right down to it so in the buffer
cache for row formatted data in order to
get to say column four in our example
here we actually have to read each
database block into memory and then scan
across each column or each row I should
say to get to the column we're
interested in so we actually have to
read quite a bit of data and that's kind
of the the first reason that row based
data is slower than columnar formatted
data in the case of getting column four
from the column store we only have to
access the actual column data itself and
then there's some other technical
improvements or features that we've
added into the column store that make
this scan of that column or columns much
much faster than they would be if we
hadn't done that again the column store
is allocated through the SGA we talked
about this in the first office hours
it's a pure in-memory format it's a pure
columnar format as well it's not
persisted on disk the row store is still
the persistence mechanism in Oracle none
of the on disk formats have changed and
none of the transaction processing has
changed the optimizer cost-based
optimizer chooses between a roast or
query and a column store query and the
fact that we don't change any on disk
formats means that not only do your
applications not have to change because
the cost-based optimizer is deciding
does
makes sense to access the data from the
ROS store or the column store but in
addition to that we don't change any of
our existing technology because we
haven't changed anything on disk so we
talked a little bit about columnar
format you only have to access the
columns you're interested in that are
part of the query this avoids having to
look at a lot of data that we don't need
to actually look at our access because
we only look at the column columnar data
as part of the query it's pure in-memory
so we don't have to do any i/o
associated with that that access in
addition to that we compress the data
it's a columnar data compresses very
well and we have multiple levels of
compression we typically get anywhere
from a 2x to a 20x compression level
depending on the compression you choose
you can actually choose to compress
individual columns at different levels
and we have multiple levels for DML the
default mem mem compressed for query low
also have query high and then capacity
low and capacity high in addition to
that there's when we populate we
actually create in memory storage
indexes on each of the IMC's or in
memory compression units again we talked
a little bit about that in the previous
office hours session and we may dive
into that in a later session as well the
gist of that is as we populate a segment
we actually a segment populate in the
column stores made up of one or more IMC
use rather than a large contiguous
amount of memory for each of those IMC
use each of the column values or column
c use has an in memory storage index
associated with it it allows us to prune
out data that we don't need to look at
so in the case of looking for a store
idea of 8 if we're scanning our sales
table for that column value or a column
of store ID we can actually skip over
multiple IMC use access IMC users that
have a min Max value within the range
that we're looking for and possibly peek
at the data dictionary and skip data if
we know even though it passes the range
scan
for the data in that I MCU but the data
is not there because it's not in the
dictionary
we can then short-circuit our scan and
avoid doing any work that we don't need
to do in addition to that we support
Cindy vector processing our single
instruction multiple data processing
essentially think of it as array
processing and so it allows us to do is
access multiple values in essentially
one instruction cycle this allows us to
go from millions scanning millions of
rows per second per core in the row
store tomato will scan billions of rows
per second in the column store so those
are the technology features of database
of memory in addition to that the
optimizer understands database of memory
as well when you allocate the column
store you set in memory underscore size
greater than zero the optimizer then can
has at its access different access plans
to run database in memory type queries
one of the first things it can do is it
can push filter predicates into the scan
also it can push aggregations into the
scan so I like to tell people the ideal
database in memory scan is one that we
can do strictly by accessing the data in
the column store and returning your
result back to the calling query that's
not always possible but that's kind of
the goals to be able to do all of the
work that we can in the scanning of the
data in the column store
an example output of that would be doing
a select count where we're able to see
we notice that we have a table access in
memory full this is actually an
execution plan from a from an actual
sequel statement and we see down below
the on the predicate information that
the in memory keyword tells us that we
pushed the filter predicate for part key
into the scan of the column store and we
also see then see in the session level
statistics that our I am scan rows
projected is the same number as the
fortunately I can't see it so I got some
windows here so if I can move it is the
same number as the rows that we returned
and so that I am scan rows projected is
actually a very good statistic to
determine how much work was actually
done during the scan operation of the
query in addition to that we support
skip to hash joins so hash joins can be
actually turned into scan and filter
operations with the use of bloom filters
and so that's another significant speed
up in the the optimizer has to to use in
the inquiries that do joins so if it can
choose a hash join it can then turn that
hash join from a creating a hash table
which it would still do in addition if
there's a filter predicate it can create
a bloom filter it can then use the bloom
filter as a scan of the larger table as
a filter predicate as opposed to having
to probe the hash table to find matches
instead it can use a bloom filter
effectively turning a hash join into a
scan and filter operation first on the
on the smaller table in the hash join
and then scanning the larger table in
this case the storage table creating a
bloom filter on store ID and then
pushing that as a filter predicate into
the scan of the sales table this can
provide up to a 10x performance
improvement over a traditional hash join
and then we have in memory aggregation
or something we call vector group I this
provides the ability to actually do a
group buy or the aggregation from a
group I'd in the scan of the column
store this is a little bit more
complicated picture here but essentially
what we have is two dimension tables a
products table and a storage table begin
they're part of a join to the sales
table in addition that we're doing a
group buy here and what winds up
happening is we use key vectors rather
than bloom filters when we create or
when we scan the products and stores
table
we push those as filter predicates into
the sales table and in addition to that
we create an in-memory report outline
think of it as a multi-dimensional
spreadsheet and we do the aggregation as
we're doing the scan of the sales table
now that was all very complicated I just
kind of glanced over that there's
actually all of these features are
documented in not only in our in memory
guide in our documentation but also in
our technical white paper there's
actually a separate technical white
paper that describes in-memory
aggregation in great detail and so if
you're curious about this again I just
covered the very basics of the fact that
the option exists it's I just encourage
you to take a look at that white paper
and look at the details of what we're
doing this shows up in an execution plan
is a vector group I so why not just
cache the table in the row store and
what I've got here is a an example from
a demo that I do but what I'm going to
do is instead of showing you a slide
we're actually going to flip over and go
to the actual demo that I have running
on my laptop in VirtualBox
so this actually this demo picks up from
where we left off in the previous office
hours
we've already populated objects in the
column store so if we're going to show
you that and we've cached those objects
in the unity cool in the buffer cache
and so what I'm going to do is I'm going
to show you two queries and in-memory
query they're exactly the same query and
in-memory query and then a buffer cache
query and you'll notice that when we do
the in-memory query all of our data is
populated in the column store and then
we do the buffer cache query it will all
be cached in the buffer cache because we
won't do any physical reads it'll only
be logical iOS and this goes back to
actually one of the questions we got
asked a head of the session a
pre-session question about well what if
I just cashed my entire database or
maybe my schema in the buffer in the row
store in the buffer cache wouldn't that
be just as fast and let's see if we can
prove that it wouldn't be so what I ran
initially was just showing the segment's
populated in the column store knows we
have bytes not populated as 0 then I ran
the first query select the max total
order price from line ordered line order
table came back in four hundredths of a
second
we see the execution plan shows us table
access in memory full and
look at the statistics associated with
that notice we have all of our I am scan
statistics lips stuck resizing showing
us that we access the column store to
run that query our second query is a
buffer cache query what we did is we put
a add no in-memory hint there it's the
same query though and notice it's going
to take a little bit longer to run are
we
okay just checking the chat to make sure
everybody
see this notice now it took 19 seconds
to run that query so that was well let's
see what happened
oops now we did a table access full for
the line order table so we don't have
the in-memory keyword there
so we've accessed that table from the
row store or the buffer cache and then
notice that now we don't have our I am
scan statistics and we don't have any
physical reads just session logical
reads accessing that data all of that
data was cached in the buffer cache but
notice it was significantly slower the
reason for that is a the ability to do
Sindhi vector processing the access of
just that single column against the
column store and the ability to push
that max aggregation into the scan of
the line order table to get that data
back much much faster than we can get it
from the row store obviously this is a
very simple query it's meant just to
show the initial query answer the
initial question of why database in
memory so much faster with columnar data
then then the traditional row store in
follow-up office hours we'll explore a
little bit more about what we can do is
vector pushed or sorry with predicate
push down with hash joins with vector
group-by and we'll also explore some of
the other features but for now it's kind
of what I wanted to catch we're about
20-25 after the hour so let's kind of
turn this open to questions at this
point could you hear me now
I can hear you now just like the guy
okay there's nothing commercial
currently nothing in chat right now okay
so if you guys have stuck with us for
this this far if you do have any
questions please feel free to to ask
them in the chat we actually had a
couple of questions pre session which I
can go over again the first question was
if we if I'd use the full database cache
feature and just cached all of the data
in the buffer cache would that be as
fast as the in-memory columnstore and I
think we've shown that the answer is no
it would
B and Oracle does have both the full
database caching and the automatic
BigTable caching but those are row store
features not column store features one
of the other questions that came up
shashank are you on the on the line can
you hear me Andy ah now I can hear you
great social shank is one of our
development managers for database and
memory and one of the questions that was
asked was support for external tables
how does it work and what if you want to
change the file is wondering if you
might want to tackle that question yeah
so starting in eighteen one you can mark
your external table any table that has
the organized external Tran therefore a
memory the way it works is if there's
any modification to the table then we do
not will continue to support stale
tolerated only so we won't support any
modifications to the external table so
you'll have to specifically specify
parameters they'll tolerate is true
which then allows us to basically
cashier your table into in memory so you
pay the cost of loading it from the
external data source as you normally
would but then we will go ahead and
format it into the column store like we
would any normal row he and after it's
fully populated then issue a query on
the external table and you'll be going
directly through the column store if
there's any modifications unlike with
row heaps where we track invalid rows
and then we will keep things from
fictional trans actually consistent
that's not the case for external tables
so will continually continue to operate
on the stale copy of the table in memory
you can always repopulate the table and
bring a fresh copy of it into the column
store and it will scan against it so the
message
there's no automatic repopulation that
you have to do that manually to get a
fresh copy of that data that correct
track great correct and then this is an
eighteen one feature that's also correct
it's correct great and then it requires
the scale tolerated parameter as well
but correct okay and actually folks this
one's a good question
or timely anyways because we actually
have another one of the developers has
posted out on his smart scan deep dive
blog if you can see let's say I got to
share this sorry about that
I'll do this this way out on his on the
smart scan deep dive blog roger McNichol
has actually posted additional
information about in-memory external
tables and how they work with database
in memory so that's actually available
the blog's oracle.com smart scan deep
dive but you can actually get to that
from the database and memory blog if you
go to the menu and oops that's not the
database in memory blog I did the back
before it popped me right there we'll
get there and show you the under the
menu option we have a DB IM resources it
used to be in the in the on the center
page but has now been put into the menu
and if you click on those resources not
only can you see all sorts of resources
we have available the white different
white papers are in memory aggregation
white paper but if you scroll down to
the bottom we have some important other
important related blogs to database of
memory the last one being the smart scan
deep dive blog that Roger writes and so
if you want to get or take a look at
that from some more details obviously
you can also look at the 18:1
documentation online as well
thanks Shoshanna anyway yep we have a
question came in from the cat okay so
are there any are there any datatype
restrictions for the columns for example
can we use Club JSON etc yeah so there
are so I Oh T's and hash clusters are
not supported populated in the column
store lobs are supported in the column
store but only inline lobs so out of
line lobs are not populated and JSON is
supported especially in twelve to twelve
one supports JSON but in twelve - we
actually added a special binary format
of JSON and that can be populated and
you can run then analytics very great
very good performance for analytical
queries on JSON data in the column store
in 12.2 that's actually documented in
the JSON developer's guide if you want
to get more details on that one thing I
like to focus rank I just want to
quickly add on that so if you even if
you have a lob comm that's not in memory
and that's not inlined you still will
maintain the lob locator in the column
store so you'll still see you can still
see very significant benefits within
memory depending on the query so if your
query is touching multiple columns or
scanning multiple columns and some of
them are not lobs and just simple
scalars you'll filter out those rows
very quickly and then you only apply the
remaining credit gets on those passing
rows involving those not come so you so
even though you won't you know you just
have a lob locator in there you can
still see massive benefits within memory
so you can reduce the row set first
within memory and then only have to
access the club column separately
outside the column store that correct
great okay thanks yeah that's a great
tip another
came into chat says I've I've in memory
implemented on a rack I see some objects
populated in both the instances queried
TV dollar sign I am I'm score segments
what happens if we were to lose one of
the instances so in rack we do to
support scale out we do a distribution
either by row ID range or by partition
or sub partition so if you have data
let's just take the the kind of the
default case of row ID range
distribution so if you had say a four
node rack you might have close to twenty
five percent or a quarter of the data in
each of those rack nodes so if one of
those nodes were to go down then
obviously since it's in memory that data
would not be available to the query in
memory what that means is we'd have to
go to the row store to pick up those
objects now if you have an engineered
system we have the ability to do
duplicate we have a duplicate option to
the alter table or create table in
memory command and in that case we can
actually mirror object or imc use
between different column stores on a
right in iraq environment and then that
would mean that if one node were to go
down that data would be mirrored on one
of the other nodes and you would still
be able to access that data that's only
on engineered systems but if you're on
commodity hardware or if you haven't
specified the duplicate feature then
that data would not be available but of
course the query will still run it just
won't run as fast as if it had run if at
that node was still up so the data
that's in that node will will not be
available now if the node stays down for
any length of time
I believe the restriction right now is
or the timeout is up is three minutes
then we'll look and we if there's room
and the other column stores will
repopulate that data that's no longer in
the the node that went down to the other
nodes assuming there's enough room in
the column store hopefully
answers that question yep thank you
nothing else went at currently okay I
had two more questions that had come in
priest session one was we have lots of
DML is there going to be fragmentation
in the columnstore and Allah is Shashank
do you want to take that or you want me
to do it so I answer it you I'm gonna
put you on the spot there I think you
can go ahead and do go ahead okay so if
you have lots of DML actually what
happens is when when data is updated we
actually marked a change in the column
store in metadata and a snapshot
metadata unit and so if you come along
and query that data and that data is
been marked as being changed we'll get
that change data from the row store but
we actually do
repopulation and trickle repopulation in
the background because the goal is to
keep the column store the IMC or the
columnar formatted data in the column
store in sync with the roast or if at
all possible and so what will happen is
in the background based on heuristics
that say if a certain number of column
values have been changed then we'll go
and schedule that IMC you not the entire
object now just the IMC and objects made
up of one or more IMC use in the column
store will repopulate that IMC you so
that it has all fresh or data that's in
sync with the row store we also have
something we call trickle repopulate
which says that if there are enough
cycles available and we're not busy with
repopulation we'll even go change
I am see us or repopulate IMC's that
don't meet the threshold but in the
meantime if there's are again changes if
there are inserts however what will
happen is will populate that data into
new IMC use so I guess the the the the
answer is no there won't be
fragmentation within the column store
because we'll always repopulate the data
into new IMS use the data is essentially
read-only once it is populated
going back to our first session the
columnstore is made up of one megabyte
extents and each IMC is made up of
multiple one or more of those one
megabyte extents so when we repopulate
we just allocate new memory to a new I
MCU and repopulate and replace that I
MCU for the one that was that had the
changes that we were repopulating to fix
hopefully that makes sense no no no
other questions I had one more ok I had
one more look like Shashank went back
and answered a bunch of questions
previously which is great so hopefully
folks got those questions answered right
away what is the best way to get all of
the tables repopulated after a restart
so remember since it is a pure in-memory
format if the instance goes down whether
it's a rack instance or single instance
if that data will then no longer be
populated in the column store because
it's that's kind of a part of the
in-memory part of the of the feature and
so what will happen is on restart we'll
have to repopulate that data if there's
a priority set we will populate that
data essentially automatically after
restart if there's no priority set which
is actually the default then we'll we
won't we populate until first access now
in 12 to though we introduced a feature
called faster in-memory fast start what
that does is allows you to create or to
enable fast start by creating a new
table space dedicated to to the space
for the column store and we'll
essentially well what we'll do is well
checkpoint the column store format so
all columnar compressed within memory
column stores or in memory segments
storage indexes on it on the data in the
format that it's in the column store
will check point that to disk and then
we'll if we have to repopulate based on
a restart we can repopulate that already
pre formatted data into memory and
that's kind of about the fastest way you
can get the column store data
repopulated is to literally read it off
of disk in columnar format and put it
right into memory
and that was introduced with 12.2 called
in memory fast start
hopefully that covers that question
actually I have a question now for you
mr. Abbi okay so getting wasted you know
getting stepping back with a techy stuff
a little bit I'd be really understood
know what the typical use case is you've
seen with database and memory for some
of our customers well that's a good
question so since its database our
memory is focused on analytic reporting
analytic processing naturally data
warehousing is a very good use case for
database of memory because typically
folks are running lots of reports
against very large amounts of data in
their warehouses bi business
intelligence type applications key
performance indicators dashboards
especially since you can we can populate
not only tables partitions and sub
partitions but materialized views can be
populated in the column store now with
as we mentioned earlier the question
about external tables that also opens up
the world of big data to database
in-memory because now you can actually
use populate some of your big data into
the column store and run an align
analytics on not only your big data but
your Oracle data as well it's also very
useful for mixed workload environments
not necessarily pure OLTP because we
won't speed up DML statements we won't
speed up inserts updates and deletes
we're really talking about analytic
queries scanning data but in mixed
workload environments where you have
reports that you're running against your
OLTP data database of memory can be
significantly more efficient at doing
that we actually have a demo out on OTN
that was done with the real world
performance team that shows an OLTP
system were actually a mixed workload
system but the workload was broken up
between OLTP and reporting and it shows
that database and memory reporting
doesn't affect the OLTP workload the
performance of it at all doesn't doesn't
affect the performance what this means
translated what that means is you can do
real-time analytics you can actually run
analytic queries on your actual OLTP or
real-time data in real time without
having to wait for it to be extracted to
say a data warehouse or maybe a decision
support system or some kind of a data
Mart or an operational data store of
some kind you can actually run those
queries right on your oh Lt P or
transactional data and it frees you up
to be able to run queries that you might
not be able to run today simply because
you don't have to create a bunch of
indexes you don't have to worry about
the i/o footprint of running those
queries and those cores are run much
much faster than they would without
database in memory that you think is
that that cover it there's Greg yeah
yeah thank you
sure nothing doing that currently ok so
we can I don't actually have I think I
covered all of the questions that I got
ahead of time so we'll we can wait a few
more minutes see if anybody has any
questions otherwise looks like we might
wind up wrapping up a little bit early
today
I was just scanning back
some of the other questions that we had
that shank answered to see if there's
anything else we might have missed it
doesn't look like it so with that if
there aren't any more questions give you
another minute or so if you're shy oh
here we go here's another question does
the higher there's a higher level of
compression sorry Gregg I'm just going
to jump right in does a higher level
don't come with any overhead it does it
can so there's a fault level compression
mmmm for kori low and I'll letcha shank
jump in if I don't get this completely
technically accurate but basically uses
dictionary level encoding in some
run-length encoding but typically not
very much what that means though
translated is we get typically anywhere
from a to about a 4x
conservatively 2 to 4x compression rate
on the default compression we actually
can operate on that level of compression
the dictionary based encoding directly
in a matter of fact our in memory
storage indexes actually leverage that
to avoid scanning imc use that don't
have data that we're going to meet or
they're going to meet the filter
criterias in addition to that been
compressed for kori hi it was a little
more aggressive about run length
encoding and so that can that can speed
up compression or queries as well so we
know we can skip over a lot of large
runs of data the two higher levels of
compression though men compress for
query hi our capacity high and capacity
low those add a zip light compression to
the data and so that does take some
overhead to decompress but it's only on
the order of maybe 10 or 15% at the
outside at the very worst case and I
tend to tell people if you're getting a
10x performance improvement on your
queries paying a 10 to 15% overhead for
the decompression is more than most
cases is more than acceptable for the
trade-off of having more space in the
column store for more data and so the
answer is yes
but it's a very small amount of overhead
and typically more than offset by the
advantages that are offered by the
columnar format of database in memory
one thing I'll just add really quickly
was that is that often times query high
like the comparing query low and query
hi often times query high can actually
gives you better performance than query
low and the reason is because would
query high even though we're focused on
getting more compression the format that
we use can often be more amenable to
aggregations in particular then the
dictionary encoded format that we use
almost exclusively the query low and so
if you are and you know you're running a
workload of the aggregation heavy and
you're you have these measure columns
that you're aggregating on with it's no
sum or average or whatever often times
those columns would benefit with query
high as opposed to query low so it can
it can run either way
great ok thank thanks just right that's
a good point I'm gonna let me flip back
just back to my presentation if I did
want to point out before we wrap up if
there are no more questions that we do
have I'm actually Andy there's one more
question might take a look at the chat
and you'll have to see it first
okay let me know so what go back there
we go so we do have a lot of information
about database of memory out on OTN and
our in-memory blog and of course we have
our new in-memory guide so if you do
think of other questions after the
session ends before the next session
there are quite a bit of resources quite
a few resources out there plus I showed
you in the blog we have the resources
page available through the menu for you
to get to some of the same information
though ten is a great resource
again the blog is I kind of consider the
blog to be a kind of a portal into
database in memory so I'm going to leave
that up and then let's tackle what the
question is so we have
looks like we've got an example where
we've got multiple rows with multiple
columns in this case three rows with
three columns each and the question is
how do you find out Row 3 column 1 given
Row 3 column 2 in an in-memory scenario
or the columns stored in an orderly
fashion sorted by row ID row number
actually the cut the column date column
columnar sorry data is stored based on
the order it shows up in database blocks
and we for each IMC will read a range of
blocks typically with the default
compression it'll be about 500,000 rows
but the order between different IMC uses
not is not pertinent it doesn't it's not
terminus tick it's just only within and
I MCU so the question is what wheel well
let's go back to the example in so now
we're gonna do me my gonna get me to
change screens again so I haven't been
very good at that in this session let's
go back and look and get it to go
backwards okay I gotta get I can get to
the scrollbar notice that we're gonna do
a table
access in memory full I assume I'm
sharing that screen you are yes and so
we're going to access all of the columns
not just particular columns so in the
case of selecting against column 1 and
column 2 maybe we're going to filter on
column 1 as being a certain value and so
show me all of the column two values
that meet a filter predicate on column 1
we're - you can scan all of the columns
assuming we don't filter out for in
memory storage indexes basically going
and doing a table access in memory full
so that's kind of the takeaway we're not
doing row ID access the row IDs are
there so we can then stitch the
relationships between the columns back
together as we return it from the
resulting query I don't know if that
hopefully that answers the question from
Shashank I don't know what shake you
want to chime in were you trying to lie
so you were trying to clarify the the
yeah yeah we're just going to bit easier
the easier just to speak than to tend to
the fight sure so I mean I I wasn't
quite I was also I wasn't sure if this
was a question related to his previous
question which was in terms of how do we
find you know that how do we project
back a column based on the scan of
another column was there some costs
associated with stitching I don't know
if it's related to that but why the
first just to draw out the proper
representation of how the data is laid
out so it's more like what I was trying
to show in that example is all the all
the column one values or accident for
all the rows column one is laid out
first then column two for all the rows
and then column three for all of those
and so when you do your scan or you're
touching column two and you identified
that hey row three is the one that
passed for column 2 then you have this
mapping or a bit vector that says no
column three passed and maybe other
columns like five and ten pass now to to
quickly index into column one to fetch
out the passing rows so call you one
pass pass back column one for Row 3
column 1 for row 7 whatever it is off
that passed we have a sort of an
indexing structure that allows us to get
to these rows very quickly and most of
the time if they're diction encoded
columns then that means these column
values are all fixed with values so you
can very easily randomly access into
this vector of columns to get to the to
the column values for the passing rows
well that's kind of hard
to understand but dude that makes some
Sun yeah I almost need a whiteboard for
that for that explanation that's that's
great so you guys are getting the a
treat because you're hearing from an
actual developer who wrote the code so
that's that's a pretty pretty cool thing
we've been very fortunate to have
Shashank join us today and be able to
provide those kind of explanations
looks like he he's three is followed up
with another question or so that no
actually so the question was did I hear
that on RAK conventional not engineered
system we have the option to populate
the objects in a preferred instance and
not all of them we actually did a blog
or a blog series on rack and then we
actually did another series or blog post
that followed up on that for 12 - I
think the question though is is it
possible to populate in a subset of
column stores in other words if you got
a rack system maybe had four nodes four
column stores allocated would it be
possible to populate the data it'll make
it simple in just one column store
rather than in all four distributed
looks like talked about the fact that we
use distribution in rack to allow us to
scale out the answer is yes you can you
can do that as a matter of fact EBS the
e-business suite folks have a white
paper that talks about that because for
EBS they don't allow a paralyzation to
to scan across Internode's across the
instances so they restrict parallel
query to just a single instance and so
if we're scaled out that leaves us kind
of out of the out of the picture there
because then you'd have to get the other
data that was in the other nodes from
the roast store so it is possible to do
in 12 1 we use services to do that it's
not it's not simple it's not that's not
that hard we do describe
how to do it there in the documentation
but it isn't something most of our
customers need to do it's a more of a
specialized case however in 12:2 we made
it much simpler because we added a
forced service sub-clause to the
distribute clause there's a lot of
clauses in Oracle in the syntax and so
you can actually assign a particular
object to a service and then you just
run that particular service on the
instance or instances you're interested
in and then the object will populate and
then you can access that object by
connecting to that particular service
and get that data from the column store
so it's much easier to do in 12-2 than
it was in 12:1 again we have blog posts
to go into quite a bit of detail on how
to do that on the blog for both 12 1 and
12 - but you're welcome so there's a
thank you there so I think all we have
for the tent right now yeah ok
other than the going back to the our
tips are additional resources shashank
do you have anything else you'd like to
toss in your on mute yeah no that's the
great thing about office hours they're
really supposed to be kind of more
informal you get to actually talk to you
know real developers folks that
understand the different not just
database of memory but I know we have a
couple other office hours today if there
aren't any other questions I want to
thank all of you for sticking around and
suffering through my navigation which
and then the phone issues we had at the
beginning but I think we had some great
questions hopefully you got them all
answered again if you have further
questions we will have look for
announcements on further office hours
and of course you can go to the blog or
my email address is there we're always
trying to help you
your database and memory questions and
make use of database in memory when all
possible so with that if they're I don't
see any other questions we're almost at
the top of the hour I think we'll go
ahead and close up and again thank you
very much for attending</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>