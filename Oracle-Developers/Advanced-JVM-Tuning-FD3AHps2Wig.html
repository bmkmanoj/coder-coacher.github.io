<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Advanced JVM Tuning | Coder Coacher - Coaching Coders</title><meta content="Advanced JVM Tuning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Advanced JVM Tuning</b></h2><h5 class="post__date">2015-06-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/FD3AHps2Wig" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone I'm David Keenan I am
manager of the language runtimes and
performance group at Twitter tell you a
little bit about my background I've
worked at Twitter now for about a little
more than two months before that I was a
performance architect for the Java
platform at Oracle and before that its
son did that purchased about eight years
and been dabbling in performance of the
Java runtimes and Java and languages in
Java on the runtime since 98 so I'm
gonna usually I've done a series of
these talks around jams JVM performance
of Java one and a lot of times I can you
know I try to give some insight as far
as where the optimizations are going for
the JVM this time I'm not gonna be able
to do that too much I will try to hint
where I think is where I think things
should be going and you know potentially
are going but you know I'll kind of give
it a Twitter spin for this one okay just
get control
okay okay sorry I did not have control
of my slides there for a second I know I
do okay so the first section is just a
general overview of what is performance
tuning and what is performance analysis
what are the steps to take you know
start off with a top-down analysis
approach top-down performance analysis
typically you know happens and it's
actually the first thing you should
consider when you have control of the
highest level of the stack when you can
change the code of the highest level of
the stack you should take a top-down
analysis approach first basically this
is monitoring your target application
under load you take a look at system
level diagnostics such as you know
vmstat MP stat you know the simple tools
that are on your they're available on
your operation operating system
basically give it an idea if you know
you're churning you know user space CPU
whether it's so you know you look
spending a lot of time in the kernel
disk i/o and such that will give you an
idea where to go and sometimes it'll
point into the JVM JVM little
Diagnostics are mostly garbage
collection blocks and that's really that
the most common place you'll see a
performance performance issue with with
Java so you want to take a look there
and see if you if you're gonna rule out
or at least point to garbage collection
as your initial problem finally if you
don't see GC as you know blaringly
obviously wrong or you know spending a
lot of time there you want to profile
your application under load so there's
there's many profiling tools out there
and the idea is to give an idea what
what aspects of what parts of your java
code you're spending a lot of time in so
once you're able to identify that that
piece of code you take a look at it you
try to analyze what where it's spending
time make modifications to to make the
code more efficient from a from a
runtime perspective but most one most
importantly try to reduce allocations
that are spurious and unnecessary if
you're able to control the rate of
allocation or limit allocation at the
application level you're reducing
pressure on the garbage collector any
reducing pressure and all other aspects
of memory management in the VM itself so
that's your frankly that is your biggest
thing you can do as an application
developer is
limit the rated rated allocation now
that being said there's there's
obviously couches there and there's been
a lot of a lot of talk of how to do that
I'm not gonna really be terribly
specific you know pooling can be bad so
be careful in that regard but you know
there's obviously ways to reduce you
know how much how much you're allocating
and you should usually consider that
when you look at when you find a method
that is that is you know showing up in
your profile and you see allocation and
a loop you know that might be the
problem that's that's basically where
I'm where I'm getting at and and then
and I repeat right this is it's a it's a
it's a cycle it's not a it's not a
one-time pass every time you go through
this cycle you you should expect to see
some performance improvement sometimes
not but you have to keep going back and
trying second general approaches a
bottom-up bottom-up analysis approach
you go down this path when when you
don't have control of the upper layers
of layers of the stack this is really
the approach that's taken most often
when you're doing JVM and OS performance
optimization so if you're trying to
optimize for the JVM you will take a job
bottom-up approach to understand what's
happening at the lowest levels of the
stack and how you can influence that so
the first step is to monitor CPU level
statistics against the target
application under load this is using
hardware counters things such as cache
misses you trying to track cache misses
path path length you know branch
prediction issue issues things like that
then then the best next step would be to
use you know profiling tools that can
give you that level of observability
down to the heart at the hardware
counter level for instance there's you
know I'm gonna come in spent many years
using what is now Oracle's studio
performance analyzer but there's there
are other tools that do the same level
of introspection intel vtune for
instance is another one but the ability
to track at C VLK CPU cache misses and
point to the upper levels of the stack
that are causing you a problem you know
is this is that's the tool you want to
use now and
approach you're not looking for the Java
code because there's not much you can do
with the Java level to really manipulate
cache misses okay if you're you know
walking a data structure a lot maybe but
you know it often is the the the the
internals of that that that code that's
the problem so the next step is to look
at the generative assembler what what is
the hardware instructions that are
issued based on the Java code that
you've written and then that's where
magic comes into play you know you have
to understand what you know what x86 is
or what what spark or whatever the
target assembler language that you're
looking at and tried to understand how
you know that code is generated and what
optimizations may or may not be missed
you know it's it's this is where that
that level of experience comes into play
next step would be manipulative OC or
scaled scholar compilers you know those
that are that are manipulating the
bytecode sometimes you can go one step
higher than that and look at the the
common language your libraries that
you're using whether it be you know the
core libraries and the JDK or your own
internal stuff typically anything higher
than that you're not going to really
impact from a bottom-up analysis yeah
it's actually as you go further down
this list it's diminishing return from
this approach so again as I started off
in this this discussion initially with a
top-down analysis that is you know for
those in this room those that are
dealing with Java code top-down is the
way to go
and bottom-up is just so you can
understand what what you know yeah the
other aspect of the way doing things ok
the performance triangle
latency throughput and memory footprint
are very delicately tied together you
know as you and in many ways we want to
be able to to either increase throughput
and reduce latency or I'm going to keep
a hold on memory foot but all at the
same time unless it you know there's
there unless you're doing drastic
manipulation of either one that either
parts of that you know points in the
triangle that impact you know that that
behavior you're not going to be able to
do that for instance if you want to
improve throughput the only way you're
gonna do do that without impacting the
other two as parts to other two points
the triangle is to have a faster CPU or
you know have a faster compiler
something that does drastic changes that
impacts the efficiency of your code but
typically if you want to if you want to
reduce latency memory footprint goes up
right the kind of the best that this is
the tried and true way of reducing
latency in java applications now is to
have a very very large heap and really
avoiding garbage collection and and
promotion in general growth and further
internet and other slides so as you can
see in this you know footprint gets
bigger latency gets smaller throughput
actually got smaller to if you wanted to
induce increased throughput you're
increasing everything more or less in
order to increase throughput where
you're really more like really looking
about the looking at the amount of time
I'm sorry the amount of work you're
doing your try to optimize the amount of
work over time and when you do that in
tuning in Java you're you're you're
ignoring latency and that in that regard
it's going to get bigger and and you
often have to have a very large heap to
avoid garbage collection as well so
increasing throughput right now is
really you know you're increasing all
aspects of the other triangle
same thing with smaller footprint if you
want to small their footprint latency is
going to go up right it's the inverse of
the two you know the way we way we share
lower latency and Java now is to have a
large heap so if you're trying to screw
to limit the the size of the heap you
you have to keep a keep close eye on
latency same thing with throughput as
well
you know it's the classic trade-off okay
next topic of the next section is
performance metrics so metrics are
important you have to choose the right
metric you know in order to to actually
achieve the goal that you're trying to
go after and identifying a metrics is
tricky business you want to make sure
that the metric that you identify
attracts what's important to your user
you you know you know also if efficiency
is important to you you want to want a
metric that tracks that for your company
and you know what influences your bottom
line and what are you willing to trade
off you know as I mentioned in the
previous section that performance
triangle you know there is trade-offs
you have to have a realistic approach
you know to this defined success well is
important as well you can as I said that
the processes for performance analysis
are improving the applicant your
application is a cycle so if you don't
define what success is you're not gonna
be finished you can always keep going
after it
general rule of thumb this is not broken
don't try to fix it because there's
other problems you can go after and
perfect is the enemy have done so you
have to I mean defining success is very
important you know everybody wants it
all right I mean we want high throughput
fast response times and a small
footprint but there you know that's not
free no it's just the reality as you as
you as you go after the metrics that
you're trying to track so make sure that
the metrics that are targeting your your
what your customers are looking for and
use statistical analysis when looking at
those metrics because metrics are the
high rates of variability can be useless
in some searching situations a good
example is average response time which
I'll not really a big fan of as you'll
see you know high rates of variability
an average respond you basically don't
see rates of variability in average
response time you're just looking at
what happened at the end you can have
very large outliers then you go and you
can have you know
you don't know the shape of the data but
that you know your your your data at all
so it's a you know it's important to you
in that situation you should use
statistics okay
throughput metrics you know the most
common one is is transactions per second
you know also known as pages per second
queries per second hits per second and
it is a good measure of top-line
performance it is - as far as you know
and also an efficiency metric as well
and I've listed average response time
here as a throughput metric because in
many ways it is you know it is an
inverse of transaction per second its
time over the number of transactions you
know sometimes it's useful as a rolling
average but but often it's you know the
common mistake I see is is seeing
response time and CPU utilization as the
common you know as that your primary
metrics for tracking performance
maybe we ran regressions and and it's
it's a good way of holding the line on
efficiency but a good example where
those two metrics fall fall apart is say
you were trying to reduce response times
by by doing concurrent garbage
collection what would happen right your
response times will go down but your CPU
utilization would go up because you're
doing GC work concurrently that seems
like it would deliver something good to
your customers however it would be
marked as a as a regression if you
tracked average response time and CPU
utilization so keep that in mind
and lastly so CPU is Latian is a is a is
a good use of a good metric for
computational efficiency it is not
something you should ignore but you
should have taken into consideration
with other optimizations it is good for
for capacity planning that as I said
it's not good for development regression
testing lanes you metrics
you know I've kind of listed in the ones
in order of popularity for myself so
maximum response time is something that
you always should have an understanding
with the worst case for your customers
are it doesn't have to be a primary
metric that you're tracking your goals
against but you should have an
understanding of when it falls over
completely 99th response this time drops
the most egregious outliers and will
give you a bit more consistency in the
metric itself less variability maximum
response time is you know you're it was
always your worst case so it can be
highly variable depending on their you
know the the rate of garbage collection
on the state of the application and so
on so 99% I was probably that would be
my suggested metric for tracking
response time 99th percentile response
time does fairly well it can give you a
false sense of security though in those
situations where you have you know you
know many many young garbage collections
that are doing what you know that are
performing very well and a few very
large full GCS ninetieth person present
our response time may miss it completely
so any any one of these top three
you know metrics you should be in
combination with the first thing of the
first one in combination with the next
two to give you a full picture now
mentioning critical injection rate here
as a new way of kind of looking at
response time and it's it's it's a an
example of it is critical J ops and
inspect jvb 2013 what that is is is is
is it's the achievable throughput under
an SLA response time restriction so it's
the notion of you know what is the true
put I can achieve if I can have if my
99th percentile time is less than 50
milliseconds yeah that's a pretty pretty
pretty handy tool for sizing for what
you're looking for your cost for your
customers it is in inspect ATP 2013 it's
a it's a it's a composite of several
response times but there's a lot of data
there so and and don't don't use average
response time yeah it really is not a
latency metric please just don't fool
yourself it's been used for many years
and I think it's impacted how
actually I you know frankly I can say
frankly it's impacted how JVM
optimizations have happened over the
years you know we're in a situation now
where we don't optimize melt well for
low latency because of average response
time being so important for such a long
time so change that memory footprint
metrics um you know the best memory
footprint metric for for java
applications is the heap size after a
full GC in a live data size you know
many times you don't end up seeing that
but yeah I'll list it in an upcoming
slide how to find that native process
size you know this is the way I do it on
my back and analytics gives you a good
idea of what the the reserved memory
size is psi aux pieds in an app it now
and lastly static footprint so why why
does static footprint matter well it
probably doesn't matter to a lot of
enterprise application developers
however if your application needs to be
downloaded by clients over over a
network it's gonna matter right so
having an understanding of how large
your jar is in that situation and what
level of compression you're using is
important okay now I'm gonna these next
slides I will try to give as much detail
as I can but frankly it's a bunch of
garbage collection logs I'm gonna point
to one value there are there's tooling
and development by Twitter and other
companies to make this a bit more easy
to understand and Oracle does have tools
that they they have available as well
but this is the tried-and-true way
that's common to everybody so you can do
this just by doing you know your post GC
or print you see details and such so I
will you know try to give that but I
apologize if it's confusing and the
slides will be posted I'll put them on
slides on SlideShare myself as well so
if you're ever you review later okay so
as I mentioned the first one is
calculating live data sauce and
essentially you want to want to track
the of the size of the old generation
after full GCS and I've highlighted the
last one in blue is this being the size
of the heap after after a full GC
7-hour 98k next one next way of looking
at cackling live data size is checking
the size of old generation after young
GCS and in the event that no full GCS
occur I mean if we're tuning our
applications well we should avoid the
full GCS so in this case you're trying
to track and see what the rate of
changes and if you look at those three
values that I've highlighted in blue
there you can see that it's it's
increasing you know so promotion is
happening the old generation is getting
bigger so I'll be at slowly okay so the
next next basic step for for JVM tuning
is sizing the old generation a good
starting point for savvy a old
generation is two times the lab data
size at say say so this is a general
recommendation of a starting point I say
two times because you need to have at
least the size of your lab data Scaletta
size and if you size it too closely
you're gonna you're at risk for
depending on the garbage collector but
you're basically at risk for promotion
failures not having enough space in the
older generation to promote your
transient objects in the young
generation if that happens your full GC
so you should have enough padding there
if albert optic promotion rate causes
frequent CMS cycles and gets to the
point where you're concerned about you
know concurrent mode failure or
promotion failures with CMS both of
those cases cause full GCS then padding
it a little little larger is gonna be
important sort of so what you need is a
notion of promotion rate which I'll
highlight highly next coming slides so
in the case of your live data size being
five gigabytes your starting point
should be 10 gigabytes that's the old
generation size alone not the overall
heap size so basically if you're just
setting em s and M X ignoring the size
of young generation you know 10 gig is a
good starting point but really it
involves the size of young generation
well as well and I'll highlight that
coming up but so it's important to size
the the old generation because you know
there's no need for GC pauses
unnecessarily
and if you don't size them the same X M
s and X MX the same the only way you
grow the old generation is rifle GC
so in when you're running an enterprise
application in a production it's you
know unless you're trying to over commit
memory on a system I see no other reason
to to set these values different so
sizing a young generation is is so a
good starting point for general
recommendation for for looking at
throughput and and latency is decides a
young generation to be equal to the old
generation it used to be that we'd have
it smaller but the allocation rates and
the speed of these processors that we
have these days really kind of merit you
know having a net enough having that
much space in the young generation for
for allocation you want to increase the
size the young generation if you have a
higher allocation rate in many cases
young generations are through two to
three times larger than the older
generation many in some cases a lot
bigger than that now don't be afraid to
set it lock large because GC times are
dominated by copying to the survivor
survivor race is not necessarily not not
the size of the young generation it's
just a lot of size of the live objects
so if you size a young generation so
most objects die in the young generation
you're going to have you know fast young
GC pauses and general rule of thumb
higher allocation rate larger young
generation so an overall example of an
enterprise application covering the last
two slides say add a memory cache for
three and a half gigabytes your overall
live data size was four gigabytes you
have a higher allocation rate in
transient data but you do know that most
objects die in a young generation then
it's the general recommendation you want
your young generation to be equal to the
old generation and that's what this this
set of tuning does the overall heap is
16 gigs and you're specifying the young
generation to be 8 gigs - xmn is the
same as new size and max new size
together so we're fixing the young
generation size at 8 gigabytes and then
the remaining is the old generation and
that is 8 gigabytes as well which one
but I'm sorry oh absolutely yeah I'll go
into that yeah
okay let me so let me answer that
question further on in the presentation
so that question it was is there ever a
situation where the young generation
could be sized to log too large where
would impact death rate of objects right
okay
so we'll talk about that later and your
question was would you ever increase the
size of young generation beyond the size
of the old generation and yes to that as
well and we're gonna go right into that
okay so I hope this is basically the
deal see the sections that I have coming
up are tuning for throughput to me for
your latency and tuning for footprints
so we'll go through specific aspects of
it there and I'm gonna try to go a
little quicker through there so we have
more time for questions I think it will
have a good amount of them okay so the
next step is choosing your garbage
collector right so for throughput
general rule of thumb use parallel GC it
is the quickest throughput collector
because it's not trying to do anything
concurrent simple as that
though certain look low server response
times you want CMS as is the older
technologies kind of tried and trued the
problem is that it can be very highly
tuned to your allocation rate and your
promotion rate but if that changes it
could it can it's a pretty steep cliff
so it's pretty pretty brittle g1 is the
current current development focus if you
can tune g1 very easily to get very
close to CMS pause times however it is
slightly faster because it has to deal
with remember sets and and there's more
work involved with rate barrier but
there's more work involved in in in in
the aspect of doing a young GC
collection for g1 because it's trying to
keep an eye on concurrency and what it
can do what work I can do concurrently
that being said it's pretty darn close
so my preference today is to use G 1 GC
mainly because it does it's not nearly
as brittle as CMS so keep that in mind
I mean if you're trying to optimize the
last 2% out of your you know response
time yeah yours probably stuck with CMS
but if you're if you if you're really
kind of concerned about efficiency of
development and not having thing it
falls on its face in production g1 my
pupitar choice in that regard ok
general recommended GC tuning I'm sorry
GC logging flags so these are the ones
that that you know we generally
recommend print GC details and print GC
time to time stamps or the tried and
trued ones that everyone should be using
and log GC to a file
also as good as well so it doesn't you
know it's not going to standard out or
standard error and screwing up your your
your logs so the rest are just various
levels of information I'd like print
heap at GC it gives me an idea of what's
going on with this the tenth of the
spaces like it give you the size and
survivor spaces ten Aryan distributions
very important as well you should in and
if you're able to well you know actually
I'm just gonna say at this point with
these flags if you take a look at the
the job of performance book by Charlie
hunt a colleague of mine in the past
goes through this in great detail so I
would highly suggest that book okay so
this is calculating calculating
allocation rate as I mentioned before
having understanding what your
allocation rate is going to be before
and so basically it's it's it's the the
rate of the if you look in red I don't
it seems to be coming out okay then from
here I can't really tell very well but
but the value listed in red is is the
size of the young generation before the
garbage collection and the value of is
listed in blue is that is that is a
sizing young generation after that
garbage collection and the two yellow
values is that is the change in time
during that between that those two
points in time well it's it's the time
spent between the garbage collection
events and that boils down to a you know
allocation rate of sixty two megabytes a
second for this application so that is
allocation range next is promotion rate
so this is this is this is essentially
changing that it's tracking as the
changes in the size of the old
generation and overtime and the sizes of
the old generation are listed in blue in
this application is actually rather
tuned well with it and it's only it's
only promoting six six kilo but you know
6k a dot a little more than six K data a
second okay now now we're gonna just
jump right into the tuning for latency
and I'm gonna start off with CMS okay so
enable CMS there's been CMS flag some
good flags they have four CMS is is CMAs
cabbage before remark so this will do a
young generation scavenge before every
mark fayus reduces the work of what
needs to be remarked greatly highly
suggested parallel reference processing
reading enabled used to be well yeah I'm
assuming these two flags will event will
be default for CMS if CMS was a more of
a development target but I would suggest
you know investigating these two flags
the last is initiated mission occupancy
fraction seventy is actually a very good
default and that's what it is it pushing
it up further basically means that you
want CMS to act more like a throughput
collector you know you're saying you
don't have to start your concurrent this
basically says what is the fraction of
the heath to start concurrent activity
so if you want if you push it up it will
start doing concurrent concurrent work
for old generation collection later if
you push it down it will do it earlier
so if you see concurrent mode failures
and you're running CMS turning this this
down is a good idea you know you're
giving your CMS more time to do the
concurrent work that it needs to do a
lot of the phases are not parallel in
CMS so it has no notion of the CMS
threads getting behind and not being
able to keep up with the allocation rate
can happen and this is this is the knob
that you get you tune to manipulate
their basic tuning guidelines you know
size of param gem I'm going to mention
it a few times I would set the old
generation size to be two times the size
of lab data size and start off with
young generous size equal to the old
generation size as as before increasing
young generation size you know you do
that to to to handle higher allocation
rights and increasing young generation
size if you see promotion rates being
high this is kind of an inverse way of
looking at things than others this is
this is different this is this is how we
do it at Twitter and that that's why I'm
mentioning it here mainly because memory
is on the table in our configurations
for our low latency applications so so
you'll see in the examples that give
that that some of them have rather large
heaps if you sum something sometimes in
actually a more traditional approach for
tuning of CMS's to to to limit the size
of the young generation to have lower
young generation files of pauses so you
know you
it can go either way so young the size
of the young generation does does see
can influence the rate of or the length
of the pauses for the for the young GC
pauses yeah but when you're able to
control the rate of injection for your
application the allocation rate and the
promotion rate then you tune it up so
increasing the size a young generation
so if it suffers premature promotion
yeah okay you're gonna increase the size
a young generation too if you to
basically alleviate premature promotion
premature promotion is the notion of GC
is happening too often and my objects
are not dying young they're not dying in
a young generation if they just had a
bit more time they'd be able to die that
yeah so the way you influence that is by
making us a larger young generation it
also is so essentially that it decreases
the rate of GC and gets more time to die
you want to increase the old generation
size of promotion rate is too high so
you'll see concurrent mode for it can
become concurrent mode failures which
invokes a full GC if you don't have
enough space in yellow generation and
the can and the CMS threats are not able
to keep them so increasing the side of
old generations can be important there
okay tuning for latency for these are
this example of a pretty well tuned CMS
application you notice I have CMS
scavenge before remark on the second
line I'm on the fifth line I have CMS
occupancy fraction it's at the seventy I
believe that is the default now though
but I'm pulling from from older log I'm
also manipulating the survivor spaces in
the skin this case because I'm trying to
tell her that the activity of the old
gender and young generation to the
promotion rate but I'm sorry to the
allocation rate that I have and the big
takeaway is where this is is this is the
survivor a ratio tuning and in the size
and yumjong
we have one question for the slide
I so in later versions the JDK I'm
fairly certain that that changed and the
only notion is has gone away but I will
I will look into that I I don't think
that's the case because it's you know we
see a pretty big impact when it
manipulating this flag throughout the
course of the application so yeah I
don't think only is is part of that
anymore but I could be wrong
yeah that's probably worth looking into
yes
but what is high I mean obviously I
think it's it's so the question was what
is a high promotion rate and was a high
allocation right a high allocation rate
is essentially if you're seeing well if
you see premature promotion so so it's
it's directly related to the size of
your spaces and how big you can be as
far as the Czar's allocation rate is
concerned the biggest knob you have to
nip you late allocation rate is your
application so if you have if you're
seeing you know pre merger with
promotions you need to argue longer
young generation and the reason why
you're seeing that spirit river
promotions probably suppose your
allocation rate is high so what you'd
see is is your young generation growing
I'm sorry your old generation size
growing until it you know I had two full
GC or or CMS cycles in soon so turning
g1 for latency I'm not getting a lot of
detail here mainly because I have not
investigated it it to the point where I
can give equivalent CMS flags like doing
it the way we do at Twitter you know
kind of looking at when we have
promotion rates and allocation rates
under control I don't have the
equivalent flag set for g1 just yet so
I'm gonna give you an idea of what I
will look at and you know the general
approach for g1 do you want it should be
much simpler to tune you know
essentially you want to you know start
with really just setting the next GC
pause max GC pause in g1 I'm sorry also
the overall heap size you'll see you'll
set the equivalent heap size that you're
running with CMS to start with but I
would not set the young generation size
I wouldn't I wouldn't start with messing
with a survivor ratio is off the bat the
way that g1 tries to target its pause
time target the way it tries to achieve
as pause time to our target is greatly
by manipulating the size and young
generation and and it's asking in the
aspects within it so if you fix it it
kind of short circuits its adaptiveness
right so again one last notion for it
for g1 I mean you would go after if you
if you are if you get to the point where
you see it's not keeping up the rate of
allocation yes increasing the size of
young generation makes sense is the next
step but don't do that first and as
you'll see as you look into y1 output
logs you'll see how much time is spent
copying how much time is spent with
remember sets and you want to tune into
to to keep me get remember sent
processing and the time spent
manipulating remembers that's low you
want to see most of time span copying
tuning to consider for g1 initiating
hee-ha-ha consent % you know this is a
you know same as CMS essentially max GC
live threshold percent is the amount of
live objects in the old generation
before it makes collection occurs this
gives you an idea that this knob allows
you to manipulate whether it's really
whether you're going to be more
throughput mode or more concurrent mode
for for g1 he would keep waist percent
is that is really how much fragmentation
can you tolerate in the heap how much
how much lost space and mix GC count
tart count target is is really what is
it sends how it divides up it's
concurrent work you know and and that
that's one of the thresholds that tells
it how much concurrent work you're
allowing it to do um and and and is
actually the last three are all the same
type of activity if you take a look at
that MP oqr article by Monica Beckwith
she presented a couple talks on g1 here
in Java one that's an excellent article
to kind of jumpstart you looking at g1
for for tuning okay so here's a sample
set of tuning that that really is this
is where up where I'm at with looking at
g1 I to your compilation and increase
the code cache sizes cuz that's
important when you use tiered
compilation and that's pretty much it
you see I'm just setting the target the
occupancy and use using g1
okay tuning for throughput parallel
juicy that's assuming it's the most
efficient collector for fruits for
throughput and if you're trying to avoid
garbage collection there's a lot of
garbage collection schemes that can you
know can happen I would start with this
one as well because it's been can be
more around it is the mote it is the
most efficient garbage collection
wouldn't we know it's just not trying to
do anything anything cooker bulge old
generation need to be two to four times
the size have live data size mainly
because that even though it's a parallel
old generation collection it's still
pretty darn slow so you know you want to
kind of avoid that and give space not to
have a full juicy young generation is
usually three-quarters the size of the
heap often used with with aggressive
tuning aggressive absent ear compilation
and disabling adaptive sizing and tuning
as a cyber survivors basis directly is
really that's when the when you don't
have to last five to ten percent of
tuning really five percent of tuning
that's where you start to manipulate
those lugs here's an example of a tuning
set with parallel juicing notice the
size of the young generation is twenty
seven gigabytes and the size of the heap
is twenty nine
that's pretty heavily tuned for
competitive benchmarking actually really
wouldn't do that in production but this
is actually a tuned tuning set for JTP
2013 okay do you one for throughput g1
for throughput right now is actually you
can tune g one very similarly to
parallel GC to get pretty equivalent
behavior again g one still tries to do
some concurrent work and the overhead of
the young generation is lower I mean it
is lower than and then parallel old
however the tuning is is similar and one
of the approaches you know I can
envision with G one that there would be
a throughput mode for G one eventually
I'm gonna concurrent you know low pause
modems would you want here we got a
question up here
aggressive ops does nothing you're right
but the reason why it sits in there is
that it makes a little more portable if
you're using older older versions of JDK
7 and newer version of the JDK 7 that's
why I still have it in there
it also does what does it do now no it
does nothing you ready yeah
yeah no before okay yeah you asked me a
question yeah
it stopped it really I mean when we
won't remove from a jitter gate 6 - j TK
7 it did a few things and as we've gone
through the updates in JDK 7 at this
point in JDK 40 it's doing nothing at
all yeah I could be corrected in might
be some corner case they've added sense
since I've left but yeah notice that the
initial code cache size and the reserved
code cache size in this situation if
you're using tear compilation which I do
suggest that you do is going to be it
has a lot of potential will be faster
then then just see - alone but you know
you got to increase the size of the code
cache did you know otherwise it fills up
stops compiling and then you'll be why
is everything so slow yeah here's the
exit good ok tunings I label this long
slot is the same to do the same thing
again okay I'm sorry I'm confusing
myself tuning CMS for throughput you
basically configuring it to avoid
promotion you want to when you during so
so as I said CMS is rather brittle so if
you're really going to go after CMS for
throughput you're basically going to
tune it to have very large young
generations you're going and you're also
going to need to control your
application to limit promotion and kind
of clamp down allocation rate to enough
clamping down allocation or it enough
that you you know you don't have
premature promotions within CMS so
separation you know that having an
application design that separates
stateful and stateless will allow you to
know state like that spaces and Salus
applications you can target those more
to fit within CMS young generation
should be big just like just like
with with parallel old very large heaps
very very large old generations you're
basically using memory to avoid full GCS
you can tune the survivor spaces
manually to to make sure that you're not
promoting unnecessarily and here's an
example set of two linked tunics EMS
okay there's a typo and new size and
exercise they should be both 16 not 18
19 and 16 I'll fix that before I put on
SlideShare so that would in this tune
situation considering that the new site
should be 16 there would be only two
gigs available for the old generation so
that that's this steep cliff if you if
you cannot promote into the old
generation so full juice a and if you do
end up doing concurrent activity if you
if you lose the race that's a concurrent
mode failure so it just it's a very
steep cliff from CMS tuning for
footprint the stuff that though going
down this path is relatively new
you know I there's not I don't have a
lot of experience targeting from tuning
for a footprint well the general method
methodologies are pretty well known
we can't tune for through per footprint
until we are able to use the garbage
collector to ensure that we have low
latency we're not there yet
so once the garbage collector can sit
once we can tell the garbage collector
that my max pause is 10 milliseconds
where my max baucus is 50 milliseconds
and it actually achieves that you know
we're not able to we're not gonna be
able to trade-off CPU for for footprint
right so right now it's a rather
difficult thing to direct your mind
around and being successful but in many
cases if you're copying your tuning
parameters from one application to the
next there might be cases where you can
reduce the size of the heaps that you're
using so so old generation it also still
needs to you know you still have to have
enough size enough space in the old
generation so to type Don two times the
live data size start off with young
generation to be half the size of the
old generation so you know you notice
that I had it be equal before and
previous configurations it's half this
time because we're trading off fruit
more frequent young direct garbage
collections for reduced footprint the
strategies to reduce young and old GCS
independently until the maximum
acceptable end-user response time is met
so you have to have an undo but have a
in order to go down this path as well
you have to be tracking what's important
to your end users the response times
very very closely
if using parallel that's definitely not
low pause you're basically trading
higher response times for lower
footprint and lower through
so you know you're not you're not never
going to achieve low pause and reducing
footprint with paralleled here's an
example of the flags for this
configuration I got a typo there it's me
and she's not doing great
these tears must be switched
no MS NMX
I'm sorry MX &amp;amp; MN should be switched MX
should be 8 MN should be 4 now I'm
trying to make the young generation half
the size of the old generation ok
footprint for g1 so heap in this cases
can be 3 times the size of lab live live
data size mainly because we're not going
to try to tune the young generation
you'll want to allow g1 to adapt you
know and kind of settle on what's the
best young generation size increase the
parse the pause talk target will
decrease GC overhead and make you a bit
more successful in that regard obviously
it's the same strategy you're trying to
reduce the heap sizes with and keep them
down while you're maintaining your
response time so it's gonna be a balance
between decreasing your heap size and
setting your max GC pause Milius to
something that you know you think is
appropriate but you know if it's 5 look
if you're really you can you know if 500
milliseconds is where you don't want it
to go above you know don't set it at 100
milliseconds and try to go after yeah
reducing the heap size you got to be
reasonable and you know who's do you one
you know it's heuristics are still we're
still in flux and we're still trying to
settle on what's the best policies and
the edges are a little rough still so if
you're trying to go after low pauses
it's it's it's hit or miss so that'll of
the larger pauses are you're gonna have
more success with we had a question over
here
okay well the recommendation before was
was to set set of two times but then
also set the young generation as well
which mean you know and when you do that
it's not we need when you differentiate
the sizes in the young generation the
calculation ends up being a larger
larger heap right you're trying to I'm
setting this here three times the live
data size and not allocating any space
for the young generation so the overall
heap will still will just be three times
the size of the live data size right if
you take it I think I can well Larry's
here I'll just jump that quick but in
this case I'm saying to be two times the
size of less out of size but then
overall heap the young generation needs
to be half the size of that heap right
so it's you know bigger so if you're
live data size this is two gigabytes in
this situation yeah all right so yeah
it's actually it's the same as up the
settling of the same size I mean just a
different tuning here's an example of
that in g1 notice that the just sheer
amount of flags necessary for g1 yeah
once we're successful with that it's
gonna be rather a enlightening and not
have to deal with so many flags
okay moving to CMS parallel new and
coherent mark sweeper required flags two
times the live data size but the young
generation should be half the size of
the old generation alright so in the
case of a two time two gigabyte live
data set the old generation needs before
gigabytes new young generation needs to
be two gigabytes so your overall heap
size is six gigabytes with the young
generation of do that right you have two
gigabytes promotion rate needs to be
load enough so CMS concurrent mode
fellers you know you know and don't
happen you don't lose the grades so
that's where you know application level
code changes you know if you're really
gonna go after a lower footprint and
maintaining response time CMS really
does require that you have a handle on
your allocation rate your promotion rate
so that's going to be important strategy
is the same as the parallel j--
GC is to reduce to young and old GCS
independently until a maximum acceptable
use response time is met tune the young
generation first then the old generation
through CMS avoids chasing your tail bit
because the ratios change as you change
the size of your generation using a
sample of that tuning
the reason why we with a listed survivor
ratio tuning here for for CMS and I
listed in every case for CMS is that you
know it's it's one of the knobs that you
can manipulate how allocations are
handled so it's it's something to look
at okay all right we're doing pretty
good on time here I think I'll leave at
least five minutes for questions at the
end
common performance tuning performance
issues okay size of perm gen is an awful
lot in enterprise application
development sure it goes away in Java 8
but you know who's moving the Java any
times well nobody is but when it comes
out it takes a while to get there right
so so it permed the key thing that
remember is perm gen collects when it's
only collected at a full GC you know so
if you know if your perm gens not sighs
well for your application it's it you're
gonna just invoke full GCS unnecessarily
and it's never I mean it if you can see
right here in this example the last blue
line you know that that perm gen is is
just scraping it's actually in this case
it's larger than what the size or this
what's used is one kilobit away from a
kilobyte away from me overall size right
it's not you know so what happens here
is is an allocation app and that perm
gen happens it could be anonymous
classes and you know things would know
it's it most likely like jsut anything
that populates that the permanent
generation and I'm gonna get my example
because okay it's a little too confusing
but and and essentially if you get to
the point where you're scraping up the
size up to the maximum size for the for
the permanent generation and if it is
indeed full you're not clearing space so
you actually get into the cycle of full
GC after full GC after full juicing so
it's real important to keep an eye on
that and you can see it I mean if you
using CMS there's output to tells you
when I'm gonna happen if you do one of
the previous flowers that data that I
mentioned for for garbage collection
which gives the reason the cause GC
cause it will tell you that it's
Parham gem collection at least in later
versions of Jadakiss island so that's
the recommendation I have for for
enterprise application is just size it
big I'd rather see you size it big and
then see that you have extra space and
then reduce it then to have the full GCS
in production it's just not working it's
really not worth it
I think 256 megabytes big the largest
I've seen see the perm gen to be
configured is 512 megabytes and that was
a really a special case 256 should be
well beyond what you need in my opinion
that's kind of where you want to be you
don't want to you just don't want to run
into that if you if you if there's a lot
of churn a lot of change in your
application if there's a lot of
development that happens that goes into
production quickly tracking the total
perm gen and they used perm gen and kind
of give an idea and flagging and maybe
maybe give yourself a warning when you
get close would be a good idea besides
it size of code cache you know typically
if unless you're running teardown tear
compilation it ends up not being too
much of an issue however if you're gonna
run to your compilation and I do suggest
it frankly because it me no health store
applications start up pretty
significantly and essentially what it is
it's running the interpreter and then
running c1 decline compiler instrument
you know instruments code and then you
get better information for this for this
server compiler so you have the ability
to actually generate better surfer
compiler this sort of server compiled
code if you use ter compilation the
scrubbing of the other of the code cache
is a little is it needs work basically
because the code for c1 and c2 are in
the same cache now you're doubling the
size and it's running out of space they
are working on that I believe there is a
solution now I don't know if it's been
released I don't think it has but you
know the way you do that you get around
it is just tune it up so that's that
well I'm sorry what's your question see
in older versions that a JDK the common
side this is the problem is that you'd
see compiled failed if you did print
compilation so print compilation you'll
see compiled failed in Java 7
there was a sweeper a cleaning thread
and a way of cleaning the the code cache
the problem was is that it and what
basically what happens is you have to
profile your application and you'll see
the interpreter consuming higher levels
of the application in the profile and it
would be straight you'll see it right
away at the interpreters like lights up
and the reason is is that the sweeper as
it stands now starts kicking out any
code sometimes a kicks out code that it
has to recompile so it goes back
interprets an ink it's in a cycle loop
they used to be very rare now but many a
few several years ago if there was a bad
compile or he got an a compilation loop
you know where where we try to compile
and I had a T compile and ran into a
problem and tried it all over again
you'd see the same level of you know
hiring interpreter rate that's relevance
temporary right and avoidable completely
if you have the code cache size high
enough but they'll fix it it's just it's
really a sweeping problem and and it
just needs a better algorithm okay so
open JDK development at Twitter so I'm
gonna try to go quickly through this so
we have a couple of minutes for
questions ok so Twitter runs Java and
Scala on hotspot why hotspot because
it's the most highly optimized managed
runtime for any language at this point
frankly it's open source and we and we
we were able to realize massive
performance gain moving to to to to a
hotspot from other technologies we own
and optimize our platform we build out
our own diagnostic tools we build test
and deploy open JDK directly and we
optimize hotspot runtime compilers for
Scala and other implementations and we
are working on a tailored GC for
Twitter's needs specifically our latency
requirements are extremely low we
maximum response to maximum latency
maximum end-user response time of 10
milliseconds which is requires a bit of
creativity right now to achieve so it
but our goals is to contribute back all
of our work to the community we are
working closely with our with Oracle
Java development we're collaborating
through open JDK and we'll post our
stuff on get up - I mean twitter twitter
twitter really you know what we do to
deliver what we deliver our service - to
the public we want to make public to you
I mean there's really no record
restrictions as far as we're concerned
we're really just ramping up now we've
made some pretty key hires over the last
couple of months you can find you'll be
able to file follow us at Twitter JDK so
that's new I just kind of came up with
that and haven't set it up yet so the
meantime follow me tags Keenan and I'll
let you know or or you can see down in
the footer Twitter inge if you follow
that that username you'll you'll you'll
see everything that we're doing as well
ok so thank you very much
I can I can ask questions sessions
yeah upfront yeah so so the question is
what is the recommendation for books for
JD I'm tuning the best book to believe
the most modern the most complete one is
is Java Charlie hunts Java performance
tuning book I mean it's it's specific to
hotspot it gives you but but it also
tries to give you the fundamentals up
tuning and of itself you'll notice you
know I mean I worked with Charlie I was
you know on some of the content and
you'll see how that's reflected in how I
what I presented here so yeah Java
performance tuning actually no it's just
Java performance I apologize
yeah but chart charlie hunt is the
author and be new John as well question
I'm here okay
you allocating are you allocating very
large objects okay so it could be that
adaptive sizing is still is enabled so
if you you have to disable adaptive
sizing and it's use adaptive sizing if
you disable that then you should have
direct control of your survivor spaces
there's two flags though there's initial
survivor space size and survivor I'm
sorry initial survivor ratio and so
vibration I believe but there's one
that's tricky it's called target
survivor ratio you think hello that's
the tart no it's not actually what that
is is that the target capacity of your
survivor spaces so look that one up I
would avoid targets in vibration unless
you know exactly what you're doing and
if you understand it and it makes sense
for you then then it's probably good to
go but you got it you got to understand
what it does first and it's not it's not
what the name is poor so yeah question
here
I mean there was a notion of settling
now most likely what's going on in your
application is that you're using like
soft references or other references that
need to resolve there I mean when you do
a full garbage collection if it has live
objects that are pointing back to the
young generation and and they may be
dead that those objects have to stay in
the old generation until the until the
other till those objects in the young
generation die so sometime there is some
flux so you know I would do if you're
using explicit GC you probably have to
do several in a row to make sure that it
that those objects that are into the
young generation and if it's still alive
load on the application you know it can
be influenced in that so it's probably a
rolling average you have to look for in
that situation because I'm really the
only way you're going to come to a
steady-state if you have cross
dependencies across that the the
generations is really stopping injection
like you know is stopping work coming
into the system and then you can come to
a steady fate and know where you're at
but if you're not able to do that then
yeah it's gonna be a rolling average so
it's regression testing and it's at a
steady state meaning are you still under
load or is it is it just and then do do
do stuff roll and see where where it
settles that's that's what I would do P
and the reason it doesn't it takes a
little while because there because of
the cross to them because of objects
either being soft references and needing
other resolve their references before
they're before they're collected or they
have their other objects that point back
to objects in the young generation that
are still there yeah
yep well system GC will invoke a full PC
at that point in time there's also a way
of doing it when CMS I don't remember
the flag but you can look up yeah
exactly
as long as you get one thing you have to
make sure is that well if you're doing
CMS there's flags that that will that
will set may because you're using CMS
and if that's the case then there's a
flag that said that specifies whether
it's explicit or not and what how does
it interpret explicit GC I don't I don't
remember what that is you can you can
look that one up but if you're using you
know - explicit GC flag it will disable
that invocation okay then it most likely
its CMS and it's probably one of those
for us yeah
control that yeah yeah yeah exactly if
you take a look at you know if you just
do Java
- xx : plus print flags final capital P
capital F capital F I think that's right
it's basically dumps the state of all
the flags you know at the end of the
invocation of your JVM so you'll see
everything that it that's that set yeah
that was missed that was a mistake I
tried they should have been the same
max knew size and new size yeah if you
if you're doing that level of tuning
that we're talking about here know would
never do that because it's just it's
it's a waste of time yeah no no it's
only it's it's it the the generation
that you're resizing is that is the
generation it needs to invoke the GC so
young generation is pretty adaptable in
that regard so now it doesn't it's not
necessary you know if you're running a
server application I would set both the
yunyoung generation and the old
generation to fixed sizes so there's not
there's no as to resizing in between
it's just you know unless you're the
only
I said in the presentation the only
situation where you'd set the maximum
heap size and initial hip size different
is if that you're trying to you know
over-provision the system if you're
trying to run 96 gigs of heap but you
only have 64 gigs that actual memory and
you think you can get away with it but
you won't really you might for a little
while until something wakes up but you
know it's you know that's that's the
only case I've seen people try to do
that yeah yeah exactly
in certain on servers clients that's a
different situation yeah you don't want
to use only use what you need right we
have weight off up full GCS yeah yep
that's how it works now that's there's a
lot there's a lot to that though I mean
there's a lot of control on the changing
code so yeah
okay so alright so if your if your the
way you're if you're running out a
hypervisor with with with separate OS
instances and those in those OS
instances have binded CPUs meaning that
they can't be go beyond a certain set of
CPUs that they're running on I there's
not a lie I don't think there's a lot
that really matters because it's gonna
it's only the JVM is only going to see
you know what it sees within you know
what the aspect of what the operating
system has I'm sorry so if they're not
binding it then you probably want to
limit the number of GCC threads that are
run yeah I think that's the probably the
biggest thing you can do that being said
if in if you like in situations where
like high frequency trading and you're
trying to get big sub millisecond
response times a lot of time they run
those systems at utilization rates that
are like less than 5% sometimes less
than 10% and the reason why they do that
is they get if you're well below the
curve of you know basically response
times will kind of chug along increasing
a little bit as you increase your CPU
utilization and right around sixty
percent they spend one up a lot sorry I
think in the mic but what but if you get
down well below the knee in the curve
sometimes that tuning changes because at
that point the ratio between mutator
threads and CPU caches and what's of
what's available on the CPU is quite a
bit different so sometimes running more
threats in that situation is better but
that's not if you're running virtualized
you're trying to fully use more
efficient use the system more fully so I
think that's what you do is is clamp
down the number of GC threads also don't
do flags like use Numa at the same time
because it will know it doesn't know
enough to what's going on if you're
using C States or I'm sorry
yeah CC sets or creating the term yeah
that's right
and yeah it's that's a little different
because that gives partial observability
into the complete system and in that
case you definitely have to tune your
thread counts because the JVM will just
think it has all the threads that are on
the system and you'll have you know you
will be competing for CPU research
during garbage collections so okay
I would use to use Nemo when you're
running on an uma system first off using
parallel old garbage collection it's the
only one that really works with it right
now
I think g1 will be soon no it's not
default on not CMS or pÃ¤rnu is the
parallel parallel young generation
collector for CMS parallel old is just a
throughput collector it used to be just
parallel GC parallel old is on by
default at William M Numa wouldn't be on
by default because it ends up being
counterintuitive if you're not running
out of new terseness system what it does
is it chunks up the young generation
into the CPU the local CPU Neuman owns
right so if you're running on a 4-way
you have four you know regions of the
young generation that those and those
threads that those T labs which threads
are allocating into you will stay local
to those regions all right so we have
local allocation and young generation
but in the old generation that doesn't
really happen
so we interleave across the across the
nodes in the old generation so the old
generations allocated interleaved across
the Numa nodes and the young generation
is spread you know across the new
Minoans and allocated locally the reason
why we do that is that the the heap is
allocated at initial initial ization of
the JVM and an operating system always
say okay I'm running on a Numa system
you just you know you're starting your
thread you're allocating memory let's
put all that memory next to that
starting friend so if you don't use you
Numa use Numa or you don't enter leave
your memory on a Numis system you're
basically your your heap will be
allocated on one Numa node most likely
and let depends on you know what the
configuration the operating system is
and then you know N minus 1 well
basically yeah I'm not going to try to
do the calculation quick on the fly here
but most of your allocate most of your
memory accesses will be remote in that
situation
yep and then every every memory access
is going across the memory bus exactly
that's that's that's the benefit nun
doesn't do anything it just not if the
collector doesn't know how to do it it
could that's actually not a bad idea
maybe they should do that because then I
would least be interleaved ride but no
it hasn't been Oh does use Numa
interleave the old generation if you're
running CMS that's not a bad idea though
John okay good
excellent John is the one of the GC
architects you can also ask some
questions or isolating those or the GC
threads to a socket and your application
threads too you know dividing it that
way yeah well you can't do it like
manual they can't do it on the command
line but you can do it by interesting
you know inspecting your threads and
doing it that way but that's that's
really corner case stuff right there
numerous systems right now I'd say
modern to to modern Intel processors on
a two-way less than 5% it's really not
that much not a two-way four percent
you're getting close to the four-way
close to 15% yeah you as far as
difference and overhead yeah I had seven
easy oh yeah you can do that just by
doing numr control
- show you know I actually I don't I
don't remember what it was if you really
look at the Newman control tools on
Linux it actually will give you a
diagram of what a local access and what
a remote access
you know what those timings are on that
system you're talking you know probably
a magnet 5x magnitude difference between
the two which is not terrible when
you're talking about a couple
nanoseconds frankly so yeah but that's
really from memory I don't remember I
just you know I don't remember what they
are I look them up any other questions
both may I mean right now it's it's it's
it's both yeah I mean the efforts just
starting and we've been in we've had
designs that we've been considering for
a while but you know we've we've we've
hired the critical mass at this point
that we can actually start actually
heating so good good show I'm sorry
yeah it's openjdk and we have every
intention to work with John and his
colleagues to get them all back all this
stuff back yeah yeah totally
so so the question is like management of
GC vlogs I think and what do we do well
we have our own what we do at well I
mean it's a probably if you're going to
post process and try to queue like you'd
like understand the state of an
application over time that's a big
problem it's a lot of data um you know
there's various different ways to do it
right now the most common way I've seen
is really only keeping a certain amount
of days worth of data and then and
because for one it gets unmanageable
from a space point of view but also from
you know what do you do with all these
GC logs and correlate and timing
differences and so forth you know I
think that we think there are some tools
out there that that manage that but I'm
not terribly familiar with it I would
you know actually I'd walk the floor and
see if the guys that do system
management and system using system
management tools they bet I bet they
would well somebody's get a solution ya
know Christmas
it is a difficult problem developing
workloads that kind of model what
happens in production can be a kind of a
red herring path that take right I mean
it's it's something because things could
be so different in production and
sometimes you know you know standing up
your production systems in it you know
you know something that would be
actually equivalent is nearly impossible
so you know what would the success that
I've seen is is trying to get enough you
know it's state of the application tied
with the garbage collection logs and
getting an idea of what may have
happened but there's there's a gap there
right there's not a lot of profiling
tools that give you those two EEZ full
stack analysis there's not a there's
very little allocation profiling tools
that I've you know seen be successful
and actually accurate there's BCI tools
sure but which is bytecode
instrumentation whether you know you can
target a little bit of code and find out
what's going on but yeah there's gaps
there so there's not a you know if
that's a that's a big problem and I
think you know that that's that's a
problem space that we'll see you know
starting to see some activity in but I
don't have an answer for it
tuning on production no no it's it's
it's it's we we try to like I can't give
it any obviously I can't tell you what
we're we're exactly what we're doing
right so but but it is if you can find a
way to non-disruptive Lee do it in
productions non-destructively is the key
point there right that would be perfect
right now that's that's the kicker so
you know I think that would be cool
gonna help you also know you can figure
that one out but so that's yeah yeah
there's a lot of folks out there that
spend a lot of time doing that trying to
model application square correctly okay
oh we're I don't think we have anything
else right now so nobody yelled at me</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>