<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Best Practices for Evaluating and Optimizing Java Applications for a Cloud App Stack | Coder Coacher - Coaching Coders</title><meta content="Best Practices for Evaluating and Optimizing Java Applications for a Cloud App Stack - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Best Practices for Evaluating and Optimizing Java Applications for a Cloud App Stack</b></h2><h5 class="post__date">2015-06-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/y0U7NxKFA5Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you everyone welcome to this
session the session is talking about
some our best practice for evaluating
and optimization optimizing in severe
GBM operations in cloud application
stack okay so first is some introduction
for myself I'm working in IBM java java
technology center for three years and
malia focus on the JIT compilation works
and my recent work a focus on the
websphere liberty performance tuning
also some multi multi talented JVM
performance work okay hey Sakura
so it was supposed to go of such talk is
I introduced some some new performance
metrics in cloud environment that is
important for cloud the first eight
density the second is a warm up time in
cloud and our some best practice to
improve system it metrics from JVM from
IBM's JVM perspective here is the
overview of today's talk first is some
introduce of the new matrix performance
matrix and the second how we increase
the density in with JVM and how we
decrease the warmup time in in trivia
finally at the end is some summary and
our next steps we might have take 45
minutes and my house 15 minutes for the
questions okay so first is some
JVM application performance metrics
there is some classic classical matrix
for example the throughput response time
and Slapton memory footprint and some
combination of them in cloud environment
in a very simple a cloud application
stack graph as a in the you know in the
bottom there's the hardware and the
native OS that is shared between the
applications and the service some VM or
OS containers that isolated resources
and behaviors of applications running in
cloud and within the container series
traditional Java JVM application stack
series application middleware and JVM so
what will was a new requirement for
performance matrix in cloud we need to
understand what we want to achieve
through cloud the first is always do
more with less
so first is decreased cost by sharing we
in cloud and the second is that in cloud
we should be able to elastic computing
and we can always get our computational
resources and vary in time so here is
two performance matrix in cloud first is
more familiar with most of you for the
density it's a number of the JVM
applications that we can run in same
hardware and it's mainly limited on the
hardware resources like a memory CPU and
i/o and when talking about the density
with milli focus on the memories usages
and high density means as a low cost to
run applications in cloud and second is
warm up time
JVM needs to profile and compile methods
at the wrong time so it takes time from
the application start to you get to the
expected a performance for them to peak
performance and it's different with our
classic start out time or the deployed
harming cloud it is how GBM can quickly
deliver the performance when there is
some computation resource elastic so why
this is more important in cloud because
he clouds airs on the mount
computational resource allocation or
free when we are going to like a double
our computational resource we want to
see the performance is just doubled as
expected quickly not wait to the JVM to
warm up like it spent five or ten
minutes and series also will be some
work love the movement happens in cloud
due to some like a cost or efficiency
reason efficiency reasons we want to
move the application server from once
one machine to other machines and we
want to get to the performance on the
new application server as quick as
possible so with less one month time it
means we will have good user experience
people can always when we are allocation
resources so we could also get to the
performance immediately and we it was it
is also possible for us to launch
applications but to handle some
shortened shortened workload the boost
such as that series some online flash
sale in seconds when somebody there's
millions of requests coming if you are
launching a JVM application and it goes
peak very slow say it is not it is
wasting time to when the when the
Trivium is or when application is fully
optimized the same all the workload is
only or is already finished
okay in this benchmark I will use some
been in this presentation I will use
some benchmark to illustrate our works
to improve these two matrix so we use
the Liberty and the tape trader plus as
our benchmark Liberty is a lightweight
application servers we use the IBM
bluemix and day trader is a standard
benchmark running on Linux Linux at x86
and with IBM jdk 7.1 so and the day
trader will walk loader has 50 clients
as I said in this benchmark okay
here is the first topic how we increase
the density in JVM there is a lot of
working OS containers or some like
talkers or Wardens to increase the
density in the OS container levels
saying what is what can be done from the
Chilean perspective the first is always
share classes share classes is a feature
introduced in Java SE file and it has
just classes it is just class memories
between multiple JVM instance then it
can reduce the memory footprint for the
classes and the second technology is our
multi tenant - JVM it's mainly it can
run multiple applications in same JVM
instance IBM provides a multi-tenant to
support multi tenant the JVM in our jdk
as a tech preview and you can get more
details from the multi tenant a JVM
community you can have a try it down and
see how it how it runs and how it
benefits the like a density there's also
other techniques in in JVM to reduce mod
memory footprint but we just briefly
introduced here this is a compress
reference we use a little bit
object reference means for the 64-bit
JVM
it saves heap space and save also JVM
native space okay it's a it's a general
technology not especially
yeah and there's also some we can add
adjust hip size and the GC policies to
reduce number footprint the rule is
always we spend times in G seat and the
reduce the keep memory footprint like we
can reduce the ten-year space or like
increase the threshold for heap or
expanding okay
here is the how how share classes
reproduce the number of footprint share
classes can be enabled with - ex share
classes options JDK and it is introduced
and it is already designed mainly for
improved use the included startup
performance and not to make a target for
the memory footprint issues so the T for
the share classics cache is small it's
only need to cache classes loaded at
start start a face and in Liberty and
the day trader runs D for the shared
memory for the printer is like only
takes 11 megabytes and the total
application is 320 megabytes so it is
very small portion so we in like so we
need to adjust the share classes for
cloud to for the memory footprint issues
so being larger class cash for the
density purpose and you can use option
XSS cm x to enlarge the share the memory
cache and with with was a big share
class Cashman footprint can reduce a by
so if I'm megabytes
it takes about 10% of the memory
footprint so and the next is
multi-talented JVM I'm not sure if
someone has attended a high density JVM
introduction yesterday I give a quick
introduction here multi-talented JVM is
a you can run multiple Java applications
on one JVM instance and you can see the
diagram that the series two applications
running and the co common classes are
shared in the JVM and to see how see how
their own copies of statics and class
initialization status for their common
Posse are shared classes and and making
and to guarantees that the applications
that do not affect each other it also
makes certain special classes and a
method that be tenant aware such as
system tour exists it will only exist to
the tenants that invokes this Cisco also
it's a used resource consumption
management SEM to isolate the resources
between the multiple tenants so memory
footprint and merely reduced by sharing
the JVM the native is shared and the
classes and the cheetah compilation
result is also shared and it can also
share some objects like some common
string objects between the multiple
tenants it can also share yes all the
all the static fields need to be
isolated it's an internal JVM
implementation we keep array for all the
statics and we keep the index in the
original places daughter static static
field so it's the index is sent across
all tenants but we can use the index to
that's different data's in four
different tenant okay so in multi tenant
a JVM memory is reduced according
depends on how many classes is shared
there's different levels a different
level class sharing can reduce can
results in different memory footprint
improvement currently IBM dedicate we
support we share the java class library
and you can see the most left leftist
diagram when we launch a new tenant will
share the java class library it's only
keep its own copy of isolated data heap
space threads and middleware
applications class and the compilation
result and at the right right side when
we have shared all the middleware and
all the applications then each tenant
each tenant to only need to keep the
isolated as data and the heap space and
its own threads so it's a big saving on
the memory footprint this page is our
multi tenant to JVM normally photo
printer results it's you can see with
the regular JVM when we launch a Liberty
instance for day-trader it takes up
about 300 megabytes and when we run the
share classes share the JCL
multi-tenancy JVM when we launch a new
tenant it only increases like 200
megabytes for printing increasement
you me we what if another tenant I need
to use yes its key system property is
readed and initialized some status
fields in the crossover usually and we
keep two copies of studies so they are
separated yeah yeah and for the shell
and the when we share the Liberty and
the day trader memory footprint is
estimated according to the memory
breakdown we we we show the in last
rites and so if he sees that because as
most classes and as most classes is from
the Liberty middleware so when we share
the liberties there is a huge drop in
memory footprint they can't down to 77
77 megabytes and when share older all
the classes it is six to eight megabytes
so it totally like near 80 percent
degradation in the performance in the
memory footprint and with all the with
all the em with multi-tenant or the
optimal density is only depends on how
many his memories it used for example
when we shared our JCL liberty and a day
trader we see that the in sixty eight
megabytes most of them sixty sixty-five
is a heap of space
so here is a quite simple summary of the
application densities share Casas is
used to reduce memory between the
multiple tenant and multiple tenants JVM
can significantly improve density but it
depends on how many classes can shared
between tenants so optimal model would
be that in the we can use a share
classes to reduce the number for print
across the multiple JVM and they use
multi tenant JVM to reduce the memory
footprint in same OS containers now we
start the part of how we decrease the
warmup time in JVM service this is the
technology we use is Ã¤Ã´t ahead of time
compilation it is so Sarah is it catches
JIT compilation resulting in early runs
and in later on city uses a pre-compile
the cheat code so it has skipped some
profiles and compelling in the linear
later runs the second is still the
multi-tonal JVM the theory is that we
launched an application on a already
warmed up the JVM not a coda JVM there
are some related techniques to with warm
up any to to some craft clarification
here the first is a yeah you have to you
can use you you can use different
application it depends the benefit it
depend on how much classes and master is
shared for example
some some german parameters is shared
some JVM parameters is a 10-inch scope
you can specify like for example the
heap space you can spit specifically for
each tenant and for some options like
 or entire GC policies you have to
be shared across all the tenants so the
first is a QuickStart QuickStart is only
used for is it's mainly is decreased
jitter optimization levels in class the
loading phase so it cannot help the
warmup time it helps the startup and the
second is there is some investigations
like how train applications before
launching the real workload but this my
needs some modification to the
application and it will changes as
application status when there is real
workload okay first is ahead of time
compilation still the default a odious
is for staff performance it is the early
design that for startup so the issue is
its casually small it only crashes
compilation result for law and media
optimization level it had under cache
other high level jitter compilation
results for the performance reason
because LG has its own own overhead in
performance and the default a ot cache
size is small it can only accommodate
comparison resulting in start-up face so
it's a from the test our measurement it
tunneling improves at warmup time it
helps startup time so what we have done
is we simply we turn enlarge the Ã¤Ã´t
cache we cache as much as possible the
alt compilation results and later and
below is some options we have set we
enlarge the share class cache and IOT
cash and some jitter data cache
but we haven't a turn on IOT for
high-level commemoration yet so so test
no result will be combined with multi
tenants results so in multi tenant at
JVM new tenants common classes are
loaded and methods are already compiled
so new tenants need to compile its own
tenants called method for example when
we share that java class library the new
tenant need to compare its own
middleware and application classes and
when we share the middleware it's only
need to compile its own application
methods and when application is also
shared and application is same so it's
most likely it's done to need to compare
a lot it can get to the peak per moments
immediately ok here is some warm-up
result for liberty and the day trader
the base baseline is regular JVM we
because the peak performance a different
instance mode so I have a license or to
100 and I add a full workload around 30
seconds and rest two seconds for the
regular JVM it might takes like 300
seconds to reach the peak performance
and with Ã¤Ã´t it can reach half half
performance quickly because most normal
compared master days is cached and it
can load the compilation results quickly
but it'll still goes to peak for moments
very slow the reason is that current IG
VMs do you need to recompile the Ã¤Ã´t
version to an on alt version from the
performance through portal performance
consideration because LT copy is not
fast enough sand in our alt copy and the
green one is a multi-tenant who is
shared across library
we can see that it's a speed-up much
faster than the regular one but it and
the and it takes about 200 seconds it
saves about about 30% to warm up time
and the shale Liberty and the sheer day
trader
results are estimated based on how many
masters is kinetic imposing Liberty
module and the day trader modules and
how long they can get enough profiling
and a good compound when we share the
Liberty the estimator is it's going to
take like one minutes to reach the peak
performance and when we share all the
classes the peak performance is going is
like two getting getting really fast and
the only time it need to spend is some
class initialization time in new tenant
because every tenant has its own needed
perform its own tenant class
initialization okay here's just a warmth
our warm up towns summary with
aggressive oil T we have sees that 30%
of performances immediately and that
still still slow to get to the peak
performance so we can use a ot
aggressive a ot to improve warm up time
when launching new JVM instance and with
multi tenant Jane JVM warm on time is a
decrease according to how many how many
method is already pre compared in the
JVM and for first attendant to warm up
it can not it's still need technical
select a or T to help and back to the
earlier training application
examples there could have been new
scenario seeing multi-tenant for example
we can launch two sim applications in a
multi-tenant JVM we train the
applications in tenon to number one and
when there is real workload comes we can
put the workload on 10 application on 10
and number two the real application on
10 number two is still clean because
they are isolated and it can get a big
Pomona's immediately so this means that
with some with this new technology we
could have designed some new scenarios
for some like something like training or
related stuff
yeah bloomix use water now yeah I'm I'm
mainly for yeah yeah yeah I'm not quite
clear but I would like to stay a Cistus
is performed are not to improve mix it's
mainly focus on a JVM part we just
around the application in the JVM and
see how it can include oh is like a
multi-tenant and improve the ways a ot
not performed in bluemix
okay there is some next steps first we
need still need to innovate with bluemix
to provides a high density and quick
warm-up JVM around hand and design some
new scenarios if we we can utilize some
features like for example like MT and
they can fully utilize the quick warm-up
or the high density capability and next
we need to enhance the share classes and
the LT in cloud we need to define design
a more aggressive or more suitable for
cloud environments share classes and
ahead of time compilation because
currently implementation is mainly for
the staff and it it cares about the
throughput performance not not fully
designed for density and a warm up the
last CR still need enhancement
multi-tenant JVM his we need a more
powerful automatic refresh theory we
still cannot automatically Sharon
Liberty now so Liberty is our highest
priority as you can see that it share
Liberty it can contribute the most for
the warm up and for the density and also
we need to decrease the performance
overhead for multi-channel the JVM there
is still some overhead in the
multi-talented JVM in performance
it's reported overhead because
multi-talented JVM has need to do some
isolation its need extra overhead to
reference statics and also it has a
resource consumption management to
isolate the machine resources between
tenants and it needs the psychos in JVM
ok I think we go really fast today so
yeah
the pocket is actually down with gbmt I
in turn interface or something
we have testing for GMT I interface and
currently says it is not isolated yet
for example when you are referencing
which which object a reference the
string it could be in mod from the
object from multiple tenant it will
explore some such internal information
now it can't be modified as technically
to separate the debugging information
from tenants
no it is 1/100 the Java capability it
has some limitations you can find it in
the IBM multi-tenant a JVM community
community for them some like HIV genuine
support and some like GUI support is not
supported for multi-tenant it can
isolate some some kind of arrows not all
errors for example the auto memory that
can be tenant is specific it will not
crash the JVM and for some like Jeremy
internal like segmentation thought it
cannot handle the internal scope yeah is
that all this micro system exists all
like other members through need the
tenant aware processing in Nigeria
it's a part of them is measured for
example the share classes gog gog CEO
test days is currently supported in IBM
GD k and share liberty and the day
trader is not supported we just do the
memory breakdown or the message
breakdown and estimate how much time it
might need
it's not implemented we're trying to
deliver this features on bluemix to
improve the cloud experience yeah yeah
yeah yeah so there is a lot of changes
for example we we shouldn't use JVM as a
virtual virtualize the much virtualized
them I always saw machine yeah this is
speaking is that multi-tenant is is it's
designed for cloud yeah it's it's like a
JVM container not always yeah it's
provides another level of virtualization
I haven't think about it yet yeah sorry
yeah
yeah yeah
you means that how how is separates the
resource like a file limits like you
know JVM or yes with multi-tenant and
you can specify we can turn the another
support our limit yet but we have
support like so as a limit we can
specify how many strategy you use in a
tenant and like a CPU limit HIPAA memory
limit so it's a I think it's doable you
multi-tenant follow the current resource
consumption management design yeah I
didn't catch clearly about your
questions maybe you can totally okay
yeah
yeah yes
at least you can share the thumb cut and
the you means that you you are have to
Tomcat applications and yeah yeah oh I
see you should say you have one a to
application seeing one Tomcat server it
stunts knee for example you can use if
you are deployed to Tomcat and used
deployed to wasn't to Tomcat are
multi-tenant I think it is from memory
footprint it can it will it would be
similar but the multi hence you have
some features like I'm not quite
familiar with the Tomcat so Montana can
we manage the resources for each
applications and it can have
opportunities to share classes between
two applications because the Tom Carter
uuuu explicit a share the Tomcat but no
it's a tangent note again if the classes
shares that same I come from the same
same class loader
yeah SIM card holder for the share cost
maybe we can talk later and I'm not
quite captured your question yet yes
there's a demo seeing the community
multi-tenant JVM community service
I think there's there most for like
we're launching how we launched multiple
process and how we there's some Rio
tater on the community yes
I see there's the demos for like
resource consumption management and I
see
I think world war attacker has similar
work for the multi-channel support yeah
so thank you everyone
what hi hi I Oh density I I'm not I
haven't I'm not familiar but I heard
some someone yeah be amazed working like
we if the Iowa happens on the same hard
we're all in the same same near machines
it will have some optimizations like we
do not go the outside I oh we just have
some internal links between to jail and
instance like we can pass some Isle Isle
Isle data it should not yeah okay thank
you everyone</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>