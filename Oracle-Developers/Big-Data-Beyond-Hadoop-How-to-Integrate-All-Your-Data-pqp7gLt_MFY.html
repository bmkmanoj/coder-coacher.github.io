<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Big Data Beyond Hadoop: How to Integrate All Your Data | Coder Coacher - Coaching Coders</title><meta content="Big Data Beyond Hadoop: How to Integrate All Your Data - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Big Data Beyond Hadoop: How to Integrate All Your Data</b></h2><h5 class="post__date">2015-06-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/pqp7gLt_MFY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to my session today big data
beyond Hadoop how to integrate all your
data before I begin the session I want
to introduce myself shortly my name is
Kai vaina I work for talent we offer
open source integration solutions like
Enterprise Service bus data integration
data quality BPM and so on and if you
have any feedback for this session I
really appreciate it you can send me an
email or send me a Twitter message or
contact me via seeing or LinkedIn social
networks of course you can come to me
after this session too so I really
appreciate every feedback okay let's
begin with your session first usually I
start off the key messages of my session
I've three key message as the first one
is you definitely have to care about big
data in the future to be kept
competitive I think this is really clear
that big data will be very important in
the future and to really use the
benefits of bit data to get a business
value you have to integrate different
sources and that's what this talk today
really is about all that you combine the
different sources and use the value out
of it and the third key message is that
big data integration is no rocket
science or at least no longer rocket
science because there are really cool
frameworks and tools available which you
can use which helped you a lot with
integrating different big data sources
and that's what I'm talking about today
and so let's look at the agenda at the
beginning I will talk a little bit about
a big data / ticking and the challenges
of big data and they will talk also
about some real world use cases of big
data I do this so that we have the same
understanding of big data and of Hadoop
and so on and I also do it because I
will later show these real-world use
cases when I show my examples of how to
integrate them and how to use them in
practice and afterwards the main part of
this talk of course is the integration
of big data sources into a tube and here
i will show you open source frameworks
and i will also talk about open source
suites which will support you by
integrating all this stuff into your
Hadoop cluster and finally I will talk a
little bit I'm very shortly about how
you can create
big data components for these frameworks
and tools you can use so let's the
agenda let's begin with the big data /
signal shift and here I really just want
to shortly talk about what's big data
and why should you care about and I
really love this side here so um the
side days if you can measure it you can
manage it and I think that's really a
good reason why you should care about
big data because it makes sense to make
decisions data-driven and so in the past
it was the problem that the highest
person highest-paid person's opinion was
very important because nobody could say
anything else because you had no data to
make a decision and now the gut feeling
is no longer justified because now we
can integrate all these different data
sources and analyze it and that's what I
will talk about today and I will show
you the tools which you can use for that
and oh that's really a good reason you
should really do your decisions
data-driven in the future and not just
with gut feeling like many people did in
the past and so I also want to define
and what is big data when we are talking
up with big data today so and I think
this is very important to understand and
the definition of this three wheeze um
it's really excellent in my opinion to
definite define big data and though what
of course everybody's talking about
volume so when you're talking about Big
Data you talk about gigabyte about
terrified of petabyte know the volume is
very important of course but the volume
is only one part when you are talking
about big data also same important is
very bright arity so you have to take
data from many different sources to get
their most business value out of it you
have to integrate all of these four
different data and combine it and then
analyze it that's how many use cases
where really this is the business value
how would you get out of the data so you
have to get the data out of social
networks and blog post and logs and
sensors and so on oh that's really
important and the third part is velocity
so of course people one
to get the answers very quickly so the
rush is real time it's so even good if
it's near real time that's definitely
not possible in all cases but that's
really also goal of big data and what's
also very important and it's the fourth
movie that's the value and that really
is the road to opportunity so we are
always talking about big data from a
technical perspective but really one of
the key messages is you should start
business driven me you should really
think about what do you want to achieve
with this big data analysis don't just
out with a dupe and put your data in it
and you don't know what you want to do
with it probably the project will not be
successful then so that's what I want to
define big data at the beginning and now
let's get to remote technical parts and
why do we want to do it and how do we
want to do it and here the problem is in
the end everybody wants to analyze this
big data stuff because we want to get a
business value out of it but before we
can analyze this big data we have to do
as a major tasks before and today I will
talk about the first two that we have to
do this big data integration so we have
many different sources with big data
stuff and different data sources and we
have to integrate it and combine it and
then sometimes you have to manipulate it
so you have to sort it or filter or
aggregate and that stuff and that's very
important tasks which you have to do
before you start analyzing it and that's
really also the main part of this talk
today so we have to integrate all these
different data sources and combine it
and manipulate it before we can do the
business analysis out of it oh that's
really very important thing to
understand because many people just talk
about it analyzes but you just have to
get your data before you analyze it
right and now as I said I won't also
introduce some some really well use case
at the beginning because I will refer to
them later when I showed a different
tools and frameworks which I use to
solve these problems and if you use
cases here the first one is one from rec
space and all of these use cases have to
reference at the bottom so you can also
read more details about it later if you
want here I will just explain it shortly
what they did and in this case I'm rec
space as a system male trust and where
they have a lot of different block
messages and they really want to analyze
this data and with typical ETL tools and
data warehouses this has not been
possible because it was too much data it
was really big data especially about
when we are talking about the volume and
though what they did say pushed all this
data into her tube for several reasons
and the main reason is the performance
win though they were now able to do
nightly MapReduce jobs to analyze this
big data logs which was not possible
before if their systems before and they
also now can do ad hoc queries I talk
query here does not mean real time but
it's fine if you have to wait 10 minutes
or 20 minutes or maybe an hour for an
answer but at least it is possible to do
some analysis with this log data which
they have and now they are really happy
with it the second example is t-mobile
they use big data to deduce their
customer defections and what they did is
really one example which many many
different companies do when they get
business value out of big data they
combine many different sources again I
will say it more than once today you
have to integrate different sources and
that is what t-mobile did they
integrated their messages from Twitter
and from facebook and from their
customer relationships of course also
from internal systems at the most but
I'm they combined all of this data and
analyzed it and though they could really
deduce their customer affections by half
so many people did not go to Vodafone or
any other company because a tmobile
analyze this big data stuff and
contacted these customers probably made
a good offer for the next two years or
so and though they could reduce the
customer defections so that's another
use cases where really a lot of
different data sources have to be
combined the third example is macys
probably in the US it's very well known
it's a huge department store they have
about 800
at stores in the US and what they do and
they do it for every store they have in
the US and every city day of a Mesa
store they analyze the different prices
from the competitors so when you are new
york they go to their website of the
effect competitors and to other sources
and check their prices and it went in
one city a computer is a lower price
then they also reduce their price but
only in this city and they do it for
every city and every night and you can
imagine that that's a lot of big data
and once again from many different
sources because I'm their competitors
don't offer them a rest service to
analyze it and they have to probably
implement their own integration to that
drive and though once again many
different data sources a lot of volume
and we have to integrate it to analyze
it and it's only possible with this Big
Data technologies like a dupe because
otherwise it would take weeks and it
would not help because you have to do it
in somehow us right as I stated here in
less than two hours they can do it and
the fourth example it's a little bit
another one it's not about performance
or predictive analysis but it's just
about storage for a compliance for
example so I'm this example is from
global parcel service in combination
with T systems and they had a problem
for compliance they had to store a lot
of data so here we are really talking
about terabytes of petabytes and the
problem is they could not store it to
tapes because they had to do queries
afterwards for compliance reason so
usually they don't query the data but
they have to be able to query it so
sometimes I don't know all four weeks or
whatever they have to do one query to
look for them data for compliance reason
and the solution again I'm was to store
it on Hadoop cluster because that was
very cheap but nevertheless you could do
queries on it right so you do they did
not need real time here but you can do a
query and wait two hours or so that's
fine here but that was really a good
solution for them because I'm Oracle
databases or whatever too expensive and
tapes were no solution because they also
had to carry it easily
oh that were for use cases I will once
again refer to some of them later when I
show the solutions and how we can solve
to integrate these data sources to
implement this use cases and now I just
shortly want to talk about the
challenges of big data because also am I
see this a lot at customers and so again
we offer open source integration
solutions for customers also for big
data which I will show later and some
challenges we see it every customer and
they are really very important to
understand to know that you can
nevertheless solve your problems and the
first is that just definitely limited
big data experts so I'm probably also in
your company if you are not working for
facebook or google or Cloudera or
Hortonworks then you don't have so many
experts and that's really a problem
because you nevertheless want to analyze
different big data stuff to click stream
analysis and implement recommendation
engines and so on and so you need easy
tooling and so on to do that the second
challenge is that you when you have big
data and you have poor data quality then
you have big problems right so
nevertheless even if it's big data you
still have to validate this data and
maybe transform it or use any data
quality algorithms to improve it and so
on and here again you can write that by
hand or you can use good frameworks and
tools to solve these problems easily
also without huge big data experts the
challenge if you want to select a tool
but you want to buy it which should help
you I'm from a business perspective then
you can look out and say a my industry
is banking or telecommunications where
is this tool and probably you will not
find a tool which can help you to really
solve your business problem without
integrating your data so even if there
is a window of any analytics tool they
still don't know what data you have for
data structures what interfaces for
technologies so you still have to
integrate all of that stuff and that's
the same for for a tool selection for a
technical perspective so
there is no big data product which can
integrate with your tool so with your
technology with your data formats so you
still have to do the work by yourself or
by consultants which I be of course but
still it's a lot of work and you need
good frameworks or tools to help you
integrate all of the different data
sources into this big data cluster right
and to solve these challenges as I
mentioned it am you probably will not
reinvent the wheel right all this by
yourself the Cheney will not come so
there is no Cheney which will give you a
good solution so in the end you still
have to learn and understand how can you
integrate all of this big data sources
into your pic data cluster to analyze it
later and that's what we are talking
today and about in this talk and there
are many key messages here too so what
word many different richie researchers
set or what i am saying here is not my
idea but I freitas in different articles
from different experts often the simple
models are the good ones which you
should use for your big data stuff so
you should not start with the machine
learning and complex stuff as you will
see later in the demos which I show you
it's also possible with easy tooling for
many use cases of course not for all use
cases but often that's the best solution
to use easy simple models and also
sometimes ms these researches have
thrown it's also good to let some people
from outside on industry look at the
data because they don't have any idea
about data and they combine the
different data sources in many strange
different ways which you would never do
because you're an expert in your
industry and this often solves many
problems or gets new business value out
of it and so we should be creative and
look at many other use cases of course
like therefore once I've shown you in
the beginning and then we should think
about how should we combine this data
and that's really the main point here at
the end you have to combine the
different data sources combined it
integrated manipulated play with it
that's
resolution which we want to solve and
that's what I am showing you today so
again thinking about business
opportunities first then choose the
right data and combine the different
data sources and the third part is the
easy tooling and the easy tooling is the
part which I'm talking about now today
which i want to show you how you can
really easily integrate this different
data sources so again when we talking
about big data from a technology
perspective now different alternatives
which are available on the market and
well let's take a look at different ones
because the question is how to process
the big data and the first solution is
parallel ETL tools and that's really
just one slide here it's really an
interesting article my parallel ETL
tools are the wrong way because these
tools have to send all this big data
over the net to compute it and that
doesn't make any sense at the end and at
all so we can use these tools anymore
with these big data stuff because we are
not talking about megabyte or of
gigabyte we're talking about terabyte
and gigabyte and then the next solution
I'm this is a really interesting talk
I've attached a link to the video into
the slides you can of course pro a
process big data also with no Hadoop and
no java that's possible so there's a
really interesting presentation how you
can do it with with air along and piping
and so on the problem here is which I
see when I go to customers and they
asked me the first question and you
should implement this for me because
nobody can implement ere long or
whatever and the second question is who
gives me commercial support for this and
though usually this is not really a
solution and this is why we are coming
to the alternative the title of this
session today already said it how to
integrate data with a dupe this is
definitely the de facto standard for big
data processing so I think that's really
no question as you can see here almost
every huge vendor uses Hadoop
right it doesn't matter if it's IBM if
it's Oracle if it's amazon SI p whoever
everybody uses a dupe and even microsoft
right was a dotnet house so they didn't
use Java in the past but even Microsoft
uses her tube for big data processing
and I think that's really the last
important dying to see that Hadoop is
really the de facto standard for
processing big data I don't want to give
an huge introduction into Hadoop today I
don't know already knows Hadoop so most
of them okay so just maybe it's just two
slides or so for the people who have
never seen it really i do mr.
open-source apache framework which is
used by all of these different vendors
and it is really built for big data and
for distributed processing but
nevertheless you can use it on commodity
hardware and that's really an important
important point here so I'm you don't
need any Oracle rags or any stuff like
that which is very expensive but you can
store it on commodity hardware comedy
does not mean and it's really old stuff
with I don't know 200 megahertz also
unique good service but these servers
are much cheaper than the original
breaks and all that stuff and what's
also very important you have a simply
programming model and that's a
difference to the slides I showed before
I from my friend Pablo will use a lung
and pipe lenso on though with a dupe
it's relatively easy to implement that
stuff and one reason why this is also
working so good is because it's a local
computation in storage so you don't have
to send all this big data over the
network right Hadoop is managing all
this and you can store it on different
servers and also it's distributed but
you don't have to develop this this is
done by Hadoop under a hood and so what
you do in your employ in the end and
that's the end of introduction for
Hadoop you do their Map Reduce chops in
the end it should be named shaffer
reduce because in the middle you have to
shuffle your stuff but I'm to explain it
shortly you do a lot of processing on
every server and then the results are
sent to another server and there it is
or summarized together to get a result
and this result is sent to a client to
process it and now we have a dupe which
we want to integrate today and with a
dupe we have the Hadoop file system
that's a distributed file system where
Hadoop stores all of its data and that's
mainly where you have to integrate all
your data you have to push it to the
Hadoop file system and there it can be
analyzed it can be analyzed for example
with MapReduce code that's the code
which just a distributed processing but
there are other frameworks around for
example pick which you can use which is
for some people easier than to write my
produced code it's a a higher-level
language let's say it this way do you
don't have to write a lot of code or you
can use hive wife is a framework which
is using a very similar SQL similar
language so also people which no sequel
can write may produce code in the end
because these two frameworks generate
the MapReduce code under the hood I will
also talk about them later when I show
the frameworks and products and besides
there are many many other frameworks and
products so Hadoop is really a very
important project but it's just a basic
project and that many projects arise on
top of it right so that was really the
introduction that we have talked about
and big data about her tube why we use a
tube and i hope you already have
understand understood why we have to
integrate so many different data sources
and transform them and load them into
the hadoop cluster to get a real
business value out of it and now I'm of
course i also want to show you how you
can do this integration how you can get
all your different data sources into a
tube easily and the first part here is I
talked about an open-source integration
framework and afterwards I will talk
about open source suites which you can
use so i use this slide very often it
explains the different alternatives for
systems integration so on the left side
you'll see
integration framework that's really the
small thing then there's the Enterprise
Service bus that's a little bit bigger
product and then there's the integration
sweet that's a huge product and if you
look at the features let's talk about
integration framework first if the
integration framework offers you
connectivity so you can connect to many
different technologies and endpoint so
for example to HTTP or ftp or chain s
our RM i RM for example also to HDFS and
so on though he the integration
framework offensive the connectivity and
also the routing and transformations so
you can get your data from ftp and
Sheamus and whatever route it and
transform it and in the end put it into
your Hadoop cluster and the ESP on top
of it offers the same features as the
framework but on top of it it offers
many more features like additional
tooling and monitoring and also
commercial support that's often a huge
point which I see and why people use an
ESB instead of just an integration
framework because I'm usually these
integration projects are mission
critical systems and then you need
commercial support and usually you don't
get commercial support transfer
framework in many cases because it's
open source frameworks but you get
commercial support foley tooling the
product and finally there's the
integration sweet the integration suite
contains all the features of the ESP and
the framework and on top of it it offers
more features like business process
management or a rules and chin or
whatever ms would you look at now that's
for the definition what I'm talking
about integration framework is P and
integration sweet and now let's look at
an integration framework first I today
I'm talking about a Pecha camel so um
that's my favorite frame bug in my
opinion it's also on the best one for
most use cases if you want to learn more
about different integration for EM herbs
I nice slides here I'm on my blog you
can take a look at it I compared with
spring integration and mule and some
other esps and you can also come to
myself
tomorrow I'm there I have the session
again it's a little bit updated and
there I again talk about different
integration frameworks I also show live
demons of tooling for tables and for
talent and for spring sauce and so on so
if you're more interested in that you
can come to my talk tomorrow but today
I'm here just talking about apache
camera and what does a paycheck emma do
apache camel intake implements the
enterprise integration patterns though
most years peace or integration
framework dust is in the end so I'm
we've heard enterprise integration
patterns before not many people okay so
let's talk a little bit about that so
we're talking to pay today about how to
integrate all your data into Hadoop so
and what you have to do for that you
have to do two things the first thing is
you have endpoints connections to
everything where your data is to put it
into Hadoop and there's the second thing
is you need to transform it you need to
filter it to sort it and that stuff and
that is what enterprise integration
patterns define so I think this picture
shows many of them so you can see here
and there's a filter there's a
content-based router and so on there's a
translator transformer so often many
people have used these patterns they
didn't know there is a pattern maybe but
they used it and they often implement
this by yourself and that's really not a
good idea because that's the reason why
there are integration frameworks because
integration frameworks like camel
implement these patterns for you so you
don't have to write all of this glue
code by yourself integration frameworks
implement these patterns for you and so
when you use an integration framework
you can use endpoints like here in this
completes a file or it can be any other
endpoint which is supported like FTP or
HMS or sales force or whatever and you
can use the enterprise integration
patterns like a transformer and a
splitter and a content-based router and
all these patterns are also implemented
by the framework so you don't have to
write all this glue code and it's all
implemented by the integration framework
for you though that really reduces the
efforts a lot
when you integrate all your data into
Hadoop I will skip the architecture I
think just the more words about
integration frameworks um they have many
different connectors or components or
endpoints they're different net have
different names for every framework but
in the end it's the same it's some
connector than many different endpoints
and here a camel for example has about
140 endpoints in the meantime and you
can also create your custom components
very easily I will talk about this at
the end of this session so another good
feature of a paycheck camera you can
choose different domain specific
languages like java and xml that's
probably the most used pepper dsl's but
they are also DSS for scala for grooving
for even for cuddling yeah and another
thing i'm very important to know um
integration frameworks a very light
light wide so a paycheck ml and also
spring integration for example in the
end it's just some libraries so you can
deploy it everywhere in the chava
environment you can deploy a standalone
application as java application but you
can deploy it in an application server
or web container right or in a spring
container or an osgi container or any
cloud so in the end you can use this
integration framework like camel
everywhere in the Java platform or
travel environment and apache camera
release enterprise-ready it i think six
or seven years old now and it's open
source bottom it scales very good it of
us all enterprise features like error
handling or transaction support or
monitoring and a lot of tooling of it
from for example from talent or from
jboss and therefore you get also
commercial support by these vendors
right so it's really enterprise-ready
this is one example i will showed you in
a life team moto i will skip it but
before i will go to the live demo um
let's talk now specifically of about
Hadoop integration so if you already
know Apache cameras oh there's no
surprise for you so um you just have to
use some different connectors when you
want to integrate to a dupe so this is
one example where you stay camel HDFS
component and with this component I can
get my data from where I want so in this
case I read data from ftp right and then
I send it to HDFS so to my a dupe
distributed file system and that's
really that's it and this is two lines
of code and these two lines of code run
so under the hood the integration
frameworks implements everything for you
so you don't have to use the f tftp api
or the HDFS API from Java or any type
conversions as all that stuff that dust
under hood for you and this name is when
you want to read em from HDFS again from
the Hadoop cluster then you use the from
HDFS so then it's a HDFS is a consumer
and you read your data from there for
example in this case it's just a file
and you send it somewhere this is used
for example when you need to transfer
your data somewhere else for example for
analytics or for Excel or whatever you
want to do with the data right though
it's really that easy to integrate your
data into a tube with an integration
framework all the glue code is
implemented under the hood by camel for
you and it's the same for each face of
you can also directly store your data in
HBase in the column-oriented m database
on top of head pesci a tube in this case
it's an example with the xml dsl so i
don't really like it i prefer teach our
dsl but i also know people which like
xml so you can choose whatever you want
they have the same functionality and now
before i show you a lifetime a--let's
again look at one of the use cases which
i showed you in the beginning where we
have t systems which want to store a lot
of huge data in the Hadoop file system
but they also have to be able to query
it and how do we solve it here in this
case I have on the left side i have my
different log files and different and
whatever is there data and i use apache
camel as ETL tool so I load the data
from these log files and wherever they
are stored from server I transform it if
necessary and then I load them into the
Hadoop file system so it's the main
example here
I will talk more about am varying and
processing and transformation when I
show you the open source suites later
because that's much easier if you use a
tooling on top of it Biff camera you
still have to do the coding you have
just to ride the roots so it's much
easier than writing the glue code but
it's still coding and with the tooling
on top of it it's even much easier that
you definitely will see it later and
probably agree so let's take a short
look at a demo at Apogee camel so I have
two parts here first I have here my
IntelliJ this is really trusted code and
this is one example where sear on one
more complex root in camel so this
integration logic is called root in
camel and what you do here um you just
have to extend a root builder in camera
and then you implement your route and as
you can see here um there's a
domain-specific language used so it's
very easy to read and to implement so
probably even if you have never seen a
patchy camera before you understand what
is happening here so let's take a look
in this case I read data from a file
directory this is why I'm using the file
endpoint so all files in this directory
I'm reading them you can also always add
parameters to it and then you do some
specific custom processing so you do not
just have to use the implementation
which are available you can always
implement your own stuff and add it here
in this case I ever transfer implemented
a transformation pin and this beam just
makes the string to other case but here
you can implement your own custom logic
whatever you want you can also call web
services he or whatever but you can
really do what you want after this
custom transformation I use some of the
enterprise integration patterns and here
you see again they are already
implemented so in this case I'm reading
em CSV files from my directory so I have
two unmarked the study of format then i
use the splitter pattern to split it
with the comma is tokenizer and finally
i use the content based route
which is another pattern and which
decides depending on the content and
where they measure this message is sent
so in this case I'm just sending it to
different file directories or to a mock
or of course you can also send it here
to HDFS right or to hbase that's the
same code you just um switch this line
here and say to HDFS and then it stored
in HDFS camel under the hood implements
the data transformation and and and then
the object I'm matching enzo enzo this
really runs also if you change it to
HDFS you just have to add your
configuration we are the ? so i will
show you an example with HD SF here yes
ok I'm so usually i'm using also today
for the demo i'm using Hortonworks
because i also had some problems with
caldera sometimes but but that's usually
a library problem then I know because um
maybe we can talk about like that later
because it's off topic but um because
every took the Souper Salad different so
but but also i'm not showing i'm showing
Hortonworks later in the open source
sweet and what I'm here showing it's
just a man and a test I unit test
because that's also very important thing
which I want to mention when you I talk
about integration frameworks for testing
this stuff because you can write unit
tests for your integration framework
roots and like you do any other unit
tests with j unit and that's really very
easy here because you just um have your
roots in this case I really show you a
class from the petrol camera project
though it's open source you can download
it and this is just a unit test from
former HDFS component which I've opened
here and here you can see um these are
tests for put us is called it's called
HDFS producer test and there's also
consumer tests and here i just send data
to the HD SF component and afterwards
you have to do your configuration so far
some in this case and for the tests of
course and we are using the local file
system instead of HDFS so for a unit
test you are storing your only file
system instead of HDFS but em in the end
that's really the message here if you
want to integrate your data into a tube
with camera and you can use any
endpoints and you read your data from it
and then you probably have to use some
patterns to do transformations or to do
em splitting or whatever you have to do
here and then you finally can send it to
the Hadoop file system or to hbase
usually this really works very easy of
course I'm HDFS or Hadoop is
nevertheless a complex system so you
probably have to do some configuration
to f it right once and and then it works
but you do not have to write any glue
code under the hood though that's really
the important thing also in this example
you just have to dare to and to HDFS and
some configuration and that's it also in
this test there's no other
implementation or configuration done the
only thing done here what you could you
see is 44 really verifying that the test
is okay so this is the assertions
afterwards so and what I also want to
show you is the one example for the
documentation I'm use the documentation
for example for the camel HDFS component
and here you see all the different
options which you have for you configure
and that's the benefit of integration
framework like camera you can use many
different components like I said camel
has 140 at the moment and you can use
all specific features like here you can
see you can nevertheless use all options
of HDFS or for example if i use HBase
the camel component then you use the
specific options for HBase but
nevertheless it's a generic framework
where you don't have to write much code
under the hood and that's really a huge
benefit in my opinion so let's look at a
time okay um oh that was a short team
about apache camel i hope you I'm kind
of feeling about how you develop with
camel and integration framework so that
you'd really reduces effort a lot
and you don't have all the glue code you
just say what end points you want to
connect as consumers and then you send
it to hbase or HDFS and let it usually
after you have done all the
configuration right ones and what camel
does not have yet is any pic components
or hive components or H catalog
components oh that's other important a
dupe frameworks there are different
alternatives here what to do you can use
Dave as you can do call them from
command line you can also of course
called him from within camel for example
we had a camel excite component or dare
you can really use command line commands
afterwards or you can build your own
camera component I will talk about this
at the end shortly or well dessert
options not really a good option um so
but besides what i want to mention
besides these endpoints i also said that
camera is different data formats which
you automatically convert within camera
so you don't have to implement that
stuff and there are pluggable database
for csv and for chase and soap and EDI
and sip and so on already available and
as we are talking about a tube and
there's also for example a camel afro
component available so afro is a data
serialization system used in a patriot
tube and with the camel afro component
you can again use this afro um without
any coding so here's again a camera root
and here I get data from a direct
endpoints otay rick is just a
synchronous call and I say Marshall it
as I use in the lifetime of with CSV and
hear you say Marshall it to afro format
and then send it somewhere in this case
just a lock but of course here you can
again send it trust to HDFS too so
that's just important to know that
camera is not just about endpoints and
connecting them but also about the data
formats because and when you implement
that's a component once you can do it
I'm under the hood it does it under the
hood and you don't have to write this
code all the time so it was open source
framework to use you still have to use
the coding there now I'm what I want to
show you is an open source suite which
you can use and the suite has the
benefit as I
before you can use the tooling on top
and have many other features today I
work for telling of course I show you
the open source talent big data sweet
but there are others only market for
example phobic data I think you can use
pentaho is another open source product
which you can use or of course there
many proprietary ones here once again if
you want to see more about a different
ESPs and options and honor if a really
nice presentation which discusses
different alternatives for ESPYs and
integration sweets but today I'm talking
about the talent open studio and the
cool thing about the talent open studio
is that it's also under pesci license
and is open source and it also has
connectors for all these dupe interfaces
so you can amuse pig and hive in Scoob
and H catalogue and that you can use all
these without any coding and integration
and that's really easy as you will see
I'm also in the demo and on top of that
we have an enterprise version with more
features and so on but today let's talk
especially about the absorption this is
a screenshot but i think the life theme
was more interesting so let's recap
again a use case from the beginning
click stream analysis so I've explained
you at rackspace how they do it and now
i want to show you how we do it can you
do it with open source tooling of talent
and what we are doing here again we have
on the left side many log files and we
want to do click stream analysis
analysis and in this case and we do it
on both sides so first and we have to
extract data from the locks we have to
transform it and load it into HDFS and
afterwards and we have to manipulate it
and so on to make it available for
external use for analysis so the
analyzes itself is no part of talent or
any other integration tool so this is
for example done with axle or if
teradata data warehouse or with tableau
or any other bi tool or analytics tool
or whatever but nevertheless you have to
make the data available to these tools
or maybe even push the data into these
tools and I want to show you a click
stream example here I'm how we am so
consulted with
easily and what about clickstream why do
we do that I'm again the business value
for clickstream analysis which was not
possible in the past because the data is
too big for analyzing so you need Hadoop
or them system like this and this is
also why I'm Hadoop invite invested so
much time in had a yahoo invested so
much time in a tube and made this for
example you get business value out of it
because you can find out the most
efficient path for site visitor or you
can find out what the customer will
probably buy in the future or you can
fix or enhance the user experience of
your website it's typically use cases
and I really get business value out of
your data when you integrate it from
different data sources manipulated and
then it is ready for analysis by bi
tools or data warehouses or whatever so
and what I'm doing here I'm analyzing
and clickstream data though that's semi
structured log files they look like this
they have no real structure like a
database but at the end of course they
have some structure though I'm almost
all text data is any structure right um
and here are examples I'm this is just
for reference on the slides I will show
it to you in a live demo so let's take a
look at the talent open studio so the
telecom studio is also open source and
apache license as i said and what's the
benefit here compared to integration
frameworks and within decoration sweets
like talent or pentaho you don't write
the code under the hood but you trust
model these integration problems and
mappings and with the tool and the code
is generated under the hood and with
telling that's really am very easy so
you on the right side you have your your
components and connectors and dragged
and dropped under the middle and then
you configure them and that's it and
afterwards it runs without any coding
and that's really easy i really
recommend that you try it out by
yourself downloaded its apache license
it's for free and you can download it
and try it out and here's a use k in
this case just the first example I just
use HDFS components to amri data into
HDFS so here's an HDFS output out
put in talent means write data to this
component and all I do here is
configuring the unburied should be
stored and here's configure the
connection to my Horton back sandbox in
this case I'm telling is not
distribution dependent so we can use any
distribution we won't like hortonworks
or map our or Cloudera or pivotal or
whatever because we trust em generate a
native java and Hadoop code so this is
the reason why talent can be used with
any distribution and here you can really
integrate any data from any data source
into the Hadoop file system in this case
this is the first example just to show
you that it works is just with
hard-coded strings but you could also
use em ASAP as input or whatever or
Excel files or a database or whatever
you can use any data and transfer fair
to the Hadoop file system and also with
the input you can afterwards read data
from there and transferred anywhere else
in this case just a log but of course it
could be transferred to any other
connector then you can just run it and
the code is compiled in the background
and is executed and you will also see we
shall live debugging so that you can see
that everything works here in this case
it's not really big data it's just 15
rows just to show you an example um how
it works basically and now I want to
show you some more concrete examples I'm
here I really have implemented the cliq
string example with the complex log
files in different ways in this case I'm
first clickstream a is the one where I
move the different locks and other
m-files to HDFS so it's really just
drag-and-drop click and configure and
that's it and then you have different
options for example you can use the hive
components or H catalog components so
I'm here i use a catalog to create a
table again its chest configuration here
I say okay create a table with a name
and so on and then I load the data which
is already in HDFS to H catalog when it
if you don't know which catalog emits a
table management system which you can
use for hive and for pig and for may
produce
and then you can use for example the
hive components for analysis so here you
just do a hive connection and then you
do in this case I just drop the table
because it's a demo and I have to drop
it because afterwards I can create it
again and then Here I am right my wife
sequel similar script which I can
execute and a MapReduce code is
generated under the hood so there are
also other components like 4-h pay so
you can pick directly and write your
data in the HBase again here for the dmo
I just have are hard-coded strings but
here again you can use any of the other
connectors you can use a sequel database
and no sequel database is AP Salesforce
or whatever you need and of course you
can also map the stuff so for example
here I have example with our pic
components and in the middle of a
graphical mapper so I'm here you can
also use different resources from HDFS
and do graphical mapping on it under the
hood it generates in this case the pic
code for you right so let's show you an
example here pic aggregate is an example
where we use different pic components
and when we look at the code now look at
the code then you can see that here it
just generates the pic code here and
some Java code but this is all native ma
chava in MapReduce code and apache code
so this is the reason why we can support
every distribution here so it doesn't
matter if it's Cloudera Hortonworks or
whatever yes you cannot change the code
you can add custom code at different
spots where it makes sense but you can
it change the code directly that's um
that's my intention by talent because
I'm if you have a code generator and
change code and you can do very ugly
things and after what you have to bug a
lot and though I'm in our opinion it's
better if you care for example Emma
chava component or you can also use em
code directly in the mappings so let's
show you one example here so I'm as you
have seen here many different components
which
you can use for the integration into
Hadoop one other feature which i also
want to show you even if it's just for
the for the commercial versions of this
is not in the open source version and
these components here really generate a
direct native MapReduce code though this
is without pig or without hive and but
here you use you generate direct native
MapReduce code and here again you can
use graphical mappers and here again you
can write em java code so here I'm this
is Java code this is such have a class
with a method you can write any custom
Java code you write here and use it in
these graphical mappings and do the
mappings and transformations and this
really under the hood you can also run
it this generates MapReduce coding is
executed on your Hadoop cluster and so
as you can see it is very easy with such
a PL tooling which supports big data to
integrate all your data into a tube but
also do the transformations and so on
because it's not just about integration
it's also about transformation of the
stuff in manipulation and then you can
make it ready and then it depends on the
analysis which you want to do and either
you can use bi tools like tableau which
can directly connect to the HDFS
filesystem or you can export it if your
interface needs it if you the business
user knitted for example you just export
it to to csv again or whatever so as you
can see here this is my hadoop this is
my Hortonworks I'm Hadoop file system
and here I have oh I should go on and it
hears are the first part of it so this
is the result which I have stored in
HDFS and with just one or two clicks
okay in this case I used to be a hive
but I can also do it via via HDFS
components I put it into our CSV file
and then the business user can use it
again so then you can use tools like
Excel or whatever so let's get back to
the slides here if some screenshots for
the two after used a tool to export it
for example to axle then you can analyze
it with excellent in this case and for
example if the clickstream analysis you
can find out which users in which
countries
stage and click on which products for
example here and they once in Florida
clicked on clothing and shoes and you
can analyze it by H and so on and find
out that the people with the age of 22
and want to are especially interested in
this or that in products and depending
on this you can recommend them different
stuff that's the stuff like Amazon does
it or ebay does it right when they show
you new other auctions or products and
as you've seen here and the second
example is here with tableau so you can
use any tool you want on top of that
after its integrated manipulated what we
I told you use in the end or you can
also use for example talent to directly
push it into our data warehouse so these
ETL tools like talent have connectors to
data warehouses like Taylor like
teradata or like Amazon redshift or all
of these green plum so you can push it
there and then to your analyzes with
your comment tools which are used in the
past to so um and as you have hopefully
seen here um no matter if you use an
integration framework with coding or the
tooling on top of it with an ETL tool
like talent it's very easy to and
realize stuff like this click stream
analysis it's very easy even if you are
no a dupe specialist and nevertheless
you can do a lot of click stream
analysis and all of that stuff one last
part I'm just two or three minutes more
i also want to talk shortly about custom
big data components because sometimes a
component might not be available it
doesn't matter if you were talking about
integration framework sore about about
the integration suites or ESP maybe you
need a component which is not available
like i said for camel there's no hive
component yet or for example you could
write an impeller component for talent
so um or you have some custom data
formats which you need to integrate into
a Hadoop and transform it that's really
very easy um i will show you just one
minute here um how you can do it with
camera and with camera it's for example
i've written a sales force component and
you just have to implement for classes
or so and it's really very easy you just
define how your your i should look like
and then you do somewhere in the end
point you have to implement the
connection once
this is the native API of their of the
product you want to integrate like sales
was in this case and then you implement
your consumer and producer again here
you just once write the code and then
you can use the sales force component
like any other component in camera so in
this case I do a unit test and here you
can see I've written a sales force
component and really you can write a
production-ready components in two or
three days usually and the same is for
talent so when our talent and
professional services writes a component
for a customer its maximum five days of
course it depends on the complexity of
the component but it's really very very
easy to do this okay and last but not
least of course you don't have to write
components and if you just need
something once for example you can also
use rest or soap components so this is
one example with camel so here use the
HTTPS component so the rest component to
make calls be erased to any interface if
you don't want to write a component or
connector so old it was a lot of stuff
that I I think for many people if you
have never seen these two links before
but I hope you got a really a feeling
for it I hope you understood that if you
want to use pray a big data in Hadoop
and you want to get business really out
of it the most important thing here is
you have to integrate different data
sources combined them manipulate them
and then you can do your analysis with
any other bi tool and as you have seen I
hopefully you have agree with me that um
it's no longer rocket science you can
use frameworks or you can tools on top
of it and that really can help you a lot
with integrating every data into a tube
so that's it for my talk I thank you a
lot for coming and if you have any
questions i think we are almost out of
time you can come here to me thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>