<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Wholly Graal: Accelerating GPU Offload for Java | Coder Coacher - Coaching Coders</title><meta content="Wholly Graal: Accelerating GPU Offload for Java - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Wholly Graal: Accelerating GPU Offload for Java</b></h2><h5 class="post__date">2015-06-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/uxDd5KfYdJU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to talk about holy Gras it's
it's it's about growl the compiler and
project Sumatra in general so my name is
Christian and telling her I'm with with
Oracle and working the hotspot compiled
team we have your two guests the men on
the left is Christian bimmer he's from
oracle apps he works on substrate vm and
and and crawl in general and the guy on
the right is hassan teas from AMD and
AMD is working closely together with
with oracle in adding HSA back end to to
growl for project Sumatra and so I'll
give you a short introduction in work
rallies and what symmetries and these
two people will then life a little
deeper and tell you what they've done
and and what we are trying to achieve
with with so much r and all that could
you please yeah didn't usual disclaimer
you've seen it probably a couple times
already so yeah as I said I'm a little
bit of introduction in what growl is but
the architecture and and then percent
will tell you about adding a new backend
that you've you back end to grow okay
what what is grow what is growls vision
it should be an extensible dynamic
compiler and for for object-oriented
shower programming and you know it's
it's it's it's a typical compiler I mean
it's the only thing that's that that's
different from the compiler we have
right now I mean probably most of you
know that that Hospital has two
compilers we have a client or server
compiles you want to see two and both of
them are written in C++ and the biggest
difference
with growl is growth is written in Java
so it's it it's basically another Java
program that we run on on the vm and and
it being written in Java opens up some
possibilities like the last point year
that you can use something that's called
Java snippets where you write basically
compile it intrinsic in Java code and
the compiler can then just parse them
and compile them into into a graph and
then later you you use that graph but
Christian will tell you a little bit
more about this can you so yeah that
that's how the architecture kind of
looks like so we have the hotspot vm and
and you you can you can kind of do what
what we have right now so prolly is
basically an add-on to a regular hotspot
repository so you have your typical
layout and your view c1 and c2 compiler
and then there is another directory in
there it's called growl and they're all
the child classes of the kraal compiler
and you can you can build hotspot in in
kind of two different configurations oh
actually three but the third one is
basically the regular one that you that
you know right now but the the other two
new configurations is you can either
have gras in hosted mode and in hosted
mode means that growl is just another
java application on the side it just
happens to be a compiler and and you
compile growl itself and and and and the
other applications you're running with
c1 you interpret them you come back
compile them with c1 and c2 if you run
tiered and and you can send particular
methods that you want to compile
withdrawal to growl so it's it's a very
easy configuration to to kind of make
development because that the compiler
itself is compiled with something that
you know that works because it's hot
spot c1 c2 and then for especially for
unit testing or something
this it's very it's very simple to do
because you say i have this unit test i
have this one method that adds two
numbers and i just want this one method
to compile with growling Tanner just
send it over and composite for you it
gives you back compile code and you can
run it if you want or whatever and the
other configuration would be the growl
only mode where basically you don't
compile C 1 and C 2 you just have drawl
and everything gets compiled with growl
so when you run your hello world
application and there is a method
scheduled for compilation growl we'll
take it and compile it and run it for
you but that also means if you run in in
Gras only mode that you that you have to
bootstrap growl especially during
development cycles when you when you
develop growl itself you so when you
when you boot it up it has to kind of
compile itself so first the first method
you use cattle for compilation it will
run that the compiler itself would run
interpret it until it gets warmed up and
compiles itself and eventually it spits
out the method you schedule for
compilation that's basically the second
bullet point we draw can replace the
compilers you could easily if you wanted
to just build a hotspot configuration
where you have c1 and crawl or c2 and
grow it doesn't really matter that's
basically the last point you can
integrate it in whatever way you want
and in particular interesting for for
this talk is a is the GPU offload so in
general growl is is an official OpenJDK
project so it was started it has a
couple of members it has a couple of
reviews and committers who contribute
code on some people all right go
somewhere or collapse somewhere AMD but
anyone basically can join and contribute
code to to to the project it's on
openjdk java.net
oh yeah yes so yes so the question was
if crawl is using the the regular Java
heap to the compiled methods and the
answer is yes it's it's just another
java application that if it so right so
the question was can you can you
restrict the memory consumption of growl
and the currently the answer is no
because it's this is this is a research
project right now it's not it's not
intended for protection use right now in
what the plans are to to leverage gwan
so just to have a couple of 1g one
region that's just dedicated to the
compiler and then in that region is
whatever size you want to make it and so
we're thinking in this direction but but
right now it's just using memory what it
gets and it competes with your
application so it's a it's an open
openjdk java.net and there's a some
interesting information on the wiki
especially from from the labs people
it's it's it's very simple to to
actually build and run stuff because
they have a lot of we collaborate with
with the University in lens and a lot of
students of working with crawl it it's
it's similar to you probably read in the
last 10-15 years a lot of scientific
papers where Jack's a vm was mentioned
Jack's a vm was was so popular in
research because it was easy to program
with because it was written in Java and
so right now students and professors are
already picking up growl because it so
easy to work with and it's it's also my
in my opinion it's even easier than
sharks because checks was the whole vm
and this is just a compiler do you have
your your your stable vm and then you
just fiddle around with the compiler and
in my point of view teach it to the end
zone that's why
the the last people kind of they had
they have this MX script it's coming
it's coming from Maxine and so you can
just clone the growl repo do an era x
build it builds it for you and then you
you just I think MX vm is is actually
running it ya think so and the other
thing that I de in it so it actually
creates for you configuration files for
IDEs like eclipse or netbeans that you
just import the stuff and it shows it to
you like everyone else sees it and it
has checkstyle included and stuff like
that and I don't say if it does
reformatting so that the whole source
code of the project looks the same so
we're trying to keep it uniformly yeah
that's that's gross or if you want to
check it out use that link and and so
much or in general it's also an open
shady a project with local JDK java.net
website and the wiki where a couple
people from oracle of putting
information and it's it's basically
about leveraging GPUs so we are trying
to you know data is becoming bigger and
bigger and we're doing and we get more
coercion we want to do everything in
parallel and GPUs can do things in
parallel so we won't want to use them
but we we don't want to specifically
write code for them we just want to
write plain java code and we want the BM
do the work for us figure out what could
be potentially be running parallel and
and so that that that's what project
tomorrow is about there's also because
it's a retiro genius environment it also
brings up stuff like hard with data type
support and things like this and and I
kind of mentioned this here because it's
related and so also part of Sumatra so
it's not only growl and GPU backends is
part of Sumatra also other stuff like
arrays to that oh
we're thinking about new data structure
bigger data structures and then things
like that so if you want to participate
in that go to the website sign up for
the mailing list whatever they're there
already a couple prototypes in the maker
repositories about data flattening and
things like this so if you're interested
in that please join ya do I haven't oh
yeah I just wanted to show you this it
that that's basically what this talk is
about um so we have a compiler in this
case it scroll and we the input is plain
java bytecode and and then we have
that's basically the configuration or it
would what we could do right now is
growl is kind of bound to 21 host CPU it
kind of has to because when you compile
your C++ code it needs to be for one
host most of the time we will be x86 but
it could be sparkle whatnot and and so
growl also then supports other backends
we have we have a kind of very minimal
PTX back end which is for NVIDIA GPUs
it's basically just a proof of concept
to show that it can do it it can run
very simple stuff but but AMD in these
HSA is it's much more mature than PT
axis and then that's not literally that
later so and put that that's that's
basically the configuration you have
them and you could extend the lower part
here to what not you have the more cheap
use the better right and you could
support all of them in one box by a
couple nvidia cards by a couple of AMD
whatnot chips and stick it in there and
and you could potentially use it so and
draw these arm since it's written in
Java it's it's very easy to prototype
with and that's the main reason why we
chose it because it's so easy to do
stuff with it we're not saying that
crawl will be the compare
we're using eventually in the product
but for now it it makes the most sense
to use it because we can prototype too
quickly and most will tell you that it's
good yeah so all right so Christian will
tell you something about brawl and
truffle okay I know you're all here to
hear something about GPUs but I steal
five or ten more minutes of your time to
go a bit in the other direction and the
other direction is what can be layer on
top of the JVM and the byte codes and
then also leverage things like the GPU
back end and that's the project that we
have an Oracle labs called troffer which
is basically a very easy simple way to
implement all sorts of programming
languages especially dynamic programming
languages so you just write EST into a
Beretta and then we use grawr to compile
that to optimized machine code and
currently we have prototypes for
JavaScript for Ruby for pison for our
which is a language for statistical
computations and they're all based on
this comment traveler framework which is
more less little API how to write your
language implementation and with the
help of growl it's compiled down to
optimize measuring code so you can see
that more or less as a second input to
your chit compiler other than normal
java bytecodes that Java Sea generates
and if you have more in if you have
interesting growling truffle and we can
talk later offline about that so I don't
want to go too much into that right now
but the really interesting thing and why
we are doing all that and by really
liking the GPU Bagginses if you have
this picture where you say oh you have
all these languages coming down to growl
as the JIT compiler and we go to the
next slide we have already seen that we
have growl with all this different
backends and of course what happens if
you put it together
on the next slide you get basically all
these nice features of GPU or float for
all the languages and that's exactly
what you want so you don't want to have
your JavaScript vm your Ruby VM or
having separate JIT compilers that all
need a separate GPU of load well you
really want is that you as a language
implementer just can say oh I have a kid
compiler with all these backends and if
you a GPU vendor then you say oh I make
it back end for growl and you can
automatically leverage it for all of
these languages so that's our big hope
that we can really achieve this with
ground and preferring I want to spend a
few words on how coral is organized and
how it is structured so that you get a
little bit of background of what the
scientists are then talking about
internally growl is really quite
standard just-in-time compiler for Java
which means it takes java bytecodes as
the input it passes them it does or the
goodness that optimizing compilers are
doing so doing method inlining which
means it replaces calls to small methods
with the method body it's doing escape
analysis for example so you have objects
that's located in a meso then does not
escape the method then there's no need
to allocate it on the heap it's just a
the allocation is eliminated and the
object is replaced more or less by
scalar values so there's all this high
level goodness there but then you see
this to blue boxes the first is vm
lowering and then there's architecture
lowering so crowd has really too
dedicated phases where the virtual
machine can influence the compilation
which means that if you have a normal
setting for the hotspot integrated for
to a generating x86 machine code then
you would do you're lowering your lower
your allocation node to some low-level
nodes that access the usual T lab where
the memory is managed and then you have
an architecture lowering for doing x86
lowering
but there's a really dedicated faces so
for the HS al work no can you then the
only thing that needs to be replaced and
changed these two dedicated faces you
can reuse the whole rest of the of the
grad compiler and you need to plug in
and no only very little things and yes
oh the truffle API is one level on the
top so you write your interpreter in
Java and since the interpreters written
in Java it's there is java bytecodes and
these chava byte codes are the input to
grow so there is no separate real front
end to growl it's really the interpreter
itself is going in and yeah i know i was
a little bit in precise it's technically
it's a bytecode front end but logically
you can see it as a different way
integral yeah and just to give you a
little feeling of how is a compiler
writers daily life and I would save it
draal it's pretty nice because for
example if you want to have a growl
compiler note that as a comparison you
really have nicely looking java told
you're saying oh i'm comparing two
values x and y you ve use java
annotations we use chera reflection you
use all this nice meter programming
facilities that java offers you to
really get a good productivity and we
have a lot of visualization tools to
visualize the output and i hope they're
used for two so most interesting
question what do you get in terms of
performance so currently growl is
definitely producing code better than a
client compiler but not as good as the
server compiler so we are working on
that so that's for the steak JVM 2008
chava benchmark suite and if you go to
the last slide of mine for non chava
workloads like for example for scallop
benchmarks we are in closer to the
server compiler
so I think if a little more effort on
our side and we are heavily working on
that hopefully we can reach the speed of
the Java compiler also for your normal
travel workloads on x86 but as I said
also another goal of grad is to really
have this extensibility to experiment
with other architectures and if that I
give to the Sun to tells you how he
leveraged and ground for offloading to
the GPU I can everybody hear me I think
the mic is a little low no it's working
go okay hi so I'm wasn't venkatachala me
thanks for coming to our talk and I work
for the AMD runtimes group and I'm very
pleased to be talking to you today about
the collaborative work that we've been
doing with Oracle to get this new h sale
backhand cogeneration support into the
Gras jit compiler so this is going to be
the agenda first I'm going to talk about
why you should be interested in GPU
offload and then I'm going to talk about
what are some special considerations
things that we should pay attention to
when compiling Java for the GPU and then
I'm really going to touch on the smarter
project Christian already introduced it
and I will also introduce H sale or
heterogeneous system architecture what
is what is that how are we leveraging
that and what is H sale namely the
intermediate language that the JVM
back-end will produce and then we're
going to get right into the details of
this prototype that we've developed in
goo all to omit the H sale code so that
Java can be compiled for the GPU and I'm
also going to walk you through some
interesting code example so that you can
kind of get a feel for what does this
HCL code look like and you know what can
we do with this okay so why should you
be interested in GP awful so typically a
GPU has many more courses in a CPU so if
you want to thank orders of magnitude
the CPU has anywhere from one to eight
cores GPU will have on the order of
hundreds of course so why is this
important so data parallel applications
typically data parallel applications is
the same computation that's reputed
multiple times over
multiple data and the results of the
computations are all independent so each
computation can potentially be run on a
separate core resources permitting so I
have at the bottom I have a little
example where I'm squaring all of the
elements of an array so for a simple
case like that the individual square
operations can each be done in parallel
on separate course provided we have
enough course to do them so we can save
power and performance typically whenever
we have data parallel parts of the
program to offload these parts of the
program to the GPU instead of running
everything on the CPU okay now however
there are some interesting
considerations things you need to pay
attention to when we're compiling Java
for the GPU so one of them is that Java
needs a programming model for the GPU
compilation so a way for programmers to
be able to specify what parts of the
program can be offloaded to the GPU and
that's that's pretty obvious and the
nice thing about JDK 8 is it already has
an API that's pretty simple to use it's
called a stream API and we have
leveraged this in our prototype and i'll
be showing you code examples of this
later the other thing that the JVM needs
to be able to do is obviously it needs
to generate code for the GPU while it's
executing on the GPU cpu and while it's
generating code for the cpu as well so
in other words what we need as far as a
JVM is current cerned is a multi aissa
compilation framework we can compile for
multiple Isis or instruction set
architectures and then finally what
would be nice is if the JVM can emit
some an intermediate language which can
then be translated or converted into the
final instructions of the actual GPU
target I said instead of compiling just
for that particular dpi sauce since
there are so many GPUs out there that
have different architectures instruction
sets it'd be nice to if the JDM can emit
an intermediate language and that brings
across the advantage of portability and
as I'll explain H sale is one such
intermediate language that's what our
JVM is emitting and it brings many of
the advantages that java bytecodes bring
to java okay so christian already
touched on this mantra is an open data
clear project that
was founded jointly by Oracle and amb in
2012 and the goal again is to enable GPU
GPU enable meant for Java in a way
that's seamless and transparent to the
end-user so the programmer does not need
to know anything about the GPU they
don't need to know how to program for
the GPU they just write all their code
in Java making use of the stream API and
the JVM takes care of all the mechanics
under the hood that's that's basically
the idea here and again what we are
doing is developing a prototype of
sumatra that leverages the garage
compiler which is a wonderful
infrastructure easy to work with and the
akh sale emitting HCL intermediate code
so I know some of you are wanting to see
some code and this should whet your
appetite this is an example of the kinds
of code that our prototype can already
run and the nice thing about this
example it's deceptively simple but
there are many different things going on
in this example so we have this name
info class which contains a string
called name and it has a boolean
variable called exists and it has a
routine that's used that takes a
parameter a string and checks whether
the name exists in that string pretty
simple operation right and then we
declare an array call all names of
multiple of these name info objects and
so what we're doing here this is JDK 8
stream this is an example of the stream
API syntax I talked about we're
iterating through a stream stream of
integers ranging from 0 to the length of
this array of all names and then for
each element in that in that array we're
checking to see whether the name that
that element represents is contained in
this particular string called long text
right so you can see that there are many
different things going on here there's
array accesses there's method call
support there's support for this JDK 8
stream API syntax and and also their
support for this calling JDK library
methods so in this case that would be
string dot contains so many different
things going on and we can already
handle this in our prototype okay so
brief introduction to the heterogeneous
system architecture actually
HSA is a foundation that whose aim is to
standardize the interface interfacing to
the GPU and they do this by two means
both through a runtime stack which we
call the hhsa stack and through an
intermediate language which we called H
sale and so again the goal is to be
aissa agnostic so abstracting away from
actual details about the individual GPU
architecture and make things making
things seamless for the programmer and
so this so the HSA stack its
heterogeneous system architecture has
has two advantages that make a really
great platform one being the
seamlessness which I've already talked
about the transparency the user just
needs to write their code in Java
doesn't need to worry about the
underlying details of the GPU hardware
and that the second thing is a concept
which we call shared virtual memory and
what that is is that the program running
on the GPU can access host memory
without having to do any kind of
trickery we can just pass pointers
across to the GPU and have have have the
program running on the GPU access
objects that are in the CPUs memory
which is really cool because now we
don't need to do any data copying across
you know back and forth between the CPU
and GPU which can be really expensive if
the objects are moved by garbage
collection so I'm not familiar with the
mechanics of how that works but I know
that from the GPUs point of view and and
from the point of your the program
running on the GPU they don't need to
worry about the fact that it's move you
know they a pointer is a pointer
basically
yeah it sorry yeah sorry tuned to work
yes that's a problem of course so if
you've him if you're moving objects
around we kind of have to do something
about this but so the reason why what
son doesn't know about this yet is
because they have an encounter do yet
that they are not there yet but that's
an issue yeah how exactly we are going
to solve it I don't know yet maybe we
will watch cheap collections something
until the GPU is done or I don't know
yet yeah it's a good question and we're
in the middle of a prototyping effort so
these are issues that will come up and
will you know we'll decide how to deal
with them go ahead yeah
okay so moving on so the execution model
it's a parallel execution model and you
can think of it as a grid-based
execution model basically you have
multiple work items executed executing
in parallel or as parallel as a hardware
will allow it in on a grid and there's
special nominal eicher that we use
namely the term colonel refers to the
main body of it defines a main body of
computational work that is to be
performed and the term work item a work
item is a specific instance one instance
of that computation for a particular
data set so down below again to the to
the left I've Illustrated coming back to
our squares example we're iterating over
an array squaring each element you can
think of each work item as being once
one instance of a squares computation
squares computation 4141 array element
and you can think of like in the right
as I have depicted you can you can think
of all of the square operations combined
executing together in a grid as parallel
as the hardware the underlying hardware
will allow it and each work item has a
unique ID that's just something to keep
in mind when looking at code so the
brief primer in 2h sale this is this
again is the code that our JVM back-end
will emit and again this is an
intermediate language so what that means
is that HCL itself does not run on the
GPU but it gets translated into the
target instructions of the GPU by a
software runtime layer known as the
finalizer and the code is an ASCII text
form which makes it easier to debug them
say binary code and if any of you have
looked at x86 assembly the code is very
similar to x86 assembly and just as an
example I've dissected a simple
multiplication instruction to give you a
flavor for what are the different fields
and as you can see you know there's
there's a there's an instruction
mnemonic and this is followed by a type
modifier that tells you you know what is
a data type you're you're you're
operating on is it unsigned or signed
for example we have a length modifier
that tells you the size of the data and
then we have operands and typically the
destination operand will appear first so
in this case s3 is the destination where
as s0 s1 our sources and all of these
these are registers okay so now we're
getting to the actual details for the
prototype that we've developed so again
this is a soap prototype for compiling
Java to the GPU and it's meant to be
seamless in that you know the programmer
writes their java application coding it
against the Java 8 streaming API and the
JVM is able to handle the compilation
and produce HCl code which then gets
finalized so that it can execute on the
GPU so that's that's the basic idea and
we are leveraging the OpenJDK crawl
project to do this and we have done some
initial performance testing and on the
Mandelbrot application and police report
that we're seeing a 10x improvement in
performance when running the compiled
age sale code on the GPU versus running
the Java code on the CPU using using
Java threads
um I don't have the specifics specifics
of those numbers in front of me but
that's that's a question that we can
answer offline we have people who have
run those experiments who can give you
the exact numbers okay so this is a
diagram that illustrates how does Grall
fit into this whole HS a one-time stack
and how does everything work together so
at the very top we have our java
application so if you look at the top
left which gives you an overview of you
know what is the workflow that's going
on at the very top we have our java
application and we're just coated
against the stream api and growl takes
the byte codes of the application and it
creates an intermediate representation
just like any other compiler creates an
ir and then their number of phases where
it does optimizations and then it
finally produces the h sale code and
that eight sale code runs into through
the HCL finalizar which converts that
code into the actual target instructions
of the GPU that you're running on and
then finally and the result of that runs
on the GPU it's pretty straightforward
yeah yeah the whole method the whole
method yeah okay so now we get into some
fun stuff i'm going to walk you through
some code and this is first of all just
to give you a flavor for a sale code and
also to point out some of the issues
that that i mentioned about the
characteristics of h sale this is a
simple example where I've Illustrated
I've gone back to the squares example
where we're scoring elements of the race
so if you're like at the very top
left-hand corner I have the code that is
squaring again the elements of an input
array and then storing them to an output
array so we have an array input called
in and I'm doing in of icons in a by
setting out of i equal to that and so
when Java Sea compiles that you know
what the JVM is going to see is a
routine which I've called in lambda
dollar 67 right below the arrow which
takes three parameters it takes the
input array it takes the output array
and then it takes the variable I which
is a range that that we're iterating
over and you can see in the body of that
method we have the same square roots
operation and to the right what I've
Illustrated is the kernel which defines
the main computations that take place in
a sale vhdl code for the body of that
lambda method now this may seem like
there's a lot of going on but actually
this code is very close to what you
would normally see for x86 code and let
me explain that a bit so at the very top
the very top two instructions that I've
highlighted in red to the right those
are loading the the array arguments into
registers the pointers to those arrays
into registers and the thing to keep in
mind in this example is that we're
loading being we're being supplied the
pointer to the entire array this
includes all of the header info for the
array we're not skipping any of that so
this goes on to illustrate the idea that
a pointer is a pointer in H sale you
know between pointers just like ordinary
pointers you know like we would do and
see so we get the we get a pointer to
the whole array and what that means what
that implies is that in order to access
I element of that array you need to do
some address computations in order to
access an event that's most of the work
that's going on in in the body of that
HCL code which I have not highlighted so
you see in the multiplications you see
the loads the additions I mean all of
that is addressed computations and in
the middle of the body I've highlighted
a multiplication instruction which is
the actual squares operation that's
taking place so again really the point
of this example to get you a feel for
what H sale code looks like it's pretty
it's very similar to x86 code and also
to illustrate the point that a pointer
is a pointer in a sale I mean this
shared this notion of shared virtual
memory is a very powerful concept so go
ahead
yeah that this is yeah this this is
actually yeah again this is a prototype
this is a prototype type back end that
we have developed so you know we have
not completely explored all you know all
possible optimizations that we could do
with the code but this is but this gets
us started right but there are a lot of
optimizations that Grall already does
with the code and you know so now I'm
just going to I'm going to walk you
through some examples of other kinds of
Java 8 code snippets that we support and
this list is by no means exhaustive it's
just to give you a picture of what all
we can do what are the interesting
things that we can do a question over
there
how do you know so the question is how
do you know that this is something that
you can compile right so currently we
are using a unit testing framework and
we have a way of you know inside the
grawl implementation we have a way of
indicating if a particular operation is
not supported then we can throw an
exception saying that it's an
unimplemented operation and that support
is already featured in grow and that
that makes it pretty straight forward to
see you know what are the things that we
have not added support for but yeah
mm-hmm
right so look oh yeah sure
yeah absolutely yeah but as you
okay so I'm going to walk you through
different code examples and again this
is not exhaustive but these are some of
the interesting things that we can do
with this framework interesting JDK 8
code snippets that are using the stream
API that that our prototype is able to
compile so the very first one gives you
an example of math intrinsic so here we
have we're iterating over in in stream
and then for each I integer in that
extreme we're accessing an array called
in which in this example it contains it
doubles so an array of doubles were
accessing that and then we're taking a
square root of that and then we're
taking that result and then we're doing
a multiplication with that and then
storing it to an output array so you can
see already with this there's a number
of different things going on you have
the support for math and forensics you
have support for arrays we have support
for doing the multiplication with
medical operations plus support for this
entire stream API syntax being able to
handle the syntax so the example
following is pretty similar except we're
storing to a boolean array the value
value of true or false depending upon
what the result is of a string contains
operation so we have at the very right
we have again we're iterating over and
in stream and then we're we're using
your route using each index as an index
into into an array of in this case its
strings and we're checking to see
whether each element of that string
array contains the substring hello so so
in this case we have string manipulation
getting involved the call to string dot
contains below that is a pretty simple
example we could have used JDK 8 syntax
for this as well stream ified this we
just didn't here we have an array of
shapes and we're checking to see whether
each element of that array is an
instance of a particular shape so in
this case a circle so we have support
for the instance of operator
and this is a very interesting example
because it shows highlights two ways of
tackling the same problem the problem of
how if you have an array of points how
do you and you want to increment the
x-coordinate of each point in that array
how do you go about doing that and
there's two ways of doing this using the
JDK stream API you could use we could
use in streams we could also use an
object streams and both both techniques
are supported in our prototype just
briefly go over this we have defined a
class called point which has the X&amp;amp;Y
coordinates and we're defining an array
of these points called point array and
the first approach uses an extreme where
we're iterating over a range of ants
ranging from 0 to the length of this
point array and then for each element
for each point in that point array we're
doing point array 0 x dot x plus plus so
that's one way of doing the increment
but there's a more nifty way of doing
this using object streams we're down
below we've done down below we can take
the entire array of points and convert
that into a stream so that's what we're
doing down below here we're doing a
raised stream point array and now in the
for each we can do dot for each p arrow
pdx plus plus and here it's understood
that p is a point just based upon the
context so the second approach gives us
a much much more concise syntax for
being able to solve the same problem and
this is just one example that shows that
how we're able to support both in
streams and object streams and building
upon that we can take general
collections objects such as lists and
convert them into streams and work with
them as well we have we have that
support in our prototype as well
handling those kinds of JDK constructs
so in this example I have we have an
ArrayList of points and we're converting
that arraylist into a stream and then
we're doing the same thing we did in the
previous example you know we can do a
for each p arrow p x + + use a very
concise syntax where it's understood
that p is a point so
slightly different topic atomic
operations this is an area that Tom
didn't Oh in our in our team who will be
speaking later in our samat rebuff he's
been working on this and a patch for
this support is forthcoming and we
support atomic operation such as atomic
increment that's an example here so in
summary gpif offload is beneficial from
the point of view of improving
performance as well as saving power we
have contributed an H sale cogeneration
back end to the growl just-in-time
compiler it's already been checked in so
please download it give it a whirl there
are a lot of interesting things that I
can do I've only touched the surface of
it but hopefully this talk gave you a
good impression of the many different
things that can do and yeah as as you
know as Christian pointed out as the two
questions pointed out that Roland krofl
are really the gateway to being able to
compile not only Java but other dynamic
languages on the JVM and please come to
our samat rub off we have a ball later
today 530 hilton imperial ballroom b and
it's the second and i have a number of
references here these are references to
the HCL spec and information about
sumatra the garage it compiler homepage
HSA foundation and we have upcoming a
developer summit 2013 apu 2013 which is
an AMD developers of it we encourage
participation in that as well thank you
so questions he's the first right I was
curious what particular what you're
doing about convergence than the north
end of the work group when this problem
trust all okay so i can tell you we do
have support for control flow such as
branches we have test cases which i
didn't demonstrate here when you have
occasionally example and we do support
those those type of control flow
operations we also support method calls
as well another type of control 12 i
might be the last part of your question
so that strictly on a GPU you can have
all right of the diversion from taking
different part although but evened up
serializing that work generally okay and
a real performance anniversary anybody
need to be you go to correct before
interview code and in order you got
anything
authorization to reduce the divergence
of work of the work groups in generated
by the pilot not at this moment I
believe that is work in progress
ok so the kinds of operations that you'd
like the target again that is most
beneficial to target are the kind of
data parallel operations that I pointed
out so whenever you have computations
the same computation being repeated and
all you're changing is the data and it
was also the people depend on each other
then there's an advantage to running the
separate sub complications separately on
multiple cores and that's where the TV
will really comes in yeah away
operations for example operations where
your your different computations on a
stream of data again and everything can
be done in power those are really the
kinds of operations which are putting
toxins into like crisis memory them
because the amount of computation you
say has to compensate
so one thing to note is there is a get
we have a concept of shared virtual
memory again so so the program running
on the GPU can add can access the a
pointer of the CPU memory without having
to have and that eliminates the need to
shuffle data back and forth between the
CP with the GPU right
weasel
right yeah but so yeah let me let me add
to that I'm like the PTX GPUs they don't
support the virtual memory address space
so to even there you have to you have to
copy around your stuff so if you're
doing array would not have to take the
array and copy it over to the other
memory and and even applications today
that the do cheap you offload even for
them it's it's it makes it makes sense
to do that so you kind of so there's a
lot in even your question is it's kind
of in the direction how we finding
parallel ism and how do we know that
it's really beneficial this is yet to be
determined I mean it'sit's kind of every
surgery thing right we don't have an
answer to that yet because we cannot
execute all possible java code yet AMD
has started I know how many months ago
like four or five months ago five months
ago there was zero HSA code and that's
what they where they are right now it
you cannot run everything but it's um I
know that some people of the University
in Manchester day of their doing
research thing on that so there the the
approach a.m. they took is to to
leverage the stream API because it's
very obvious that it's parallel right
but what you want is actually you write
your for loop and the BM nose for you
that it's parallel but that needs some
research and and so we're working with
with some people from universities on
that yet please
cobra all the points go through the it
could be something
so the question is does growl support
full speed debugging and by then by that
I assume you mean setting breakpoints in
the compiler and it still runs full
speed not yet no I mean we don't even
support that agency in situ right if we
have a break point method we don't
compile it I think the same applies to
to evaluate if you compile it in the
house it mode it's obvious because it's
it's see to compile but so far growl
doesn't have support for that but it's I
I had a bug once assigned to me for many
many years to to fix that I can't
understand it but we should do something
about this yeah because when it when it
when we have a compiler that's written
in Java we kind of need something like
this right yeah please
for a lot of work what are you using for
your final so I answer the first
question in you to second so the first
question was what I what are you guys
doing with with the generated hhsa or
PTX coke because it's text-based
assembler right yes we put it in a cult
blob it's just it's just a stream of
texts and we just put it there but and
and when you try to call it you crash of
course because it's nothing but when you
send it off to the GPU it that's the
right thing yes so there's a software
layer known as the finalizer and that's
part of the HSA runtime stack that it
takes stage sale as input and then it it
compiles that into the instructions of
the Jeep actual GPU target that is
yeah so he just mentioned that but there
is a there's an simulator also checked
in so you can basically run code it's
just not real hard work can you do
useful performance analysis with the
simulator it's not cycle accurate now
another question the very back so the
question is can you kind of allocate
temporary memory on the GPU to do local
computations
right
um the HSA stuff there is no how to work
yeah that should run on yeah it
certainly runs on linux yeah I know that
it runs on linux at ease x86 yeah yeah
and if you you could well you can what
you can do is you you can join the
forces and and add some functionality to
PTX because then you just need a mac
laptop and you have your cheap you built
in so in fact i can run PT xcode on my
laptop that that works which we're just
waiting for AMD to release hardware
so think think we're out of time here
yeah one more question may be quick yes
it does quick answer</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>