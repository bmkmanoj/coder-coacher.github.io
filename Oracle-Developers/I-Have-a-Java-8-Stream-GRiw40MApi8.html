<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>I Have a Java 8 Stream | Coder Coacher - Coaching Coders</title><meta content="I Have a Java 8 Stream - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>I Have a Java 8 Stream</b></h2><h5 class="post__date">2015-06-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/GRiw40MApi8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so thanks very much for attending
it's great to have you here so I'm going
to talk about streams one of many
presentations at JavaOne talking about
streams I think many of us are coming in
from different angles so you can attend
I think you can send multiple
presentations on the streams and get the
best out of it from seeing different
people's views on how they presented so
how many people have been to a streams
presentation already okay so everyone
wears about 25-30 percent so I've seen
basic stuffs I can go for as well for
people so the agenda go for some basic
stuff to start to warm things up then
I'm going to talk about the parallel
game you might have heard that in
streams you can go parallel and use all
the cause of your system and keep them
hot it's not necessarily the right thing
to do but we'll explain
I'll help explain reasons from why you
might want to do it and why you might
not want to do it and then I'll talk
about the future near and far of streams
I will see how far we get forward I
might not get to the future because it's
too far away we'll see okay so the goal
if you can get something out of this is
to help you build a mental model of how
streams work so when you write your code
yourself you have this mentally model
mental model in mind when you're writing
this when you're writing your code
there's code here you can download you
probably download it now for Wi-Fi works
and might play along if you have grasp
is installed and chrome installed you
might be able to run some of the
examples while I'm running some okay
I'll try and speak a bit louder okay so
if you have graph is installed and
chrome installed like my Mac does you
might be able to play along with some of
the examples as I go through them so
what's a stream it's an abstract view
over some data that's an abstract way of
explaining work stream is to perform
aggregate operations on sequentially or
in parallel that's essentially it it's
just a view over your data it can be
view over a list and array lists lines
in a file stuff like this characters
over a string so forth like this but
what's it not really kind of stream is
an overloaded term
so you'll see it used in other places
too it's not really at the moment a way
to consume IO based events it's
primarily data orientated at the moment
it's not really something to be reactive
if you want reactive stuff rx Java is a
thing you want to use and also this
little new class in Java 8 called
completable future which i think might
be the basis of some form of reactive
approach to streams it's a very
interesting class - it's not really a
way to richly distribute computation
over a network there's a there's a
session on distributed stream tomorrow
or Thursday I can't remember
and there was a session on rxg avec
yesterday if you have a time machine you
can go back and watch it so there are
two things here you if you're interested
in how streams can expand out in terms
of concept they all copy the same
patterns though you'll see filter flat
Map Reduce concepts in these two in fact
coherence talked about at the keynote
they were using just stream to
distribute computation in quite a clever
way but this distributed stream is a
richer way of describing the computation
over Hadoop over coherence and stuff
like this but the same concepts apply so
let's go into the basics and let's start
some code going I hope you can put your
screen is a little small I was hoping
for a bigger it's not DevOps style
cinema screens um so this is this is
really basic what I want to do is I have
some words a list of words I'm not
saying what the words are yet and I just
want to count them up and I want to
filter out all the empty strings so list
has a new method on it called stream
like so and we call filter we have a
little amber expression filter out all
the strings that are empty and then we
do count like so that's it so just to
show you that it actually works you'll
see a very exciting number come out 68
so that you can imagine writing that in
a for-loop - there's probably not a
little difference between them well it
does look at it up up the level of
abstraction what we're doing here
there's more sort of what we're doing
and we're not saying how were doing it
was no there's no garbage variable
counting up the sum and so forth like
that going on we're just counting up
stuff so what is what does what is count
composed of so let's have a look go into
the implementation of stream I'm
actually count is composed of two other
things a map and a sum so let's replace
count with Matt and some like so so how
do we count in a stream we map all the
elements to 1 and then we sum up the
ones it's that simple
sometimes you have to twist your
thinking when doing the stream ways
you'd normally do an imperative way so
we have essentially two intermediate
operations filter and map and sum is a
terminal operation and the work starts
we call the sum it doesn't start when we
call the filter and the map that's just
a recipe to say what to do when we kick
off for some map so what we create a
fused little class inside that does a
filtering does the mapping and then does
something like that we don't copy a list
of words into a new list without that
they're none empty strings we don't
create a list of Long's and then some
more up it's actually quite efficient
underneath the covers what we're doing
so what is some actually composed of so
let's have a look at its implementation
in stream it's composed of something
more generic called a reduce operation
so now look what we have we have filter
Map Reduce you can do it locally on your
computer you can do it big in Hadoop
abouts essential concepts going on here
filter MapReduce so let's move on a
little bit now well you could do this
it will give you the same result as
before so when it's ready to run did
that run under that compile let's run it
again there we go
I just compiled 68 so it ran we went
parallel but the key question is did
that go any faster
or not well we'll answer that later do
people think that we'll get our result
faster or the people think that we'll
get the results slower
good quick good answer so I had 68 words
in probably not so we'll get on to more
reasons why that's the case and so for
about so so let's switch rather than the
rather than counting the words let's
join all the words together to create
one big string so let's get rid of our
map and instead let's sum let's change
this to a string up here and call it
it's got WS like so and we want to write
a little lambda a strings a and B atom
together joining together with a little
space in between like so so is that do
you think that will join all the words
up together correctly I think there'll
be a problem at the beginning this man's
on the book on the game here let's have
a look so I forgot to do the safe harbor
statement before so I'm doing it now you
can you can read it if you really want
through that's all by joining here so
indeed we have a rogue extra space here
so perhaps reduce is not the right thing
to do in this case for it might work
well for integers but for Strings in
this case maybe it doesn't work too well
let's let's get rid of that
so oh now maybe if the stream is empty
what's going to happen are we going to
return null do you think what happens if
there's nulls in our streams so what we
have to do is we have to describe it
saying this is an optional value which
may or may not hold a string and if we
print that out come down here we get a
lovely safe harbor statement printing
out optional there's no space fair okay
so is there any can anyone see a problem
with that is it efficient no musically
lots of garbage player you think it's
going to stress the GC out a little bit
can I create lots of strings lots of
memory going on it's the right thing to
do is really is hunt for something
called collect and it has a bunch of
recipes that you can pick off as a
static import collect joining with a
space this is the type of thing you
should be more reaching for rather than
reduce when looking at stuff like this
if you want to collect a list or set
something is collect as you really want
to look at and if you recently closed
you're introduced something called
transducers I don't know if you've heard
of those but these collectors are kind
of similar in a smaller way they take a
reducing function they take a reducer
and return a new reducer out of this
they're kind of similar so that's the
thing you want to actually look for and
use for and you can avoid the optional
and reduce and in fact reduce comes with
a bit of baggage which I didn't explain
there was 0 there was empty string and
there was a function and there are
certain rules that will be talked about
in other streams presentations more than
here around identity and associativity I
put them in the code here so you can see
what they are as like a cert so you can
have a look but the empty string should
be asserted on the identity and the plus
a plus B and so forth should be
associative and Stewart marks and I
think the next talk will actually get
into that in more details I'm skipping
the rules of the game if you like
in that bit also if you go parallel here
there's certain constraints on the
functions that are getting called they
have to be stateless again other talks
will get more involved into that better
than constraints you have here because
this will get caught by multiple threads
when we go parallel so you have to be a
bit careful because unfortunately Java
is a mutable language although I'd like
to remove removed mutability most of it
from the jar language we can't do that
so we're stuck ok so that is the basic I
just wanted to show some fun stuff so we
can actually rot13 that safe harbor
statement with a new string stream so
let's have a look what I did so we have
our text or our safe harbour statement
we map each character rot13 it and then
we collect it to a new string builder
creating a new string builder appending
the code point each time and if we go
parallel we have to merge intermediate
results and get the string back so let's
print out that so you can see the rot
for a team version and a longer version
together just a bit of fun okay so
that's quickly the basics so going
parallel I showed a bit going parallel
there by default the stream is
sequential but we can make it parallel
and essentially we have one bit of
information for the whole stream
pipeline doesn't matter where you put
parallel in that pipeline it's always
either on or off doesn't matter where
you put it you put it multiple times and
it's to get results faster maybe it's
not a magic bullet you can't put
parallel on all your streams and expect
it just to somehow wish magic computes
out the air and make it go faster
unfortunately we can't do that type of
magic so parallel is greedy it almost
always takes more work to get stuff done
using as many CPU resources as possible
you give it all the resort you give it
all the CPUs it will try and use them as
much as possible keep them all hot an
attempt to get a result faster yes
yes sir wait is busy if I do I'll get to
that in a bit oh and if I don't if I
forget to address your question please
raise it again so here is the basic
algorithm we have underneath for covers
at a high level a concurrent algorithm
so what we do is we split the source of
n elements like those words we had those
anywhere those 68 words into L leaves
preferably covering each each leaf
covering approximately the same number
of elements and then we evaluate
sequentially each lis producing L
intermediate results and then we merge
those results together producing a final
summary result that's it but the thing
is each of these steps can be all
happening concurrently and in parallel
and the trick is to coordinate this all
together and make it really efficient so
it sounds it you could do each step
sequentially here one by one they want
but that wouldn't work very well so
let's have a quick peek under the hood
so I've got some more stream code here
so let's have a look what we're doing so
we're starting from an int stream going
from naught 2 1 to 7 this is the stream
of numbers all the way from north to 1/2
said in the range we're going parallel
we're boxing them and then we're
collecting them using a collector called
grouping by and I'm going to you is
going to group all those numbers by a
key so I want to return a map of string
and list of integers those numbers yes
into an int to an int integer yes yes
you have to use that we care because
boxing has a cost I hopefully have time
to get onto so it just boxes it up and
we box it up so we can use the grouping
by operation and create a map a list of
integer here so the grouping by I want
to group by the current thread each
integer is processed on and if say
integer 2 is processed by thread ID 1 in
industry or 4 is processed by thread ID
1 they'll both appear in the same list
in the order that were processed so this
I found this
surprising way of actually peeking under
the covers of what's going on in a
stream so let's run this and I'll print
out the entry set using another method
we added to Java rate called for each on
that which takes a unit value that's
searches have a quick low pass a default
method on that there's lots of default
methods sprinkled around which make it a
lot easier to do stuff now it was
previously the case in Java 7 let's run
this let's see what's the output okay so
we get a bunch of output here let's see
this so we have seven fork/join pool
threads and one main thread civic first
thing is we're using fork/join
underneath the covers fork join is
library written by doug leave a delay it
allows you to do sort of decomposition
and merging back of results i talked
about in that concurrent algorithm that
algorithm i showed there the base the
basics of it is supported by fork join
and we see that each pred processes a
bunch of integers and if you look it's
approximate minimum size is about 4 so
we have a hundred and twenty eight
integers and a minimum chunk we're
processing on a thread is about 4 so
that means
128 divided by 4 is 32 so we have 32
leaves but we had just eight cores so
we're actually compressing more tasks
than threads to balance out work if you
like because of thread scheduling
effects and stuff like that so we fudge
factor it by 4 to create a sort of
larger computation but you can see it's
processing it's also processing in order
in bits so we see naught 1 2 3 4 5 6 7 8
9 10 it might jump up 15 32 each thread
gets different one you notice it's lumpy
as well we get different sizes some
process for some process 28 some process
more a reason why it's lumpy is there's
not a very high cost to this pipeline
we're showing it's actually not a lot of
work to do
so we're we're sort of exposing out fred
scheduling effects and stuff that takes
a while for the fork/join pool to warm
up and this is a very cheap operation so
what we can do we can fake some work
yeah we can call peak and we can consume
CPU buy a bunch of tokens I copied this
from a benchmark framework called jnh
which is I'll mention a bit later so we
can consume CPU by saith say ten
thousand one hundred thousand tokens or
something like that let's run this we're
pretending to do some more work when
we're doing this mapping and let's see
what let's see what the output is now
what we see is much more evenly
distributed as we do more work we're not
so much at the mercy of thread
scheduling effects because they're a
minor part of a computation now compared
to the overall computation it takes so
each each thread gets about sixteen
elements to work on so it's nicely
balanced out so that was parallelism of
eight now if you look at this on the
stream we just have parallel have any
arguments to parallel you can't control
the number of threads directly per
stream execution or anything like that
it's based essentially on or off so if
you want to control the parallelism at
the moment you have to go in and set a
system property in form join so if I run
the same one here with this system
property and set my parallelism to zero
and run this let's see what we get
we just get it on the main thread okay
so that's the way you control it we
haven't expose out a way to control
powers in just yet at the ended I will
talk a bit more about how we might be
able to do that but we we took the easy
route for moment because exposing out is
actually quite hard to do in the correct
way you might want to do this if you're
running in a web server or something
like this
where you're you're sizing your your
your cores to process requests rather
than a compute intensive operation per
request of somebody so you have to
balance out your resources carefully and
in fact I expect most application
servers will disable the parallelism so
that streams always go sequential
yes one let's try it so I'm going to
place a zero by just one and I'll run it
so we get two creds we get the main
thread and we get one fork joint
essentially it's it's the number of the
fork join pool parallelism is supplying
so I'm going to apply it to the main
thread I'm going to use one fork joined
pull thread essentially but so you can
play around download code and have a
play as well you can try it out you can
max it bugged your number of cores I
don't think it makes any difference
given that number of cores like that so
that's split up into two two pieces of
work I think yes now this is part of a
library next talk I think we'll go into
that in more detail about how it works
under the covers but essentially this
grouping by here
it essentially returns a collector a
little hard to explain very quickly but
at this if you want to group like multi
maps if you want to easily create multi
maps grouping by is a thing you should
use and by default it will it will group
into a list that you can you can change
it to group into a set or do counting
and stuff like this you can build up a
sort of description of how you want to
reduce at the end and the terminal
operation with grouping by that's kind
of how I connected to closures
transducers if you've ever ever seen
those come out so that's a little bit
under the hood so let's talk a little
bit more about when to use parallel
streams so I'm going to talk a bit about
heuristics and measurement I'm going to
talk about visualizing stream pipelines
so the first bit you can think of
calculations on the back of a napkin
just to give you an idea of how or when
you might want to use parallel streams
and then I'm going to show you Tillet
Eve how you can visualize what's going
on and that you may find that useful to
apply to you
in stream calculations just to get an
idea of how things work okay so there's
a great article just written by Doug Lee
and a bunch of people like Brian myself
alexey shavelev and this is an article
about when to use parallel streams so
there's a URL this is like a little FAQ
of what's going on why doesn't it do it
this way and so forth it's really well
written and I think it's a very good
guide and I'm going to use some of this
guide in this talk and I'd love feedback
on this what don't you understand about
this what can we improve on this because
this is a temporary location to put it
somewhere else more permanent but I
highly recommend reading it
so the heuristics so that document
they're based on sort of current
hardware can't con common hardware like
this MacBook and so forth like this if
you have a sequential version and it
takes say a hundred microseconds or more
you might have a chance of getting a
parallel speed-up that's the cost it
takes to warm up all the threads and
split things up and merge things back
together again about 100 microseconds on
on current hardware but if you you can
estimate this I'll show how but if in
doubt I highly recommend measuring
yourself and the best thing to do that
with is a tool called jmh Java
measurement harness and that's a tool we
use in the jdk group ourself to do micro
benchmarks and to do nano benchmarks and
stuff but it's also useful to do
measurements on streams and the code on
I have two examples in jmh you can look
at so say we have a collection what I'm
calling a source highlighted in red and
I have a parallel stream and I have a
map with a lambda and a reduce with a
lambda so question you can ask is does
my stream source have good splitting
characteristics I will dive into what
that actually means but does it does it
split like I said in that concurrent
album does it is it nicely balanced with
a number of elements in the source for
example because if it is then equal the
cause will have equal number of work to
do what is the size of my source M is it
small or is it large if it's small what
is the cost involved in that if it's
large so these are questions you should
ask so we get two Landers you can ask
what is the cost per element of which I
call Q which are called Q which is
essentially lambda 1 plus lambda 2 what
is the cost of that that is a cost of
processing an element through the
pipeline each time and you can ask stuff
do do certain intermediate operations
remove elements like filter or do they
add elements like flat map or does it
short-circuit the stream like limit if
you've got limit on your pipeline and
you're going parallel that's often a red
flag it's actually extremely hard
to optimize limit and make it go
parallel because it has states
associated with it maybe you can get
into that afterwards if we want to talk
about is but it's extremely hard to
parallelized and is that unboxing or
boxing going on if you've got boxing and
unboxing going on you're going to fresh
that you see so you have to watch out
for that so the cost of this pipeline
would be n times Q and you can actually
guess Q and Doug talks in this article
you can guesstimate it to the number of
lines of code in your lambda so X X plus
X X plus y a sum or something like that
is one line so if n times Q is over over
10,000 then parallel execution might be
worthwhile it just so happens these
numbers are rounded up you can fudge it
factor with a zero or two more if you're
not feeling so brave or not but it's
kind of funny it works out like this but
it we kept this simple this is kind of
back of a napkin type of calculation you
can ask yourself it could be the number
of lines it could be based on given that
a sum is about the smallest most fastest
thing you could do it could be based on
units of sum instead where one is the
unit of sum and 10 is U is 10 units of
sum and so forth like that so in summary
what it says in this document is is that
using parallel when there's not enough
computation could cost you about a
hundred microseconds on modern hardware
and using it when it's justified could
save you at least as much as this or
maybe hours depending on the types of
computation you're doing so this is a
graph I got of measuring a sum of in sin
an array so I did a raised stream but my
array and then called sum on it like so
and the blue line is a sequential stream
compared to a for loop normal for loop
and the Green Line is the parallel
stream so we see just looking at the
schedule stream but it's really hard to
be too followed maybe we'll get that at
some point
but it takes maybe about thousand
elements to be to for-loop if you're
dealing with a list of string list of
stuff like that we can get as good as a
for loop using an iterator but we're
iterating over an array and an index
it's very hard to be but we see it it
gets equal to the imperative loop at
about a thousand but look at the
parallel is really really slow really
reset and then it slowly starts to pick
up and about 10 between 10,000 and
100,000 it crosses over so that's this
point where it starts to get worthwhile
and that kind of hits that margin so
doesn't make this up so it within this
fudge factor it's sort of it sort of
gets and if you've run this on your own
hardware you should see something
similar I think and then afters we reach
a million elements we get our 2x I ran
this on a two core x86 system if you're
doing benchmarks like this on your own
make sure you down clog the CPU to 2
gigahertz and set it to performance
freeze of CPU because if you don't
you'll be subject to turbo boost which
has a very strange effect and you'd be
subject to the CPU doing power saving
techniques and your benchmarks can go
all over the place you've got to watch
out that if you're doing a benchmarks
yourself to get meaningful numbers so
that's what I call a load Q some is
about the lowest you can get so here's
an odd isn't here's the other extreme
which is a high Q here's generating
probable primes in big integer there's a
method to generate probable primes and
that's actually quite an expensive
operation so we see the the the
sequential compared to the imperative
approach is the same as 100% one for 16
64 probable primes doesn't matter as
soon as we get to four probable primes
on a parallel we're already 2 X in fact
we're over 2 X which is kind of strange
how can we be faster than the number of
cores in the system and I think is
actually the sequential is not running
full speed for some reason I don't know
why that is strange should behave I've
heard people say that you can encounter
this because when you're running it
sequentially you hit thermal range of a
chip or something like that I don't
quite understand that but I think the
sequential is running slower and the
parallel is
running actually fast so that's what we
get this artificial boost up there so
that makes sense so you could be in a
range of extremes here low so the lower
the cube high the end needs to be the
higher the queue below it needs to be so
what about visualizing these stream
pipelines to understand the splitting
techniques and to understand what the
intermediate operations are doing so I'm
going to render a representation of a
parallel stream computation to a dot
file I'm going to convert that dot file
to STV I'm going to be at view it in the
browser how am I going to do this so we
like before we got a collection got a
source you want to make a parallel
stream out of it and what I want to do
is I want to adapt that stream so I can
listen to what's going on how it's
splitting and so forth like that so when
I doubt the source stage a monitor for
splitting or decomposition of those
elements input into the intermediate
stage which is that which is the map
there's something under the covers that
each collection implements it's called a
splitter ater it's like an iterator on
steroids and we can it a splitter rater
as its name it can be iterated over in a
very efficient manner and it can also be
split in two and then you can iterate on
the two concurrently but I'm not going
to go into details of split erases
itself that's that's how I'm adapting
underneath the covers and then I will
peek before the terminal operation stage
so I can count how many elements are
going through the pipeline between the
source and the intermediate because
there could be a difference between the
two that's a better way of expressing it
I have a function which evaluates I can
take a stream source in I can take a
function that represents my intermediate
stage in this case my map and I can take
another function that represents my
terminal stage - dot dot represents a
way to spit out my dot file somewhere on
the file system in fact let's actually
have a look at that code
okay I'm using j-unit just purely
because it's a great way to execute
stuff in the IDE per method without
having lots of in a class static inner
classes with main methods in it and it
was pre and post actions so it's
actually turning out to be a useful
framework just for demo purposes so
let's have a look at what's going on
here so I'm going to show first of all
how an ArrayList behaves in terms of
splitting characteristics and I want to
graph it and what's graph doing so graph
is essentially taking my source stage I
passed in from the collection my took my
intermediate stages identity it's doing
nothing and then my terminal stage is
just a reduction like so just a warning
here but it's doing reduction on boxing
so don't just blindly copy that as a way
to do it it's just for demo purposes
this so you watch out for that and in
fact there's ways to get around that so
it's going to essentially graph the
source stage computation and do a
reduction so let's actually run that and
cross fingers actually works there we go
you can make that bigger okay that's our
computation of ArrayList fair about Rea
list is for a million elements 2 to the
20 like so let me try and explain what's
going on here in this so we have our
root node at the top we split it in two
and then we split each node and so forth
like that down and so and our depth of
the tree is not one two three like that
and we create one two three four five
six seven eight tasks so the reason why
I'm creating eight tasks and not 32 like
I showed in that just that peek under
the hood is because I am limiting the
parallelism of the stream to two so if I
if I change it to eight and just run it
again
I will get a much much bigger graph like
that you see that so that's 32 tasks
coming down there you know it's very
hard to display in the browser for
presentation purposes so like I want to
limit it let's change that back so you
can imagine if you've got a an 80 core
machine you're going to get a quite a
big graph come out so let's get back and
explain what's going on here so each
node here represents a full joint task
called counting completer and on each
task knows its size of the elements that
covers the top level task covers two to
20 elements it knows its spread ID and
there's the time it actually starts
processing like so we split it in two
and each node gets half the dark gray
squares here mean I know my size and I
know all the sizes of my split and my
sub splits down that's an important
thing to note because it means we can
optimize under the covers when we do the
parallel computation so this this node
got acted on and split on Fred eleven
one there Fred 11 and that one got split
on Fred 1 so you can see there they're
operating concurrently here
I wouldn't take too much in terms of
times in terms of performance because
I'm running cold from the JVM so I
wouldn't read too much intervention on
the performance test and in fact if
you're not on 8 you 20 on the Mac you're
going to get even stranger results
because the granularity of nano time is
actually milliseconds on the macron 8 in
8 and you have to upgrade 8 you 20 and
it would be much better terms of
granularity it was fixed recently so we
split we split we split and then the
round ones here represent the actual
oops I didn't mean to do that I don't
want my photos let's pause now watch
I'll go I'll do it here instead
so the round ones here represent the
traversal state if you like the knot
nodes in the graph are just saying how
I'm traversing so when I traverse a leaf
here i'm saying i get i get these
elements in and I get these elements out
that because the the intermediate stage
with just identity there's nothing going
on there and it says the duration it
took and then I'm calculating a queue
based on the the duration divided by a
number of elements there so you get
approximate but if I I think I be
interested to hook this up in some way
to jmh so I can get some accurate timing
results as well as rendering of the
graph to see how things work out but
that's had some exercise for later so
you can see that message here is a
railer splits really really well
ArrayList is very good it splits very
nice and evenly we know the size of
everything that's good too
we get essentially an equal amount of
work so let's have a look at a different
thing let's look at hash set any one
thing hash set will hash set be the same
as a ray list we different you want to
know why it might be different it uses a
table underneath doesn't it do you think
be do you think you'll be able to split
evenly what do you think it will be
lumpy okay so look
okay this is hash set so at the top here
we have a light gray square and light
gray and signifying means I know my size
but I don't know the size of my splits
so when we split we only get an
approximation we divide by two saying
well there's probably approximately n
over two elements covered by this node
here but we don't know exactly how many
and the reason being is hash set is
under the covers a hash map and it has a
table and in the table each elements in
the table is a bucket with a linked list
of elements and approximate half and
half might work out if you have a good
algorithm for hashing your keys so if
you key if your hashing is good to go
good hash code on your object you're
going to get an approximate distribution
of stuff in in then we split again we
split again it's all approximations so
let's have a look down at the bottom
what's going on here Oh on our left hand
side on the right hand side here we
don't have any elements at all what's
going on we we have zero elements coming
in to the pipeline and it's all shifted
over to the left here and we have double
the amount of elements that are
approximately so we have an
approximation here but we have double
the amount of elements processed here
why is that that shouldn't happen it's
actually a bug it's actually showing up
a bug here and the reason being is ABC
if I can explain this um we're doing
integers and the hash code of an integer
is the integer itself and the hashing of
that in hash map it takes a top 16 bits
and shifts them down by 16 and excells
and move the lower 16 bits so it's it's
Majan we're dealing with a million
elements just taking the top four bits
and shuffling with the low four bits
these make unique cash codes but it's
all shuffled up to the into the top of
the rapers now there's no conflicts here
in terms of keys each element gets a
unique is unique into its bucket in a
sense it's linked list the size of the
actual table is double that
a million elements get shuffled into the
first half of the table in a sense we
need to improve our hashing algorithm in
in the implementation of hash map I
think and I didn't realize that until I
looked at this so that actually some is
worthwhile I need to log a bug I think
there's a bug logged already we need to
prove the hash code but it only for
integer if I did this with strings it
would much more evenly distributed so
you can see that if this if you had a
bad hashing function in hash set and you
did parallel computation like this I
don't be using half my cause the other
cause will be just doing nothing there
is a bit of load balancing because we
have more tasks and cause it might
compensate out a bit but I'm not being
efficient as I could be okay let's this
will take a little bit of CPU time it
gets scheduled and work it might finish
quickly so there might be an L to do
more work but we're not we are reducing
our ability to load balanced essentially
so we might not be as effective there'll
be some conversation that's why we
actually over balanced as well as
precisely because we can get lumpy
computations like this ok so hash set is
a has reasonable splitting
characteristics if you're got a good key
hash code generate good keys so let's
have a look a linked list do you think a
linked list will look like a hash set or
an ArrayList do you think you look like
a ray list how do you think ArrayList
how many things something completely
different okay one lady over there let's
have a look should take that ok let's
have a look at linked list
oh let's let's scale that down what heck
is that
if that goes on and on and on why is
that well it a linked list is inherently
sequential you know you just have to go
through linearly one by one by one by
one so the way you parallelize this is
you have to copy elements into an array
when you're splitting and then operate
on that in parallel and when you split
again you copy another set of elements
in pureed and you split again any copy
another set so it's essentially a right
heavy treat we're creating so we're not
really balanced at all so you see the
size here is two to the 20 the first
split we take 124 elements off the
second split 248 and so on we
essentially an arithmetic progression
growing bigger and bigger and bigger as
we go as we do more work and if you
notice as well we operate on Fred I eat
ID one for the first two elements first
two tasks and we changed to Fred fred ID
12 and then we change back to Fred ID 1
what we're doing is we're switching the
splitting because if we just split on
Fred ID 1 all the time would be
generating a huge number of tasks and we
actually start getting out of memory
area so we have to be quite careful
about how we control how we split like
this no it was a nice little trick Doug
found because I couldn't work it out and
don't help me so it just goes on and on
splitting like this is so it's but
linked lists generally has quite poor
parallel performance so he using a
linked list you have to be careful see
it takes time to copy those elements
into an array it takes more time to copy
those elements into a to operate on but
actually operating on the elements
themselves you're not going to get any
whim so usually you can get parallelism
with linked lists if you've got a very
high q-- but otherwise you might get
poor performance out of that so watch
out for a linked list
so we have a linked list what about an
iterator instead so I I got my content
which is here I'm getting an iterator
out of it and I'm saying I know my size
of my iterator so I'm creating a stream
given this Raider in size I'm using an
actual little util I added that's the
actual public code you'd use to create
an iterator we've actually made it
difficult to create streams form
iterators slightly on purpose
because it's they have as you'll see
certain characteristics so the iterator
of a known size do you think it will be
similar to what I show with a linked
list or something different
same yep it's exactly the same so linked
list has got the same algorithm as
iterator and that in fact many of the
queue type queues concurrent queues and
that will have the same splitting
characteristics so if you don't if we
don't know the size of the iterator
instead this is an unknown sized
iterator and his question mark we don't
know what it is we can still copy a
hundred and twenty four elements off as
a first split we can still grow alias we
don't know what the size is at all like
so let's see an iterator of an own small
size so so we have something of two to
the 12 instead elements and we know its
size do you think that will be different
shaped to the the large iterator
well-known size 2 to the 20 elements
let's run it
it's actually different now because our
size is small we actually start
splitting on the arrays that we we've
actually copied across because we what
we want to do is stop splitting when n
over 8 times 4 we get to 5 1 12 we
actually stop splitting as if there's no
point splitting anymore let me go back
to ArrayList and explain that I don't
think I explained the threshold of
splitting so we have the ArrayList again
what we want to do is stop splitting
when the size we cover in the task is
the top-level size divided by the
parallelism times 4 so we stop splitting
here we say there's no point going
because we we have enough tasks to do
work so on this case with the the
iterators because we know the size at
the top we can split and then we start
splitting the array instead because the
threshold is is much lower than that
when the iterator was very large and
then we start doing some work here so
it's slightly not quite so right heavy
as the large one so if you have low if
you have low-end but high Q you can get
better parallelism how to be reiterate
er
so we have an iterator of unknown size
and small we don't know its size have a
quick look what that is we see this
right heavy shape again because we don't
know the size we don't know the root
thing winters actually stopped splitting
so we just keep going and going going we
don't know what n is at the top we don't
know what the threshold is so we just
keep going and then we stop a certain
point that make sense I think it's very
it's easy to download code and play with
it as well to experiment with it so
here's another one files dot lines so we
added a new method to files which allows
you to get a stream of lines from a path
so it's txt file so I'm opening up
Ulysses not we use these by James Joyce
and I'm graphing the lines and graphing
with a empty intermediate stage and just
counting the number of lines up like so
so if I run this one I get the same type
of right heavy tree from files outlines
I don't know how many lines are in that
file so I do not know what n is so I
have to copy off the elements that's
actually a poor algorithm for files dot
lines though can anyone think of a
better one we could implement instead I
think the guy from Maurice who's going
to talk next actually implemented it in
his book he's just released on lambdas
and streams we can mend map the whole
file we can chop it in half and go up or
down to find the next carriage return
and work from that so we can actually
get better splitting characteristics
from files in this if we if we implement
a better argument but we were lazy or we
didn't have enough time to ship Java
rate so we had to do the dumb thing and
implement it using an iterator instead
but we can do better in certain cases
there's a number of improvements we can
make to certain sources to split better
ok so here's a stream range just to get
the point across another example so
let's have a look at the range
characteristics of a splitter our
same as an ArrayList so often a
technique to go paralyzed you start off
with an in train j-- over the where you
want to go and you use that as a trigger
to do some work okay I got twenty
minutes I think joined carry on on that
okay so let's have a look at some
intermediate ones like so so let's have
a look at filter so I want to create
some content of million elements filter
out all the even ones and then reduce it
so this is just an ArrayList again as a
source so we see that nice splitting
characteristics but now let's look down
here at the the actual traversal we see
we're actually only see be in and out as
half a difference here so we notice that
the filter is only letting through half
a number of elements so that's what we
would expect and is again there's an
even amount of work here if we're going
to filter below a hundred instead rather
than just all the evens so we see we're
actually getting in here over thirteen
hundred thousand and we're getting out
of 100 because we're building just a
hundred and we don't care about all the
other elements in in our list and so all
the other cause don't have any all the
other tasks don't have any work to do so
that obviously is not going to paralyze
at all it's just going to be excusing
the equivalent of sequential so if
you're doing a parallel thing with
filtering and chucking away most of the
stuff there's nothing for you to do in a
sense so let's have a look at flat map
do do people of people familiar with
flat map ok so flat map is a bit like
map on steroids it allows you you can
use flat map to do filtering to remove
or you can use flat map to inject new
elements into your stream so I can
substitute each element for n number of
other elements essentially so in this
case what
doing is I'm substituting each element
in my stream for a range of integers all
the way up to that element times 8 so
I'm injecting quite a bit more data and
it's going to grow and grow and grow as
the elements get larger and larger in
the source stream essentially so what I
want to simulate is more work so if we
look at a flat map here look down we see
in in these 128 but out grows and grows
and grows and grows like so does that
make sense okay just have a look at that
again I'm limiting it to a thousand
elements in my input so thousand tire
divided by eight is 128 I think but I'm
injecting a lot more elements into the
stream so the first task has some work
to do second task has a little bit more
third more and so forth like that so
it's going to be very imbalanced
computation this is what this guy is
going to take a lot longer right at the
end here she'll take a lot longer than
the first one again don't trust the
duration times here because we're
dealing with warm-up timing and Java and
the jet and stuff like that but that's
that's the idea so you're doing flat map
stuff like that it could affect the
distribution of your the balance of your
computation so I'm going to skip Linnet
I think and just go to
don't most yes question can can you
repeat it a bit louder please
are you can't control it it's all under
the covers so you have no current way to
control the splitting tactics except by
writing your own splitter ater in fact
but there's no way under the covers to
say okay you stop splitting when you
reach this threshold instead of a one
I'm going to do by default that's
something we could add in the future we
need we need some form of strategy to to
specify how that how that's behaved but
we need mechanisms like that in the
future I think so here's some terminals
we showed reduces of terminal operations
so let's say we got an ArrayList of
integer and we want to find first now
this would kind of a silly thing to do
because we can easily calculate the
first element in the array list we're
just picking off but it's just show you
how the computation works because you
might have other work going on in the
intermediate operator just to show you
how the graph works and find first so
let's have a look so if we look at fine
first fine first wants to pick out on
the the left for the first element in
and it gets the result but we see some
some tasks get cancelled because we've
already found what we want so we don't
need to do some tests so we can cancel
those but some tasks run and we can't
cancel the thread so we have to wait
till it's finished we have to wait for
the fork/join pool to quiesce before you
return the result but some tasks don't
actually get to run this it's called a
short-circuiting operation and we can do
the same for something called find any
yeah
so fine first we'll find the first
element in the stream as input to it so
the first element comes in that's the
result I find any is any result in the
stream will do now when you run that
sequentially it's naturally going to be
the first element but if you run it in
parallel because we split find any
doesn't return the first one here it it
it will return the first one that
finishes first essentially so that makes
sense so it doesn't matter we cancel we
cancel this task because we're not
interested in the first we're just
interested in le element so we split it
all up and start executing the first one
first one wins and some tests don't
don't run so it can actually be more
efficient find first and find any
depending on the types of computation
you're doing so if we look at a fine
first filter yes am I want I don't care
about half my tree all the elements to
the left of me but I don't care about
days I want to filter all of those away
and I still want to find the first one
so if I run that there's nothing going
on here there's no al tenants on the on
the on half the tree nothing there and
there are elements coming out here and
it's actually the second element we want
not the first element and it's a result
there that's why it's to come in and one
comes out one one comes in one comes out
but we further away that result because
this one one that makes sense was like
so you can I had a lot of funding as you
can apply to your own streams and
experiment with it if you just hook up
graph is and the j-unit framework it's
very easy to experiment with this stuff
and hopefully you get it a feel for
what's going on underneath the covers
that two inches if these say where this
is going to work for you
change the algorithm mean change the
internals of how we compute well we can
search for files dot lines we could
implement that for Java nine or weaker
and we can also back port it to an eight
you 60 release or something like that if
we wanted to so because it's all
internals it doesn't affect a public API
we could actually incremental II put in
improvements to Java eight releases as
well as nine yes we would not try to
have negative in the yeah hopefully
positive but again it depends on what
you're doing because you if you're
parallel stream is a linked list then
I'm not sure we can pull a rabbit out
the Hat and depending on the data okay I
got ten minutes left so let's zip
through I'm going to skip boxing I may
come back to it and prove my animations
so that the future Nitin in relation
what I talked about here so a lot of
couple of you asked about how to control
the parallelism essentially what we need
is some way to express strategies and
have a sort of this is my straw man here
and then what it's going to look like
but if we had a way to pass in a
strategy when we go parallel to say no
parallelism is a simple strategy if
something I Oh based parallelism if
there's i/o going on in streams you have
been very careful with i/o in the
fork/join streams but if if there's a
way to express as i/o going on we might
be able to do better job we might be
able to control the splitting with this
types of things into what we don't want
to do is make sure but when you define a
strategy is specific to a particular
machine configuration we need to make it
general so it runs on a two core a core
eighty core type system and these
strategies are meaningful across app
across those types of hardware as I said
there's improved splitting
implementations like files dot lines
there's also some improvements but I
need to make to the limit operation to
improve it in certain cases I didn't
show limit you can play around with that
in the intermediate ops and render the
graphs to show what's going on
we can add more operations there's
certain operations that are missing that
people keep asking for like take while
skip while if you're familiar with Scala
closure stuff like this we all have
these types of operations we didn't add
them in the first round we probably we
may add them in Java 9 the thing is they
have they can have poor parallel
performance as well because essentially
we have to check when the peda cut is
true we have to stop the calculation so
we have to manage some state we're going
parallel and should that predicate mean
we stop the calculation right now or do
we let more stuff go through as a sort
of strict and a relaxed type of take
well skip while we can have so about I
think we can add those there's other
sources we added stream to split on
pattern we didn't add it to match her so
we can add sources too there is probably
a bunch of sources more we can add the
stream method to and we can have a bunch
of helper methods if you use flat map
there's a bunch of methods we can add to
help you do that
one of them might be on the class called
optional and another one might be on
stream called AB na label if you play
with flat map it's sometimes tricky to
to use optional with it and to create a
stream so a longer term so in the in the
near term we could actually develop a
pluggable spi so you can add your own
operations in rather than just have the
ones that we supply for you so we pulled
out something called cumulate which is
prefixed some because we didn't think
anyone would use it but if you wanted to
use it then you could if we had this s
you have this spi you can add your own
back again we could add something called
by stream which allows you to deal with
pairs of values if you have used maps
with streams and laptop entry can be a
little painful sometimes so equal to add
a bi stream like this but on a distant
horizon and just before Brian talked
about it got pulled out from the keynote
we was going to talk about value types
and extending generic set of values and
stuff like this and I think it might be
some talk in the ending keynote about
this I think again it might might be I
look on I look ahead on the horizon like
this and I see these coming along and
I'm thinking well if these come along in
Java 10 maybe
remember that safe harbor statement that
trust beyond that if he's come in 10
maybe we shouldn't do these things now
because you might be able to do a better
job so we're trying to balance if you
put these in now we go oh no we could
have done so much better in 10 so when
these new language and library speech
come along it affects what we do so
could we bears in stream in we've added
in stream to operate an inch to make it
faster so we don't do boxing in stream
does not extend stream but could we in
the future make in stream extend stream
event with value types and generics over
values I hope we haven't messed up on
the API design to make it happen because
if it could that would be really good
could we have tuples we have to 'pls do
we need by stream I think perhaps we
might but we'll see so we my opinion is
I'm hedging my bets against by stream
I'm not hedging my bets against
pluggable operations because I'd like to
I think there's more features in a10
that would make that better so there's
this tension between what we do now or
what and when we're waiting so it
doesn't mean we're just sitting on her
back sides not doing anything we're
actually thinking okay here's the most
dreamy talks there was one the other day
I didn't see about Solaris and dtrace
which I think was doing similar things
to what I was doing but using dtrace
under the covers to see how the
computation works is your code parallel
ready this is the next one after this
which we'll go into more detail and then
there's a buff tonight on you've got
streams in your collections where you
can ask more questions hang outs of 7:00
tonight in the same room myself Stewart
marks and like do we go so you can ask a
lot of questions ask more questions
about this the future
short term long term Cerf like that and
then there's a workshop by Stewart Marx
on first
they I think if you want to learn more
about streams and that's that again
properly done and that's it and thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>