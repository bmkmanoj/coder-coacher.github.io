<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Java Performance: Hardware, Structures, and Algorithms | Coder Coacher - Coaching Coders</title><meta content="Java Performance: Hardware, Structures, and Algorithms - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Java Performance: Hardware, Structures, and Algorithms</b></h2><h5 class="post__date">2015-06-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/HYJGGh0BjdM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Darrell Gove and I'm Co
presenting here with Charlie Hunt
most people know Charlie from the book
Java performance and his bikes halfway
across the country to be here today so
I'm hoping it will should be a good talk
so we're gonna be talking about
performance hardware structures and
algorithms so I hope you're interested
in those topics otherwise you're in the
wrong place this is one of those
interesting things that were encouraged
to put in our decks I'm in the Solaris
studio team so I spend a lot of time
looking at low-level disassembly stuff
and I also be doing some other talks
here on Wednesday on some of the stuff
we do for optimizing oracle codes and i
wanted to particularly point out a
couple of things here
one of the big hot topics of the minute
is security you know writing secure code
and this is and so we've got two things
going on for that one is a talk on
Thursday about the tools we've got for C
C++ but your developers you'll be
interested in this and we've also got a
hands-on lab if you do that kind of
stuff and want to see what can be done
and then moving on swiftly this is what
I want to talk about today so first bit
where to look for performance and then
we're going to talk a bit about hardware
and then we're going to talk about data
structures and algorithms and efficiency
and things like that and then nice
summary at the end so where to look for
performance so the tool I'm going to
have screenshots from that we're not
doing live demos today is these studio
performance analyzers one of the
products I work with or work on and so
this is something we've actually changed
in order to make the life of developers
easier hopefully anyway so if you have
comments on what done in the next
release which is 12.4 let me know I'm
very approachable
so one of the things that we found was
you have a profile of an application and
typically you're kind of thrown straight
into the profile and this is where you
spent your time and so the immediate
thought is okay I spent all my time
there I'm going to look at that hot
function and that isn't always the best
thing to do there's there's a little bit
of time when you need to be like
circumspect and look around what the
where the time is actually being spent
so what we ended up doing with the
product was to put in an overview screen
which says you know this is this is the
amount of time you spend on CPU which is
someone goes these three things here and
then this is the amount of time you
spend doing other stuff or waiting for
stuff to happen which is off CPU and
this is running on Solaris we get really
good detailed metrics of Solaris and so
you typically think of user and CPU and
system time there's also trap time which
is where the processor is doing
something to handle a trap and then
there's a whole bunch of stuff which is
okay I've got to pause this process
while I do something that while I do
something might be waiting on a mutex
lock it might be waiting on a condition
variable behind the scenes or it might
be waiting on i/o all these sorts of
things and those fall into this kind of
waiting category down here they're not
actually on the CPU that the processor
speed is entirely irrelevant at this
point it's how fast can you wait so and
different approaches are used depending
on whether you've got lots of time
waiting or whether you get lots of time
on the CPU and obviously you get
different things when you have user time
mode versus system time so diving in a
bit here's a really really dumb bit of
code to give an example you know I'm
writing the letter a out are writing out
was out a million times or something and
I spend a lot of time in the system
surprisingly enough so that's a really
silly thing to do and it's trivial
to fix instead of doing the right of one
item add a truck and write out ten items
add a trend you know in my pointless
birth code do a hundred thousand times
same amount of work and what you notice
is the amount of system time drops by a
factor of ten and why this is important
and interesting is that if you look at
profilers in general they tend to focus
on the use of time the use of time is so
much up in your face that you might miss
the fact that system the system time is
high and this has happened to me in the
past where I've been looking at a
profile and going okay this looks really
healthy this is great I've got nice
profile and then this little voice in my
head has gone what why does performance
suck and yeah okay there's something
else I'm missing and in some cases it's
system time and that's a nice one
because system time is time spent doing
stuff for your application and not time
spent in your code and you as a
developer want to spend time in your
code and not have the system do stuff on
your behalf so it's a nice the overview
screen I showed you two slides back is a
nice way of making sure that you're
spending time in userland doing stuff
that is useful in your code not in
somebody else's code now Java is
interesting in that when you start up a
single threaded process behind the
scenes the JVM starts up a whole bunch
of other threads doing stuff on your
behalf
these are garbage collection threads
these are threads to do all sorts of
interesting things
so here's my one thread that's doing my
bit of work and here's the stuff that
JVM is doing in the background on my
behalf and making that thread happy and
keeping it happy so the blue line here
the state there are sleeping so that
threads sleeping waiting for something
to happen it could be like wait peered
or whatever these ones down here use a
lock they're waiting on condition
variables some of those probably the
garbage collection threads and they're
in different states now if I flip back
to slide or three slides here you see
here's the sleep time here's the user
lock tie
that is kind of a natural consequence of
what the JVM does there's gonna be some
sleep there's gonna be some user lock
time whenever you profile a jar for
application and so some of the time
that's good and fine some of the time
that is where you need to look and go oh
why am i doing what am I waiting rather
than doing something useful the other
thing that we we did in this release is
I think fascinating but then I'm kind of
like in one of the the weirder sections
of the compiler team and hardware
performance counters is this is fun this
is yeah odd but fun so a chip a
processor has event counters so every
time you execute an instruction it
counts the number of instructions every
time it spends a cycle issuing its
instruction that's down here it's so it
increments a counter the counsel number
of cycles and that's really useful
information if you can read it correctly
you don't to get too much into reading
too much into it but it gives you a clue
of what's going on there are some couple
of other counters that are interesting
and data cache misses and I'll talk a
little bit more about that as we go
further on but this is saying I need
this bit of data and I had to fetch it
from a level of cache a level of memory
or whatever so this is actually this
counter here is a good indicator of an
issue and a good indicator of a problem
so that's something that's useful to
know and then the this came up at a
spark t4 processor and that has an
interesting counter called commit zero I
think which is a really kind of obscure
name and what it means is that on this
particular cycle I didn't do I didn't
have any instructions that finished so
it's actually the number of cycles where
it didn't do any work you know it's of
hand-waving type of way so we've got six
I'm gonna have to read off here 6,000
seconds of cycles where I didn't
actually do anything
and I only spent you know basically
8,000 seconds on the CPU which a lot of
time spent on the CPU but a lot of that
time there are no instructions that
completed and that indicates a problem
that's not good and you can see that
reflected in the CPI if we look up here
CPI about 4 so there's 4 cycles between
every instruction so the processor
itself is not actually doing a lot of
work here the converse of CPI is IPC
instructions per cycle and that's a
useful metric for another reason I'll
come back to that in a bit
and that's point 2 5 it's reciprocal so
what I like about performance counters
is that they give me an explanation of
what might be the problem it's not that
they go to them and say ah you know this
is definitely what the problem is or
whatever they don't necessarily give me
like the first order this is the problem
but they give me a nice warm
confirmation and I think are his where
I'm spending time I reckon it's this I
can look at the performance counters and
aha it is that or no it isn't that I
have to come up with some other
explanation which also happens and in
the latest version of our tools we
collect some by default
well not quite by default you have to
ask we have some default counts as we
can collect and they're very useful now
I talked about CPI now CPI is a number
that you can end up with this huge
result so 143 cycles per instruction
that is like you know if you if each
cycle was a week that'd be half a year
of waiting between instructions that's
really abysmal and it's really tempting
to say oh that's huge I can probably fix
that by doing something smart and you
probably can but the important thing to
the important context to have is that
the total run time here total user time
is 900 seconds and you've got 30 seconds
of this so even if I fix this as a
problem I'm only going to get 30 seconds
back so although it might be tempting to
go there's sort this by CPR and find out
where the
where the pain points are you've got to
also bear in mind what you expect to
gain our gainers and so if you look at
this particular thing here there's
nearly 900 seconds that's pretty much
know that's about 90% of the overall
time is spent in this other routine and
if I'm going to spend any time
optimizing anything I want to optimize
the bit where the time is spent not the
bit which is obviously really painfully
slow but even here a CPI of 30 for one
instruction is issued every 34 cycles
that's a lot of time between
instructions that's going to be that's
gonna be fun to optimize there should be
some low-hanging fruit there so that's
kind of the context of how do I decide
where to look but in some sense what are
my expectations coming out of this how
much time do I expect to get back if I
fix it let's talk a bit about hardware
in terms of how the JVM adapts to that
and what you can do to help with JVM so
I came out reasonably well perhaps I
should have used a darker font so
processor sits like the middle of in the
middle like a spider in a web and there
are certain things that kind of go with
the process which is memory caches TL
B's I'm gonna talk a bit more about that
in a bit that is stuff that you don't
really have a lot of control over and
the JVM kind of can't do too much about
on the other hand there are some
features in a processor that the JVM can
deal with and I put instructions and
quirks there the instructions is obvious
enough the different processors
implement different instruction sets in
the the various SPARC processors we've
typically added an instruction every
generation or a handful of instructions
every generation which is quite
interesting and what's great about the
JVM is that when you start your
application on the new processor within
the latest version of the JVM someone
has gone to the trouble of saying well
what are the new instructions and what
can we do with them so if your processor
has and the example I use in a bit as a
pop count instruction if it has a pop
count instruction I'll use it if I
don't have a pop count instruction then
I will use equivalent sort of vanilla
code that works around that so the JVM
can adapt to the hardware and as
hardware improves so the JVM can exploit
that which means your time to exploiting
hardware exploiting new features in
hardware is basically as quick as you
can upgrade the JVM which is fantastic
because I sit with hardware folks a lot
and you if you've turned up to any of
the talks about like the m7 processor or
the software in silicon efforts what
you'll discover is that it takes about
four or five years to go from basically
the design to an actual chip that's out
in the hands of the public and then if
you start looking at how that how long
it takes to then say we're going to
start using those instructions and that
for that instructions to be common out
in the marketplace in the in the market
then it's another four or five years
before you can say well that instruction
is definitely going to be on the
processor that I run on and so what
we've got in terms of innovation in the
instruction sets to actually being able
to use it in applications can be as long
as like ten years and with the JVM as
soon as that hardware is out there the
JVM couldn't take this existing app and
say this instruction is in the processor
and I'm gonna start using it so that's
suddenly once the process is out we can
start exploiting it you had a question
sir
I'm sorry
so um I think that in a couple of slides
I think that we'll have something that
answers that hopefully anyway if not
grab me afterwards um okay so there are
some things we can in the JVM you can
actually do and exploit immediately and
there's some stuff which is kind of like
a bit hard for the JVM to exploit on
this side here and so that's kind of
what we're going to focus on a bit in
this talk but I want to first of all
talk about this side and quirks quirks
is fascinating and probably stuff that I
shouldn't worry too much about but when
you're actually placed to the the
process of teams you go they say well
okay this is really good but we need you
to do this funky instruction sequence
and if you do it this way it's this
speed and if you do it backwards or
whatever or standing on one leg it's
this speed so can you do it standing on
one leg with your hands tied behind your
back and then run like a dream and so
there are some interesting quirks so you
can work around because you know that
process you're you're running on right
so hardware in forensics so intrinsic is
a bit of code that there's some hardware
underneath that can accelerate so
examples like you know if you can do mem
copy or whatever then there's often
hardware that you can say okay here's
there's a bit of memory you want to copy
go away and do it in this example I'm
you could looking at bit count which to
returns the number of bits that are set
in an integer or whatever on recent
processes at spark and x86 and pretty
much everywhere there's an instruction
that's called pop count that returns
that value and now so when you're
running on one of those processors the
JVM can say ah I know what you're trying
to do and here's an instruction that can
do it otherwise if you think of the
instruction sequence you've got to go
through to count the number of set bits
in an integer it's really lots of
instructions and it's a really a bit of
a pain to be quite honest there's a good
number of instructions anyway and this
is like three three cycles or something
so they're much more efficient of course
most people don't need to use bit counts
but yeah
the way it goes and then going on to how
does it look in the the OpenJDK stuff
well here's a nice URL I'd noticed
number of people taking pictures this is
a good one to take a picture of because
I think remembering it could be a pain
so in this particular bit of the the jvm
we've got some intrinsics here and we're
saying well if you see this particular
operation then actually we can do this
in hardware on the on this side here so
this is a snippet of the code that's
much more there where we're taking that
intrinsic and converting into something
that is particular for the instance of
the hardware you're running on so this
is an interesting slide I need to give
you a bit of context here so in the rest
of the talk going to be talking about
algorithms and data structures and
things in a moment but there's the
difference between what you as a coder
write what the Java compiler emits and
what the JVM produces for the hardware
and the difference here the minimal
level is inlining so you or I might
write some nice bit of code that makes a
lot of logical sense it has many layers
of call stuck in it
and the JVM says this is fantastic this
is really interesting code I like it a
lot I'm very happy with it and I notice
you're using it a lot so what we're
going to do is flatten it so instead of
having three functions you have one
function which contains the code for all
three functions sort of flattened out
like I know a squirrel
after lorries gone past so what we have
here is the the user view of a curve of
the code and the Machine view of the
code which is different it's sometimes
quite hard to map between them and it's
difficult actually for the tools to
present that in a way that is really
approachable so we're going to have some
some pictures from the tools which have
the user mode stack which is what the
program over
expects to be happening and the machine
mode stack which is what happens after
the to the code after the JVM has got to
it and made it optimal and so you need
to be aware that will swap between these
two and hopefully we'll identify them
otherwise you're kind of on your own and
may you know make the best of it but
there is this difference now I'm going
to quickly dive through CPI and
performance CPI is one of these metrics
I've laughed at for years because it's
almost meaningless because if you're a
developer or in the compiler team or
ever you know that you have a lot of
control over the number of instructions
you use to do something so if it's going
to take a lot of cycles and I'm worried
about CPI I can use more instructions
and that lowers the CPI and it looks
like everything's happier of course you
know that you've used more instructions
than it could have could have done so
there's a lot of control over the I bit
there's not so much control over the C
bit so actually it's kind of not
necessarily the most meaningful metric
to look at in isolation you can't look
at a book code so our CPI there that's
that's no good at all
but it gives you an indication of what
sort of optimizations you can do and
that's how I think you need to use it is
if you have a high CPI you have rare
instructions being issued and most of
the time you spent stalled and
unfortunately the the most common calls
are stall is memory the memory subsystem
so that's where we'll focus a bit and
the most common reason for problems in
the memory subsystem is data structure
efficiency and usage so if you see a
high CPI lots of cycles and few
instructions then it's probably data
structures now that doesn't mean that a
low CPI is good bizarrely it just means
that if you've got a low CPI you're
asking a lot of instructions and that's
you're keeping the processor busy but
are you actually doing it efficiently
our Summers instructions
ones that are doing useful work or have
you got inefficiency in the way the code
is written and so either one of these is
you've got work to do but the type of
work is different now this is a the two
minute spiel about how processes deal
with memory and I love this sort of
stuff and it's kind of like peculiar so
memory is sort of organized into pages
sort of it's not actually organized into
pages but that it's allocated on a page
basis and so what you have on the
processor is a structure called a TLB so
when you have a virtual address in your
application the TLB translates that
virtual address into a physical address
into a particular chip DRAM chip which
has the data in it and it's the job of
the TLB to do that translation so you
put a virtual address in it says ah okay
virtual address I know where that pages
and then the remainder of the the
virtual address represents an index into
that physical page of memory so you look
up memory on the TLB and then you use
that to find the physical bit of memory
and then when you fetch that physical
bit of memory you don't just get if
you're loading an integer you don't just
get an integer back
you get a chunk of memory called a cache
line which is typically 64 bytes across
pretty much all processors in some
instances there's you get 128 bytes
sometimes you get 32 bytes depending on
the the cache that you stick it in so
there's some weirdness around it but
basically think of as a 64 byte chunk so
that's the granularity of memory and
then cache is that bit of memory of RAM
that is on the processor that holds that
memory temporarily whilst you're using
it and eventually it will get kicked out
of there and it will sorry
it will get kids out of there and next
time you use it you have to fetch it
from memory again and the idea of the
cache is that if you're using something
a lot it will be in the cache and it
won't cost you much because and this is
the critical point here if you're
accessing stuff in memory it's hundreds
of cycles it's it's you know like
sending a parcel hot around halfway
around the world it takes forever to do
that and the cache is much much closer
so this memory is can be like six
hundred cycles and cache can be like
three so if your data's in cache
that's brilliant if it's in memory
that's painful and that's the cause of
the low CPI so I'm getting data from
memory it's gonna be painful so how do
you what strategies do you use to deal
with that there are two basically common
strategies one is to use memory
efficiently so if you pull in a cache
lines worth of data make sure that the
date that all that cache line is being
used don't just pull it in a byte from
it because you've got 64 bytes use those
other 63 bytes or something useful the
other strategy is data memory level
parallelism where you say well okay I
want to use this cache line but actually
I'm going to use this bit of data as
well and this bit of data you know
pretty soon so I'm going to fetch these
ones in parallel so those the two
strategies we're going to employ and at
that point
oh no I've got one more slide to go here
and and this is another I think
fundamental thing we're going to have to
talk about you know you know the
complexity stuff the order n N squared n
log n depending on the algorithm so if
you're going through a linear array
you've got n elements in it and you're
searching for a particular element it's
gonna take your order n sort of
iterations through the search to reach
element if you've done something of a
binary search on it it's n log n or what
know it's log n for the number of
iterations you've got to do before you
find the element but what this doesn't
include and from a hardware perspective
this is probably the critical bit is the
so the cache misses now the cache miss
as I said is like hundreds of cycles so
if you have one cache missed you could
have done a hundred instructions so if
there's a few instructions you can throw
at an application that reduce the number
of cache misses then it's actually worth
doing that and that means from a
practical point of view if you've got a
slightly more complicated algorithm that
you want to use it takes a few more
instructions to do it then there's a
good chance you'll get a payback if it
reduces the number of cache misses
you've got so it's worth adding in
operations to reduce reduce cache misses
right with that I'm going to hand over
to Charlie to continue thank you thanks
Darrell
while Darrell was talking I made a
couple of notes and I was doing this the
other day in front of my son who's a
nine year old by the way and he asked me
what are you doing and I said well I'm
getting old am I gonna make a few notes
and he kind of looked over his eyebrows
he says dad you're already gold anyway
Darrell mentioned the product here are
the screenshots this was a product that
was originally developed at Sun called
Sun Studio performance analyzer through
the acquisition Oracle branding changed
the name of it to Oracle Solaris studio
performance analyzer now there's a few
of you out there probably saying oh
shoot we run on Linux
we can't use this here's the nice thing
this also works on Linux wonderful job
at Brandi there's a little got you with
running on Linux and if you do this on
the 64-bit version of Linux you'll also
need to install the three are the i386
libraries the C libraries so if you want
to try this tool out I'll also say about
the tool anybody that I've ever showed
this tool to it through them they still
use it today so the reason why we're
showing these in the examples here is
it's because it's it's the tools that we
use
and we use them not because their Oracle
tools or they were Sund tools were the
most productive using them and we enjoy
showing up to other people to make them
more productive another note that I made
we have a question do we have do we have
a well it is a free one to use so it's a
free download so if you do a search for
Oracle Solaris studio down our Oracle
Solaris Allaire studio you'll find the
URL for it it's free to use you can buy
support for it so if it's something that
you become very reliable on you know you
want to have support for it you can buy
a license for it I think I said thousand
dollars per user I think I don't quote
me on that but I think that's what the
cost of it is another topic on the on
the number of intrinsics in the JVM so
Darryl showed the URL that you'll never
remember there's hundreds and hundreds
of intrinsics in that light in that
library in that file there's also some
specific ones if you look if you're
familiar with the open JDK a structured
there's structures that will say
directories that Wilson to be named OS
or they'll say OS underscore CPU there's
some additional intrinsic sin those
directories also so there's literally
hundreds and hundreds of of intrinsics
the thing that I would think that I
would want to know as a Java developer
is the intrinsics that are there
oftentimes are for library calls that
are part of the standard JDK libraries
so something to keep in mind is
oftentimes you look at maybe a data
structure that's in the JDK and you say
you know what I think I can write a
better algorithm than what's in here the
risk that you run is there might be an
intrinsic in there in that method that
you're going to rewrite as your own so
as you start to get this temptation to
want to fly your own source code or
write your own algorithm you probably
want to do some profiling of it
especially if it's in a critical area
because there may be an intrinsic for it
in that standard
JDK library okay enough of the notes so
what I'm going to do here is I'm going
to take a look at an application an
application that I'm looking at here is
actually an old spec gbb benchmark and
the reason that I'm showing this is not
because I'm trying to make fun aspect or
the spec organization or the benchmark
that they that they wrote the intent
here is if I was able to modify the
implementation of what spec implemented
here what would be the benefit and it
turns out this is a really good example
of if I could modify the source code of
this spec benchmark I could realize a
really nice improvement the other thing
that I'll say here is for you guys that
are really good performance engineers
and this would probably make a great
interview question for a performance
engineer if I could give you a choice of
of what you need to solve a performance
issue in an application what resources
you need and the answer that I would
give to that is give me your most
knowledgeable application developer I
want that person who knows the code and
what it does the reason for that is I
can have him tell me what this code does
and if I can make changes at the
application level I get a bigger bang
for the buck for my investment at the
higher levels of the software stack then
I will at the lower levels whether it's
at the JVM or the Java class libraries
so give me that combination of that
performance engineer and also that
application developer put those two
together and you'll get a really good
return on your investment in performance
okay so we ran this this application and
what we found here in Machine View so
this is the view where you get the
benefit from inlining and what we're
seeing here is we're seeing about 40% of
the entire time spent in this
application is in this one method called
delivery transaction pre process that
gets our attention right away okay so
what is it that this thing is do
well if we look at the source view it
tells us that this hot line of code is
doing this thing that's called a
warehouse pointer retrieve stock and
it's passing in this thing called an
item ID and that's take that one
statement is taking about 20 percent of
the application execution time so okay
so what is this retrieve stock thing
well it's retrieving this thing called a
map storage data and this calls into a
thing called a it's basically a hash
table so if you're familiar with the JDK
class libraries you know that there's
this hash table that's the thread-safe
hash table and then there's this hash
map that's the non thread-safe so this
would be the hash map version of it so
here we see the where it's doing the gap
on this on this hash table on this hash
map and this is taking about 20 percent
of our time say okay so we're doing a
hash table lookup what can we do here to
improve its performance so we start to
talk with our application developer well
before we get there let's kind of visit
it what does a hash table do so
basically you get this index or a key
that comes in you do some hashing on it
and that hashes to some element in a an
array and you find its value and there
may be some collisions which maybe go
off in some chain of buckets I think
most of us are familiar with in general
what's involved with a hash table and
our overhead here is you know doing a
lookup we've got an overhead of an order
of 1 and our cache misses for doing that
order of 1 is also 1 so ok so we talked
to our application engineer and we see
that oh this item ID that we're passing
in here happens to be an int so the
first thing you might be sitting out
there and saying is AHA we need
something that will take primitive types
and a hash map but we don't have one of
those in the JDK so what do we do well
let's talk with the application
developer a little bit further we find
out that the item ID happens to be an
integer from 1
and where n is a configurable number
that's passed to the application when
it's initialized it turns out in this
particular benchmark that number is
20,000 so our item ID is limited between
one and twenty thousand so okay we're
using a hash table to hash an integer
between 1 and 20,000 to find a value so
what could we do here ah aha we could
use an array why do we need to hash an
integer when we could use the actual
item ID as the direct index to the value
so that's what we did we said okay let's
pull out this hash map and plug in an
array instead and we look at our
overhead here our to do our lookup it's
an order of of 1 and also our cache miss
is an order of 1 but we also realize
that we've got a bunch of instructions
that we're no longer doing and we're
also not doing some pointer chasing so
what's the what's the impact here so
we've got on the left hand side here is
the amount of time that we spent with
the hash map implementation and then on
the right is when we replaced it with
the array so this was done for about
five thousand seconds and we dropped the
amount of time that we spent in this
method by about 300 seconds from a
throughput side a number of transactions
per second that we're executing it
increased by about nine percent so this
gives you a good example of the choice
and the algorithm that you're using also
the benefit of having that application
developer there who understands what
this code is doing and what values and
particularly used by the application ok
so shifting to efficient implementations
so if we go back to our profile again
our second highest contributor here is
in this method called customer report
transaction process it spends about
twelve percent of its time there
so what is it doing so if we look at the
source code level we see it's doing this
thing creating an iterator a history
iterator and then it's iterating over it
so this is time that it's spending
iterating over a tree map so our tree
map from the JDK we all know is at red
black tree so we've got a an ordered map
that we can do quick searches on and we
can do iteration on and each time that
we insert something we have this
balancing effect of the red black tree
so we look at who are the callers of
this process and we can see that here's
this iterator they're on a tree map a
value iterator so if we look at what's
going on to the inside of this we talked
to our application developer and this
history says well basically what we're
doing is we're adding some new elements
to the history and we're removing the
oldest ones we might look at this
initially and say Jesus kind of sounds
like a queue more than it does a a red
black tree and I would agree with that
because that it turns out in this
benchmark as far as I remember I don't
think that it does any searches of this
tree so you could potentially replace
this with a queue because of what you're
doing is you're adding or appending new
elements and you're removing the oldest
ones that sounds like a queue and then
you'd want the ability to iterate over
the queue what if you were also doing
searches searching is you know having a
rapid search on a queue probably
wouldn't be very good unless it was
always a relatively small queue anyway
we decided to stay with a tree map a
red-black implementation and if we look
at the initial implementation of a tree
map of what is in the standard JDK you
basically have a node which has a key
and a value and then it's left and right
child so when you iterate over a tree
map you basically are going all the way
to the left of the tree to find this
smallest node and other words the
smallest value and then you start
iterating over the tree so the lookup
cost here as I mentioned earlier is a
log M likewise your cache miss over
ahead here is login but when you tart
talking about iteration you've got n
elements here so your iteration is an
order of n and then your cache misses
are also an order of n so what we said
was because we're iterating with nodes
and these nodes could be scattered
anywhere in memory because we're
iterating we'd like to get that stuff
lined up and arrays because when a laser
arrays are laid out of memory they're
going to be laid out sequentially so if
you've got a set of object references
those object references are laid out in
that array so what we said is let's do
something different with the know to
make them wide nodes maybe some of you
were thinking well this looks kind of
like a bee tree and it is it's kind of
like a bee tree what's a little bit
different than a bee tree at least in my
interpretation of a V tree is each one
of these data and values would also have
a child off of it in this case we're
keeping it an array of values and then
we're got a left and a right child so in
this case our lookup still ends up being
a login and our cache miss being a login
and our iteration being an order of n
but our cache misses are reduced by the
width of the node that we have and with
a way that we ran this after we modified
this and did an implementation of this
wide node tree map as we made it a a
wide node of 64 elements so there were
64 values in each one of these nodes so
here is an illustration where I was
trying to capture the generalization of
what it is that we're really
accomplishing at the hardware so in the
upper part of this if you look at the
standard JDK stream app with a a single
value in a given node and on the right
hand side think of this as a cache line
we're only using a portion of that cache
line to capture that
node or that value within that node in
the bottom we've got this wide no that's
got an array of values in it and we're
able to have more of that cash utilized
with more of those values in that node
so that's the generalization that I'm
trying to make here is to have you
understand why this is the case so
here's the profile of the wide node tree
map and we're comparing that to the
standard JD case and you can see here
that we've reduced this quite a lot by
about seven percent from a throughput
standpoint we spent about 300 seconds
less so a rather substantial improvement
yes so what we did is if you are
familiar with the tree map
implementation there's a key and a value
so we did an array of keys which were an
object type and then also the values
which were an object type so we were
actually it would be an array of object
references there is still some pointer
chasing that's happening so if it was a
history element in each one of those
values as each one of those references
you would then say okay here is the
reference to the history object which
would then go off to some you know point
off to some history object and then
there may be some piece of data within
the history object that you're looking
at that's true yeah that's true that the
kit we don't have a guarantee on the
cache alignment yep okay since Darryl
did a nice job at kicking things off I'm
gonna turn it back over to Darryl live
for the concluding remarks so suddenly
one concluding slide and I'm have
questions so the points were trying to
make here is that there's some stuff
that JVM can do there's a JVM can say
AHA we're running on this platform
there's a really nice instruction that
does this so I'm going to use that
instruction or it might say well I can
do prefetch or whatever so there's some
things that JVM can automatically do
where
you have an advantage over the JVM as
you can understand the hardware and you
can understand how your algorithms
interact with caches and cache lines and
TLB s and say ok I can change this to
make it more efficient so that's where
your skills come in as a being able to
say I understand the hardware and how
the JVM is gonna use the hardware I can
use that to make my application better
and similarly working with someone from
the applications team is a is a really
good advantage to doing this because you
get the inside skill the stuff that
isn't in the source code that says IO
this this number has a limited range so
you can exploit that to improve the
efficiency of the code still further so
as I said cache misses are like the
number one problem we see in
applications when you look at
performance of an application you say
well this sucks
it's normally cache misses it's kind of
not very exciting as far as the you know
the the big reveal at the end of the
film goes and the villain today is cache
misses but there are some things you can
do efficient use of the data that you do
fetch from memory and also minimizing
the number of hops to get that useful
data room from memory so with that what
questions do you have
and then difficulty hearing so sorry
yeah
right yes yeah
ah if only that were true so the
question was in see your you can reorder
your data structures so you get the hot
data structures together and the cold
rarely executed will really use ones
sort of further away and sometimes the
compiler can do that on your behalf
and sometimes you can't so it's still a
very valid optimization and one of the
things I do when I do performance
profiling is say oh yeah you know you're
using these structures and you're not
using these ones why have you got them
both in the same object but because
maybe you can move them into a separate
object you get better cache efficiency
okay that's that's a good a good comment
because yeah we we don't it's a luxury
when you go oh this is a single process
I really want you to optimize the
performance of this one process here and
that's it we're often dealing with like
middleware a database and some other
nastiness that's off in the weeds
somewhere and yeah in studio we have
that problem a bit and we have a tool
which profiles the entire system and
then we can look at where the time is
being spent across processors so not
just the one that you're currently
dealing with the other processes going
on in the background that they're
causing your process grief so you've got
to start looking at the whole system on
Linux and you've got a profile that does
a similar sort of job further for the
entire system so you just have to look
across many things
the certain periods well life
what's the rationale why did you choose
the number of went to the arena
so that we did some testing of various
de so we basically made the tree map
implementation configurable so that you
could say whatever size of number of
elements in that in that node we found
in general the the trade-off between the
space that was quote wasted that you
know if you started to remove elements
versus the performance you know so we
did some experiments where we said well
if let's compare things like 16 and 32
and 64 and say 128 I think we even did
some experiments at like 192 there
wasn't a lot of performance throughput
difference at 128 or 196 we started to
see more of that sort of curve going
upwards or I should say downwards at
once you went from 64 to 32 to 16 so it
seemed to be a good trade-off between
the memory versus throughput performance
right
we did look at that one of the
challenges that we found is and if
you're familiar with prefetches there is
this latency battalion between the time
when you issue the prefetch and when
that comes into memory and one of the
challenges with it is how do I know when
to issue the prefetch and have I caught
the data too soon or has it been here
and then it be fictive because somebody
else wanted that cache line so we did
some things with experimenting with
prefetching but we couldn't find the
sweet spot to be able to say here's the
place where we know that the next node
we're going to be getting is you know
you know either the right or the left or
you know or the parent whatever it might
be yeah right yeah yeah yeah we did
explore it and we just found that it was
a really hard problem that predicts far
enough in the future when we needed to
issue the prefetch for the next node
yeah
right so typically the hardware does a
really good job of caching instructions
where you can sometimes get performance
gains is by laying out the code
differently but I don't believe the JVM
does that kind of layout because it's a
bit hard to do well do you do stuff
laying up reordering the blocks of code
to fit better into the cache as far as I
know I don't think we do thanks
right right I don't think it does for
the generated code that yeah I don't
think that we do okay
oh sorry oh does the different
algorithms the garbage collector use
make a difference I think that's right
up your street challenge on this
particular workload I guess there's a
couple of ways to answer this from a
throughput standpoint and then the
number of transactions executed per
second so just in general when you have
a a workload that your primary focus is
on throughput you'll generally always
get the best performance using a
parallel collector in other words it
does parallel collection of young
generation and parallel multi-threaded
collection of old generation and they're
stopped the world if you start to move
you and shift your focus where latency
of you know of a transaction so in other
the time you issue a request so you get
a response and you're looking for things
like 95th or 99th percentile or
worst-case pause time sort of
constraints then you may start to shift
from a parallel collector to something
like a concurrent collector of some kind
so it kind of depends on the application
and this in and what we were looking at
here and in the example that we worked
with the percentage improvement that
you'd see in this application it
wouldn't matter which collector that you
were using from a throughput standpoint
the benefit would be essentially the
percentages that we showed
do you know anything really lightweight
that was just a metro system shut down
give me details about the cardinality of
that's a really good question there's
been some conversations that we've had
along this line of it kind of ties into
a couple of questions that we had
earlier along the lines of prefetching
or using sort of hardware counter
information to guide profile
optimizations if you will the challenge
with it is even though in general
hardware counters information is
something that is isn't very doesn't
have a lot of overhead with it from a
virtual machines executions perspective
the overhead is too large too large and
too intrusive to be able to make a very
quick sort of optimization to answer
your question specifically I don't know
of anything that will do that I don't
know if something like CPU track or CPUs
does that would do something like that
at a JVM level but I don't think you'd
get the correlation of because I think
ideally what you'd like is you don't
have a given data structure and it's
methods and its accesses to those fields
of having the hardware counters at that
level you know as CPU track or a CPU
stat level on Solaris you just get you
know here's how many misses you know
from a process standpoint it's it's your
zero overhead is really really tricky to
achieve yeah on this sort of thing so
yeah there might be ways of getting
approximate numbers but I don't know if
anything will get it would zero right
you know a production environment that's
a really hard one yeah that reminds me
you know you know the tool that we that
we have the screenshots from often a
question that we get is you know how
much overhead is there with this and
typically I would say characterize it
that the overhead is somewhere between
five and ten percent so your application
might run maybe five to ten percent
slower than it would without a profiler
so it generally is is noticeably better
from the standpoint of the induced
overhead than traditional Java profilers
you know I think that's one of the
things that I really like about it is
its overhead is is not as noticeable
yes it does do sampling you probably
know more about the specifics of the
tool so does sampling a hundreds samples
per second and you can do a lower rate
and potentially a higher rate but
obviously the higher the rate the more
probe effects you get and so 100 is a
good starting point and some instances
if you've got an application which has
got lots and lots of busy threads even a
hundred is it starts to introduce two
larger Pro perfects and if you're
profiling for like an hour's worth of
material you know one of the more
awkward situations I had was I was
working with somebody else in Oracle and
they said oh yeah here's some profiling
data and they're collected I think it
was like two hours worth of profiling
data and we couldn't find a machine with
enough memory to load the resulting
experiment into it and if we had it
would have taken quite a considerable
amount of time to load yeah you it does
that and you might need to tune the data
rate but in most instances the defaults
gonna work it does usage it does use a
JVM TI interface
yeah how recent was AVM TI how recent
with working with gbmt i there was some
tremendous improvements in an a
concerted effort in improving the
reliability of jb MTI in the java 7 time
frame
the recession on profile of the speaker
that Jim
I don't maybe that's a limitation on the
sampler they were using profile they
were using I don't know yeah yes you
might have yeah I don't want one of the
big improvements in JB MT I was the
async call trace yeah
the profile this profiler does use the
async call trace yeah but for the
disassembly it's you know that's the
instruction you are executing so you
might find there's a disconnect between
the two stacks I guess so one thing that
you can see in this tool that I know
Darryl really likes this is and I like
it too is there there's a view that you
can say they show me the disassembly so
basically in machine mode you can say
show me the disassembly and it'll show
you the actual assembly instructions and
the other cool thing with it is I'll
actually show you so you saw in the
slides where there's a yellow
highlighted area that shows you the hot
line of code it'll also show you the hot
assembly instruction so there's a way
that you can set it up also this so if
you compile both the native code and the
doer java code with symbol information
you can actually see the embedded
generated code within your java code and
i think that's really cool okay
you know I've heard a lot about the JIT
watch or two but I haven't looked at it
yet and it's been something I've been
wanting to do I think that's a really
neat tool from what I hear and something
that I think opens a you know kind of
peels back the layers that allows you to
take a look at you know what the
compiler is doing because it has set
been such a black box as far as how it's
taking and optimizing your java code so
yeah I think it's a it's a very good
contribution okay well thank you very
much for turning up</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>