<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Using the Same Docker Container for Development and in the Cloud | Coder Coacher - Coaching Coders</title><meta content="Using the Same Docker Container for Development and in the Cloud - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Using the Same Docker Container for Development and in the Cloud</b></h2><h5 class="post__date">2015-06-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Ae2oZmBMnl8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so what I'm going to tell you about
today is darker with continuous delivery
I mean dr. is really popular I just gave
session before this one so it run across
the street to be here on time and it's
not only popular in Java one but almost
everywhere as I will also show you in
the comic sheets and it's really working
well together with Jenkins to create a
continuous delivery pipeline if you have
any questions feel free to just ask them
in between no need to wait until the end
of the session so bad about what's
continuous delivery I mean probably much
you have already heard about it but it's
all about automating your delivery
process and making sure that the quality
of your software is good enough to be
able to release on a regular base I need
you have to continuously improve on that
to even automate more and do even more
regular deployments and so it's more or
less about the deployment pipeline where
you do all kinds of stuff like compiling
quality checks testing and at the end
you do the deployment to a production
environment to bring old software you
built as a dev ops team or whatever team
you are in and make sure that the end
users will receive the functionality you
have built as quickly as possible so
that's not laying on a shelf somewhere
waiting to be released in the next year
yeah what's a typical deployment
pipeline for java applications i think
there are some common stuff that's
coming back almost always like Jenkins
for instance and the tools you're seeing
here most of these tools we've also used
for our set up the set up i'm going to
show you is from a project we did that's
running live in production on docker and
an azure cloud sorry to say it's an
azure cloud but it's working so folks of
this presentation is darker and how can
we use Jenkins to make that build
pipeline and make it as flexible as
possible so fr instance Microsoft is
going to raise their prices of the a
circle
out or they're going to mess up
something and we want to move to friends
in San amazon cloud we can do that with
minimal configuration changes and then
automatically deploy our complete
environment again on another place or
maybe even deploy it on your laptop I
did the same stuff that works running on
the azure cloud I'm running that on my
laptop as well like you should I show
that later when contains delivery
started a bit and even now you see lots
of companies they say I will do contains
delivery we build an application like a
war file and then that's the qualities
Jack then we deploy it through the dtap
environment I mean that's not really
continuous delivery I mean what do you
do with the stuff that's underneath the
application the application server and
all other requirements that you have on
your machines if you get a new hardware
or you want to move to another cloud you
have to install that stuff over and over
again and I mean applications wii
version them we know what is deployed
one but for infrastructure often it
isn't versioned you don't know at what
point of time what was deployed where
which version of the application server
which version of java so for me contains
delivery is about deploying the complete
environment not just the application but
also application server and as much
around it as possible the other way you
have infrastructure as code and a lot of
other benefits and doctor is more or
less the solution to most of those
problems we show you in this
presentation why that's the case these
are some sheets I borrowed from the
doctor people it's a common way to show
why doctor is a good solution and they
compared it to a problem we had a couple
of years ago a transportation problem we
have oil barrels pianos stuff like that
and if we just throw them all in one
shape and probably the piano would break
and maybe you're upset or glad that you
don't have to listen to it anymore and
you want to transport that stuff not
only in a ship which also want to
transport
in a van or whatever so if you put it
all in one big oh you have to get it out
get it in the car see that it all fits
again so the solution was containers you
already guess it probably by putting all
the stuff in separate containers we
could easily move it move it around
different transportation vehicles and
get it to the place that we wanted to
bring our stuff too same thing is for
our software nowadays you have lots of
types of software with databases
services front end applications back end
applications and so on and maybe now you
have deployed it locally on your
development machine because you're
developing and later your company
decides are I want to deploy it on our
company servers but then after a while
they hear out loud that's really cool so
now do it in the clouds and then you are
okay it will cost me about a month and
then it's running in the cloud but what
if you are ready I put everything in
containers that you could use not only
on your laptop but also in a server in a
cloud you can just deploy those
containers directly with minimal effort
and it's easy to move it around the
places where you want it to be friends
were cloud vendors change prices you're
not stuck to the cloud provider anymore
with a press of the button you can
change your cloud provider and
immediately benefit from price changes
of that cloud provider compatibility
doctor is really compatible with lots of
stuff as long as it's running on Linux
because it uses linux kernel features
you can also run it on Windows or Mac
but then you have to use a little layer
on top of it they call it booted docker
or you run just a virtual with Linux
version on your Windows machine and then
you can also use it then you don't get
all the benefits and as you can see here
I've got a raspberry probably you all
seen it before but it even runs on a
raspberry keep in mind the raspberry
architecture is different from the
architecture of your laptop in your
server so if you create a container for
your raspberry you cannot die
we move it to your laptop because the
architecture is different moving from
your laptop to cloud it's the same
architecture so you can just use the
same containers it's something to keep
in mind so almost running anywhere I can
show you a little demo of course I have
a raspberry and it's treatable so now I
can I have a set up on the raspberry
where I can deploy my application in a
tomcat server net tomcat server that's
running in dr. the details i will show
you later but i will just show you how
easy it is to start a container and get
your tomcat application running
so you can see here already it's using a
lot of caching features if nothing has
changed it's just using cash which is
really beneficial because then it can
build it Foster it don't need to
download everything from the internet
again it's just there already and we can
now have a look at which docker
containers are running now we can see
there's one docker container that's
running or starting because starting tom
cat on a Raspberry Pi takes about two
minutes laptop it's a lot quicker the
limited resources take some extra time
you can see the port mapping which ports
on map which I will discuss later
and one feature that's really cool as
you that container that's running as
some isolated stuff on your host you can
access it from darker so you don't need
to go to that container with ssh which
is something a lot of people that start
with docker they immediately put an ssh
server inside a container so they can
ssh to the container and look what's
going well or wrong and see what's what
they need to fix but that's not
necessary with docker there are lots of
commands to access the container you're
running for instance this command it
just follows the system out of the
system error of the Tomcat container so
everything tom cat is now sending to the
console I can see it directly here
without having to log into my container
so that's really nice feature I think
I'll come back to this one a little bit
later because it takes about two minutes
to start
so why should we use docker I mean lots
of other features are earning their you
could use virtual machines and other
stuff we'll explain in a little bit
white virtual machines are a lot worse
than container features but it really
enables continuous delivery you can
provision your own complete environment
and deploy everything at once instead of
just your application and it's really
easy to use because docker it just uses
to stand up linux commands so if you
want to set up a container which I'll
show you in a minute you just use linux
operating system commands I've also
tried chef and puppet and tools like
that to provision environments but
therefore you need to learn a complete
DS l only to make one file that you
probably never change or only change bit
you have to invest a lot of time to get
it up and running that's not necessary
with docker you can just use the
standard commands that you're used to if
you're using Linux it's easy to move
cheap low resources I just gave a
presentation about using docker on
raspberry for Internet of Things it's
really helpful there as well because you
cannot really use virtual machines at
all on a raspberry because it's too slow
for that
now see if it is ah we can see now that
it's started so let's have a look if it
is working it's a stand Oh the question
is how much memory my raspberry has it's
a standard model be so I think it's 512
megabyte so nothing special there and it
runs ok takes a while but i never tested
it under high low so we wouldn't run
production environments on it but yeah
yeah dr. runs fine and it's just don't
get that slow talker starts in
milliseconds that's that's not an issue
I can show you we already get the result
so the docker we see in the system
logging that the I locked the request so
Java one and it's also displayed and
browse also it's working just a simple
application sorry hurt maybe the
resources for raspberry are a bit
limited let me show you how fast a
container without Tom kaat starts to
show you how quickly it really is it's
just boom cat that slow it's not docker
that is taking two minutes
so what I did here I don't know if you
notice it at all but I just start a
container a really simple one and go to
the best shell of that container and it
starts instantly it's with the blink of
an eye it's working I can can do it
again if you haven't seen it so now I'm
again on my raspberry pi and I'm in my
container it's that quick
so this is the command I used to just
start a simple container it just uses a
raspbian base and it's just starting the
best show its interactive with the
terminal just some default commands to
start it so this is a way to completely
set up a container without configuration
I will show you in a minute that you
will probably want to use docker files
to configure your container because
that's a lot more manageable than just
starting a container and configure it
all manually that's not something what
you want so what are the advantages
compared to virtual machines I think
better than everything with a virtual
machine and mean it's more disk space
efficient it's faster startup Quaker
it's compatible mean with a virtual
machine then you have a colleague that
is using this virtual machine and then
you have to install software for that
then you get from another colleague
virtual machine if to install so for for
that it's not really comfortable you
still get the isolation you're having
from virtual machines so stuff you're
running a lot of docker containers on
one machine is still isolated from each
other and it's really helpful for
versioning the docker images you can
just put them in a repository it's sort
of like a git repository and that way
you version your complete environment
which is really helpful and follow
resources such as the raspberry it and
it's not even an option to run virtual
machines the doctor is more or less the
only choice there I already talked about
this one I don't really like the other
provisioning tools that I've used most
of them seem okay until you want
something that's not supported out of
the box and you have to figure out how
it's working and chef and puppet and
order tools quickly take up a lot of
time time I don't like to spend on
getting just an environment up and
running but what what's dr. I mean there
are lots of cool tools out there and I
can tell you a lot of cool things about
all kinds cool tools but is it really
that cool and does it have future I mean
it's relatively new
started just lost year but they're
really growing big the numbers I showed
their the old numbers in the new ones I
think there were two months in between
it so the growth is enormous Microsoft
is even supporting it they're bringing
out tools for it all big vendors are now
supporting it they even got a new
investment round where he collected 40
million dollars and there the next thing
they said was oh we don't really need it
we have enough money for this year
already so it's really going well for
them at the moment and the big moment
was lost year in June when the first
official stable version was released
before that moment they advised not to
use doctrine production there were some
people that couldn't resist it and we're
still running it on production but it
wasn't advised by doctor themselves
after this moment there was a stable
version and you could use it on
production and that's something we are
doing it sends a couple of months and
it's working really well before that
time backwards compatibility and
stability were not that good sometimes
when i upgraded docker before a
conference and suddenly all my
containers stopped working so that was
not so nice but now since the 10 release
it's all stable and it's really easy to
upgrade it and there is a there's a big
ecosystem around it i mean if you have a
central dr registry where lots of
containers are already there so
containers that include tomcat of jboss
or a glassfish they're already there you
don't have to create them yourselves you
can just use them and use the
configuration that those people already
made for you and we're even some
official images that are supported by
the doctor people lots of businesses are
already using it I mean even Spotify is
looking at it to move this docker to get
everything in containers Google is not
using docker directly but they are also
using container features and they are
using the same container features the
doc
using and does container faces on our in
a linux kernel for a couple of years
already so the technology below
doctorates it's quite a proven
technology existing for a couple of
years only doctor was as what they did
good was they made it easy to use it and
it did a marketing very well I think if
you look at the the increasing numbers
and the popularity of it but that's I
think a reason why companies are
believing that it's stable enough to
move to doctor already while it's not
really on the market for a long time and
but it's really using proven technology
and now he is it to start it i mean if
you have wintered this is the only
commands you have to execute at standard
and you've been to repository i can
download it it's a bit of a petty that
are there was a lot already a darker
application within a boon to so we're
going to rename it to dr dot io which is
sometimes a little less convenient if
you have to use two different kinds of
commands or you make an alias or
something like that sorry
yeah sword or fish you have a boot to
dokur project that's more men to run on
OSX and on windows so you can use the
features it sort of is a minimal linux
distribution with dr. in it but you
really you can just run it on every
Linux system it's it's supported by all
of them and also the cloud vendors
they're all supporting it at the moment
so it's not really a preferred choice it
at least not that I write about yeah
it's one of them with a lot of people
use other stuff so it depends a bit I
think you can just use the linux
distribution that you're using in your
company already that's the easiest thing
most of the time sorry I so the question
is can you install it on your Windows
laptop not directly you either have to
use a Linux virtual machine in it or you
can use boo to dokur which essentially
is a minimum virtual machine with linux
in it so i would suggest install it
directly on linux because then you have
the most benefits
so the question is would it make sense
to use the boot the docker MH on bare
metal I don't know if it's really
possible because it's a something of a
virtual machine which is running on top
of something so i don't know if you can
run it on bare metal but i wouldn't
suggest it then it's better to just run
core OS or some other linux distribution
it's only for people that are running
windows are always X mainly ok so the
question is how does this relate the
development and then do you mean running
your IDE within a container or ok
nowadays doctors mainly used to run
applications like services databases or
websites stuff like that it is possible
to run GUI applications I've seen blog
posts about someone running Firefox
within a docker container but then you
have to connect with V&amp;amp;C or some other
stuff to that container so it's a bit of
a hassle I haven't really seen it being
used at a large scale at least not for
IDEs what you can do is run your IDE
within your host and then run your
application server inside a docker
container so you can quickly fire it up
start it it starts in milliseconds go on
or use a docker image to run your test
then or something like that but really
GUI based applications is a bit harder
so what's a bit of the technology it's
just using see groups like I told you
before it's I think for at least six
years already in the linux kernel so
it's really tested well especially by
Google and they're still improving it so
doctor is just benefiting from the
improvement that Google and other
companies are making to the Linux kernel
okay now do the really the stuff what's
all about creating docker containers and
not just the basic ones but a bit more
advanced ones what I've done here is I
created a base container the one of the
main benefits of dr. is that you can
inherit containers so you can stack them
on top of each other you can make a base
container which includes for instance
Java and then you can make containers on
top of it for instance for Jenkins for
nexus for your application server and
that saves a lot of disk space and you
only have to configure your base image
once instead of making for virtual
machines with Java and net and
everything in it so that's really a
great benefit so what I've done here I
created a file called dr. fog for
general base so all configuration has to
be in a file called docker file and what
I say here is I want to base it on
window saucy so run is just I want to
execute some commands probably quite
familiar if you're working with Linux
already it's just linux operating system
commands so anything you can install in
Linux you can install an entire
container I you Cosette environment
variables so it's quite straightforward
these examples I mean might take a while
to figure out exacting for instance this
is some magic to accept the license
automatically I mean I didn't made that
up i just looked at the standard images
that are available in the doctor
registry and I just borrowed a bed of
the configuration there and now I have a
second container that I want to build
with sonar in it or sonic qube sorry
okay so the question is where did it I
declare that the previous container is
called general base I'll come to that in
a second so so this is a bit on top of
it for sona or sonic qube as it is
called nowadays so a benefit from the
general base where java is already
installed and I puts owner on top of it
so again some standard commands at the
bottom you see expose what exposed does
it opens up the port in your container
to your host so now I expose two ports
one for the weapon to face and one for
the data the data as necessary sir
Jenkins can put the data to sonar and at
the bottom the commander's run to start
sonar that's essentially the most
important part because else the
container will just stop if there is no
command to run so this is the structure
of for the directories I just have a
main directory with some scripts are you
now you can just execute command line
commands if you want because the file
has to be called docker file I made two
directories one for general base 14 Sona
and this is the part where your question
probably was about where's the naming
coming from here I say I want to build
the file and the docker file which is
located in the general base directory
and I gave it a name general base so
this is where the naming takes place
yeah yeah it's the for naming yeah and
the next I go to the SONA directory and
I built the sonar container and then
they are stacked on top of each other so
for instance if I want to change the
toner container I want a new version of
Sonic cube what do you do you can just
use the old version of general base so
you don't have to build general base
again you just create a new doctor file
with the newest version of Sonic cube
and you only execute the commands at the
bottom or only build the SONA part the
rest is just cashed you don't have to
build it again only if you throw it away
manually you can't
we're all images and containers then you
have to build it again but if it's there
and you didn't change it you don't have
to build it again and this is everything
you need to run the container the base
you don't need to run because it's
included in the sonar container so we
only start the toner container and here
we say map these ports so at the the
right is the port within the container
and this is the port in the host for
instance if you have 10 docker
containers all having tomcat they all
exposed port 8080 now how do you run
that on your host at the same time
that's quite easy you just change the
run command and you say oh map port 9000
to 80 AD and for the next one map port
9000 012 8080 so it's really convenient
to run all kinds of stuff that is using
fixed ports you can just map them and
it's easy to see now what containers are
running if you have a lot running on one
machine which is possible you can run
hundreds of containers on one machine
they're not really resource-hungry so
it's not a problem only take into
account then you have to limit resources
somehow because else one container will
get in the way of the other containers
and here we can see how long it's
running poured mappings and all the
stuff that you probably need one looking
which containers you have I already said
I don't like these cells and all kinds
of two specific configurations and
things I need to remember there are some
things you need to remember about docker
but I mean start/stop how hard can it be
to remember that it's all the commands
that you need you can follow the locks
as I showed before you can see which
contains are running also show that
before you can see the death of a
container so you can see what changes
were made since you started the
container so somebody added a file in
the container made some differences you
can see there
yeah yeah at the time the question is
how do we see how can see the Tomcat
logs that are being lured let's see
I mean don't get logs or just sent to
Thomas system out a system error so
doctor just shows them to you yeah now
you can see almost anything that happens
in your container with the lock command
of dr. it's really easy to work with
okay so the question is if it's not pipe
the system idea I've come to that in a
minute but a doc is all about isolation
and separation of concerns so what you
should do is separate all the data that
you're writing from the application that
you started so friends if you have
application server you should make a
separate container for the data like log
files and stuff like that so you can
export the stuff that's in that
container and have a look at it if
there's something wrong and the
application server should run in a
different container so if there is
something wrong with application server
or you want to upgrade it you just throw
away the container for application
server start a new one and still use the
data container let you use with the old
application server container but I
showed you that in a minute hopefully
it's clear on the top you can see the
running processes within the container
so almost anything you can see when you
ssh within the container you can also
see it directly from with some darker
commands and if you're toying around a
lot with darker creating lots of images
seeing if it's works you end up with a
lot of containers and a lot of images
and you might want to clean them up
because it eventually costs you disk
space so this are some commands to clean
up your containers and your images we
keep in mind once you remove them and
you build stuff again you have to build
everything again and it will all be
collected again from the internet so
there's no caching anymore so what we
did is is this we created a build
environment and a d-type environment
with different kinds of containers that
are stacked on
of each other so we have a general base
with the SONA which is already showed
you and here we have app server base
with the different environments you can
also see the Jenkins data container
that's what I I just told its you should
do this for all containers we were a bit
lazy we didn't do it for like sonar and
an application servers but if you're
really setting it up well you should
create separate data containers and put
everything that needs to be persisted in
the data container because the normal
containers you should assume that you
can just throw them away and start a new
one so make sure there's no persistent
information in it because if you throw
it away you don't have that information
anymore so really put that in data
containers so the question is why do you
need a data container why not write it
to the house system it's an option but
in the end we use these kinds of tools
for isolation and separation of concerns
and if you are going to throw everything
sorry back on the host then it's it's
not really nice it is possible you can
do it it's not a problem but from the
pure nicest solution you should use data
containers and it makes sense if you
think about it because you want to
separate everything and just isolate all
separate stuff so why not isolate your
data
sorry yeah the container ends up another
system but it is isolated you cannot
access it from some other process
because if you put the data directly on
your host and another container is
manipulating the data you probably end
up having issues oh maybe another
container is removing that directory for
some reason then you're you lost
everything so adds a bit um what you
like but nicest what they advised to use
data containers okay so the question is
why'd you call it the data container
it's mainly because it's for data that
you want to persist so that could be
locked files if you want to possess them
maybe you're not interesting and
possessing log files at all but anything
you want to possess so could be log
files could be database stuff anything
but all stuff that you don't want to
possess you put in a normal container
yeah i'll show it in a minute how you
configure it and then yeah it's called a
volume there for you today the container
they mix the terms a bit someone times
the question is if you can go around it
and back them up okay so the question is
can you move containers to different
machines I will show you in a minute you
can do that you can export them for
Institute or bowl and then import them
again but I would advise to only do that
for data containers of volumes and not
for normal containers because that's a
bit of a hassle when there are better
solutions but I'll come to that in a
minute
okay so the question is do you started
Jenkins container on the Jenkins data
container no the data combiners you
don't really start it i'll show you also
in a minute how you start it up but so
this is a bit of the explanation how do
we work with those data volume state
containers however you want to call it
and the docker file for Jenkins I
configure where the Jenkins home is the
Jenkins home it contains all the
configuration of the jobs and Jenkins so
that's data we want to possess if we
throw away Jenkins the Jenkins container
and start a new one we don't want to
configure all our jobs again that would
be ashamed so we say all the data is
stored in a / for / Jenkins data next
thing we do is we start the data
container or volume and we just say we
have a volume it's in this directory the
name of the data container is Jenkins
data container and we want to base it on
window saucy or whatever doesn't really
matter which operating system you choose
and as commands we just want to run
through so this starts up data volume
which you then can use and you can then
start the Jenkins container which is
shown in the bottom so do the port
mapping and say I want to use the data
container which is called Jenkins data
container so this makes the stuff in
this container available in the new
container the Jenkins container is that
clear not really active the data
container is not running the Jenkins
container is running
II don't I think you have to process
because you stayed off the debt contain
which is working but you don't really
start it so it's not really running it's
yeah you can see them separate
containers what I showed you before with
the docker PS commands you can see that
there are two containers and the data
container that's not started and the
application container or the Jenkins
container that's the one that's started
and that uses the data container that's
not started it's a bit weird um yeah
they are started but they're not they
don't keep running it's when i started
with it i also had something like hey
why don't you start it i mean you want
to use it if you don't start it you
probably can't use it but they have some
mechanism inside it that you don't have
to start it it doesn't have to be
running you can still use it a ya bit as
if you say the volumes from or what do
you mean Oh in the dock abilify yeah you
can use at volumes yeah that's a
possibility as well yeah yeah so you can
use all kinds of mechanisms within dr.
there's no standard way of configuring
it it's just what you like and this is
what I came up after the built
environment and the dtap environment I
was looking at I how much disk space is
that costing for me I have to admit
Jenkins wasn't completely covered so
later increased a bit because we
installed extra plugins but this gives
you an idea of how much data is
necessary for all the containers i
showed you before it's less than a
gigabyte try that in the virtual machine
I mean probably only have nexus running
in one gigabyte so that's really really
nice and you can see that they're
stacked on top of each other this
container is used by all the containers
that are below
so it's really a stacking mechanism and
the nice thing is this is the time it
cost me to build all these containers so
download java download nexus jenkins
application server everything and start
it up it took me four minutes and 11
seconds and that was mainly because you
only added thirty AM bit download line
so if you have a faster one it will be
even faster so that's incredibly fast I
mean if you have a slow virtual machine
this is the time it takes to start one
so now we come about the part of moving
at the containers I mean when we have
them somewhere we might want to move
them to another place for instance every
production environment in the cloud and
weren't the same thing on my laptop
that's exactly what I did the
environment we had in the cloud I ran it
on my laptop you can do it with
exporting and importing you already see
the commands are getting a bit less
logical than just starting and stopping
containers and don't have to remember
them I come to a better solution within
a few slides you can backup data
containers you see it's already getting
a lot harder than the previous slides
restore data containers and then run
restore containers and that was the part
where I was really a puzzled because if
you start the containers you have
exported and you import it somewhere and
then you you want to start it you have
to give them all the commands and
environment configuration that is
configured in the docker file you have
to supply it again to the run command
because it's lost all of that so that's
a bit hard so now you can see here I
have to supply this to the run command
and but it was already in the dockerfile
the same for the command to start
Jenkins so it's not really convenient to
export and import docker containers and
I wouldn't advise it the only part where
it is useful is for importing and
exporting data containers because that's
persistent data and you want to move
that around then you want to export an
import but for normal containers there
are better solutions which I will show
you here
so on the left there is a docker file
you've already seen that a couple of
times now you can just build a docker
file that creates an image and we can
then push that image to the registry
which you can see at the right upper
corner its morale of lesser get
repository you can compare it with that
at the commands are quite similar and
then all the clients can use the image
that you've put into the registry so if
you have a d-type environment or you
have for the same images that you want
to download you can just ask the
registry for the image and you
automatically get it so that's really
comfortable really easy because we want
one ring to rule them all so just one
image because the disadvantages if you
have multiple images you create them
every time for instance if you have a
d-type environment you start on D on
Monday you run the docker commands the
containers are built and those
containers they contain a step where our
packages are updated on Monday next step
you go to test environment accepting
environment and on Friday you go to
production and you build everything
again again all the packages are updated
but maybe on friday you already have new
work packages and on Monday and that's
something you don't want so you really
have to create one image and use that
for all the systems to make sure they're
equal and it's really easy to start a
doctor registry or you could use the
public one and there is a public one
available with a lot of images inside it
so don't get everything I didn't invent
it myself it was all there but if you're
working in a big company probably you
don't want that your software is all put
in a public registry because you put
your production code inside it you
probably don't want it at least our
companies don't want that so it's the
best solution there is to just create a
private registry it's just like an
on-premise get repository or with what
you can want to compare it a registry is
as a default image if you don't have
specific needs like security or
something like that and this is the
command to just start
it if you have more specific needs you
can do a bit of tweaking on it and after
that this you just have dr registry and
you can push images to it so how does
that work it's quite easy you have a
container you change something to it for
instance add a file or something like
that and you do a commit here this is
the server running the registry so I pr2
has no not nice naming conventions and
you give the stove a name so more or
less attack and this will give you a new
container and then you push this to the
central image registry so and this we do
on the docker client one so it's for
instance a machine in the cloud
somewhere and then we have a second
machine which is somewhere running in
another cloud and we want to pull the
image that's changed from the first one
so we can pull it again and because I
run it and we see here already this is
the same ID as the one that's created
here so you're more or less mirroring
all the containers and the stacks of
containers from the first machine on a
second machine
sorry
yeah so the question is if you change
the container manually then it's not
possible to recreate it again from the
dockerfile that's correct so I I
wouldn't advise it directly I would
always do it from within a docker file
and then you can just you just get
stacks on top of each other and you
commit that one so it's not such a nice
example to say just add a file that you
should do it off truly docker files yeah
that's correct so this is one of the big
advantages of docker if you have changes
dr. just sees oh it's a diff only small
piece have changed only those discs I
have to go to the image registry you can
see here they are pushed to the docker
container image registry and if you want
to download them only the updates are
downloaded so I'm no longer moving
virtual machines around over the network
we just moved if surround over the
network so that's a lot faster and it
looks like the sorry
yeah okay so the question is if you have
a base image and you change it or the
change is propagated to the containers
build on top of the base image not
automatically you have to build them
again and I think that something that
you want because else if you change the
base image maybe all the other
containers fall okay so the question is
can you fix the actual version of the
base image yeah you can just give it a
name so you can include a version in
there if you want yeah maybe you can
even die get out and I haven't used that
so this is what we see if we are using
on the second we're looking at the
second client so where we pull down the
changes first we see the images that
were there it's only the one in the red
then we do a pool of the new version we
see that one that's blue is added and if
we do a new command we can see we now
have these two images which are the same
as on the server where we build
everything and pushed everything and as
you can see here it's less than a
megabyte the changes so that's all that
has to go over the network it saves a
lot of network bandwidth compared to
virtual machines no no no the question
is are only the configuration file
changes coming over now it's actually
really the container so you built you
change your configuration files or your
daughter file you create a new container
but that's probably only minor changes
from another container and that def is
sent to the registry so it's not the
configuration but really the container
differences yeah as you oh sorry
h/t good luck
I must be a deaf person about it how you
doing though
yeah okay so i understand correctly you
want to know how if I build a nice
application in my IDE go through Jenkins
and access how do is that does it end up
on my machine I show you in a few sheets
so I really use the registry because
it's far easier than exporting
everything and importing everything and
it really is the way to go don't use all
the exporting importing except for
persistent volumes not for normal
containers and I think I told is already
use a registry don't build everything
every time again because then you get a
difference between the sub the different
containers the disadvantages if you only
created once and you deploy it only
after a week to production for instance
then it can be that in the meantime
you've missed some security patches
because you didn't build them over and
over again but I think that's a logical
thing this is a better overview of em
doc versus virtual machines virtual
machines they need a hypervisor they
need to translate the instructions they
cannot use the maximum performance of
your resources because they have some
overhead doctor doesn't have that it
runs directly on the on your cpu so it
can benefit from all your resources if
you're interested IBM did a presentation
on dr cohn about it they compared bare
metal with kvm and docker a really
extensive comparison and each other
doctor almost got bare metal performance
which is nice I mean we can do more with
the same resources or save money if we
need to convince a manager I mean what
do you want more so this is about the
Jenkins part it also answers that thing
a bit of the question how do you get
from your IDE to a d-type environment
let's see so here I have a bed of code
sorry here I've got a bit of code and
this is being deployed it's just a
simple HTML fronting for application now
let's just change this so this is just
now you there's some Java code behind it
as well but I now just change this one
is a bit quicker give me a moment I have
to commit the changes to get ya you have
to believe me for a minute because I my
duplication isn't working and also have
to look like this while typing all my
commands so sorry I'll be back in a
second
so we're back what you see here now is
let's see which one I mean to have this
one this is the Jenkins instance this is
just copy from the one that we're
running in the azure cloud so it's just
exactly the same container that's
running there we have it I have now
running on my laptop because I didn't
want to depend on some wireless
connection that might fall off and these
are more or less the Detapa vironment so
d.t.a epi and as you can see here
there's this is a test and I change it a
bit so it should pop up if we do a new
deployment i use the build pipeline and
come back to that later I just started
deployment quickly because it isn't
taking long but it takes some time and I
first continue with a few slides while
the deployment is running in the
background that saves a bit of time
because it's compiling the Java code and
running some tests so it takes few
minutes yes yeah correct sorry for do it
in the background I just committed my
changes the line I added that I
committed there to pushed it and then
started Jenkins you could also use get
hooks to automatically start a Jenkins
job after the comment that's also a
possibility up so uh sorry
I have to admit when we did this that
this was quite a while ago and we didn't
do a lot yet with docker on a larger
scale so we don't use a registry yet so
that's the reason I now say really use a
registry we did it without it but it was
the wrong choice but that's something
you learn when you're doing it okay so
the question is a foreign jenkins i
forget do it automatically get the data
volumes now the data volume is already
running on that instance so I don't
deploy the data volume again I only
deploy the application again and that's
all persistent stuff just stays running
there because if you deploy it over and
over again all your data is gone if you
start new stuff that's really I really
advise you if you like it just try it
because it makes lots more sense with
filling around to data containers and
everything if you just try to bed and
see what the differences are between the
different setups
auditory that will close
what do you mean with will prosper or
your post starting do you mean the
doctor registry yeah yeah what you
should do is you build your stuff build
the docker container in your build and
then push it to the registry and then
pull the stuff from the dtap
environments that's that's the right way
to do with what we did now is on every
detail environment we build it again but
it's not an Isis solution let's see we
already have deployed to environments so
let's see if some changes are popping up
here that's from part 55 so it is
working I go into detail of the
configuration and the sheets in a moment
but i just wanted to show you a bit
about how long it's taking you see here
it's just milliseconds r this is
milliseconds I think this is seconds but
it's really quick and even while we are
building the container again there if we
just pull it it's even quicker if you
only pull the changes and starting a
docker container that's really just
milliseconds so you can see here it's
almost no overhead over your build
process building java and all other
stuff takes a lot more time than just
starting it starting at this done within
i think 10 seconds and if you optimize
it probably you can get within five
seconds or less so it's really quick
yeah
okay yeah so the question is can we put
multiple observers in the same vm or
operating system everyone's on crack
yeah you can just do that and you can
even use the same darker files and
exposed port 8080 or whatever port you
want and then when you start a container
you just do the port mapping and you can
start as many as you want just increase
the port numbers is that answer the
question yeah yeah because if you run an
application on port 8080 you cannot
start another one on that port yeah yeah
then so the question is you always want
to use the same port and you would need
to use a load balancing or something if
you want to do that yeah but if you have
to application service run it running
application surf probably runs on port
8080 which you cannot have two apps
running on the host on port 8080 yeah
that's possible but it's possible to run
multiple instances but then those
instances run on different ports and the
question of your the man behind you was
fine I want to run them on the same port
and and that's not directly possible
yeah that's yeah you can stack the
content oh sorry you can stack the
container that's also solution yeah okay
yeah that's also possibility but then it
gets a bit i think the configuration
will become a bit harder so question
then is do you really want that or can
you better just use a load balance or
something like that
so the question is I'll do or this
docker map to the ec2 and how do you use
the configuration of ec2 and get the
doctor on it I'll come to that in a
minute with what we do is we just
configure the connection settings to the
different machines so it's just SSH we
access it with ssh former from Jenkins
we just connect to it and execute all
docker commands on top of it right on
the instance yeah so there are we
installed or coronal instances
automatically through Jenkins yeah and
then we put everything on top of it so
it's really just not made a process if
we move to another cloud we can use the
same Jenkins process and it just
automatically installs doctor on the
instances we get and delivers to
complete environment again in minutes
yeah yeah and the only thing we now need
to change if we want to move to another
environment already IP addresses and the
username passwords of the different
servers but that's centrally magnitude
in Jenkins so you can adjust that in one
place and all the jobs they stay the
same that you make in Jenkins so that's
really I will come to it in a moment as
well but first let me ask question who
if you were using Jenkins ah so that
answers my question why Jenkins
everybody is using it it's quite easy to
use so why use anything else I've seen a
lot of things delivery implementations
where they use some proprietary software
or other software which nobody knew then
you have to learn that first I have to
figure out every thing that's maybe not
working optimal there it costs you a lot
of time so which is opted for something
that's new known that stable easy to
work with but then one thing that was a
bit of disadvantages in you quickly have
a lot of jobs and Jenkins and it becomes
sort of a mess I mean which job do you
start when and yeah how do you do they
relate to each other it's quickly
becoming a bit a mess so now we look for
different plugins and this was one of
the plugins we really liked it
to build the pipeline plugin it's quite
a simple plug-in but it fits our needs i
will show you some of the parts word
doesn't fit the bill anymore if you want
some advanced use cases it's not always
is helpful for you but this is more or
less it so you can see we do a build
back end and then automatically the
development at the top the development
environment is built so the image is
built or the container is built and and
the container is started and we do a
short test to see if the container is up
and running and the same for the test
except in the production environment you
might notice one thing here this is a
small button play button this enables us
to after we start the deployment this is
all deployed automatically and this is
only deployed after we press this button
so it we don't want our stuff to
automatically end up in production which
first we want to press on a button it's
just a small extra step to prevent some
small errors the same goes for the front
end-front end is just an angular
application quite simple it's more or
less the same setup and we just have a
combined pipeline so we can press one
button and everything is deployed
automatically to deafen test environment
you could put a gate hook on it so that
it does that automatically or you can
just press the button or if you want you
only change the front end you can just
start the front and build and only the
front enters deploy it again so the
button you saw for the automatic and
manual deployment that's what are we
sorry then I will show you in the next
slides are that's implemented so it's
really simple I think the one at the top
every one that Jenkins you this is
already familiar with you just say after
this job build these jobs so they will
be executed automatically and we have a
manual step
and these projects are only built when
we press the button then the build
pipeline plugin again what I said it's a
really simple plug-in no advanced
features but you can configure it a bit
you jet just say which is the first job
how many builds you want to see if you
want to allow the manual triggers so for
the acceptance of production and that's
more or less it then your upper running
and you can use your Jenkins jobs and
then it looks more or less like this the
blue stuff we didn't this is more or
less the registry we didn't use that
already but that's something I would
really suggest use the registry don't
build stuff over and over again it's
really simple just a built environment
we commit stuff to get everything is
checked for quality and everything is
brought up within seconds and this is
where I better than magic the connecting
to the different surface and the ec2
cloud or wherever you have those servers
running as long as you can access them
through ssh is going to happen so you
just configure the different surface so
the deaf environment application my
sequel some hostnames usernames
passwords is hidden underneath so if you
have to change these ones you can just
change it here and this is the
configuration within the job and this
uses just the name that is used here so
if you change any configuration you
don't have to configure your jobs again
which is really nice a purpose over ssh
plugin is really nice you can use it to
transfer files which is shown at the top
you can use it to execute commands so we
use it to transfer our darker files for
instance or whatever you want to
transfer and then start docker on
whichever machine you want it might be
that you have a somewhat more advanced
use case where you start a job you have
two jobs running in parallel and then
you have a third job that you only
allowed to start after the parallel jobs
are finished fencing's one parallel job
does the unit tests and the other one
does the functional testing or
integration testing and you won't only
want to deploy in the lost job if
everything is running successful that's
quite easy you can easily manage that
within Jenkins you just say from as post
build action build two parallel jobs and
after those parallel jobs are finished
then build my last job for instance the
deployment pretty easy problem is with
build pipeline it looks a bit messy
because it doesn't really understand
parallel ISM and it shows the lost job
twice once in EM blue and once in green
once it's executed so it's not really
nice so error already we are looking at
the boundaries of the plugin hey I think
you can even hide it with some CSS
tricks that shouldn't be really a
problem but if you want more I mean I've
seen really advanced plugins and in
Jenkins that have even retry mechanisms
so if your deployment doesn't go well
because there are networking issues then
you can automatically let it retry stuff
and do all kinds of fancy things mostly
with again a diesel groovy or whatever
it is possible and the nice thing of
Jenkins's you can just reuse all the
existing jobs that you create it this is
just the layer on top of it so you can
start simple and if you have the use
case that you need advanced stuff you
can use more advanced plugins but my
advice is try to keep it simple because
I've seen lots of contains delivery
pipelines fail because they make all
kinds of assumptions and if one
assumption goes wrong and everything is
broken and I mean it is the central
pipeline in your organization if this
doesn't work you cannot do anything
anymore you cannot get to production
anymore so try to minimize the
possibility for bugs these are some of
the plugins that you could use if you
want really more advanced stuff like
retry parallel and all kinds of stuff
you can do there but keep it simple I
mean why should you make it difficult if
it's not necessary
so demo we already have seen that these
deployments are finished and think so
these are also finished I had to press a
5-4 moment so now we can see on deaf
that the new code is being deployed so
part 55 we can see it on test on
acceptance we see nothing and acceptance
we see nothing so that's in our
front-end code so now I can just say
deploy my front-end code some refreshing
issues of think so it's running now so
we can see in about 10 seconds then
everything started that's even quicker
now so now the docker containers are
started again with the front-end code
the angular code and it's done already
don't know now I've look when we see the
new code so it's that quickly it's
really fast a docker doesn't have really
over had over your normal deployments
and that's a big benefit there are no
real disadvantages of docker that was at
least not that I really saw when I was
using it I was quite skeptical in the
beginning at sounds cool but you
probably have some issues with it but I
didn't really had any issues it's really
fast and almost no over at and it's
working really easy I was in a team
building a word second building an
application and it contains delivery
pipeline in one week from scratch no
specification yet and just build it and
it went off really quickly we managed to
achieve it we had some colleagues it was
like at a like tom cat but he never
accused the docker before but in about
an hour it changed our complete detail
environment use jboss it's that easy
even for people have never used it its
it's really a good tool thing
okay so the question is if you deploy to
the environments or the containers
running no you shoot the idea of dr.s
that you don't upgrade containers or
anything you stop the container throw
them away and start a new one
oh and the question is how do you get
the war file and everything in the
containers that's done within Jenkins so
Jenkins puts the war file in the
location and then it runs a docker
command to build a container with the
new war file and add that container is
done deployed on the dev test a MP
environment but you really should do
that through the registry so within
Jenkins you build a container with the
newest version of your application you
push that image to the doctor registry
and then on dta MP you pull the doctor
registry and get a container that you
want or the image that you want yeah
yeah yeah so the question is what to do
with environment specific settings if
your friends have a d-type environment
you probably have four different
connection settings for for databases
you have more or less two options one is
to include all the configuration in one
container and deploy that same container
on for machines and for instance when
you start or you could choose to start
the docker container and then at runtime
supply the configuration settings then
you can still use the same docker
container on for environments or the
last option is to make environment
specific containers on top of the normal
container so you have an application
server container and then you make a
container on top of it for dt a.m. p
that's a bit less nice but it's an
option I think the nicest is to supply
the configuration parameters when you
start the container with the run command
if you just supply it there I think
that's the nicest way to do it err I
have to stop so thank you all</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>