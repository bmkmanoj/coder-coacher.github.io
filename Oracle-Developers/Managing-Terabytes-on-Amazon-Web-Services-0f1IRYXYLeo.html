<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Managing Terabytes on Amazon Web Services | Coder Coacher - Coaching Coders</title><meta content="Managing Terabytes on Amazon Web Services - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Managing Terabytes on Amazon Web Services</b></h2><h5 class="post__date">2015-06-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/0f1IRYXYLeo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">today you get two speakers in price of
one which is a bargain this is me my
name is borrow I'm a developer advocate
at the company named Jeff frog in a
minute so my job is hanging out with a
real smart guys in our company and then
coming here in talking with you about
those stuff when the stuff get
complicated too much for me to
understand I just bring those guys on
stage so here you will get Fred famine
which is our chief architect and the
alder alder and containing part and he
will do all the smart part of this talk
now this is how it's going to work m.j
frog and as you might see we're a little
bit obsessed obsessed with frogs who
ever attended our booze probably got one
of the Frog t-shirts and this is a
Israeli based company is start up about
40 people much more that appear on this
couple of months old picture and yeah
and mostly frogs we have two products a
one of them is ben tre which is a cloud
distribution platform a and we weren't
going to talk about Ben Tre today the
other is our tea factory which is a
binary repository in some aspects of
artifactory architecture are going to be
covered today this is what this talk is
about artifactory is a binary repository
manager we believe it's the best one out
there we actually know it for fact
because this opinion is also backed up
by the community just number of small
projects host their binaries on
artifactory guys like spring jenkins
typesafe Gradle etc et-cetera every time
you download spring you actually
download from artifactory every time you
do something in scala in SBT you are
going throughout the factory it's kind
of a kind of a good recognition so I'm
just for it's not it doesn't really
relate it to the talk but just for you
to be
octave a little bit to wake up after
whatever you did up until now who uses
out of the factory good good good m okay
who is the other guy we love you because
you are going to be the next users of
our divisor yeah who uses the really
other guy who heard about the other guy
this guy no no yeah okay and any
cavemens good come to speak to us later
we will upgrade you to be a mother and
software develop okay really this is
nothing to do with the talk okay just
for the fun this is our tea factory in a
2006 this is how we used to work with
maven without a factory and then you put
a proxy in place it's just a proxy for
our artifact this is how it all started
and the UI was great actually you see we
meet I personally miss it I think this
is how it should look now but it doesn't
and since then if there was some real
big evolution in artifactory it's an
open source project in essence it got
some traction you saw the project and
everything it was it is a successful
project so a based on it we deliver we
developed another flavor of the factor
which is the pro version the power pack
it has additional features enterprise-e
a and everything and this is how it
looks in the architecture how it's used
to look up until six months ago so we
have all this spring in the middle we
get for web app and jersey for a rest
api and there is some guy called jack
rabbit see jack rabbit this guy jack
rabbit is an implementation of a real
juicy are no one heard about it but it's
a real one a 117 java condo
repository em it's a it's jase are for
java condon repository for a
implementing staff like content
management system etc etc and I think
the only implementation of Jesus are its
broader called jack rabbit we used to
use it very heavily energy factory as
our storage mechanism I'll explain how
it works in a second it's no more there
we rewrote our storage level from
scratch in version 3 and it did a little
bit of magic factory free it's ten times
faster just before because we ditched
this guy so we killed the rabbit but how
it worked how it used to work in how it
works now it's actually check some
storage you not get users check some
storage they I kind of proud of it
everyone who heard about get know about
the checks on storage they didn't invent
it we didn't invent it neither but we
were definitely there before them so we
were there first this is how it works
all the binaries are stored under their
checksums in your file system or your
database as you prefer and here is the
actual file and the mapping between the
actual file and the path is in the
database here it is this is the DB
record that mentioned that the file is
actually there and this is this guy is
should look to you in the files in your
tree browser as repositories labeling
local etc okay so basically it's a
little bit like Java heap you have this
object line anything in the hip without
actual address that is accessible to you
and then you have references this is
more like like it so you have this a
checksum name of the binary that doesn't
mean anything to you and then you have a
reference to it and of course you can
have multiple references to the same
file which means that copy is free move
it's free synchronization is very cheap
it's a traitor and of course
once it's like Java hip should have a
garbage collection and we have one the
good thing about being a garbage
collection kind of storage is that this
storage is append only it's right only
we only add new files never delete them
never override them and then there is a
garbage collection process that goes
through the records in the database see
which checksums are not referenced
anymore in any by any reference and
delete them I synchronously which means
it's very very fast as we don't need to
deal with our right fred is going to
talk about it in a minute actually yeah
so as I understand we have awesome
storage and you are here not to hear
about our storage one Java wanna go
exactly 10 wanna go a thread give a
great talk about a how we took this
artifact story pro version and created a
software as a service offering out of it
so now we have this cloud offering you
can go and get our tea factory instance
on a cloud so now we have the online
version 2 it was announced in 2009 and
since then all those guys that I showed
you earlier jumped in and we now serve
the bynars of all of them plus a many
many others a for us there are two main
things about cloud self-service which
means that you can go and get your own
instance and multi-tenancy multi-tenancy
means we have a lot of tenants each of
them with their own artifacts on our
servers and when I talk about many
tenants there are many tenants okay it's
like skyrocketing just about in the last
two or three years
the platform that we based our
artifactory cloud offering is a amazon
web services and now we are getting to
the essence of this talk how we manage
this match terabytes on amazon ec2 so if
you don't believe i have some billing
invoices for you from amazon and i'm not
sure we can see here in February it was
16 Tara in May it was 22 and just before
I came here we got the August billing it
was thirty two terabytes on amazon cloud
now comes the interesting part how the
heck can we manage all this on amazon
and when it comes to the interesting
part the real guy will do the job okay
you're good I'm good okay okay so yeah
you can see Pass me the slides when I
have to go to sleep so the fight this
time you walk to how one of the many
thing about the file storage and the
garbage collector is that we never never
modify the file okay we just add the
fine and then from time to time we
delayed to bunch of them inside the
garbage collector so there is a lot of
system on top of the file system like
yours or NFS s3 and all this kind of
system that spend a lot of energy
managing changes of the same file okay
this is something that doesn't happen on
our platform now on our environment okay
this is why we need to be a lot a lot
lazier once you have right on me and
then garbage after you need to be a lot
of lovely here on your management so
what we find out is for example when you
do a meta data request is the file there
when it was put what is the size all
this kind of meta data about the fight
for us it's just the database request
okay this is the big difference we don't
access the actual amazon fight storage
where we put our terabyte of binaries we
do that on the RDS database of Amazon
okay so any kind of meta data
information about the file and all these
kind of things that the user is asking
is not going to the file system but if
you use apache web server and all these
kind of things they're going to check
again and again what is the state of
your file system if you do a head
request this is the more one of the main
interaction without the factory we are
receiving multiple tens of millions of
had requests per day just to know about
the information of the different files
that are on the different repository of
everyone okay things are not changing
that fast in the world and when you do a
get request of course we need to make
sure that it's there and at the end of
the day you need to get the bike so we
do at this time a real file system
access but in our system which is what
we based we kind of trust or believe
that the file is there we don't even go
and check and do any kind of
methodologies get the fine okay again
when you do a file system request and
when you redo a put it's the same
mechanism here there is something that
is a little bit special compared to a
normal file system which allow us to
isolate the different people that want
to put binaries inside our system is
that we put the file first in a p5 store
what we call the p5 store we calculate
the checks on and once we have the fine
and we know that everything is ok we
move the file to its final location okay
if you manage your file system by
yourself on the Amazon and you give
access to all kind of different
processes to create file in all kind of
different environment you get into a
world problematic of philo king and
having multiple people trying to create
an elite the same file and have also
inconsistency okay one of the main issue
is that you don't want the file to be
readable until you know that it's
perfectly the good fight with a good
check storm at the good place and with a
good
okay so the only thing that the file
system provide you which is atomic
basically it is a move request so I'm
going to show you how we do we do that
so we say okay we have to move our
application which used to work for any
user on its own file system and internal
environment on AWS which storage to
choose to put our binaries okay by the
way when we started which was 2009 there
was no obvious okay RDS didn't exist on
the Amazon RDS is the relational
database service of Amazon how many of
you are using ideas okay not not so it
didn't exist so we manager on MySQL
database and mysql server on the amazon
ec2 stack and so it added quite a lot of
complexity to the different usage the
five store which is checked some storage
and the database mysql which behave with
the five system in a completely
different way as you may understand so
basically are very fast after we
launched we managed to get the RDS so we
extracted this problematic out of the
database service and we got high
availability high availability
everywhere it's I mean it we took it
when it was alpha know when it was very
very young it failed us since two
thousand and nine and ten once it was
the audience system the Milky database
zone okay Felder's once in the last
three years more than three years of
operation okay and we have an extra
router I think we are more on more than
you know the last number but three to
four thousand I owe for second on the on
the mysql so very very stable and a high
durability and availability smoochies on
it's always there so basically RDS
completely remove the need for
high-level storage and high level
database now the meta data / factory is
quite small okay we have
than 20 gig of mysql database okay
that's not the terabyte that we are
talking about about define themselves
okay that looks good so what about the
file system what what we're going to do
with the file system again we had to
choose let's see so we have the CD and
the Amazon s3 the dynamo DB we could put
also the binary inside mysql itself as
blobs okay so i don't know if anyone
ever tried to do that I hope you stopped
fast so it's just I mean the MySQL GBC
neither the MySQL client the mysql
database it's just not made to manage
blobs and managed by name is we could
have moved to an oracle database oracle
streaming and the oracle DB is doing a
really good job at managing huge blob in
and out but i don't know how many of you
tested the oracle database on amazon we
are still as topic you have for
information yeah we are multiple can I
can I don't know if I want to say that
we are almost finished a half a million
a year of amazon and the cloud cloud
costs so it's quite painful yeah I know
I know I saw that thank you so big
storage on Amazon always available easy
to download you can put gigabytes and
gigabytes amazon s3 that sounds logical
it's eventually consistent everybody
loves the eventual consistent there is
some kind of couple T of em that helps
you but basically there is two big
problem the first problem so it's a mega
20 or everybody like it I but basically
we cannot use it okay we cannot use the
eventually consistent in an environment
for to the the other one is that the
first reason is that like you
on the on the explanation of bowel about
database and offer I saw our database
and our first all need to be in sync
okay if inside my database someone is
telling me this file exists and this is
the checks on I need to be sure that
it's available and that's it there okay
s3 doesn't give me them okay the other
thing is that s 3 is built for heavy
heavy read sorry about that okay which
means that when you put a file you need
to read the same file at least 10 x 20 x
30 x okay that's where s3 gets really
really powerful it's tough to distribute
and and this is the usage so a static
website on s3 and all this kind of stuff
can be really really powerful and really
useful for us we have a binary
repository manager and spring and all
those guys for example they are doing
every time there is a spring developer
that is doing a chrome it its launch
build on their bamboo server and it goes
to our server and it distribute all the
results of those artifacts they are
doing some couple of tests they are
reading the same five to three times top
and then they move to the next commit
and they move so it's continuous
integration ok so the files are written
there again in our file storage and read
back most of the time to three times top
okay when it's a big release of course
it's it can read it can be the same file
that is read 20,000 times when it's a
spring ladies but most of the time it's
just the level of read and write is not
good for fourth that's one of the big
issue for system the other thing about
s3 is that so we need and i seed file
system okay if the database and this is
where the point we have a transaction
that says this meta data with this file
exists in the database you need to exist
on the phone system so the only way to
have strong acid on your database is to
do a move to find your system whether
you do them for your information in
artifactory 3 dot 0 it works only on jdk
7 okay we took the decision it was only
on Java 7 okay the main reason for that
is the new io the java you are you on
the java server
and the ability to control an atomic
move on the niÃ±o level and to control
ceilings and all these kind of things at
the low level of the file system in Java
7 is very really saved us a lot a lot of
attack especially on a Windows system
about making sure that the fire can be
moved from one place to another atomic
okay so we need a seed file system so sv
was not good what will you choose next
time is elastic block storage how many
of you are using EBS on amazon okay oh
not much so he BS high-availability have
availability everywhere well not really
fully beers a question that is quite
some time ago as soon as a launch it can
I have any BS mounted which is multizone
Ricky data center inside amazon no it
does not ok so basically are you cannot
you cannot do it what you need to do so
that's from the famous French movie that
I watched too many times and the bajo
really love to put the picture again ah
so you get a t-shirt if you know the
name of the movie but basically you have
to do it manually ok rapid any sales
nobody oh no no tissue it's exception so
from this is a French movie from 66
you're not you know ok so it's called
like on Broadway I don't know what you
say
so what did we decide we did it EBS okay
we needed a lot of VBS and we needed a
terabyte on EBS what we did is basically
EBS plus lvm and it served us until very
recently until six months ago this is
the platform that we were using to
manage hola terabyte and to go basically
to serve the needs one of the main thing
about Amazon and what's nice about
Amazon is the elasticity so you just pay
for what you use okay so if you buy
storage and you go to emc and you say
okay I'm going to get 20 terabyte of
whatever storage okay you pay 20 a
terabyte and you wait for the customer
to start to use the term back ok on
amazon it doesn't I mean you can do it
like that but you just pour in the money
and giving money to Amazon it's not like
they really need it so the girl bit of
course is to stay elastic even at the
storage level how do we make okay and
how do we create on our system and
elastic storage that can go and so we
can pay the terabyte that our customers
are really using okay so what we do is
that every time we start a new instance
which can serve customer data we put
basically lvm stripping so it's Sweeting
to both of them and reading from both of
them in the same time okay one trick
here when you do a vm on EBS don't do
sequential because what happened here
with the with lvm is that he's going to
start to write to this one and this one
is going to sleep until you fill up this
one okay and an EBS that is sleeping and
that is not sending io from one
connector when it starts to wake up
there is a bunch of virtualization and
stuff and and so you get into the
machine gets really really locked down
for 4 minutes okay I'm not kidding it's
kind of freaky you cannot do it I mean
it's very really lock down once the EBS
because the EBS it's a mounting the
drive on the on the low level of the
virtualization of Amazon okay so you
don't have access from the OS level you
don't have access to this attachment
process of the drive of the EBS to your
to your operating system so whatever is
going on below this you cannot really
really control it okay so if you want to
take this time to mount the second
machine basically there is nothing you
can do you have to wait for so we do
stripping to make sure that both EBS are
always read and write and that is always
activity on both of them and so they are
always hot and it worked really really
nice so we want here to a 500 gig
basically this is how we start this is
the basics and once we start to have
tomorrow we add two more okay so if I'm
good in math it's one Tara and normally
when it gets to six here sometimes so we
do 1.5 but most of the time you start to
do one tail I don't know if how many of
you knows but the disks inside the EBS
and the blocks inside EBS that are
managed inside the amazon block storage
it's one terabyte okay so once you take
a full one terabyte of storage on amazon
it's all for you okay so there is no
shared when you take the 250 you share
the disk actually and you share the disk
access with some other guys okay once
you take one terabyte it's all for you
okay so very fast once the machine and
start to get big and the customer on the
same machine we already tenant so the
same / data here can is pleated between
the different customers they have all of
them a different chilled under the
sludge data folder so once we get a
certain limit of we start to take one
terabyte blocks okay and here usually we
start to get also quite good performance
increase so one of the main thing the
EBS are really nice like you so they are
not backed up between data center and
the Amazon term of service or service
the license agreement I mean speculate
that the old data center okay control
down they can shut down the old data
center and they don't pay you back okay
you have to replicate between the data
center on the same region
basically you have to pay twice for
everything you buy okay they are really
I think you get the trend then you know
get you anyway so what we wanted to do
is first of all so to do EBS and to do
after of the backups and the backups for
for our customer ok so the this is Tori
plan for the backups goes like that
basically I just think you know this
picture there is to kind of people the
one that do backup and the one that will
so where can we do the back up so that
if one of the data center fail we can
recover the data as fast as possible on
the other data center and we set all our
customer up and running as fast as
possible ok so again ok for backup which
one to use so here we went back let's go
back to s3 maybe this time it's going to
be good and we're going to manage and
and put the binaries on s3 as a backup
as a low hand back up but not as a
permanent feed right solution ok ah was
not very complicated I mean I don't know
how many of you played with the s3 and
sv upload but you have to be patient ok
a lot of time into it fails you have to
retire you have to multiply the threads
that are hiking to it you have to make
sure that you speed up the buckets
correctly if you write the same file too
many times with too many sides also
there there is all kind of limits and
tricks and annoying things but at the
end of the day we managed to do the
backup of the files so we back up all
our files we also did a backup of the
database just for the customer actually
so that the customer can get their
database out of our system and we keep
them in sync ok so we are the dominant
system that do this back up to sv of the
database and the file system actually
still walking like that but not on a
permanent basis and we keep them in sick
so our
actually instead of backing up all the
files from scotch we do backup of only
the data we manage so a bunch of threads
to manage the efficiency are for
extracting the data from the from the
database and from the file system to the
s3 not to fully the online system I was
a little bit challenging and of course
always always keep the data isolated
make sure that one customer is not
seeing the data of the other ah by the
way I don't know if some of you knows
about dropbox mechanism about their
checks on storage okay but they have a
global when you download something from
the door box in the URL you can see the
checks on okay so you can download
basically anything and so if you know
the check some of the file you can
actually download it okay and so they
say no it's not secured or whatever but
I mean this is the point is that if you
have the check storm of a file you have
the fun okay the mechanism of the
encryption system either someone give
you the check song okay it means that
you know the file and in enid Iggy you
want to give you access either you have
the file and you and you know is
checksum okay there is no way to invent
a checksum and say okay I'm going to get
the secret difference department and
fine ok so in dropbox they're all
checksum and checks on storage is
completely deduplicated for all that
customer okay for us we still keep one
GC per customer and 15 store for
customer okay so the first almost
completely isolated but we had this
discussion about G duplicating the
checks on storage between all the
customers so again for backup there is
also glossy a that popped up two years
ago if one thing like that so once we
had in as3 it's really easy it was a lot
easier than I thought you go to the
bucket and you said everything that is
depending of all kind of wolves please
put it for four glaciation okay it costs
basically a lot lot less than the
fullest three a penny per Giga but when
you want to access it
you know buglet okay so you you have to
hope you will never have to hopefully
for us we did actually immigration we
never had to use the glass here but we
know at least if something goes very
wrong we know how to get back all the or
customer daytime so it's part of our
service level agreement and we know that
the data is there okay rarely rarely
Richie okay actually it's not true for
most we not from glass it was before us
three there was a lightning bolt in
Ireland that destroyed one of the data
center and all the the engine and
everything and so we had three customer
over them and we had to lead them to the
other data center and so we use the s3
back up and transfer it was no much 1.5
terabyte at the time so it was not that
much but it took a long time waiting
okay so just remember to check some
storage our DB like all that point to
the path and so we thought that we can
do better for all the customer basically
edss so basically it's how can we get up
and compress all the customer EBS s okay
and how can we make all of them and
instead of having because I mean we are
a binary repository manager used a lot
on the JVM languages and chava languages
okay and so all our customer they have
the log4j job okay it doesn't make much
of a sense for her to have the same log
for their jaw copied to all the
customers so they do plication should
work really really good for us okay so
how many of you are upset that we are
using deduplication okay once before
that further
so again the Box disagree so basically I
say okay let's try to write we were
already actually to write our own
deduplication system and our own
compression system for five storage for
the cloud or failing to offer some kind
of global file store for all our
customers okay and something to do but
there is a way to do it on the file
system level how many of you know what
do you okay they remember the good time
use the Sun they used to do some good
technology they used to make servers
they used to make solar is opensolaris
they made VirtualBox like we talked this
morning the cost ratio between vmware
and and virtual box is infinite open
office and they made another one which
is gfs okay so this is where we looked
at we looked at gfs and basically what
we did so it went at the time that we
recruited some really good devops guys I
mean for me the kind of crazy land they
are really really good they know what
they are talking about and so we started
to put ZFS on amazon okay and to create
basically a global data first server for
solving all our binaries so all the
customer EBS s going to disappear and
they became a V NFS virtual amount of
our ZFS did you compress EBS you're
basically now using solaris and all your
notes no so we wanted to use that FS so
the natural for the deficit so Louis and
I this is why I say that we had some
crazy guys they took freebsd and the
compiled ZFS on freebsd and we created a
freebsd instance ok that's the cherry
are at the fly off a span so we created
of PBS decompile instance with the good
version of ZFS okay and it works like a
charm the
only big big issue I don't know how many
of you put freebsd on amazon it's to put
freebsd on amazon you have to create a
windows machine go figure so that okay
you can see it's a freebsd EBS good
device it has to be a Windows platform
you need to press all the windows
platform no no but because the cost of a
linux platform on amazon compared to
windows platform on amazon it's between
2 to 2.5 depending of the kind of
instance you're taking ok so it costs a
lot ok the other big annoying thing
about Windows is that windows and
stances are limited to 16 volume only 16
to abide for us on all the difference ok
so it's really really annoying finally
they have I don't know who worked on it
but there is a really good gfs stable
version for linux now so you can take
the the linux version and so we're going
to migrate to the linux instance am i
and we're going to migrate all our ZFS
and gfs pool to the to the linux
instances for the moment this is how we
walk but what happened so i like you so
actually the amount of terabytes it so
it was only on one one data center one
region ok we have to mulligan we are in
singapore and we are in Ireland ok the
total amount of terabyte before we did
the ZFS was between 45 and 47 terabyte
of Amazon ideas that we had amazon that
other people we moved to ZFS ok with the
deduplication and compression with the
16 stuff ok and it went down to nine ten
no twelve 12 7 right now we migrated
everybody everybody so we went from 45
to 12 terabyte with the office no so
what we do we put 30 NFS so ZFS has how
many use of you used edifice a little
bit so they are z.z a spoon so you
create the first pool and so basically
you do an NFS mount so it's an nfsv4
mount on the same data center it's
actually really fast I was very ready
scared of the NFS mom one of the main
thing again is that the way we use the
file system we create the file we never
modify the file we read and sometimes we
did it and we don't care about phantom
wait ok if one of the system delete the
file and the other one still actually
reading it it doesn't really matter on
the NFS data it doesn't really matter
for ok what it means is that NFS is
quite bad at managing the meta data of
changing files whenever you are changing
a file on NFS system the way is
synchronizing the the fat during the
northern formation between the different
NFS mount is really really not
performant and it's very spending a lot
of energy since we are never doing that
we have actually of pure NFS we write
and then read and NFS I or inside the
data center with the freebsd walks very
vague so we have really good performance
of the stable so the NFS mode basically
grow automatically on the ZFS so it's
managing the zpool and creating one
terabyte and adding here we put on a
terabyte and terabyte of VBS directly on
the zpool and we just go the different
zipper so we have four machines like
that now four of those freebsd EBS s ok
and what we do now is that we need to do
a ZFS even based application so that's
the bowel way of saying that it doesn't
exist there is actually some some
different solution that we are looking
at the way we are doing it today is to
use the snapshotting the GFS
snapshotting and synchronization
for your information on the machine that
we have today the replication of the ZFS
16 terabytes from one data center to
another is using thirty to forty percent
of the CPU of both machines okay that's
most of the CPU of the machine is to
actually synchronize the data between
the them and what we saw is that we
think that it's because of the amount of
deduplication we are doing in
compression most of the CPU
synchronization is meta data information
about this unification and compression
okay it's not the actual by that are
sent from one to another words the
actual calculation of of the Delta and
of all those meta data synchronization
okay because we we are creating and
deleting I don't know something like
want a UH one tail all week I think 11
and a half day a week so so we can do
better so one of the other cool stuff
that son did I don't know that there is
loosely some of you use it this way okay
it's kind of a crazy study tip I really
wanted to look at it but it's how to
manage this kind of a huge huge file
system and distributed file system oh
yeah they also did java and anything
they have also added so I do passes on
also distributed the high-level file
system but it's for for small fire I
don't think it's solve so the one we are
looking at and we're going to start to
test and do do it on a staging
environment is dl DB some of you are
using dr d be some of you knows what it
is so so dl DB it's an open source
low-level system on you of a file system
actually on your file system block
low-level file system block access and
it's doing block complication ok so it's
replicating the file system blocks from
one enrollment to another okay so it's
basically allow you to replicate the old
zpool from one place to another okay so
all basically that's it
thank you will note your driving system
any question
so this is what we do with it being
played okay the question is that once
the file is in ice tree to put it in as
3s and conversely and once we know that
it's servable and it's any industry or
correct to start to solve it from
history instead of solving it from our
former abs this is what we are doing for
being quick ok so in bin play most of
the file are read mostly and so this is
exactly how being fair walks it's doing
it locally first and then once it's
available on the on the CDN and on the
global access it's basically like a
liking to the CDN environment for
artifactory online artifactory is really
your own server to do your own build as
fast as possible on your own local
system so it's it's a lot of small
exchange I don't think sp2 so s three
today is used for the backup and for the
backup of our customers so that we can
interact with our customers so that they
can download their data and that they
can also they want to do huge applaud
sometimes so they put a big bunch of
data that they want to upload inside of
crackling and so we use it s three as an
exchange platform this is many more will
love you
yeah so today we propose only island and
Virginia but there is a great chance
that we're going to open he also
California yeah if you take a dedicated
server you can choose whatever that
doesn't so if you go to cloud-based you
can go directly we are partner of club
is so the best ways to each create
automatically a server on the club beats
and club is there in Virginia so doesn't
he try to evaluate what orifice I'm not
familiar maybe it's not so capable yeah
I am NOT major initially so I'm
completely relying on the guys that did
the freebsd they talked to me about it
because I want the ldb and did they have
a reservation about the oddity
performance so we're going to start the
test of the different solution but the
better off a scam okay but yeah I hear
you I don't have yeah
yeah so well the baby it's the
performance of the EBS so we end with
leather fest the NFS the defers what it
allows actually its ETA lowers to put
the autumn cuts are actually way way
more performance today with the with the
audience because they don't have to
manage the file system itself so it's
the NFS cash the thing is that we did
this migration with the three dot zero
migration in a same time and the 320
like bow said is a huge huge performance
increase on the application itself so
it's kind of hard to know where controls
but today we are really happy about the
performance we max a for information we
max out the network card of most of our
instances so we went to the 10 gigabyte
network card of the high-level instance
and we cash the Amazon load balancer on
the on the amount of e note so they put
a special load balancer flowers and so I
anyway were quite good welcome thank you
yeah yeah
that's a good question if there will be
efficiency gain to do the deduplication
at a higher level I me the the the thing
about the the ZFS the the way is doing
is Delta and is any snapshot things and
and all this kind of higher level I mean
it for me it looks like I was really
really surprised by the performance okay
I really didn't expect what the ZFS gave
us and well today I don't want to spend
the money I mean that FS this way I
don't want to spend the developer of
time and energy to develop something
that is already provided by buy
something like this table if you had
asked me the question your honor you're
a year and a half ago I don't think you
will have been loosened today I don't
think I have zero regrets about using
delicious
yeah there is no there is a boundary an
artificial boundary about Phoebe sdb
non-windows that we're going to break
soon but that's the only one that FS has
no boundary where by the way we are
deployed for the Oracle middleware okay
all the work amid the world they are
using and so they have 20 terabyte now
and they are mounting so they have huge
huge amount of terabytes on the defense
I mean I know customer that had to
android phone what terabyte owns a
deficit model good no further thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>