<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Containers, Serverless and Functions in a Nutshell | Coder Coacher - Coaching Coders</title><meta content="Containers, Serverless and Functions in a Nutshell - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Containers, Serverless and Functions in a Nutshell</b></h2><h5 class="post__date">2018-04-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/PKifVQUKSMg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi everybody I believe everyone in this
room has heard about containers service
and functions basically that's why you
are here right this technologists are so
hot nowadays and they're being developed
so rapidly that the stream of
information is very wide and aggressive
and it is very easy to get lost in it I
made some efforts to put things in order
and to organize everything in my mind
and basically that's why I am here to
share my experience and to share my way
of thinking so Who am my name is Eugene
Fedorenko I'm Ukrainian so you can hear
my accent however recently I relocated
to the United States and joined flexible
perhaps some of you know me as a blogger
and follow my posts on ADF practice blog
probably someone follows me on Twitter
my position at flexagon is a senior
architect and i am leading development
of their fragment product this is flex
deploy which is a fully automated DevOps
solution and basically that's why these
technologies got into my focus and
that's why I am so interested in them
so this is our scope and this is what we
are going to talk about today we're
going to start with the idea of
containers and we'll see what it has to
do with micro services then we'll
discuss docker and kubernetes having
done that I will show you container
native development platform and finally
began a talk on service and functions so
make yourself comfortable and buckle up
and enjoy the journey and if you have
any questions please say them to them
just because of timing so let's start
from the very beginning from the idea
imagine an extremely simple scenario so
there is just a simple
standalone desktop java application no
database no middleware no services
nothing just like that a simple java
application obviously in order to get it
working
we need JVM we need operating system we
need libraries and drivers and etc so
our application our system depends on
that stuff and the entire solution
basically consists of two parts the
application on one hand and the
environment on another hand and just
because of that fact even in this
simplified scenario we might start
having some problems software
development life cycle involves northern
one environment at least two of them def
and prod so there is one application and
many environments which actually means
that there are many implementations of
the entire solution and what works on
development fails on prod just because
of different gbm's different operating
systems wireless or you know face of the
moon so the idea is to put everything
together the application and environment
dependencies into one package and
delivered across environments as a unit
as a container the idea is as old as the
computer science itself the only thing
is that it had been always hard to
implement it technically there has been
always a problem a huge overhead some
people have done that with virtual
machines it works but not that good
enough to become an industrial standard
to become a way to go for everyone
virtual machines are typically huge and
packaging delivering and starting a
virtual machine is usually a time
consuming and resource consuming process
fortunately some smart guys decided to
leverage Google's kernel features such
as name spacing and control groups to
create an isolation level on top of host
operating system or
than on Hardware this idea along with
the performance of modern hardware
brought to the world the notion of
containers as we know it today these
containers are way more lightweight than
virtual machines we can run thousands of
them on a computer and it won't even
blink there have been a few
implementations of this idea but today
the factor the docker is an industrial
standard and whenever we basically think
of containers we think of docker
ok now let's think a little about
slightly more complicated application
consisting of three layers database
middleware and UI even though this is a
multi-layered application it is still a
monolith in terms of functionality but
can we distribute this application just
like that
I mean can we distribute everything
we're in different clouds yes of course
we can we can deploy database to one
cloud and middleware to another cloud
and run UI on Prem why not
no containers so far but can we apply
containers here yes of course we can
first of all we can put everything into
a container why not one container for
for everything or we can put every
single air into a container which for a
real life application would make more
sense on the other hand we followed
another hot architecture trend and
decomposed our application vertically
into micro services so let's say this is
a huge core banking system and each
micro service represents a live line of
business so the first micro service is
about loans then the next one is about
deposits and the last micro service
takes care of foreign exchange
operations so what can we do here well
first of all we can put every micro
service into a container or we can put
every single air of a macros of a macro
service into a container which again for
a real life application
would make more sense but the real
question is why am i showing this stuff
this is not because I want to become a
Captain Obvious
this is because I want to underline I
want to highlight that micro services
and containers are totally independent
patterns and they serve for solving
different problems just because micro
services and containers work well
together they are pretty often solved
together by the conference speakers by
the bloggers as if it were the same
thing as if when it comes to containers
it's all about micro services and the
other way around no it doesn't work in
this way micro services is a different
level of abstraction this is just an
architecture pattern decomposing your
application vertically into a loosely
coupled subsystems that's it end of
story so just to quickly say it again we
can use containers with a monolithic
application and we can go with micro
services will without any containers at
all
so what think a little about evolution
in architecture patterns what we have
just discussed so at the very beginning
there was a simple standalone
application then we sliced it into
layers and distributed horizontally
deploying every single layer across in
the cloud and then we decided to that
that was not good enough and put
everything into containers why not
in a parallel branch of evolution we
decomposed our application into micro
services and distributed it vertically
so that one micro service is working on
one server another micro service is
working on another server then we
distributed every single layer of a
micro service across the globe in the
different clouds and on top of that we
could implement containers so that
either entire macro service is a
container or every single layer of a
macro service is a container and it is
now hard to predict without a crystal
ball but it looks like this way is going
to be the trend the technologies based
on the idea of containers are heated by
the fact that almost all serious players
on the market are making their
contribution into this area and they
provide their own projects and services
I just put some of them on the slide
some of them are commercial some of them
are open-source and the phenomenal part
of this is that almost all commercial
projects here are based on top of open
source ones so they depend on how
successful those open source projects
are considering this the Giants and not
only the Giants gathered together into
cloud native computing foundation this
is an organization a club of companies
contributing into container oriented
cloud services all these guys are pretty
interested in the success of the idea of
containers because they have been
there they have been investing there a
lot and the biggest whales in the ocean
of containers meaning that almost
everything is built on top of that our
doctor and kubernetes and whenever we
say docker we actually mean docker
container manager but there are also
docker hub and took a cloud that I would
also like to mention so docker docker is
a tool designed to make it easy to
create deploy and operate containers
once a docker container is created it is
guaranteed that it will walk on any
Linux or Windows machine regardless of
the customized settings that machine
might have which could differ from the
machine that we used to write and test
the code the only requirement for that
machine is to have docker container
manager installed this ability of docker
containers to run everywhere is one of
the greatest benefits of containers
along with isolation and encapsulating
environment dependencies into the
container in a way it is similar to Java
once a jail program is packaged it can
run everywhere on any machine having JVM
installed so that's true that for Java
guys this containers feature is not
something fantastic so they are used to
that docker is being developed by a
company who was at the same name and it
is distributed both as an open source
project and they are patched to the 0
license and as a commercial version as
well and in a way dr. is a little like a
virtual machine they have similar
resource allocation and isolation
benefits but they function differently
because containers virtualized at the
host operating system rather than the
hardware unlike a virtual machine
instead of creating an entire virtual
operating system docker allows contain
to use the same Linux kernel as of the
system where they're running on and it
requires applications to be shipped only
with those things that are not already
installed on that hosting computer this
gives a significant performance boost
and reduces the size of the applications
we can run multiple containers many of
them on the same host and all of them
will share the Linux kernel with other
containers and each of them will be
running as an isolated process in the
user is the username space so as a
result a docker or containers take much
less space than virtual machines and
they start almost instantly and if we go
up to the cloud we will see docker hub
over there docker hub is is a
cloud-based registry service which sells
as a centralized resource for docker
images there is a public registry which
is available for everyone for free and a
private one for money docker hub can be
configured in two ways the repositories
with the repositories we can push docker
images from our local docker engine to
the cloud to docker hub and pull them
back and automatic builds this guy links
to the source control management system
for example github and triggers docker
image build process once the source code
is changed so docker hub itself has a
kind of continuous integration server
capable of building docker images
according to a docker file a token file
is a set of instructions on how to build
a docker image docker command-line
interface provides access to the her
heart via the research pool logon and
push comments so we can operate with
docker hub from our local computer
docker company made step ahead in the
cloud infrastructure and they created a
docker cloud service on top of docker
hub so it inherit
all features from docker hub and
provides more it can build and test your
code it can build docker images but in a
bit more sophisticated way than docker
hub but the main advantage of docker
cloud service in my opinion is that it
serves as a facet hiding your
infrastructure nodes behind the scene
those nodes may be hosted somewhere else
in the cloud on Amazon rule digital
ocean or somewhere else so you pay them
for your computing resources but for you
this is total transparent as you
communicate with docker cloud providing
all container related and provisioning
services so when you're creating a
cluster in the docker cloud you have to
specify by whom this cluster is going to
be backed up who is going to provide the
nodes and you have to select a cloud
provider from the drop-down list can be
Amazon a true digital ocean software and
packet so having done that once the
nodes are set up you are able to deploy
and run your containers in the cluster
on docker cloud but physically those
containers will be running somewhere
else in the cloud on the rule Amazon
digitalocean etc we can operate who is
docker cloud from our local computer by
the means of dr. Clarke command-line
interface or by the means of API
available for rest course and for go and
Python programming languages ok so we
are almost in the middle of agenda are
you guys still with me good so let's go
ahead and have a look at another big
whale in the ocean of containers
this is kubernetes so once we have
decomposed our application into micro
services and put every single layer
every micro cells component into a
docker container we are getting quite a
bit of those containers and the number
of containers
tends to increase over the time so we
need something some kind of a tool to
manipulate and manage them just recently
people were relying on technologies such
as docker compose and docker swarms
pretending to do that for example docker
cloud is based on them but frankly today
no one is interested in those
technologies anymore seriously and it's
not worth even talking about that
Huber net is the factor has become an
industrial standard and the standard
tool for the job it has been officially
announced by probably all serious
players in this area on their market
that kubernetes is the way to go and all
of them are building their container
container native cloud services on top
of kubernetes so quickly Regis is an
open-source project originally born in
Google could the latest serves as an
orchestration engine focusing on how
operations staff deploy and operate
containers at scale on the other hand it
is also an abstraction layer hiding the
complexity of your infrastructure like
docker cloud so for you the answer to
the question where are your containers
running on is totally clear they are
running on kubernetes cluster what
virtual or physical machines on purim on
cloud are hidden behind these magic
words you don't care kubernetes cares
this is his job he Benitez provides
out-of-the-box such features as
configuration properties which makes
containers able to communicate with each
other and with the rest of the world it
also provides load balancing scalability
security and visibility in other words a
cool thing and let's have a look at
architecture Internet is so to
understand what it is all about the core
thing in kubernetes platform is a
cluster I believe we're all familiar
with the concept of cluster right so
this is just a set of physical or
virtual machines or in other words
in the terminology of kubernetes there
is a master node like adminserver
and worker nodes Oh millions like manage
servers minions have docker container
manager installed on them so they are
able to run docker containers and there
is also an internal process which is
called qulet this guy is running on both
master and worker nodes and it provides
a mechanism of communicating between
master and minions an access to the
kubernetes cluster from the outer world
is available on through the master node
so whenever we want to create or
configure resource in the Kuban in this
quarter we are talking to the master
node and we use for that one of
available channels it can be cubic ETL
which is kubernetes command-line
interface this is just a tool it can be
a REST API which basically provides the
same functionality as cubic ETL or it
can be a UI dashboard and what kind of
resources can we create on Cuba native
cluster so let's have a look at the most
important ones so there are there are
lots of them but just to focus on the
most important well first of all this is
a pot a pod is a set of containers a pod
is is the smallest deployable and
scalable unit so whenever we deploy even
a single container we deploy it in a pot
on the other hand if for some reason we
want some docker containers which are
tightly coupled to each other to be
deployed together to work closely to
each other we put them in the same pot
containers in the same port always walk
on the same hot containers in the same
port share are the same CPU memory and
disk resources containers in the same
pod share the same network name space so
they can address each other via
localhost
so consider port as a logical virtual
machine while the
imposing an application into containers
and considering how they should be
collocated in terms of deployment in
other words whatever you wanted to
deploy on the same host in pre container
times put in the same pod replica said
replica set is a cubist resource that
provides a policy on how many ports
should be alive so whenever we scale out
horizontally our application we do that
by multiplying pot meaning that all
containers in the pot are going to be
multiplied also and let's imagine
situation where we have middleware layer
in our application and that middleware
layer consists of a number of containers
so we put all of those containers into
one pot so this pot is our middleware
layer now we are telling you the latest
please keep three of them alive so we
are scaling out our application having
done that our front-end layer our UI has
no idea how to communicate with
middleware layer on one hand there are
three of them on the other hand hidden
ages doesn't guarantee that their IP
addresses will be stable as if a port
dies on one node humanities will arise
it up on another note in order to make
sure that there are always three of them
alive so what we're going to do is to
create another kubernetes resource which
is called service this guy has a stable
IP address and name so that front front
end layer you I can rely on that
furthermore service provides
out-of-the-box load balancing
functionality routing traffic between
the parts service can be an internal one
to be used only inside a cluster or it
can be an external service to be
accessible from the outer world
so usually external services are created
for front and layers of the applications
okay
since this is an Oracle conference and
since I have been mostly focused on
Oracle products for like all my wife I
would like to talk a little on what
progress Oracle has made in the area of
kubernetes and containers worker guys
are working nowadays on container native
application development platform this is
a set of cloud services to build deploy
and operate container native micro
services and service applications in
terms of continuous integration and
continuous deployment they provide these
components Oracle container pipelines is
basically a CI CD tool which is called
worker
it is SRS so it is available on the
cloud and the goal of this component is
to fetch the source from some source
control management system for example
github to build the code to build a
docker image to push that image to
docker registry and to deploy it to a
people in this cluster Oracle has its
own docker registry which is called
Oracle container registry it plays the
same role as docker hub just powered by
Oracle and the third component is Oracle
container engine this is a cloud-based
kubernetes platform where you can create
clusters and deploy and run your docker
containers all these components can be
used all together as it is shown on the
slide or they can be used to separate in
combination with other products and
other doctor registries in order to
build your own development platform for
example like in this case I am totally
satisfied with worker as deployed as a
building approach and I'm fine with
Oracle container registry but I would
like to enrich the deployment part I
have three environments Def destined
brought for example and
all those environments are represented
by kubernetes clusters in different
clouds and I have a tool a CI CD -
obviously I'm using fax deploy but it
can be any other tool implementing the
concept of continuous deployment
pipeline in all consisting of stages or
steps approval gates tests of the mesh
scheduled events manual checks and all
other benefits that continuous
deployment pipeline may consist of so I
configured worker to invoke Flex deploy
via REST API in order to deploy docker
image produced by worker to my
Copernicus cluster so in this way I
built my own development platform which
consists of two projects one part is
powered by Oracle one part by flexagon
ok and having done that let's think
again a little on evolution in
architecture patterns we can observe
evolution in many in various dimensions
for example let's have a look at
location at the very beginning
everything was on Prem then some people
decided to move totally to the cloud
then they thought that we took and
decided no it's not gonna work so for
sure something should work on Prem and
eventually they came to hybrid
infrastructure it's cool in one hand but
on the other hand it brings more
complexity talking of a platform at the
very beginning we deployed everything on
bare metal then we started doing
virtualization with virtual machines
today we have kind of super
virtualization with containers in all
good times there was a simple single
standalone application or a program then
we enjoyed client-server architecture
which on one day was announced as an
outdated one and retire architecture was
the way to go today we have
multi-layered applications like a
sandwich and we don't really care how
many layers are there
for ages coat and platform have been
separated from each other
for example Java program and JVM today
we put everything into a container and
deploy it across environments as a unit
so if you look at all the stuff which
you can see that the tendency is to
decompose everything into small
independent self running pieces of walk
that distributed across the globe in the
cloud and docker containers in my
opinion along with the platform where
they're running on I mean kubernetes
were like born to this goal I would like
to focus on one more dimension of
evolution I mean functionality if we
analyze all business applications or
business oriented enterprise
applications that have been developed so
far we can see that the majority of them
are just a fancy way of manipulating
data in the database tables no matter
what technologies used it's been the
same concept the same philosophy to give
the user a tool to input data and to
read it it has nothing to do with
automating the process it has nothing to
do with automating what the user
actually does only relatively recently
people started realizing that and today
there is a big movement from data
centric applications towards process
oriented architecture following the
concepts of this architecture we
consider our application or at least
part of it as a set of processes rather
than just a bunch of screens and what is
a process the process is a flow with
some steps and obviously every single
step can be implemented in its own way
using its own technologies taking into
account a tendency of continuation it
looks like containers is a perfect fit
for those steps furthermore if we run
these steps containers on a kubernetes
cluster we can forget about
infrastructure and focus on implementing
our steps of course in order to
orchestrate containers in this way they
should follow they should meet some
requirements so they should be able to
accept parameters and return the result
in other way work as functions so it all
sounds good and looks nice there are
just a couple of things to think about
every single step in this for every
single container is totally independent
from each other but on the other hand we
need someone or something orchestrating
the entire process taking care of the
state of the entire flow execution and
another thing is that even though we
don't care where our steps containers
are running on we still know that
they're running somewhere meaning that
they're consuming computing resources
all the time and we pay the bill and
here's where Sara Ellis comes to the
scene
several us is a paradigm that brings the
idea that a unit of work consumes the
computing resources only when it is used
so whenever a computer whenever a unit
of work is being invoked
a platform implementing severus arises
it up from ashes around it and kills
once the job is done in several US units
of work are considered as functions
meaning that they're totally stateless
they serve one goal they accept
parameters or arguments and return the
result those functions can be
orchestrated in the flow and a service
platform takes care of the flow
execution in terms of state transaction
management and handling
obviously this paradigm brings a number
of benefits you can put you often hear
that serverless is considered as a code
centric paradigm what it means it means
that we don't think of infrastructure
what we have is just a platform capable
of invoking our functions that's it we
don't care where are those functions are
being executed are being executed
exactly we can totally focus on our code
implementing our functions several
platforms usually provide out of the box
the scalability functionality meaning
that they rise up as many units of work
as it is needed according to the current
load the huge benefit of service is
about billing as long as we consume
computing resources only when we really
need them we don't buy error so we don't
pay for the idle time and another thing
is that we utilize Hardware way more
effectively we don't slice our servers
into virtual machines for different
types of work we can run many multiple
units of work on the same virtual
machine
do you guys know Lucas ulema from amis I
like very much what he said about
service there is no such thing as serous
computing
there is such a thing as dedicated
service computing that means that there
is no dedicated server constantly
dedicated to your application or
physical servers are hidden behind the
Saros platform there are a number of
available platforms today on the market
some of them are commercial some of them
are open-source the big girl was like
Amazon lambda and dual functions were
the first and they came in pre container
times so originally they were not based
on docker containers
however today are your founders and open
whiskey support containers the modern
open-source projects like a fan from
Oracle and open fast from Alex Ellis
they were originally based on docker
containers so in modern container
container native Seles paradigm
functions are implemented as docker
containers and having sent that function
is is a unit of work is a piece of work
wrapped into a container with everything
that it needs like code platform
libraries drivers and whatever this
concept brings you a feeling of real
freedom if you can build your function
floats on top of literally everything as
long as every single step can be put
into a container docker makes here
everything equal and unified no matter
what technology is working behind the
scene no matter what technology is put
in the container that one that fits the
best for implementing this particular
function and furthermore since in in
this case your functions are basically
docker containers you can run them on
any container native service platform
there is no any looking here at all and
talking of all generation platforms for
example Amazon lambda this guy was one
of the first on the market and
this guy is the current leader and the
idea is that you have your own code
written in one of support languages the
list of those languages is pretty long
however it is still limited and what is
more important the runtime the binaries
of the language are provided by Amazon
so it's not yours only the code is yes
so your code has to expose some api some
request handler which amazon lambda can
invoke then you upload your code to
amazon and it creates a function for you
then the framework listens to an event
for example a request to some URL
pattern and once it happens it allocates
resources for your code invokes the
function and releases the resources at
the end this also function flow feature
which orchestrates your functions in a
flow and it has a pretty nice visual
flow designer so it looks pretty nice
and works well the only thing is that
you are not a truly free man as in terms
of instruments to implement your
functions you are limited by what Amazon
lambda can provide you and let's have a
look at a fan a fan is an open source
project
powered by Oracle it consists of this
three main parts FN platform FN f DK in
the fan flow a firm platform is
basically a fan server and command-line
interface and again this is a container
native service platform so it is totally
based on docker containers and whenever
you are creating a function from your
source code written in one of supported
languages the platform behind-the-scene
creates a docker image capable of
running your code alternatively you can
provide a fan with your own custom
docker image to be used as a function
and whenever a function is invoked the
platform runs the docker stars of the
docker container from the docker image
and kills it when the US
dun fnf decay is a set of libraries
annotations for standard input/output
handling configurations and etc and the
list of supported languages by a fan is
pretty long but again you are always
free to go with your own docker
containers and this is very important as
you don't depend on anything this is
your container you can put inside it
whatever you want in that way that you
think is right and what you think is
right you can invoke a function either
with rest call over HTTP always
command-line interface and input
parameters and return values are handled
by the standard input/output mechanism
or by the means of payload FN flow is an
orchestration engine taking care of the
flow execution in terms of state
transaction management and error
handling Oracle guys didn't invent a
bicycle here and they built a fan flow
on completion stage Java API which
handles a synchronous calls a fan
basically is a very young project it was
released in October as far as I remember
so there is no such thing as a fancy
visual designer for flows this code
centric and whenever you want to create
your own flow you have to write some
code and that code looks like a bunch of
grapes
I mean function and function and
function and for me it's awesome so it
reminds me all good with one of my
favorites in the university there's also
a UI dashboard which represents function
and function flows executions and a fan
is an open source project which makes it
being developed pretty pretty rapidly
and I believe that soon in addition to
the awesome idea it will also look
and the brilliant thing about the fam is
this is just a bunch of docker
containers which means that you can run
it everywhere you just need a docker
container manager installed you can run
it locally having just basic docker
installation or you can run it in the
cloud on the kubernetes cluster this is
free country
okay now let's have a look at a simple
example of how functions can make this
world a little better I'm going to
implement an this simple C icd pipeline
consisting of two steps build and deploy
the build step builds an Oracle jet
application it builds a docker image
containing the jet application and a web
server pushes the image to docker hub
and returns of the name of the newly
created docker image the deploy step
accepts of that name alone was the name
of a kubernetes cluster and deploys the
image to the cluster every single step
is going to be implemented as a function
on top of docker image and the entire
pipeline is going to be implemented as
FN flow so I created docker image which
are called builder I put inside it git
client no GS and Oracle giacomo line
interface so it is it is able to build
Oracle Jetta applications it uses docker
hub automated builds functionality to
produce a docker image containing the
jet application and a web server and
having done that I created a fan
application and I created a function by
specifying a route between a request
pattern and my builder docker image so
whenever I invoke my event application
was this built pattern fan platform
out-of-the-box creates and runs my
builder container as a result it returns
the name of the new decorated in docker
image I created another docker image
which works as a deployer it has cubic
ETL installed it also has credentials to
all kubernetes clusters that I am
working with it also has a Yama file
which is basically a Cuban a test
deployment profile containing
instructions on how
an Oracle jet docker image to equivalent
is cluster and having done that I
created an event function by specifying
a route between the request pattern and
my deploy docker image so whenever I
invoke my offend function with this
request pattern it creates and runs the
deployer container deploying the Oracle
application to the target environment
the details on implementing this docker
image deploy docker image and the
function on top of that along with
builder local image I available on my
blog so if you're interested in that and
a function that serves as an
orchestration as a flow putting together
build and deploy steps could look like
this so the part one accepts an input
which is a name of the kubernetes
cluster where to deploy oracle jet
application and there's also a chain of
invocations of build and deploy
functions and the result of the build
function which is a name of the newly
created docker image with Oracle Java
cajon is going to be passed to the
deploy function along with the name of
the kubernetes cluster so when we deploy
this pipeline function to our fan
application the the platform behind the
scene creates a docker image with JVM
and with this code it also creates a
route between a request pattern and that
docker image so whenever we invoke our
offend application was this pipeline
pattern it invokes the flow it invokes
the pipeline okay
one my colleague said to me you know
this container stuff is a really cool
thing it's a black magic but how about
databases how does it fit there
well frankly in terms of database it
doesn't look that sunny and clear in
terms of divorce the containers approach
doesn't make much sense for the database
layer the debate has always required a
special treatment in terms of divorce
because it is stateful because data and
code live together and they can they
cannot be separated completely at least
not in Oracle database so we just can't
provide for production a container for
the database layer we have to apply
sequel scripts to it usually containers
work well for upper layers for stateless
layer layers like middleware and UI and
those layers communicate to the database
as with persistent external server and
how to communicate is configured on the
platform where those layers are working
on for example by the means of
communities configuration properties but
in order to be consistent in order to
elaborations to leverage such benefits
as scalability maintenance abstraction
from the infrastructure some admins
prefer to run databases in containers
and it makes sense and of course the
database works perfectly for test
automation scenarios and for sample
applications in the containers and just
a couple of thoughts on containers and
so on
remember like ten years ago we saw
slides who was many application
components on them and they were
connected to each other
and it looked like hell like total mess
then on the other on the next slide so
came on white horse who was a sort and
everything got organized very nicely
today containers along with kubernetes
whose functions
provide small loosely coupled services
and we can track and handle how those
services are being invoked we can
orchestrate them in flow so basically it
looks like slightly similar
functionality but on that hand this is
only one part of what so gives us and
therefore I don't really think that
continuous push saw out but probably
they just in rigid having said that I
think that containers fit pretty well
with saw a world the only thing is that
some accents accents on some features
may be shifted from saw towards
continuous and container clusters it
seems unbelievable but the theta so
thank you very much
I hope this session helped you with your
thoughts on comparison I have any
questions I'll try to give you some
answers until we asked to live
yeah
no VM is not docker container this this
slide shows the difference between
docker and VM so since VM virtualized on
top of hardware so you have just many
operating systems on one on one bare
metal and in when it comes to docker
this is the Korea yes yes every
container can be built on top of a boon
to Debian any kind of Alpine or any kind
of there should be something so we you
for example your kernel is I don't know
your kernel is Oracle Linux but you
installed your container your docker
container manager and that docker
container manager can around multiple
containers each single one of them can
be based on different operating system
running its its own operating system it
can be based on a boon to on debian same
Oracle Linux Alpine whatever sorry what
yes it's different versions but they
share are the same limb scale on because
Linux is this is basically
yes it's lumix family and windows so now
Windows is officially supported by
docker I can't say that I played a lot
with actual combination of Linux and
Windows since I prefer to but I played
the office combination of Mac and Linux
didn't get any any issues with that
oh yes this one what is
yep
I would defy the I would say in that way
that micro-service is a architecture
pattern they decompose your huge
monolithic 'el in terms of functionality
in terms of functionality application
vertically vertically meaning that each
single micro cells we may consist of all
layers that you need HC democracies may
have you are a middle way a middle way
to and etc and after database and each
single micro service is responsible for
that piece for big piece of
functionality inside your application
and of course in order to compose the
entire solution they have to communicate
to each other probably we have a macro
cells the cells as as a judge connecting
to each single one of them serving as a
bus and for sure each single micro cells
can expose an API
what kind of API in the pattern of micro
services doesn't dictate that it can be
a recipe i it can be you know P or
sequel API it can be whatever this is
how implement that and of course he has
docker series is very nice approach to
build micro services on top of them but
it's not necessarily this is what I
wanted to down the line it was pretty
often people think okay micro services
is all about containers
yes that's a very interesting question
this is what actually my last slide was
a little about that so now I would say
that functionality and that is little
overlapping because both of them to the
same stuff they orchestrate flows and
basically both of them share self the
main the main goal to put things in
order
why do we need people because we have
many many services and we need to
orchestrate them in the flow we can do
that without people actually so I would
go with actually function always
kubernetes and I wouldn't go with people
I don't like people
it's in my opinion it's way more
lightweight than people it's way more
lightweight I mean all these things are
why I like them they're they're so fast
and they're so light and when I have
some experience working with people it's
like a an elephant so it's totally
different you know development
experience working with people and
working with kubernetes well yes so my
simple example was about creating a
pipeline so again I'm working on a
product with 4x deploy and its main goal
is to build and deploy applications and
it builds and deployed occasions by the
means of some workflows and with the
workflow consists of the number of steps
for example fetch saw scores fetch
source code for example built for
example around some tests for example
around yo code against sonar cube
deploying your artifacts to Nexus and
etc every single step is basically can
accept just parameters and return the
result it is independent there they're
all independent from each other and I
don't need to eat them alive
I don't need to keep resources for the
entire flow I just need one resource for
one step at a single time it's done its
work for example around for example
building an application and build an
application is a very conscious
consuming thing for Java applications
could be and I want to start it up as a
function I want to spend some time and
some resources on that and once the job
is done I want to release the resources
and don't need it anymore I don't want
to pay for them so for me it's a good
example of how we can leverage the
service so when we have a flow process
so that's why I came to service
in this presentations through through
the process oriented architecture
through the flows so in my opinion
service gives the most of benefits when
you when you're implementing some
process some flow so that you need only
one single step running at one point of
time the rest can be done yes something
which also attempted talking about
mutually critical so basically if you go
with those details platforms many
several platforms actually consider that
as well for example FM they call it like
hot functions so you can define that
function but the full function will die
after each request so it will walk and
result result and it will die but you
can define a function as a hot one and
it will serve a bunch of requests a
stream of requests just in order in
order to keep that alive in terms of
performance yes yes
this one
this one
Oh
so the question is would it be possible
to link processes together
yes like a pipeline well yes again so
let's see we can talk about this
actually simple simple pipeline in this
case every single step is implemented as
a function and the pipeline is a
function flow but you know what what is
function function is basically it's
basically this code so every single
function can be basically be inside a
function flow invoking flow flow flow
flow flow flow so that's why that
actually the code and your imagination
about what's happening is like a bunch
of grapes so that's why it reminds me
Elizabeth so if you're familiar with
that I don't know so but yes it is
possible and basically what we are
walking now in Flags deploy we are
working on being able to implement flows
like that flows like that this one is a
builder flow and we are going to
implement that in that way that every
single step can be just a docker image
running on some docker container it can
be a function you know expose somewhere
on to various clusters in the cloud or
it can be a function flow so at the
entire step is a flow and that way you
will feel your real flexibility so you
can route build everything and on top of
everything because every step every
function is a docker container and what
the docker container does what languages
it uses it depends on you
yes yes yes yes basically this is one of
the main tasks goals of the several
platform many of them but the main in my
opinion is this is taking care of state
of the entire flow because you know
someone has to do that taking care of
error handling and taking care of
transaction management I mean the last
two things are related to each other so
in this simple exceptionally simple
example this and their error handling
work that exceptional but you can do
that at any point in your flow you can
catch that exception at any point in you
Qi and as a result you can make the
decision what to do next you can for
example invoke a compensating function
if you've got an exception at this point
and if you got an exception at this
point then invoke another compensating
party in that case you are implementing
your transaction management
yes yes this is out of the box basically
again a fan is built on top of
completion stage Java API and actually
even that guy already hasn't had
functionality but FM is just initiated a
little bit so all the stuff is possible
you can run functions not only in this
chain but you can run you know many of
them are synchronously in parallel then
you can wait for the execution then you
can combine their results and you can
again handle exception at any point in
your chain and invoke a compensating
photo so this is just out of the box you
don't need to invent a bicycle for that
I did
now that precious of Booker would be
yes you are talking about the thing that
docker container manager when you
download it from the website and install
it on your laptop from version 1.7 as
far as I remember in edge Edition
contains kubernetes inside it and
previous versions don't do that but it
doesn't mean that docker containers
built with previous versions of docker
cannot be island communities any docker
container can run on kubernetes it
doesn't mean with which version
well it means not very old but I don't
think that you're talking about that I
think that you're talking about that the
installation package of docker the
latest version of docker comes with
along with kubernetes yes that's true
and the previous version don't do that
but it doesn't mean that they won't work
on communities quarter so there are
probably some very old versions and I'm
not in ever done that won't walk on
kubernetes
problets very old
okay so thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>