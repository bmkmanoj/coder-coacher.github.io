<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Two-Tier Adaptive Heap Management for Oracle WebLogic Server on the Oracle JRockit JVM | Coder Coacher - Coaching Coders</title><meta content="Two-Tier Adaptive Heap Management for Oracle WebLogic Server on the Oracle JRockit JVM - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Two-Tier Adaptive Heap Management for Oracle WebLogic Server on the Oracle JRockit JVM</b></h2><h5 class="post__date">2015-06-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/eaQsKYJB5xQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Raphael ahad and my
co-authors name is Adriana Sanders he's
in the audience today and we are from
the Oracles product development IT and
today I'm going to talk about to do
adaptive heap management for oracle
weblogic server on oracle key rocket JVM
it's very specific so what I'll do first
is to give you a bit of background about
what PD ID is what we do in and and how
we came across this very specific
problem that we try to solve on top of
you know jrockit JVM and then I will go
into a little bit more rigorous
definition of the problem and then and
then talk a little bit about why
existing GC policy could not address at
least in our case based on our
experience trying to use all the api's
that we can address the issue that we
would like to address so we came up with
something that we call the two-tier
adaptive heat management and and first I
will give you an overview of it what it
is and then I'll talk about the generic
implementation using gmx bean right and
then a code deep into the actual
implementation that we did on top of
jrockit javeon right on J rocket J via
so then we will discuss the low program
that we wrote to test the feature of
this ahm MX being and we'll share the
results with you of what we find and
then I will have some concluding remarks
about this particular technology and
feel free to stop me any time if you
have any questions so the periodic the
product development IT we built
provision and maintain the key
infrastructure needed for the
development operations and the price
application
and cloud operations right so so we
operate huge data centers looks like
them and so we are actually the users of
Oracle products and technologies we
don't develop the products right we just
use the products and technologies that
the development teams develop right so
in this presentation what we will do is
to discuss an issue that we faced right
when we use the JVM heap sizing for
applications and how we are overkill
overcame this issue how we address this
issue and the approach I'm going to talk
about and the results of some testing of
that that approach essentially the
solution the this presentation does not
imply that Oracle recommends this
approach for all applications right or
that this technology will be implemented
in Jay rocket or any JVM right so what
I'm doing now is trying to talk about
the issue that we faced the solution
that we took right and the solution will
be deployed inside inside PD IT
environment and we are currently in the
process of testing it more in these
stage areas and things like that now the
the problem is the following actually we
have these servers with large CPU
capacity right and we run several JVMs
in our applications this particular
application that we are talking about
are the somewhat memory intensive
application they use a lot of Java he
right and occasionally we see spikes
come in either from the lobe or from the
number of users for some reason the
spikes happen they don't last long but
they can cause out of memory errors
right and then then we go through the
recovery cycle so there's the downtime
tends to increase a little bit and then
we were looking at it at the time and we
realized that the issue is really at
first we thought the issue really
is how to decide you know how much what
value to assign to your x MX or the
maximum heap size right then we realize
that if you did that though then you
would not be either you can get into
swapping or you can get into a situation
where you don't you cannot run run too
many JVMs without causing the swapping
right so eventually we will we came up
with a more rigorous definition of what
we really want to do so the in an
environment consisting of multiple JVMs
running on a host with a large cpu
capacity right so multiple JVM running
on a host machine which has a huge CPU
right capacity and each tvm grunts
memory intensive application who's he'd
use H varies with some spikes right so
some of you may relate this to the to
the issues you are facing you know if
you have some memory intensive
applications that you're trying to
deploy into into a weblogic server
running on top of any JVM you may have
these issues right for some applications
the question then is how do we minimize
out of memory errors and swapping while
at the same time maximizing the number
of JVMs or load that the host can run
right so now it is it is quite easy to
get two of those three right you can you
you can have a large number of JVM
running on a host right and and and let
swapping take place right and you can
you can set your max memory XM x value
to a very high value so that you never
run into ome but the result may be that
you may have a lot of swapping so much
so that the performance becomes almost
unusable right now so you can get any
two of those three but we want to get we
want to we have this optimization
problem where we want to minimize out of
memory and swapping while at the same
time maximizing the number of JV ends
that can be run on the host or
all the load by JDM that can be run
right so so so we naturally gravitated
towards looking at the memory usage
right because the CPU II was not
approved not a problem for us so we then
look at the GC policies and we realized
that today the GC policy uses only a
single policy for the whole heap right
so so for example you can minimize pause
time at the expense of compaction right
so so such as chain power and things
like that so you can the the policy
applies to the whole heap right and if
you minimize if you choose the policy to
minimize past time at the expense of
compaction then you can get slow heap
compaction right and basically that you
can see in GA rocket JVM it does compact
the heat and release memory back to the
OS but it does it slowly because it
doesn't want to jeopardize your boss
time requirement right so because it
does it slowly there is a higher
probability of swapping when he use H
spikes and you can also choose to
compact the heap at the expense of past
time but it's really used because you
want to control the past time right of
garbage collection so consequences of
minimize post time policy is that if you
set to Lomax heap size you get frequent
out of memory errors right if you set
too high max heap size you get swapping
and a few je viens or load per host
right so that's basically summarizes the
issue that we were facing right and so
what did we do to overcome them to solve
that issue so first we we sort of
divided the heap into two segments or
zones right the the first zone is the
normal tier 2 tier right so the normal
tier is from 0 to the soft limits so we
introduced this concept of soft limit
right soft limit is the size below the
MX value right below the max and if the
J
IM crosses a soft limit we take some
special action right so the normal so
the soft limit defines the tues oh right
the the the normal tier below the soft
limit and the red zone or the sheer tier
above the supplement so in the normal
tier which is below the soft limit you
use the normal garbage collection policy
for example gin / right but the special
tier or sometimes we call it shared
tears sometimes we call it high heat
tier and more commonly we call it as a
red zone that extends from the soft
limit all the way to the XM x value
which is the max value right and what we
do there is that ahm stands for adaptive
heat heat management that GC policy used
in the red zone after every not normal
ogc favors the heat compaction and
return of freed memory to the host right
so so long as your jvm is operating
below the soft limit there is no
difference actually your normal GC is
the one in charge and that you won't see
any difference at all right but once the
JVM crosses that soft limit and goes
into the red zone the GC policy is a
sort of we we we add another one on top
of it which is basically looking at
whether we should compress the compact
the heat and free the memory when and
how much memory to free is controlled by
parameters that I'll talk about a little
later in the presentation right then the
memory what we're trying to do is that
we don't want JVM to dwell too long in
the red zone so we want to free up its
memory return it back to the OS under
certain condition right and we become a
little bit more aggressive in that way
that way we are actually releasing the
memory back to the host so that other
jvms can use it when they spike right
without swapping without going to the
swamp so this is designed to accommodate
he use a spiked with the little or no
swapping right that's
the goal that we're trying to do right
so the x MX is still there and it has to
say say it same semantics even with ehm
in a enabled you can still run out of
memory right if you're he views is short
up above that x MX it will be a little
cause out of memory error now this
diagram shows the so for example the
green area is the normal tier the red
area is the specialty right in the green
area you will see that used heap size
somewhere in the middle and then the
committed heap size on top of it the the
member at the heap memory in between the
committee and the used is called a free
heap right and the one above committed
heap is not allocated to the JVM right
it's a prayer it's a it's the JDM can
use it but currently not committed to it
so this is then and then you have the
hard limit x MX on top and that SD the
absolute limit if you if the JVM needs
more memory than XM x value it will
throw out of memory error right and so
so the does in the specialty r which is
the red zone we will use the HMG see
policy in conjunction with the normal GC
policy right so now this diagram shows
where the we are in the red zone because
the committed heap size has crossed the
soft limit right and so because we are
in the red zone we will the red zone GC
policy will will be in effect right and
it gets at a red zone the GC policy it
is it's initiated after each ogc right
in the in there in the red zone so now
there are some before we talk about the
them itself I want to mention some of
the parameters these are the salient
parameters only we have when we talk
about implementation you will see that
you we have implemented this as a MX
being and it has many attributes you
know some of them are just read only for
monitoring purpose others are set to so
that you can control the policy you can
set the values for them this is just a
small subset of it which is important to
understand the actual algorithm pride as
we said earlier we have the max-heap
memory soft limit this is the soft limit
of the hard memory right hard limit in
this so like i said earlier so this
parameter or attribute controls what
your soft limit is right and then min
free heap ratios some of you who use
hotspot JVM will know some some of these
parameters exists in hotspot JVM earth
this is the minimum amount of heat
expressed as percentage of soft limit
that must be free under normal condition
so we want JVM to have a little bit of
some minimum amount of memory guaranteed
right under normal condition by the way
and so the men free heap size is just
the men free percentage times a soft
limit and then red zone release
threshold this is the number that says
that we we don't want to be too eager
and keep releasing the memory in the red
zone when we don't get too much memory
back as that can be released back to the
west so we want to control it so that do
your compaction only if certain amount
of memory can be freed back to the OS
right and so the red zone released
threshold ratio tells us that you know
the the the amount of the percentage of
the red zone size that must be freed up
by each hip compaction so we won't do
compaction unless we can free up that
much memory right the memory let's use
the variable men come back heap size
which gives you the absolute number
right the number of bytes that must be
it up in order to for compaction to be
done and host free memory threshold this
is the the the imax being that we
implement it also looks at the host free
memory before it makes a decision right
if the host free memory free memory at
the host level is below this threshold a
more aggressive action is taken by
looking at the JVM the other policies
which says that whether the JVM should
be killed or you allow a certain number
of gc's to happen in a red zone before
it should be killed gracefully right so
this is the variable who chased max
allowed GC and red zone the number of
old GCS allowed in the red zone before
the JVM is considered for termination so
that means that you can control quite a
bit saying that some jvms are allowed to
have several you know several og sees in
the red zone others are not allowed to
have that much so that you know if they
exceed that number we will create fully
terminate the JVM and furthermore we
actually will see a little bit later if
in case you need to terminate a JVM we
also have a way to see collect all the
diagnostic data and package it up into
something called the Oracle has a
technology called a dr so like an it'll
packaged up as an ADR incidence so that
you can figure out why it why it crashed
so let me give you an overview of the
two th and TC balls it's actually fairly
simple right the complexity comes then
when we implement it on top of G rocket
JVM because of a few things that I'll
mention also a little bit later so after
every ogc right if the JVMs committed
heap size is greater than soft limit
which means that JVM is in the red zone
right then compact the heap size to the
maximum of the soft limit or the
currently currently used memory plus
minimum free memory right so we still
want to leave
that minimum free memory for the JVM
right but we don't want to compress it
below soft limit also so this is saying
that okay whichever is larger compact a
JVM heap to that size and either's SL or
use T plus men precise whichever is
larger if you can free up the compaq
heap size bytes and return to the hose
alright so so essentially every time and
old GC happens in the red zone we test
this to see if it is the case i'm going
to compact it right to to that level now
after that if free memory including the
free able I or buffer so when we compute
the free memory at the host level we
also consider how much I owe buffer
space can be freedom right so if if that
is less than host memory threshold then
we are in a serious situation so we need
to take care of that if the used memory
is still under soft limit we will just
compact the heat to the soft limit and
let it be right it may go through many
young jeezy and OTC in there but we
don't want to kill it yet otherwise if
the number of GCS and the red zone has
exceeded the maximum allowed GC in the
red zone then we will create an ADR
incident if ever it is configured and we
will schedule a graceful kvm termination
right so that's that's the the gist of
this a solution approach right but we
will talk a little bit more about so
what does it do it allows that the three
things it does right the first thing is
it allows us to increase the X and makes
value the maximum value so that we can
avoid or minimize out of memory errors
when he be of age spikes due to an
unexpected load spite or regular but
short hi logs right so and then the
aggressive heap compaction and the red
zone minimizes swapping and allows the
other JVMs on the same host to use a
free memory right
and so and then finally it reduces the
and when we show these some statistics
of the tests that we did results of the
tests we did you will see also that the
committed heap size tends to be a little
bit lower than the case when you don't
use this hmm X me right and in resulting
and mo jvm czar load that can be run on
the host so how did we implement it
remember we are not in the product
development we cannot go and change the
product code right so what we did was to
look at as a user of this technology or
product what can we do from the user
level and so we decided we found that we
have this MX beam that a technology that
we can use so we developed an MX being a
application that that you can use any
ambient browser to define the soft limit
to find the the GC policy by setting
appropriate values for the MX being
attributes and monitor the red zone
behavior right so and essentially you
deployed this into your weblogic server
right running on top of Jai rocket JVM
although I have not been specific yet
right now about Jay rocket JVM I'll come
to that but you deployed into as an
application and then you use any ambien
browser to look at the attributes and
set them right however we also allow you
to set many of those attribute values
with a genie at the JVM startup time
right so you can also set them at start
a time in fact the easiest one that we
use is essentially default right the
default is essentially it takes the it
looks at MX where m x value and makes
the soft limit twenty percent below it
twenty percent yeah so so that we out of
the box by default right the values are
designed to take care of Otto memory
errors right so you what we did was
essentially if it was running with 1.6
gigabyte XM x value we start with two
about XM x value right and we enable ahm
so HMU will automatically create the
soft limit at 1.6 gigabyte so you don't
lose anything in terms of what what you
had before right because the soft limit
1.6 gigabyte is the same as your x MX
before right you should not see any
difference in the normal operating zone
right but but it allows the f to go
another up to 22 gigabyte right without
causing out of memory errors right and
based on your policy you can also
control how long it will stay there
right if it is not supposed to be there
for too long will you can terminate it
right based on the policy you define if
it is a if you want it this this this
application is important you want it to
stay for a long time you can control
that also right now so these are some
again this this is not a complete list
some of the attribute values that we put
right whether you want to enable a dr
incidents whether you want to the
important one i hope i have a point to
him the important wants us so for
example you will see one call the
max-heap memory soft limit right that's
the soft limit and by default its twenty
percent of the max right and then and
then there is another one called min
free heap ratio by default it's ten
percent of SL right so so those are the
attribute values that are important and
they say there's a lot of those that
control it and then host free memory
threshold is hundred megabyte right some
of those so when we bring up this MX
mean these are already defined with the
default values and you can change them
there are times in which some of these
like heat block enabled and all you will
and you'll know only later because this
is a very j rocket specific so now we're
going to talk about okay this we talked
about the concept that we want to do we
talked about generally how we want to
implement it from the user level right
using mxb now we will talk about
specifically what we did to imply
mint and hmm X being on top of weblogic
server running on top of che rocket JVM
right so what we saw was that India
rocket JVM one week when we increase the
X ms8 XM x value remember earlier we
said you were running with 1.6 gigabyte
of XM x and we increase it to two
gigabyte right and we said the soft
limit to 1.6 gigabyte what happened was
that all of a sudden derogate JVM
started grabbing more memory right so
it's a it when x-mas is a x MX is
increased the committed heap size also
increases for the same application right
so it grabs more memory and keeps it in
free memory heap right so we don't want
that to happen because it can change the
behavior and the G but another good
thing about the g rocket JVM and that is
why we were able to implement this
solution on top of it is that the
garbage collection am being in it
provides two important methods api's for
us right one is the sub set heap size
lock to true and what it does is that it
takes the current committed heap size
and makes it max right so so the JVM
thinks that's the MX value x MX right
and then the next one the most important
one is the set allocated heap size
target to X that means you want to
compact the heap memory do because it's
a target size target it doesn't actually
come back exactly to the eggs that you
specify it may be X plus or minus Delta
right there's another complication there
because if you are comparing if you're
trying to compare with X equal to X you
might not get that true right so because
of these two we were able to do some
adjustment for the rocket heap
allocation policy and that is we will
try to log the heat at the lock the heat
at soft limit
when the committed heap size approaches
soft limit right to control the hip
growth we don't want to grab more memory
beyond the soft limit right and
therefore we try to force it to c SF
soft limit was the max right by setting
that log to true and then unlock it
unlock it to expand beyond the soft
limit or to allow committed memory to go
below the soft limit remember jay
doggett JVM will eventually come compact
the memory and create free of the memory
back to the OS only thing is it doesn't
slowly and we try to do it a little bit
more aggressively in the red zone so now
we're getting down into quite a bit of
detail the it's just designed to say
that what we do is after each ogc if the
heap size is already logged right and
and the committed heap size is less than
free memory or committed heap size is
greater than max free memory then we'll
unlock the heat because we want it to
grow beyond soft limit or let JVM reduce
a committed heap size below whatever it
is right now now otherwise if the heap
size is not locked and the committed
heap size is greater than soft limit you
know minus Delta and things like that
are there because as I said earlier you
don't you if you set the allocated heap
size to the soft limit you might not get
it at soft limit you might get a plus or
minus off limit that's why those little
deltas are there if the committed heap
size is greater than soft limit then you
are in the red zone do the red zone
action right and I'll show the red zone
action a little bit later otherwise we
try to look at whether the whether the
we should set it to soft limit right at
the current committed memory right if it
is close to its a committed is close to
the soft limit we might set it to the
soft limit
now the red zone action is interesting
one so like I said earlier it first
compute the it first computes the target
heap size which is the max of use T plus
cadman free or soft limit right exactly
like I said earlier now if the committed
heap size minus target heap size is
greater than release he threshold that
is can I release at least a release our
heat threshold in this current situation
right because I know that the I have to
go to the target level and with it from
the current committed to the target if I
can free up enough memory i'm going to
free that up by calling that set
allocated heap size target do the target
heap size and after that i still test if
the host free memory is greater than
threshold host free memory is yeah
greater than threshold you don't do
nothing right because if the host free
memory is below threshold then you start
looking at if the use heap size is
created and soft limit right then you
will schedule and if the number of GC in
the red zone has already exceeded the
max allowed in the red zone max GC
allowed in the red zone then you
schedule a shutdown right otherwise
compact the heap if it has not been
shrunk yet so down to the soft limit
right now so that's that's the algorithm
it's not actually not very complicated
right the only complication the first
slide that we use show that we showed is
because of jrockit JVM behavior of
grabbing more memory when I'm accessory
increase that's why the first part is
there now so so we wanted to test this
right and we we tested it in many
different environments but the one that
I've mentioned here is the
we we run a test application that that
test application runs a number of
threads each consuming from six megabyte
to 10 megabyte heat memory so they vary
from six to ten each thread right so in
that test environment is a single host
with eight gigabyte of RAM and running a
weblogic admin server and for managed
servers and each managed server is
started with x MX of 1812 megabyte and
soft limit of one for 50 megabyte why
did we choose these odd numbers because
we wanted to study the behavior of the
application close to the soft limit and
going back and forth near there how it
is behaving so and then at the start you
will see that there's about 300
megabytes of free memory in each gvm I'm
sorry three hundred megabyte of used
memory in each JVM so we run 145 threats
/ JVM so that means that he bo CH varies
from 870 megabyte 21 11.45 gigabyte 14
50 megabyte for each JVM right now the
test is run for 30 minutes and we done
it four times first we don't we don't
enable the aah MMX bean that's called
that's what we refer to as no ehm that
means hmm X bean is disabled it's not in
the picture and then we have the load
with spike and no spike so what I said
earlier about this 145 threats each
using up six megabyte to 10 megabyte is
the normal load right in addition in one
test case we will inject a 300 megabytes
spike into into the run into that
environment right occasionally it turns
out about four or five times during the
30 minutes we will inject that 300
megabytes spike because we want to
understand also how it handles it right
so we have then four cases in five
the first case is no ahm with no spike
and the second case is no ehm would
spike the third case is hm-hm is enabled
and we have no spike and a chimes
enabled and we have spiked so four
different configurations were run so
observation is that substantially lower
the OC post I'm all collection pastime
right that means that essentially it
reflects back into the response time of
the application where the response by
response time I mean only how much time
it takes to allocate the memory that we
want to get right so us by using the ehh
and we will we will we can lower the
total pastime old collection post I'm
actually for that matter total post I'm
including old collection plus young
collection right even if you add the two
you still get you still come out ahead
by using hm and that also shows up in
the improvement in the response time
right and for the case of Noah hm no hm
if we reduce the XM x 2 1 450 we get
auto memory errors right so that means
that you cannot leave it at one point 1
450 for this application you get into
auto memory error and if you increase it
right 22 1812 megabyte right in the case
of hm you get less swapping right less
better better response time les hace
pastime in the case of know each and get
higher values for those for those
measures so this is the test that we did
notice too that the number of OC right
the count for OC goes up the first one
the OC post time if you look at the post
time the red one of course is the one
without a hm right red bars are no no
ehm the green bars are the one with ATM
right so the first the first set of bars
right
for these four jvms you will see that
the post time is quite a bit different
right between between no ehm and ehm no
no H and post time is quite high total
post time right but interesting thing is
even even though ahm is doing more Oh
see more more old collection right
because every time you try to compact
the heap it actually does a DC right and
so the GC count goes up but because we
avoid swapping it actually turns out to
be that we gain in terms of past time
essentially we lower the post time right
so if you add up the young post time and
all post time we are still below the
case where a hmm is not used in the last
and this column where the allocated time
is given you can see the difference
right this is measured just by those
applications that are trying to grab
memory how long does it take for them to
get that memory right to increase so
essentially you will see that allocate
time can be thought of as a response
time for this and they are quite quite
low compared to know hm and if I look at
what we did was we also run vmstat go
ahead oh the number of times young
generation was involved the counters are
the number of times they invoke and the
post time is the total total post the
time for them young generation yeah this
is what turns out to be the case that
young generation increases because I
think we have a little bit less free
memory than others in the a.m case
because we compact it down a little bit
too close to you even though we we still
observe the ten percent I mean yeah yeah
it's not expensive at all you will see
that so this is this is the swap
activity as measured from vmstat right
because I wanted to make sure that i
will look at the holistic
picture right what is happening at the
at the host level and so the swap in
activity you can see that we have only
about average of 66 whereas if we don't
use Ahn you have about 638 right and
then swap out activity you will see also
that if we have hm we have 246 without
HM we have 780 right this is the rate by
the way right so what we are showing
then is that in both measures we the HM
seems to come ahead come out ahead right
for this particular application I must
always remind everyone that we looking
at this somewhat memory intensive
application right and it may or may not
apply to other applications right in
particular CPU intensive applications
may not benefit from it at all right so
now this time around we start putting
the 3 300 megabytes pike all right let
me go for what a little bit so that I'll
come back to this one so you can see the
spikes are showing up in the both swap
activity for both right oh by the way I
forgot to mention that in this one the
the if you look at the no hm case much
of the time it is hugging the zero line
it's there's not much of swap happening
the only thing is OS is doing something
behind you know maybe it's pushing out
the admin server to the swap dates and
things like that so some of those things
that we don't control is happening at
the waist level right and so this is the
300-member swap activity you see all
those spikes here that they correspond
to the to the spike in the memory
allocation right so now we still ok we
still ahead of it but everyone suffers
now because the allocate time has gone
up to 16 seconds two seconds two seconds
two seconds but without hm they even
worse 13 seconds seven seconds right 26
seconds 39 seconds right so
in both cases whether there is a normal
load or whether there is a spike we come
out ahead in terms of total OC of total
pastime as well as the response time
right and and we avoid out of memory
errors so what when are we going to use
how are we going to use this hm cinar a
time right so we have to use cases one
is essentially to minimize occasional
out of memory error so we will deploy hm
MX being in the weblogic server on G
rocket JVM it doesn't work on top of a
hot spot right because hot spot doesn't
have hot spot JVM doesn't have those two
methods that we talked about and we
start weblogic server with twenty five
percent higher value for x MX right so
for example 2 gigabyte instead of 1.6
gigabyte right and then default
attribute values are designed to
minimize out of memory and swap so you
basically deploy it enabled it and
that's it right oh no no you deployed
and it will it start your weblogic start
your jvm with x MX twenty percent
twenty-five percent higher than what you
have before right because by the time we
set it to twenty percent below that MX
for the soft limit it will be just right
for it will just go inside with the XM x
value before hms used right now to
increase load where possible I don't
know whether it's possible right
sometimes it may be possible if you look
at it you see lots of free memory and
the JVM lots of time right and you you
are tempted to say why Cannot I use some
of those free memory to to run one more
JVM or to run more load right things
like that so if you want to lope
increase the load by increasing the JVM
or increasing the load on each JVM you
can start with soft limits set to the
working set size by working set size i
means that you compute essentially i'm
going to have a certain number of users
certain number of load i will need
I will need about let's say one gigabyte
always right so that's your working set
size and then set the x MX to quite high
to avoid over me it doesn't have to be
twenty percent or twenty-five percent
above you can say that Oh sometimes I
spike like crazy I can put it up fifty
percent more right so you can put it
there and adjust your hmm X being
attributes accordingly there are lots of
them like like let me see right that you
can use to control the behavior of the
ahm MX mean hm attribute values can be
set and one entered with Mission Control
enterprise manager or any am being
drowsy and they can also be used in
weblogic diagnostic framework watch
rules and it's an it's an M veena that
at the end of the day right you can see
it from there and you can you can create
watch rules from it all so so so in
conclusion what I'd like to say is that
like I said earlier this solution is
this presentation doesn't mean Oracle is
endorsing the solution for all kinds of
applications right it doesn't also mean
that this technology will find its way
into the future garbage collection for
any JVM right however we are going to
use internally at Oracle and what we are
saying is that we don't need to use age
and if we have you know if we run if we
run if we run n n gbms each consuming up
to M maximum amount of memory on a host
whose whose Ram is greater than M times
n right that means you're running let's
say for je viens each take one gigabyte
you have four gigabyte you have six
gigabytes of memory at the OS level in
the host you don't need the HM right you
okay you will need a gem if you want to
run more JVMs or you want to avoid over
me and things like that right so we
could benefit from hm if eight is less
than M times n so you want to run right
n je viens approximately each consuming
m
maximum heat memory but when you
multiply M by n you that it is greater
than the available heat memory RM in the
host right then then it may be
beneficial for you to run it right and
the application may have load spikes
raising the heap size to em but usually
it will be in usually it will be working
at the working set size of W which is
less than M right so you know that in
normal condition it's not going to use
up n but if the old spike maybe use up n
times n right so and then we want to
avoid out of memory errors and swapping
swapping Nia's performance degradation
essentially then we set the soft limit
to w + XM x 2m and configure some swap
space right so that's all I have we have
about maybe 10 minutes for questions and
answers
Oh
yeah and you want us to throttle the
load when when you're getting our good
idea i think but we it's not in the
scope right now we don't want troubling
is a good idea and it's a good idea to
this can initiate it because it knows
it's going to go out of emory yeah yeah
but but the devil is in the details how
do you throttle is the issue that we
need to grapple with I think it's a good
idea but I don't know I it's not we have
not thought about it yet I don't know we
can kill some threats if you go if
that's possible right or we can shut
down some connections from somewhere
right question any more question
so we by the way I forgot to mention
that we also deployed this in the stage
environment of our big applications
where the heat goes up to full full
gigabyte right so at that time though we
were trying to make sure that there is
no negative adverse effect of this
solution so we deploy it and we watch
the normal load right and so we were
never exercised to the point where we
want we went into soft limit we went
into the red zone so that's why we have
not uploaded in the production because
we want to increase the load in the
stage area to observe what happened but
to answer your question it was for
gigabyte Max's 4 Giga bedroom that's in
the host level but by the je viens
running up to 4 gigabyte in heap size
in this case they are running different
applications so they don't they are not
running the same application so you have
jvms and so they may be they may be in a
cluster doing load balancing and the
cluster right but what we do is from the
host level and from the JVM level we're
looking at which JVM is escrow is
operating in the red zone and whether we
have enough memory in the house to let
him go there to let him stay there good
question because it was what we were
using right on top of our fusion
applications you're a fusion
applications and deployed in the cloud
is using gear rocket eviane right now
right and therefore our problem is with
jerica divyam the our problem is to
solve it on top of jrockit JVM and
luckily we found those two two methods
that we used right and I don't know if
those two methods are there in hotspot
JVM but by the way we also talking to
the the JVM development team they know
they know about this technology and we
will look into that also
you
any more question well thank you very
much for attending I know it's the last
day of the conference</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>