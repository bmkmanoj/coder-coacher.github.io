<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Apache HBase 101: How HBase Can Help You Build Scalable, Distributed Java Applications | Coder Coacher - Coaching Coders</title><meta content="Apache HBase 101: How HBase Can Help You Build Scalable, Distributed Java Applications - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Apache HBase 101: How HBase Can Help You Build Scalable, Distributed Java Applications</b></h2><h5 class="post__date">2015-06-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/KZps2dzr_u4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay good morning everyone my name is
Alex Schulman today I'm going to be
talking about building HBase
applications so before we get started
let me give a brief intro by myself so I
work at Cloudera as a software engineer
I've been working on HBase for about two
years since August 2012 I've been
writing tests and test automation and
currently I'm doing a lot of work on AWS
on Hadoop and if you're interested in
kind of things that I'm working on you
can follow me at a underscore schulman
or you can email me at Alex at Claire
comm a little bit about cloud era so
cloud era distributes Apache Hadoop we
have our own distribution called CDH
which stands for the cloud era
distribution of Apache Hadoop and that
includes HBase which is a topic of
today's talk Clara also distributes a
tool called cloud our manager which is a
proprietary management and monitoring
tool for CDH makes it easier to
administer a cluster so if you have five
nodes it's pretty easy to do things
without a visual interface but if you
scale up to 500 you're going to want to
know a little bit more about your
cluster and see what's what's going on
but Clara's roots are in open source so
we employ over 50 committers across
community representing over 84 committer
ships in fact more than a dozen
top-level Apache project started at
Cloudera
and we also employed Doug Doug cutting
who is one of the originators of of
Apache Hadoop at Yahoo so in today's
talk will be talking about building
applications against HBase but before we
begin I want to quickly scan the room
and see who here has heard of HBase
before ok it's a good start
who here has used HBase ok who here has
written applications against HBase very
cool and who here has either filed a bug
or contributed code to the Apache HBase
project a couple folks ok very cool so
what is Apache HBase it is a distributed
column family oriented datastore it's
made for storing lots and lots of data
so in the mid-2000s Google had a problem
they needed to index the entire web and
the current tools available weren't
really cutting it so they decided to
build out their own system their own
distributed system for analyzing
everything that was available on the web
so they built out a distributed file
system and then they just built out a
distributed framework for analyzing what
is on that file system and then they
built out in abstraction to the data in
that file system that represented a
table so all that work was cataloged in
their papers and one of the most
important papers they wrote it's called
the BigTable paper and that's what HBase
is based on so HBase is known for being
very highly scalable so we've seen
customers have data on the order of
petabytes it's known for being low
access sorry low latency and favoring
random access and it's also known for
favoring consistency over availability
HBase is also non relational and I'll go
into that why that's important in in a
few slides when we talk about how it
relates to a typical our DBMS HBase is
built on top of HDFS which is another
open source project called the Hadoop
distributed file system and I'll talk a
little bit more about that and also the
licensing is Apache open source so if
you're interested in the project you can
pull down the source code you can run it
and we're always interested in getting
new contributors and committers so if
you're interested in contributing let me
know and talk to me after the talk so
HBase is fairly popular and it's used
extensively so here's kind of our nascar
slide kind of the who's who of who
contributes to HBase and also some
customers who basically use HBase for
the mission critical workloads so in
today's talk I'm going to start off kind
of pretty high-level I'm going to talk
about the ecosystem kind of where HBase
fits into everything I'm going to talk
about how it works and how it's
architected and that will give us more
context as to what makes sense for HBase
to be used for so I'll also talk about
to build your first HBase app and I
promise you one slide with a complete
application and then we'll look at HBase
use-cases
so like any technology HBase is good for
some things maybe not optimized for
other things and I'm going to give you a
framework to analyze what applications
you can write on it so that when it's
time to write your application you can
determine if it's a good fit or not and
then finally once you've written your
application you want to consider what
happens when you sort of have the whole
administrative lifecycle so when you
upgrade your cluster what your
applications still continue to work very
important questions and then there's
time at the end for questions
okay so brief intro to HBase so HBase is
a distributed key-value store so HBase
takes key values and represents them in
the form of a table
so like any table you'll have rows and
columns so for example here's a table of
contributors to various projects in
Apache so we have two rows in this table
we have row for Doug cutting and a row
for Todd lip con and let's say we want
to find out a piece of information about
mr. cutting
for example his height so in order to do
that we need to construct the key in
order to get a particular value so the
key would be first to go so the way
construct the key is you first isolate
the row and the way you isolate the row
is via a row key so the row key here is
cutting so once we have the row now we
look for the column that we're
interested in to get the cell so in
HBase columns aren't exactly columns
there is a notion of a column family and
a column qualifier so in this case info
is the column family and the column
qualifier is height also within a given
cell we can have multiple pieces of data
that are time-stamped so typically when
you read the piece of data by default
when you read a piece of data you get
the latest value but you can also get
previously written values so if we get
the rope
the Roky the column family column
qualifier and the latest timestamp as
key we can get the value which is 9 feet
so let's take a look at another example
let's look at the second row todd lip
con so we want to find out what his role
is on HBase so we go to the Todd Lipton
tea loop conne row key and we look at
the roles column family and then we look
at the HBase column qualifier and we get
looking at the latest timestamp the
value we get is committer so let's
formalize this so again the primary key
is like an index in our DBMS terms the
columns have a notion of a column family
and a column qualifier so the reason we
make this distinction is to separate the
physical storage notion from the logical
notion so when you define a schema for
an HBase table you only need to specify
the column family the column qualifier
is something that comes in as you insert
data so when you insert data you can
actually specify the qualifier and that
way you can have logical columns that
are created at kind of run time instead
of at creation time so all data itself
is stored as a byte array so it's up to
the developer to understand what the
data means once it's pulled down so
there's no notion of a VAR car there's
no notion of a decimal it's really up to
the developer to code that logic in
finally we also have the notion of a
sparse table so because it's just key
values it's certainly possible for a key
and a value to not be present and so
that way we can have sparse tables also
as I mentioned before different
different time stamps are available for
a given cell so if you want to record
some history on a given value we can do
that so why is this important well when
you write your application and you're
looking to get the last n values the the
read pattern can be optimized based on
this the schema so you can say I want to
store the last five values for a given
cell and
so when you are typically looking for
the last five values you only have to
read one cell so let's review what a row
looks like in HBase so every row has a
primary key those primary keys when
they're stored are lexicographically
sorted and that's for to optimize a
random read use case there's a timestamp
associated with each piece of data and
of course each row is made up of columns
and the intersection of a row and a
column it is referred to a cell and the
data in a cell is byte array once you
pull it down
of course the apps must know how to
handle them and I think really important
point here that didn't touch on before
is that rows are strongly consistent so
if you need to acid properties within
your application for example if you need
rights to be written in a very specific
order you can use a given row to do that
there are note there's no
transactionality between rows and also
there's none between tables so that's a
key distinction between HBase and a
traditional relational database
management system okay so let's say we
have a table just a logical grid and
I've grouped some of the columns into
column families with column qualifiers
so how do we actually turn this logical
representation of a table into data on
on a hard drive so the first thing we do
is we group by rows so contiguous rows
form things called regions and then
regions are subdivided by column
families so once we do that we can
actually store it so when I when I
talked about each base being a column
family oriented datastore if you look at
any single file it only contains data
from a single column family and then
within each file the data is stored in
Lex graphical order so it's not like a
columnar store it's sort of a hybrid of
the two
okay so big picture we've seen how the
the logical representation of a table is
turned in to files but where are those
files stored
so HBase relies on something called HDFS
which is the dupe distributed file
system and HBase also relies on
something called zookeeper so zookeeper
is a tool for coordination within the
system and it serves as a source of
truth for data that's really important
to the system so HBase relies in
zookeeper and HDFS and I'll go into them
in a couple slides and then the HBase
applications in turn rely on HBase so
what is zookeeper I don't want to spend
too much time on this but basically
there's a project called Apache
zookeeper it's open source and the
reason this project exists is because
coordination is notoriously difficult to
get right and so instead of every
project having to re-implement it it's
easier to reuse it so to keep your
stores small amounts of data and it
stores it in a file system like way one
of the things that zookeeper is used for
is for leader election so within the
distributed system you need a place
where there's sort of a source of truth
and so if your applications are looking
for for example what the HBase master is
then they need to get that data in a
reliable way
and zookeeper serves that purpose among
others HDFS is a distributed file system
so HBase stores its H files in HDFS and
it's stored hierarchical and it's also
fault tolerant with high availability so
that's important in a distributed system
so what does HBase look like so HBase
runs on a master/slave pattern so HBase
has two types of processes there's the
master and there's the region server so
when we talked about a table being
broken up into regions the reason that's
done is because we want different
machines and different processes
responsible for serving different
regions so for example if we have a
table it's broken up into five regions
if the table is correctly balanced then
each of those regions will be served by
different region server so the master on
the other hand is the one that deals
with metadata so for table creation
table deletion
snapshots things like that the master is
the is a daemon that takes care of it of
course the master in and of itself is
kind of a single point of failure if it
goes down you can't create any more
tables that's a bad thing so you can run
multiple masters and by doing that and
relying on zookeeper to keep track
basically we can have high availability
in HBase so what happens is all of these
all these processes they're always doing
something called a heart beating to
zookeeper
so they're constantly sending a ping to
zookeeper letting zookeeper know that
they're alive and so if at any point if
there's a network partition then
zookeeper will say I believe you are no
longer live so I will move regions off
from you on to different region servers
or in the case of the master if the
master stops heart beating zookeeper
says ok I don't believe that your longer
available so every other master process
that's trying to heartbeat
I will elect one of them to be the new
master and then any request that I get
for the master location will forward the
new address not the old one so this is
really important in cases of let's say
rolling upgrade where you're going to
have to shut down the master at some
point and so this is how you keep HBase
available and serving data while you are
doing an upgrade okay so let's start
some data so let's say we have four
tables some tables are bigger than
others and hence have more regions so
table one will need three regions to
store all its data where is Table four
we'll just need one region then there's
also this fifth table called meta
so what could meta possibly be used for
well suppose you're interested in a
piece of data right and that data lives
in a region and that region is served by
a region server so if you need to find
the region server to talk to to get a
piece of data that kind of sounds like a
key value problem so the key being the
region and the value being the machine
to talk to so HBase is really good at
storing key value pairs so meta is just
another table
and it can be stored on any region
server so for example it's on region
server - but if region server - becomes
unavailable meta has moved to a
different region server and so the
question becomes how do we find meta and
the way we do that is the location of
meta is tracked in zookeeper and so
typical clients interaction would start
with zookeeper to find meta and then
once once the information in meta is
cached you can find where any piece of
data lives in HBase so then how does
HBase relate to a typical RDB yes its
server-side that's a great question so
going back to this slide I forgot to
mention that zookeeper is run as a
zookeeper is run as a set of processes
so we recommend an odd number and so if
one were to go down the data is still
available on the others who keeper nodes
we typically recommend an odd number so
that is possible to form a majority so
if there's conflicting information you
can have a quorum good question by the
way
that's a good question the question is
how can you how does region server
compared to the data that it's storing
and does is there a compute component to
it and so forth
so each of these processes is run on its
own machine so for example let's say I
had a cluster of five machines so the
backup master would be running on the
first one the master be running on the
second and sorry six machines and then
each of the four regions servers will be
running on the respective machines so
each of the region's servers has access
to its own memory its own processing its
own disk and so forth and then to add
one more thing to it data is stored in
the Hadoop distributed file system and
and so what we have is on each of these
machines we also have a data node and so
that data node also stores data and when
you have a piece of data in HDFS it's
actually written out three times so what
happens is that although you have one
region server responsible for storing a
piece of data or sorry you have one
region server responsible for serving a
piece of data that data is actually
available on three different machines
okay so now that we have some idea of
how HBase works how can we think about
it in relation to and our DBMS for in
terms of how data is laid out for those
of you familiar with our DBMS is you
know it's structured and row oriented
HBase is semi structured in the sense
that you can create a schema at you know
when you create the table but you can
also logically add to that schema when
you add more data hence it's really good
for data that's semi structured or data
that's kind of evolving in terms of the
schema again within our DBMS you can
define a create time but you do a lot of
planning on the front end and it's a
little bit painful to add or modify that
data after it's it's kind of set versus
HBase is a little bit more forgiving in
that regard transactions so in our DBMS
this is where our DBMS is
really shine so of course you have acid
semantics and transactions across the
database for HBase you only get those
guarantees across a single row so you
can use that to your advantage if you
need aspects of it in your application
in terms of querying so trishal a
traditional RDBMS is very good at
processing complex queries in sequel
which is universally accepted and
adopted and used HBase has a simpler
query language so we have the notion of
some primitives for like a get a put a
scan in increment so basically if I want
a particular key value I can ask HBase I
can perform a get operation so give me
this key I'd like this value
same thing with but I'd like to put this
key in this location this key value in
this location and that that's how it
works
so you can combine these different
simple operations to create a more
complex data language but generally we
don't have sequel as a part of HBase
proper however there are some projects
smokin' source projects that add a
sequel layer on top of HBase one of them
is Apache Phoenix which is sorry not
Apache Phoenix just Phoenix and that's
being created at salesforce.com and
there's another one called chef odeon
it's relatively new
it's from HP and there are some other
also sequel on HBase projects available
in terms of security definitely
important for your data so HBase just
like an odbms has authentication
authorization so for authentication we
use Kerberos and for authorization we
have a standard set of ACLs and right
now new work is being done to add ACLs
on a cell level in terms of indexing so
in our DBMS we'll have an index on on
any arbitrary column in HBase we only
have row key index there is work being
done on secondary indices and so so that
in terms of the data size this is where
HBase really shines so traditional RDBMS
will top
on the order of terabytes but HBase can
store on the order of petabytes of data
and we've seen that in production with
our customers and then in terms of
getting data in and out if you can store
a lot of data you need to access a lot
of that data quickly so within our DBMS
you can have thousands of sequel queries
a second HBase we've seen millions of
simpler basic queries as well per second
so perhaps you've heard of other no
sequel data stores such as Cassandra or
Mongo so HBase what makes HBase a little
bit different is that in situations
where there is a network partition there
is necessarily a tension between having
consistency or having the data being
available and so in those cases HBase
favors strict consistency over being
available but what we've seen is that
availability is very good also HBase was
designed to was designed kind of from
the ground up to work on top of HDFS and
to integrate with MapReduce so there's
great Hadoop integration and then in
terms of how it stores data we do an
ordered range partitions so we have key
values LexA graphically stored on disk
and then it emits automatically scales
and shards so if you add more region
servers you can store more data so if
you have twice the amount of data you
just need twice the amount of hardware
and everything should be fine and of
course HBase is sparse storage so we
looked at tables with key value pairs
missing HBase supports that okay
so we've gone through what HBase looks
like and how it works so how can you use
it for writing clients so there's a
fundamental tension between the amount
of data that you can get into and out of
HBase and the latency so let's start at
the top so if you use the standard HBase
client you can you can have very simple
quick operations but they don't return
very much data on the other hand you can
use MapReduce if you need to do things
like full table scans and of course with
MapReduce that system has its own
latencies and so forth but you can up
with quite a bit of data at that point
there's also HBase replication so HBase
replication is a tool for business data
recovery where you basically write data
to a particular you write data to a
particular cluster or set of clusters as
it's written to the original cluster and
that data can also be further replicated
out to other clusters but it's it's sort
of like a like a lazy right so it's
about two to three seconds usually
behind but that's also way of getting
data into and out of HBase so if you're
writing your application you're going to
want to pick an API that's right for you
so there's the Java API which is sort of
the workhorse it's the one that we see
90% of the time being used there's also
the restaurant rift API is which are
remarkably similar so the Java API is
full-featured it's the one where we add
all the new calls to but because it's
constantly evolving there is a risk that
you can have incompatibilities so for
example if you write your application
against you know HBase 92 and then you
upgrade to 94 some of these calls may
have changed a little bit and in subtle
ways and that might break your
application requiring rewrite or
recompile I'll go into an example of
that towards the end of the presentation
some of these are kind of subtle so the
Java API all it needs is access to
zookeeper you just need to tell it
here's where zookeeper lives and it
figures out the rest versus the rest and
thrift API is so everyone here is
familiar with the REST API one of the
great things about it is it's stateless
you just form a request and send it to a
location it is slower and it does
require its own process to run and I'll
go into that in the next slide
thrift API is so the thrift thrift is a
set of bindings and so different
languages have different thrift bindings
so basically thrift is a contract I
expect this data of this type in this
format and if I pose a request this way
I will get a response and then I'll
parse it in my own language of choice
but yeah lightweight stateless so if you
want to start
working with the rest api it's actually
very simple all you have to do is pull
down HBase and run the start command
give it a port it defaults to its own
port if you don't want if you don't do
that and then you just send requests
using your favorite HTTP client to that
endpoint and the the API definitions are
noted in the link below if you're
curious v API also very similar way to
get started so you have to start the
daemon on some node that's on the on the
cluster and then you just send requests
to that endpoint and the endpoints are
defined in the HBase type thrift file
which is in the HBase project but since
this is Java one I think that most
people are interested in looking at how
HBase applications will Java HBase
applications work so I've what I've done
is I've kind of broken down the
complexity of a typical HBase
application and I've also added the kind
of the cluster topology here so we have
our usual suspects HBase zookeeper and
HDFS and then I've also added MapReduce
and yarn because most people who are
working with HBase are also interested
in working with MapReduce so if we look
at the HBase application which is on the
top half of the screen we see that
there's a component called the HBase
client so the HBase client includes
within it clients for talking to
zookeeper HDFS and MapReduce and so all
you have to do is is import the HBase
client into your application and then
you have all the bindings for talking to
HBase and the associated systems your
application code then will be compiled
against the HBase client and against any
dependencies that your client depends on
so there can be some trouble here
because there are a few different moving
parts and they're independent of each
other so what we try to do at Cloudera
is we try we invest a lot of time energy
into testing the api compatibility of
different versions that we ship and I'll
go into that little bit later but let's
suppose you want to write your first
HBase application
how would you go about doing it it's
actually very straightforward so HBase
relies on maven for build and test and
so it would be a logical good choice to
use maven for your application
so you have a total of two dependencies
you need Hadoop and you need HBase on
your on your on your build path and then
your application will correspond with
that so I'm going to go through it line
by line so first thing you need to do is
you need to implement you need to import
the configuration all this does is it
tells your application where to look for
the key value pairs that correspond to
the configuration and that's typically
that should already be on your path so
that's your just kind of work out of the
box you have to import the HBase
libraries in HBase client and of course
for exception handling you need to
import exception ok so very basic client
we start off at the top so we give the
table a name so in this case it's
arbitrary I'm also giving it a timestamp
at the end so that we can run this
multiple times without any kind of
collision and then you also have to
define at least one column family for
your table this one is called my column
family ok so now let's use the HBase
client to actually create a new table so
we load up the configuration and then we
create something called an H table
descriptor and that is just a logical
way of reasoning about what your table
looks like and the each table descriptor
will take just your table name and then
we can add the family to it which is on
the next row
finally my admin dot create table will
actually interact with zookeeper which
will which will interact with master to
actually create your table on the system
and to kind of verify that things are
working well what we do then is we get a
list of tables and we go through them
and we just print them out so when you
run this application every time you run
it you should get one more table coming
out does that make sense I can stop here
for questions
the configuration file so typically a
configuration file will be just just a
minimal XML file it's stored in the
default location as Etsy Hadoop conf and
what it will do is and also at CH base
conf and both of those should be on your
path and so what that will do is it will
tell you the location of the file system
and location of zookeeper there's a
minimum set of properties that you need
to define there I think if you run your
application locally like on your cluster
I think it should work by default
because it'll work against your local
file system and then it'll be listening
on the local port yes
that's a yeah that's a good question so
the question is if you're running the
client remotely like let's say from your
laptop you're trying to talk to your
cluster which is in a data center the
question is do you need to somehow get
the XML onto your laptop the answer is
yes you do so it is basically two files
that you need to put on the path so I
can repeat that yes those are the files
mm-hmm and then HDFS site and then
coresight the client actually just needs
to talk to zookeeper
actually that's not sure the client
needs to talk to both zookeeper and all
the region servers and so you need to
adjust your firewall settings
accordingly sorry the question was does
the does your client need access to all
the machines in the cluster and the
answer is yes yes
the question was if you're using
Kerberos are there any issues importing
the XML to the client the configuration
itself is so getting the configuration
to the client is kind of an out-of-band
process the client just assumes it's
there but when you run with Kerberos you
have to there are a few configuration
changes that you need so for example so
you definitely need to tell it where the
KDC lives that's going to be in the
system configuration and then you also
need to tell it where your key tab is or
BK and needed yeah any other questions
okay I'll move on ok so then in order to
run this app so a few of you brought up
the point that you do need your
configuration on the path so if you're
running it remotely if you're running it
locally you actually don't need to don't
need to do anything for the
configuration you just pull down HBase
pull down your favorite version then you
start HBase and then it's going to be
using the local disk and it's going to
be listening locally and then you just
build and run your application pretty
simple I would recommend doing this in
IntelliJ if you do it just from the
command line and a text editor you do
have to move some files around to get
things to work properly but generally
this is a very vanilla case and it works
works well so I encourage all of you to
try this at home yes that's a good
question so the question was does this
include the installation of zookeeper
when you run HBase locally in this way
what happens is that it starts one
process and that process includes
zookeeper and HBase so include zookeeper
and HBase master and a region server
yeah slides will be available and you
can definitely come talk to me after
after the talk so you'll have a chance
to explore the Java API but I just want
to kind of give you a high-level lay of
the land as to what you're looking for
so Java API includes something called
the
figuration which we went over it's just
an object that sort of keep key value
pairs based on the XML files that are on
your path so there's that there's also
the data definition language and the
data modification language so the data
definition language as we looked at
before in our example would be the like
the H table descriptor and the admin and
then the data modification language
would be things like increments what's
deletes actually don't have any of those
in my example but they're relatively
straightforward all these operations are
atomic except multi put and the reason
multi put is not atomic is because it
operates on multiple rows and we don't
guarantee atomicity there in terms of
getting data out the primitives are
relatively straightforward scan get and
filter so you're not going to have full
sequel here you're not going to be able
to replicate everything you can do in
sequel but using scans and gets and then
applying filters which are relatively
sophisticated you can get you can get
relatively sophisticated queries okay so
yes I know
so HBase takes care of that for you the
question was for boots do you have to
care about locks okay so one of the main
reasons people choose HBase is because
it works so well with MapReduce so it's
designed to work on top of Hadoop and
that's why the integration is so
straightforward so if you want to have
MapReduce titute to your client
application you just have to add one
more dependency I should note that there
are two versions of MapReduce MRV one
and MRV - so MapReduce one is the
original sort of MapReduce there's no
there's no notion of resource management
there recently in the open-source
community there been efforts to add
something called yarn which is called
yet another resource negotiator and so
now HBase sorry now MapReduce can be an
application on top of yarn and they're
they're QoS guarantees there now so
things like CPU and
Marie are more fairly distributed
between different jobs that are run by
different folks and so this this helps
the multi-tenancy story of hadoop but as
a result we now have two different
versions of MapReduce MRV one and MRV
two and it's pretty straightforward to
kind of work with that you just have to
add the dependency that matters to you
so there's an HBase dependency called
HBase server which is kind of strange to
bundle something called HBase server
with your client application but that's
where the map the MapReduce bindings lip
and then you also need to add the
MapReduce bindings themselves which are
in either Hadoop core from our v1 or in
Hadoop MapReduce client job point for
yarn and then when you modify your
application you just have to have the
right import so dot map read for mr v1
dot MapReduce for mr v2 there are
generally API compatible so anything you
write on mr v1 is guaranteed to work on
MRV to the reverse is not quite true
it's generally true but there are some
exceptions and so those are noted in the
link below
ok so now that we have an understanding
of how we can go about building
applications we should ask what we can
build and what's appropriate so the next
the next set of slides are adapted from
a talk by my coworker John Shea he gave
them at HBase Khan but it's an hour long
talk and I'm going to try to do it
justice in 15 minutes but what I'm going
to do is I'm going to give you a
framework for thinking about what HBase
is good at and not so good at and then
trying to look at your problem as the
right questions and then understand
whether whether your problem is good fit
for HBase but before we begin you know
people ask well what system is best
it depends right so what we found is
typically applications are i/o bound and
you generally get 100 apps per second
per disk you have a limited i/o budget
and you can be smart about it by
parallelizing your rights and you know
doing sequential read writes so you
minimize seek on disk however you can
also win by having less i/o and you need
to pick the best system and tool for
your workload then we have to think
about what's being stored so there are
primarily two kinds of big data
workloads we think about things like
entities or we can things think about
things like events so so what are what
are entities so an entity is something
that has a real world equivalent right
so for example I have a fleet of
aircraft right so each aircraft is a row
in my HBase table right or I have a set
of accounts or users or I have some
sensors somewhere right but basically
each item has sort of real-world
equivalent and it's a row in the
database and here we're talking about
something on the order of billions of
distinct entities so events are
different right events aren't bounded by
the world they're not bounded by physics
so you can have as many events as you
like so so you have to ask you know are
you storing event data or are you
storing entity data and also you have to
ask why am I even using the system right
am i looking to have an operational data
store where I'm constantly getting
requests and then writing out responses
small bits of data or am I using this as
an analytical store so that I can
perform analytics in the background
asynchronously and that so those
questions will definitely inform your
use of HBase also how are you getting
data into and out of HBase is it coming
in from from flume or some other tool is
it coming in in batch mode or real-time
mode is it coming in one at a time like
you know a few clicks for a given user
or is it machine data and then also is
this data being written sequentially or
randomly and that also depends on your
schema right so if your schema is for
example if your row key is like
timestamps right then your sequential
data is going to be sequential
if you hash your or if you salt your
timestamps then it's going to be random
okay so event centric data so examples
like if you're pulling in data from the
stock market right so you're constantly
getting stock ticks if you're looking
for historical metrics or if you're
looking at users performing operations
on your website and you're doing a click
stream or if you have some sensors like
for example if I have a set of aircraft
and there's a sensor in each engine
right I'm constantly getting readings
well you know the aircraft is is flying
right but also even if you so even if I
don't add a single sensor but I change
the frequency with which data is
collected I can really increase the
amount of data that I get and also keep
in mind with event center data data is
always coming in so I don't have to
build the single additional aircraft but
I still get data okay so what happens
when you have a lot of events sorry when
you have a lot of entities and you're
also collecting time-based data on those
entities so events on those on those
entities then you get a lot of data and
so at that point you have to ask what am
I really interested in am i interested
about entities or my interested about
what happens to entities so entity
centric questions I'd like to know
everything about this particular entity
so I have a set of I have a set of
customers tell me what their you know
contact info is right so that's a pretty
straightforward question or I'm
collecting I'm at getting data or I have
data about about what happens on a
particular event and I would like to
collect it but I only like to collect a
finite amount that's an entity centric
question or I'd like to scan a
particular range of time for a given
entity and find out what has happened
versus event event and time sensor
questions are more about aggregations
they're more about full table scans or
large table scans
so for example I'd like to compute an
aggregate or I'd like to compute an
aggregate of aggregates so so or like
within the entire set of events that
have happened find some events and match
criteria right so two very different
sets of questions so a clutter we've
been we've been helping customers build
HBase applications for years now and
we've seen customers who are successful
and some less successful and what we
found is that successful applications
fall into certain patterns right and so
it's really helpful to think about these
patterns as archetypes so if you're
familiar with the design patterns the
Gang of Four design patterns this is
very similar so we can kind of say well
you know my application is kind of
similar to a Facebook message store and
that's a good application on HBase so so
that's that's a way that we can reason
about it so what is a good use case for
HBase simple entities I want to store
information about the world okay
messaging store so like like a tumblr
feed or like like the Facebook use case
a graph store so the original use case
for HBase and BigTable was indexing the
internet which is basically a big graph
or a metric store like I'm collecting a
discrete set of metrics about a
particular entity things that HBase is
probably not intended for large blobs
so things that are greater than three
gigabytes don't really belong in HBase
the reason for that is because HBase is
doing a lot of a lot of like data
writing and data modification in the
backend in order to make your data
readable quickly and so there's always
writes going on
not only are there writes to the file
system so remember when you write a
single piece of data you're writing it
in three locations depending on your
configuration so that's something to
consider and so if you're constantly
writing and rewriting these huge files
you're just going to crush your right
path HBase also does things called
compactions so at some point in time
when files get on we
or we have a large number of files we
compact those files into one and four
constantly rewriting large files that's
going to be very painful it's going to
increase the latency of the system
overall so you don't want to use it for
that looking at a naive database port so
if you have an existing set of tables
and if you translate them one to one
it's not going to work out the way you
think it's probably not a good idea
in part because HBase doesn't have
transactions and so you're going to have
to re-implement a lot of logic in the
application layer so looking at so
another example would be an analytic
archive so an analytic archive is
storing kind of everything that happens
all the time
and I'm doing full table scans on this
data to look for trends and patterns
HBase isn't really good at that if you
like doing single individual like scans
or or gets but things that are good at
full table scans are MapReduce so
MapReduce is designed kind of for that
use case so the kind of the canonical
example is a row counter right I want
know how many rows are on a table I look
at every row so so I guess the lesson
here is it's important to use the right
tool for the right task right and so for
kind of short amounts like random reads
random writes HBase is great if you're
doing longer table scans may want to
look at another technology or you may
want to change your approach to use
MapReduce and of course maybe so a time
series database is kind of falls in that
maybe category also if you have a lot of
good workloads but they're all running
on the same cluster that could really
stress your cluster and so it may turn
some use cases into not good ones
sure so the question was can you talk a
little bit about graphs tour I don't
have too much context into into the
graph store but maybe if you have a
specific question I can answer
that's that's it's very possible I don't
I don't have too much context on on
graph use cases of HBase yes that's
right
so the question was we're writing in
three places is that related to HBase or
is that related to HDFS so that's
related to the underlying file system
HDFS the question is is that
configurable by default the standard
replication factor on HDFS is three but
it is configurable what we've seen is
actually going above three isn't
particularly useful but yeah depending
on depending on your use case you may
not need full data fidelity so if you
want to set it to one it's definitely an
option
that that's a good question so let me
see if I understand it so DV need to be
written in three places and or n places
and it's possible that at some point
there's a failure along the path and so
how has that managed so there's a
high-level explanation and a low-level
explanation the high-level explanation
in terms of the HPC use case is that we
just get that for free right so the
folks that developed HDFS they said let
us take care of that your data will be
stored
don't don't think too hard about the
problem but but knowing how HDFS works
HDFS works by having a write pipeline
and so what happens is you say my
replication factor is 3 I would like to
write my data onto three separate nodes
and so the right is only returned
successful once all three notes have
been written
okay okay so the question is in terms of
data protection so we're writing on a
certain set of nodes and let's say one
of those nodes goes down and we have to
write on a different node are there any
security implications in terms of access
control in terms of that so I'll request
two HDFS go through well they go through
HDFS and HDFS has its own unix-like
security parameters and so where the
data is stored is is is really unrelated
to the access control underneath it
basically all it just says is if my
request is valid then I'll proceed to
look for the data wherever it may live
does that answer your question okay yes
sir
that's a good question the question is
is replication available on a
finer-grained basis than just the entire
file system like for example can you
have replication on one particular table
so in HDFS they don't make the
distinction so it's either you have this
level of replication or you don't with
in HBase we do have the notion of HBase
replication which I talked about earlier
and that sends data to a different
cluster versus HDFS will send data to
different nodes but on the same cluster
so you can set up replication on a table
by table basis yes
okay I think I understood the question
the question was if full table scan is
not implemented and you have a
particular time range how do you access
it is is a question so so in HBase you
can certainly scan a full table and to
end you're welcome to do that it's just
going to be a little bit slow if you're
constantly doing just sets of scans so
we typically recommend MapReduce for
that
okay I'm having a little trouble
understanding your question maybe we can
talk after sorry that's right
good question so the question is time
series DB is implemented on top of HBase
and so in that case well why do we
consider it maybe so time series DB it's
not it's not a naive implementation so
they've done optimizations that make it
work well for HBase so for example what
happens is if you just add time series
data to a table like one after the other
after the other you're going to get
something called hot spotting so
basically because data is stored within
a within a given file lexicographically
and your timestamps are ordered LexA
graphically because that's the timestamp
you're going to have all the writes
happening on one particular region on
one particular machine and the rest of
your machines are going to be sitting
idle right and that's a problem so
you're going to want to basically find a
more intelligent way to manage those
rights and that's what they manage to do
great question by the way okay so
looking at a kind of like a good
application for HBase would be like a
Facebook message store so you have small
amounts of random reads and writes your
data tends to be you know small bavaria
and you know it's somewhat time-series
but it's mostly random read writes and
so if you want to design something like
this your row might be the name of the
feed or the name of the user or the
inbox then you Roky would be the user ID
or the user ID and then a timestamp if
you want to optimize for indexing you
can use your column families as as
indices and then you call them qualifier
would be the time or the conversation ID
then plus the time so where has this
been used Facebook is the most notable
example
Shami messages telcos uses for SMS and
MMS and then also you can also think of
feeds like Tumblr and Pinterest as well
as very similar
use cases so how much data are we
talking about well as of 2012 Facebook
was reading seven billion messages a day
which which constitutes 75 billion read
and write operations per day it was
mixed read write roughly half and half
and then the total amount of data just
raw data is 2 petabytes and of course
when you put it on to a file system like
HDFS it's going to be 6 petabytes of
data so a lot of data a lot of messages
and that's in 2012 so you can imagine
numbers a little bit bigger now ok so
what does HBase not good at so preface
this by saying that we put in a lot of a
lot of thought a lot of forked into
HBase
but of course no system is perfect some
optimized in one direction some do it in
others but that's also not to say that
you can't do a lot of things in HBase
there's quite a number of things you can
do but you may hit some issues if you
scale to the order of petabytes using it
you may be fine that lower scales and of
course if you're doing a lot of sort of
yes or good use cases but you're running
them concurrently that could pose
difficulties for your system but
actually that's something that we're
working on one of our engineers mateo is
working on QoS for HBase that'll make
reads and writes more fair and prevent
starvation of requests that said you
know we're continuously working on HBase
and so if you are use cases and great
today it might might be good in the
future ok so let's look at a bad example
so a native our DBMS port so if you do a
one-to-one copy of tables if you have
your column qualifiers being field names
and you try to do management of joins
and secondary indices in your
application layer it's going to be tough
I mean number one you're reinventing the
wheel excuse me and number two yeah so
you're going to suffer from from lack of
relationality and it's going to be
difficult so the best thing to realize
that HBase is not a sequel database we
don't have transactions and you can get
around this in part by denormalizing
your data but do you realize that you
will have duplication as a result
okay so in summary if there's one thing
that I want you to get from today's talk
is that random-access short reads are
really good for your age base so storing
small amounts of data queering small
amounts of data we're really good at
that we're also good at storing lots and
lots of data over time we integrate well
with Hadoop and we scale horizontally
well so if you need your request to take
half as long you need to add twice as
many machines also HBase is fairly
popular in the community and in industry
so if you have HBase expertise there are
definitely people who want to hire you
so but HBase is not good for full table
scans and aggregations so generally what
we recommend if you're doing a lot of
these use cases you may want to like run
a workload to pre aggregate your data
and then put that data into HBase
reasoning about the aggregates is is
easier than doing the full table scans
each time also multirole multi table we
don't offer that and of course large
blobs so we are actually working on the
large blob use case that's something
that we working with Intel on so you
should see that in an upcoming release
okay so see how we're doing on time okay
we're just about out of time so I'm
going to walk really quickly through
this so application life cycle upstream
we do our best to maintain compatibility
and then in CDH we test very heavily to
make sure that compatibility is enforced
so when you write your application you
don't need to recompile or rewrite it
when you upgrade but those guarantees
are bounded within the blue boxes so for
example going from CDH 4.1 to 4.5 and
onward you should be perfectly fine
there's nothing you need to change if
you move to CDH five you will have to
recompile and maybe even rewrite parts
of your application of course we
documented and we deprecated ap is where
appropriate but that's something you
should be aware of data compatibility is
a different story you don't sort of want
your data to not be readable we're very
keen on making sure your data is
readable so in our testing we make sure
that as we test CDH we also end up
testing the version of Apache Hadoop
Apache HBase that is based on
and so when we do upgrade testing from
three to four to five we also end up
testing the upgrade path from 90 to 90
to 94 and to 96 98 so 96 and 98 are both
NC dh5 okay so one of the cool things we
do we also support rolling upgrade so if
you need your cluster to be up while
you're upgrading like if you're running
a mission-critical workload it's
important so there are some scripts in
Apache to do this within CDH we also do
testing via clutter manager and so
that's only available between minor
versions major versions you will have to
shut down your cluster and that's also
true for for the upstream version as
well okay so that's the end of my
prepared remarks if you have any
questions you can talk to me after the
presentation today or you can follow me
at twitter or send me an email okay
thank you I'll open up for questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>