<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CON8342 - Improving the Performance of Your Java Application: Getting Beyond the Basics | Coder Coacher - Coaching Coders</title><meta content="CON8342 - Improving the Performance of Your Java Application: Getting Beyond the Basics - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>CON8342 - Improving the Performance of Your Java Application: Getting Beyond the Basics</b></h2><h5 class="post__date">2015-12-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/WwTT5xUa2J4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi let me get started my name is Marty
its Goods I'm the project lead for the
Oracle studio performance analyzer my
colleague UConn Yama is the deputy lead
and we're gonna talk today about Java
profiling and how to get the most out of
profiling Java applications next time
okay so next time so first of all what's
a performance problem there are lots of
things that could be performance
problems one is that job simply can't
handle the load it's required to handle
another is that it uses too many
resources in order to do its work and
whenever you see a problem you have to
ask yourself is it worth fixing you have
to make sure that the benefits you get
from fixing it is worth the cost it
takes to do it
there are also subjective criteria as to
whether if something is good enough if
it does it take too long to finish does
it seem to take too much time to respond
and as I said before you need to look at
the cost of fixing versus the aggregate
cost of the problem one observation I've
made in too many years of programming is
that most untuned codes have low hanging
fruit you can often get factors of 3 or
4 just quickly looking next slide so if
you have a performance problem or you
think you do there are lots of different
ways to triage it to look at what's
going on you can use bin time or frankly
a stopwatch or the wall clock even to
see if there are problems in order to
understand it you need to be able to do
repeatable runs you need to make sure
that each time you run it you're doing
the same thing so you can understand the
progress you make in tuning you also
want to make sure that when you do
collect data when you do a run that
you're using for profiling that you do
it at a realistic scale with realistic
data you don't want to get into the
problem where you've done your profiling
measurements on something with
has ten operations but in general the
program does 10,000 operations you're
not getting the answer you need it's
often important to understand whether or
not the problem you see is a scaling
problem most problems have some scale
factor some intrinsic factor it could be
the number of requests or the size of
the database any any number of other
things like that and in order to
understand a performance problem you
want to see how does the behavior change
with varying the scale factor it does it
go as the square of N or the logarithm
of n which would be great or does it go
as some higher power which is terrible
you also want to be able to distinguish
between a global performance problem
where no matter what you do it takes
longer than you think it should or a
particular corner case when I ask for
this particular kind of data from this
particular database it takes ten times
longer than any other request and also
you want to know is it a problem with
the average performance or with the
distributed performance you might have
an average response time of let's say
100 milliseconds which is wonderful but
if the variance is such that sometimes
it's a second or five seconds that's not
quite so good next slide so in order to
diagnose a performance problem you
really need a repeatable test case and
an analogy I like to make is between
medical diagnoses and performance
diagnosis for medical diagnosis you do
things like blood pressure and
temperature chemistry and the equivalent
of that for performance is understanding
resource usage the runtime transaction
rates if you really want to understand
what's happening you may want to do an
x-ray or an MRI
and a CT scan in order to relate the
behavior to the structure of the body
likewise for performance problems you
want to get data that relates to the
structure of the program and sometimes
you really care about what's happening
dynamically in medicine you do an
angiography which shows you what what
blood flow is a function of time for
performance measurement you want to get
a time-based picture of execution but in
any case the bottom line is that you
need tools to do measurements next slide
so I talked about okay that's right so
if you care about tools what are the
requirements for tools the most
important requirement is accuracy you
want the tool to tell the truth and
you'll see in a little bit how not all
tools do that some tools and
technologies are better than others at
preserving the behavior of the
application being tested you also want a
tool that will minimize the increase in
time due to measurement you don't want
to do to have a program that runs for
let's say a minute but have it take ten
minutes if you're doing collecting data
on it that's gonna distort the behavior
and not give you reasonable data it's
important that when you profile you
capture the key data that allows you to
show what's really happened and that's a
day to be unbiased that it reflect
accurately the behavior of the program
if it were not doing measurement and
accuracy is not a given there are tools
that distort the behavior and give you
measured data that does not rate relate
to reality next time another requirement
for performance tools is scalability you
need to know whether you're you're you
want to be able to do the measurements
at scale which means you need to deal
with something at the scale of the
largest database you care about not the
smallest you care about how many clients
are ax ax accessing a server for example
if you need to support a thousand
clients you
don't want to profile a run which has
five you also care about scalability
with the program size for us we care
about multiple gigabyte executables and
executables that might have a hundred
thousand source files in them on modern
systems you care about thread and CPU
count there are modern chips that have
thousands of virtual CPUs in a single
chip and you also care about running
time you want to know about a run that
takes tens of seconds versus a run that
takes hours you often don't care about
tuning something that runs for one
second because at most you'll save a
second but sometimes saving a second or
even 100 milliseconds off a particular
operation can be a really big win if
it's done a lot next time there are
basically two different ways of doing
performance measurements you can do
statistical sampling or you can do
tracing and we by far prefer statistical
sampling it's very scalable you can SWAT
all the sampling rate to manage the
amount of data that's being recorded and
often you will have millions of
instructions executed between samples
now that's actually a good thing because
in statistical sampling the act of
taking a sample will alter the behavior
of the program for a little while and
you want there to be a long enough
interval between samples so the program
settles back down again to its steady
state that the disadvantage of
statistical sampling is that you can
miss non-repeating short duration events
but I would argue they're not very
important for performance another risk
is that you get behavior that can be
correlated with the program behavior if
you're sampling on a clock that is the
same one used for the program you can
get really bad data a long time ago I
actually wrote a program to show that I
can write a program that would use 95%
of the CPU but the accounting system
would think it ran on the CP
you not at all you don't want you don't
want your tool to be subject to that
there are two ways to do statistical
sampling one is to trigger by a clock
tick hopefully the profiling clock is is
not connected to the clock that would be
used for scheduling programs it shows
you where the CPU time is being expanded
and on Solaris it will also show you
where the program is when there's
waiting for something like it's waiting
for a page fault or it's waiting for i/o
you get to see all the time that's being
spent in that as opposed to on Linux
where you only see the time spent when
the program is running another way of
doing statistical sampling is by
hardware counters most modern ships have
counters that count specific events in
the hardware how many branches were
taken how many floating-point operations
were taken how many cycles were consumed
how many instructions were committed and
so forth profiling on that basis can
give you a very interesting and accurate
portrayal of what is happening in the
program we recommend statistical
sampling for general performance
analysis next slide the other way of
collecting data is by tracing which we
do not recommend it's basically
recommended only if it's the only way
you can get the data you want the
mechanisms are either by installing
instrumentation and the key method you
care about interposing on library
functions or taking events from JVM Ti
in the case of Java profiling the
advantage of it is that you can very
carefully tune the measurements you do
to the problem you're searching for you
can do memory allocation and
de-allocation or i/o operations or
synchronization operations but the
disadvantage of that is that it requires
a very careful interposition which
technically is not non-trivial to do and
also it's scales poorly and it can
significantly distort the behavior
the application when it's running that's
fine one of the key things about
statistical sampling is to understand
where the program is that's called
capturing the call stack for many
programs capturing the call stack can be
very expensive however usually when
you're doing sampling the cost of the
unwind is only a few percent of the
total run time it's the in general it's
the increase in running time when you're
collecting data but it's usually pretty
small it's also important to capture the
full call stack if you have the entire
call stack in your samples and you
aggregate them all together you get a
very nice dynamic picture of the call
tree what the program is actually doing
and you can attribute metrics to each of
the paths through the code in our terms
inclusive time which other tools call
total time is the time spent in a
function and in everything that it calls
including everything that they call and
they call and so forth exclusive time
which some tools refer to as self time
is time spent in the function itself
only both of these are essential to
identify hot branches to understand
where the time is really being gone
going in the code it's important to know
that in terms of Java profiling Java
stack unwind is not well supported in
many tools there are tools that have
deferred sampling whenever the JVM is at
a safe point and that will give you
incorrect data as we'll see shortly also
when you run with the hotspot JVM
inlining takes place whenever it
compiles methods and that's also
difficult to understand but really needs
to be shown if you want to understand
what your program is doing and lastly in
many profilers if you're running a mix
Java and C++ application you can only
get to see when the Java calls out to
C++ you can't see what happens B
and that's often important to understand
the overall behavior of the application
next slide so I've talked about what the
performance problems are what tools
should do and now I'll talk about
accuracy and go through a simple example
this is a simple example that does the
same calculation in two ways one is done
with an explicit triply nested loop that
adds one and the other is done also
triply nested but here each level calls
the next level down so they're doing the
same thing except for these extra calls
that take place because of the nesting
however the expected performance is
really pretty much the same you'd expect
them to take the same amount of time
they're doing the same computation the
data that I'll show you is collected
with three different profilers Java
flight recorder the NetBeans profiler
and the studio performance analyzer the
next two slides will show you the source
code of the sample the example that
we're using I'll use it for this section
and in the next section to talk about
detailed Java issues later so the next
slide in this slide you can see that you
can see the test does - has two calls
compute simple and compute deep and
here's the source of compute simple very
simple there's the triply nested loop
there's the sum and you'd expect most of
the time to go there next slide next
slide is the compute deep routine where
you see the outer one calls next level
one next level one calls next level two
next level two calls inner some and
inner some does the same sum that was
done in the first example you can take
these we recommend take the source from
these slides and try it on whatever
profile you want to see how good the
answers our next time so I'm first going
to talk about the NetBeans profiler in
the NetBeans profiler that what you
notice is first of all the code ran for
about 34 seconds before but now it takes
40 seconds so that's 10 12 percent
increase but what's really important is
that it shows all the time was spent in
main which is correct but 98% e7 percent
of the time was spent in compute deep
which is not true no time is shown
hardly in compute simple this is the
wrong answer next time so look at the
function list from that same application
you still see the same numbers not
surprisingly but what's important is
that inner somewhere you know where we
know most of the compute deep
calculation is actually done doesn't
even appear the next slide other example
we chose was the Java flight recorder
same program now you can see all the
time and EEMA in main comes from compute
simple one of the two and virtually
nothing is done in that compute deep or
in the two nested levels and again the
inner sum is simply missing the next
slide it turns out JFR we've complained
to them and told them what was going on
they do have a nun they do have an exit
okay they do have an option that you can
set called debug non safe points I don't
know why they named it that but if you
do that now you get much more reasonable
answers
right next slide I think that made it
did me alright I'll do the slides um in
this one if you set this non default
option you actually pretty much get the
right answer now you don't really know
that it's right because JFR only shows
the sample count which not and not the
CPU time so you can't tell whether or
not you've actually missed something
next slide so the next slide is using
the studio performance analyzer here you
can see it takes about thirty thirty
four and a half seconds in main and
that's about equally spent in compute
deep and compute simple performance of
complete simple which has all these
calls is about the same as compute deep
because of JIT compilation takes care of
a lot of the time and the actual runtime
is about the same as if you ran without
collecting any data at all here's an
example where we've taken in the
previous slide you can see compute deep
shows that shows here that there's an
expansion that can be done but in law
but compute simple has no expansion in
this slide we've expanded compute deep
and now you can see it calls level one
calls level two calls inner some this
gives you the right answer and what's
more shows you how you got there in the
face of inlining looking at the hot
functions you can see that the self time
or total time is spent in computing
sorry is spent in compute simple and an
inner some which are the two places
you'd expect to see it so I've talked
about this simple example and now I'm
gonna talk about some of the real
intricacies in understanding Java
executions it's not trivial so you think
that the JVM is just a C++ program
so it should behave like any other well
that turns out to be somewhat naive it
turns out much of the code is
interpreted by the Java interpreter it's
not generated code based on your Java
sources it's interpreted some code in
the JVM when it runs is actually
dynamically constructed into the data
space of the JVM Inc that includes any
methods that hotspot compiles but it
also includes the interpreter itself in
profiling at any instant in a lifetime
of the program any thread actually has
two calls that it has a Java call stack
which represents the methods that are
being called in the user's java code but
it also has a native call stack and you
need to tie the two together to get a
true picture of the actual execution
that's especially true if you have Java
calling native into native code into
your own native call code in C++ you
need to be able to see what happens
after the first entry point if it's also
true that the JVM consumes resources not
really directly connected with your
source code it can do job of garbage
collection which can induce stall it's
in the user Java and it runs with many
threads both user threads and JVM system
threads and it can we'll have safe
points during execution which can induce
sampling bias any profiler you want to
use has to deal with these complications
so I'll talk very briefly about the
studio performance analyzer it's a set
of tools for collecting and examining
data it runs on Linux and Solaris on x86
and SPARC it can collect data on a wide
variety of applications written in C C++
Fortran Java mix Java and C++ using
OpenMP or MPI and it can deal with
complex enterprise scale applications it
has a GUI and command-line interfaces
with many different views of the data
and you can drill down to understand
what's happening by filtering and
filters are available from each of the
views from the timeline from a list of
processes or threads etc and it's free
free download and use license the goals
that we had in designing the tool was to
collect accurate data we use internal
JVM interfaces and that captures the
inline methods it avoids sampling bias
with respect to save points and the tool
is designed to show Java source level
execution as well as the real hardware
level execution you can see metrics of
performance on each source line of your
Java you can seamlessly navigate call
stacks between Java and native code and
you can see the data at the byte code
for Java code or at the machine code for
either a hot spot hot spot compiled
methods native code you call or the JVM
itself and it will expose the internal
JVM activity that uses resources that
includes hotspot and garbage collection
we have two ways of looking at the data
that we record Java source level
visualization which we call user mode
shows only those stacks that run user
Java code only those threads rather that
run Java code it shows the complete Java
call stack including all the methods
even those that are inline by the JIT
compiler and it will show the native
code of mix Java and native call stacks
for disassembly it shows bytecode it
also aggregates together multiple
versions of the same method so if you
have a method that was interpreted part
of the time and then hotspot compiled
and then hotspot we can file all the
data is aggregated together
it also can account for times where the
Java stack cannot be unwound by the JVM
typically it's less than 3% of the time
when that happens we attribute the time
to a pseudo function that whose name is
no Java Paul stack recorded this design
decision for that is that it was
preferable to lose some data than to
induce synchronization in the JVM which
is what other profilers do to make sure
they can safely online the stack we also
account for time in the JVM run time by
a pseudo function called JVM system we
also have machine level visualization
that we called machine mode that shows
you really what's happening all versions
of JIT compiled methods are shown
independently we show that function
named interpreter for the interpreted
methods we show all threads both user
Java and the JVM internal threads you
can usually identify the JVM system
threads by the functions in the call
stack the names can tell you whether
it's garbage collection or hot spot
compiler and in machine mode for
disassembly we show the machine code the
actual machine instructions on the chip
so here's a picture this is the same
application that same simple code we
talked about before
so now here's the source view of the
data you can see here's the source view
and you can see time is equally spent on
these two calls time in compute simple
is spent on the source line that does
the actual computation in looking at
compute deep which is at the top here
you can see it's caused the nested level
one nested level two and inner some and
he
or some and you can see that time
propagates its way up from the inner
some where the work happens to the
outermost caller compute deep as I said
you can look at byte code and in this
particular example not surprising most
of the time was spent on the byte code
is spent on the byte code D ad which is
doing the actual adding one in the code
this is showing you the byte code
expansion of inner some but part of the
interface also shows you who called it
or some and wanting to call some what
what inner some call in this case you
can see it was called by nested level
two and that's where all the time lines
we showed the call tree before here's
the call tree in user mode again it's
the sorry having trouble with the
buttons here you can see in lining
compute deep expanded you see there's
thirty four and a half seconds in main
and it comes from these two branches
which we've seen before if you filter
one of the things that we can do very
easily is filter so in this case I've
chosen to filter so that we're only
looking at data that has compute deep
somewhere in the call stack in this case
you don't see any change in the inner
computations and the nesting you do see
that the time in main is now down by
half because it only represents the time
in Maine when Maine has called compute
deep if you take that which that call
stack in user mode and switch to machine
mode you get a much more complicated
call stack which shows what's really
happening you can see that
the thread starts it goes to Java main
it calls J&amp;amp;I etc goes through all of
this stuff and eventually gets to
interpreter interpreter calling
interpreter even and then you see nested
level 1 that's a hotspot compiled user
method there's a little more interpreter
and there's also compute deep that's
another hotspot compiled method so in
this call stack almost every human goes
on is in the JVM or it lived seed RSO or
the interpreter but there are those two
hotspot own compiled methods that show
up this is a picture of the timeline
that shows the execution of the program
as a function of time so what you see
across here are call stacks in each of
these you can see there's a little gap
there that's because you're seeing
snapshots taken every 10 milliseconds
but they're lined up consecutively so
you see the behavior this is the call
stack here represented there that
represents where main calls compute
simple here is the user call stack where
main calls compute deep calls nested one
nested two and inner some and you can
see there's a pattern here what's what's
represented here is twenty iterations of
a loop calling first compute symbol then
compute deep and you can see here's a
particular loop marked off if you switch
them to machine mode you get to see a
quite different picture for the first
part computes simple it was hotspot
compiled very early in the run and what
you see is this particular stack showing
the leaf in compute simple if you look
at the other half you can see there's
actually a transition that takes place
at about 20 seconds and the reason for
that is before a hotspot recompilation
you have this call stack showing the
interpreter calling nested level one
after a recompilation you see
interpreter calling compute deep that's
because the JVM recompiled that hotspot
method and did it in lining everything
in to compute deep as opposed to showing
this at level one and in lining in to
hit here's a picture of the same time
line expanded way out so what you're
seeing is from is only 2/10 of a second
in here and you can see actually here's
where the transition took place this is
the point at which the hotspot compiler
which ran on thread 56 came in and you
can tell it was hotspot because it says
things like compile broker or situ
compiler etc that tells you it's hotspot
was active at that point and the change
that take place in the call stack for
compute deep is directly as a result of
that hotspot compilation you can also
look at the disassembly in machine mode
and now what you're getting to see is
the actual machine instructions executed
and what's interesting about it is down
at the bottom down at the bottom here
you can see the innermost loop has
actually been in lined into compute deep
into nested level one rather who's in
line into nested level one but the loop
has been unrolled so instead of having a
loop around these ads you have them
rolled out that makes it much more
efficient to execute so I've talked now
about the issues about performance
problems what to look for in tools how
to understand the execution of Java code
and now I'll talk a bit about how one
would go about identifying programming
efficiencies in a particular code so
what's the workflow to do this the first
thing you should do
is pick a repeatable test case for
measurements if you don't measure it and
repeat the measurements you won't
understand what whether or not you're
making progress or why you're making
progress the next step is to find that
find the CPU hot spots what are the hot
methods call path source lines and
instructions and you want to be able to
tell is the CPU pipeline stalled is it
waiting for data as in a TLB miss or a
cache miss or a memory latency or is it
waiting for some instructions to
complete like floating-point operations
or divide which can take many cycles if
the CPU pop pipeline is not stalled if
you have high call counts then you've
got too much overhead in call and return
less of a problem in hot spot because
hotspot does inlining to meant to take
care of that but there also might be
non-productive cpu time like a busy wait
on a lock where you're doing
computations but they're actually not
doing anything and the next step is of
course to fix the problems revealed in
the data and how you go about doing that
depends very much on what the cause of
the problem was so how do you find
pipeline stalls by far the easiest way
is to use a hardware counter that
directly measure stalls it's a direct
measurement of the CPU time that's lost
because the pipeline is stalled and it
can be directly correlated with the
specific instruction on which it happens
on spark chip for example the commits
saw hardware counter has exactly a one
instruction skin so you know the
instruction above it is the one that
that's stalled this is very useful for
understanding memory or other pipeline
stalls another way of doing it is to
compute based on instructions per cycle
or a cycle for instruction clearly
reciprocals of each other in order to do
that
you can profile with both an instruction
counter and a cycle counter and both of
those are available on many if not most
chips if you have low instructions per
cycle or high cycles per instruction
that indicates an inefficient use of the
pipeline it means that it's it's an
implicit indicator that stalls are
happening it's difficult however to use
IPC and CPI to compute how much time is
actually lost because of it whereas
stall a stall counter gives you the
exact time and measuring CPI or IPC is
very noisy at the source line level and
the instruction level level because both
those counters have a variable amount of
skin on most machines for memory access
you can reduce stalls memory access
thoughts rather include cache miss times
or memory latency times or TLB miss and
you can reduce stalls by changing how
you lay out the data if you could lay
out the data density so the hot data is
in the cache great you can also improve
memory stalls by reducing the levels of
indirection both in referencing data and
in calls you can reduce memory stalls by
co-locating the hot fields of a
structure so they're on the same cache
line that means the reference to fields
you only have one cache line to fetch
and you can also fetch the data in the
right order in the reference order so
that you can exploit Hardware
prefetching which again many modern
chips do so I'm gonna go briefly through
an example of how you would tune a CPU
problem the example we picked is an
older benchmark code spec jbb 2005 and
we're gonna do source line changes which
I must point out are not legitimate for
publishing spec numbers
but for turning your own program of
course you can make source line changes
that's the benchmark models a 3t
business three-tier business system it
has random input that measures user
transaction in the first year it uses
Java collections for the third tier and
the way the benchmark is set up it does
know disk or network i/o so here's an
example we took a profile and we see the
hottest hottest line is actually this
line 121 in retrieved stock very
expensive line if you look at the source
you can see that's a pretty simple
method all it does is call an access
method nothing much to know there if you
look at the bytecode of it however that
access method has several byte codes two
of which are quite expensive one is
invoke interface which is actually what
a call becomes and you can see it turns
out this call goes to two different
places a gap on map storage and a value
of on an integer however almost all the
time comes from the map storage what we
learn from this is the data structure
being referenced is called stock table
and it's using an underlying data
structure called map data storage which
is based on a hash map class now hash
maps are great if you have randomized
data but in this case the indexes we
care about are consecutive integers so
we can choose a different storage method
and that can be critical for performance
in this case we're going to replace hash
map with ArrayList
this is optimization one now if we do
the profile the cost in
in this stock table get has gone down
from almost 70 seconds to 45 and a half
it's an overall improvement in the
actual application of about 18 percent
another thing we noticed back then and
this is a picture of the bike code after
optimization one you can see there's
still a lot of time spent spent in check
cast now check cast is a bike code that
verifies that the data structure being
referenced has the appropriate type for
the method type safety in Java comes at
a cost and there's an example of the
cost so one thing that we thought was
gee we can do better than that
instead of doing a stock table get of ID
we can just fetch the index so we did
that and sure enough there's a big
improvement check cast time has gone
down to nothing it's not even in the
disassembly but it turned out there was
no real improvement in the execution of
the program the reason is that what we
have done we've made an optimization
that moved the memory fetch from one
place to another and that moved the cost
of the cache mass from one function to
another so you have to be careful not
always making what seems like an obvious
improvement will actually improve things
and again that's a reason why it's very
important to have repeatable test cases
so you can see are you really making
progress it doesn't help to reduce the
time spent in a particular source line
if that just moves it somewhere else so
what are the lessons we learn from this
example using a more efficient storage
class saved eighteen percent of total
time we replaced hashmap with ArrayList
but eliminating an expensive
check did not help memory access costs
moved but it did not go away and you
could use CPU hardware counters to
confirm that there these are cache
misses and that they move from one place
to another another thing is that memory
latency can dwarf other inefficiencies
in your program so here's here's part of
a semi marketing slide why you should
consider using our tool it gives it's
great for general CPU performance tuning
it measures Java and native code it
meant it can be used to measure the real
behavior of the production code on
production scale runs that is you don't
have to collect data on a toy boat toy
example you can do it on the full-scale
example so you're getting the data
that's relevant to the run you care
about it's very easy to drill down to
see what where the time is going you can
use different views you can filter them
to isolate parts of the program so you
can eliminate all the stuff that's not
relevant to the area caring but you're
looking at now the tools run on both
linux and solaris and they support
cross-platform analysis to record on any
one of the supported platforms and look
at the data on any one of the others and
in fact we support remote analysis you
can run the GUI either on a Windows
machine or Mac OS machine as long as the
data is on one of the supported systems
and you have to run the backend of the
GUI on that supported system so again
one of the advantages of our approach
accurate data get the JVM interfaces
capture the inline methods avoid
sampling bias with respect to safe
points you can see that Java source
level abstraction in the hardware
execution you can see source line level
metrics you can do Naseem las' nation
during Java and Jay and I you can look
at bytecode or machine code for user
Java or hotspot compiled methods in the
JVM and you can see the internal JVM
activity that uses resources and as I
said before it's free to download and
use for more information there's a
website available for all of studio that
will include ret sessions to our tool
and although it's somewhat late there
were two relevance related sessions in
Oracle OpenWorld
one on application data integrity and
one on how to build an open stack
developer cloud so I will leave that up
and I'm happy to take any questions yes
yes we were careful to use only Oracle
products in the comparison they're done
by two different teams surely you've
encountered that doesn't try to go
underneath the layer we don't think so
it depends on what you're trying to do
but if you're looking at the source if
you're looking at the source level view
and looking at source lines trying to
drill down insight oh this is the java
source code what really happen do you
not even get you're not gonna be able to
do that with flies record so it just
depends on how deep you want to be able
to go and that's why this was proposed
as a next level
and also if you use the flight recorder
you have to use this magic perhaps
documented extension in order to get the
correct you know first rule of
programming of performance measurements
is if you don't have to get the right
answer it's really easy to make get it
fast yes
sorry the JVM keeps its call stacks in
in its memory and if it has to do
certain kinds of operations it has to go
back and change those call stacks to
reflect where this method really is now
instead of where it used to be and
during that operation it is not safe to
on one stack it uses a different
interface to get the data which does not
rely on safe points you are requested
the mechanism of JVM Ti and the function
that gets the Java call set were
specified for up by us and defined
within the JVM to do it and the design
decision was made that it is better to
report sorry I can't do this than to
either give the wrong answer or simply
defer it so you don't get the signal at
all it's a design decision and one that
we still think was the right decision
yes
that's kind of beyond the scope of this
talk there's a paper that was given at
supercomputing maybe 10 years ago on
data space what we call data space
profiling whereas our tool for native
languages can do data space profiling
that as it can cut it can tell you which
cache misses were done on which elements
in which structure rather than just tell
you which source line or which
instruction caused the cache miss and in
that paper we found got a 20%
optimization by changing the layout yes
we say that the overhead for native
languages is about five percent and for
Java is about ten percent so you'd
expect to see five or ten percent
slowdown but not more than that well
that's where you get it yeah if you have
if you use it and you have any issues
with it do tell us about it because we
the tool is fairly heavily used with
inside with in Oracle the JVM folks use
it a lot so do several other internal
groups I'm not at liberty to talk about
any other questions yes
no for reasons that will be embarrassing
for me to relate man marketing decided
the tool would be called Oracle Solaris
studio even though it runs on Linux no
we don't support Windows we support
Windows for remote access to the data
but not profiling on Windows no friends
don't let friends use windows yes I
understand
any other questions yeah
yes yes and no it depends on if you're
running on Solaris you can run the
application and then attach to it and we
will be able to get all the right data
on Linux however that doesn't work
because the OS on Solaris when you
attach and turn on a profiling it will
turn on profiling for all threads in
existence and all new threads that
arrive on Linux it will only turn on
profiling for the thread that makes the
call so that's one particular problem
with it there is a technique that we
have for doing it you can start the
profiling with a flag that says pause
the data collection and specify a signal
that says when this signal is delivered
toggle that flag so this is done
actually for lots of people who are not
interested in the first hour of the run
but care after that they'll start the
tool with that flag and say don't
collect data an hour into it they'll
send the signals and start collecting
data and that works on Linux as well any
other questions
well thank you for staying for the last
session or the last day of the
conference</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>