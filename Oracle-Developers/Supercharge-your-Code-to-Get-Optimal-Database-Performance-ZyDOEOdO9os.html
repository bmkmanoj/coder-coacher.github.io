<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Supercharge your Code to Get Optimal Database Performance | Coder Coacher - Coaching Coders</title><meta content="Supercharge your Code to Get Optimal Database Performance - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Supercharge your Code to Get Optimal Database Performance</b></h2><h5 class="post__date">2018-03-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ZyDOEOdO9os" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good afternoon for those of you who
don't know me yep you probably missed me
at the keynote I'm Gerald I was the MC
this morning and I give a talk a 40
minute talk I think now about how to
supercharge your code for optimal
database performance I named it fancy
for actually how to write good
performant code so that you don't end up
waiting for a database all the time and
what really started this talk was this
sort of mismatch developers thinking of
how databases work and deviates thinking
of our programs work these days and you
know they seem to kind of talk like
across each other not really meeting and
I had the luxury of being a developer
and the DBA and my previous lives and so
I got a bit of experience in both sides
and I thought hey you know what maybe
let's just share some of this insights
that I've found because what I've found
is that whenever it comes to databases
most people think of those things are
just old and slow and actually 90% of
the time very confidently I say that
it's actually the code that is not
written perfectly well to interact with
your databases so the talk is around
relational databases in general how they
work about a show this on Oracle
database and just before I kick off how
many developers do we have in the room a
show see a raise of hand perfect how
many TV ace do we have in the room okay
so those are your two best friends there
for the developers and how many people
have you believe the databases are just
suck and old and they will you know they
will never perform and I just do
everything in memory in parallel as much
as I can
well that's good okay there's one two
three okay well that's good okay perfect
so show you a couple of tips and tricks
and they're really kind of simple to get
your head around it but once you
understand why it is the way that it is
green or it will have this funny safe
harbor statement we always have to put
up our lawyers to say oh you cannot sue
us if I say we announce something blah
blah blah we had this funny thing with
the Java 10 this morning is it coming
out this it not well actually it didn't
put any of that in because there is
nothing I talk about in the future but I
put my own safe harbor statement in
that's just because as I talk about
performance here you know just be clear
let's be clear that what I show you
today is not necessarily actually the
fix for your actual performance problem
that you currently face right there's
many different reasons why you might
have a performance problem I just show
you three very common things and then
the second one is you know performance
is relative now I'm doing this all it's
actually not quite true anymore I used
to do is all on VirtualBox now I'm doing
this all on a tiny little docker
container running mortal database on my
laptop right so what you see today in
performance numbers does not necessarily
reflect the numbers that you will see
you on a proper big server this only
runs to gigs right and keeps it all nice
and contained to make the point okay so
but before they again and we will do
this as long as my daugher container
runs
let's just get back to theoretic so what
is performance it's actually a question
that I saw my past there used to be a
performance engineer tuning software
cetera I said before this kind of gives
me those two life's are left with DBA
and temp developer and one of the things
that I found very useful back then is
actually to first take a step back and
think about what actually is performance
you know and if we want to express it in
a formula what would this formula B and
of course as with all of those more
theoretical questions you have 10,000
answers and you know everybody thinks
he's right and of course I'm right in
this case and you all wrong I believe
that performance is just a simple
formula of latency times throughput as
in how much can you do in a given time
and how many of those can you do in
parallel so when you apply this you know
latency is basically if you take a unit
of work whatever that may be well that's
your database transaction whether that's
your payment transaction as a whole your
business transaction whether that's
running through a whole batch load
whatever that may be that unit of work
the latency is just how long this will
take and then a throughput is how many
of those you can do in parallel
sometimes you cannot do anything in
parallel at all but a lot of times you
can actually run in parallel and then
the important thing here is also back to
the basics that a unit of work
in computer software in general with
computers in general there is really
only two states that we have and inside
its processing so it's running on CPU
and doing potentially useful things for
us on CPU aka our calculations that we
wanted to
oh it's waiting for something else right
and actually when you take it down to
the level that applies everywhere so
it's like you have an application well
your application is either busy or it's
waiting for the user to send something
over right let's keep the payment
transactions for example well if no
payment happens my application isn't
doing anything if a payment happens my
application is doing it then you look at
the application well at its processing
now or it's waiting for example for the
database rather than actually as we go
further this applies to all of those
stages and when you keep it at that
level of course there's very many
nuances to this but when you keep it at
a simple level I find at least you think
differently about performance right on
the goals that is sort of always have to
keep in mind is you want to process a
unit of work as efficient as possible
right so this is like wherever you can
reduce latency write better code more
efficient code etc well then obviously
do that right when you can reduce wait
time you want to reduce the wait time as
much as possible so you know what are
you waiting on this code database and so
forth and this is one of this talk what
this talk is about well you should do
that right because if you shaved our
latency and the wait time the overall
unit of work processing time will go
down and then also one thing that you
should always keep in mind and it gets
more more abstract these days with many
many different layers of frameworks and
so forth you should avoid unnecessary
resource consumption right so if you
don't have to be on the CPU don't be on
the CPU if you don't have to scan 10
terabytes of data but only 2 gigs then
you know don't scan 10 terabytes of data
and that doesn't really have anything to
do with the actual unit of work being
processed per se who cares whether you
process you know you've spent all your
processing and all the CPUs or only one
but it actually is important for the
throughput the resources that you don't
use are free for something else to be
used whether that's another thread of
your application whether that's a whole
lot of application all together running
on the same environment ever so forth so
those are kind of my things that always
stick there when I
code you know and try to keep those
goals or apply those goals to my code as
much as I can now going back to the
formula if performance is latency u
times throughput so what does this mean
if I would like to produce or process
10,000 work units so again the work unit
is an arbitrary piece of work to be done
let's take credit card of payment
transactions so if you would like to do
10,000 of those payments per second what
does the what does this formula equate
well one of this could be that in order
to get to 10,000 your latency if your
latency is one millisecond and you can
process 10 in parallel of one
millisecond you can do a thousand a
second times ten is 10,000 right so if
you want to achieve ten work units per
second you have to meet something like
this a millisecond latency lieutenant
parallel now sometimes you cannot always
achieve a 10 at one millisecond latency
sometimes it's more so it could you
could achieve the same thing by saying
well okay if my latency is 10
milliseconds then I have to at least
achieve a hundred units in parallel and
again this is what I found somewhat
important to keep in mind when it comes
to hey how much do we actually want to
process right what's our performance
requirement etc performance requirements
become more more arbitrary it's like
everything's web scale internet scale or
whatever right it's like we have to
produce millions and millions millions
but that's fine but what does this
actually mean right because sooner or
later you have to bring it down well how
much can the physical hardware actually
achieve and therefore how much physical
hardware or more physical hardware do
you need to get to that goal if you get
that wrong at the beginning you will
never ever achieve your performance
requirements you then end up ten months
later or something oh we need more
Hardware we need to react attacks etc so
I found this formula simple formula of
course with a lot of nuance is also very
very useful in a latency times
throughput it's not not that difficult
and then you have to adjust those two if
you can achieve less latency that's
great then you don't necessarily have to
achieve the parallelism if you cannot or
if you want to achieve more parallelism
then you have to tune the other angle so
forth now when we take those two sides
applications and databases there is a
well I was called somewhat of a
impedance mismatch of how different
things work so it's Chavo day to day
right we launched Java 10 so a stick
around of Java in this example and
Oracle database and you know for if you
want to scale your java application well
you will take your units of work and you
will give this to thread pool or
something and the many different threads
work in parallel perhaps on a cluster
and so forth working through those work
units right and whether that's then
containerized running in kubernetes and
so forth but you would essentially take
all those unit of works you have and try
to paralyze them as much as possible and
then actually run those on the CPUs and
get this throughput that we talked about
now when you then hit the database and
each of those things have to talk to the
database this all sounds well and good
for application were running at the
database there's more important aspect
into it all of those work units will
simulate to go over the network and
we'll have this network up and you will
actually now wait for the network and
you won't wait for the network once but
you will wait for the network ten times
ten thousand times however how much you
process in parallel and the database
actually will have to do a lot of
repetitive tasks when you give it the
statements to perform and we will see in
a second why that is so
sometimes although it seems like it's a
really good idea to just paralyze on the
App tier as much as possible and then
hit the database in parallel as much as
possible so you know usually if the
formulas like for each thread I have one
connection or to the database well
databases don't actually quite work like
that and sometimes is actually better if
you can paralyze as much as you like on
the app to you but hit the database less
often but with a bigger pipe not only
because you already reduce the network
round-trips right so we went from in
this case four to one natural ground
trip so that saves you ready on wait
time but also actually there's less
repetitive tasks that a database does
and I'll show you in a second but just
let's just quickly jump into how we do
is with Oracle today so I said I will be
talking about Oracle but all those
concepts apply actually to all
relational databases
now when you know talk to developers
or to anybody but a DBA sometimes people
tweet they just ass black boxes
let's say we gave it to the database I
don't know it takes four HS to do
something right it's like I have no clue
what's going on DBA the database is slow
and then usually a DBA goes like the
network is slow right it's like it
becomes this huge playing game right and
sometimes you get to well database is
slow it doesn't do anything at the DBA
comes back and goes like well the
database is idle you don't do anything
and you go what's going on now the funny
thing about Oracle actually is that you
can trace pretty much everything that's
going on in the Oracle database and
that's something that is what make or to
big over the years ago and you know
again coming back to the same principle
as before well let's keep it simple what
kind of the states of a database what
can it be well they could either be
working right so we gave the unit of
work to database do something with it
great or it can be waiting right that
might be we do some processing here at
the application layer or maybe the
application itself is waiting right
nobody's doing anything because no
transaction is being no payment is being
processed at this point in time but
again it's kind of those two binary
states right our database is doing
something or not and this is usually
very easy to figure out just by looking
at CPU utilization now when you then
look into the database right so here we
have is the database doing stuff or the
application doing stuff now let's say
the database is busy all right so we
know the application is waiting for the
database well the database itself can be
busy you know working what I said before
that means actually processing on a CPU
right doing actual calculations for us
or usually processing on many many CPUs
databases are multi-user since a long
long time they tend to process in
parallel since decades literally so you
know it's either running on on CPUs or
the database itself could be waiting and
for what could it be waiting well things
like from disk the network
sometimes for another user to actually
commit so that we can do this out just
to update to our row or so forth right
so again only two states the database
itself can be waiting also working or
waiting and why do I tell you this it's
kind of like already
easy way to figure out what's going on
with your application right so first of
all let's figure out that we doing
something else a database doing
something okay the database is doing
something well now let's have a look
right is the database actually doing
something always are waiting for this
because you know we scan 10 terabytes of
data and we know that disk is slower
than CPU and therefore database is
waiting for disk to respond or is it
actually doing processing for us now as
I said before it Oracle actually you
know exactly what's going on because we
tend to trace everything so when we
process on CPU we have something that we
call statistics that is just countless
that we continue here and when we are
waiting we have something that we call
weight classes right and you can
actually look at the database for your
given database connection and figure out
exactly what's going on is it processing
okay what is it doing or is it actually
waiting for something else do we have to
tune something and today we have about
thousand eight hundred or thousand eight
hundred and four statistics in the
Oracle database and twelve 201 and
actually now I'm gonna jump over and
start up my database and show you some
of this so this means there's already a
lot of different countless that we have
in there you probably cannot really read
this can you let me zoom in here a bit
here we go so let me just start up my
database so we do have Oracle database
and docker containers if you didn't know
that yet so that makes it quite nice and
easy so hence why I don't do go to books
anymore
so just starting up the database quickly
so we have thousand a ton of different
statistics that tell you actually what's
going on within a database so this is
all the stuff that's being done on the
CPU level database is up and running
there we go
how do we then just do here quick can
you read this on the back is this big
enough or do you want me to make it
bigger
it looked like lettuce okay that means
bigger okay so all right there we go so
I'm just connecting through my database
quickly
let's see
setup we're in and then of course
hang on if you just
you know see around this count here
thousand eight to another for statistics
so this is not an interesting hint can
actually retrieve everything in the
Oracle database with sequel and actually
all relational databases tend to have
that so a lot of this stuff actually
available for you through plain sequel
which means you can actually build this
into your application as well if you
wish to do so or into tracing tools and
so forth I hope this comes around
alright they were there it looks way
more bright they're here and over there
and to the point that you can go ahead
and for example say hey you know what's
going on for my angle database side we
call those sessions which is your
database connection you can quite
happily just throw in a sequel query
this for example was for my session now
and tell me you know tell me all the
statistics that are going on for
essentially my step is a view that gives
me everything from my session and now
for this execution of this one query I
see exactly all the statistics that
happen right so it's like CPU use by the
session database time round trips so
this is natural ground trips and so
forth and so forth so it gives you quite
a lot of detail of what's actually going
on not that you need to know every of
those thousand eight hundred statistics
but actually it is already telling you a
lot of actually what its it is doing in
case you are puzzled and then likewise
let me jump back here into the slides
likewise we do this for weights so if
the database is waiting whether that's
you know disk etc we track those in
weight classes and there's 13 of those
weight classes so we go from you know
what you see their application cluster
cetera to all the way down so there
again you can actually pinpoint when you
actually issue a call to the database
you can then pinpoint and you're waiting
there you can pinpoint what is actually
going on and you figure out exactly or
you can't figure out exactly you know
it's director done what it's done do we
wait for something do we have to tune
etcetera that just basically highlight
this because it's not a black box at all
so and and you know sometimes also DBA
is kind of
guards this impatient from you it's like
oh no no no you know you're not allowed
to see this you wouldn't understand
anyway but really you know DBS and
developers should work together and
there again we have a thousand eight
hundred eleven wait events in total as
well now they look very old a thousand
eight hundred I was like this is weird
this seems like a made-up or something
actually turns out wait events there's a
lot of those in a category that we
classifies others which are eight events
that usually don't occur a lot to do
with internal things and so forth the
way you ants are actually much much less
that you can find out and most of the
times it's at a commit time or this guy
oh and so forth on a well-tuned database
so that's just the theoretics right so
really just keep this in mind
it gets you quite a far away when you
deal with databases already now let's
actually jump into some of those common
pitfalls that we have seen the first one
I tried to give them all fancy name so I
call this commit savior of your data and
yet you here really is what does
actually happen when you commit the row
in the database so you know you're like
okay there is your payment transaction
coming along I'll do an insert statement
into my role perhaps a few insert
statements and so forth and then I
commit this and so when it's data
actually written to this when is it not
written to disk and sometimes people
have the wrong assumptions there as well
so in order for us to commit to begin
with to commit the transaction we have
to do some DMS just short for data
manipulation language rights or insert
update delete essentially are DML
operations and then once they all run
and asset transactions as we know we
will rediscover lately that they are not
so bad right Mongo just rediscovered hey
yes a transaction sometimes they're
really really cool which means it's an
all-or-nothing approach so you insert a
bunch of rows you can update a bunch of
rows you can do whatever you like
within this one transaction and then you
decide whether you would like to write
this to this I can commit or you want to
rollback everything an issue
right sir but you have to have some
manipulation done before you actually
commit now when you talk to some people
they think that or I had this monster I
should say you know some people I
actually had a guy once who told me
well obviously databases are slow
because every time you insert a row or
you update the rule delete a row it has
to write this to this so it's constantly
writing stuff to this and I said well on
a high level you're right it has to
write it to this but that's actually not
quite true when you insert the row
update or the literal you're actually
modifying this data in memory and then
or it will recall this the buffer cache
right so you actually go against memory
if those operations are very very fast
whether you do it on main memory in your
application or main memory on the
database does not differentiate that
much as compared to disk
however when you do then commit there's
the interesting part right so while we
commit so now the data has to go to this
right because it's an asset transaction
we say we commit this data we never ever
want to use this data again and that's
choose like commit time we actually sent
in this data to this right we call those
redo logs at Ascona journals and so
forth those are basically heavily
optimized ring buffers if that rings a
bell so structure where you basically
write in a ring and then the other one
picks it up and reads from a ring so
that you never have any contention now
that's essentially what you write to
disk and the idea being is that you know
you want to have this commit happen as
quickly as possible so you just
literally write the changes to disk and
then later on an async confession
database will basically flush the data
out from this buffer cache from this
memory area to disk and if it crashes in
between it has those journals to
actually redo your data to give you a
consistent state now why is that
interesting well because I see the redo
log sorry well because when we take a
closer look at while I see a lot of
times people just using Auto comment
right it's like a commit every row I was
like yeah of course right I run through
ten thousand rows of a batch process or
whatever I just have auto commit on or
just commit every row well you know
commit as it's
very trivial on your connection right
you just say connection dot commit and
you're done in Java actually means
there's some work to be done by the
database right and there's actually
quite a little bit of work to be done by
the database it's still compared with
trivial but still so first and foremost
the database will generate what we call
an SC a no system change number that's
basically your transaction ID
incremented to the next one so it's just
as large incremented it always goes up
and up it will then write this state
change or those changes to the reader
locks recall the leader rocks the system
background process called the log writer
it will then release locks on table
right so iam asset transactions if I
update a row somebody else is not
allowed to update the row while I
haven't committed or rollback
we're waiting on each other once I
committed will then release this lock on
that table or the row I should say if
you save points and transactions there
are transaction save points that you
know you don't have to do an
all-or-nothing approach you can actually
keep as many safe points as you like
throughout your transactions if you have
some logical points where you would like
to commit but not everything's done mood
remove those and then we perform
something that we call commit clean out
so basically get rid of the information
of the transaction happening in your
data and then mark the transaction
complete so those are two four six steps
that the database performs and it
usually performs it very well this is
all what happens just when you say
commit right and still there and all of
those steps actually happen on this
there's only two steps that requires to
write to disk so this is when we write
this video information to the lock to
the journals and when we perform this
commit clean out everything else is just
CPU based operations as well okay
so
why am I telling you this right so going
back to what I said before a lot of
people leave all to commit on or just
say hey I commit every row so here is
some example in Java let me see what I
can zoom in here again
whoops can I can I can I so just an
insert into statement that we have
before and what we do here we're Emma
nope doesn't good what we do here just
for 10,000 rows in this case you
basically set the value yeah hang on I
lost it there you go that's better so
insert into statement that we have over
here and then you know we basically have
to bind variables and we just say okay
here's the incrementer off the for loop
and then we set the string and then we
execute this insert now it's called
execute update in Java and then we
commit the row and then we go to the
next line and you will find that if you
do this it's actually quite a bit slow
as if you would just move this commit
outside because of this extra work that
we have to do with this extra writes
that we have to do on disk so let me
just quickly demo this to you what this
actually means
and I'll bring up here a little where's
my mouse again
where's my maps there it is like heaven
let's eat and I will bring up here a
little there again tool that a colleague
of mine wrote called DB monitor which
essentially just graphically visualizes
what's going on on the database so that
kind of makes it easy that's you gonna
we can connect yeah there we go
that just shows us what's going on so
this code that I just showed you I have
it here as well so here again you know
here we do you commit outside off the
loop we call it seek home of committed
ant and then further down further up we
have the same thing we call it commit
every row right and we commit here at
every row and then the insert into
statement use the same it's very trivial
insert into statement and we just run
this and take the time before enough
then see how long it takes and then
actually also see what's going on on the
database okay
you guys ready
everybody asleep okay sounds good as
well I'm ready
well I kicked this off let me get this
in the foreground so here we go down
here we will see that it will hopefully
pick up 10,000 rows nevermind my java 8
launcher and there we go we see some
stuff coming up here so now my 10,000
rows are running so insert 10,000 rows
and I commit to every row right so row
in commit throw in commit and what you
will see there actually is that it took
about 15 seconds to execute this and up
here we see CPU usage Andreessen all
this orange thing that we classify here
as commit that essentially means waiting
for this log writer to write those
journals to disk to to the files to the
reader logs and when we look out down
here now we will see that let me make
this bigger again here we will see that
actually this run took 15 seconds
committing every row and only 4 seconds
when we actually didn't commit every row
where we actually just told the database
hey write all those 10,000 rows keep it
in memory all right and then we commit
at once and I we just right through to
this once so we have to actually quite
some time saving I run it one more time
that you can see that I'm actually not
making this thing up so it should vary a
little bit so we see here again the
spike coming up and we see quite a lot
of time is in this wait event log file
sync over 50% of that's exactly what
we're talking about before so see
actually yes my session for my
application point of view is waiting for
the database yes I'm running out of
those loops etc and you know the
database is taking its time but actually
the database meanwhile only has 50% of
its time actively processing on CPU and
50% of its time it's actually waiting
for you for the disk to write this
journal to disk so here we got this
again fifteen point nine seconds and 3.5
seconds right so we see here quite a big
drop
one third of the time essentially going
on right and just read this one more
time why not because I was like to run
thanks three times you know I was making
sure that you didn't have a happy
outline or a bad outlier or something
like that but the picture always remains
the same right so there's two approaches
to this alright so one would be you
could turn around and go like I am
committing every row I don't care you
know to your database so that those log
files links disappear yes you could do
so by having faster desk so you're now
getting the DBA over yourself actually
to to tune the database all those log
files but actually the real approach it
would be well perhaps you know take out
those commits from the from the loop or
turn off auto commit if you have turned
it on and just commit at the end of the
loop and not only will you get rid of
this weight event but it will actually
get much more performance because you
also tell the database less to do and in
this thing and don't forget that every
time you commit it's a round-trip right
so and there's one example with you
commit inside the loop we go
database insert my row database says hey
I've done it
database commit database goes like hey
I've done it that's for network ground
troops mobile the other ones only to net
program chips hey and sore throat hey
ants at the road down rather than only
after 10,000 times doing that 20,000
metric round trips we give two more
round trips in commit yes done or one
more round trip I should say so you know
keep that in mind as well we actually
here also not only cutting out database
wait time but we also cutting out quite
a bit of network wait I'm righteous
Chetty
between those two so that can get you a
long way and therefore jump back here
into my slides
so you know it's like well attested
before 1516 seconds you know my laptop
here
of course that's unit a bit so that we
will make sure that we see those wake
events going on with about three seconds
to execute if you commit outside so now
people come up to me and say well that's
great so should this me does this mean I
should never come in right it's like
obviously that means writing to this
it's really bad as I go it does it mean
I only commit I don't know I'd be full
night of full moon or whatever no the
answer is actually you should commit
mine your business logic requires you to
as simple as that right so it's like if
you have something like a batch process
and you run through you know I don't
know five gig of file said you have to
load it or perhaps even hundreds of
cakes of fast to load this in well the
work is basically re-execute Abul so if
you die somewhere along the way you can
actually you know they are still have to
file and you can kick off again so do
you need to commit every row probably
not should you commit at the end of the
file well maybe also not maybe you find
somewhere logical path in between right
where you say hey we process the million
rows and we commit we process the next
million rows and you usually see this
manifests itself where you have a little
table where you basically when the
father arrives you insert the row and
using your father's their status knew
nothing process and then while you
process the file you actually keep track
and say okay this father went to three
million rows right and with the same
transaction that you actually modify our
insert load those three million rows you
also modify this counter and saying okay
commit nuts there and if you fall over
you can pick up there so that would be
one way however you know there's also
non restartable workloads so if we go
back to payment transactions well you
probably will have to go want to commit
after every row because every row means
a payment transaction going on and you
don't want to have yourself waiting for
a payment until myself and somebody else
bought it or something like that right
so you want to essentially keep the
latency as small as possible and
therefore commit as quickly as possible
so then you are back to what we said
before right if you have to commit every
row well you know make sure where you
can save some time somewhere else where
you can change down latency and so forth
and but importantly as well this is
sometimes missed out asset commits mean
metric roundtrip
all right so Matt network although slow
are all the fast are not the fastest
memory access right or even context
switches from use of the system space in
the CPU and so forth so then out be
aware of that every time you ask to
commit you go with a network you have
immediately three milliseconds
round-trip usually I'm a good Network
right maybe you get out a little bit
less sometimes it's way more just
wasting the term on the network really
right and then of course with Java and a
couple of other programming languages
turn auto commit off it was very funny
because last week we had this debate
going on on Twitter yeah all to commit
to always have it on right it's like why
would you care is that your and I was
like only a fool puts on auto commit we
should get rid of all to commit it's
really really bad and you know it's like
I had a funny little fight over the web
at the end I decided my life's too short
to continue like that but remember or to
commit could be your enemy because it's
on by default
especially for Java other programming
languages a lot of database drivers have
the same thing as well so just be aware
but by the JDBC standard by default it's
on for any relational database you might
want to turn it off otherwise you go
like I didn't commit you know and still
it's really really slow yes because the
driver did it for you and you know every
time you sent the insert or one okay
moving on to another fun topic row by
row was slow but slow so now we took the
commit out of the equation but there's
actually still further optimization that
you can do here this gets where it gets
a bit more interesting and slow by
slowest from a former colleague of mine
Cronkite very popular very famous for
this quote it's like well if you do
something real by row just means slow
but slow but sometimes we in the
iterative
programming thinking you just think
about this it's like you know it's a I
get an array and I loop over the array
for each group etc of course I would do
it now we've lambdas that have changed a
bit but the same is actually quite true
for the database of world as well so I
always start with an example here and
everybody goes again I mean what are you
talking about right so what would you
rather do if you would have you have a
table purchases or I don't need we just
want to know okay what are all the
purchases done since there was the first
of September 2016
woods what would you rather do would you
select each purchase each row from the
time on fetch it down over the network
and then sum it up in your Java code
what would you just do a statement way
so I select some from everything and
then just fetch one result back and
everybody goes usually well what's the
right one any any hints any pointers see
little people second exactly right
everybody goes like well of course you
would do a sum right I mean how stupid
do you have to be not to do it or well
funny enough that I had to do test ones
where they actually came up with the
answer being the first one and then they
said I want Gerald how did you know that
this to some function that was like
there's documentation so that makes a
lot of sense right obviously you would
do databases say you know have a lot of
data and it you have a query language
with it right so you would go ahead and
say hey if you want to aggregate
something sometimes some some something
up of course you would just say you know
select some blah and then you get off
the one row back you will not just fetch
potentially 10 million rows or the
network just to loop through it in your
application makes a lot of sense so this
is the right answer the second one
however a lot of people actually do not
know that this also applies for when you
for example do inserts that you can do
the very same thing as well and you know
this goes back to a fundamental
difference of how we think when we
program and some of us program in the
imperative methods and how databases
work and databases work best what we
call step based processing so databases
you know are built on set based theory
and everything and they work best when
you say here's a task to perform and not
perform it over all that data and not
here's a task to perform at now get a
row and do the task and get a row again
etc now for the sequel statement I
showed before it's very very simply they
actually you do set-based processing
without knowing where you say select
some from value from purchase where that
set-based processing you tell the
database scan all the data you have to
do and just give me the answer sometimes
so for instance statements and so forth
we don't do this where we actually do a
very iterative processing where we can
like okay next next next next
databases will always perform or almost
always performance should never say
almost mostly perform better and
set-based processing when you apply this
and you know the nurses next question of
like well we said we don't put any logic
into the database you know everything is
outside the database your database
agnostic etc frankly my opinion is you
should just put the business logic where
it makes most sense if you're crunching
a lot of data you might want to put it
to the data layer if you do a lot of CPU
heavy processing you might want to do
this on the application and avoid net to
ground trips and so forth right but will
necessarily say one is really really
evil and the other one is the best but
really just put it where it makes sense
and in general food database databases
anything you can execute in a single
sequel statement this is most likely to
be the most performance because it is
essentially a declarative language you
say I want this and then the database
will do many many things many
optimizations to come up with something
that we call execution plans and so
forth and of course you also avoid
unnecessary round trips with that so
long explanation let's get to code again
same example this is exactly the same
code that we have before we do insert
into statements and then we have this
for loop over 10,000 rows and we commit
at the end right so this is essentially
the reparative processing it's like next
row the next row the next row done
10,000 metric round trips how can we
actually speed this up well we can apply
the same set-based processing to our DMS
to our insert statements as we do with
sequel state or sorry select statements
and it's basically known as about
processing or in in Java actually we
call it batch processing so what changes
here is that
let me see what I can do this I should
just mirror my slides and I forgot how
to do this
see where's my box there you go
oops of course the wrong I see no that
was my cursor now get it so this is the
same code we have before I'm sorry I
just don't seem to get this working with
my suit where we said
execute the insert statement and then
commit and now what we change yours
actually rather than the execute insert
statement we say add patch and then we
have here a new line that says execute
patch and what this does is actually
will batch up your insert statements on
the driver in an array and then once you
say execute patch it will sent this
entire array over to the database and
say please execute this one sequel
statement with all those values at once
and we go from 10,000 metric round trips
to one round trip and then an additional
one with the commit so before we have
three seconds that it took to process
those 10,000 rows to insert those 10,000
rows and he guesses how much this will
take if we do set-based processing rough
numbers anybody
point one second okay so 100
milliseconds we get in the back anybody
thinks this is way too fast or maybe
even faster no obvious we will find out
so again same codes here as before
execute up the commit we call this
insert row by row and then we got this
batch batch or set-based processing down
here where we have add batch in and
execute batch and recall this insert set
paste and we check what this will do row
by row got this guy here again our
monitor here kick it off okay ten
thousand rows it started so pop we see
cpu same as before
I'd already finished 3.5 seconds there
in the back and then we wait up five
seconds and we see 73 milliseconds so
let me try to zoom in here right so 3.5
which we have before now we're down to
73 milliseconds inserting 10,000 rows we
went from 15 seconds before right where
we had the right time to set trigger and
we do this so let me try this one more
time you actually will notice that we
only had one spike it's so fast that
this 2 second window here doesn't even
populate this accordingly anymore so
there you go a little spike heparin 4.1
seconds we wait 5 seconds
71 milliseconds alright and that we'll
be doing now is we took out the network
round-trips 10,000 network round-trips
com2 down to one right we also tell the
database actually don't execute this
interest statement 10 thousand times but
executed wants with 10,000 values and
now the database can apply the same set
based process saying that you will do
when you do to select statement move the
zone it goes like okay here's my
statement I got my statement I got my
execution plan and now it's just three
executors were just with the array all
right
then it comes down through essentially
something that he would also program a
new programming language in Java where
you say okay here's the unity of work
right now just call this 10,000 times
but don't have to do all the legwork of
saying like I received a sequel
statement this incident to statement do
you have to pass the statement is
already there cetera cetera is a lot of
legwork for the database to do because
it's a multi-use a very highly
concurrent system right now so that
could be 10,000 people interesting at
the same time and it will still work
right because it does all these things
but we're set based processing on insert
statements you can see I run it one more
time again I'm not making things up here
you will reduce this wait time
drastically right and actually so you
see that the CPU time this little spike
that we have of the three seconds is now
gone that's exactly the database
processing on CPU writes during ok
insulin so statement is here I have to
do all this work so this is why I said
before it can be processing being on CPU
hopefully doing our work that we wanted
to do well in this case it actually
turns out the work we wanted to do can
be achieved in 70 milliseconds the rest
of the 3.5 seconds it actually does its
internal things right that rule is this
on CPU and processing but it actually is
not anything useful for us but actually
just for the system to work so just keep
that out there as well because a lot of
times you get those folks who go like
100% CPU utilization our code is perfect
right not a single way
there you go like yeah you have a
hundred percent CPU utilization because
you waste a lot of cycles doing
unnecessary stuff right if I if you
would tune your code you maybe you get
down to 20 maybe you get down to
whatever right they say just because you
haven't possessive utilization all you
probably never even get there anyway
because the scheduler will kick in with
the OS but that is not a metric of
efficiency right there just means okay
you're not waiting for anything you're
busy but are you busy doing your stuff
perhaps perhaps not okay
then the last example so here we have
those numbers again you know yesterday
took a bit longer it was about four
seconds when I did this on the train so
then people sometimes say rightfully so
okay so in 10,000 throws well what will
happen if an error occurs right unique
key violation or something right it's
like the road is already in there you
cannot enter the second row because the
same well yes actually your batch at
that point will be lost from the driver
the driver will essentially run through
it it will hiccup and will roll back
everything right so you've lost all your
modifications doesn't really matter so
much if it's the first row Meadows way
more when you roll nine thousand three
hundred twenty seven or something and
now you have to do everything again well
actually turns out a lot of databases
have remedies for this since a long long
time and this is what we call DML error
logging and essentially what you can do
with that is in your sequel statement
it's still the same insert into
statement you put the daguerreian log
errors and what it does for you is
essentially creates you a second table
for it and every time a row fails the
database will essentially write this
value into the down the table into this
error log table with the course why it
failed and what you Center your jiva if
this is your batch of 10,000 will go
through successfully and then you can
for example check okay and my error logs
did any errors occur will duplicate
values yes okay I can probably just fill
them away because that means the rows
already in their parent key not found is
that true ok have to do something so
this is one way how you can build around
and of course it doesn't mean you can
always apply this right I mean sometimes
you have to react to an error right away
but if you want to do reset based
processing and you know when we go back
to something okay I need to load ten gig
files into it or something like that you
can apply this technique to take
advantage of that makes a lot of sense
when you have a lot of errors occurring
in this scenario last but not least bind
very what
perfect on time I think so this is the
one other misconception that we
sometimes see is again personal
experience where we once at this debate
where a developer was concatenating
sequel strings with the values and a lot
of people said you know you shouldn't
really do that you should you spine very
or or something I was like well I don't
really know what that means but why does
it matter at the end of the day the
database will concatenate those strings
anyway for you I was like now that's
actually not quite how databases work
right so why do we have behind variables
and again every relational database has
those things we call them binder and
also think sequence Oracle's and bind
variables there was a database that come
through slightly different but it's all
the same concept and why do we do this
well because sequel statements by
themselves all strings
alright you pass in a string incidental
blah and bind variables allow you to
essentially to bind already the right
data type to good strings so although
you could technically perfectly fine
right and so the end to test and then
concatenate your values in you will send
your numbers and dates a string and then
the database will actually have to
convert them back to numbers and dates
right because only against the database
is a string but there is another caveat
to this when we actually come up with an
execution plan or a database comes up of
execution plans it has to parse a state
and write of the sequel parsing is
essentially or the sequel execution
essentially always looks like this
sequel comes in database parses it
database executes it all right so I'm
passing we have two different things or
two different methodologies we have soft
parsers and we have hard parses and
essentially what happens on the parsing
is they you send a sequel statement in
the database goes ahead and says okay
well first let me check what other
syntax is correct right you'll be right
select for more than you just give me an
arbitrary string which is not that it's
equal so let's do syntax check very
quickly string operation
semantics check do you have access on
the table that's the table exists this
cetera cetera
do you have permissions and then we do
what we call a shared pool check so
again the database is actually smart
enough to cache those statements for you
in memory right it does not do this all
the time it will say okay something's
checked past semantics check past you
allow to execute the sequel statement
it's a valid sequel statement now let me
actually see whether this statement
already exists in something that we call
shared pool shared pool because it's
shared amongst all the different
connections of the database and now it
will essentially hash this value or that
sequel statement and say hey if this
hash is already in the share pool I can
now just take the execution plan and re
execute this so we call this part of it
soft parse very have very fast happens
every time you execute a sequel
statements essentially it's just string
based operations there are some
performance optimizations in this world
or didn't really point out there just to
make the point the other important thing
about this is this is done by your
connection so your connection on your
app tier actually resolves to a session
process on the database so you have a
dedicated process usually over there and
that does the work for you so this
happens concurrently so it's not just
one thing you know you've funneled
through all your parsing but it's
actually happens concurrently of all
sessions there and it's very very fast
usually this is not an issue however
what happens now if we say well does
this statement already exist if it does
the database will just get the execution
plan and execute the statement and
actually we'll databases through there
or let me go on if it's not it will go
through the heart pause and this is
essentially where it goes from potential
optimizations and then Rose sauce
generation that's fancy database terms
or internal term support it will say
okay so you want to access that table
what's the best method of accessing it
do you have an index on there should
attack scan the entire table is a
petition there's five indexes on there
etc cetera the more you join and so
forth the more complex that gets right
and so the database is running through
this we have a functionality called the
optimizer where this actually runs
through all those potential
permits
of how to access your data it's not just
blunt and saying I just scan five
terabytes every time you sent this right
oh because there's a primary key on
there and you gave me a primary key
value I'm actually scanning the primary
key value and retrieves the router you
asked and so forth
and then it does actually something very
very cool in my opinion anyway what we
call the execution plan generation of
the ROS generation so what Oracle does
in that case once we have the execution
plan we actually dynamically generate
code seek out that we compile on the fly
and sometimes even assembly code and we
store this piece of code and the share
pool
so that's essentially your instructions
for running this and it's very very fast
we basically do just-in-time compilation
long before Java was even there but we
do it sea based and so you need a sink
compiler when you install tool anyway I
find this cool because I'm a geek but
that means it's very expensive right
so we have to run through all those
optimizations we have to find out the
right execution plan and then we compile
this code and we stored us in the shared
pool and so that means at that stage
while it's very CPU intensive if we have
to check the data dictionary have to
check various different things to come
up with the optimization and then
basically put this execution plan into
the share pool and a concurrent manner
so that's very CPU intensive a lot of
legwork once you have done that next
time the sequel statement comes along it
doesn't have to go through this anymore
because that's how this is the same
sequel statement we already know will
happen so let's just take this compiled
code and then just run it for you right
so this is what this actually happens
with the soft parse
now what this results to what this looks
like if you write the sequel statement
it says select text can test where ID 4
v 3 we take the entire text and then we
hash it and then we generate a sequel ID
if you then run the same statements but
just a different value 8 7 8 8 7 9 well
we take the entire text and we hash it
but you know now your text your string
has changed so therefore the hashing
algorithm will give you a different hash
back right so now you have a different
ID and we run through how it pauses
again if you use a bind variable
in Java you know we do this with
question marks it will essentially take
this hash it it comes up with yet
another sequel ID because it has not a
question mark but the second time you
come along and execute this with a
different value it will go like well
hang on you know hash it well that hash
already exists and that can do if we use
it again you can actually retrieve this
quite happily by a sequel yourself and
if you called viola SQL or sequel where
you see all of those statements
appearing as well as the secon values
and so forth
so last example here what we're doing
here is essentially we're on the reverse
now we select in this time 10,000 times
form a table and fetch every row but
what we do here is in the one case we
here concatenate that string together in
the for loop and then just fetch execute
and fetch the result so we go through
this hop passing because we have one two
three four five and so forth and the
second time we actually go ahead and
only create this statement once we use
this bind variable here outside the loop
because I can reuse it right it's the
same statement and then I actually just
pass in this parameter here and then
execute that statement okay
so last demo we call this odd passing
let me run this again
ten thousand rows that we now retrieve
obviously you know we should do
set-based processing to begin with and
we'll just fetch ten thousand rows but
just to show the point CPU spike coming
up there quite a lot of how pausing
going on so every seed color spray
here's our database goes for all the
step size and tree just told you and
actually will see this takes a long time
this takes about twenty seconds now or
something like that to run there you go
nineteen point two seconds we wait for
five seconds now we do this with a
prepared statement where we only pass
the statement and once and again
retrieved ten thousand rows we see CPU
coming up here as well but now actually
we're not going through this hard pass
and we see that actually we use less CPU
and we only used 6.3 seconds rather than
19 seconds so in the interest of time
and don't run this three times because
I'm already over I believe but I show
you the results here so again you know
just by avoid this heart passing by
using bind variables you can again
drastically reduce your latency to the
database as well as free up some
resources so I we didn't use that much
CPU because we didn't have to go through
so much processing so CPUs available for
others to use now there's another big
big thing to say about bind variables if
you use pine variables you also avoid
something that's called sequel
injections which is a security risk so
if you have a statement that you
concatenate together people figure it
out that you concatenate that together
you can potentially inject other sequel
statements as your value something where
you go like select blah where and then
quote and then the idea will ahead and
then you could to for example just pass
in is the value quote again semicolon
and then do truncate table or you know
give me all the the users and so forth
so that's not a sequel injections we
actually deliberately will throw an
error in our driver you're not allowed
to execute many statements as one but be
aware of that as well you should
actually use bind variables wherever it
makes most sense
and that's already the end of my talk I
hope you enjoyed it there's a couple of
you will get the deck afterwards a
couple of useful resources we have a
performance tuning guide that this
essentially takes you through this and
many many more things with a sequel
tuning guide that also explain to you a
lot of hi to write performance or sequel
statements themselves database
development guides with their talks
about ok how to architect and so forth
and then JDBC you related content and
the code I've just shown you is up on
like interpreter so you can't just go
ahead it down with it as well thank you
very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>