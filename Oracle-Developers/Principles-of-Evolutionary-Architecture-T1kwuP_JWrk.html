<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Principles of Evolutionary Architecture | Coder Coacher - Coaching Coders</title><meta content="Principles of Evolutionary Architecture - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Principles of Evolutionary Architecture</b></h2><h5 class="post__date">2015-06-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/T1kwuP_JWrk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Rebecca Parsons I'm the chief
technology officer for thought works and
I want to talk to you about evolutionary
architecture and so primarily I'm going
to start by hopefully explaining
although given the fact that you're all
here hopefully you understand why you
should care and then talk a little bit
both about how this compares with
emergent design because while there is
certainly continuum between design and
architecture I'm going to be focusing on
a very specific part of the problem then
I want to go over some principles that
underlie evolutionary architecture how
we should think about it and how you can
start to explain it to people about why
this isn't some crazy cowboy thing that
is going to destroy any semblance of
system stability and correctness within
your architecture and then I want to
focus on some specific techniques that
that show various aspects of these
principles and help is actually
implement an evolutionary architecture
and then I want to close it off by just
kind of bringing it back into these are
some concrete steps that you can
actually take we can also start a pool
to see how long it's going to take
before I trip over my little leash here
because I do like to wander so first off
why should I care why is it important to
think about evolutionary architecture
what's wrong with the current system and
the first problem really is that the
expectations on the pace of change are
radically different when you could have
when you could think about long business
cycles and long release cycles you could
actually do more in terms of putting
some long-term plans in place we used to
talk about three year plans and five
year plans and most people in the
business world now laugh at the fact
that you could even conceive of what
your business might look like five years
from now because business model
lifetimes are also shortening it used to
be that you could think about being in a
business for a decade or two or more and
that's simply not the case anymore and
it's getting harder and harder to
predict the future it used to be when we
looked at systems that we had a pretty
clear
standing of what path those systems
might take and how our business might
change and what we might have to do to
support our business as things change
and it's harder and harder to predict
the future I've never been any good at
it but I'm getting even worse now and
finally when you think about how fast
things move if you miss your window of
fame you can miss every revenue
generating possibility that you've got
and there are all kinds of charts that
show you know new ideas start out and
then there's this huge spike where
everybody pays attention and then either
you you continue on that path or people
say oh yeah that was fun for a little
while but i'm not going to hang around
if you have the ability to monetize that
spike you can take advantage of that
window of fame if you're not prepared to
respond to that then you're going to
miss it and for many ideas you're going
to miss any possibility you had of
capturing any monetization from your
idea so this is why you should care this
is a different kind of world that our
systems have to operate in than they
used to and so we have to approach
creating these systems in a different
way so what about design and
architecture obviously there is a
continuum and some things that you might
consider design in other cases might be
architecture and then what's this
evolutionary versus emergent so when I
talk about evolutionary architecture I
am talking about the fact that we have
an understanding of what constitutes
good we have a goal and if you think
about evolutionary computation things
like neural networks and genetic
algorithms there is an objective
function there is something that tells
you specifically what constitutes good
and you're evaluating all of your
possible responses to see how you can
move towards that sense of goodness
emergent on the other hand is a whole
lot more ad hoc what is actually coming
at me and how am I going to respond to
that and we have some vague notions of
what goodness looks like in that content
and when we think about emergent design
when you're thinking about I'm going to
code my my object system in a particular
way and now a new requirement comes in
and now I want to refactor it because my
design is emerging from that there are
some similarities to evolutionary
architecture but what we're talking
about is at a different scale and it's
important to keep in mind and this is
one of the ways you can start to help
more traditional architects who are very
nervous at this concept of evolutionary
that you still have a fitness function
you have something that tells you this
is what constitutes good and that is the
goal that you're heading towards and
that is the fitness function that you
use to evaluate all of these
intermediate decisions that you're
making so this is the distinction we're
drawing it's very important that you can
specify for your particular application
what constitutes good do you have to
worry about scalability and security
what level of scalability what level of
data protection what level of
availability this varies widely from
system to system and we need to take
that into account in our fitness
function now design an architecture
again we have a continuum here I'm not
going to be talking about things like
object design I am going to be talking
about the more architectural components
how do my larger blobs communicate with
each other how do they coordinate their
activities there is there are parallels
between this and an object design but
I'm thinking at a much higher level
we're talking about the LEDs here we're
talking about how we decompose our
application what our data life cycles
look like all of these things are more
in the realm of architecture than they
are in design and so I'm going to look
at evolutionary architecture how do we
talk about architectures in such a way
that we can continue to evolve those
architectures emergent design is on a
very different level so what then are
the principles of evolutionary
architecture I'm going to talk about
these different five principles when we
make decisions
what we are trying to achieve when we
think about architecture and design how
communication is going to occur what's
the role of testing this is after all of
that works talk and has to have
something about testing in it and then
Conway's law how are we going to think
about the organization and I'll go over
each of these in detail so for those who
are scribbling you'll see these again so
the first the last responsible moment
what this is talking about here is when
we make decisions and we want to wake up
we want to delay decisions as long as
possible as long as responsible but no
longer now that seems a little vague so
let's talk about this a bit more first
why would we do it because this is going
to maximize the amount of information we
have available to make a good decision
makes sense the longer you wait the more
that you know it's also going to
minimize the amount of technical debt
that we accumulate within our system
because of a decision that we made so
for example if you know that you have to
have some kind of inter-process
communication and maybe you think it
should be a queue and you decide very
early on I'm going to use a very complex
queuing system that is going to require
my ops team to keep this thing supported
and such every single story that you
write from that moment on have to take
into account all the complexities of
that queuing system if you delay that
decision until you know specifically
what kind of queueing features that you
need you have erased that technical debt
we normally think about technical debt
either in terms of our systems at the
most current level or is my system messy
but there are other forms of technical
debt if you want to think about this
from that perspective it's any drag that
you have on your development process and
making decisions earlier then you need
to make them and incorporating tools or
functionality that you don't yet know
that you need puts a drag
on your development process and that is
technical data so how do you define this
latest or last responsible moment and
that's where your architectural fitness
function comes in when you have a
decision that impacts one of your
critical drivers you want to make that
decision you want to be as sure as
possible of that decision if it's going
to have an enormous impact on the
architecture of your system which many
of your critical drivers will you'll
probably want to make that earlier not
necessarily because you know anything
yet but because of the impact it's going
to have on the development process and
if you don't make some of those
decisions early and start thinking about
those drivers early you might have much
more rework than you would have
otherwise you can delay decisions that
don't have a lot to do with the LEDs
that you care less about because you can
make compromises there in a way that you
can't on your fundamental drivers so
even though we don't suggest big upfront
architecture one of the things you do
have to do very early on is decide what
those architectural drivers are what are
the things that are most critical for
your particular application I did have
somebody tell me once it does not matter
about the application to be a
responsible for computer scientist all
applications should be fully redundant
and fully back up in all of this inside
I'm sorry but the sandwich ordering
system when there are delis all along
the street is not mission-critical and
it doesn't make sense from the
perspective of business value to put in
a lot of effort to make that thing fully
redundant if I have to put in my
sandwich order twice the world is not
going to end so it is important to
decide which of these illa tease our
most important because we do have to
justify the architectural decisions that
we're making in the costs that we are
incurring because of those and so you
can have a discussion on which of these
LEDs are important and use that fitness
function then to prioritize your
decisions
this means that this is not based on
guesswork we have put in thought that
says why are these particular ility is
important to us and we've had that
conversation with whoever our
stakeholders are this is why we think
that's important this is why we think
you should care about this this is why
we think you should pay money in terms
of development time or tooling or or
whatever else you're putting into this
to say okay this we have to do so that's
the last responsible moment now this is
one of the first places of traditional
architects get very nervous it's like we
can't delay any of these things well yes
some of these things you should delay
some of these things you should not the
second point we want to have as one of
these illa DS that we talked about along
with stability and recoverability and
all of that we want to talk about
evolvability we want to talk about this
notion of thinking about our
architecture from the perspective of its
ability to change and the thinking about
ability to change does not mean
predicting the future does not mean
figuring out what I'm going to want in
nine months time and building it right
away that's not what evolvability means
because that relies on predicting the
future instead we want to think about
how do we make this system as adaptable
as possible and the first thing is to
think about how we break up the
responsibility between these various
components and actually one interesting
way of thinking about this is how does
the business the stakeholders think
about what's happening in their system
because they are going to conceive
processes and they are going to conceive
services from the perspective of how
they view the business and so rather
than thinking about this from the
systems perspective if you think about
this from the business perspective you
are more likely to come up with units
that the business is going to want to
move around if you think about this from
the systems perspective you're going to
construct something that's very easy to
build systems
not necessarily reflect what the
business is actually trying to do a very
important part of this is considering
data life cycle and ownership we went
from a time where we have these big
monolithic databases and and the the
goal really was to put as much in those
big databases as possible and then all
of a sudden you've got data that is
owned by different parts of the
organization there is a fortune 100
company that we worked for global reach
and the only person in the entire
organization that could actually have
the authority to sign off on a data
migration was the global CFO now he had
no interest in looking over the data
migration of the customers that are
scattered all over the world but because
of the way they had architected their
data he was the only person who had the
organizational authority now he did the
smart thing and hired somebody to go off
and do it for him but the point is these
pieces of data do belong to different
parts of the organization and even
though it might be convenient from a
systems perspective to keep them all
together if we can separate them out we
can talk separately about protection of
data we can talk separately about life
cycle archival backup all of those
different things that come into how do I
treat data so think about data life
cycle and ownership appropriate coupling
this is not loose coupling this is
appropriate coupling sometimes because
of your realities the things that you've
decided are important you might want to
use tight coupling so you need to think
about how it is these systems are going
these various components small C
components how they're going to interact
how does the business think about them
and then decide what's the coupling that
I would like to have loser is better but
there might be things that draw that
drive you to tighter coupling so don't
just go in with the mantra that
everything has to be loosely coupled an
asynchronous because that's not
necessarily right I've been in this
business a long time and I have not
found any hard-and-fast rule that it is
always right to do
anything excepts think about the problem
lightweight tooling and documentation
one of the challenges for agile in all
of its forms whether it be agile
architecture agile software development
is no documentation and that's not true
it's not that there is no documentation
it's that it's the appropriate level of
documentation it is documentation that
has a specific purpose it is
documentation that has a chance of being
kept up to date because it is central to
something that is happening within the
system if you're going to write
something and put it on a shelf and
nobody is ever going to look at it why
did you write it a lot of what we're
talking about here is let's not do work
that doesn't have to be done whether it
be write code that doesn't need to be
written a lot of the times when we try
to predict the future we end up writing
code that never gets used some of the
more horrific statistics are the
percentage of code that actually gets
used more than a few times and it's you
tends to be about ten to fifteen percent
of code over systems so the other
85-percent your business is paid to
write is pain to maintain for what
nobody uses it same for documentation so
we want to think about evolvability as a
first-class architectural principle but
then we also want to make sure that our
development process supports
evolvability as well and one of the
things that we have found to be very
important is internal software quality
and you focus this really on how easy it
is to change the system and what does
that mean it means you think first about
how easy it is to understand what's
going on you don't need to maximize for
how quickly you can write something so
much as how quickly you can change it
and that means how quickly you can
understand that six months after you've
written it or someone who didn't write
it now it's one thing when you start in
a green field and you can have all these
wonderful principles what happens when
you come into an established code base
when you've got hundreds of thousands of
lines of code or millions of lines of
code how do you know where to start how
do you take something like that when
your business is decided I T is now a
critical asset for me I need to be able
to leave all these systems quickly get
going i T what do you do well the first
thing you need to think about is where
are the hot spots you can run metrics
over the entire code base and you're
going to find some truly horrific code
we've actually had overflow errors
because the number of violations and
some of these code bases got so high
just because it's bad though doesn't
mean you want to fix it because you also
want to look for things like where the
check-ins happening what do you know
about the business plans over the next
few months you know if something hasn't
changed in five years even if it's a
disaster area maybe by the time you have
to change it it's going to be so
obsolete you can just rewrite it the
other place I like to look as I like to
talk to the ops people and the testing
people and the support people and say
when they tell you they've changed which
part of the system when do you get
nervous when do you get scared when they
say oh we're rolling out a release of
this and that's a good clue to some
place you ought to focus on so what's
changing what you have a reasonable
expectation that will change and what is
actually causing real problems and marry
that over your code metrics and that's
going to tell you where to start often
when we talk about metrics will get the
question what is the magic number what
is this number that says my code has to
be this level of quality and it's not
just because I'm a consultant that I say
it depends thought works for since the
company started has been giving code
tests and we have a database of every
recode sample that was ever sent to us
along with the subjective evaluation of
how good that code was so we ran these
quality metrics over them and what we
found for these various problems which
have changed over the years is that
there's a minimum level of technical
complexity to the problem that any
problem that is has a has a better quote
unquote better software Quality Score
does not have enough logic in it
actually solve the problem and that
floor varies from problem to problem so
there is no magic number that applies
across all of ours because every problem
has a different set of essential
complexity to it and that's in part what
these software quality metrics are
measuring and so it's actually more
important not to focus on the specific
numbers but on the trends so you want to
measure this thing continually and keep
an eye on it as it's improving the other
benefit of this is one of the other
questions I often get asked is okay I'm
going to I'm going to pay my development
team is going to take time to clean this
up how do I know it's not going to get
messy again and the answer is if you're
continually monitoring it you know when
things are starting to go wrong you can
make a conscious decision about allowing
the quality to deteriorate because
sometimes that's the right thing to do
but at least you know what's happening
and you can make it clear that something
needs to happen now a big part of both
architecting and developing for
evolvability is to think about what is
the cost of changing my mind
particularly on some of these design
decisions that are critical to your
drivers you want to make them a bit
earlier because of the impact that
they're going to have on your system but
you don't have all of the information
you don't want to do this for all of
your design decisions but for some of
them you might want to think is it worth
me adding an abstraction layer is it
worth it for me to do something that
makes it easier to change my mind as I
said you don't want to do this for all
your design decisions because once again
then you're just putting all kinds of
complexity in but if you have
this sense that this is a critical
decision and there's information that
you don't feel like you have build
reversibility into that part of the
system so that you can change your mind
so that's thinking about evolvability as
a first class design development and
architectural decision now the next
thing I want to talk about is how we
just design our communications for a
time there was this feeling like if we
had any data that was being communicated
every time it went out maybe it should
be Delta's maybe it should be the whole
blob and there was this back and forth
what you want to actually think about is
how am I structuring the communication
and if I am thinking about what does it
mean to be able to adapt the first thing
you want to be do is be conservative
about what you sent as soon as you put
something out as soon as you publish an
API as soon as you put data in a message
there are going to be people who are
relying on the existence of that data
and that makes it much harder to change
you also want to be liberal in what you
receive caveat sasai dabao making sure
that you don't open up security
vulnerabilities you shouldn't pay
attention two parts of a message that
you don't need if all you need to
collect is the zip code because of some
kind of profiling that you're doing
don't parse the entire address don't
parse the rest of the message because
then if those things change your system
will have to change even though you
didn't need it this isn't going to solve
all the problems we can't prevent
breaking changes but we can make sure
that the only changes we have to respond
to our ones that we ascend absolutely
have to not things that we are
accidentally connected to so only
validate what you need and this actually
holds for any information exchange there
was a system that I dealt with they
actually did the right thing with
respect to a package they brought in a
package they didn't tweak it until it
was unrecognizable
but what they did instead is they built
an entire suite they had a hundred and
eighty-five reports directly accessing
the packages database so even though
they could have upgraded the functional
system without any trouble at all
because of the way they structured these
185 reports they had to delay upgrading
the system to get critical features that
they needed because of all these reports
and it was simply the way they wrote the
reports they didn't think about the way
they were coupling these reports to a
package so you need to think about this
not just for messages but also for the
way that we're interacting with
databases another thing is when do you
decide this says I know this is near the
bottom used version changes when a
contract must be broken sometimes you
know this is going to be a volatile area
of the system and you might actually
incorporate versioning on your message
contract earlier than you might
otherwise again this is a decision to
make not all api's need to be versioned
in the same way just think about when is
the right time diversion it make sure
that you think about as a producer of
data who is relying on me and what level
of control do I have over that if if the
only consumers of your data are in your
small group you've got a lot more
flexibility than if you don't even know
who might be accessing your data okay if
there's one thing I want you to take
away from this talk it's this slide
architecting for testability in our
experience if you think about how easy
it is to test something you will tend to
end up with an architecture that is
easier to change and easier to
understand because there is a
correlation between what makes something
easy or difficult to test with what
makes it easy or difficult to change so
aiming towards testability results in a
clean architecture it results in an
architect
sure that is far easier to change some
examples of this when you've got a lot
of business logic built into your
messaging layer it's very difficult to
test if you use your messaging
infrastructure for messaging it's much
easier to test you can separate the
components we've got the right kinds of
tools and you can think about separate
functional testing of the individual
components and then just make sure the
wires are put together it's also easier
to understand a system when you know
where all the logic is when you don't
have unnecessarily done necessary
duplication of business logic because
it's spread across various parts of the
messaging layer again business sensible
components if you if the system is
chunked up in the same way that the
business thinks about its data and its
processes it makes it much easier to
think and be able to explain to the
business how their system works we
actually had a CEO of one of our clients
in the UK he looked every month at the
functional tests tweet that was run on
his product because he said this is the
only way I know what my product really
does and because of the way the system
was architected he could actually
understand what was going on in the test
from the business perspective this is
the CEO of the organization out the CIO
the CEO so if you've got the right kind
of correlation between how the business
thinks about the data and the processes
that exist within within the business
you can write business sensible tests
that the business can understand and of
course there are lots of different
levels of testing I'm going to talk a
lot about contract testing a little
later on but you want to think about
testing at different levels and this is
going to drive your decisions on what
are the different what granularity
you're using to chunk up your system so
you want to think about what does it
mean to do an end-to-end test what does
it mean to test a particular area of the
business functionality
what is my unit test coverage looking
like all of these different things are
just as much architectural decisions as
they are development decisions and then
of course if you're going to be doing
this level of testing you're going to
need a build pipeline you're going to
need to think about how do i do all of
the different testing last sign that
says build pipelines support the volume
you need to think about the level of
automation you're going to have and be
able to run all of these different tests
so architect for testability you get a
nice results in an architecture that you
wouldn't necessarily expect but if you
concentrate on what it means to test it
properly you'll get a nice architecture
now finally what does it mean for the
organization so Conway's law paraphrased
is organizations design systems that
reflect their communication structure
and I often look very bright when I walk
into assistant and into an organization
I just watch who has lunch with each
other who talks comfortably with each
other and I can identify okay if you got
this group in this group trying to
integrate something it's not going to go
well because as people they don't talk
very well and if they're building
systems and as people they don't talk
well chances are their systems are not
going to talk well to each other either
so broken communication within an
organization and organizational
structure often results in broken system
integration and it makes system
integration much harder silos often
result in broken communication and it's
interesting because there is a tension
here when we think about things like a
center of excellence I want as you know
a center of excellence for maybe my
integration tool you are building
organizational silos I walked into one
of our clients and there was no way any
piece of business functionality could
get released by that organization unless
every single group participated because
of the way they had chunked up the work
you couldn't have one group responsible
for one piece of
business functionality because of the
because it was spread across the entire
organization every delivery every
deployment meant everyone in the
organization had to be involved in it
and all they would have had to do is
change the responsibilities to more
closely align with the functionality
rather than the systems to be able to
get around that so if you're building
silos based on technology into your
organization you're making it more
difficult to make systems work
healthcare.gov to me is a classic
example of this all you have to do is
look at the way those contracts were
parceled out and anyone could have told
you it was going to be a nightmare it
was too easy to write things where your
individual work happened properly and
the system as a whole failed and so the
last line just says if you don't want
your product to look like your
organization change your organization or
your product one of the examples that
Martin uses on this someone who is going
to be writing a compiler was given a
team a crack team of specialists but
scattered across four locations and he
said okay that means i'm writing a Ford
pass compiler doesn't really matter if I
try to design a three pass i'm going to
get a four pass anyway so I might as
well upfront think about that's how I'm
going to design it so if you don't like
what your organization looks like and
what that applies for your product
you've got to change one of those two
things it's very difficult to fight
Conway's law now I want to spend the
rest of the time talking about different
techniques that we use in various parts
of the technology stack that that
implements some of these and I'm going
to again there's a slide for each of
these so you don't have to scurry around
writing these down and the first is
database refactoring now one of the
interesting things about database
refactoring I've been involved in agile
software development for quite some time
and there's been this fascinating
history first it was ok this is ok for
the developers but the business analyst
still have to write you know
all encompassing use cases and then they
they got into stories and then the QA
people said well I can only test the
entire system and now QA understands how
to do it er ative testing and now you
got the architects saying oh but I still
have to do my big upfront design and the
database people who said well of course
you can develop however you want but I
need my entire enterprise data model
constructed before we start each one of
these different disciplines is learning
that there are ways to chunk up your
work and yes it does require thinking
about things differently and yes
sometimes it is less efficient because
if you absolutely knew the future you
could of course take the straightest
path there but we don't absolutely know
the future and so database refactoring
is a way to think about what the
critical problems are if you do go live
with a database that you know you have
to change so what does this look like
basically the refactoring word is
important here refactoring is one of
these very abused words and it off it
means i need to go completely change my
system because I messed it up the first
time but what refactoring actually means
is making a small series of changes that
follow a pattern and so the database
refactoring book lays out a series of
patterns for database changes and you're
going to decompose your big database
changes into a series of small changes
and each one of these refactorings you
can think about it as either a tuple or
a triple if you include the code changes
that you need to make because the first
thing is what is the change to the
database and by adding a table am i
changing something but the second is
what do I have to do to migrate data to
support just that part of the change and
so you're breaking up not only the
database change but also the migrations
that are necessary to support this and
then you can compose these changes so if
I've got some massive Reese
true of my database I decompose it into
these individual small refactorings just
like what refactorings are supposed to
be very small changes that that don't
change behavior and then you compose
them all together and that means you can
test these things you can test the
migrations in these small pieces as well
and for anybody who's done major
migration conceptually it never sounds
like it should be that hard but the
problem is data is messy and by chunking
up not just the database changes about
the migrations it's much easier to find
okay well where is this magic column
that it was put in 15 years ago that has
this you know is used for for different
purposes and the migration doesn't
really take it take that into account
yet you can find where those things are
and so it actually in mex your
migrations go much more smooth smoothly
because you can identify the data
problems which are making the migrations
go wrong and then of course you can
version control the changes so then you
can know which version of the database I
have running in which of my different
environments and then finally you apply
these various database changes to your
succession of environments so you can do
individual testing of the migrations in
the development environment of the QA
department environment you can apply
these data migrations because hopefully
you're using production data and your
test environments as well and you can
continually test so this says apply in
the various environments during
production promotion so that's database
refactoring and what it does is it
solves one of the major problems the
dba's have to cope with when they're
thinking about evolutionary databases
which is what do i do once it gets in
production because the data I have in
production is valuable and I need to be
able to migrate that and that's work
that I wouldn't have had to do if I
could have figured out all of this
database stuff at the beginning except
because you can't predict the future you
couldn't anyway so this technique
actually allows you to incrementally
evolve your database so continuous
delivery is the second tech technique
and simply put continuous delivery is
the ability to put your system into
production whenever you want to whenever
the business wants to and in order to do
that there's all kinds of things you
have to do you have to make sure that
you can in fact reliably build your
system you have to make sure that you
can easily deploy your system and that
means automating lots of things
automating your environment so that you
understand what the configurations are
rather than having snowflake servers
everywhere you want to automate the
configuration so that you know once
you've installed your system level
software you've got all the
configuration settings appropriately
appropriately set so automate build
employees continuous integration and
then automate the testing at all levels
so that you know that you can reliably
put this thing into production
fundamentally the goal here deployment
should be boring the number of times
I've been involved in these deployments
where there's a party at the end because
we didn't crash the system this time you
shouldn't be celebrating the fact that
you didn't crash the system you should
be celebrating the functionality that
you're putting out and you want
deployments to be boring and the only
way you can ensure that at three o'clock
in the morning some poor human is not
going to make a mistake is to automate
it so you automate everything there are
important consequences for this that
means among other things that your tool
selection should look at tools that can
be there that that you can automate the
deployment for some of these wonderful
fancy gooeys are pretty marketing
materials but it makes it very difficult
to automate the deployment so you want
to think about what it's going to take
to support automated deployment even for
your tool selection because you want
deployment to be boring now this last
line says just because you can release
at any time does not mean you have to a
lot of organizations panic at the
thought of doing what some of these
internet properties do when somebody
shows up and on day three they're going
to write a piece of code that no one's
going to look at and it's going to end
up in production being used by real
users and they're panicked at this
thought there is absolutely nothing in
that continuous delivery book that says
there's anything wrong with manual
review just because it could flow
through quickly doesn't mean it have to
has to but isn't it actually a good
thing if the business decides to make a
critical change that you know that you
can safely deploy it you know that you
have got the minimum level of risk of
deployment so just because you can
doesn't mean you have to but it's
important to be able to do it the whole
purpose of this is to minimize the risk
of deploying software that means that
you can deploy it more quickly you can
deploy it more safely and that means you
can start to enable some risk taking in
the business as well I want to
experiment on something it might not
work you're not going to risk an
experiment if there's a high probability
that deploying new software is going to
crash the entire system but if you know
you can safely quickly and quickly
deploy and roll back if you have to you
can allow the marketing department to
experiment on some things to see if
maybe something might work better or
have your sales organization work in a
different way so that's the point here
we want to increase the probability of
success of a deployment so that what
everybody's talking about at the end of
a release is not the fact that we didn't
crash the system but then we actually
got something useful out there for our
users and this one doesn't doesn't stage
service choreography this is another way
of thinking about how we want to compose
different parts of our system
for a long time we were talking about
service orchestration and we wanted to
have a very structured we wanted a
command and control sort of arrangement
for how our services interacted with
each other and the way where this
metaphor comes from think about the
conductor of an orchestra there's one
person he's got the magic baton and he
has complete control of that Orchestra
and that Orchestra although it has each
of the individuals has their work to do
there's the master control who is saying
this is what the beat is this is what
the volume should be you've got
centralized control now think about a
dance number yes there is choreographer
and that choreographer has laid out a
vision for how this performance should
unfold each of the individual performers
knows what their role is and then the
choreographer steps back you don't see
the choreographer in front of the ballet
troupe as they're performing that
choreographer is off to the side because
those individuals who are doing the
performance understand what they're
supposed to do and they are working in
their local interactions to deliver on
the overall vision of the choreographer
so that's the that's the objective of
what we're trying to get to with service
choreography rather than having some
centralized control system you want to
decentralize that control because it
makes it easier to change if I have
centralized control and i want to change
one minor interaction over here i've got
to touch the centralized control
mechanism that's the only option I have
by decentralizing it I can limit the
scope of the impact potential impact of
the changes that I make now this does
introduce new kinds of failure scenarios
and in a well-practiced Orchestra if the
conductor is doing his job and the
people are doing their job you know
exactly what's going to happen there's
very
little probability that something is
going to go horribly wrong yes a string
on an instrument might break etc but in
general you know how the script is going
to play out when you've got a
choreographer there's a lot more
variation going on because you don't
have a centralized control structure so
the individuals have more autonomy and
that means things can go wrong that
really couldn't go wrong in the
centralized scenario so we do have to
think about different kinds of control
of different kinds of failure scenarios
and we have to think more explicitly
about what's the communication
coordination that has to go on between
these individuals on the other hand
because we have this level of
decentralization we have the ability to
structure and change things to match
what our needs are rather than also
having to think about what's going on in
that master conductor so that's what
service choreography is all about we're
hearing a lot more about things like
microservices you could you could argue
this is the next wave of
service-oriented architecture what many
people when we first started talking
about SOA thought was possible that's
the way has almost become a dirty word
because of how it's been co-opted in
some places but a lot of this is the
vision that we first had for trying to
decompose these business processes
around the functions that are being
performed and service choreography has
an awful lot to do with that kind of
contract testing so again if I'm going
to allow these different parts of my
system to evolve independently if i'm
going to have pieces of functionality
that are responsible for different parts
of my organization how am i going to
keep track of this off I've got this you
know n by n communication problem if
I've got all of these different groups
working how do I know that these that
this whole system is going to work well
when I have all of this stuff going on
independently and all of this D central
is
and that's a lot what contract testing
is all about and basically you can think
about this as an acceptance test for an
interface boundary or an integration
point and that way you're developing a
system a you're developing system B and
we each have acceptance tests we each
have contract tests that represent what
are the assumptions a is making of B
system in B is making of a system and
you are in fact documenting this in that
acceptance test so it's no longer locked
up in the mind of the enterprise
architect who is the only one who really
understands how these different pieces
of the system fit together you've got a
documented and now system a can happily
develop on its own pace running these
acceptance tests and when something
breaks and you realize the change I'm
making my system is getting ready to
violate the assumption someone else is
using about my system what is the role
of an automated test it's to trigger a
conversation you know right then as soon
as you've implemented that piece of
system a is functionality that you've
got to go talk to system B because
you're breaking one of their assumptions
you don't have to wait until the massive
integration and all of a sudden poor
system bead you know falls down in a
heap because you violated their
assumption you don't have to rely on the
fact somebody remembered oh yeah every
time we change this thing poor system B
fall down in a heap you've got it
documented what this does is it allows
us to maximize parallel work system a in
septic system B don't have to line up
their delivery schedules they don't have
to line up their planning meetings
because system a and system B each knows
what the other one needs from it if you
use this as in conjunction with pasta
dough's law about how your systems are
communicating you're going to maximize
the possibility that system a and
substantive be are not going to hurt
each other even though they're not
talking much because we're the critical
communication is happening
through this contract test in those that
series of tests you know exactly what
system B cares about from system a's
perspective and anything else you change
you're safe now of course you got to get
the contract test right but those might
evolve over time as well as I said
earlier this used to be the role of a
lot of enterprise architects they were
the keeper of the knowledge of how data
flowed through systems and how things
either went writer went wrong between
different systems one of the best
Enterprise architects I ever met she
understood how data flowed through this
retail organization how the different
branches interacted what the problems in
the product catalogs were she knew all
those things in her head and as long as
she wasn't hit by a bus or somebody
offered her a whole lot of money to go
somewhere else the organization was fine
but this role of the enterprise
architect is one that really does not
scale well and this this contract
testing is a technique to start to pull
that information out of the heads of the
people who understand all the legacy
that exists in enterprises so
evolutionary architecture the critical
step here is to be able to define what
matters in your system for some it's
going to be security for some it's going
to be up time for some it's going to be
the speed with which you can bring a
system back up by having this
conversation early on when you're
talking about a system you can also
start to get buy-in from the business
about why you actually do want to stand
up to servers just in case as a retailer
one of your systems goes down on Black
Friday you want to be able to bring it
back up quickly and you can make that
case if you think about these are the
things that are important to my system
so define your architectural fitness
function
up front and this is not a technical
conversation alone this is a business
conversation what are the features that
the business cares about that have
architectural implications that's where
you want to start once you have that you
can start to say when do I have to make
particular decisions this might also
require you to convince your procurement
department to be able to move more
quickly on contracts potentially because
sometimes the reason you're making
decisions early isn't for anything other
than the fact that you can't get
anything out of your procurement
department for nine months but you want
to be able to have the conversation of
why it's important to be able to delay
this decision or why it's important to
make it now and then understand the
various forms of technical debt we
talked about this with respect to the
last responsible moment but we also
talked about it with from the
perspective of internal software quality
just because you've got a steep steaming
pile of mess over here doesn't mean you
have to fix it if you're not doing
anything to it it's not dragging your
development process it's not true
technical that the purest and me would
love to be able to go fix all of the
problems and systems but that's not
practical the pragmatist says I'm going
to focus on the areas where this
technical debt is hurting my
organization and that is in the areas
that are causing active problems or that
we want to change we're going to be able
to guide how we evolve the system on the
basis of those things and then implement
evidence-based reuse the number of times
I've talked with architecture groups
where even part of their their
performance review is how often are your
reusable components reused and so they
have to figure out across this entire
organization how someone is going to
want to use something so they can write
in advance a component that can be used
by someone else when they don't know
what the detail requirements are they
don't even necessarily understand what
the business priorities are in that
small context and they certainly don't
have the kind of personal relationships
the entire development organization
because they're so vastly outnumbered
and yet somehow they have to divine from
a crystal ball or something how somebody
sometime is going to need functionality
you can't do it rather look at
evidence-based reuse where you start to
see oh there are three different people
who are doing this kind of similar thing
now I want to look at what those three
things are what they have in common
where they differ and then I'll combine
them together and roll that back out
again so evidence-based reuse rather
than trying to predict the future and
then create and critically maintain your
testing safety net it is irresponsible
irresponsible to adopt some of these
techniques if you do not have the right
kind of testing safety net but if you do
you can very safely make massive changes
to a system because you know when you're
going to break something there are many
discussions about agile and how what
it's looking at it's it's irresponsible
it's you know ad-hoc it's Cowboys the
discipline that comes from the safety
net is what stops this from being so
dangerous if you have this safety net
you can make radical changes to your
architecture you can swap out major
components of your system stack because
you've got a testing safety net that's
going to tell you whether or not you got
it right and so you need that safety net
but with that safety net you have the
ability to evolve your system rapidly in
ways that you didn't have to predict in
advance so that your system can respond
to the business changes that your
business is asking you to support thank
you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>