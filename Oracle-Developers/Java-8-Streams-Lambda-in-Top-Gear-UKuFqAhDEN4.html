<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Java 8 Streams: Lambda in Top Gear | Coder Coacher - Coaching Coders</title><meta content="Java 8 Streams: Lambda in Top Gear - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Java 8 Streams: Lambda in Top Gear</b></h2><h5 class="post__date">2015-06-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/UKuFqAhDEN4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">we're going to continue the the
precession of lambda talks this
particular talk is about the library
support that was added as part of
project lambda and so when we started
talking about adding lambda to Java we
were mostly focused on the the language
features the language features are
important they're a foundation but if we
had just done language features and no
libraries we wouldn't have bet you
wouldn't have been able to use it
effectively on day one you would have
had to start you know built built
building up and so we looked at the
existing collection libraries and we
kind of asked ourselves if we were
designing collections from scratch what
features would they have in a language
with lambdas that they might not have
had in 1997 when these were designed and
kind of looked at the differences and
you know tried to figure out how we were
gonna retrofit this functionality onto
the class library so the the form that
that ultimately took is an abstraction
called stream so there's a new package
in the libraries Java util stream it's
actually relatively few classes in it
there's a few stream classes stream in
stream double stream and some utility
classes and that's really about it so
the surface area of this API is actually
pretty small but the density is pretty
high so we're going to talk about some
use cases for how the how the streams
library would get used and then we're
gonna dive a little bit into the
implementation and look at the cost
model and performance and parallelism
and some of the cool under-the-hood
things okay so you know what what
hopefully you'll come away from you know
this talk with is not just lambdas are
cool presumably the other talks of
lambdas have convinced you of that and
that's why you're here but actually how
can I start using this tomorrow when I
go download the developer preview of the
JDK and how is it that this is going to
make my code more readable more
performant less error-prone
etc so
like I said a brief overview of the
language features actually no skipping
over the lime juice you have you all
seen they any of the lambda talked so
far this year everybody good all right
because if you haven't you'll be
confused alright so one of the many
reasons to do lambdas was that it
enables better libraries so obviously
you're going to be able to build your
own libraries and you will build your
own libraries there'll be open source
libraries available but we've also sort
of given people to start by adding you
know and adding some features to the
collections and to other classes with a
deity within the JDK and so we've
integrated the streams streams
abstraction into all the collection
classes we've also integrated into
various other places in the JDK so you
can get a stream out of a bit set or a
bufferedreader or a random number
generator or various other places so
stream is an abstraction that you will
be seeing in a lot of different places
if you're building your own collection
like classes or container like classes
you should consider exposing a stream
view because then users will be able to
get the benefit of all the cool stream
operations okay so why do we care about
this we care about this because a lot of
the COBE right every day is expressing
what we call aggregate operations or
bulk operations you've got a collection
of things and you want to summarize them
somehow you know who is my highest
performing salesman who is my highest
performing salesman you know by region
or by state or let you know summarize
that trends in sales volume by month and
and region and item etc and historically
the way that we have expressed this kind
of logic is you go get your data from
somewhere from a database from a web
service or you know wherever dump it
into a collection loop over the
collection pick out the elements you
want summarize them the way you're going
to and then put the answer somewhere and
this works we've been doing this for a
long time this has some awesome
downsides one of them is it's
fundamentally a sequential process
because the the for loop in Java is
fundamentally sequential but also the
the code associated with with doing this
can get very messy very
quickly because this often a lot of
intermediate results and you know the
the order of the code is is not net does
not necessarily match the order of how
you would think about the problem so in
a lot of other domains for example with
databases databases let us specify what
we want to claret of lis other languages
like C sharp has given us a declarative
mechanism in the form of link for I want
to be able to do queries on this
collection so why can't we do that and
so this is all you know this is our
answer Java hates answer to this this
question is use the stream API and this
is something that has been enabled by
adding lambdas to the language so we'll
start with a you know simple example of
an imperative loop over a collection I
want to compute the total sales volume
for transactions that occurred in New
York so we loop over our transactions we
test you know we test each transaction
for whether to be included or not and if
it should be included we increment the
counter this code is trivial we write
code like this all the time there's
nothing wrong with this code per se it's
easy enough to read but this approach
doesn't scale very well when the when
the summary operation that you're gonna
do gets more complicated this approach
starts to fall apart and like I said
it's fundamentally sequential so I'm
gonna show you the streams version and
your first reaction is going to be wait
a second you said this was simpler the
streams code actually looks more
complicated to me so please sit on that
reaction for a moment while we you know
what we know while we go through it so
here's the streams version of this code
so again transactions is like an
ArrayList of transaction or something
like that and the way that we saw you
know that we solve this problem is we
say take the take take the transactions
turn it into a stream that's cheap
operation apply a filter to it so we
only want the ones where the buyer was
in New York and then we apply transform
to it saying for each one that's left's
give me its price and then we apply an
aggregation add them up okay now a
couple of things to notice about this it
looks like this would have to be more
inefficient than the first
because gee you've got filtering and
mapping and summing it looks like you
might be making three passes on the data
that's actually not true you're only
making one pass on the data the filter
and map operations don't actually do the
work they just say they just apply the
filter or the transform to the stream
and all the work it's done when you get
to the sum and so it looks like a little
bit more code but we've also factored
things out into sort of a much more
orthogonal you know orthogonal
representation and when we get to more
complicated examples you'll see why
that's a benefit another benefit of this
approach is we've inverted the control
of the computation and the top
computation the client is in control
that for loop is syntactic sugar for
give me an iterator and each time
through the loop I asked the iterator
for the next element in the collection
that means you have to do them
sequentially you have to do them in the
order that the collection hands them
back to you
you can't use parallelism you can't use
a lot of the tricks that we might use to
to speed this up by inverting the
control of the computation and we're
saying here here's some behavior to
apply as your filter here's some
behavior to apply to select the elements
you want it's kind of like you wind the
library up and it can go and now it's in
control of the computation and it can
use laziness out of order execution
parallelism all sorts of tricks that are
not apparent in the code because the
code is about the what do I want to
compute not the how do i compute it and
the library is highly tuned to focus on
the how do i compute it and you know
part of what we're going to be talking
about today is what's going on under the
surface so that you can have some
confidence that this is actually as fast
as I promise you the other big benefit
because of that inversion of control is
the option for parallelism so the only
difference since you can't see it in the
back the only difference between the
second and the third one is instead of
asking for the collection for a stream
I've asked the collection for a parallel
stream the rest of the code is identical
the filtering mapping summing is the
same you we explicitly select between a
sequential and a parallel execution but
after that all the stream operations are
the same in fact even the
lights are the same both stream and
parallel stream return the same type
they return stream okay so like I said
you might think what's wrong with the
first one it looks simple
the second one looks complicated let's
look at as we scale the problem up so if
you take this example which you might
have seen before
this is just sort of a typical kind of
thing that we end up doing with
collections so if you read all the way
through it what you're doing is you're
taking the transactions where the buyer
is over 65 for the and then you're
taking the sellers associated with those
transactions sorting those sellers by
name and printing them out okay but in
order to figure that out you actually
have to work your way through every line
of code and if I made a small change to
the problem the structure of the
solution might change quite a lot
because this is a very ad hoc way to do
it there's a lot of what I call garbage
variables in this so for example the
sellers collection and the sort of
collection are not part of the answer
they're just part of the way I get to
the answer but the sellers collection is
the first thing you see when you're
reading the code so you might actually
be confused to the into thinking that
that's important again I keep saying we
want our code to read like the problem
statement we want the first thing the
user see is to be the first thing that
gets done because then they can actually
follow along what's going on going on so
on the bottom here and I'm sorry in the
back you're gonna have to like book your
heads up to see it it's the exact same
problem rendered using the streams
library what you can see is the code is
much smaller the code is also much much
easier to read take the transactions
apply a filter that says we only want
the ones with a buyer over 65 for those
get the seller throw out the duplicates
sort by name and print them out okay so
if you look at this code on the bottom
if you could see it you would
immediately know what's going on the
code reads like the problem statement if
I say if I make a small change to the
problem
it likely means we make a small change
to the code because the code is so
cleanly factored so for the simple
example on the previous slide
yeah the streams one looks a little more
complicated once we get into any kind of
real-world aggregation example the
streams examples look a lot more compact
so sort of let me compare these
approaches the way we've been doing it
before we're dealing with data on the
individual datum level the streams
approach is dealing with data at the
aggregate level
okay so we've up leveled the computation
there's a lot of both what and how in
the imperative version whereas the
streams version is much more just about
the watt and it leaves the library to
focus on the how the code reads more
like the problem statement you don't
have the steps all mashed together you
don't have these extraneous details like
garbage intermediate variables being
leaked out into the scope and as a bonus
not only is the code cleaner and easier
to read but it also can be parallel if
you want it so this is a better way to
express aggregates and if you've if
you've programmed in other languages if
you program the c-sharp with link or
many other languages have facilities
like this it's just a lot easier so you
know the good news is we're you know
bringing this programming model of Java
but there's actually a lot of good stuff
going on under the surface that that
doesn't just make the code pretty but it
makes it efficient and flexible so a
stream is basically an abstraction for
specifying bulk operations aggregate
operations on a data set and that data
set could come from a collection or it
could come from an i/o channel or it
could come from a generating function
the data is somewhere else the stream
doesn't store the data the stream just
processes the data as it flows from the
source you know through the operations
and in fact streams can be infinite
unlike collections so streams aren't
collections but you you may use a
collection as a source for a stream all
right it gives us an opportunity to
uplevel the way we describe aggregate
operations at and that expose that
exposes a lot of really cool
opportunities for optimization one of
them I've already talked about fusing
which is you specify three operations
but then when you actually when we
actually gets
valuated they get smashed into one-pass
on the data that's called fusing so
we're not doing filtering the mapping
then then aggregating we're doing filter
math abrogation all in one you know one
fuse to pass similarly it exposes the
opportunity for parallelism and also for
laziness if you're doing an operation
that is you know find the first person
that has this characteristic you don't
have to examine all the elements in the
collection you only have to examine and
enough of them to either find one or you
know discover that there aren't any
and so by expressing computations this
way we expose a lot of opportunities for
optimization so we organize these stream
operations into what we call pipelines
and the pipeline is pretty simple it has
a source it has zero or more what we
call intermediate operations those are
things like filtering and mapping and
then one terminal operation that
actually does the work and that might be
an aggregation operation like some might
be a foreach you know might be a search
operation like fine first and that's
where all the work actually happens yeah
the next slide will reveal that but
that's a very good point this is there
is a lot in common between this approach
and a builder pattern and in a builder
pattern you're always returning yourself
here you're not always returning
yourself but you're returning a new
builder if you if you want to call it
that way if you want to think of a
stream as a builder because it's
something you can add operations to it's
returning a new builder so if you look
at this you know if you look at this
pipeline which is same one I had on the
earlier slide you convert the collection
into a stream you apply the filter you
apply the map and then you do the
summation our source is the collection
of transactions the filter and map
operations or what we call intermediate
operations these are lazy they don't
return a result they return a new stream
they don't actually do any filtering or
mapping until you actually execute the
terminal operation and then all the work
happens when you get to the terminal
operation and so I can rewrite that
expression this way to show you what the
types are and this makes it a lot
clearer so instead of instead of by
chaining them together with a nice fluid
style which is how we would write it
ordinarily for production code but if
you want to see exactly what's going on
I've written it with a bunch of
intermediate variables just so you can
see what the types are when you say take
my collection and give me a stream it
gives you a stream of transaction when I
apply the filter it gives me a new
stream of transaction but that has fewer
transactions in it when I say map it to
the price it gives me a stream of its we
have a specialized version for in-stream
to reduce the box and costs and then
when I finally do the sum operation
that's when I get an answer I get an int
out of it okay so these are the types
that are going on in that chain fluent
expression you just don't necessarily
write them out by hand every time okay
so when we organize streams into a
pipeline like I said every pipeline has
a source and there are many ways to get
a stream so
so you can ask a collection for it by
calling the stream method or the
parallel stream method if you have an
array you can call the arrays dot stream
method and get a stream out of an array
if if you have a generator function
there are there are static methods like
in stream range or streamed iterate or
streamed up generate which will take a
function and generate a stream by
repeatedly applying that function there
are factories where you can get a stream
from an i/o channel so you can save
bufferedreader dot lines and get a
stream of strings that are the lines of
a file or files dot walk or you can get
a stream of paths of the files in a
directory so like zip file you can get a
stream of entries so almost anything we
could find in the JDK that was an
aggregate that could have returned an
iterator or could have been iterable or
something like that
we've retrofitted stream onto so that
you can do you can say take my zip file
filter out the entries that are you know
not the directories I care about and
then for each do such-and-such using
stream and if the the pre-built ways to
make a stream don't meet your needs
there is a roll your own API using an
abstraction called splitter ater Paul
will talk about that more later
basically a splitter ater is the
parallel analog of an iterator it
provides a way to get access to the
elements like iterator does but it also
provides a way to decompose an aggregate
into two smaller aggregates so that you
can do recursive decomposition so the
stream source manages access to the
stream elements decomposition and also
hidden underneath this is not something
that shows up in the in the public API
are a bunch of flags and the flags are
you don't have to know about them but
you probably want to know about them
because they help influence your mental
performance model so there are a bunch
of flags about what do we know about the
underlying stream are the elements
already sorted are the elements distinct
do we know the size of the stream do we
know the decomposition characters of the
stream do we know whether the order of
the elements is significant or not we
keep track of these any Len you'll see
you'll see why so here's a chart
of some of the stream sources in the JDK
and a description of their
characteristics so like an ArrayList
like all collections we know the size
and we know that the order matters
arrays are indexed where assets aren't
the order in a set doesn't matter arrays
and ArrayList also have excellent
decomposability if you want to divide an
array into two you chop it in the middle
you get you know of one access to both
sides so it decomposes very cleanly on
the other hand a linked list also has
the same characteristics because it's a
list but it has really lousy
decomposability because the only way to
split a linked list is into first and
rest and so if you try to do a parallel
decomposition you're gonna have a tree
that is very heavily balanced to the
right and various other various other
collections like like tree set tree set
has not quite as good decompose ability
as ArrayList because it does roughly
divide in half but you don't know
exactly the sizes of the two halves so
you're losing some information when you
decompose but a tree set you also know
that the elements are distinct and
ordered so for every you know for every
one of these streams internally we
represent these are these flags and
you'll see how these get used alright so
let's move down the pipeline on to the
intermediate operations intermediate
operations like I said they don't do any
work they just sort of adds something to
the list of things to do when we get to
the work but they also can affect these
characteristics so for example if I do a
map operation that's going to preserve
the size same number of elements in as
out but it might mess up sorting or
distinctness on the other hand filtering
doesn't preserve the size but it does
preserve sordidness and distinct us and
and so as I build up the chain I start
out with a certain set of flags and then
I go through my intermediate operations
and some of them inject new flags and
some of them clear various flags and and
and you know at the end of the pipeline
I know something about the
characteristics of the results some
intermediate operations
are better behaved than others basically
filter and map are really well behaved
because those can operate perfectly in
parallel because that's their completely
pure functions of their input whereas
sorting you can't see the first result
of the sort until you've consumed the
whole input so you have to sort of bring
all of the you know all of the data
together into what we call a barrier but
before you can proceed something to keep
in mind when you're you know analyzing
the performance of these things so
here's here's a chart of some of the
intermediate operations and what they do
to the flag so for example filtering
removes size of nests mapping removes
distinctness and sordidness but preserve
size of nests the sort operation injects
sordidness might have those might have
been unsorted beforehand but sorted
afterwards and in fact because we keep
track of these bits if you have a sorted
stream like maybe one that comes from a
tree set and you do a sort operation on
it the sorts of know off it doesn't do
it and that's why we keep track of these
bits because whoa it we may be able to
optimize the execution of something
based on something we know about the
input so this is very different than
what you would do with an iterator right
collection can give you an iterator and
all the iterator can do is give you
access to the elements of the collection
it doesn't know how big the collection
is even though the collection knows it
doesn't know if the elements were sorted
even though the collection knows they're
sorted right so this is a more
information preserving approach to a
data source
finally like some of the operations like
a limit which is you know take the
stream and truncate it so that it's no
more than this many elements this is
what we call a short-circuiting
operation it doesn't have to examine all
the input it may stop before it gets to
the end okay almost done with this this
little section when we when we the last
stage of a pipeline is the terminal
operation and that's when the work
actually happens so when you invoke the
terminal operation some for each reduce
fine first etc that's when the work
happens and that's when it decides
whether to those sequential or parallel
right so when we create a stream we
create it either as a sequential stream
or a parallel
stream we passed this characteristic of
sequential or parallel down the stream
as we build it and then when we actually
do the terminal operation that's when it
either is going to do a sequential
traversal or a parallel traversal of the
elements and here's where these stream
characteristics really come into play
because now you know something about the
characters of the whole pipeline so as
an example of a really cool optimization
that we do let's say you're doing a
pipeline of operations and the terminal
operation is to array if you don't know
the size of the the final results you
end up creating a bunch of little
collections or arrays and then you merge
them into one big array at the end if we
know that the whole pipeline is sized
then what we're able to do is allocate
one big array before the thing starts
and then when we decompose in parallel
we tell all the little tasks ok you
start writing your results at offset 212
into the array and that way we avoid the
big array copy and that's the whole
point of keeping track of this
characteristics of the source and
passing it down the good news is it
doesn't show up in the user programming
model so you don't have to think about
that as long as your source has
correctly identified as characteristics
and the good news is the splitter rater
implementations throughout the
collections have all been carefully
tuned you know to provide the best
decomposability they can for the data
structure and all the information they
need so mote you know you mostly get the
benefit of it the reason we're talking
about it is so that you can sort of see
what's going on under the hood this
isn't just a dumb iterator so summary of
the terminal operations most of them are
about aggregation accumulate them into
an array summarize them using reduction
collect them into something like a
collection or a summary table we have
specialized reductions like some min max
count we have short-circuiting do they
all match this characteristics do any of
them match this characteristic do
something for each element or find an
element that you know that you know just
find me one element the interesting
thing about the searching ones
we have two versions fine first which is
fine the first one in the streams and
counter order and there's also find any
which in a sequential implementation is
probably going to be the same thing but
in a parallel implementation is
basically saying go ahead and burn more
cycles so that I can get my answer
faster I don't care which one you give
me it's explicitly non-deterministic but
it might get you the answer faster and
that's largely what parallelism is about
parallelism is about burning more cycles
to get to the answer faster it's
actually not more efficient but if time
is what you care about it's often a good
trade-off okay so we've covered the
recovered the Stream pipeline anatomy
let me you know outline sort of the the
restrictions so you know like this lady
said stream pipelines cut our kind of
like builders you're doing a bunch of
setup steps and then you're doing the
work you can only use them once you
can't take a stream and do four reach on
them and then do for each on them again
you'll get an exception that says sorry
this stream is used the big restriction
is don't modify the source while you're
querying it okay this is actually not
that hard to do but you know via the the
fundamental assumption is you've got
your data you can modify it before you
start querying it you can modify it
after you're finished querying it but
while you're querying it don't modify it
and similarly the lambdas that you pass
in shouldn't be accessing mutable state
because that introduces non-determinism
into the you know into the the query the
best thing is banish side-effects
completely you know don't do anything by
side-effects and you know sometimes that
seems hard to do it's actually easier to
do than it looks so for example if
you're tempted to write a pipeline like
this where you know you you take your
transactions you apply some
transformations on them and you want to
dump them into a list your temptation
might be to say you know for each and
then for each element put them in the
list don't do that
hand slap okay there's a better way to
do it there's a way that to do this
without any mutation and that paralyzes
better
this is the crutch we reach for but we
have to learn not to there's a you know
there there's there's a mechanism for
saying collect these into a collection
for me and this is you know doesn't use
mutation and it paralyzes better so when
you find yourself tempted to do it for
each where you're you're you're
modifying the state of something in the
for each block think is there a way I
can turn this into a reduction or into a
collection they're almost always is okay
let me talk I keep using this this
phrase reduction if you have if you
don't have a functional programming
background this will be new to you it's
a scary word it's a really simple
concept okay I've got a sequence of
input and I want to compute a summary of
it
some men max average what have you and
what I'm gonna do is I'm gonna start
with a base value so for some the base
value I start with a zero and I have a
function that combines a partial result
in a new element and I apply that
repeatedly to the elements of the stream
so summing is simply a reduction with a
base value of zero and a combiner
function of plus and so for example here
if I want to sum up the elements you
know a1 through a n this is what I would
do you know this is the the expression I
want to compute the sum method is
actually implemented as do a reduction
where your base value is zero and you
combine a partial sum in a new element
by adding them together okay similarly
the way we implement the count method is
we say map every element to one and then
sum them up okay so we provide some you
know some canned reducers like some min
max etc but you can use any function you
want to perform a more sophisticated
aggregation and as long as that function
is associative you probably haven't
heard this word since elementary school
right associative function means the
order in which I do things doesn't
matter and all of those of you who were
thinking back when you're in elementary
school when am I going to need to know
this that day has come
it's really simple they just didn't tell
you in elementary school if you're
combining function is associative you
can do the operation in any order so if
I want to do things in parallel
I can reorder this first add the first
two then add the third then add the
fourth in two you add the first two you
add the last two and then combine them
all right so associativity is the key to
parallelism if you're going to combine
things and you have an associative
combining function then you can do it
safely in parallel very simple they just
forgot to tell you that at an elementary
school okay so a little illustration of
how reductions work because everyone
learns better with pictures so I have an
in-stream that has the elements 1
through 5 I've generated that with a
generating function in streamed range
and I say reduce it with the base a base
value of zero and a combining function
of plus and so we start out with zero
and then one at a time we fold the
elements in and and combine the partial
result with the new element and in the
end we get 50 okay so that's all
reduction is it's a scary sounding word
but this is what enables us to replace
nudity of accumulators with something
that paralyzes more cleanly if we're
doing filtering and reduction together
remember I talked about how these fuse
together into one pass if I say take my
range filter out the ones let you know
you know that are less than less than to
a surface are less than equal to two and
then reduce the remainder it's just
gonna drop those and not provide those
the reduction and only provide the you
know the you know the remaining ones
inserted reduction process so these get
fused together into one pass okay very
straightforward okay
it does not know you the programmer are
asserting too that uh so if you say some
yes it knows some is associative if you
say reduce and you pass it a function
you had better be passing an associative
function otherwise we'll get the wrong
answer let's take the questions at the
end because I'm with I sense that we
could not get to any more material if I
start taking questions down okay adding
things up into numbers is easy very
often what we want to do is accumulate
things into collections and so we have a
mutable analog of reduced called collect
and it takes basically a recipe for how
do I collect things as a parameter and
and it does a reduce style computation
by mutating a collection but the
mutation is all done by the library and
so the rule before about don't do things
by mutation doesn't apply here and there
are a bunch of the good thing is there's
a bunch of canned implementations of
collectors that do things that you want
so as an example if I say I want to
collect the transactions over a thousand
dollars into a list
I say take my transactions filter them
by price and then collect them to a list
so collectors to list is just a recipe
for how to do this this reduction that
you know makes an empty list and adds
elements to it and paralyzes cleanly and
safely without you having to do any
additional synchronization if I want to
do something more complicated like let's
say make I want to group my transactions
by seller so I want to map whose keys
are sellers and his values are a list of
transactions by that seller I do the
same thing I say collect and my
collector recipe is grouping by him and
the grouping by takes an argument it's a
classification function so it basically
divides the the input into buckets
according to this function and then for
all the ones that are in a given bucket
it's six them in the list and puts them
in a map and again this paralyzes
cleanly okay
so collect is a very powerful
slice-and-dice you know are querying
mechanism
all right if we want to pick out the
most valuable transaction for each
seller we're doing the same thing we're
building on the grouping by but we use
another form of grouping by where it
uses another collector to process all
the elements in a bucket so grouping by
divides the elements into buckets
according to a classification function
so we have one bucket for each of the
sellers and their transactions go in and
instead of if we don't want to just
accumulate them into a list we want to
summarize them somehow like take the
maximum value we can provide another
collector another one of these recipes
that says for each the ones in the
bucket categorize them using this
collector so this approach here we get a
map keyed by seller and not a list of
transactions as the values but one
transaction the highest value
transaction and we've skipped over the
intermediate step of collecting it to a
list and then picking out the highest
one of the list we as as they come in if
a higher value one comes in we kick out
the old one to put the new one in okay
so grouping by very flexible and you can
compose these these collectors
arbitrarily deeply if you have a
functional programming background
collector as a cat a morphism if you
don't it's cool
last crazy example and then I'll move on
I know Paul is getting nervous that I'm
using up all the time so if I want to
categorize transactions by buyers and
sellers so I want to map from seller to
a map from buyer to the transactions
between that buyer and seller I can
compose these grouping by things
arbitrarily deeply so I can say the
first level is grouping by seller and
then the downstream collector for
grouping all the way to fur processing
all the ones by that seller is another
grouping by which groups by buyer so how
much code would it take to write this
out by hand you know it's like it would
fill the slide it's hard to read it's
hard to write it's easy to get wrong you
look at this and once you know what
grouping by does it's really obvious you
say collect first grouping by seller and
then within seller grouping by buyer and
then within each buyer seller combo
stick them in a list
done pretty nice alright so I'm gonna
hand this over to Paul and Paul is going
to talk very fast about our estimate the
canister of helium yeah okay
so Brian talked about parallel so we're
going to talk about little how it works
under the covers so it's really
MapReduce in the small if you've used
hadoop mapreduce in the big terabytes of
data we're talking about kilobytes to
gigabytes of data what's maximally
addressable let's say as arrays or
something like that to 231 minus 8 I
think in Java and so forth like that so
we got many chips cores per machine but
each one is not getting faster we got
GPUs you've probably heard about
projects some are and stuff like that
things people talking about packed
objects and stuff about how do we use
all this how do we sort of take
advantage of all these calls and keep
our cores really really hot well we have
fork/join it's been in seven for quite a
while but if you have used it it's
pretty low-level if you go do a lot of
work with fork/join to get right there's
a lot of boilerplate you have to do so
instead what we do is your level of
extraction in streams and we use the
fork/join underneath internally to get a
parallel decomposition and parallelism
so there's many factors that will affect
performance there's things like the size
of the data how you decomposed like what
Brian said about splitter raters how
many cause you have cost per elements
and so forth like that so sadly this is
not a magic bullet what we've given you
is a way to easily go parallel but
doesn't mean you should always go
parallel and what you do the key is to
actually measure so parallel computing
here is parallel is not always faster
so let me just okay so paralyzing is an
explicit but unobtrusive thing so it's
always sequential by default with your
stream parallelism is something you have
to opt into essentially so when we
create a stream pipeline the orientation
is either serial or parallel you can
choose by putting a parallel there or a
sequential there the last call always
wins
so we don't end up with bits going
parallel bits going sequential it's very
easy it's sort of a flag on your
pipeline parallel sequential last call
wins
so unless otherwise stated the results
should be deterministic so if I've got a
list here and I've got it I'm getting a
stream out of it I'm mapping it to
something and I'm collecting it to a
list if I do another list here going
from parallel stream map collector lists
those lists should be equal right how
many people think those should be equal
but he hands up its equal
the idea is list has order so the list
has some determinant isn't it is saying
these are the order of my elements in if
I do transformations on this with
operations that preserve order I will
expect a correlation coming out so if
you there's an implicit thing going on
in sequential I expect my result when I
go parallel to be equal to that's a
pretty important thing but here's a
non-deterministic example instead say
I'm doing a stream and I'm filtering and
I'm finding any what about saying is I
can get any item out of there that I
want from my stream as people go
throwing up at the back there we're
going to have a lot of that later on if
I go parallel I can find any two but we
got bunch of cores working away finding
elements give me the fastest one
possible is t1 equal to t2 all the time
no it's not so you have to be aware of
the operations but you choose when doing
this there might be some surprises that
you find when you don't get results when
getting parallel you think they are so
you have to be careful when choosing
your sources and your operations and
getting parallel results might be not
what you expect
so let's take an example of a stream map
some and what's going on underneath the
covers at a very high level here so a
parallel pipeline is decomposed into
many sequential sub pipelines what we do
is we split up a big big sort of
pipeline into lots of mini pipelines of
exactly the same thing so our source is
sort of split up or decomposed it's a
bunch of smaller things say we got four
cores we might split up into four cores
here so we got s1 s2 exactly the same
pipeline going on and we execute those
pipelines in parallel and we get
intermediate results r1 r2 r3 and r4 and
then we have to merge those results back
up using our sum operation to get our
big result at the end that's MapReduce
so unfortunately peril execution is
always more work to do this we have to
do work to split it up we have to do
work to merge it back together again so
there's an overhead here and we're
fighting and there's law we're fighting
the degree of serialization in splitting
it up and merging it back together again
so for small problems it's likely that
sequential is usually faster not always
the case but it's usually the case if
you've got a small data set it's likely
that sequential get you faster because
it takes work to split things up and
merge them back together again so to
notice here one in the fine print that
we had we pass lambdas into the maps
here so what happened before we went
sequential we passed a lambda into our
map but now we're using the same
functional value that we passed into map
and now it's going to be called
concurrently and each of these pipelines
run sequentially in parallel so that is
why you should not mess around with
state in your land as if you can all
help it because you're going to get
surprises these are going to be called
concurrently on multiple threads so be
very careful with this this is what
gives us wiggle room to optimize here
you make things stateful will help you
go faster if you've got the right data
set for you
so here's an example illustrating
parallel execution so we got entrained
closed one to eight parallel some so
what's actually going on here under the
covers so we've got our data set here
from one to eight as a range and what
what happens we want to decompose this
and split it up so this is really easy
to split up this is we know the size we
know we can chop it in half when we can
we can split up into two sub ranges from
one to four and five to eight like that
and then we can split it again those
splits can actually happen concurrently
on each side and then we get four here
at the bottom and then we decide well
there's no point actually splitting any
further because if I do it's going to
take more work and I won't get any speed
up compared to the sequential so now at
each of these leaves here we can
actually start summing up sequentially
and in parallel so just to get a point
across here what's done this associative
function here we've actually grouped it
according to the brackets here this is
how we decompose it up we grouped it in
this area as long as we don't monkey
around with the order here we can come
we can work all this out in parallel and
bring result back up so we can sum up
one task completes here another task
completes there concurrently and another
task completes here and now we notice
we've got two leaves that have actually
completed here so we can actually merge
a result back up to the parent like so
another task completes we can merge back
up to the parent and now that task is
completed we can merge back up to the
parent again now what's actually
happening here is the thread that
completed operates on the parent and the
parent all the way up to the top here
now if what you've noticed here is they
go back to here these all these nodes
here down on the covers are fork/join
tasks all of these are what our counted
completed instances if you look at 4th
join counter completed is an
exceptionally interesting class and
fourth join it has more java.com code
and that's a sign it's doing something
really really smart
okay let's go back I'm jumping ahead of
myself here okay
so here's an example of a range closed
parallel with a filter instead and this
gets across the point to the base value
that Brian talked about earlier in the
sequential reduce so if we remove all
elements that are less than four what do
we do at the leaves but don't have any
elements left they have to start with a
value and that value is zero because
we're dealing with an empty empty sort
of set here so we have to give it
something and it has to be zero if we
put one there it will give us an
incorrect result it will give us the
result based on how many things we're
running concurrently so it has to be
zero which is an identity value here so
we'd get the correct result getting my
cup you can experiment this and try it
getting pair it's very easy to see how
you get the incorrect results there so
identity and associativity it's not just
associativity it's a bit more a little
bit more in maths at that other identity
too so this is an example of what we
talked about how to optimize here how do
we go from parallel mapping to the sum
of squares and going to an array now in
this example here we know besides at the
beginning and we know the size at the
end because map does not change the size
so what we can do is we can create our
output array of eight at the bottom here
and then we decompose like I've shown
before and then what we can do is we can
operate on the leaves and we can stuff
things into the array and we know the
indexes as we split into the array
because we know the exact size we're
dealing with so at the bottom we know
the origin and bounds of where to put
elements they're being mapped to the
squares into the array so if one leaf
completes it just stuffs them into the
index at the right position but this can
happen concurrently so it can all happen
in parallel at the same time nope no
buffering of arrays no copying of arrays
here as you've noticed back up the tree
we don't need to do that we just stuff
it all into one large array and we get
some quite good performance that way and
we can do this because
and underneath the covers in the streams
we're keeping track of these flags and
we know exactly what we should do for
arrays and so forth like that
so another warning to not assume
parallel is always faster in these cases
very easy to do but it's not always the
right thing to do so sometimes it's
slower so the important thing to do here
is to actually measure what is going on
so in terms of considerations you have
to ask yourself a certain bunch of
questions like how good is my stream
decomposition some streams decompose
very well like ArrayList some not so
well like link list yes to ask yourself
does my terminal operation have a cheap
or expensive merge step what are the
stream characteristics going on am I
putting a filter in my stream that means
I don't know the size anymore so we
can't utilize that optimization to help
speed things up there are also primitive
streams as well we've included these for
one reason for performance because we
know the boxing hurts performance all
that pointer chasing means that your
cash is get polluted or you have to draw
you caches getting validating you have
to pull more stuff for a main memory and
so for that that can hurt performance as
well so here's a simple performance
model if you have n that's the size of
your data set and you have Q that's the
cost of the element going through the
pipeline an approximate cost is n times
Q so the larger your your n times Q the
higher the chance of good parallel
performance now it's really really easy
to know n it's not necessarily easy to
know Q but you can sort of reason about
it qualitatively so if you if you are
finding say probable primes using a big
integer that seems quite an expensive
operation but if you're summing up
values that seems a pretty cheap
operation so maybe with probable primes
you don't need a big data set to get the
parallel performance but with the Sun
you might need a larger data set
so intuition and measurement so as I
said again small data search sequential
usually wins watch out for the boxing
going on because the pointer chasing so
simple pipelines are easier to sort of
guesstimate about what's going on so an
approximate measure is is n greater than
10 K we haven't pulled that number out
of a hat I'll show you I'll show you why
later
maybe q equals 1 do you have 10 K more
elements in your in your stream are
using reduction using some something
like that that's that's pretty easy to
guess meet over sums cheap 10 K elements
more complex pipelines are harder to
reason they'll say if your source is an
iterator or the pipeline contains a
limit operation that short circuits
limit is exceptionally annoying to
implement in parallel that's we can use
the stream flags and so forth to
optimize in certain cases but in other
cases it's sort of a big memory buffer
and I using complex reduction using
grouping by so you can set them bracket
your stream users into two different
forms is it sort of simple and easy to
reason about or is it more complex to
reason about the key thing is if in
doubt measure there's a tool called jmh
the java measurement harness written by
alexey shavelev and this allows you to
measure things very easily about
worrying about what's going on
underneath in hot spot Java's statically
typed language but it's extremely
dynamic underneath what's going in the
hot spot you might not be measuring what
you think you're measuring because the
hot spot is doing something that you
don't know about so using a measurement
tool or a measurement harness is a great
way to get better measurements and what
you're benchmarking so just like we have
a legal notice at the start every
developer who's presenting is going to
save some performance numbers should
have a notice saying don't trust my
performance numbers okay even though I'm
going to show it these are intended to
express characteristics of what's going
on but don't trust me measure yourself
because that's real
important so I got some performance
numbers yesterday on my macbook and it
very quick so these are not incredibly
accurate but what I wanted to show you
is sequential versus parallel on my
macbook here eight Fred MacBook Pro of
arraylist the stream reduce some and
in-stream range some of Nohr to em and
on the right-hand side I have n going
from ten to a hundred thousand and I
have a rayless linked list and int range
and then I have divided it into
sequential parallel and the speed-up so
I just want to show some patterns here
what I've talked about before let's get
let's get the bad news out the way first
so the performance when n is small is
terrible so when beta is small
sequential usually wins so we see that
the ArrayList linked list and range they
have a very very poor parallel speed up
they don't speed up they slow down
because of all that work splitting
things up and merging things back
together has a cost but if we look at
the number n for 10000 we see on our a
list we're almost breaking even the
sequential and the parallel are reaching
almost the same time not quite so the
linked list link like linked list has
poor poor a parallel decomposition it's
essentially extracting things from an
iterator so we're not as good as
ArrayList in decomposing in range is a
bit faster and it's interesting to
compare the boxing and non boxing
performances ArrayList contains a bunch
of integers an int range is dealing with
primitive ins and you can see that the
int range here I'm getting a 7.9 speed
up here now that to me is slightly
suspicious I think there's I'd like to
get more accurate measurements going on
here but it didn't quick but we're
almost getting to the the Linnaeus
parallel speed-up of my eight core
system here whereas we get two point
five an ArrayList and the reason why
valley's is all the point in chasing
dealing with integers so you have to be
aware of boxing here too so those are
the three things I wanted to get across
on these measurements
I have some code I'll put the github and
you can play with this yourself and see
what you get with your measurements as
well okay so shifting goes slightly if
you want to make streams there's a bunch
of ways you can do so you can get them
from the collection you can get them
from factories or you can write an
iterator which we don't recommend unless
you really really need to because it has
poor parallel performance or you can
write a poorly splitting splitter ATAR
and we have abilities to do that or you
can write an optimally splitting
splitter ater like we what we've done in
the collections but usually probably
what you'll need to do is go to a
collection or to a factory to get your
streams and you won't have to write
iterators and you won't have to write
splitter raters that's what we hope if
you need to as a way and splitter ater
is the key thing underlying all of the
way we get that parallel decomposition
this is the thing there's a parallel
analog to iterator this is a thing
that's really really important it
essentially is a combination of
splitting and iterating hence the word
splitter ater and it reports a bunch of
characteristics we saw some of these
characteristics in stream how did the
stream get these characteristics it got
it from a splitter ater so there's a
bunch of things like size distinct or
the sorted we saw in streams but it's
also some other ones like is it
concurrent do we get a splitter ater
from key set of a concurrent hash map
for example is it a mutable non null or
sub sized and stuff like that for
optimizations so here's a splitter ater
here we have a bunch of methods to
advance or traverse now this is not like
an iterator iterator is actually really
poor because it has two separate metas
next has next and if you've ever
implemented an iterator it can be a pain
in the backside to make sure you've done
it right how many people implement an
iterator badly
I certainly have it's and it's poor
usually your next method calls has an X
and the user calls has next as well
there's a lot of bookkeeping going on so
we can make this more efficient in terms
of traversal there's a splitting method
and then as a bunch of properties like
the size and the characteristic and so
forth we've talked about and it's
important to note that the size is an
estimate and
the characteristic tells you otherwise
and this is what we do with our used
operations we talked about earlier we
fuse our operations into an instance of
a consumer and then we push elements
into this consumer as we go so this this
consumer instance that we pass in will
be doing the filter map and the Sun
rather than doing each of these things
separately going for and that's how we
push the code down to where the
iteration occurs and that's how we get
some performance okay here's an example
of decomposition with splitter ater so
if you have an ArrayList
it's got good people decomposition if we
get the splitter rater of it it covers
the whole of the ArrayList just like an
iterator does but if we split it we get
a new splitter a 2 which covers the
left-hand side and there and the old
splitter a 2 so that will get shuffled
up and goes to the right hand side so
via the old splitter either sort of
morphs into the the right-hand side if
you will and you can keep splitting like
that and so forth like that
and each of these instance of splitter
races can be handed off to separate
threads to do their work iterating and
traversing over likes fellow that so we
get about nice balanced split using
ArrayList but a decomposition of half
set is different we don't we don't know
we don't have an array in hash set of
all the elements what we have that is an
array of the buckets of the elements in
a hash set so we might have a linked
list of elements we might have a
red-black tree if the linked list is too
big so when we split over this if we
look at our for splits if they only have
an estimate of a number of elements here
and usually you would expect over a
large hash set or hash map but the key
distribution would average average
things out so you get an approximate
estimate each but if you notice here one
of them doesn't actually have any
elements at all but that's the price you
pay there but we try and optimize based
on the data structure as we deal with so
collections have a splitter ATAR and if
you want to integrate with streams look
at the collections code and see how it
does it the stream method or default
method on collection
as a as a method call to stream support
which takes a splitter rater of the
collection so look at the code if you
want to integrate your own in there so
I'm going a bit fast but summary streams
I think are a powerful framework and
hopefully we've given you an
understanding of what's going on the
covers to why it actually can be quite
powerful I think you'll find it's more
compact performant and less error-prone
to write certain code and it's an easy
path to parallelism but you've got to be
careful it's not always the best thing
to do and measure if possible and it's
deeply integrated into the collections
but it's not restricted to collections
so if you have your own stuff you can
integrate in there too okay so we have
four minutes three minutes thirty
seconds for questions all right just a
quick announcement if people are going
to leave go ahead but do it quietly so
we can hear the questions Mario
I don't understand why you need to
severity like for say alternatives
okay so the question is why do you have
a flag for ordered and a separate flat
flag for sorted and they mean two
different things
ordered means the order of the elements
matters so in a list the you know the
order of the elements is significant I
have a first element in a second element
third element in a set the order doesn't
matter okay so that's what ordered means
sorted means that the elements are
sorted according to their natural order
so sorted always implies ordered but not
the other way around so so the elements
of a list if you do a distinct operation
on a sorted list you'll get the distinct
elements in the you know in the order
that they appeared in the original list
if you do it on an unordered list you'll
just get a bag of them right so we use
this for optimization if we see that the
source is unordered there are certain
optimizations we can do because we know
it's okay if we scramble the order but
if it's an arm so for example when you
sort an ordered collection you get a
stable sort if you sort a you know an
unordered stream who are allowed to do a
non stable sort which might be faster so
it's all about optimization
so so the question is if we if we do a
parallel execution how finally do we
decompose the source and the answer is
that's not specified what we do is we
decompose it so that each of the cores
has some work and then we have
heuristics that look at how busy the
fork/join pool is as a hint of should we
split some more or should we keep going
sequentially so the goal is to keep all
the cores busy because if you do a bad
decomposition where all the easy ones
are on the left and all the hard ones
are on the right you're not going to get
any speed up and you'll have you know
half your cores idle so we try to keep
the cores busy but not split so finally
that each core has a long queue of work
to do because that means we've done too
much work to decompose right the base
very drink actually uses the size of the
sort of optimization number caused and
we slightly over provision based on a
number of course to deal with sort of
thread stores and stuff like that but
not too much that we do too much work
so the question is can you specify the
number of threads and the answer is you
can do that very coarsely so all of
these there's a default fork-join pool
that the VM creates on the first
parallel operation which you can access
either through the streams library or
through fork/join you can control
through a system parameter assistant
property how big that pool is or whether
to not use it at all what you can't do
is have fine-grained control over for
this query I want to use this many
threads and the reason for that is
there's been fair amount of research
into into this sort of stuff about can I
provide hints can I provide my own pool
users always provide the wrong answer
and you end up with you in you end up
slowing down the whole system and so we
there's you just throw it in the pool
and it happens and you can control the
size of the pool so if your may have say
four VMs on one system you might want to
give them each a quarter as many cores
you can do that at the VM level but then
thereafter that's it let's get some
other yeah some other people over here
yeah so the question is why make
parallelism explicit at all why not just
have the system figured out and the
reason is that parallelism introduces
non-determinism Java has historically
had a very strong sequential bias so if
all of a sudden we were throwing
concurrency and parallelism and users
without them expecting it they would get
very strange errors data races out of
order execution and so the default is
deterministic
so we picks a sequential as default
because you get a deterministic results
and then if you're willing to accept the
non determinism that comes with
parallelism you can explicitly select it
so it's largely a matter of not
surprising people they're not kicking us
out yet so working on team cards
you want you want a zip yeah you you you
you want zip
yeah not yeah oh yeah we you start to
run into other limitations when you when
you try to do that and so we we
experiment with that and back to backed
off we're gonna take another run at it
later
so a stream can take a set of integers
right that's an ordered collection of
elements but it doesn't ever stop you
can model an infinite collection like
that with a with with a stream there's
no reason why a stream has to have a
finite size so for example a stream can
be backed by an i/o channel the i/o you
know you may continue reading elements
from your Twitter feed you know until
the end of time you can represent that
as a stream
you don't Newt ate the source right the
source comes from somewhere else what
you don't want to do is you're querying
against a collection you don't want to
start inserting and removing things from
it in the middle of your looks like
we're being kicked out of the room so
thank you very much thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>