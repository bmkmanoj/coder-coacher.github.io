<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>With GC Solved, What Else Makes a JVM Pause? | Coder Coacher - Coaching Coders</title><meta content="With GC Solved, What Else Makes a JVM Pause? - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>With GC Solved, What Else Makes a JVM Pause?</b></h2><h5 class="post__date">2015-06-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Y39kllzX1P8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome everyone my name is John Carlson
I know some of you here some of you have
worked with directly some of you I work
with directly right now so I work in a
zoo systems I could only work on the c4
garbage collector before that I worked
at Sun and Oracle on g1 and it's
probably where most of you know my name
from the open JDK mailing lists and
prior to that worked at Intel and HP so
all on hotspot so got a little bit of
experience in the virtual machine and
hopefully trying to impart that a little
bit here with a focus on what we would
what we've kind of been doing it as oh
and you know so anyway let's get started
so tail of the talk is with GC soft what
else makes the JVM pause as we consider
at a zoo but GC is a solve - GC pause
either solved issue we don't have them
and you'll see that other people do but
they will actually people actually
worker and going to get rid of it so so
let's start
so go talk about a little bit the
motivation for this talk going to evolve
in more detail there quick recap on what
save points are most people here who
vote for the VM I know them understand
them love them hate them
they're here but for but for people who
are not VM people I want to give a
little background the media this talk
non GC safe point operations this is
where as I said we we want to spend a
wee bit more time I'm going through it
because we don't consider GC important
that anymore and then we'll have an
explanation of some of the other work
which you may or may not know that
happens at AG at a safe point not just
necessary at GC safe point and then one
of the things which is closed near and
dear to our heart of the Azul there are
time to save point factors in issues
what's basically the areas way
and an individual thread would be
deleted and getting to a safe point and
then hopefully at the end we'll have
some Q&amp;amp;A okay saying hoping but we don't
know so motivation so but stable and
somewhat concurrent collectors exist and
they have done for a while so we have G
1 which we have worked on for several
years because CMS we got balanced GC
from IBM which is you know kinda looks
like g1 a little bit and so but we
consider a result GC pauses are solved
we don't pause and the c-4 collector
they are a thing of the past and you can
see so basically c4 does that and those
also work ongoing at Red Hat with the
Shannon door
Christine flood and Roman heading key
doing good good work there and so we
expect others to get to a place where we
are currently at I get involve Evo get
there very soon so so save points ok who
here works in the VM ok good so actually
including this section was was was up
appropriate so what is a safe point so
for an individual sorry the save point
is a known point in the execution where
the state can be examined and updated
okay so why the save point operation is
an operation that the VM performs when a
thread is at a save point and I want to
make a distinction between say a global
save point and a local save point here a
global save point is where all of the
threads and the application threads are
stopped there at save points so the VM
basically does them all and a local save
point is specifically just talking about
that individual said ok so why do we
have them okay well the VM sometimes
needs to perform operations which are
atomic however to
the current threat or or to one thread
or two all the threads so most of those
we understand or 4gc okay and but we
don't consider them as interesting is
what we have found in the next set of
issues okay and GC logging in open JDK
and then the other VMs is quite honest
about what's happening there you know
there's there's only a little bit of an
attributed time mmm and also it's easier
to do some things when it in a safe
point you don't have to worry about
locking things out concurrent access so
it's a much easier thing T it's a much
easier mechanism to employ which is why
it gets employed for things that it
shouldn't necessarily need to be okay
what do you mean and mostly well eva
different different law different
logging different GCS
basically okay we'll get to that in a
minute
we always you'll see that with the GC
log is sometimes hides things from you
and it's not necessarily intentional
okay but we'll get there okay so we bit
more so we are safe points in an
individual threads execution okay so
call return sites so when they when you
make a call and you return back the via
the the the VM does a basically a poll
to see whether it should basically have
a safe point okay we've got entry and
exit to the JVM runtime certain parts of
the VM are implemented in C++ code so
the jet code and the interpreter code
call into the runtime when it comes back
out they checks to see if it needs to do
a safe point okay and things while I'm
talking about that like blocking
operations logs allocations okay one
other thing it's important to realize is
when you're working in interpretive mode
every single byte code execution is
followed by a safe point
okay so this is why your your your
interpreted performance can sometimes go
redone because it has to do this check
as well as doing the dish put dispatch
so you want to get things compared as
quickly as possible okay in our place
backward branches on non-current heat
lips when your engine compile code the
compiler generates a poll before it does
the the loop back to the start of the
loop okay now replace G&amp;amp;I code so when
you enter into G&amp;amp;I code that said
believe or not is considered at the safe
point even avoid may not be blocked the
reason is is because heap access to
objects in the G&amp;amp;I code are done through
handles so the JVM can actually modify
the contents of the handle of data
because an object has moved has been
relocated with no visible effect on the
GI cord okay so and as I say just one
yet today the F and G and I doesn't
necessarily block well a safe point
operation is in progress okay so how do
you save points work okay well so an
application thread makes a request of
the VM okay it could be an allocation it
could be something else but basically an
athlete could be a system GCE you name
it the application that makes a request
for the VM for a state point operation
okay so an internal VM thread requests
that the application threads block when
II reach their next safe point so the
running application threads will
continue running until we reach that and
then they'll say oh I'm at a safe point
okay the requesting thread is already
there he's the one who requested the
operation he's at the safe point you
know any threads which already blocked
see waiting on monitors they are
prevented from restarting they're
considered to be at a safe point okay
and then when said once all of the
threads are our archivist eight-point
operation and the state is well known
the JVM actually goes ahead and performs
the safe point operation when
done yeah the VA internal said goes
ahead and starts restarting the threads
it can be restarted that's that's it
simple yeah okay but there's some issues
with seed points so though wondering
things we have discovered is trains can
be blocked for an arbitrary long time
okay some of that is the time to reach a
safe point okay the first thread that
stops he's sitting there doing nothing
well the VM is waiting for the last
thread to reach a safe point okay now
imagine when you've got a lot of threads
in the system some of which are going to
be running some of which are being
contacts which though this is this
Quebec we're square time okay then we
have the actual duration of the safe
point operation itself okay and then
within the safe point you we have other
visible seed point operations which may
piggyback some the VM tries to amortize
the cost of bringing all the threads to
a stop and tries to keep it and may
execute one or two safe points at a time
our seed point operations well the
threads are at the same safe point in
addition there's some hidden work the
DVM me perform okay and this is why we
buy it back to your point mostly because
this stuff is not necessarily included
inside the GC log okay and then we have
a time to wake up well think about the
case where the interim of thread is
scheduled off the CPU so it's restarted
and threads when the number of threads
you've got the number of threatened n is
the number of CPUs now you got all of
these threads which haven't been running
executing for a while and they get
restarted the CPP scheduler is going to
put these guys on the chip so the guy
who's trying to restart the rest gets
pushed off chip all right you know so
basically know you've got a delete and
restarting the other threads so at the
zoo we kind of characterize sieve points
like this there's a you
the GC part at the top where it's
visible in the GC login tells your book
and your a whole bunch of stuff what you
don't see underneath because no one
informs you over time okay okay but
there are a couple of flags which will
not on by default to get some of this
extra visibility to clarify the water as
well okay so the first one and we
actually advocate using these and
production is print GC application stop
time okay and what this does as it shows
how long the application was stopped for
a safe point operation
okay very detailed and then the Kerala
does print GC application concurrent
time which shows how long the
application was running between safe
points okay
so the important thing is the output for
these flags even avoid the see print GC
is above a misnomer they come out for
every single safe point which is why we
tell them to you people to use them
because if you've got GC logging on you
won't see the safe points which are non
GC originated this will tell you that
there's there's actually save points
occurring I mean I don't know what the
real term would be I mean you can think
of terms like dark safe points for
looking like dark matter
you can only infer that they're
happening but there be a wee bit too
cheesy so some of the open for this okay
it was actually from spec JVM 2008 this
was just a straightforward one they look
at the times here okay so the
application time and that first one was
stopped for a bit twice as much as what
the actual GC work was you know so
basically you're doubling your pause
okay now look at the middle one there's
almost three times as much exactly
actually more than three times as much
so basically this is why you
you have to be careful you know you have
to watch this is the this is a benefit
of these flanks because your GCE log is
only mostly honest it's not telling you
what else is going on okay so which
operations are safe pointing but are non
GC okay well and hotspot there's a bit
30 of them okay but most of those are
user generated and when I talk about
user generated I'm talking about gbmt I
events you know heat fed dumps deadlock
detection you know he dumps all of those
canned stuff class redefinition they're
not interesting the user your user
generated if you don't know you're doing
them then the VM you know just you
should know you're doing them okay but
we're interested in the ones during
normal execution ones that just run you
know it just happened when you run your
application okay unfortunately nuf
there's only a few of those we have to
worry about okay so the first one until
I talk about is compiled method the
optimization the same thing to bear in
mind is these are probably so the next
set of problems which you have to think
about if you're if you a once GC is
already that the picture this is these
are the next set of problems you're
gonna run into okay and we have spent a
while what is all going through and
working on these guys and you know
there's the solutions to them it does
just that you can do it just takes about
wafer and bread work
okay so compiled method the optimization
what is it
well when the JIT compiler compiles a
method it makes assumptions based upon
the class hierarchy analysis okay when
you do class loading or class unloading
that involved it's those are those
assumptions you know you add all of a
sudden nervous an extra subclass to a
superclass
you've an interface may have more than
one implementer you've now invalidated
the assumptions made by the JIT compiler
when it compiled up one of those methods
it's unsafe it cannot execute that code
because there could be two fields how
does it pick the difference you want one
but not the other
so it's unsafe so hasty through are we
that old unsafe code and then so that
the method but can be recompiled with
the new class hierarchy so maybe tweak
during the optimization may generate
different code but it basically has to
throw it away and there could be many of
them to throw away depending upon how
much flux you have in your and your
class holding okay so some of the more
classic cases like jsps where we
generate classes that's a classic one
where there's lots of in classes which
are 10 january to temporarily and any go
away you know so that basically we can
change your your hierarchy and you know
if the compiler may also want to when
you unload the class recompile method
any way to to optimize it back to what
it was again so it goes both ways
ok no one by it's locking in vacation
does that people know what bias locking
is okay so i'm by default you should get
an orbiter ok so this was an
optimization unlocking that was based
upon the observation that basically one
lot one thread will typically lock and
relock the same lock over and over and
over again ok and when that happens the
vm biases that lock it means they can
always go through the fast path locking
well when that lock becomes shared then
it has to go through the slow path
walking you can no longer just go
through the biased path ok and to
basically revoke that bias it does that
at a safe point ok so and again there
could be many many many locks that it
has to go through individual locks
probably not so much but when you
combine them together then it can start
to be a bit of an issue
okay for Steve points well so the gvm
does force save points at regular
intervals and the reason why it does
this is to try and amortize the cost of
cleaning up certain items if it does
lots and lots of lots of little
incremental steps you don't see a big
bang well but you've still get the cost
to encourage our house eight point works
you've got the cost of bringing the
entire system to a halt doing the
operation and then restarting it again
you know so I good we are a good example
that says more of the deflation when you
know and we'll talk about that but
that's one of the things that gets hot
it gets happened and it's done one is a
cleanup item well can we do it questions
at the end okay and you know so
basically this it did the vm does this
to try and avoid a big buying group of
work and a single safe point so it's
trying to amaura ties the cost by doing
it incrementally okay so I mentioned
earlier that there's some invisible work
that takes place during the safe point
you know so the first one is inline
caching so game the people here know the
vm people know where poly inline caches
are okay so angling cache is basically
optimizes a call site it foot makes
basically binds a call site to a
particular receiver type okay when you
have a different receiver type that's
called a cache miss and so the English
ditch of a VM goes to a J transitional
stub to call the actual target method
okay at a safe point these call sites
and these stubs are cleaned up so that
basically the next time you come through
them if four C's are rebinding at the
call sign
unkar that's what n line caching is
incur anyway cache cleaning and it's
invisible you don't see a girl contended
monitor deflation okay
dark intended sorry or our waited on are
inflated occupy occupies resources no
because typically there's usually some
memory associated be there may be
adjusted cup of java objects the
earthing is is a for Cecil path locking
your so you're no longer going through
an optimized walking path okay and at
the next safe point after the monitor
becomes a toe that monitor will be
deflated again on crime again the
solutions exist it doesn't need to be
done at a safe point you know there you
could do it concurrently like we do it
as though it's just sometimes easier to
do it at a safe point okay you an end
again a fake you imagine it's okay for
an individual monitor but let's look at
Cassandra I mean Cassandra you know
every single object that gets created in
Cassandra it gets waited on you know so
just I mean I think the figure was
thirty thousand inflated monitors per
second Bart permanent you know that's a
huge amount it is constantly inflating
if you don't get ready then you're gonna
end up running your resources you know
so that's why it's why it's done you
know to avoid like a particular case so
another thing you know an invocation
code of this way only the JIT compiler
engineers will probably get this okay so
how does the how does the the VM pick
which methods to compile how does it
determine which ones are hard well it
keeps track of how many times have been
invoked okay now for things which are in
the compiled here you don't want to
continue to continue incrementing the
counter so what it does is it decays the
current hotel void overflow these
corners are used and in internally in
the jet to calculate a block frequency
and branch probability and so as result
if it overflows is going through off the
optimizations so one of the clean up
items is for things which are you know
for methods which are in the but which
haven't yet been compiled
it'll go through roughly about a third
30% as a dictionary class sees in the
dictionary and I will visit every single
one of the methods and those classes and
it will basically just decay the
invocation Koerner just to avoid
overflow okay so again not very doesn't
sound like a lot you know just imagine
when you got 10,000 classes in the
dictionary
that's around 300 y'all classes every
single save point you know and it
doesn't encourage the methods you know
one of these classes could have you know
you know 2,000 methods you know so so
again and this is hidden from you you
don't get this year okay go stand on the
commander on the JIT compiler theme one
of the other things that happens as
compiled code marking okay the compiler
we talked about invalidated come
compiled methods earlier but I deal you
want to get rid of those gangs well if
it's on the stack you can't and so what
this cleanup tasks does is it walks
through all the thread stacks all of the
thread stacks okay
walking the frames and keeping track of
which methods it sees which jet compiled
methods it sees okay those ones are
considered life even if they've been
invalidated okay can get rid of it until
actually gets popped off for this tank
okay
the invalidating methods which are not
seen and not protect your scan those are
the ones which can be freed okay now why
do we want to do this well so you can
clean up the code space you want the you
know you've taken up this code space we
have method which is no longer useful
it's dead it could've been a classic or
unloaded could have been an invalidated
method you want you get yeah code space
useful for another hot method as quickly
as possible okay and if you don't well
worst case you run in a court space and
that's going to disable the reject
compactor jet compilation so
no you're running an interpretive mode
you know Vivian throws up a lot warning
saying jet compiler disabled how many
people have seen a very few yeah see
because if Flo flies away so fast
okay all of a sudden you just got
performance problem to solve
okay so there is a flag where you can do
to show what's happening and this hadn't
work again it's not on by default it
what it does is it Tracy's those cleanup
times I think this was ice about
generating by JDK 7 okay so you can see
deflating out idle monitors now look at
the application time here the
application time is basically two tenths
of a Mary 3/10 of a millisecond
you know we've dumped in your 10 times
that deflating the monitor the inflated
monitors you know and you can see that
this actually cleanup work was actually
associated with a GC pause you know and
the GC pause whether the the actual
cleanup work was you know 20 percent the
actual pause itself okay so basically
you know and then the next one oh but
what the one don't belong mmm there
wasn't a GC pause there what the hell
happened what says what save point is
this associated with we know there was a
save point we don't know what it was in
so you have to see from my guesses is
polliver
it was probably a forced save point you
know and this is from a Cassandra run
and so basically every mill you know
every you know a second or so you're
you're taking a 5-4 save point and it's
cleaning up and it's basically taking
five milliseconds so something to me you
know we're all okay so one of things we
are interested in it is oh and where we
have spent a law of effort basically
you're solving this issue is tame to
save point this is what happens when a
thread stop the first thread stops and
then
it sends up block there at the safe
point while the VM brings the last
threat to a safe point we actually see
some fairly significant times there or
we did until we have solved most of the
issues within this insane and open GDK I
think you're still going to see the
issues ok so the first one large object
initialization okay what is that
well imagine initializing a 10 gigabyte
ring it's a standard size database
object there's nothing unusual you know
300 K cube this is this is just a small
object okay well to initialize to zero
you typically have about I know it's on
the order of about one millisecond per
megabyte now they're on a single set it
so one thread goes ahead and zeros it
this memory okay there's no safe points
any openjdk code for this so just
imagine because one thread doing this
well the VM is try bring you bring you
to a safe point you're like no I'm not I
need to initialize this object well how
long is it going to take you yeah okay
so the next C the first safe point after
that is erect after the object is
initialized and being published to the
heap
you don't need to do that you can
actually pull during the initialization
it just adds a tiny bit a complication
to the garbage collector and heap
iteration can be done we do it with
anything okay
re copying same issues you're copying a
large 10 gigabyte array okay you don't
necessarily need to initialize it but
you're copying from one to the other cm
issues done annotate loop no safe points
while you're doing them again you can do
it just adds a Libra of extra
complication so now if you do this or if
FS was done what you wouldn't
necessarily need to wait
until the complete object was
initialized until the state point
occurred okay so no one J and I handle
allocation well so here you z-zap whose
application uses GN i yeah quite a bit
okay how many handles does your
application Al Queda roughly thousands
ten thousands okay good to know
well so inside the VM handles are
organized in blocks r32 okay the blocks
are chained together the allocation of a
handle tries to go from left to right
and it tries to fill in the holes and
the reason why it does that is because
those last blocks to scan during the GC
the handles are considered part of the
root set of the GC okay so it basically
tries to get as much density and the
left-hand side as possible okay but that
requires periodic rebuilding of a free
list okay
the the handles are chained to the
handle locations are chained together
for faster and that walks the entire
block chain okay to populate that free
list there's no safe points while doing
that it's a VM internal code so you're
you said you ate a few thousand handles
it's probably okay you're probably not
gonna see it okay consider imagine 300k
handles and a single chain that's not
unusual we see that that's a common
thing for one of our configurations okay
orders of magnitude these kind of things
don't scale little linear okay again can
be done concurrently it doesn't
necessarily need to walk the entire
chain it can be done just takes a lot of
time and effort J night critical regions
okay
big people who use J and I okay the
people know about g and i critical
region is okay so
and in the vm community they'll know one
is GC locker regions so basically what
happens is you you call one of these
critical routines I think it's get could
get get string critical really string
critical what happens is the blocker the
GC you know it's effectively you want to
use them for you know objects which you
don't want to move during the day you
know you're you've got naked hand unique
it pointer to them so you don't want
them to move while you're operating on
on their contents so you walk at the GC
it's like an I think of it as a form of
object pinning okay so when you enter
when I said enters a GI critical region
and then our thread wants to signal a GC
that GC ain't going to happen okay it's
blocked okay
when the last fed comes out of a
critical region that's when the GC fed
that's when the GC can proceed so now
you got fed who may have been wanting to
do a system GC he may have had an
allocation failure he's blocked
you know you've introduced a little bit
of a latency there for this guy because
he can't make progress well at least one
thread is in one of these GC Locker
regions okay this is to talk about
just imagine the effect on a thread
who's waiting to do a GC okay so another
area again we spent a lot of time
investing and looking into the serial
key we took you a after Kirk I want to
try and get almost done counted loops
okay mentioned the earlier J compilers
don't generate save points on the
backward branches are currently loops
well why well the compiler knows the
loop will terminate so it doesn't is now
you're never going to go in and death in
that amount of time between safe points
but and at the end of the loop that
that's where the compiler generates so
basically got this block of code
generated by the JIT compiler
no save points in it okay imagine a
large trip curtain it's probably not
okey you know if you've got a loop it's
going you know close to MaxEnt
iterations you probably want a save
point in there right now the compiler
won't put that one in there okay
question does anybody think it's okay
for small trip currents to do this yeah
exactly
good point so consider you've got loop
it's basically memory may God use
working on and I old maps bait buffers
so basically memory memory maps a
portion of a file 100 times okay no save
there's no safe points between those now
let's consider but you guy can get
potential page fault or in every
iteration so basically now you're at the
systemic issues just because there's no
safe points on the court no neater you
could you could pass a point in here
okay so take the save point it's such
and you know it's been we've been
working on a resume for number of years
basically you know we don't and you know
it's a number very important
consideration for our customers we have
more complaints of a safe point and
systematic time to say point in systemic
issues then we do any of any other type
so it's one of it so we actually have a
set of tools that we have developed
there they're available you can use them
the you know you know wait the GC log
fair analysis is in specific but G
hiccup know you could use that it's a
Java agent based thing it basically
takes the posts of a running set of a
running application okay we also have
this internal tool which are used by our
customers Z vision and in there we have
a specific line item it shows any code
path that takes longer than a
millisecond to reach a state point you
know remember our customers are looking
for
or Layton sees less than a millisecond
so this is visibly be about certain car
parts are cut off point
so GC log file analysis ok few Appa
I'm please excuse the small point okay
now look at the scale okay so bottom
here got a whole bunch of pauses and
it's basically the two and a half
millisecond range okay and we got a few
space going up to where this I have
forty forty or forty yard milliseconds
okay so this this is from a customer we
had and you know we basically told them
yeah you know we've got because you can
see they've got some so what's causing
that well wasn't the GC so I'll type to
save point so basically we're at the
mercy of the operating system at this
point you know or do these were
determined to be systemic issues you
know and this big one at the forty
milliseconds I can't remember what it
was but it was it was not GC
I mean basically this this is the time
it took to get to the point where the GC
could run you know near the actual GC
part of us and I think that's what was
like 600 micros of something like this
so just imagine look at this forty forty
three male 41 milliseconds for six
hundred microseconds this is why it's
important to us okay so G hiccup so as I
see it's a Java agent okay
it basically takes the heartbeat of the
running application okay so and what it
does is it plots things longer than the
heartbeat it basically you know and it
plots the Delta so you can see here we
actually have some hiccups in the twelve
millisecond range maximum I think is
maximum is about eleven point eight okay
and you can see again we have this floor
ruin about one point two milliseconds so
basically for that point the heartbeat
had gone for one point two milliseconds
okay shows a couple are things what the
mean point
the things aren't but though the
important thing is is these spikes
they're not caused by GC they're caused
by something else and we also use this
to identify time to save point issues
okay so this is so you see there's
basically a few you know and we can you
can use the log file to to drill down in
to figure out what the VM is doing at
that particular point
it also shows them as a presenter no
look again we're we're kind of at the
mercy of operating systems operating
system issues here I mean there's such a
sharp cutoff and when you look at that
from a presentation point
I mean basically at this point they will
the OS decideed now I can you know
you're asking too much and so as a
result you know we invest in these tools
we invest is what we handhold as we find
areas inside Zane that where we have
time to save point issues we work around
them we basically record them up to to
solve open JDK will pretty much have
them you know it's it's one of it so
once shannon door gets hurt once g1 as
concurrent as philly concurrent this is
the next set of issues you're gonna run
into when you're worrying about latency
on crime and as i said we have some
expertise eNOS okay and by that never
open eye out for QE yes Kirk
oh okay GC Locker okay so what happens
is when the last thread leaves so so
Kirk wants me to just to reiterate
though what the GC lockers so when the
last red leaves the GC locker region any
one it will basically force a garbage
collection at that point the reason is
is because you have kind of went beyond
where the GC would have occurred if it's
like for an allocation failure
so once the initiate because you've kind
of went over your your-your-your
resource limit as it were so it wants to
basically bring that down we don't have
that sister within zing we have we
locker parts of the VGC we don't lock it
out completely sorry is speculative GC
well I'm the point what the point was
you've got one guy who's wanted to do a
GC because we see of an allocation
failure and he's kind of he's kinda
blocked he's at a save point but he
can't make progress it's it's a you know
it's it's it's basically one of those
other lanes he issues you know he's
unfortunately most people don't care
they just with a leaky objects Billy
nilly you know they don't care about
what the other threads in the system are
doing so you know all of a sudden you go
out and if you're measuring that fed
know of a sudden you have a latency
which incorporates the time not just for
the GC but the time it took for the GC
Locker sussed me to be released yeah
I again I can't comment about CLR I mean
but it's mean I called it pseudo object
pending him it's not real I mean the
whole idea is you don't want these
things to be around for a long time you
want them to be operating you want them
to be around and operate them fairly
quickly so because again you slow during
you know at least an hotspot and you you
basically sword in your VM you prevent
GCS from happening again I would have to
school and do some research in the CLR
mechanism to continue what the
differences I know what's the second
part yeah
well think yeah think about it this way
yes exactly I mean we're at the safe
points occur so the question is couldn't
there be safe points already in the
current elope yes there could be but if
you regret you know where where do they
occur call sites return sites logs et
cetera any way you call into the VM
potentially okay well most places were
you calling to the VM if you've done a
lot of aggressive in lining and believin
telling me tell you see to does a lot of
aggressive in laning you may not have
any of those in your loop body and so as
a result you have to again you had just
as something to be cognizant about
typically yes but in the and when you're
looking for for the absolute best
execution time the best compile code
that's probably not going to be a safe
point and not work yeah
yeah it's not on by default it's in the
product but it's not on by default
no the photos are live the GC log
analyzer and G hiccup only both G hiccup
looks at G hiccups are from the agent GC
log analyzer only looks at the GC log
lines themselves but member we also to
our GG Tian Jing Zhi Zing's GC log is a
lot more verbose the printer which I
told you I showed you the safe points
that didn't come up
well we actually print those are we
actually include those in the GC log so
we know which c4 what the sieve point is
4 + GC log analyzer annihilation
I believe so yeah no in fact we
recommend we and saying we we buffer Leo
Pro quake considerably open GD k is just
started to buffer
I'm sure the shed and door guy is
because again we're looking at doing
things concurrently they're going to be
working on buffering but yeah it's
typically sulfur Isis it's very
inexpensive and a lot of our people will
run with the end production is getting
less expensive and open JDK so for
example I know that Twitter runs with GC
log and on our sorry G C log generation
and production you know they're prepared
to take whatever tiny we had is to get
better information and better visibility
and what the system is doing yeah
huh
well unfortunately there isn't I really
a good guideline what you have to do is
you have to use the flags to figure a
fair as one I mean the first one there
it's typically because if there's one
typically needed to be there the best
thing to do is again is you know figure
out where where the where you spend in
time
well the VM is trying to bring the thing
to a safe point and that you know right
now those no and there's no analysis
tool other than we have we again other
than Z vision which can show you them
you would have to actually do a Paula
Cole inspection as I said we've actually
had a lot of time and experience because
you know for us pauses are not an issue
I mean we we're in the microseconds you
know but so OpenJDK is kind of liking
behind us in that respect
openjdk mailing lists that's probably
the easiest you know if you're looking
to get something pushed in the openjdk
you know ask questions on what is a hot
spot GC use hot spot use or say hot spot
developers are the OpenJDK lists okay
and some references so we're just
pointing to you know i did borrow
heavily from cliffs inline caches and
co-state optimization blog paint blog
entry you know there is some OPI way i
used open JDK 7 that's when I'm more
familiar with for my timer Oracle those
there's a white paper on Azul C for
collector and if you want to take it you
know download G hiccup run it try it as
I said we use it it's one of the things
that we you know generous customers to
January as a martyr of course because we
need to see that their system is is
choir ok yes
well any any time you do something and
you you know depends on the mechanism or
how you do it I mean if you're recording
if I fed itself is recording its own PC
and I'm doing some unwind mercy is a
signal based mechanism you look at what
the mechanism by how it does the sound
point and then how heavy weight that is
and then what's the frequency of the
sampling rate I mean something like V
turn you know a kind of they can be
heavy weight depending upon your
sampling frequency we we do some point
but it is because we we know the the
layer the internal memory structures and
we know the layer that the threads and
we know that you know things are regular
and things like that so we kind of do a
lot of things using dead reckoning
you know from every record I'll be
cached a lot of information to avoid
having a V computer so when you
transition across you know from one-call
T other real cash there's some things
tell again just here void the overhead
of the actual sampling itself but yeah
we mean we we don't see a big overhead I
I don't have the figures yes Kirk of
what
a bias locking I don't think it's worth
it down to be honest with you I really
don't I mean there's and modern CPUs you
know they do a lot of this tracking
themselves and you at TV appearance we
it just adds an extra level of
complexity and and the locking code that
doesn't need to be there you know makes
sense yeah okay any other questions
no okay enjoy the rest of Java one</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>