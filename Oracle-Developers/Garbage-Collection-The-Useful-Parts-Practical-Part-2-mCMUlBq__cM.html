<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Garbage Collection: The Useful Parts (Practical), Part 2 | Coder Coacher - Coaching Coders</title><meta content="Garbage Collection: The Useful Parts (Practical), Part 2 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Garbage Collection: The Useful Parts (Practical), Part 2</b></h2><h5 class="post__date">2015-06-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/mCMUlBq__cM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right so I guess we'll get started a
few more stragglers coming in squeeze
into the middle don't be shy hug you
neither we're all friends here at
javaone watch out for Duke running
around today as well on his big suit is
he's good good to hug who was here at
John session on user groups Sunday fear
a few of you excellent okay so there is
going to be a little bit of similar
material because I'm going to basically
cover off again very quickly some of the
theory that John cupboards and then
John's going to go into again covering a
lot more of the sort of practical tuning
situations some which will be repeats of
what you've seen but there's also
another four or five scenarios we've
thrown in for this particular talk as
well so this talk is about hot spot
garbage collection it's not java fix or
anything else so if you came in for java
effects or something else you're in the
wrong room so hello my name is Martin
verburg CTO of a company called J
clarity and we do performance analysis
tools for mainly java and JVM based
applications and we started out by doing
garbage collection analysis which was
really good fun we also do a lot of work
in the community with Java user groups
and the java community process etcetera
so if you want to talk to us about
anything to do with the java community
or how to contribute to Java itself come
talk to us you want to talk to us about
garbage collection problems in you know
we eat eat sleep and breathe that stuff
with me here is John Oliver he's
principal engineer and he eats garbage
collection logs for breakfast and spits
out analytics we're very lucky to have
him we believe that empirical science
actually wins when you do performance
tuning and the old school way of
guessing lighting black candles and
sacrificing chickens does our industry
no good so what we're going to cover so
I'm going to cover parts one and two
very quickly going to go over the theory
again we did warn people for this talk
that you really needed to either go to
john's talk earlier yesterday or take a
look at our materials online so if you
want some more in-depth knowledge about
the theory of garbage collection
especially in hot spot itself then we
can give you some pointers but hope
a little overview we give you will set
you on your way well then talk a little
bit about what I call setting the stage
when you should actually go and tune
your garbage collection okay and what it
is you actually want to get out of it
for your application when you tune the
garbage collection because a lot of
people just randomly fiddle with flags
and kind of cross their fingers and hope
and pray that their app will work a lot
better especially if you're a minecraft
player and then John's going to go
through a whole list of these sort of
real-world semi reworked real world
application scenarios which are
basically the eight or nine most common
things we've seen clients come and talk
to us about so it includes things like
very likely memory leaks things like a
lot of so many objects being allocated
that your poor physical Hardware can't
cope anymore so maximum allocation rates
people putting in system GC calls in
your source code friends do not let
friends put system GC in their source
code there are one or two very very
special circumstances but there is like
this hidden internet committee that you
need to apply to to be allowed to do it
we're not covering the g1 collector okay
there are lots and lots of really good g
one collector talks this week i'll only
say this if you have tried to use g1
before seven update 40 you were probably
wasting your time it was horribly buggy
it wouldn't meet its performance targets
every single time we tried to run it in
our lab we would come up with problems
pretty quickly 7u40 has changed that
I've actually run a really small set of
experiments on it and it actually does a
pretty darn good job if you need extreme
extreme low latency low pause garbage
collection right you want to really talk
to the guy over there guilty know if you
wave your hand he's the CTO of a company
called Azul who do a alternative JVM
which is fully Java compliant and it's
very very highly regarded in the
financial services space and other
industries that have a lot of low
latency requirements they do special
magic stuff and I'm Sean gills got to
talk on that sometime this week as well
so can have a quick retrospective of hot
spot
garbage collection theory just quickly
does everyone actually know what I mean
by a hot spot yeah right so this is the
OpenJDK / sun / oracle java that we all
know and love right so it's not I beams
j9 it's not these all system it's not a
few of the other ones like sapa exeter
out there so the java heap so when you
first start up a java application as far
as the operating system is concerned
basically a big ball of memory has just
been eaten up and your operating system
more or less doesn't really have any
visibility inside that doesn't really
manage any of the memory inside that ok
so the java manages its own memory
inside of this space it's a very sort of
key thing to remember this is why java
can do things like dynamically resize
the heap it can shift objects around
when it wants to all sorts of stuff but
as long as it's self contained within
that Java heap space so it's split up
into kind of hemispheric spaces it has a
couple of has three young generational
spaces and the whole idea of this is
that lots of small frequent garbage
collections happen in this area because
ideally in most real-world applications
from the studies done in the 1980s and
90s in particular most objects died
young in your application okay you might
have a variable you've declared in a for
loop or in a short running methods or
something that basically does not have a
lot of scope in your application this is
opposite to things that tend to move
into tenured which are things like that
might be hanging around your HTTP
session for a long time items that you
might be putting into a cash or other
other long-lived objects right you might
have extremely long running methods or
methods that are holding on they'd have
got references to other classes with you
know some nasty circular dependencies
all sorts of situations like that so
intended space it tends to be a lot
larger in size and the collections that
run over there tend to take a lot longer
to run and sort of impact your running
application a lot more heavily so the
idea is generally speaking you want to
do lots and lots of small collections in
your young space and as few as
collections as possible in your tenured
space as a very very broad general rule
there are always exceptions
and we have a slide on this later
whatever we tell you today do not just
write it down and blindly apply it to
your application please please please do
not do that okay tuning is an empirical
science you have to measure exactly what
your application is doing and then apply
the theory to it so I talked about the
week generation will hypothesis or the
young generational hypothesis and
basically again this just says that most
objects die young right so this is the
whole reason why the JVM and a lot of
other managed runtimes like the dotnet
CLR have this kind of hemispheric
garbage collection memory layer okay
because they need to treat the different
regions of memory differently in terms
of their garbage collection algorithms
and how they approach the problem okay
you don't really want to scan objects
that you know I going to be alive let's
say an object you put in a cache that
you want to live for the entire lifetime
of your application there's no point in
getting garbage collection threads to go
look at that every millisecond right
however you've got other objects which
are you know variables inside for loops
that sort of thing which you do want
garbage collection to go and look at
very quickly so there are basically two
broad categories of collectors in
garbage collection theory okay there are
actually a lot more but these are the
two most common ones that are talked
about one is called a copy collector
sometimes it's called stop and copy
sometimes you'll hear it called stop and
move if you read academia it's called
Cheney's algorithm there are another
five or six ways that it's cool it's
called in the literature which as an
aside is probably one of the most
frustrating things about this whole
space a lot of overloaded terminology
and a lot of academic terminology which
clashes with what i would call sort of
day-to-day business type terminology or
day-to-day developer type terminology in
the space okay so you can find Stack
Overflow answers and that sort of thing
which are talking about the same area
but with completely different names and
you'll read an academic paper and
they're calling it something else yet
again it's quite quite a hard thing to
get ahold of so copy collectors as used
in the JVM run time especially in the
young collection space parnu etcetera
and basically what happens is that you
have a whole bunch of GC threads that go
through from GC roots so that might be
starting from objects you've allocated
on a on a stack from a local threads
objects that are coming in from Jay and
I objects that might be being used as a
lock so as an object that using as a
synchronized object and your Java code
things like that so threads will trace
from those roots and find objects that
are still alive okay so a lot of people
think that garbage collection is about
finding dead objects it's actually the
opposite you trace through and you find
things that are alive okay and this is
one of the main reasons why garbage
collection is now becoming increasingly
important to Java developers because
we're writing bigger and bigger
applications we've got more RAM which
means there's more live objects to go in
trace so for a lot of people who have
done things like recently up the RAM on
their application and they're all sudden
or moving to a larger server or they're
getting more traffic in and all suddenly
getting GC problems longer pauses that
sort of thing and they don't know why
because I haven't changed your
application this is one of the reasons
why there's a lot more for the GC to go
through and scan and find so the copy
collector basically scans through fine
stuff copies it into another little area
and memory and then once everything's
being copied over you can then switch a
pointer very quickly and make the space
that it's been copied from nice
effectively nice and empty ready to be
have new objects go into it Markham
sweet collectors again also used in hot
spots famous one that everybody likes to
use and or complain about as the
concurrent mark and sweep click there
that runs over the tenured space and
basically you go through you scan
through you find all the live objects
and you do tick them with a little mark
and you say this guy's our life then
you'll go through and perform some sort
of a delete operation so you sweep sweep
out all the dead objects and then
there's an optional phase which some
mark and sweep collectors implement in
some don'ts where it actually goes and
compacts
memory so if you sort of think if you go
through and contiguous block of memory
and you go in and you sweep out all of
the dead objects you sort of lift with
the Swiss cheese type of effect right
and if you have the Swiss cheese effect
sometimes when you're trying to create
new objects into that whole area there's
not enough holes to put those new
objects into however if everything was
nicely compacted down or defrag if you
remember your defragging from your hard
drives then you've got this nice big
block of space you can allocate them to
so that's why some collectors do a
compaction face but of course it has its
own performance impacts the java guys
went absolutely mad on this i can't
remember the last count of GC flags
guilty can you remember to 80 or so i
think it's something utterly ridiculous
and so that the combinations and
permutations of GC flags and java is an
impossible problem to solve so we
typically say remove all the flags that
you've put on when you were guessing it
was chronic what's going to go on and
only apply flags really really sparingly
less flags is actually better less
tinkering is actually better okay the
JVM or the hotspot JVM generally has
pretty good ergonomics and it does
readjust itself according to how your
application behaves so quite often you
only need to tune one or two parameters
for your particular use case in order to
make everything happy okay we have seen
customers that have applied 25 30 flags
often conflicting flags it's just a
horrible horrible mess performance
cheating smell exactly we do have some
flags that we would consider you should
put on mandatory even in production okay
the good thing about GC logging on
hotspot is that generally speaking it's
pretty low impact okay as long as you
don't go switch on all the crazy flags
so log your GC to a separate log file
please please do not mix it in with your
normal application log because in
nobody's tooling works and nobody can
read the log and find out what's going
on is a print GC details flag which is
pretty much the minimum information that
you need to actually make intelligent
decisions about what's going
and there's something called print the
ten uring distribution and the tenure
and distribution is all about at what
age and how quickly that objects go from
the young space so these objects that
are supposed to be generated and killed
very quickly and move into the old space
where they're supposed to live for a
much longer time and one of the smells
we talk about is premature promotion for
example where objects that should be
dying young okay for whatever reason
being promoted out of the young space
early maybe there's not enough space
maybe new objects being created to
quickly few other small scenarios like
that as well those that's one of the
indications that you may need to do some
tuning so those three flags really
important to switch those on you've got
some basic hip sizing flags everyone
knows probably nosies good old X minus 6
mix that's basically the size you're
telling the operator operating system to
set aside XMS is pretty much how much of
the heap you're starting up with max
perm size who hears had a perm gen out
of memory era yes everybody are you all
spring and hibernate developers yes this
is why you're getting permed in out of
memory exceptions yeah the first time I
fired up a full jboss spring hibernate
stack very first thing it did was give
me that exact error permgen is going
away in Java rates it's being replaced
with this thing called the meta space
come talk to us afterwards if you want
to find out the details about that in
short it kind of souls some of the
problems and you'll probably find
generally speaking you're going to less
out of memory errors related to sort of
permanent metadata but it certainly
doesn't solve all the problems you can
still get classloader leaks other bits
and pieces so it's not a smelly
permanent ban data's away next thing you
need to do need to know do not touch GC
flags until you've actually measured
what is going on first okay the number
of times we've seen people look on stack
overflow or read an old Oracle blog from
sun block i should say from 10 years ago
saying
you should sit this leg to this and that
flagged a net and that works for all
applications nonsense time has moved on
applications have changed hardware have
changed you need to measure before you
touch flags oh and did we mention magic
also happens when you touch certain GC
flags certain underlying constants in
the garbage collection space change
magically without telling you so by
default for the main collectors in the
hot spot most young objects survive
either 4 or 6 collections depending on
which click do you're using before they
shift onto the old space so basically
the garbage collection system says if
you've hang around for four or six
collections it probably means you're one
of these long-lived old objects you're
probably in a cache or an HTTP session
we should shift you on ok if you touch
certain flags that number magically
jumps to 15 and you don't know about it
so the entire economics of how your GC
behaves changes and this is one of the
many many subtle things that goes on
behind the scenes and even though we've
been looking at this now for what 18
months two years we're still looking at
the source code inside jarvan going gosh
we didn't know it did that we get
surprised every single day there is lots
of tooling you can use to help you out
in the space if you're looking for a
good free tool HP jmeter is probably the
one would recommend it has recently been
updated but generally speaking it's not
a consistently a well updated project if
you're using Java five or six you can
probably get away with it if you are
using Java 7 and won't be able to parse
large log files probably isn't going to
do the job there's a bunch of other
optional ones there GC viewer garbage
cat very cool name in your IBM j9 users
couple people I hope you also use
hotspot otherwise unfortunately you're
probably in the wrong talk and of course
corporate shill al tools the best but
don't listen to us you can actually with
some cheap tools and a bit of regular
expression pausing solve some of these
problems yourself
however if you I do have serious
problems you need to start understanding
the shape of how your application
behaves and for that you need the
tooling to show you the graphs and give
you the analytics here's an example of
some of their tooling so this is HP
Jamie to the free tool and as you can
see it gives you some useful analytics
and starts coloring in things for you if
it's if it's basically giving you a
warning so look up here so the pointer
so for example the percentage of time
being spent doing full GCS which are
very stopped the world's pause
everything nasty GCS 15 percent of the
time that's a bit of a danger sign it's
spending over half of its lifetime doing
GC as opposed to doing useful
application work that's also not good we
like to say in this Turk at the back
okay kurkure we need it whenever he's
teaching this performance tuning course
always like to say magenta bad so you
see magenta bad so you get these nice
indicators here's another HP jmeter view
showing how much heap is being used
after a garbage collection has happened
okay so it looks like there's a solid
blue line here it's actually made up of
thousands and thousands of tiny little
GC points okay young collections happen
a lot even up to tens hundreds of
seconds depending on what collector
you're using so you're seeing a trend
here that even though lots and lots of
these young garbage collections are
happening more and more heat that's
still being used and then we get some
full GCS here at the end which are
trying to clean it all up but the
overall heat usage isn't dropping so
maybe this heap isn't correctly sized
might be a possible memory leak would
probably need to see a lot more data to
know because this is only been going for
95 seconds so this is very typical of an
application server or web server startup
you'll see that a lot in your logs and
you need to think about ignoring that
first startup period and wait until your
applications run for about 24 hours plus
we have here a heap letters recovering
reasonably nicely so this is over about
three or four days worth of log again we
have thousands of these young
generations happening and dispersed by
full generations and if you sort of look
along the bottom here you generally see
that it's a flat line if you drew
something across okay so generally
speaking this application problem
probably isn't leaking any memory and it
does does recover by itself okay however
you do have quite a lot of these full
GCS happening which might which might be
pausing your application so you might
have it have that particular problem
what else we say about this it's really
interesting that you can actually read
human behavior from garbage collection
logs you can talk to kirk about that but
people logging in in the morning night
time people logging in the morning night
time must have been a morning meeting
here considerate Sara you can tell when
people go after the pub early on a
Friday talk about allocation rates this
is how your objects are being allocated
on the heap so here we've got sort of
650 mega seconds again we're talking
about what don't matter days worth here
and again you can see where lunchtime is
how much time everybody's going off to
lunch nobody's putting data into their
payroll system getting quite interesting
you get these sort of pores time graphs
notice here that the young collections
happen really rapidly and generally
speaking happened very very quickly okay
so you're talking about point zero one
of a point zero zero something of a
second to run a young collection okay so
your end user Gimli speaking will not
notice this however we have a full GC
over here which stops the entire
application don't forget took eight and
a half seconds so if you're somebody
like eBay or if you're an online trading
system or an e-commerce gateway and your
users are having to wait for eight
seconds for a response they might go to
another customer site another sorry
another user so so you lose business bad
things happen you get graphs on your
permanent permanent occupancy so if
you're a spring hibernate jboss user
this is a very very useful graph to look
at so you know how to resize your perm
gin and I talked before about ten uring
thresholds this is how at what age
objects go from young into the old space
so here you can see that
most objects are being promoted at the
age of four which is what we would
expect for some of the default young
collectors and hot spot but notice here
that it's not an insignificant amount
that are also being promoted at 32 and
even the age of 10 k you're quite often
see things getting promoted early if
there's a lot of objects being created
or if people are doing things like
allocating really large arrays which are
just one big block of memory that
sometimes can't fit into the young
generational space right how we go time
not too bad cool so that was the very
very very quick run-through through some
of the theory if that kind of flew by a
bit too quickly for people come and see
us afterwards and we can cover off some
more for you and show you some of the
other slide decks of materials we have
on that so when should you tune were
your actual performance goals and we
always talk about three things latency
throughputs and footprints which
basically means how many poor how long
your pauses you want what you pause us
to be or how much time do you want to
spend pausing how much of your
application would you prefer to be
actually doing real work as opposed to
doing garbage collection and the
footprint basically the size of the heap
can you actually make it bigger could
you maybe make it smaller and we talk
we're gonna talk about little bit about
application lifecycle it's very
important to make sure you've understand
understood the natural curve of how your
application behaves over a time period
otherwise you might tune for the case
where users have logged on on a Sunday
night which is very quiet this is tuning
in a case on Monday morning when
everyone's logging in it's very busy and
know your hardware without knowing your
hardware you again don't know how to
tune your GC correctly so winter tune
not randomly when there's a pause in
your application as we see quite often
okay pauses can happen in applications
for all sorts of reasons might be locked
threads might be waiting on an external
call to a database could be all sorts of
reasons might not be garbage collection
does anyone here read hacker news or
reddit project yeah they always say it's
always java's garbage collections fault
it's just kind of a default meme on
there but
really always of them so look at what's
happening on the machine okay look
what's happening with things like your
thread pools are they actually waiting
on an external resource to come back to
you okay is there actually a problem
with threads being deadlocked for
example okay once you've ascertained
that it's not one of those things then
actually go and have a look at your GC
logs okay definitely look at GC before
you go pulling out your your kit profile
or other profiling tools okay profiling
tools are really heavy weight they will
always give you an answer all right so
whether you're looking for an answer or
not it'll give you an answer and quite
often tuning GC can be a much faster
cheaper operation than pulling out the
profiler and trying to hack you know
trying to debug code on the fly and
things like that right talked about
latency throughput and footprints so you
need to set your performance goals what
is it you actually want are you a low
latency trading system and you're not
allowed to pause for more than 15
milliseconds on any particular trade are
you an e-commerce system and anything up
to maybe two seconds as acceptable it
really depends and again this is why it
depends on what application you are
actually running so we talked about
things like what is the application
throughputs we talk about what is the
maximum pores times labs maybe you're
allowed to long pause times over the
period of 24 hours maybe that's
acceptable because you don't get so many
user complaints who knows it's
definitely up to you in your sls you can
typically tune for two out of three
three things so you can go for reducing
your footprint and getting low pulse
pores times you might be able to tune
for having low pores times and having a
pretty good application throughputs your
applications doing a reasonable amount
of work but you can almost never get the
whole new trifecta of all three right so
it's the same triangle as time quality
and money same as true and garbage
collection so as a general rule of thumb
this is a great slide to sort of take
away there's usually a trade-off that
occurs
right so if you're going for a better
throughput that usually means you're
trading off some latent sins and
footprint not always but again as a
general rule this is the case so on and
so forth application lifecycle this is
vital we constantly get people sending
in GC logs that are like 300 seconds
long saying my application is terrible
it keeps pausing and ice and then we
look at it and go but all you've just
done to started up your application
server your application hasn't even
started running yet you need to
understand what the natural life cycle
of your application is whether that is
24 hours whether that is a week whether
there is an hour one client we have
actually has a six-month life cycle
we've been running a very long GT tuning
experiment with them going on 18 months
now so on and so forth and this is why
we personally don't run any live demos
either because you can't generate enough
information and a live demo in time to
make a meaningful decision again like I
said before you can infer a lot of human
activity from the GC logs which actually
can be quite helpful and in the general
performance tuning overall goal because
hey maybe it said everyone's logging
onto facebook at a particular time or
maybe everyone's jumping onto minecraft
is a bit particular time maybe you could
stagger that sometimes you can instead
of solving the GC problem or the
performance problem with technology
sometimes you can just shift humans
around to stop stop doing the thing that
hurts even as a stim / temporary stopgap
when you're tuning GCE you need to know
about your hardware how many CPU cores
you have right because GC threads need
to run on your cpu in order to do the
garbage collection the more calls you
have the more parallel togc could be for
example right you might be able to split
those numbers up any way you see fit
your memory bandwidth metis if you're
heading the limit of how much memory can
be shoved into the CPU across the bus
then you've just hit that limit there's
nothing else you can do except for
either create less objects or go buy a
bigger hardware and because we're at an
oracle conference it means a big extra
data it's a lot
spec thing for example we have a open
source tool that you can use which just
fires memory at your application which
is really interesting and so you can
fire different shapes of activity at
your application to see how it behaves
let's my section done hope it wasn't too
rushed I'm now going to hand over to
John who's going to talk about some
actual tuning scenarios okay yeah so any
of you they come to come along yesterday
you will seen a couple of these already
but I have some more in there and just
going through some basics of what you
can pull out of GC logs using the
tooling and improvements you can make in
order to or flags you can turn on in
order to improve the performance of your
application if GC is the Cobra so one
thing to be worried about while tuning
is it's an iterative process you can't
just throw throw the flags on throw
loads on at once start it off and then
just hope it'll work you want to apply
one at a time rerun your benchmarks
rerun your tests look at your
improvements and then then if you've met
your your goals stop you're done if not
keep going and and try to diagnose what
the actual effect is currently GC tuning
still needs some human interaction been
trying to work in our tools to actually
automate this process a bit and do some
analytics but still working on it seeing
obviously will disagree with this just
uses it using and they take care of it
no tuning but again take the punching
advice here with the grain of salt it's
not a specific advice for your
application
tuning has to be done on a per
application basis you have to make the
measurements on yours so don't take the
numbers that we've used here plug them
into your application and think they'll
do anything they they will probably make
things worse and again avoid people on
Stack Overflow that just have their
flags of use this it all works it's
great so quick example go through is
possible memory leak one problem we have
with memory leaks in garbage treachery
logs is you can't necessarily ascertain
from purely from the log alone if a leak
exists as Martin point said earlier
there's possibility that your heap is
just under sized and your application
hasn't stabilized yet so it just needs a
large EEP to grow into and it will
stabilize at some point and then to
actually confirm if you actually do
think you found a leak you then use
marry profiler such as visual vm which
will show to actually confirm and find
the source of the leak so here's a log
that does have potential does look like
potentially just continually going up
and up and up we've got color coded red
x's here for our full GCS so even after
our full GCS our memories just going up
and up but we have hit the top of the
heap this is only a ten-minute log and
it's only 450 heap so this application
could still be warming up you don't know
in this case could be a decision to just
increase the heap see if it keeps going
or you the main knowledge about your
application to know do you is is this
reasonable for your application another
possible one a lot more subtle this time
again you need to look along the bottom
of the graph where you have the UGC
is very gradual upward trend or is he
hanging out so it's possible and you can
see the seasonality air of you I you
have days again activity during the day
nothing at night activity during day
nothing at night and after number of
days you end using more memory than you
started you keep going up so it could be
quite possible this guy looks looks like
someone's doing a low-latency
application here fri sister system GC so
someone's induced to GC in the middle of
the night it's done nothing all night
and then gradually done some work during
the day for the clean-out memory again
each night but everyday memory just
keeps going up so again of the possible
league so in order to confirm if you've
seen this pattern that you actually do
have a leak you can fire up the visual
vm moon profiler and the person you're
looking for typically is a are objects
or bury that are surviving a high number
of generations show you houses second
and then you want to use the allocation
sacked ray so then find where inside
your code that that memory is being
allocated from there's also possibility
of using the cabana little giant pillow
so here is a the output of visual vm on
a application does have memory and you
won't look at the shape of the number
generation so this is the number of
generations the that memory has to have
survived number of collections the
memory is five we can see that these
guys are surviving a lot more
generations than everyone else so this
is probably the
likely source of our memory leak so then
you can go look at the allocation call
tree trace it down from this is again
side visualvm trace it down into your
application code and that's where we
found it here so that's the source file
leak inside our application so yeah
tight remember profiler and look at
these generations and look for the
memory that is surviving lots of garbage
collections just very quickly visualvm
is a free tool that's bundled with the
JDK itself and you can also download it
independently okay so we're not telling
you to go buy a commercial tool for this
yeah stream good tool sorry are your
collision yeah go from
okay so what's what's the largest he can
communicate on the JVM so so the
question is how big a heap can you
allocate with the JVM with the hotspot
JVM I've forgotten what the latest limit
is I'm pretty sure I've managed to
allocate at least 64 gig heaps on Linux
very safely I think either Kirk can you
do you know what them exercises half a
terabyte so you can go up to half a
terabyte using hot spot okay well is
that sorry that'll be 64 bit yeah okay
so another possible problem you can
diagnose a long post homes for an
application obviously this is the number
one issue for people looking at low
latency and wanting responsive
applications and probably the number one
complaint we actually see there's quite
a few points tweaks you can do on this
from resizing heaps changing collectors
to potentially going off heap using
unsafe and it could show up as a user
reporting paused applications frozen
gooeys yeah just your application not
responding and take sport of decided
they want to uninstall Java we never had
this problem we had PHP that's great so
here's a log and looks scientifical
sources pattern lots of young Jin's
going up going back down one thing to
note here is we've got this quite big
five gig heap but when we do a
collection we come down to buy the gig
so we're not we're using our application
doesn't actually necessarily require the
entirety of this big heat
so then look at the GC pause time over
application tab and see for our full GCS
at least we're getting about half second
pauses on it which if you're running a
web app here may be okay about four you
know latency system media streaming
whatever 500 milliseconds is horrific
and then we're getting about 125
milliseconds for our young generation
collection since they're smaller so one
solution you can do to this quickly it's
just resized to heap down if you bring
the heap down as it astro run
collections more frequently but there's
less work to do because there's less
memory to trace every time so now see
we're getting much more frequent garbage
collections but hopefully the actual
duration should be lower now this so
we're now half the time we're spending
inside our GC we're now down to 250
milliseconds for our full generations
and about 20 milliseconds for our young
gem but still not particularly great
quarter of a millisecond so you can we
vote fer to the CMS collector that these
logs were working with the parallel
collectors that are full stop the
world's GC so during the entire duration
of that GC or application threads were
stopped and can do anything so you're up
for something responsive however moving
to CMS works in parallel with your
application so it doesn't for most of
its phases so it doesn't stop your
application so long and you can see here
the full gc's have gone
and then if we look at the pores times
here these are the paws times from
various phases of the CMS and now
they're down to was there are two
milliseconds so from the actual post
times of the full G seized CMS's the
their dads two milliseconds is almost
nothing um was still at about ten
milliseconds for for the young Jen's but
there was the price to be paid they say
there's always a trade-off when you're
doing this tuning so we brought our
latency e down but our throughput has
gone from six damp sent down to
forty-nine percent so our overall
application is spending more time paused
but the length of each pause is actually
lower other long pause time solutions
possibly use parallel GC threads
increase the number of threads that the
garbage collectors using which can speed
up problem is could reduce the
throughput of your application again
because now your applications being
staffed of cause you could also increase
your context switching move to
alternative collectors move to a
concurrent collector move to ICMS move
to g1 on g1 you could actually set the
pores time goals so you can say how long
a pause you want to withstand and then
it will try to tune itself to meet those
girls haven't seen it reliably work but
apparently there are improvements in
seven you 40 which means that it's
likely to its improved dramatically and
should start meeting those goals if you
have extreme solution again go take a
look at as all if you're wondering for
extreme low pulse times if you want to
manage your memory yourself and go off
heap also potentially
in offline mode so if you're running a
cluster of machines pull the machine out
of rotation force a full GC then when
it's done putting it back into rotation
on a load bouncer another problem you
can identify is premature promotion this
way this is the problem where objects
getting from the young generation into
the old generation too quickly because
there's perhaps something like memory
pressure or the they can't fit inside
the ownership so this log the customer
set various flags and the important one
here is the knee ratio which that's the
ratio of the under a Shinto the odd
generation so with this law with this
configuration the young duration is half
half the size of the old generation we
can see from the graph that at age one
over time about twenty six percent of
our collections involve a premature
promotion so there's also of objects
getting up into al gen that could die
very quickly solution to this drop new
ratio to one are young gents now bigger
objects can survive there for longer
they can die out they can fit that we
young generation or hypothesis and yep
it fixed it we're now only doing about
four percent up into the full percent
premature promotion and hopefully they
actually give us less full garbage
collections system GCS when you call
system GC you get a full stop the world
compacting GC its chemical painful space
for urinal are cheap so again good show
up as tech report there the dev admins
are seeing lots of full garbage
collections particularly a regular
period and seeing the
mgc inside the log so this is an example
from ad craft default minecraft install
where every one of these red points is a
system GC and there's thirty four
thousand of them so every half second on
a pole system GC every half second this
has led to a terrible throughput of even
a periods where not even playing not
even doing anything you're only getting
51% throughput on your application so
fix it by adding the disabled GC flag
suddenly you're up to near 99.8 said and
your graph looks a lot cleaner we tried
talking to notch about this but he still
refuses to listen another one that we
mentioned earlier the entry Deitrick
decreasing your latency can make you
throughput worse throughput being how
generally how much time your application
gets to execute so could be seen as
batch jobs over running the and things
just generally taking a long time that
shouldn't do so looked at a parcel log
from this and this is some of the system
information you can get you can see that
this one is sure you can't actually read
it if you could read the numbers of say
getting 61 seconds on a hundred seventy
second log so you're only getting about
sixty four percent at the time where
your applications actually running rest
is sitting in GC our current rule of
thumb is you should be getting about
ninety-five percent so here's the post
times and getting quite a lot of system
GCS
but if you actually look at the
percentage of time it's the young
generations they're actually dominating
because there are so many of them
they're adding up to a big problem so
here we're spending about twenty-five
percent of our time doing young
generation collections and about another
five ten percent inside the system GCS
oh sorry full gc's looking at he puffed
GC we can see we're just hitting along
the top of the we just the heap just
looks too small it's just having to
continually do collections leading up to
a lot of time spent inside the garbage
collector so yeah go the other way we
previously had a 250 Meggie bring it up
to a thousand hitting our nice sawtooth
pattern back not getting all those full
GCS and less pauses what's nicer however
hasn't helped as a huge amount down to
like still got like fifteen percent of
our time inside the spent inside the
young generation GCS and yeah being
about April eighty-one percent
throughput now so can also switch over
to the PS young Gen from the parallel
you collector the previous graph was
using parallel new switch over to the PS
young Gen this can run faster due to do
having to do less synchronization work
in parallel with the concurrent mark and
sweep I'm now down to about five percent
of our time spent in GC and again yeah
look at the summary getting 95%
throughput by switching collectors so
gone through some examples of what
screwed applications look like but what
is the healthy one look like typically
II 95% throughput so ninety-five percent
of time your application is executing
we're not spending in GC about once at
less than one second is quite good for a
generic web app and in footprint
generally just small as better so this
is what a healthy application can look
like nice sawtooth pattern is going up
and up and up getting full GC going back
down and yeah flat base so no real
memory leak that's kind of what a
healthy app looks like here's a
minecraft server after you've had after
you've had the system GCS removed again
after a bit of stabilization period you
then get your sawtooth pattern let's go
another interesting thing you can
identify from GC logs which is not
specifically about GC but again it's
some interesting stuff can pull
potentially max allocation rates if
you're maxing out the bandwidth of your
memory so again your application looks
to be performing texsport socio
applications form badly no idea where
why take a look at the GC log so a
typical application could look like this
again various days but yeah over a
number of days but you have small
periods of very high allocation but in
general it's kind of a distribution
towards the lower end but we've hit the
bandwidth there's this hard cap up here
and we're pushing up against the hard
allocate the maximum allocation rate of
this server which is about eight gig a
second which is but reasonable for a non
server class machine now if you look at
the histogram of the allocation rates
then see ya it's all pushed up against
this so again see this pan you could you
could be pushing up against the limits
of your machines bandwidth so fix this
a vendor CPU in a fast basket better am
stop allocating so much memory rewrite
your application also potentially don't
don't share your hardware don't
virtualize with other people that are
consuming the resources so sorry eat
understand some basics of GC theory one
of the guys that works with us richard
has his blogger insightful logic which
he covers some details about the various
collectors turn on GC logging so when
you get your performance problems you
can diagnose their issues and remember
when you're doing your tuning you're
always you're you're always making this
trade-off of pores times versus
throughput versus heap size and thank
you very much and any questions
yeah sure
if you want to so the question is if you
want to write efficient collections
should you rather use soft references or
weak references it kind of depends on
the longevity of what you want that data
in the collection to live for so you
effectively talking about caching here
right I don't know which one is high not
sure yeah I have to look at that one
feather I'm actually not sure which one
you choose Gil Gil Gil behind you who
knows knows the answer this one
okay so the answer there everyone who
didn't hear killers use weak references
not software its references soft
references will get you into trouble and
don't write your own cash any other
questions yep okay so the question is
you can fix some memory problems by
actually having your memory go off heap
right so a classic example this of this
as if you're tryna store say a two gig
cash worth of data right you know it's
going to be there permanently and
forever for the lifetimes replication so
why give it to Java to manage okay java
never needs to actually garbage click
this so in theory you can take that all
of that data and manage it outside of
Java using memory mapped files or other
such techniques quite a few than no
sequel solutions do this especially if
they're built using Java technology and
so that that memory just gets blocked
off and gets managed outside yes
do we have experience of profiling
Hadoop not on where your site you so you
should you want to maximize a throughput
for her Duke it again it really depends
what you're using the Duke for right I
mean are you running like an overnight
met reduced job we've got plenty of time
for it to finish then having long pauses
is absolutely fine for example it's
pretty much a typical batch application
with that sense really so yeah use
something like the parallel collectors
that give you good throughput and if you
would stand it give yourself quite a
large heat yes
so what's the biggest difference between
using virtualized hardware and real
hardware when it comes to garbage tuning
in virtualized hard way gimme is going
to be slower in general do too yeah
reduce memory bandwidth less CPU but
this thing in general you just on the
slow machine it doesn't have any kind of
specific problems relating to garbage
collection yep oh yeah yeah because
you're sharing a cpu with with someone
else on that on that server right
because there's another operating system
that someone else's god or another
application someone else has got you've
basically just got less threads to play
with right or you've got less calls on
the CPUs that you are allowed to use so
yeah generally speaking of you so I'm
going to go allocate you know 16 threads
to do some garbage collection and the
other the other guy next to you who's
paid more money to amazon has stolen all
those CPUs in yeah it's it does
basically doesn't doesn't work as you
expect so what does our take on g 1 i'll
take on g one as we have not done enough
science to make up our minds on g one
when it first came out and it was
officially supported in seven you for so
java 7 update for we ran it very quickly
on some some tooling we've got
internally ran on a few swing desktop
apps you know eclipse net been to tell
aj ran on a few basic web app spring
hibernate that type of cruddy type
application it was a disaster right it
would code sig dump the JVM wouldn't
meet its paws time goals just it was a
bit of a horror story to be fair now it
has improved rapidly over time and in
the very quick experiment that i've run
back in the lab 7u40 has passed all of
my very quick tests but that's
definitely not proper science right and
so we need to spend probably another
good two or three weeks I think Kirk's
done some more independent research but
okay so for those who couldn't hear Kirk
with 7u40 you still need to know what
you're doing but if you do start
applying some of the tuning some of the
some of the some of the theory that we
talked about today in 7u40 is looking a
lot more reliable is looking like it can
meet some of these pores time goals that
was you know supposed to be the holy
grail all this time ago
so the best way unfortunately is to run
the application for that natural life
cycle and this is why with the client
we've got has got a six-month life cycle
making tweaks is a really painful
process however for sort of shorter life
cycles we use that tool called Hawkshaw
which is our open source tool which
basically just helps you shape shape
memory in the same way as your
application behaves I won't say it's the
easiest API to use in the world yet but
that's what we're hoping to get more
open source contributors towards to make
it more of a day-to-day tool for
developers to use and we view I mean
I've used that personally a couple of
times to just try it try g1 out
especially fire memory edit see how
those and if you don't want to put
something into production you can always
come up with a parallel infrastructure
and then you sign like Apache jmeter to
test it and see how it responds so you
can you test it on the staging instance
so if so we're going better so the
question is I talked about a little bit
about premature promotion and objects
being moved from young Jin to the old
gin too quickly but before they can have
a chance to die young as it where it's
almost like promoting your you know your
six-year-old kid to go into high school
it's kind of in LG I use so what's
really going on is the young GI only has
obviously so much space in it it's
usually usually shaped a lot smaller
than the old gym and a few for example
allocate a array of a million integers
okay that's one continuous block of
memory in Java right so JVM goes can I
shove that into young Jin Young Jin goes
hick no you're not fitting here there's
that nice old gym space over there
though go go chuck it in there and so at
the very first garbage collection it
happens or even before a garbage
collection happens when the actual
object gets allocated for the very first
time sometimes it can bounce all the way
to tenured straightaway and if it's dude
site like memory pressure that you're
just allocating the loads of objects so
you just have to get others out to make
room what will be in your young Gen then
will be typically filled with very young
objects so yeah it could be aged one
stuff yeah just justin bieber selling
tickets at a concert as a classic
example that right that the server just
gets hammered
it
a lot of that
very
because a lot of those
so the question is so in the young Jim
it's typically a copy collector so you
copy the live objects into either one of
the other young generational spaces or
yes it does get copied into tenured and
then your question is is that fast or
oh I see so you kind of saying why do
you do hemispheric GC on the entire heap
as opposed to just young German that
what you're getting at oh how you get
memory back from Elgin right so what
happens in all Gina's well I'll just
talk about the concurrent mark and sweep
collector which is kind of the one that
a lot of people tend to use so that runs
in parallel with your application marks
things as being live sweep stuff that is
dead and leaves that Swiss cheese cheese
behind so the only way you really get
that memory back and in terms of a nice
big big block of memory to allocate
larger objects into is if you compact so
at some stage generally what will happen
is that a full GC will have to take
place which then can pack set memory
back down and that's how you kind of
recover some space in all Jim there is
also another old gem collector which is
just the parallel old collector which
does that connection phase as a default
so sort of sort of two different ways
that they can that can work we'll sketch
it out for you on a piece of paper and
probably make a bit more sense
Oh compaction compaction is expensive
because you have to move memory around
yeah yep they just get ignored right so
basically it's that they still
technically if you were to look at the
RAM there's still some sort of
information there but it can just be
written over the top it's basically what
happens yeah just it's just treated as
empty space that can be written into
it's just typically a point of swizzle
vieira but okay so you may then they may
then die in the next garbage collection
but if you're running the parallel you
think got to do the compaction to then
compact and get rid of the hot d get rid
of all the holes that made but if you're
using concurrent mark sweep you've now
swiss-cheesed your memory and increase
the probability that at one point you're
going to get a promotion failure because
you're putting all these young objects
up and then deleting them so you're
fragmenting your memory more by
promoting these objects that don't need
to be there you know questions yep
mm-hmm
yep
and say
great
so the question is a few gotta cash
which you know only gets refreshed once
every 24 hours well it's the best way to
debate it way to handle it so again it
really depends how big that caches and
whether that's causing your job a heap
space a problem if it's like a you know
a 200-meter cash it's probably not if
you're talking about a 10 gig cash then
you probably don't want to deal with
that in Java memory necessarily
depending on what type of vm using oh ok
yet so the right so there then it is we
are quite reasonable to actually store
that cash off off the heap either in
memory or even in external data store or
even on the file system depending on how
how fast you read right needs to be to
get that data
oh sorry so you're you're you're only
getting four gig of ram in your virtual
environment and your jvm is four gig as
well well then dear then you need to buy
more ram where you need to get more ram
from your virtual hosting provider that
is all you can do unfortunately in that
case yeah you can
yeah so we have seen some customers for
example install SSD drives to replace
their old spinning rust because the SSD
drives when they store the cash on the
disk itself as fast enough right so they
start using an SSD drivers kind of slow
ram as it were
oh yeah it's great business for us we
love it more virtualization please it's
good yeah so this is over ass ya like
like Kirk suggests that there are
definitely some some clever ways we can
do it oh we're being kicked out so I've
come grab is just outside</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>