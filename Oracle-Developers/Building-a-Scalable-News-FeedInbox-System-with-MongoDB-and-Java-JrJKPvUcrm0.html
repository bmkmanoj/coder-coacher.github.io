<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Building a Scalable News Feed/Inbox System with MongoDB and Java | Coder Coacher - Coaching Coders</title><meta content="Building a Scalable News Feed/Inbox System with MongoDB and Java - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Building a Scalable News Feed/Inbox System with MongoDB and Java</b></h2><h5 class="post__date">2015-06-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/JrJKPvUcrm0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so welcome to this session building a
scalable inbox system with MongoDB and
Java thanks for attending I know it's
sort of the last leg of gerawan so
you're the most courageous people in the
conference but hopefully this is going
to be worthwhile my name is Antoine
Grable I've been at Tenjin MongoDB Inc
we just changed names for almost three
years so I've been there for a while
I've worked on the MongoDB core server
as a kernel engineer and then on the
Java driver and recently switched to
working with customers and trying to
implement solutions so let's try to make
this session a bit interactive so I'll
take questions at any time we really and
the agenda for today
first off will obviously stay the
problem then look at simple schema and
queries then focus on the Java
development this this wall idea of the
inbox is not just Java of course there's
a lot of database involved and so some
of the examples are actually easier seen
from the shell so a project in advance
the the amount of Java code is is not
that that large but at the same time
just staring at code is not that great
and we'll see all the different design
options for your inbox and then discuss
the pros and cons of each solution so
the problem let's get social
you probably recognize a lot of those
icons every single social application
which is like probably the the most
dynamic business these days every single
of those applications are something in
common
basically you have a network and that's
why it's social and when you have a
network the most basic feature is to be
able to send messages and receive
messages and that's the core of most of
those systems if you think of Facebook
of course
you want to post something and then your
friends want to be able to see it in the
case of Twitter is really the same thing
the difference is that potentially with
Twitter you have millions of followers
for example but so let's call it just
inbox but you can also think of it as a
newsfeed really and it's both a
requirement for a lot of new businesses
and it's also one of the toughest
problem to tackle so here's an example
you have your this is the Twitter
interface you want to type in your tweet
you press send and then it gets sent to
the cloud somehow somewhere so we want
to understand where it's going and what
are the options for it now this needs to
be fast and reading your inbox needs to
be really fast too when you read your
inbox to see messages from all your
friends or your follow ease distance to
happen is really fast and again what is
going to happen on the system side so
let's start with the schemas how you
would basically model your data in
MongoDB
and first off let's I don't know how
many of you are familiar with MongoDB ok
so that's pretty good so just for this
we don't know MongoDB at all it's
basically document-oriented database
it's made to be scalable it's open
source and a lot of interface is based
on JSON even though there is absolutely
no JSON anywhere in the inner workings
of the database but whenever a document
is presented to you in the shell it
looks like a JSON document which is
really easy so basic crud you want to
save your first document VBS is
basically your database dot test test is
your collection name a collection is
equivalent to a table in sequel so DB
that test dot insert and then you just
have your JSON in there
and the shell understands JSON as a
document so if you just download MongoDB
log into MongoDB with the shell and you
type DB that tests our insert in this
JSON is going to work you will have your
document stored in the database there is
no schema to declare every document in a
collection can potentially be different
but typically here you would have a
collection of say users now if we want
to find the document you do DB detest
that find and you can specify one of the
fields to match on so here we match on
first-name Antoine
note that Mongo you have the full
features of JSON so you can match a
field nested or within an array and that
lots of operators that lets you do very
complex matching so you're not limited
to only the root document and inequality
so here we're just doing an equality on
the first name Antoine that we change
the document we just inserted and you
can see on Mongo added automatically a
unique ID to the document you can have
your own ID or you can add Mongo set the
ID for you then we can update the
document by specifying a matching rule
and the new document or the fields you
want to set so in this example we do a
divot attested update we match on the ID
that we know now and we just override
the full document to x1 y2 again this is
just an example you will probably not do
that in reality but we can set to
whatever else we want there is no schema
again no fields that are that are
mandatory and then we can just remove
the document DB that as a remove with
that ID so that's really the the first
line so we type in a shell now we want
to create a sandbox system of course we
need to have users and it's so much
easier to presenting user in JSON than
in regular sequel you can just go for it
store whatever you want
you have the types in there so for
example here it's not properly indented
but we have a set document with the
address and then you can imagine that
some users would have more fields some
users may have Facebook ID Twitter ID
and so on
and then at the end for example we have
the location with the coordinates this
can be used to have a Geo index on it
and and implement geo searching so you
can see the different types for example
birthday
you have a date type in there and B Sun
actually supports more types than you
have in JSON so there are some extra
types like date a lot of different
numbers like long integers binary data
also so now I took those those users and
this is not an actual real user this is
from a fake name com generator and it's
so I created about a million of those
users inserted in the database now I can
look at the statistics of that
collection so it tells me I have a
million users in there the size of all
documents is almost a gig 630 megabytes
average object size is 630 bytes then
you have some extra stats to understand
the storage of MongoDB there's a padding
factor the padding factor is interesting
it's how much space Mongo adds at the
end of each document this way if you
decide to update your user later on
there is some extra space say you want
to add a new Facebook ID you may need to
have a little little extra space that
the document has done it to be migrated
on disk this is more advanced it's not
really the subject today and then you
can see the index sizes by default you
will just have an index on under squaty
the primary key in this case you will
probably want to have indices on
location so you can find users in a
in an area you probably want to have
indices compounding this is on a first
name last name and then if you have
Facebook ID twitter IG you would
probably have index on those two
potentially sparse and this is so that
if only 10% of your users have a
Facebook ID only 10% of the this index
is gonna only going to be fairly small
about 10% of the full size compared to
Sedona squaddie index
does it make sense so far any questions
yeah right so you can specify unique
constraints unique on a company index so
that's the main constrain you can you
can set and by the way indices are a
pure b-trees very close in inner
workings to two sequel so if you use the
sequel you will feel right at home with
Mongo indexing the nice thing too is
that you can have an array an area value
and we'll we'll have examples of that
and then have an index on the array so
now what kind of queries will we do on
our users you may want to do the finds
by email and so in the Mongo in the shot
syntax d beta users that find and then
you just specify the email now if you
look the way to look whether this is
fast or not is to do and explain if you
don't explain manga we tell you exactly
what it's doing here it's saying basic
cursor scan object 1 million that means
that it went through the entire data set
of users to find all the ones that match
that email this is going to be slow if
you're trying to build Twitter it's
gonna be too slow so just create an
index easy enough Davida users that
ensure index on email from there it will
use a b-tree that will find the email of
very fast so typically you go from a
full table scan that can take up two
hours to typically milliseconds
so no no citation there so now we do
have we do need to create the user
relationships and that's a very
important decision so this is a
many-to-many type and just one quick
word on on one-to-many so when you have
a one-to-many relationship meaning you
have an item and then you have several
items that belong to that first item in
a case of Mongo you can actually embed
those items in in the the primary
element say you have a shopping cart you
could have a document in a case of
sequel when you normalize you have your
shopping cart as one row and then you
have the items of the shopping carts in
a different table that references that
shopping cart in case of Mongo this
one-to-many relationship because
extremely simple in that you just embed
those those items in the shopping cart
you have an array of items you cut down
on an entire collection on a huge amount
of rows you cut down on one index for
the relationship in one direction and
you actually don't lose any feature
whatsoever yes so a very good question
how do you embed algae manage depending
factor when you start embedding data
into one document so this is a very long
topic but in a case of a shopping cart
it's pretty straightforward because
typically it doesn't grow very much
typically towards the end its I mean the
size doesn't vary very much and either
if you have a very static size then
there is no problem if the size grows
then Mongo will automatically adjust the
panning factor to try to avoid having to
move around the documents you can
prepare if you like to today you would
basically prepare yourself so add an
extra field that that prepares the
document you can also make use of an AR
let's call power of two sizes which
makes Mongo always put documents in a
space on distance that's the power of
two to say you have 128 by it's 256
bytes so this size is very usable and
you typically have quite a bit of space
to grow so now this this is for
one-to-many and in the case of
one-to-many you have a lot of ways to
really improve things over sequel in a
case of many-to-many which is like here
so you have users and followers so you
can either the first solution is to keep
a list of the followers in the user
second solution is well let's talk about
the first one that seems like a bad idea
in a case of Twitter because you well
actually this is well yeah the followers
one is bad I'm gonna get confusing
forwards and follow me sighs so in case
the followers well if you take extreme
cases like Lady Gaga or Justin Bieber
you would have to stake I don't know how
many like millions and millions of of
entries in that the duck there the user
document not a good idea especially if
you start indexing that when the
document gets my grade aids those index
entries need to be updated that's very
dangerous number two is a list of
followers in a user so the user decides
to follow people on average I don't know
how many people someone follows on
average I guess about 500 and the good
news here is that you can typically put
a cap if someone is really following a
million users typically they're they
don't have a life anymore or it's
probably about but in a lot of examples
companies I work with they are no
problem putting a cap on the number of
people you can follow now so this would
still be embedded in the document the
third solution which is more sequel
style is to have a separate collection
with a relationship where you have the
follow
or in the following this is pretty
straightforward and you may wonder well
I might as well you sequel in that case
but you still get the power of the
documents you still get very flexible
for the users and you would pick this
solution if you basically want to do
some indexing or if the lit you don't
want to put any limit on the any of the
numbers that you want to store and the
fourth solution which is not intuitive
is to have two relationship collections
so one full of followers and one full of
followers so you store both sides of the
relationship in basically documents that
are exactly exactly symmetrical so this
is an important decision for the system
so we'll spend some time on it if you
have a few million users and about a
thousand follow a limit then the
simplest most intuitive solution is
solution number two you just take those
users the followers in the user and you
do have to handle the padding properly
make sure that there's not too much
document migration going on but people
are not following and unfollowing people
at a crazy rate it's not going to be as
much as what people send so this does
not need to be optimized as much as me
as much as the messages but you need to
be able to read to to get the list of
followers very quickly in this case is
very is very quick you would of course
have an index on the following array
within that document so that you can
pray for this user give me all the
followers and for this user give me a
loaf Louie's now if you want no
boundaries and pretty good scaling you
could go for solution number three which
is what you would do in insane
normalized fashion and then that
collection is let's talk about the last
one if we want to say max scaling
we would say solution number four with
the the relationship in both directions
does anyone have an idea why we would
ever do switch number four what's the
advantage of solution than before so
this has to do with sharding so when we
say max scaling you actually want to
distribute all those relationship across
many servers now say you have your
relationship collection
you don't want total your relationships
to be on the same box you could use
replication to create the secondary's
but then it's potentially stale so say
you want to build Twitter you will need
to scale you have many shards you want
to split that the relationship table
across those shards you can only split
one way you can only pick one key that
you want to split so say in the case of
the solution 3 you have a followers
collection so you have the the following
and then the follower you could if you
ask the system give me all the followers
for this followi the system would know
exactly where to go since it's it's your
documents are divided according to that
key so they will know that for Lady Gaga
for example it's a given ID this ID
belongs to this shard so the million
entries that are following Lady Gaga on
this shard so your application goes
straight to a server gets other
relationship back is pretty fast the
problem is the other way around now for
the other like the the people that Lady
Gaga follows and that basically I mean
probably not herself with a lingerer
accounts probably like the staff that's
like following what bunch of people so
in that case Mongo has no idea where to
go because there's only one side of the
of the view so it needs to broadcast the
query so say you have Twitter you have
say 50 shards whenever you want to
the other side of the relationship you
need to do a broadcast query 250 system
and get the result back and it's a kind
of an important it may be on your
profile when you log in you want to see
like all the people you follow so if you
use solution for you're actually
stirring both sides in two separate
collections and each has its own short
key and so both can be scaled and
directly routed to a server does that
make sense yeah okay so the question is
how do you force this to to stay in sync
and that's the main problem with scaling
when you start splitting things around
and duplicating content that's when you
can run into problems and so when you
write one side the relationship and you
write the other side the system can fail
in the middle so they are there many
ways to sort of protect against this
some are like heavier than others but
you could go through a job queue for
example so you have a job queue that
says those people are in a relationship
now and your worker picks it up does the
first right if it dies in the middle the
job is still there in the queue and the
next time around each will do the first
one again and do the second one until it
actually succeeds so now example of
recent ship day now we have a very
simple document here this is solution
number three we have the user and the
following so the user is actually a
follower and then it's a very simple
document you may want to add some extra
information in there like are they real
friends or not not just acquaintances
and then looking at the stats I have
again 1 million documents in there and
in terms of index I would this is
actually pretty interesting too I have
an index on user and followers so does
that make sense a question there
okay
so if I'm taking this case here so the
question is for assertion 3 when I have
a single collection with a relationship
how would i shart I would just shard on
the on the user I mean on the on the
follower well it depends what is better
for your business if you shine the
follower you can get basically all the
people that that user follows very
quickly so if you go to the profile page
you can show all your following all
those people but the relationship to get
from this user it's see you won't
display the inbox or well actually you
want to show the person without the
photo is for that person then that takes
a longer time so it seems like the first
case is more important if you want to
display the people you follow and also
the newsfeed you would want to short by
the user here and then when someone
wants to see what was following me that
would be slow ok so now a important
optimization here you would think I
would just have an index on user I would
just say well Mongo give me all the
relationship for that user and here I
have a company index on user and
following the reason is that we're
trying to cover our queries we're trying
to make it faster so that when Mongo
looks at the storage it only uses the
index it doesn't actually look at the
document at all
because he has the following idea right
in the index so this part of the
following here is not important at all
actually it's it's important if you want
to delete that document if you did want
to do that relationship and you would
also have the index in the others in the
other direction with following end user
this way the other side of the
relationship that you query that does a
broadcast right once it gets to those 50
shards you would still use an index to
find all the people that were following
that user so here to be complete you
will have user following and follow a
user
and the reason you could also argue I
could have just user and followi as
single indices the reason they are there
is because we want to cover those
queries and we want to avoid looking we
want to use the index in RAM we want to
avoid looking at the actual B Sun and
Jason on disk okay so the question is
can you see that in in in explain yes
very good question this is a very
important optimization you can do and
that will be shot as index only in the
explain actually there's an example here
so we create so DB that follow ease
ensure index user and following that's
the company next on the - and the
question here here was why not
not just index on user we shall see now
we want to find for this given user
all the follow ease all the the people
that does user follows if we do the
explained we can see that the the
millisecond is zero so very fast for a
million item is not even taking a
millisecond important note to actually
get the cover in X so here we have the
proper index to cover that query right
all the information is in there in the
index actually it's not because by
default Mongo always return the entire
document so Mongo doesn't know if you
have 100 other fields in that document
so it's it's actually going to go to ram
or disk but you find that document and
soon back to you now if you state
exactly what you want you state I want
just a following number and I'm not
interested in the ID you have to disable
the ID because that's on by default
that's kind of a pitfall
so here I actually specify the fields I
want following one ID zero now you can
see index is only true and that breaks
it down from zero millisecond to minus
one millisecond or something very low
so if we want to find all the followers
of a user then we just need the opposite
index so we just reverse the
relationship if you have a single server
with this both sides of the relationship
would be qualify stick equally extremely
fast covered in X if you mix in a large
sharding then one side will be directly
routed then super fast the other side
would be broadcasted then super fast but
still broadcasted ok any question before
we move on to dark to messages here yeah
in this case you typically want to look
up for that user the followers of the
flu is yeah so here those actually
strings and you can do a regular
expression typically if you want the red
X to be fast you would have to it would
have to be left anchored but whatever
you can do with the B tree in sequel you
can do do here pretty much yeah
okay okay so I'm forgetting to repeat
the questions the mic but so the
question is for solution 1 &amp;amp; 2 when you
store the the array of relationship in
the original user can you run into
concurrency issues so every right in
Mongo is atomic so if you typically you
would do an ad to set so you have an
operation that's let's say say Mongo add
this entry to this array and make sure
it's unique in there so even if you have
several people so we'll click at the
same time and coming in you know you
more have duplicates one caveat of the
ad to said is that it's not backed by an
index because that's within one user so
that goes back to having a limit if you
have a thousand items in there finding
the uniques is again like 7 milliseconds
but if you have a million that's a
different story
yeah
okay so the question is there's been
some bad press on Mongo as far as
logging is concerned and scalability
with that are there any improvements
coming in and thank you for the
opportunity to talk about this so it's
been yeah it was the main caveat of
Mongo and we heard a lot of bad press on
it with the global lock because in in
the case of sequel it sounds horrible if
you have a single lock for the entire
database every time you do write
especially if you have a transaction and
you may be changing many different
things that sounds pretty horrible in
the case of Mongo actually it was
confined to be only affecting a few I
mean a portion of the right heavy
projects the reason is that when Mongo
does the right it's always on a single
document it doesn't like it doesn't do
transaction across documents it's always
confined to a single piece on document
that document has a maximum size of 16
Meg's and it's always contiguous either
in Rama on disk so the first step we
took is to make sure that the data is in
RAM and so we have a yielding process
even before we talk about multiple locks
whereby Mongo would first try to access
the data see that it was on disk he
would give the lock back then wait that
it was rooted in RAM then take the lock
back do the modification in RAM and
release the lock now if the data is in
RAM even with a single lock you can do
about a hundred thousand writes per
second on a laptop
so the yielding behavior if it's perfect
would actually not require any further
locking in improvements that being said
this behavior was not perfect because
sometimes you need to touch a lot of
different indices and so we've been
working both and improving the
building and also creating more granular
locking so now we have lock per database
so in effect in this situation we would
have one database for the users one
database for the relationships one
database for the messages when I said
database I don't mean cluster and wall
environment I just mean logical database
since there is the first days no code or
server involved it just gives you an
extra lock and that gives you an extra
boost so the collection level lock is
not there yet but the good news is that
typically you don't have a in sequel you
used to like 800 tables it doesn't
happen with Mongo typically it's much
more saying you end up with maybe a
hundred collections because things sort
of embeds each other and then in the end
you can sort of separate into databases
and if you end up with a dozen databases
really you won't have a locking problem
and replication by the way is using its
own lock since Mongo 2.2 so that makes a
huge difference - okay so let's get back
on topic so now interpret of the message
so here are typical message the IDE the
author ID username Y store the user name
by the way
well if we are just the reference to the
to the author every time you want to
display those little tweets you need to
go fetch that user information so you
may want to actually store some extra
information in that tweet like the
username like a link to their thumbnail
and so on the problem that happens when
they change that but you could decide
well I never want people to change
username or once they tweet it with a
username that's which will remain with
that reasoning
or maybe you'll adjust by the bullet and
have a batch processing that goes around
your tweets and just change the data in
there but it's important that if you
have 10 million people looking at a
tweet you don't go do an extra tell me
in a request to get that information now
you have the text itself the credit time
location again if you want to do some
interesting geo and then tags so tags
here you can leverage the power of JSON
and stores many tags as you want in that
in that tweet typically this is fairly
static I mean the person when tweeting
the add tags to it and and so from there
it's not going to grow very much and you
can index that list in the case of
sequel you would be stuck you would have
to create a separate table with many
rows in there a relationship if you
store string a comma separated string
then you can index it so it doesn't do
any good
what's here you have you have your array
you can create an index and then find
all the tweets with this tag and for
this period of time very handy in terms
of stats I insert a 21 million items and
they will have them same text
unfortunately but they have varied other
properties but in fact there's no
padding factor here in terms of index I
just have the primary key ID and then
the author UID and created so you can
very quickly find all the users all the
messages for that user for a period of
time and so that's how we implement the
ad box so when someone the user wants to
see what they sent that's called the ad
box and we just need a simple index on
it index on author plus time then to get
the last 100 items I do message finds
the author ID sorted by credit
I'm descending limit 100 this is very
fast again so many seconds here for 21
million documents on my laptop it's just
the battery speed and it goes straight
to the right spot in the index and
doesn't need to sort anything it's pre
sorted by the index yeah
so the question is we have a company
next year if I don't pass in the created
does it still work
yes so as long as you specify the
leftmost parts of the index you can't
take it from from the center or the end
but so if I was to query on created it
would not work but created if I do if I
want to find all the messages for that
user I don't care about the time you can
you can do that
so the question is fu the UID is a
primary key that I don't need an extra
index Wow
you can't do that here because an author
is going to have many messages so and
the primary key has to be unique so the
typically the primary Keys never
compounded the only reason to compound
the against the primary keys if you want
to cover extra fields but it's it's not
actually very it's not actually allowed
right now right yeah that is quite easy
yeah so you don't have to worry about
down the square the manga will give you
a unique one right there right so here
you can query on a given message so if
you have the message ID you can get it
from the primary key ID the underscore
ID and then you can also get only
messages for an author all the messages
for an author based on time if you
wanted to say delete all tweets then you
will need an extra cross all users you
will need an extra index on just create
it so that you can select that portion
okay so let's talk a bit about Java and
then we'll go back to some of the
solutions yeah so an index there is a
downside to having an index the question
is is wouldn't you want to just have a
little bunch of item in this is on most
of the fields each index needs to be
maintained takes some amount of disk
space and RAM space and depending on if
it creates random i/o you can actually
become a ball neck to your system so you
have to think if you're missing an index
that requires then your system is just
not going to work at all because it's
gonna start scanning millions of items
before saying anything so missing the
right index is the biggest problem the
second biggest is having extra indices
that are not supposed to be there and
create a lot of slowness on the writes
if you're mostly reading not writing
then you can go ahead and create a lot
of them so Java the Java driver is open
source available on github and maven and
Mongo dejar is a driver B zona jar is a
subset that just has the piece on
library in case you want to write a plot
your own project that uses piece on
basan is actually a pretty cool format
it stands for binary JSON
Java is probably the most use MongoDB
driver receives active development by
MongoDB and the the community the
drivers include of course all the cruds
everything you can do in the shell
support for replica set connection
pooling distributed reads to the
secondary servers it has of course the
piece on stairs at the steriliser
because that's how it's talking to Mongo
it also has JSON see riser this
realizers so that's pretty handy if you
have your document and you want to see
what you insert in database printed out
in your Java logs you can just do Java
json dot serialize and you can even read
from a json file and using the dis
rÃ©aliser
supports grid EFS also so to give you an
idea here where this is the code for
implementing the the messages in this
case we're using mafia mafia is an ORM
that's also maintained by MongoDB and
and so very simple so you create say you
have your message class and then you
create your mafia object here we're
ensuring an index on the sender and sent
ad and then if you want to get a message
you just do find giving the message
class and then you have the field you
specify the filter in there getting a
message you could argue that you just
need the message ID for some reason here
will surpass saying the user ID just to
double-check
yeah the question is any advantage of
morphia versus other ones it's really a
matter of taste mafia is the lighter of
them all I would say the main two are
our spring data and mafia I think there
are some new ones that are coming out or
already available mafia is on the
lighter side is more like sort of a
Google style of a forum Spring data is
closer to what you used to with with
spring in general people have complained
on mafia was not worked on for a while
the reason is that the main engineer
actually joined tangent MongoDB and was
really busy for a while but now actually
have a team working on it now yeah
the question is can you just use
adjacent objects I mean if you using
Jason do you really have to use morphia
so there are two ways you can see your
java development either you want to do
it the java way which means to define
your class with your fields and then you
want to have a way to map the fields to
it in that case you either use mafia or
you somewhere a.m. or you do some sort
of Dao that will fit in your fields the
driver also includes a basic one that
can map directly the other way to see it
is that your objects are just hash maps
and they're like dictionaries saying in
Python so you have your object and then
you you go through it and you get some
keys from it and actually we'll see an
example of that to wait right so if you
use the Java Jason this rÃ©aliser it
will give you a basic DB object which is
a shmatte so it's basically the dynamic
way of doing things so now this is
pretty straightforward I'm gonna skip
because we was shown on time so save
then send by and you can also pass a
list of user IDs this is just to give
you an example of the syntax of morphia
really it's the same as the shell so we
try to because we're trying to make it
as convenient as possible we're not
going to reinvent the wheel every time
but you can see that the GAO here is
very simple now the graph store in this
case we're actually we're actually using
the the barebone objects so in this case
we have two collections two DB
collections friends and followers and
when we start up the class when we
create the the DAO we just obtain them
from the driver so from the database
edges we get the collections we ensure
the indices this is not best practice
ideally the indices are done by your
DBAs not within your codes because you
can easily then you have a developer
that says hey I need this index now and
you push the code in production and boom
is going to create the index and lock
everything for a while but anyway then
if you want to find the friends of a
given user you you create those basic DB
objects with a key and a value it's a
little bit heavy to read but it's really
easy to use because this is really just
a hash map and then in your when you
think of it it's really like a JSON
you're building a JSON and JSON is just
keys and values so - map works pretty
well it's actually a link - map so that
it actually maintains the order of the
keys and then you get you do the fine
you obtain the cursor you go through the
cursor and you get the result
so followers off is the same idea but in
the in the reverse and and then here we
make sure that we only get the fields
that we want so that it's covered so
that's why we're putting in the squared
is zero and oh one by the way that the
keys in the document are just you and
all the reason why it's not to make it
cryptic but it's to make the documents
smaller so that's useful yeah exactly
that's the case for so when you say
follow you can see we're saving in both
collections and follow we're moving from
both this is like the ideal worlds kind
of implementation in reality you would
have a try-catch and then if you have a
cache you typically want to retry
because potentially only one side was
saved
now because the idea so the question is
can you make this a transaction when
you're saving those two part of the
relationship what you have to remember
is that they may belong to totally
different servers so when you go to
sharding and Mongo ass is going to one
document is charted by one key the other
one man another key so you would require
the database system to implement
transaction across servers from you for
you and really you don't want that to
exist because it's so sensitive and then
what happens if one server is down what
happens if one server is slow you want
to control that in your code so it's not
the easiest but that's the only way all
technologies some technologies add you
give you like a transaction on top but
it's typically like very specific and
it's tough to get right if that makes
sense
so how do you end all mistakes so as I
was saying earlier there are different
ways of doing this the heaviest way is
to do a two-phase commit which has been
done for many decades where you can
prepare the first collection say I'm
going to write second crochet I'm going
to write you basically do a pending
transaction and then you commit the
transaction and we actually use this
within Mongo itself but in the case of
banking is the same as if you have your
bank account that's a spending
transaction and after when it's cleared
on both ends then you mark it as done
and then you have actually have a record
if one of them dies instead of a record
that is a pending transaction somewhere
it's not active yet that's the two-phase
commit and I invite you to this is a
good write upon or a MongoDB org now the
lighter solution is to just have a job
queue you take the job okay I need to
write this relationship if you died at
any point in time you will pick it up
again because it's not marked as done so
you just mark it as processing and
that's equivalent to sort of a transact
log reconciliation like what the finance
sector
the finance sector has been doing this
for four decades now without transaction
across systems and so there are many
ways it's a little bit of a first step
but once you get it right for your
application you can work pretty well so
now let's get to the design options and
we have less than 15 minutes and that's
pretty important so four approaches to
getting this inbox right if an unknown
read further on right Becky Redfern on
right and inbox caches final read is the
most simple is a simplest approach you
someone sends a message we write it only
once to the message collection and then
if you want to find the inbox for user
so we need to first find all the people
that that user is following and then
gather all the messages from the other
people Lady Gaga Justin Bieber gather
them all and present them to the user
it's very easy to implement with Mongo
first we need to fetch the list of
followers
so we do DB that flow is that fine for
that user we get we basically get the
list of all those IDs say it may be 500
items okay now we need to go to the
message collection and tell Mongo I have
a list of ID's give me the messages for
those IDs that are in a critical order
the most recent 100 so it's just a
single query to Mongo Mongo will come
back everybody's happy but it's actually
not going to scale so when you send a
message it goes straight to the one shot
that that is sorted by author the
messages are sorted by author when we
read the inbox is going to scatter all
around from a list of potentially 500 or
more then those messages are going to be
gathered
so actually let's take a look at the
explain so here it's not Charlotte so
it's not going to show that it's going
everywhere but in case of shutting
inwards and you can see that it's
sorting in RAM
it's a scanning order true now why is it
sorting in RAM when we have an index on
author ID and time well it's because you
have several branches that needs to be
merged together so each author messages
can be its pre-sorted but here you're
actually asking for a mix of them and
sort them and give you the last 100 so
it it will make use of say it's not
Charlotte you have a 100 followers it
should gather 100 items for each of them
put them all in the buffer then sorted
then give you the last hundred pretty
fast but not ideal you don't want to do
that every time someone logs in Twitter
so any question on on final on weed okay
so final on right so that's pretty sexy
you basically write one message per
recipient now reading the inbox is super
easy it's the same as reading an ad box
so you duplicate those messages for each
recipient and in terms of writing the
the message you do you go you find all
the followers you save in each of the
followers and then when you find you
just have one recipient and you sort by
the sent now the tough part is the right
so if people aren't tweeting that much
but people are reading a lot then this
is pretty good but your message from
Justin Bieber is going to be duplicated
like hundred million times in your
system it's kind of sad
so that's for the phonon rights and also
writing is pretty expensive you may need
to update some indices here and there it
means that the data actually has to end
up on disk there is no way to get away
from it just from RAM because he has to
be written somewhere so depending on the
system if it's more reads well we'll
talk about the trade-offs later on in
the case of reading inbox is very easy
it just goes to a single shard and goes
in order from the index he actually the
actual IO to get those messages we may
still need to get them from around the
disk he knows where to find them right
away but those messages are scattered
around within that shard it's not
covered it's not purely covered by the
index now we can run a quick
optimization I'm not going to spend too
much time on this is to try to see here
how we have to jump around to get all
those messages even if they were
duplicated we could try to gather them
in one large document so that for my
inbox I have messages from all the
people I follow but the system actually
bunched them together so that when I go
to my my inbox the system only has to
get one or two documents that's a pretty
good idea so we can create those buckets
and here for example we are we're
keeping the message counts in there so
you can push a new message and
increments the message counts in an
atomic operation and that's what we
that's what we do in there we first
obtain the sequence document if it does
not exist we create it and then we push
the message in there and we also
supposed to increment the message count
which is not done here and then when we
find the user the documents for the
Inbox
we sort by sequence number
and we can do limit to because we know
document typically has 50 items in there
tsunade Advan tage the the final write
is not that much better it's actually
worse potentially because now you're
growing documents which can be signing
more painful so not that much better or
worse but the reads now are super fast
the database grabs one document two
documents at most you're doing two
random i/o that's the absolute fastest
for for reading so now let's we have
five minutes to talk about the best
approach so let's make sure we talk
about it
what about a cached inbox so the problem
is that some lot of the followers log
into Twitter once a week or don't even
follow Twitter because there is just too
much sweets going through they just like
somebody so they follow but they don't
actually read anything and we are paying
the price to duplicate all those
messages for all those ghost users
ideally we would only populate those
those bundles
if the user is actually active on the
site so what if we add retime we store a
cache of the messages so we do the fan
atone read that's expensive right we go
around gather all the messages for that
user and then we create a cache for for
that so me since the user is reading we
know the user is active then when we do
write say the the Justin sends a tweet
we will only add duplicate to the caches
that exist so say if only out of 10
million only 1 million are actually for
reading Twitter we will only write 1
million times this message in those
caches the other advantage is that we
can make use of a nice feature in Mongo
that caps that array that will
automatically eject the all
once so we don't need to keep like those
buckets anymore we just need one cache
document and tell mangu keep it only one
Android messages in there so the load on
the system instead of having like a
hundred million times every just in
tweets forever it's now just for the
last hundred that that guy posted and
then you can also use the TTL option to
have Mongo automatically clear out the
caches of users who have not checked in
so that's a lot of cool features here
and this is all done from those lines so
if you send a message you want to update
the doc the caches that exist so I have
a cash collection DB odd caches I want
to update where the owner is the
recipient in any of the followers and so
you if they don't match you won't find
the cache and you won't update it and so
what I do is that I do what's called a
capped array so I do a push with the
dollar each is a message sort by
sometime slice by minus 50 that's an
inline basically an in-place operator
that will make Mongo only keep the last
50 for you this is all atomic by the way
so very convenient then when you want to
read your inbox the first thing you do
is look at your cache is my cache there
if so it would be a single document a
worst-case is single random i/o and
that's it else you fall back on the fan
on read that's expensive but hopefully
it's not going to happen too often and
then of course you will get that list
and then you will create a cache for
that user and then the TTL it's not
within here but you just create a
digital index on the caches so that the
caches are older than a
you just delete them so now when you
send you can see the ones that are
dotted are the ones that don't exist so
they won't be actually written to so
we're saving probably about 90% of the
writes here when you read on the cache
hit is a single read that's still
actually even faster than the bucketed
approach in the case of a cache miss
that's painful and you can sort of tune
the size of the cache and how long you
want to keep them and then we don't have
any more time but we already talked
about the trade-offs and in the end I
think we can agree that the inbox cache
is is the winner of course you would
want to tweak depending on what you want
to do and depending on the system
another one may be better I guess
let's take any questions before we
finish yeah so the cache is a regular
document so it would be backed by the
disc if you turn off the Machine and you
start it again it will still exist you
don't have to recalculate them but the
idea is that typically if you access it
it will be stored in RAM if you keep
accessing it it will still be in RAM all
right
I think everybody is ready to two
questions already but don't hesitate to
come see me later on yes so the size
should be up somewhere at least on the
JavaOne websites or otherwise no is it a
to contact meets Antoinette MongoDB calm
and or get my business card then I'll
make sure I send it to you thank you
everyone</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>