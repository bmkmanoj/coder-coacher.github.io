<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Next Step in Automation: Elastic Build Environment | Coder Coacher - Coaching Coders</title><meta content="Next Step in Automation: Elastic Build Environment - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Next Step in Automation: Elastic Build Environment</b></h2><h5 class="post__date">2015-06-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/O0QqboVR7W4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I am I'm coast k i'm the creator of
thinking's and I'm City Okla bees and
here i have disagree he's another
developer in jenkins project than the
boss of autocrat beat so between both of
us we spend a lot of time on thinking
Scott basin and so it well naturally
that's what this talk is about justice
it be context very briefly I think its
audience it hopefully you know what
thinking seeds but it's an open source
continued integration server that we can
you know go to the website to download
and what is that this talk is all about
so in projects in the Dinkins project we
occasionally do a survey about you find
that what user is doing you know they're
so that we could feed that into the next
stage of the development in one of the
survey questions that we had passed
people how many slaves they have and
then we discover that the about
one-third of the user said all right I
only have one machines that is the
racine that run Jenkins master that's
where they do the build and yet in the
same question the eighteen percent to
the user so let's say like a one-in-five
said I got more than fifteen machines
connected up to a Jenkins instance so
that's a pretty large deployment and I
find that interesting that there seems
to be this stark divide between people
who are really leveraging the power of
the massive amount of computers 50 /
slaves on the other hand there's like
you know master amount of people for
only have one box to throughout the
program and that you know it used to be
that not many people actually could
afford to have 50 machines but nowadays
obviously thanks to the virtual machines
and cloud and all that having 15 Essenes
it's actually very easy it's quite cheap
not you so the fact that the people you
know can can or cannot leverage that he
gained the productivity seems to be
becoming more and more important so I
know I know this myself because I used
to maintain my own Jenkins instance
nowadays they took it away from me but
I'm back there now driving it obviously
I'd started from one box so this
as like a very very first installation
of of what became Jenkins in one box in
the 52 CPUs and this was a point running
six concurrent builder buyers project
and clearly single machine wasn't up for
it so on one Christmas break I wrote
this distributed build support in
Jenkins and so started putting a few
more computers the first one is like a
recycle before with some workstations
and I kind of grew from there and that
you know because I just needed those
machines to be able to keep up with the
workload which i think is the one reason
why people need the elasticity in the
build environment or a lot of computers
to throw out the problems if you're
doing it right if you have any
reasonable amount of testing just
keeping them running all the time at
visible frequency it probably defier
you're having more than one computers
you know and in fact if you have more
computers if you just for example run
tests a lot more frequently so it used
to be that we run some of the tests only
once tonight because if I take me like a
two hours to run and that that was
deemed too expensive but nowadays two
hours of computer to the only cost you
what the guy now fifteen cents so it's
really silly not to run them all the
time for every commit and that way you
could isolate the program more easily as
soon as somebody makes a mistake I
kitchen in diam pin it down to specific
individual so i can no longer blame it
on Jesse like the world would see that
it's on me the doctor progress now so in
that sense you have a lot of lot of
computers naturally but also you don't
want the overview the capacity just for
this peak workload like two weeks before
the disease right I when I was doing my
drinking since I was part of the Java EE
groups of your right before the java
wander the lots of deliverables thinks
the ship which means a lot of tests to
run so all our tests cluster would light
up to the full capacity but in the rest
of the time it's basically sitting idle
doing nothing so you know your end up
building this kind of overcapacity
that's unfortunate because this hardware
it costs a lot of money the stadium had
this program that they only get the full
capacity like few hours a week and then
there's still time it's on the imaginal
to utilize you got all and then these
things is costing the capital so in a
way not only you do what not computers
to throughout the program but he also
just want them just in time so only for
the duration of the Pope did the
workload so I think that is one of the
point about the elasticity right so for
you to do that you need the elasticity
in the build capacity so going back to
my little deployment or the under of my
desk at this point my room of 3d getting
warmer noticeably one load by few
degrees so in the wintry that's great
but in the summer it was hell but I
actually have to keep adding more and
more computers and like into the lab and
that the more air conditioning greens
are all these machines we actually
belong the My Little Giant is instance
and that's partly because that we are
shipping software that runs on various
different environment right that's the
sum of the about diverse body prop is
that the write once run anywhere so we
have to test our software own also two
different configurations and you know in
some of those you have multiple ones
like a Linux has a different distros and
their versions if you want to make sure
that you have in a test environment to
be able to build and run these things
you're looking at multiplying these
instances to quite a few numbers and the
way this makes this capacity planning
problem even worse because you know
sometimes it's not very easy to let say
convert a windows to abp OSX slave you
just can't do that so you kind of have
to make a guess as to how much computer
how many computers you need for disk
embargo and it turns out that a lot of
people are doing builds on linux and
solaris of the positive windows because
eyelids faster but you need only find
these things after you started using the
cluster so the making this kind of
upfront guesswork of how much you divide
up your resources is going to be a
challenging problem in some of the other
places
I visited they taught encode solve this
program by in a basically running lots
of different virtual machines on the
hypervisor so in theory they have like a
13 different boxes running different
flavor that the operating system and so
on but they all actually run on a single
hypervisor so if you actually Jenkins
tries to use 13 at the same time like
the whole thing gets overloaded and then
come the grinding halt so the just using
the virtual machine doesn't necessarily
help these kind of programs either so
but the ICPC so I think that another
argument plasticity that is the not only
do you need enough computers in the
right time but also you want the right
kind so that I think also points to the
need for the elasticity but it turns out
the Saudis are very obvious argument I
hope but it turns out to be there's more
to it so again because I was a guy who
was running Jenkins class or people have
this tendency that with the bills
doesn't work on the Jenkins environment
or the test fails on drinking say it
come to my office and basically blame it
on me ready it runs on my computer
perfectly fine but if I ran it on
Jenkins it's not working the tests are
failing therefore it must be the failure
of the Jenkins and I'm not going to look
into it you basically me I'm gonna have
to basically prove to them that it's
their fault right so that was my job so
every time occasionally some of the
knock on my door come into the office
and ask me like Coach K like here I
think the Dinkins instance is like
having program because here and here is
the problem so at one point one in a one
of these problems came into being it
turns out that the at this the program
was caused by the fact that tab somebody
wrote this test script that assumes that
the tongue tip is running on port 8080
so the idea was like you deploy the
server into this tonka you run another
test program and it had some
interactions and then at the end of the
day two Eric and verified it the
development that how it goes but
obviously the disturbin work very well
if somebody is already running port 8080
right and then it turns out that this
script wasn't smiley nasty detect that
and so the Tomcat they start with 5mp
fail but it
tadashi girl so you know in this case
they end up like causing unwanted
interactions between two things that
just happen to be running on the same
box at the same time and then wrecking a
ha book so you know you can imagine that
the troubleshooting programs like this
actually takes a fair amount with time
so that was my my life back then and
here comes another episode that I wanted
to show you it is it yet somebody else
like knocking on my door and this time
like you know they are investigating
this like a magical like a loss of the
server it turns out that the somebody
tried to be a good like I tried to write
a good biscuit so at the end of the test
they had this like a PQ dash f9 tomcat
we basically kill all the Home tab
instances of running in the box just to
make sure that the every server that
they lanced it's done it is kind of
great but except if somebody else is
also running talked up in the same box
and you have this face for a moment and
this only happens occasionally naturally
and there are people who lost the
several things you know this is a kind
of thing that gets test people excited
right occasionally the server to fail a
cure if you look at the load file there
is no trace like what's going on a kiss
at the Brean crash is a pic what's going
on here so you turned up with somebody
off of the pearls good so yeah I think
these episode is actually there are a
lot of those and i think that points to
another important property that evolved
in the field environment which is the
isolation between things different
things that's going on in your cluster
that if you have a sizable dent in step
form and you probably have lots of
things running all the time and then
what many of them come from different
product themes if you try to run more of
them in the same box at the same time
they there is a just a higher chance of
them interacting its each other
unintended ways and you just not trust
me you just do not want to waste your
time trying to check down this program
to remove this PTO if nine tom cat from
their tests gift it looks like my
praised single single line change that
picked me like a studio flowers the
isolate now unfortunately this idea of
ice
waiting every single thing into small
corner it's kind of an odd vc today's
hardly a trend which is that you get a
single massive multi-core systems so if
you get you know like even like laptops
like this you get the 48 course at the
same time so if you want to utilize it
in any meaningful way well you need to
run more things in parallel so I think
this is one of the a hard-earned wisdom
of the Jenkins they administrators to
try not to run multiple things in one
box and then but definitely there are
different ways of doing isolation that
cost differently so on the most
expensive side that you know you can get
the for x86 porter machines like each
contain isolated into the separate
virtual machine that will provide
perfect isolations on the other data
inside like you can do what we call a
duluth loopback slaves you run the
sleeves on the same box but just under
the different user so at least things
like a peak you will no longer be able
to interfere with each other but at the
same time they are still sharing south
on system resources so if they like a
bomb processed at using ballooning up
the memory usage that could constrain
the other piston so you know there are
there issues as well but this is so and
then the summer are in between like a
counter container technology like dr.
which allows if you kind of help back at
strike somewhere in the middle and so
these are all things that be pious
people using it to bring isolations in
telesis be now it turns out that p.m.
these are ready when i talk about the
isolation this isn't the only kind of
isolation that I needed to re about then
I've met this next one kind of get sleep
technical here but it's a really one of
my hardest troubleshooting experiencing
and tired engines history of working on
like a 10 years you kinda have to bear
with me outside they talk about that but
so this is like a huge this is one of
their fortune 500 companies in the class
pay you know and then they have this
like a very well maintained drinking
systems now it turns out that the
distinctness was running a few projects
from two different teams they are all
Maybin built and they are pointing to
they are using the same jar file but
from the different depository I think it
was
every API what I wasn't sure but the
point is this the two artifacts are the
exact same group ID and artifact idea
important actually two different
binaries so if this guy is just doing a
build it works perfectly fine disguising
the build it gets things from there
ended up work perfectly fine but if you
build this guy we stop jar file you do
like break in very subtle ways like some
tests would fail and then so what was
happening me that when they happen to be
scheduled on the same slave even like a
five hours apart like the one that came
in later which fail in the subtle ways
but to make the matter more complicated
this administrator they are hazardous
skip that occasionally cleaned up the
mail and local cache to make sure that
the deal did occasionally get to run
cleanly and I so that would hide the
program code every time in the midnight
the whole local cache get swype that and
then infection it's it's a race for
whichever king there first so well he
ready so the technical detail is
actually did I have a more right um so
yeah so that's funny another one that I
wanted to talk about this video yet
another guy come into my office and drop
in the problem on my lap is that the end
you know the sum of these test scripts
like the guy who wrote the PQ nine tom
cat if it's at least trying to clean up
but most people don't even try I so
there is just like a fork off ya know
the demon process he left and right and
then lucky nerd at the end of it it
could be bit running especially when the
test fails in some abnormal ways I saw
in those cases like the back randleman
process he did keep running and if you
can imagine that over time that the
clogged up lots of the system resources
eventually the slave would stop so these
are the other kind of programs it's also
related to the isolation which is that
be if you try to keep single slave
running long enough time it's so long
horse like a human body like if you live
long enough you start to develop
programs uni been to the you die so it's
much greater and I don't do this with
humans but it's much better to just like
inherit them run for a while while he's
an just
kill it and bringing a new guy right
that's why we call them slaves so the it
turns out that been doing this it so the
save a lot of mental hustle like you
don't want to deal with all this random
test scripts that you're some of the QA
people have written like you have more
important things to worry about and they
have more important things to worry
about this just provide the environment
but more robust against these things so
the you know the this idea of just
throwing away the instance is very
quickly and then bringing in a new one
rapidly that's almost a case that is the
definition of elasticity right so that's
what we are I think trying to make up
each year and there's a one moment that
did is sort of crystallized this very
well for us the Jenkins user there's a
dinking scalability summit last year
where we have the collinear old people
in the delight of stinking Caesars in
one room then they were comparing notes
about that you know that we are talking
about also two things but wonder the
session topic was the monitoring people
did around the slaves you know if you
have if I running like a 300 slaves like
you need some kind of monitoring scheme
to make sure that the things are running
so people from yahoo you're saying like
oh okay i use this like ass blank and
this and that and another peep guy from
sony Mobile it coming to say like we
build this like a graphing system on
their all and everyone was like in a
nodding in and they're so on and so
forth and then the next turn was Netflix
guy and they basically said we don't
really do any more in trini cuz like
I've seen as we detect the first time
the problem in the slavery just kill it
we don't even care to dip into the
problem is bringing the new guy and the
program has been manageable enough like
the whole you know the loss doesn't
happen like that the maître bother
about why that is happening so they
acted completely bypass this marketing
program because they charged us bringing
the new one very frequently so that's I
think the kind of state that you want to
get in yes it's fun and great to write
this monitoring system but it's your
time will be better spent on other stuff
so I think that's sort of like
illustrate the evolution of the typical
Jenkins deployment is that you always
start from single node and now they
start adding more and more slaves you
know in the physical box and about your
machine you started
I saw these pain points and then
eventually you'd book the move on to
more elastic environment we are asleep
you'd come and go all the time so that's
the sort of like a general pitch that
build the idea that in terms of like how
you'd actually do it on Jenkins this
feature has been around in fact a quite
a while now so the first lady that I
wrote to in this area that to prove this
API an underlying concept ECT ECT plugin
so this will talk to Amazon and you can
register multiple a mice and then
different machine configuration and you
could match that up to different jobs
through the mechanical label so you know
you might have three different flavors
and then depending on which money is
necessary Jenkins it automatically
control the workload and it since then
there are a lot of plugins like that
another plug-in that in my opinion 30
named it's aj clouds playing and what
they started this is jake ride a library
we talked to all sorts of different
cloud backends and it basically provides
the same functionality as the easy to
plug in bed but this one works well is
the things like I OpenStack and
cardstock and these things are often
used inside enterprise right so that
might work your knees better and then
support in both cases but they generally
do is in a Jenkins itself monitor the
workload and then based on the walk
loaded automatically launch and killed
on the slate and so when you need to
load turn that a lot of bills like a day
time you get a lot more slaves and app
and then overnight when the bill did go
down the slave gets identity terminated
back to the pool so you save money so
that's the idea up crabbys we build
another plug in the similar space this
one works great with the a breeze PR or
the esxi so the they manage to be my
virtual machines so I'm you know in
addition to be doing the regular
management they could also do the power
on and power off towards the bottom
machines in this station that I
described Dahlia where you have a bun
hypervisor running 13 different books
you can keep most of them powered often
thinking still it may keep our eye on as
needed so you also save it that way
and then this is also due to scheduling
based in a hypervisor manner so it knows
that the deed sleeves that belongs to
all these different environments
actually run on the single box so you do
try not to overload the slab that single
box so that's a kind of general things
that he can do more or less d you know
the same idea just implemented against
different back means a decent new
addition to this family of plug-in
sister dr. plugin I'm sure you've been
hearing about dr. a lot lately unless he
be hiding under a rock so what is
plugging that is it basically talks to
the doctor then on running somewhere
else over tcp/ip and then it'll create
like a one offs label inside the docker
container and then you know you run the
build and then bit on the used ones and
at the end of really do disappear and
they see like commit this image then
publish it to somewhere else so in this
way you can always have a clean
environment and it's kind of easier to
depreciate the same slab environment
across the board so it gets a lot closer
to the elasticity and but them in s.a
silence so I was involving number of
this bugging myself and what I
discovered is that the the the key to
make this it is scalable or the
performance efficient is actually how
you managed the file system the
workspace because if you're bringing in
the fresh slave every time is great but
if you think about it that means for
every build you have to clone the entire
workspace you know or check out the
whole three if you're using maven you
don't have to download the entire like a
dependency change every single time and
these network behind operation new dive
a significant amount of time when you're
built or test execution is like 10 or 15
minutes max so it turns out the poppy
solve this kind of problem you could
really benefit from having a great
vertical integration so in club is we
have this no we host thinking's for
people so you can actually justify
spending effort into building that kind
of auto integrations and so we call that
a motion and I wanted to talk about a
little bit that's the kind of like the
things you can do to really make the
change
shine in this elastic environment so you
can think of motion is almost like a in
a hypervisor the container that contains
slaves you know the slaves get it even
nice placing copies and it's using the
deluxe container for the isolation so
that it's the same technology that the
doc I use behind the scene but so
everyone runs on the same kernel and
sharing a single like a page cache and
so on but each group of processes get
isolated into their own little universe
each magnets their own TCP IP address
and so on so c'est bon you know they
won't interact with each other and then
the corner gives us the ability to make
sure that the one guy isn't monopolizing
CPU your stuff like that so it's really
a handy way of creating a great degree
of isolation without incurring any cost
for other operating systems that we
support like you can't always do that in
fact the cadi knock sensor is are the
only OS that support this kind of
containerization so in other cases it
more expensive so we'd have to run like
a in case of OSX fertilization reactive
and the whole TAMU and different OS
images but even there for example if you
think about the fact that the every
virtual machine is running the same
version of OS X that means that we can
vivir it's particularly known as the
Cornell same page imaging so what it
does is it looks at every memory page of
the deputies virtual machines are
occupying and if they contain the exact
same content they just consolidate
basically that into one memory page so
all the programs are running the same so
they be able to in a benefit from the
single memory page as opposed to having
individual identical copy between a
different environment so so in the end
that it allows allows us to host a lot
more boring machines and otherwise
possible and it provides a great saving
for us and then but real but we all sort
of innovation english spaces again not
just not how you isolate the execution
which there are there on any number of
ways to do it but it's actually the
workspace management so for that we use
the GFS on Linux there's a higher ani in
the
give me for my fungi being able to ecfs
on the after it spotted on the next
thats a tease the the reality of the
life so the among the other things the
what DFS can do you can create file
systems independent file systems very
very cheaply is that committing to the
side up front so what we do is when we
learned slave in this linux container we
only make the appropriate workspace for
the particular tenant mounted over so
that it's visible and then even if it's
not visible we have enough storage to
keep them around so it's like a
workspace it's kind of maintained like a
cache of things and then when the builds
come back to that node again then we
have some warm or workspace to start
with so you have a almost fully
populated memory virtually and some
audio workspace checked out so you can
start from there which is again not
cheaper and then because this is like I
unless it's mounted it's not visible so
we provide necessary I isolation between
those so the AFS or the battery fest or
the neck appeared you want to be a
smaller number of file systems that
could do things like that and the DFS is
also a copy on write file system so what
that means is that the you know if you
have it can create the clone of the
workspace in a constant time and in such
a way that only the mutation are kept as
incremental difference so say you know
the to commit to the same D poultry
lands in quick succession so like I
committed a change now and the jesse has
committed back as three minutes later so
I want to run till like at step test the
same test on these two different
comments more or less at the same time
so in cases like that I could instantly
clone the workspace you know they each
come each slave gets completely
independent work space but nonetheless
they are still sharing the content and
then adds a mutate and we only pay the
price of the actual mutation so that
allows us to quickly grow up to
concurrent fuels or the shrink it down
to the small number of sides so these
are I come to do things like that is it
takes again the special kind of story
management and turns out that this is
also offer a lot more useful not just
for work space so the main university
right the a lot of people actually know
the project share a similar set of
dependencies like Apache Commons project
and so on so if you have a reasonably
populated memory virtually available as
a separate file system and we can just
mind mount them to every slay every app
every build and then you know they some
of them will have to download some
additional stuff but everyone can start
from Kerry born workspace and they all
share the page gasps these things all
adds up determine the speed gains now so
that's the kind of how you could there
are people totally few of the ways you
could create the the Ewok space I'm
sorry idea I lost the field environment
and now I wanted to so the move on to
talk about what kind of things you can
do if you have a reasonably elastic and
highly concurrent a highly capable build
environments so one of the obvious
problems I think that everyone has is
this problem of tests taking too long to
run right if not when they probably
don't have enough tests so the body
turned up so in the drink enjoy the
freedom for the whole test cycle take
like an hour or a bit more longer than
that so it's a it's a lot of time to
wait but now if you look at it most of
these test cases are actually completely
independent so it's almost a very
embarrassing program to just take those
test runs and look at the past execution
time as a guidance to try to divide up
the test in two different groups and
then you can just take those this test
two different machines and run them at
the same time now unfortunately most
tests pre marks don't have this kind of
capability on its own like you know at
most it in the multiple dbms on the same
system but that's that can only give you
so many developer ism so in this plug-in
so they've implemented this this into
the plugin called parallel test text
editor plugin palio test execution
plugging in yes the idea here is that
this so I thought okay let's thinking
spear
I and because we know how long you're
pissed cases tick to run so you can
divide them up to different chunks and
it turned out to be a pest spray mark
can or normally allow you to specify
which subset Iran like they're the
mechanism called inclusions and
exclusions that allows you to say I have
all these tests but I only want to run
here so the Jenkins did intelligently
basically for cough the same build but
by specifying different sets of tests
for each guy sit around and so you can
create a virtue degree of the parison
and I met one of the guys who was doing
something like this we gave me the
inspiration for this plugin and he was
actually creating as many bundles of
many test groups as necessary so long as
each test group would fit within in a
minute budget so in his in his company
no matter how many tests peopled out the
whole test friended still take like I
know 20 minutes or something like a
10-minute weeks post-op setting up
everything and then actually spend
attempt to 10 minutes develop and every
time someone commits to change he'd spin
off 26 different runs that was the
number of the groups that they need at
the moment to get the whole thing rather
than a 10 minutes so that I think is a
tremendous safety that their double
pursuit get atopica piece of my peace of
mind that they get out of the
infrastructure which makes them really
productive now there are a lot more
things you can do if example another
thing you could do would be the very
dated March which is another way of
doing the build so normally when people
use the continuous integration server
like Jenkins it's the developers are
actually committing to the themed
upholstery so in this picture that's the
rightmost and then Jenkins did notice
that the change have happened and then
he checks out the code and do the build
and run the test so it's great that they
are getting the feedback the test
failure within like I don't know like a
15 or 30 minute or whatever it takes for
them to run the whole test but by the
time the problem is noticed that damage
is already done like the changes in that
is in the depository in order developers
are potentially exposed to that failure
so what happens and I've seen it in my
own
is that the as the failures in the sea
otter become more visible people
actually start to become bit more
careful in running a test before they
commit the changes its doesn't work very
well with the idea that you know that
the the tests are getting longer and
longer to take so instead in distributed
volume control system in particular you
don't have to do it that way right it's
not actually it's actually much more
sensible to have the ball / push the
changes into Jenkins forest without
going through the team people and in
this way Jenkins can receive the changes
and then run the test or whatever things
that they configured against it and then
only after it has verified that to be
successful then King script turn around
and then push the changes into the team
Lee poultry so in this way every in
bondage that comes into the depository
goats treating in sin and so you can
make sure that the none of the commits
that you're creating is going to break
the build so it still looked like an
unbreakable build select and so they can
we started using that inside the Jenkins
project as well so now I could I don't
like a happy run the whole test cycle on
my own computer anymore I just have to
hack the changes to the point that I
think it looks good and then I just send
it to Jenkins and if it works in an hour
or period automatically push the changes
longer care for what that I made a
change and I moved on to some other
changes or if you didn't work then it
will send me in an email saying I broke
the build but without embarrassing
myself by breaking news for everyone
else so in this way like I get dinner
it's I no longer have to run the test at
all and then when you are running tests
on the server you can benefit from all
these techniques that I mentioned like a
paralyzing testicle additions you can
have a more complicated fixture so
there's a lot more performance can you
can get out of it so that's another of
the things you can do if you have a lot
of to build capacity in the environment
that you can now compare the individual
push separately so now the next topic
that I wanted to cover the next year's
case is actually the workflow so this is
something they just see and I have been
spending a lot of time
especially in this year and this use
case goes a lot more than just on the
elastic good environment but I think it
shines especially in this kind of
environment so with that I wanted to
switch over to Jesse to talk about this
feature with work what we've been trying
to do is create a system that gives you
more flexibility in terms of how you can
set up the steps that actually go into a
build so uh standard CI builds in
Jenkins follows a pretty fixed sequence
of steps so there's an optional SCM
check out step we get sources from some
version control system check them out or
update them you have a single slave that
you're allowed to do the entire body of
the build on and then you have a
sequence of build steps that you can run
if any of those fail then the whole
build fails and then at the end you have
you have the ability to publish some
results and out some notifications do
things like this when you're trying to
do things like run tests in parallel or
have have these more complicated
scenarios things like validated merge
and so on you wind up having a more
complicated control flow that's really
hard to represent that way and so our
goal with workflow is to create a system
that would let you script exactly what
you wanted to do in the order that you
want to do it and put together little
pieces of Jenkins functionality that you
needed to do this in the right way so
the the main piece of functionality is
going to be your own build script so
this would be a maven build and ant
build make something like this but then
there are some pieces that are very
specific to continuous integration where
you want to get
sources from somewhere you want to find
elastic slaves that you can run tasks on
and you don't want to have to write out
all of the commands needed to find a new
machine on the network and connect to it
and do all these things you want to
publish results so that they can be
browsed by someone else using a nice web
interface so we wanted a way to mix
those things more naturally and so we we
came up with this project we called
workflow another important reason for
doing this which and not really going to
talk about today because it's it's more
of a the separate topic is that it
allows you to keep things running even
even across a restart of the Jenkins
server so this is especially useful if
you have these really heavy-duty tests
that are running for several hours you
want to keep everything going even if
some of the the infrastructure software
has to be rebooted in the middle so that
you don't lose you know a couple hours
worth of computer time just because you
needed to update something and we wanted
to have have the ability to have a whole
pipeline of building testing deploying
all be represented in one place so that
you can see that see the whole logic of
the flow as a single logical script in
terms of working with elastic slaves you
get from a cloud this is a pretty
natural fit rather than assuming that
that a build runs on one slave and that
this is just provided in the background
for you with workflows you explicitly
ask for a slave and you can ask for as
many slaves as you want and what
whatever order you want it's just a one
line command which we'll see in a moment
and you can run a bunch of things in
parallel just by using a pretty simple
parallel construction in the script and
what we've done recently just to show
how this is useful in conjunction with
parallel testing is that we've taken the
parallel test plug in the coast gave is
mentioned
and and provide a mode for you to run
the important parts of that same
functionality from within a workflow
script so i'll show it actually being
used in both modes it's here we have a
test Jenkins instance up and running and
the way that I'm going to do elastic
builds is by using the mansion system
that we were just talking about so for
most people this would mansion would be
used implicitly if you were using our
hosted Jenkins service if you do that
then any build to Iran automatically use
these cloud slaves that we're running in
this case we want to demo something
running on our own our own personal
continuous integration server it's
running on this laptop but we're going
to connect to these machines in the
cloud to do the real builds to do all of
the actual work and so we use this
mansion plug-in where you configure it
to connect to a particular account that
I've that I've set up so as long as you
have a have an account created then it
does all of the configuration needed to
find these machines log in to them with
ssh get the credentials set up find the
workspaces mount all of the files and
everything that you need but you get
whether you've down with you of any any
cloud back-end that I mention these
these beauty date guys wats hobby right
so if you install other plugins if you
the most common choice would probably be
the ec2 plugin then you would get a new
option here where you could allocate
machines from amazon's cloud in that
case it's up to you to set up the right
machine image that you want to pick and
do the authentication to it and those
sorts of things this is a little bit
simpler option so it was nice for a demo
yes
so the the first way of doing this will
just start off right now is um using a
traditional Jenkins project you know
schedule another build a bit to start up
when this one's done so this is just a
project that's kind of a an empty shell
it doesn't really do anything on its own
except say that as my only thing that
I'm doing I want to run parallel tests
and in this case the way it's it's set
up using traditional Jenkins jobs as you
you tell it the name of another project
that you're going to use to do the
actual testing which thing which I've
called sub here and then you can
configure the way that you break up
tests so here I've just gotta I'm going
to break up tests into five batches but
you might also want to say you know I
want to run as many batches as I have to
run as long as each batch only takes 10
minutes to run so my ol my whole test
run is going to take about 10 minutes
but it might take a lot of hardware to
do it so when this this so when this
runs then it's going to kick off a build
of this this other project which is
called sub and here all I have to do to
run this on cloud slaves is say I want
to run it on a machine class of machines
called standard so in in our cloud this
is just a sort of standard medium sized
machine but you can just type in high
speed to get it to run on a bigger class
of machine if you have really heavy
tests and then it checks out some source
code from get and then it it runs maven
to run some tests here so as this starts
running then it's the first time it runs
it some
it's created it's it's only started a
single build of this of this sub job and
you see this is the first time it's run
on this workspace and so the first time
it starts up but it has to do a bunch of
bunch of work to download dependencies
from the maven repository and get
everything started subsequent runs
should be faster than this because all
of this work will be done and the first
time it doesn't have any idea how long
any of these tests take it just as it
just knows that you have a bunch of
tests in your project so it's going to
run all of them at once and so this
first run is going to be slow it'll
start off work flow based variant of the
same thing as well and as soon as this
is done collecting test results which is
now then we come to the second build of
the project so this might have been
kicked off by another commit to get or
whatever you have this time it's going
to go back to the first build and it
knows that the first build adds some
test results and so it goes through all
of these test results and it pays
attention to the to the amount of time
that each one took and it's going to
collect all of them into a bunch of
category so it's just going to try to
randomly sort the tests into a five
different bags so that each one takes
about the same amount of time and so it
does that calculation and it said that
we we divided 50 seconds worth of tests
into five sets and then it's going to
start out five builds of this sub job
and you can see that it's already
allocated these five different machines
from our cloud to run all of these
builds simultaneously in each of these
builds is
when we're going to run a small subset
of the tests each one is going to run
about twenty percent of our total test
suite it did this by just saying giving
it a specific list of tests that this
particular machine is going to run so
this one we took 15 seconds that was a
lot faster and it's running all of these
in parallel now we're also able to do
the same thing with with workflows I'll
start off let's let's take a look at the
configuration of this so if workflows
everything that was sort of implied in
that in that first pair of jobs is now
made explicit so it's a little bit more
more work to write it the first time but
the advantage is that you get complete
control over exactly how all of this
runs so here I just expanded the script
so you can read it better on the screen
so the first thing that we're going to
do is we're going to run this node block
and this is all written in groovy script
so you can do anything if anyone is use
groovy so it's a convenient scripting
language for based on Java so you can
use Java syntax if you don't know any
groovy syntax and works just as well so
first of all we're going to say we're
going to run something on the master
machine or just Sun something local that
doesn't require much hardware and to
start off we're going to check out our
sources and then make a copy of the
sources that we checked out we're not
actually going to do any real work under
a local machine at all just the check up
and then we amend that the magic part is
is this call to this function that gets
defined for us when we have this plug-in
installed it says split tests and split
tests just does the does the hard work
of looking at the previous test results
counting up the times of all of those
tests and figuring out how they should
be
split into a bunch of different buckets
and the result that we get back from
this is just a list of list of desired
machines and their contents for each so
here I'm creating a map in groovy and
I'm going through all the splits and
each of these is going to be one slave
machine run that's going to happen and
so you see I can just use simple
language syntax with closures and
everything to capture all of these local
variables for easily for each of these
machines I'm going to pick up one of
these cloud machines and then I'm going
to copy over the sources that I've
already made a copy of and then this is
just um we'll make a little simpler
syntax for this in the future right this
is still a work in progress but this is
just recording the list of tests that
you want to run or don't want to run and
then we run maven so we run or subset of
tests and then archive the results and
then we come back at the end and we
collect all of the results from all of
these different test runs and we finally
record them all with the j unit format
test results okay so one of the things
the dfl gives you is just like I've
needs to look at flexibility if you
choose they did just like thing you want
so when we are showing it in the
Freestyle project like you know the tech
guys what's happening on every single
build and if that happened to be what
you like now it's great but if you
didn't like that then you're kind of
like a you know swimming against the
current in rockford it's the
programmable approach that you have a
lot more composability and flexibility
to choose these things sigh like that's
one the thing I like about it so in this
initial flow everything again ran in a
single slave because we had no idea
which tests were slow in which tests
were fast here you can see sort of a
table view of what happened in detail
throughout this flow the main work was
here
we're running maven but then if we
schedule a new build then it's going to
it's going to start flow number two and
then that's automatically going to
schedule build rebuilds of all of the
particular test runs and so now the the
cloud implementation on Jenkins has been
notified that hey I need about five more
slaves to come up right now and so it's
right now it's requesting machines from
amazon and booting them booting the
virtual machines with our predefined
slave image and then connecting to those
with ssh and attaching a pre-existing
workspace to them and setting up a tank
and slave agent on them and after a
little bit it will start to actually run
our tests on it in this case because
it's an artificial demo the tests when
we take 20 seconds and so the time
required to set up the slave is actually
a lot more but if this but if each of
these blocks were something like half an
hour hour then this would be saving you
several hours of time and hopefully you
can see that these these are not just
useful for making a pyroclastic solution
but it could be any activity that we
need choreographing or two different
machines you know that guy when I delete
things I need to generate the installer
for all sorts of different platforms
it's actually run on different machines
that's my Cadillac like a great use case
for me just like Rockford yeah or is
there things where you have to do sort
of very specific kinds of setup for your
environments you want one machine to run
an application server with your
application and then you want to boot up
another couple machines to run so I neum
tests against it that sort of situation
becomes easier when you can just write a
a pretty short script that says how the
different machine should interact this
way all right ok so if leave some time
for questions and wrap up yeah so just
that so we could have just keep it
running and if you're working but let me
just steps see to work quickly to the
last slide if you wrap up
yeah the rapper / talk Saudi so I think
the point to me d the argument for
having elasticity in the build
environment it's pretty obvious and
pretty strong you need the capability
need enough capability to keep up with
your workload but just in time you want
to provide the isolation in very teeth
in their 50 way and you also want to
have a lot of different build
environment if necessary without having
overhead and if you spend a little bit
of effort creating that environment then
you can get a lot of productivity
getting out of it and we just took that
the S you use cases like RL testing and
validated margin rock so in general but
I think this is a space that the people
are actively exploring right now so we
said I think if that's basically what we
prepare the slides if anyone has any
questions with happy to answer right max
ah synchronization though so the I think
the the synchronization between threads
inside the same walk that's something we
already thought about then so there's
one put that that's lucky you get out of
this so naturally i think the another
one is the Bobby Pollard stages I think
that's like a counter the
synchronization primitive reach the idea
is like if you are building a Content
see if I find that house like a build
and test and deploy well deploying to
deteriorate environment if you only have
one QA environment like to see realized
that the execution p1 so you could
define those three things or three
stages and tell the norcross script this
space deployment stage has the
concurrency capital one and then the you
know then the waiting and so on and
happen and your own Dame snored may
dislike a whole talk out of this if you
remember and so in your neural Kim like
we're calling this functionality to
James not operator so that he see one of
the chosen few to have to operate you're
named after him so I think we provide
some of that
okay study right to the question was
like how do we share the work spaces
when we say this like when we are
talking about this much stuff so I think
the easy ready to think of it is well I
don't know what's the easy way to think
of it I'm mentally picture these
workspaces maybe like a network attached
storage I mean obviously the same the
whole thing happened inside one box in
one kanal right but I'm in modern lenexa
can have different process groups seeing
different file system and so these like
a one slave it's like a bank process
group they see one set the file system
mounted another process scrip another
slave sees another set of file system
mounted so there's a program that's
running at the root on the Smashing host
and it's controlling all these file
system mounts and creating a new color
containers and things like that so well
like I said this is a fairy like I in a
vertical integration stuff that not much
people with care of it but I mean not
much people could afford to spend time
on but you know we we actually I think
Travis is one of your companies that we
could actually do it so this is a
service that's available on last but we
haven't like as item explain the recipe
we haven't made this software ready
machinable by other people yeah so the
mushroom plugin that like we use for the
demo is the dink inspiring that talks to
the mush and that we have on the other
side of the Internet yeah so again the
related question would be whether you
can get the same effect with EC to play
okay Lisa little bit you're right can
you do this is easy to plug in well we
can't you really can't and if you could
then we even be building this so they
turned out that the spec are doing a
proper file system management it's very
painful so well yeah anything else that
we could actually for this demo we
didn't use any of that in this little
simple demo air just sending sources
over to the to the slave each time but
actually the local maven repository cash
being warm
makes a big difference for that
depending on the size of your cash for
some projects that doesn't matter at all
but for some projects is very important
or four people doing subversion
checkouts if they have a three hundred
megabyte subversion repository and
somebody commits a change that adds one
line to it read me and subversion is
very efficient at finding that one file
that's changed and sending just that one
line of text over to an existing
workspace right if you have to do a
fresh check out if the whole thing then
your build suddenly adds another half an
hour right anything else I guess if
that's it then thank you very much and
then we have a boost seen that by the
way the extra-tall so we'd be hanging
around so if you're interesting talking
about this stuff then please or pie and
get the t-shirts and so on well thank
you very much and have a good diver</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>