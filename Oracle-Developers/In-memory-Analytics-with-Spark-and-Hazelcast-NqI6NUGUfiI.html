<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>In memory Analytics with Spark and Hazelcast | Coder Coacher - Coaching Coders</title><meta content="In memory Analytics with Spark and Hazelcast - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>In memory Analytics with Spark and Hazelcast</b></h2><h5 class="post__date">2017-07-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/NqI6NUGUfiI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so my name is
my name is Victor gamma Ayane with my
quad come with me I work in Solutions
Architect I have a developer advocate
and I'm putting weird pictures and
Twitter so you can follow me on Twitter
I'm very interesting person now so we're
going to talk about two things it's
Apache spark which is what is the
passion spark if you never work with
Apache spark before I will give you some
introduction and maybe discuss some of
the things that people usually not
missing from from documentation or they
stumble in some problems and after that
they try and understand what what what
the heck is going on
so the Apache spark is a general-purpose
computational platform meaning that you
can do pretty much any type of
computations on it
and despite of the common belief that
Apache spark is somehow depend on Hadoop
it is not so patchy spark started as the
is a separate project in apparently
people in University of California in
Berkeley they were tired of Hadoop API
and was not something something that
they get a lot of pleasure to work with
so they start with this the project and
they after that they open sourced it
right now it's under Apache foundation
umbrella so this is kind of common thing
that you hear when you win the people
talking about the patchy spark is that
it's faster than executing some of the
jobs on Hadoop and there's couple
reasons for that the first of all I
guess is that during the time when the
Hadoop was invented and the way how the
Hadoop produces run different
computations actually could do save many
steps in into the disk and it's not just
the gist but it's the story in HDFS
which is
Hadoop distributed file system and this
this like batch operations will require
some time when you dump data or read
data from the disk etc so eliminating
these kind of steps and providing more
sophisticated very sophisticated and
simple to use way how you can create
safe points of your computation helps
work to make it much faster so when you
want to use this part so first of all
Apache spark is tool for task of data
science so data science is it is modern
modern term and usually the main idea
behind a designs the people trying to
answer the questions before these
questions were even asked
so they trying to research the data
desire trying to look into data
understand what what else we can get
from from it so sometimes when you for
example running some the online
transaction type of things you pretty
much know what kind of question you want
to ask but with data science the
questions more likely you're trying to
explore your date you try to understand
what's in it and sometimes when they did
like this the the exploration in the
date in the research in the data some of
the some of the information is not it
information is available but what to do
this information already knows and after
as a result of some data science that
tasks it can be some new feature and the
product because after running these
different formulas different
computational pipelines you get the idea
what kind of stuff you want to display
or to users for example and it's more
like in this in this moment it's not not
about maybe speed on how fast you
good answer how accurate is it but just
the way how you research and how you
find something interesting in your data
and after that data processing test the
actual tend to run your production for
example you you're getting some of the
data that come into your system and you
need to produce processes data and you
have a lot of data to the process and
because of the nature of distributed
computing that spark has this is very
good candidate for solving this or third
option you tired with Hadoop it's so
slow it's too difficult to to write code
and the problem with natural fluid the
hotel but the API of Hadoop and this why
there are so many different API is
different DSL on top of Hadoop to make
this this Hadoop computation of writing
add up jobs is more you know get a
little bit more pleasure than writing
this so that's why the when you working
with spark and if you a developer you
feel you feel in a good fuel yourself in
a good place because you ran in a code
and your jobs are looking like a code
and everything's nice so let's a little
bit about architecture of typical spark
cluster or spark deployment I would say
so there are a couple components that
usually involved in the sparking a spark
when we're talking about spark but
actually includes multiple complaints
the first thing as the developer you
interacting with a spark cluster through
the driver program driver program can be
written in Java it can be written in
Scala can be written in Python so this
all this API is available and this Scala
or java application interacts through
the object called spark context there's
Scala context there's Java context so
the driver program is the
problem that would submit George and
sometimes it's actually not not not when
the people started working on spark
sometimes to get confused with like
where the codes actually will be
executed within in the what particular
moment of time so the driver program
interacts with cluster manager so
cluster manager here it can be a simple
cluster manager that just represents
some local one one single credit
application so on my laptop I'm going to
use this because it's easy for testing
easy for development however in real
life you rather use some of the existing
cluster management solutions for example
Hadoop yard or missus and the cluster
manager will be responsible for
scheduling good tasks send integer the
tasks an appropriate notes for execution
and the worker node this is exactly
where the data will be processed the
worker node will cancer will receive
computational tasks will receive some
data through the RTG will talk about RDG
in a couple more slide so and some of
the computation by the way can execute
it on the drivers so sometimes people
get confused when so you know they try
to run some of the computation with like
big amount of data and boom some driver
program crashes because the some
intermediate steps will require data be
collected for example and collect will
going to happen on a driver program and
at my crashing so the basic idea is here
you have a driver program we have a
worker node and you have cluster manager
that will communicate and submit
particular tasks that so our tedium so
the basic API concept for SPARC is a DG
which is a result distributed data set
basically it's the distributed version
of collection so some of the objects or
some of the some information will be
stored there in
in in distributed fashion and Sparkle
take care of distribution of this data
and usually many other providers that
integrate to spark they expose their API
through our DG there's some built-in
entities like you can create Rd
different file or there are some others
like you can create a digit from from
Cassandra database or with hazel cast
member grid we can talk about this in in
couple more slides and the most
important part is that with our DG data
can be processed in parallel so in this
case a bit by scaling the spark cluster
you will get ability to to process more
more data through the same same time so
the way how it looks like in this
particular case in this example the RDG
splits data in five partitions so I have
some items from one from 225 and
basically based on the key usually it's
a typical way to define the partition or
shard where the data will resign so it
will take the partition key in the
boutique in calculate partition a key so
where the data will be placed there and
the spark workers can interact with this
partition gaining data back and forth
okay now so we we have our data in our
sport cluster so data is loaded there
now let's talk about how we can operate
over this how we can interact with the
with your DG so the rail I can say two
types of operations on top of our DG you
can do transformations and the actions
and I think this is very good very good
way to remember what is transformation
which is what is action so every time
when you're like API call returns our DG
is transformation if it returns some
value or it returns some some some other
collection
but not reaction so the difference
between action and and in the
transformations that transformation are
lazy
meaning that you with up working through
the API constructing an operating on
different Rd gzn diplomatic different
methods for transformation you just
defined how the data will be transformed
it doesn't a kick kick ian actual
processing but when you start action
that the action will actually start the
computation so for example in terms of
what kind of method our transformations
and what methods are actions so for
example couple API functions that do for
example map that will apply function to
all elements in our DG that will visit
just the typical the transformation of
the data flat map will produce another
DG that will contain sort of flattened
version of our DG's that were placed
inside etc the different the grouping
for sorting joining this all operations
are transformation meaning that the lazy
they not start actual execution of of
this computation with actions this is
this actions that allow you to actually
start the computation the operations
like reduce or collect that will
basically what but what they do they
producing director cyclic graph that
represents the dataflow direct because
data is not flowing back at the rail
data is only flowing forward and this
this directed cycle graph would be
executed on on cluster
and as a result remember I said this
will produce some of the numbers and you
can also writing it directly to file or
you can write to another LGG for example
you can one of the one of the steps of
your computation you perform the
computation and the result will be
producing another energy but in this
case it is not lazy operation you
actually transforming data and kicking
in actual computation there is some
other method that allows you to store
intermediate results for example there
is method persist that allow you to
store it either in memory or in file
it's kind of it is it is kind of
operation that allows you to create like
safe points so when you can return after
after you need persistence basically
this is the way L how they achieve fault
tolerance in the cluster so that you can
persist some of the intermediate some
intermediate results and the the
basically spark also tracks the
intermediate steps and make sure that
data will not be will not be lost so the
way how we construct our DG so usually
it's a for example here I have a
screenshot for the the spark shell that
interactive tool basically it's a Scala
let's color apple that has all class
path in some of the functions that
related to to spark already loaded and
it allows you to without like actual
coding start doing something with I
start doing something good spark it's
actually very awesome tool if you try to
teach yourself it doesn't really
necessarily require you to learn Scala
but it will you will you will learn you
think about it's a DSL for for spark if
you don't want to learn Scala as I am
for example so yeah in basically there
is a different ways how you can create
the parallel version of this of this RDD
so have at least array list of some some
date and we can create a parallel
collection as that executes some of the
stuff plus I mentioned that spark
doesn't have any dependency on Hadoop
per se but it was designed to be used in
Hadoop in our infrastructure that people
used to do previously so in this case
it's also good to have some of the
out-of-the-box of abilities to interact
with adult and including a group data
set plus there's a ability to read from
HDFS read from from from from from
different different system so but they
have a support for for Hadoop so
basically this is a kind of minimum
information that you need to know to to
start doing something to spark the once
again we do have concept of RPGs which
is defined the way how interact with our
data there are transformations there is
actions information lazy actions are
eager now let's talk a little bit about
what is the hazel caste and what is a
memory data grid so the do we have how
we define the the memory data grid is
the operational data store that use
memory ram air or like a Java heap or
Java of heap in particular to store the
data and provide fast access to this
data for the parallel execution of
computational tasks so where for example
use in memory grid so in our case the
the memory grid like hazel caste is
designed to be of relational storage
store meaning that the date access to
data latency should be minimal and this
is basically way
how you can perform your old GP
operations in in the systems because of
the low latency of operations to get
input so and this is the basic basic
basic use case here is you know fast
accessing some of the data that
previously was computed for example and
or it was computed overnight so here a
couple use cases so high density caching
use case is the I would say one of the
maybe 75% of the use cases how the
people use use Scheduled Caste
in real life so basically it's a
distributed caching system that support
ability to scale out so basically you
can bring more commodity machines and
support ability to scale and up so
increasing the hardware capabilities you
can run everything in in-memory on one
one big machine high-density mean is
that the data instead of like a spread
the data across commodity clusters you
can actually have it in insight one big
big machine some some people might know
that in the memory right now becoming a
cheaper and cheaper and even in cloud
you can get one terabyte machines from
AWS x1 instances so in-memory grid also
including the computing capability so
once the data is already loaded so it
from one point it's kind of wasteful to
to move data around so this why with a
memory computing the in memory grid
computing to support ability to support
local online transactional business use
cases right so data is not moving around
data in place and we can send different
computation different computation tasks
into different nodes to prepare perform
certain transformations for example we
need to the increase the salary for
everyone and the data stored in the grid
to traditionally how we can do that we
can we need to bring the data to the
client we need to
changes in back put it back to prevent
from with the concurrent modifications
we need to put the locks around it in
this case we will increase contention
and it would be not so superfast so in
this case one once you send the
computation to the grid the old records
will be updated inside the grid and the
computation would be significant the the
the payload of computation will be
significantly slow smaller than you know
moving data around and for these days
for architecture that required like
integration between micro-services many
our customers they use a hill cast as in
the inner so when they in transition
into micro service world the u.s. kettle
cast as a data the backbone basically so
different components of the system can
interact through in-memory grid and
button generally like the our use case
that that we use for greenfield projects
people use the kettle cast as the
scalability tool for their micro
services so when we're talking about
micro service we know that the service
needs to perform one particular task or
like small number of tasks but no one
actually said that it's going to be just
a one service I mean like one process
that runs so this service needs to be
scalable and having the common view to
the data between multiple nodes this is
what halo cache provides for micro
services and for for typical web type of
use cases where we have web applications
that need to scale the web sessions
across multiple instances of application
server and this is the way how it can be
integrated we do have support of just
pretty much every server container
because we provide generic session
filter that allows to use it pretty much
everywhere WebLogic and tomcat GlassFish
some of the implementation for example
glass roof commercial version payara
has
cast session clustering built-in so a in
couple words so in memory data great it
it's open source that attribute to
license its distributed version
basically distribute versions of Java
collection API distributed versions of
Java what's the name of concurrent
package executor service Atomics atomic
long and it provides also messaging
capability so once the the different
application will talk to hazel cat they
also can use the messaging capabilities
like a queue or topic or a ring buffer
to interact between different systems
and also hidden cache has computational
capabilities including MapReduce we used
to have the this in-memory MapReduce
capability and kind of funny because API
is also highly inspired by a Hadoop API
so not that lots of pleasure to use this
one now so you can think about this okay
so SPARC does computation it doesn't
make it a compute grid no spark is the
general-purpose computation engine spark
doesn't have any storage so SPARC is
just running the data that you feed this
feed into it in the spark right the data
that was a memory so he look as provides
storage Detroit came in and
computational meat now from perspective
of data distribution it's also very
similar to way how the spark for example
distributed data there are two component
usually like to type of data storages
that data buckets we call them
partitions the partitions there are
primary partitions that store data and
there are Becca partitions that store
data somewhere else
so in case of failure one of the node
will always have some backup copy of
another node so this wide provides fault
tolerant ability for data storage for
operational data store
now and for on perspective for user for
perspective of developer data looks
pretty much as it separates with just a
concurrent hash map but in general we
taken care of data movement data
distribution we're taking care of a
migration that we're taking care of data
when network partition happens for
example etc now so the way how it works
with sparkle detector so hazel caste
provides two type of oddities that can
be used as a data source for spark job
or it can be used as the other as a
source sorry as a sync as the is a
target of result of the spark
computation so when I said usually the
spark is a replacement for Hadoop which
was suitable for batching tasks so we're
running this task of our ninth one once
the days close like the trades are off
and you can run some of some of the
analytics so and handle cast is designed
to be operational like the it designed
to be 24/7 available so in this case
some of the operational data can be
stored hazel cast after that during the
night it can be pulled into spark
cluster to run some of the computation
some data science test a task some of
the exploration tasks plus end result
will be a push back to the head ok
another option with with ability to
spark producing in streaming fashion so
it not like a spark itself it's not like
streaming solution for for processing
streaming type of data they have a
component called spark streaming that
designed to do this task so in sort of
combining to two approaches where you
have operational data store plus you
have streaming use case you can actually
use the spark map for large competition
but like very fast not very
the time-consuming oppression for
example you can do online a fraud
detection online rate limit of of the
data and data that's stored in inside
the hisoka's grid can be like a front
for your application to tomato juice and
spark on the back can produce this
different different tasks like this at
the fraud detection or the if you like
in API business you want provide limits
for for your PS and you want to check
because it's distributed API call you
have distributed front-end and in this
case it's very good very good solution
for that one so the way how it works
with witch hazel cast and how to to
connect a hazel cast we need to create
as the spark config in the past this
properties in spark config which one
will define one of the addresses of
running hazel cast member and other
things like how to read and how to write
to or in hazel cat so next step is I'm
using Java here but in Scala it would be
very similar so in this case I creating
the Java spark contact that will connect
to to cluster this example how we can
connect running cluster my driver app
will connect to the cluster manager that
deployed on this in this case is
localhost but it might be any other and
the way how to interact with hazel cast
it will be able to read data inside from
the hazel cast map and or it can read
data from hidden cache cache so as I
said hazel cast is 75% use cases it's a
caching so that's why we have two
different data structures one of the
data structure is map which is key value
and it's suitable for caching use cases
in in general but we also support the
API called J cash which is Java standard
API for for caching so basically it
allows you to work with any J cache come
client cashing providers without going
into proprietary details so if tomorrow
you decide to you know drop your
expensive Jay cash provider and go to
open source free Jay care provider so in
this case if you didn't use any
proprietary extensions use only JQ API
you can do it just just simply replacing
jars so for supporting Jay cash in hazel
cats who also have data surgical care so
we have a map and have a cache so in
basically how how this stuff is it looks
like so I will demonstrate to you some
of the some of the code example that I
have here
it's Oracle code so let's see some code
right so in in my presentation I will
show you some of the things how the
hazel cache can interact with web spark
in basically in general you also can use
the same approach to to use it inside
spark shell for example when you for
example can merge RDG from hazel cat
which is in memory map with some RDG
that you receive from the file and you
can see like if you find some
correlation here but the way how it
looks like so this is typical example of
running I guess it's some overage some
overage tasks that I have bunch of users
that randomly generated and I put them
inside the hazel cast as you can see
here I create an instance of a hero cash
client that will connect you to my hero
cast cluster and just simply putting
some of the user names and after that I
will use this data to calculate the
average age across all these randomly
generated users so I do have my member
in in general the
creating here look as the member
instance if you like like this for
example so basically there is a the
factory method called factory class ok
ok that creates new instance and after
that from instance we can get access to
any data structure that resides under
line so if I'll do its get map I can get
yet map get multi map but we did spark
connector doesn't support multi map yet
list for example we can get into various
African to the list but map is typically
it's our data structure that very widely
used so let me let me start these this
application so I can show you what
what's it what would what do we have
here
so this console app is not seeing more
like it gives me some sort of repple so
I can look inside my cluster and see
what happens there
so one while I'm running this one it
will by default it'll cast uses
multicast to discover other notes
hopefully a multicast will work here
support unroll origin use so it looks
like I'm running multiple hey look at
stuff already somewhere in in my
configuration file to sort of remove
this limitation I can do in my network
configuration there is a port I can say
how to increment so in this case it will
not clash with any running I'll do out
increment through port count equals 100
so it will out increment up to 100
hundred ports and I need to close it
then I will start with port 5 7 0 1 it's
basically just default 1 plus with the
configuration of join method
by default it uses multicast but I
already tested here with with the
conference Wi-Fi multicast doesn't work
pretty well so I will stick to just a
TCP traditional TCP I disable multicast
and I can use tcp/ip also hidden cast
around in any possible cloud these days
so we can support natively AWS discovery
issue discovery we run inside the
pivotal Cloud Foundry we support
discovery in Imperial Cloud Foundry
we're running like kubernetes we support
EGCG with support console support
zookeeper
so all this nice stuff and as a
discovery I say member I'll do 27 there
is no place like home so and I need to
also enable this one so I can run it ok
so let me run it once again so while it
starts it prints out a lot of
information about like what diversion is
running so still doesn't doesn't pick up
my my method at for some reason user
spawn one but really doesn't matter for
for purpose of this demo so it starts as
a standalone it should print me
information no it actually matters so
sorry about that but I need to kill some
other little cast potentially where I'm
running any kid who gets here you know
so we're here I know where I'm running
this one because
okay so it's it's bad I don't know why
it's asking me to run okay okay so in
this case I will simply do you cast
instance and see if it will pick up
right right xml file
okay now we're talking now with the
fix-up correct correct file now from
from my from my demo I will connect to
his Lucas it will start local so as you
can see when I start spark with this
kind of special word called local and it
allows to start spark in like single
thread mode in this case I use two
threads that define this mode and here
I'm passing configuration that I want to
connect to this class remember and I
will use this this the same host as a
driver so everything would be running on
my laptop or laptop so probably will run
out of space so yeah in this in this
application once again I will remember
we run some of the users we put them
inside the kettle cast map and after
that we execute on this user strategy
that we retrieved from from spark we
execute this operation that will take
edge so in the difference between the
way how the scala for example looks into
your collection the day it's like a
sequence of tuples in in traditional map
it's like it's a it's a collection of
entries basically so in this case it
will take the tuple which represents
string is the key and the user is a
value and it will take the second
element of this tuple it take this take
edge edge of this user and create a list
of it and after after from from this
list this flat map to double will
produce another LGG which will represent
the sequence of edges and after that
this terminal like this action will be
executed to produce actual result and
after this all output hopefully we'll
see that randomly generated
average edge is 46 and 48 minutes 46
into 46 years okay now as I mentioned
you can also write result back into the
hazel cast so I have a task another task
that will the push data back to hazel
cast so so what it does in this case it
will use again some sort of some sort of
word count
what counts in that scenario I'm sorry
and typically people saying that if you
understand if you starting here if you
starting your big data journey and you
want to write some of the big data stuff
what count is your hello world if you
understand how to do word count in
MapReduce so you can actually put the
the big data and MapReduce on your
resume so yeah so basically in this case
it will will create the work out of some
of the data and after that will write
result into the hazel caste map called
counts that I can read after for in
another in other application so in this
case the for example you need to expose
some of the results of your spark
computation to the front end which is
dotnet for example or not GS and thing
is that there is no API yet for dotnet
or for not J's for spark so in this case
you can use intermediate operational
store like hazel cat that provides no GL
c sharp c++ java api and python to
expose this data so you have your the
computational platform and here a
front-end storage plus we've got net
potentially you can also provide access
to another type of data scientists who
use excel for data science type of thing
so once data from the spark will be
dumped into the grid using
the.net client can connect you from
Visual Basic for Excel using dotnet
client connect to the cluster and run
evidence everything in Excel spreadsheet
so we have this resulted of the word
count of some of some text and this
result would be written into hazel cash
map and the way how it will look inside
the hidden cash map it will be key value
where it key will represent word and
value will represent the word count
so there's couple limitations that I
want to mentioned so so I want to make
it clear that in general tools like
Hadoop and spark they were designed for
batching use case meaning that your data
is already here and all you need to do
is to otherwise started from the tonneau
it's fine
your data is already arrived so this
kind of approach of batching approach is
not suitable for dealing with streaming
data so this why like data should not be
updated during the time when this part
will read this or respond sparkle
writers this white like typical batch
where data is not changing so the way
because the way how it's in trill and
internally organized in hazel cast in
its internal realized in in spark we're
trying to align or sparkle of spark
distribution of of data so for example
in some cases spark can gain much better
performance if sparknotes collocated
with some HDFS storage so in this case
you deploy a spark right next to your
HDFS your Hadoop and in this case you
will gain more performance because they
can align the partitions how the data
partition to cut open in spark so when
the when we change something or the
growing cluster with hazel cat
data will be reshuffle data will be
shuffle across the cluster
and this case it might change the way
how to iterator that we use internally
to retrieve data from from hazel Cass
the way do we have it's pretty much
pretty much when you're trying to change
the data while iterating your data so
what do you get in Java
grant Kiser modification except right so
in this case it distributed can calc
modification exception however you might
get either duplicates or some missing
data so some some of the corrupted data
so there's a gif coupling - kettle ket
spark so and it's a couple like closing
remarks I almost almost done with my my
presentation so first of all we do have
a reference application that
demonstrates how to Java 8 and the spark
can be used and hazel cash can be used
is called bad lab well part well part
Leopard
Beth Leopard so it is it is the white
paper that explains how it's done and
basically
cablecast runs is operational storage
for for managing bats and a spark used
as the engine for fraud detection so you
can get this from simply just ping me in
Twitter out since you link if you will
not find this I forgot to put this on on
the slide and another thing is that we
in hazel caste
we also working on streaming engine
let's call hazel cast yet and the hazel
cast yet also has a similar concept of
sparks it also works on top of on top of
concept of direct acyclic graph but it
is totally different story I probably
will tell you next year on the Oracle
code so I don't have anything else to
tell I have plenty to tell but right now
I don't have enough time to talk about
this if you have any questions to follow
me on Twitter and with your questions on
Twitter or you can ask your question
right now if you don't have any
questions anyone any questions yes
please
everything I just described it's open
source the commercial part is so we we
commercial open source company meaning
that we support the providing support
for the open source version plus we
develop some Enterprise enterprise
plugins but we use open source as our
foundation for building this plugin so
basically you can build these plugins
yourself but it's just like matter of
buying versus building so in we have
multi data center replication so you
have multiple hazel cast clusters are
colores or physically geographically
distributed areas so we provide
replication across this areas it's a
part of commercial tool we're providing
off heap implementation when you need to
support your you know terabytes of data
on one particular machine so it's called
high density identity memory and I guess
some of the security plugins it's also
in in in Enterprise version but all the
stuff that I talked about like
foundational core like all data
partitioning everything is for free all
right thanks for your time I hope some
something in the my talk was useful for
some of you and once again if you have
some questions there's my contact feel
free to to contact me I will be happy to
answer your question thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>