<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Java Caching (JSR 107): The State of the Union | Coder Coacher - Coaching Coders</title><meta content="Java Caching (JSR 107): The State of the Union - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Java Caching (JSR 107): The State of the Union</b></h2><h5 class="post__date">2015-06-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ciA7rDr1vSg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so this morning we're going to walk you
through the State of the Union in Java
caching it's actually the happiest story
that it's been in 12 years I think Greg
you started like three years ago about
five years well I got involved in the
expert group about five years ago but
only actually started work about three
years ago yeah and then in the last year
well actually we actually presented here
and I was probably audience about half
the size so I'm glad to see this
audience unless you have just like
spilled over from the lambda talk this
is not allowed to talk by the way
I know lambdas cool but this this crowd
makes us feel confident that Java and
Java caching is still cool and I don't
know we've been doing this for a long
time but it this is really I think a
good talk we've made a huge amount of
progress we're very close to finishing
and we're going to walk through some of
that
go ahead so Jess I want a seven the
oldest surviving jsr it could have been
Joe Sal one I think there's been many
people who have led this effort and it's
been a I think it's been a tough
challenge many different companies have
been involved so we sort of both of us
picked this up and I was joking to Greg
it just takes couple of Australians
together and a few beers to sort this
out which is sort of what has happened
but I think between us we've done more
than a hundred thousand miles of travel
and every time Greg's in Australian I'm
in Boston every time we have to talk
about this it's either late night for
one of us or very very early morning for
one of us so and both quite motivated to
stop this as you'll see there's there's
lots of happy news in this talk we're
very close to the end if anybody asks
you to get involved in a spec and you've
got to do the RI and the TCK and
everything else and write a spec doc
there's been about four person years of
work in this thing so far so it's one of
the reasons it just takes so long it's
actually a huge undertaking to do these
things
it's crazy I did my I remember writing
my honors thesis and that was way easier
you've got about ten patents doing
patents is way easier even dealing with
all lawyers is way easier than writing a
spec it's very very challenging but so
we've we've done a some great steps and
you'll notice in this presentation we've
had we talked about Jaso 107 and it's
the Java caching specification and as
we're coming to the end of it really the
specification is J cache right it's
really the Java caching so you know
hopefully by the end of this year you'll
never hear about 107 ever again and any
any talk so Greg and I give or anyone
else gives will really be about J cache
and we'll talk about what's after what I
said so we're going to go a little bit
through the a little bit through a bit
of history and where we are in the spec
and then a bit so a few things a bit by
example so yep okay let's move on do you
need to work say something about so
because I work for Oracle and we're
talking about Oracle products we have to
say something like this which says
everything I say doesn't exist actually
your father records lines also not
allowed to wear jeans yeah alright right
so so what is what is Java caching so
it's an effort to standardize caching
for the Java platform so bike by caches
so so caches a map like structure as any
cannot we just have a show of hands does
anybody use the education through the
years all right and what about yeah okay
so I think people basically know what it
is it's a map like structure that where
which is where the the data is stored in
memory it has a key value API and it's
very very fast for puts and gets and
removes and so what we wanted what we're
doing here is creating a common standard
for all of the different open source
projects and
emotional implementations to standardize
everybody works roughly in that way but
there's many other aspects to it as
you'll see as we go through the
presentation so to completely
standardize that to bring something like
you know what JDBC did for database
access bring a very very similar thing
so a common mechanism to to to interact
with cases to discover cases to control
the yeah we'll go I don't give the whole
talk away so so how do we do this well
we've followed through the JSP process
where the most up to date which is the
most open way of developing specs so
we've got links here you can actually
see see the spec you can go and download
it you can see the API you can see the
RI you can in fact thanks to Greg you
can actually see the TCK this is one of
the only specs where the TCK is public
usually it's a licensed licenses as part
of like Oracle's pack so you can see
everything if you want to go and build
if you're crazy enough you want to build
a implementation of this spec you can
and you can prove that it's compliant
yeah I mean this this this is going to
have the standard TCK like any jsr but
the the J unit tests that are actually
what the TCK will run are all licensed
under apache under github and we've got
we've got a maven mechanism so you can
put in your maven coordinates of your
implementation and run it against those
tests not exactly the same thing as
running it against the TCK but pretty
close and one of the handy things that
you could do with that is what I've
discovered over my years of giving
caching talks and just my experience of
doing it myself is that many people
start off with their own simple in-house
in-house cache it might be an outgrowth
from concurrent hash map and before that
you know these things tend to take on a
life of their own there's actually a
really nice opportunity that if you're
actually are going to go down that path
you could actually do it yourself
compatible with 107 you know and so that
if you then later change to an
open-source project or god forbid you
know poetry move purchase one
you it minimizes the amount of work that
you have to do one over here so anyway
that's it's you know we given that you
know I write e education ears ago and
continue to maintain it and then been a
terracotta
I mean following you know the the JCP
two point nine that the community
process has got a transparency measure
it's not really it's inspired by not
dissimilar to open source so basically
everything's up there at github in the
white normal way that you would expect
and you can see some of our fiery
conversations email on our email threads
and Google Groups as well so it's um
it's it's quite a good environment to
work in you get a lot of community input
I think even we're now even in our last
vote we had a lot of people who were on
the expert group actually voted for this
back which was sort of crazy so because
it's so open anyone thought they could
vote but you know it didn't worry us it
was really good to see extra support the
community so yeah that's that's uh let's
carry on so why we sort of alluded to us
we really want to standardize I think
it's time to standardize after twelve
years what how getting put in a cash
should work so standardized it's not
just about the API it's actually about
the concepts of caching so all of these
different implementations are very
similar but we actually once you send
wise concepts you can actually
standardize terminology which means you
can talk about things you can present
things you can design architectures so
on one hand you think I'll we're just
building a get put API other hand it's
like we're really trying to nail down
the core concepts so that you as Java
developers can design systems and
actually have that language to use to
discuss with other other developers or
other architects and also when it comes
to choosing products and choosing
implementations you can be very specific
about what the behavior of the
implementation should be and you can't
do that without a spec and I think we've
probably spent more time talking about
the concepts and trying to define the
concepts than we did actually working
out what the API look like yeah one of
the things that's been really
challenging about this is that all the
concepts seem very very simple but then
when you get into it and you realize
that my outside of the in process case
you're talking about a distributed
system there's very little across the
the jr's that have been defined that
actually deal with clusters of systems
and so you know without wanting to heart
going to find that entire world it we
have to be very very careful about about
what we say the usual way we go about
that is is to say don't assume that your
listener is in process don't assume that
your loader is in process and and you'll
see that expressed through the API we
generally don't allow instances to be
passed in but instead factories so that
a distributed system like coherence can
can go on instantiate anywhere so
obviously you know portability is a huge
thing and so you know once we define
these concepts like how do we take
something that's in process and you as a
developer loud to be able to swap that
out with something it's out a process or
remotely caching or caching across many
different layers and different
topologies so there are some things that
you may expect to be in the spec that
actually aren't there so we don't talk
about topologies at all and that's
deliberate because there isn't one
topology for caching right and we can't
actually there's no way we could
actually force when we could probably
get to a point of agreeing but there are
so many implementations so we wanted to
make it possible for people to implement
this I know another thing that you might
for you know nothing you might well
we've just defined a set of concepts
that we need that we need that the spec
relies on and then outside of that we
kind of haven't or we just mention it in
italics one example that you might find
surprising is when it comes to resource
constraints in a case so if you you know
you would think oh why haven't they
defined you know maximum size of acacia
well the answer the answer is is that
concept gets tricky as well because
because you know in big memories case
big memories case we have we have local
on heap local off heap then on the Terra
Cotta server array we've got on heap off
heat and then disk so we've got five
separate stores so if you wanted to
define so in our system you know you
need to define that generally white
by each one of those stores the other
thing is that we have this feature where
we where we traverse the job objects you
put in and measure their actual sizes so
you can also actually define your sizes
in terms of max bytes so for the on heap
storage component you can specify max
bytes local heap instead of max count
now because we don't define a topology
and we don't define I mean basically
every you know these guys do as well
cases get really really fast by having
this tiered storage architecture so
we've all got multiple stores but we
don't necessarily have the same stores
and a simple in process case will just
probably just have one store just the on
local on heap store but because we don't
define topologies we don't define
storage tiers it actually made no sense
at all to actually introduce the notion
of in configuration of a resource
constraint so it's not there and then
the same applies to for eviction so you
think your cache you should be able to
specify eviction well you actually can't
with this caching API there's a notion
of expiry when an entry may expire but
you can't actually define eviction
because when you have multiple storage
tiers you may have vicked from different
tiers so you end up having to tie it all
together
so the basic API is is quite easy to use
and then all of those other things
become customizations based on the onion
and what you do it and before you give
up all hope and head for the exit and
say well it's going to be completely
useless it's not all that all those
things that we speak about are actually
in the configuration yeah so what we've
attempted to do is standardize the API
the programmatic API but we have not
attested so there is a there is a
standard configuration which only
configures the aspects that we've
defined in the spec but in the real
world you'll either you'll either add
additional programmatic configuration to
match the implementation you have or
most of us come with config files so the
only it should only end up being one
place where you configure this stuff and
so that that configuration information
should not come into your app at all
which is what we've tried to do is keep
it as much as possible out of your
application to you because that breaks
this sort of a second point is about
portability is to ensure that we can
keep those portables so I just stand
there for a bit longer so I just want to
give you guys a
I feel for the the the the portability
side of things say so
Massimo Pezzini from Gartner is the
analyst that that analyzes this space
they've been doing so for about three
years and when you work for a company
like we do the you know marketing and
product management take these things
pretty seriously the the Gartner
projection is that the data grid space
which they used to call it they used to
call it distributed cash and they call
it data grid Forrester calls it elastic
case by data grid we mean coherence
Terra Cotta Jim fire and in Finnish band
grid gain hazel caste be an extreme
scale that category some a couple of
them are true data grids most actually
not data grids and don't call themselves
data grids
you know Jim fire does not call itself a
data grid yeah although most people
think it is so it's kind of one it's
this loosely defined term but this spec
applies to those those things so
Gardiner has predicted that the market
will be worth 1 billion dollars by 2015
it's growing about 40 percent a year the
other prediction is that by 2014 40
percent of large enterprises will have
at least one implement to be running at
least one implementation of a data grid
so I've been doing this for 10 years and
it's it's just even now it's just sort
of heating up there's not a lot of other
stuff that's heated that's that's not
already on fire like big data and stuff
this stuff comes into the big fast data
category but as has gotten a look at it
and talk to a lot a lot of their large
customers and ask them about their
implementation plans the thing that kept
coming back was you know we're really
troubled by the fact there's no standard
API so garden actually concluded that
the lack of a standard API was the
single biggest inhibitor towards this
this market getting even bigger faster
and if you think you know caching is
boring this is a pretty solid area to
understand caching is absolutely
ubiquitous it's going to get even more
important as the sort of whole internet
of things takes off you know
caching and cloud is critically
important you know so knowing this spec
and I think I saw those slides for Java
e8 on the show they've been presented
yet but in the middle of the slides
they're talking about all the different
technologies that's in Java EA the
number one thing is J cache right in the
middle so I mean we expect this to be
quite important so let's let's rip
through these okay one's so real real
quick real quick
it's Brian and I as spec leads also want
to pay tribute to Yanni Cosmopolis
Brian's predecessor who work with me in
2011 on the spec expert group expert
groups got about 18 members we've got
we've got most of the implementations
represented we have good representation
from Wall Street who are very heavy
users of this category of product in
collected antique about definition of
api's as well yeah it's huge fund and
and we also have will serve the open
open source represented so myself for
education
we've got JBoss for and finish man and
we there's there's caching annotations
which is a little subspecialty so we've
got all of the caching annotations guys
involved so it's a pretty healthy expert
group the sad reality is though that
Brian and I do most of the work and
travel okay
so said long time in the making when
this I think this is the first time
we've ever been able to do this yeah
pretty proud to be actually put a
calendar on the board so we've finished
the public review ballot and I was told
and Greg was told it was the first time
was absolutely unanimous approval yeah
that's right we had 24 yeses and 0 no
Xander oh nice no one's completely short
but certainly certainly for that
executive committee it's the first time
that they can ever have remembered
voting unanimously for a respect just
just a small point public review ballot
we are basically done we are basically
done at this point we have a hundred and
forty page specification document which
is extremely polished and
that's the thing that they voted on so
we've got a few clean ups as you know as
you get close to done people start using
it more and you get more and more
suggestions so you know we were talking
last night like we could probably do
another revision but we really have to
sort of draw a line in the sand so we've
got some clean ups to do we're going to
do this week and then we'll propose the
final draft and then go through the last
few stages but the really cool thing
that's been happening is that guy called
Joe fee Ally that that works in the
coherence team started implementing the
coherence version about four months ago
yeah and four or five months ago he's
also been helping out in the tcks way
you know might be a test that works well
for the local in process but actually
breaks on the distributor version and so
on say well it works well for
distributed but just horrible it was so
that it we're really and the Terra Cotta
guys have been keeping a close eye on
this thing as well so we are we have
quite a high level of confidence that
because because of those input that
implementation work actually getting
done that the thing is going to work the
other thing is that JBoss JBoss within
finish man actually in their release
their five points really released about
four months ago actually released the
0.72 at the point seven version of
r-spec we're 0.10 right now but they
actually implemented and got called
Golda actually came up the whole heap
review points from that so it's really
really good when it works that way so
that our eye is completely clean it's
built from scratch pure Java you can go
and grab it you can look places places
we can get this but ultimately our aim
is to get it out before the end of the
year and then we'll see what's next so
in terms of which platform is it
targeted for this is this took quite a
bit of time to get agreement on and you
would say well why isn't this Java
rightness well we actually have a lot of
implementations of this that at Java 6
that could be built on Java SC 6 so the
specification itself only needs Java SE
6 it's compiling at six
but everything else they're doing is
compiling at seven so if you want to use
the ri you have to be on Java 7 the
other thing is that we we actually want
this thing to very quickly get adopted
in production implementations and so you
know that between 6 &amp;amp; 7 there's only
of a couple of things so we actually
thought by going with six it gives the
widest it makes it makes it easier for
production systems to actually adopt it
okay and on the EU front we've we've
actually shortened down or removed some
of the features out of the spec that we
were considering for EE we were going to
be part of Java EE 7 but timing just
didn't work out so we're you know as I
said part of e 8 is where we're heading
so there a specific features
specifically around transactions and CDI
we actually have taken out of this
version of the spec we didn't want to
hold up the spec any longer we don't
have hold it up for Java EE we don't
hold up the East be 8 so we sort of took
those pieces out and we've we've
captured all of that stuff we're going
to do ok so right so in terms of the in
terms of where the project is the just
107 is up in the normal place on the JCP
website other than that pretty much
everything else is up on github so if
you go to github.com /ji 107 the the the
main page is the JI 107 spec there's a
readme there that's got all the jumping
off points links to the forums and so on
and yeah go on go and check out the
source code for kit come up with
suggestions get involved in the forums
and I think you don't like it's not too
late we will be wrap it with Brian and I
hoping we're gonna work this week and
we're hoping to get the final version
out in the next three weeks so now is a
really really good time to have a look
because I think you don't like raise a
github issue and we're like we will look
at it yeah we all just ask Joe to do
that he's working on it as we speak
no no we will look at it so you can get
up grab it from maven this is you can
actually go to maven central you can
find all the artifacts unfortunately for
some reason maven sorts this versions
weird it thinks this is version 101 so
it says the latest version is 0.9 it's
actually point 10 yeah so that's point
that's vary from our point of view
that's
10 it's version 1 was about two and a
half years ago yeah I yeah ok but you
can also if you if you build the source
locally if you just grab it from get up
then it's 11 snapshot so you can see
what what's about to come out and we
think a live in snapshot is the last ok
so caching Greg alluded to this it's
that sort of start to bringing some of
the terminology high performance we
assume it's high performance low latency
data structure in fact we treat it as a
resource so by resource it has a life
cycle like a map if you create a
concurrent util map yeah you create one
use it in your app and you sort of own
and manage that with a cache it's a
little bit different though it looks and
feels like a data structure it actually
is a resource because the implementation
has to manage that resource there's
memory
there's threads it could be distributed
across different systems that could be
connections to other systems and so on
so while it looks and feels like a data
structure and we use generics and all
this other cool stuff you don't just go
in new a cache you actually ask a cache
manager for one so when do you actually
want to use a cache so typically you
know the reason you want to use it is
because it's really fast and the
important thing is what you're putting
in a cache is a copy of something and as
soon as you put a copy of something or
make a copy of something and put it in a
data structure you then have a
consistency challenge so we don't try to
solve the consistency challenge with
where you've where you've made that copy
from we do however have an integration
package which lets you do what's called
cache doors and cache writers cache
writers and cache loaders which lets you
integrate with back-end systems they're
probably a database so the best place to
use a cache is when you know the cost of
making a copy is less than the cost of
going to get it from wherever or
rebuilding that value from wherever it
came from which means you really need to
know if you're going to put a cache in
your really need to know yeah is it
cheaper is it less expensive to make a
copy and keep that copy memory somewhere
then to go and get it again often and I
guess you see this as well we'd see
people adopt a cache and then it's like
hey my app starts performing worse it's
like why would my app start pulling
worse cash this is made to be hard
faster I say that one coupons talking
here it's yeah yeah you put a big
distributed cache that replicates stuff
and keep stuff in memory as well as
writing to the disk doing extra Network
hops and you've done no optimization in
your application to you and you know
you're going to be a problem you're
going to be in problem in trouble the
other the other problem is if you're
putting stuff in memory and you're just
overloading your heap you introduce
extra garbage collection so you know
fundamentally you know caching is not a
cure you need to know what what you're
doing however we're making it easy for
you to UM to do this so in terms of in
terms of making that judgment call a
Anan heap an on heap case will be will
be a microsecond or less in terms of
access time if you're if you're storing
off heap so big memory has an off heap
off heap tear locally that has that
really is just the costs it's pretty
much the same cost it's about a
microsecond more or less to get the data
in and out and then it's the cost of
serialization and deserialization and
then that says that's as fast as slow as
you want to make it the like using the
defaults that we have it's probably
around the 10 to 20 micro second range
then if you go across the network then
you're looking at a couple of
milliseconds and so the with cashes
cases tend to use an LRU algorithm to
keep the hottest data closest and so the
average the average of the time you get
is not just the average of those
different tiers like call it one
millisecond it's heavily skewed in favor
of the hot data set they're accessing
frequently that you're accessing at the
microsecond range so you typically
roughly gonna get you know in a big
system you might average out around 100
microseconds for a cache now that's a
lot lot faster than then going out over
the network to anything so no sequel or
database
systems are typically going to be like
for simple lookups whether it's database
or no sequel system are going to be
around between the five to ten
millisecond range so when making that
judgment call about is it faster to put
it in a case and retrieve it that's kind
of roughly the the time the times that
you can rely on and then the costs cost
is a hard thing it's like costing time
if that's what matters to you cost and
resources there's actually a large a
large part of our business is actually
doing offload mainframe offload or a
database offload and that's actually
where it's not really it's it's the fact
that you're well with a wither mainframe
it just it actually the cost is actually
a dollar cost and then with the database
what people are usually trying to do is
avoid reacting their system trying to
buy themselves time and one of the
interesting things about putting in a
distributed cache is that is that often
it's a permanent fix
yes yeah so a few few places that you
can see cache is being used is it is it
used to make your application faster or
is it to prevent upgrading something
that's slow so often you you can
actually as an architect you go well I
want to make my application faster but
really what I'm trying to do is actually
prevent having to upgrade something
that's really slow so it's like it can
be a temporary fix
to extend the life of something you
don't want to touch right so it's
interesting when you start looking at
caching so architectural II what am I
trying to achieve here and it's often
the latter not the former yeah and when
one of the I mean most developers feel
there's something intrinsically wrong
with caching because it feels dirty and
it feels like what they should be doing
is actually fixing the underlying system
that is the system of record but if you
look at any large-scale system from
Amazon or Google or any the web-scale
stuff you'll see that caching is a
standard tier that is a technique that
is used and you know that will be made
clearer once Java EE eight and spring
four actually have it in there it'll be
a standard Orthodox tool that you use
for building your application and about
80 to 90 percent of the times where I've
gone you go and stick caching in to sort
of buy your time and then you know most
applications end up with a
an amount of scale but they need to deal
with and you know in about eighty nine
percent of the time it turns out you
don't actually need to do anything other
than actually having out application
which is often a surprise
yeah because what felt like a temporary
fix ends up being the final solution
that generally only applies to
distributor cases because local cases
we're not going to go into it today but
one of the very strange things that
happens with a local case is that when
you expand out the number of app server
nodes the if the efficiencies get worse
and worse it actually it's worse yep and
it connects is slowed down okay so in
the spec I added this section on caches
versus Maps and I did this deliberately
and I think you know Greg would probably
agree is that over the last 1012 years
of being talking to people about caching
and what from the cohere its perspective
and actually user doesn't doesn't Kieran
says cash implements my implement map
yeah so this is a this is an argument we
had with his boss Cameron Cody yeah we
should never have implemented map and
there's some fundamental reasons why you
should cash is don't extend map and in
fact this spec cache interface does not
extend map specifically for these
reasons so one of the things is map when
you think about map if you use
concurrent map or hash map or any of the
linked hash map any of the Java ones or
any once you've built your salt there's
one fundamental thing that Maps do or
don't do that caches do and that is lose
data if you put something in a map and
then so and there's no other threads and
suddenly it's gone you'll probably think
there's something wrong but with a cache
that's exactly what they're designed to
do right you put stuff in a cache and
you only want it there temporarily if
it's used and you want the caching
infrastructure to decide whether you
need to keep it around so one is
lossless one is lossy and treating bikes
by making sort of cache extend map
essentially means caches have the same
semantics as map and that's not true
it's absolutely not true
we don't want to allow developers to
think ah wherever I have map I can now
replace that with cache and expect
exactly the same
antics because that is really wrong you
know you'll start to see data just fall
out of your application so they both key
value base that's good both support
pretty much atomic update this is good
and can't you know entries don't expire
what caches they may expire depending if
you've configured it event entries won't
be evicted you know the difference
between eviction and expiry an expiry
you can say hey I want this you know
entry to stay around for a little bit of
time or some amount of time but eviction
says the caching infrastructure can make
a decision itself want to throw things
out regardless of what you've said yeah
you say hey configure your
implementation to keep a million
elements and then it goes I'm actually
running out of memory I'm going to just
start affecting stuff that you're not
using right because I don't to take your
application down right I don't let you
run out of memory so and you know
different different topologies we talked
about topologies where the entries are
stored and and the list goes on so we
added a bunch of other things where you
up to the so we added like integration
so with caches if you've got to read
something from a cache that's not there
will actually call through to some
integration cache loader and try and
load it for you so that means you're
loading from database loading from JP a
learning from you know cloud loading and
the same with writing so there's another
really cool feature that was introduced
by coherence which is entry processes so
as a general distributed system as a
general principle it's faster to move
the processing to the data than it is to
move the data back to where you're
processing is so the predator the
techniques called in situ processing and
the the mechanism that brings in
situating to to this spec is entry
processors so with an entry processor
it's a chunk of code that you go and
execute in the node that is actually
storing the data in a distributed system
we're going to see the slides on that so
okay so absolutely way more than your
average cache right so so just running
through the features in a bit more
detail
so like like concurrent map we have we
have those Kaz operations so put if
absent put if absent three argument
reply replace so the idea of the idea of
the audio of the Kaz operations is it
uses optimistic concurrency so you you
claim that you know the state there the
current state and then you want to do an
update to that either remove for a put
or a place and so you you passed you
passed that operation with a new value
if it's a if it's if it's an insertion
and and what you think the old state was
you pass that across to the server and
the server then atomically decides
whether that's whether you actually had
the true state or not and if you did it
accepts it if not it returns false so
there's no locking required in a
distributed system so it's really
fantastic for performance for the types
of systems we built so that's um that's
those things the methods are not the
same as on concurrent map and one of the
reasons one of the reasons why or the
main reason why is that is that map is
obviously all in process server things
reference and so there's almost there's
no cost basically to returning things
whereas for us to actually have returned
by users added cost and so the we have
versions that don't return don't return
old values and then versions that do
return are value so you can opt in so
Brian already mentioned the integration
support read through and write through
so with read through and write through
instead of going out to resource coming
back putting in a cache and then in your
code checking the cache first and then
going to the resource with read through
and right through you between the
resource and you is the cache so you go
to the cache and attempt to read if a
keys not present it will actually call a
cache loader which is a class that that
sorry an interface that we have that you
implement which goes and actually set
tells the cache how to go and load
that key from that resource and it
actually will attempt to load it in line
same thing on a right when you write to
the case the case writer will will write
through now that's as far as we've
defined in the in aspect and that's
really all we need to do from an API
point of view at configuration time most
implementations like like coherence and
ourselves with big memory actually also
allow you to do that started right
rather than in line we allow you to do
that
a synchronously behind we call it right
behind and actually enables batching to
to these back-end systems and by
batching the back-end systems as often
it's usually a really good technique to
actually reduce load on them the other
thing you can once once you define a
loader the other thing that the cache
implementations can do is they can
predict based on what your application
is using a lot so if you've got an entry
which you said is going to expire in a
minute and you're using it a lot we can
actually start the process of
prefetching it because you specify a
loader we can say hey we're not going to
wait a minute then for it to expire then
we're going to hold access to that entry
and then we'll go and grab it and bring
it back will actually start prefetching
it yeah so the API is designed to allow
all sorts of cool optimizations for all
the different providers so you're right
yeah pretty much
read through like there's a whole
massive number of really clever things
implementations can do so again another
reason why the spec is very important
because at the moment you really you
really can't do this in an in a portable
way right it's like there's no way to do
it put in a portal way so this spec will
ask so it's start down this path yeah
the light I mean though having to find a
loader there's also a couple of methods
that actually let you warm up a case by
basically calling load with a key or
load with a collection the whole set and
then you know once when you get over to
implementations it doesn't affect the
API but I mean like you know over at
terracotta we also have the quartz
project so one of the things that we did
we have the same prefetch idea the other
thing we have a scheduled refresh so you
can actually schedule a refresh you can
be holding sort of a defined copy of
data and then on a scheduled basis you
can go on refresh it or refresh it and
that uses the Leiter's as well so it's a
really useful concept so you can define
one loader and then the implementations
can use that loader and
frontways to optimize your app Oh guys
like one point we only think the only
thing that we provide in the loader is a
batch method and if you want to rot
light a single key that's a batch method
of size 1 yep so everything is designed
to be like where possible we've made
that API batch based to avoid multiple
round trips across the network in in
situations where you have to go across
the network and also allows very
efficient listener event generation and
so on so these things are all about
scalability multiple core
implementations and so on so we talked a
little bit about topology so we're not
going to go into that anymore but if you
look at the API some things may be a
little strange I'll use the term strange
Greg alluded to it it's like when we
design the API we could have actually
you know so the H cache it's easily the
most popular I think Java caching
implementation around certainly in the
open source space and we could have
solved that API but the decisions Greg
made when he implemented that interface
were about you know in process caching
because that's the problem Kay that's
the you space were solving and then you
know eh guess enterprise then then sort
of had to change some of those features
and coherences at the other end of the
spectrum it's like of all mainly about
distributed caching so bringing these
api's together we had to make
fundamental decisions and you had map
which is also assumed that it was in
process map it's where the education API
so both of those things have acted like
straitjackets for our commercial
products so we when designing the API we
had to take these things into
consideration you imagine like taking
the you know the drawing a Venn diagram
of all of the features of the apps and
then try to overlap them and that's the
spec and can we make sense of that spec
so so for example when you specify a
loader typically let's say using a
spring framework you just say here's my
loader class and inject a loader
instance you can't do that in a cloud
because you don't know where the loader
is going to be right so you have to use
factories for everything so you need
your how to tell the implementation hey
here is the factory for my loader here
is the factory for my writer here is a
factory for my listener and so on and
this is how I
our entry process can be created and
because the underlying implementation
may need to make decisions about where
this stuff goes so same goes with the
the lock free there's no lock method in
this API because it's inefficient too
soon as you hear the network now you're
doing multiple round trips so everything
cars is important single single trips to
the cache makes it very efficient on
networks very efficient in multi-core
systems okay get a world see so Brian
and I Brian and I were born in hospitals
about 100 kilometres apart in northern
New South Wales Australia
weird coincidence so you'll have to put
up with the Australian ization of the
hello world so g'day world so so to
acquire a case we have we have caching
which is our bootstrap mechanism so the
simplest way is caching get cash and the
the case you want and then the then the
types then the types that you want when
you define it when you so we've got
we've actually got a runtime type
enforcement feature in here when you
configure a cache basically declare
whether or not those types are meant to
be enforced and then if they are it
would actually generate an error if you
tried to get around the runtime types so
this will give this will give you back a
case of integer inspring now this is the
simplest example the usual the usual
thing that like for more complex
examples you use caching to actually get
a case manager and then case manager
became an urge get cache having having
access to a cache then put just put the
key in the value and to get something
out it's just case you don't get such a
key value API so apparently I hear
there's talk of having rarefied types in
Java 9 and in which case we'll have to
fix this so we don't need to pass the
integer and string in there'll be
another version for Java 9 so let's
let's quickly go through it in depth so
interface cache extends interval you can
iterate over entries
that's cool notice you can't serialize a
cache that doesn't sort of make much
sense and it doesn't extend map so you
can get aa notice it's all using
generics so anyone that's used coherence
or eh cache will be happy you guys
though we don't have a generics API so
constantly casting from object and you
guys don't either don't either
yeah so for both of us this will
actually be a step up into a generics
API so can you imagine the pressure that
we come under working in our teams and
we've just totally changed the API of
our caches just imagine like 10 years
worth of engineering and now we just
change the API generics as generics is a
real like to get it right on something
like this is actually incredibly painful
we've known how much week time we spend
there's a huge amount of time and this
nagging doubt that you've got it right
some questions we actually refer it off
to josh bloch yeah and it was still not
sure if he got it right other so so API
is pretty straightforward get you know
get all contains load all this is that
does asynchronous loading so use
completion listener if anyone's looked
at JMS - oh it's exactly the same as JMS
- oh we've tried to keep consistency
with the Java platform so just imagine
the audiences we have their own
development teams we work with we have
an expert group and a community and
everyone saying we want this we want
this it has to be on this way so we've
tried to keep mostly everyone happy so
load all does that void puts why did I
say we're not going to return a value
Greg said it it's really expensive going
across the wire doing a port coming back
and bringing the old value back and Matt
forces you to do that the other thing is
almost everyone throws it away so you go
across the wire bring it back and then
you never do anything with it who
actually uses a map at cause Portland
does something with the value like no
one so therefore therefore you will see
on the next line the first of our
strange method names get input getting
put it's like can be put and again and
we have this pattern like get and remove
remove right so this is done you can see
the at
you know the atomic methods in here if
the comparing set but it's all done so
you don't need locks and so on and what
you might say hang on why am i doing
getting put or getting removed isn't
that the wrong way round well you're
getting the old value and then remember
then you're either putting a new value
or removing so that's that's why so we
have replace it's a pretty popular thing
a lot of people when we looked at how
people use cash they lock get a value
check it put it back in unlock and it's
like you just wanna be able do that one
hit so so I have one other yeah what are
you doing man actually you know I'm lost
was it when was that last Thursday last
Thursday we presented at the at the
Executive Committee at the JC P down in
San Jose so I actually put together that
slide deck which put me about it
took me about a day Brian put together
this slide deck primarily say that same
fear so these kind of your example but
anyway we have the the because what
we're about is performance whenever
there is a couple of ways of doing
something and one is always going to be
more efficient than the other but we
basically provide both methods and so
clear clear basically clears clears a
cache with no side effects
remove all will call this nests so
remove all will call remove on each of
the listeners that you've got if you
have a removed listener registered so
that's why we provide both methods so
you can observe a cache and if you have
listeners and you have you know half a
terabyte of storage in there and you say
how to remove remove all you don't want
like half a terabyte of data coming back
to your listener saying hey I was
removed it's like no I just want to
remove just don't tell me about it just
removed so hence we have clear removal
so before we were actually talking about
about because there's no assumption that
we're in process it doesn't make sense
to actually register and an observer
instance so instead we actually have
factories now when it comes to listeners
who ever thing called a cache entry
listener configuration so that that will
that tells you whether or not you what
you know
which events you actually want to listen
for whether you want to listen for puts
or removes or updates and the other
thing the other thing is it provides a
listener Factory and so the listen of
the factories and all cases factories
have to be serializable as does that as
do the configurations so then an
implementation can actually pass them
around the network and register the
listener where the data is yes no no so
the question was that's right in which
case you need to call so the question is
is there a listener for clearance or no
if you want to listen for events and you
have to use remove clear is basically
like drop table in a databases truncate
but the way that API is designed is we
actually can have other listeners so
different implementations so like
coherence has several different types of
lifts and listeners and you can actually
listen and intercept things in the flow
so this is just the standard level
standard level listeners so you know the
way the interface design is that
implementations can add their own
specialized types of listeners so you
know imagine this yeah and in education
has actually got to remove all listener
and so that remove all listener is
actually just called once and it acts
right which is kind of what you what you
what you alluded to the other thing is
that we have is we have a case manager
listener which listens for cases being
added and removed so the other way to
drop a case is actually to remove it and
then recreate it so you can either clear
it or drop it recreate it and we have a
listener for that so you bring up a good
point though what we've tried to do in
the spec is is deal with probably the
most common eighty or ninety percent the
idea is to provide a low cost of change
if you're changing implementations not a
zero cost of change so I guess with JDBC
at this point you probably almost never
need to go outside JDBC to a proprietor
implementation
this is version 1 we have a lot of
common ground we've got some other stuff
that we could look at standardizing in
the future you know once we get this one
under our belt Brian and I was probably
going to stay involved with this thing
to nurse to take it through to Java 8
but basically there is a we've got the
unwrapped slides coming up no no I'll
just bitch yeah we don't no no I meant I
mentioned this now then so we the way
most things are written is that is that
an implementation can basically add
things additional to the spec now for
you you in your code you can be using
JK's most of the time if you ever want
to reach in and say hang on I want to
reach down to the proprietary API we
actually have a method called case
unwrap so your case don't unwrap and you
and you put in as an argument what you
believe the concrete class is we will
check that and then give you the actual
concrete underlying case so in our case
it would be education and then you've
got the fully education API so that's
unwrapped the other thing on case dot
entry often we have a lot of metadata in
our implementations associated with our
case entries so the education is called
an element and has has about 10
different pieces of metadata so once
again you can do case entry unwrap and
so just as a we we want to I guess yeah
we we anticipate that there will be
times where you need to reach down to
the proprietary API and we make it easy
and standardized for you and also it
makes it really really easy to do a find
and your code base looking for patient
unwrap yeah and as soon as you change
you can yeah as soon as you change
providers and so remove a provider or
custom implementation area class path
like he'll get compiled Tamara so I'll
show exactly where you have used
anything that's custom which is nice
this technique actually is used in JPA
yeah it was introduced in JPA really one
hour one of expert group members
suggested we do this that's like so it's
it's really nice because it actually it
just provides that standardized way of
reaching into the proprietary
implementation so one that we talked
about entry processes and that so this
is completely unique in terms of what
caching things that we've seen so
basically
if you're in that talk about lambda you
can use lambda here as well so an entry
process you can say how you want to
execute this entry process against some
key can provide some arguments we can
have a set of keys and say hey execute
this work and you can think of an entry
processor is like a callable and the
implementation will deal with it in
terms of getting it to where it needs to
be executed and there's lots of
optimizations so this is almost
identical to the coherence API and it
means that you can do a bunch of things
that typically where you'd have to use
locks so now you can and we have a
really simple example obvious from a
cache can get the cache manager in the
name as well so cache managers we
alluded to very briefly was you know the
ability you know resources caches or
resources cache managers have to own
them and the cache manager has a
responsibility of they could be
predefined so I know
eh cache doesn't and coherence does it
you can start up an application and just
say you know get cache foo and it will
give you a cache with a default
implementation so you don't have to
create a program to create one so cache
managers do all this other stuff they
scope them their lifecycle in containers
and so on so this is the what Greg was
saying how you can say caching get cache
manager and once I have the cache
manager then I can then I can use that
cache manager and from the cache manager
you can programmatically create things
you can kill caches and remove them add
new ones and within an implementation
you can have many cache managers and
your application in terms of class
loading scoping the cache manager
basically has a class loader and it uses
that for all its necessary stuff for all
of its caches so within a container
cache manners you know almost always
bound to your to your deployments one
other thing that we added it was so the
intent was there but we added in the
past year was you can actually have
side-by-side deployments of caching so
you can have eh cache coherence you know
memcached if there's an implementation
of that all side-by-side deployed in the
same application all using their own
caches there's one really important
thing
we want to be able to do this so yeah so
that's that's very occasionally and
cashing provider that we do that so you
can thus this is a pretty great great
sort of alluded to this you know an
application basically uses caches and
between caches and the caching so helper
class there's caching providers which is
the SPI implementation and then the
caching provider can give you cash
managers and they can be named and all
the special configuration and class
letters and then the the cache manager
looks after caches gonna just whisk
through these ones we're running out of
time we've got wondering how important
announcements at the end so you want to
just whizzed through this process so you
can do dynamic configuration so the spec
provides a very simple mutable
configuration uses fluent style so you
can say hey create a cache from integer
string and I want store by reference and
this is my expire policy so and there's
a whole bunch of different things that
that different configurations and every
implementation must support this so if
you're using just vanilla sort of
caching and you use this configuration
you can be guarantee it's going to work
portably and what you do is just say
create cache get your cache manager and
you say hey create cache this is my
cache and and so on okay it's a pretty
much same as before so entry processes
you can imagine you know I've got my
cache what I really want to do is say
increment increment of value in the
cache so typically I would have to you
know get the get the entry maybe lock it
because I don't want to change it get it
update it put it back unlock it so I
have to do this if you're an in process
that's not really that costly but if you
have to go across the wire every time
and you have to back that up and you
have some sort of high consistency model
requirement on your cache then this you
know just doing a simple increment can
be quite expensive with an entry process
that we can send that processor or in
fact it's lambda expression friendly so
you can do this with Java 8 we can send
that processor to that key and say hey
do it do it with a value 42 and return
the value so one statement and to be
honest this is a lot easier to read
and what you see in the code like cash
not latke cash don't get ki increment
cashed up put key cashflow unlock key so
you can see like invoke against ki
increment processor I slide on that
thing yep so how to implement is pretty
straightforward and this is almost
identical to what happens in incoherence
is except there's nice generics here so
when this gets executed this bit of code
the cache will look after locking if it
has to do any locking passes a mutable
entry in and any arguments and I can say
hey if the entry exists then and there
are some arguments then use that
argument increment the value set the
value and then and then return it if the
entry doesn't exist then I can throw an
exception and if that occurred on a
server and can bubble back to your
client right the nice thing is I can you
can actually execute entry processes
against entries that don't yet exist so
you can say hey execute this entry
process against a key it doesn't even
exist in the cache you can go there
safely mutate that key at create the key
and then return and depending on the
consistency model if I have many let's
have many applications trying to execute
an entry processor against the same key
then it all just sort of queue up
logically they queue up so you don't
have to do a synchronized block you
don't have to like and you know in a
distributed won't like coherence these
are all guaranteed to execute on
someone's only so it's quite a quite a
nice programming paradigm never have to
write synchronized never have to call
lock so we were just round trips
I'll quickly deal with annotations so
annotations became popular about four
years ago introduced by Eric Dale quest
and and they then got they think I add
up to spring they've been added to
Grails in a few places like that so
we're actually standard we've
standardized these as well they will be
coming to you by spring for next year
and then in Java EE 8 you can you can
use them in the meantime we've actually
in the rep in the RI we've actually got
implementations that will enable them to
be used with CDI so Java EE 6 and above
spring and juice so you have annotations
to look like that that's an example
that's an example of a class one of your
classes that's actually using the
annotation so once again it's a
different style of doing the casing so
instead of using the cache API to you
know go and get and put things we can
instrument your code the containers and
framework scan engine area code use
these annotations and use whatever the
default implementation of your caching
layer is so we all want caching but you
don't really have to write much you just
want to say hey annotate this do this do
this do this and you'll be done now I'll
just do a couple of announcements in
terms of our product plans so for big
memory we plan to have a fully compliant
JK out in early 2014 and on the
coherence front is as the Greg you
alluded to we expect to have coherence
to be fully compliant in 2014 as well
and we're pretty much up to date so as
soon as this comes out and as soon as
oracle blesses us the product team then
will actually really release this as
well so alright so pretty much out of
time for questions Brian I will move
outside to allow the next speaker to set
up and we'll take any questions out
there thanks for attending</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>