<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>JDK 10 and G1, Concurrently Continuously Improving | Coder Coacher - Coaching Coders</title><meta content="JDK 10 and G1, Concurrently Continuously Improving - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>JDK 10 and G1, Concurrently Continuously Improving</b></h2><h5 class="post__date">2018-03-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/7hhU4zHWMu8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">oh good afternoon everyone hope
everybody's enjoying themselves at
Oracle code today so for the next 45
minutes we're gonna spend some time
talking about some improvements we've
made in g1 and when I'm in your shoes
and I'd go to conferences and I would
sit in the audience I always kind of
walked in this session saying to myself
I hope there's one thing I can learn
here that I can take away and I find
useful so my goal today is that you find
at least one thing and what we're going
to talk about that you found useful the
standard Oracle disclaimer here which
basically says don't believe a word I
say
so once we leave this room just forget
everything that we talked about so what
to expect here we're going to take a
look at the g1 g c-- features and
improvements that we've made in jdk 10
at least what I think are the most
important ones are the ones that I think
that you'd have the most interest in and
we'll also take a look at what I think
you could expect for in terms of
improvement in performance performance
being throughput latency and footprint
we'll probably also have some time to
take a look at a subset of what I would
consider some of my more favorite things
that I'd like to see us do with with g1
that are coming beyond JDK 10 a little
bit about myself I'm charlie hunt I
actually live local here I live up in
Libertyville over the last 10 years or
so I've been leading a variety of
different java SE and JVM projects at
Oracle I've largely been in the
performance area I've been doing
performance work since probably the mid
90s held a lot of different roles as a
performance engineer or performance
architect both at Oracle and previously
at Sun also spent a little bit of time
at salesforce.com as their performance
engineering architect a few years ago
myself and a fella by the name of B new
John wrote a book called Java
performance some really good information
there about how to go about tuning a JVM
you could take a lot of the information
and
apply that the G one as far as the
approach and methodology goes the second
book that I co-authored with a couple of
other folks who worked on g1 myself
included is largely centered around g1
so the Java performance companion if
you're in this area of working with g1
and you have some needs to tune it it's
it's an excellent resource it's largely
dedicated to the g1 so here's our agenda
so we'll talk about two of the major
improvements in and JDK 10 related the
g1 the first one being full GC is now
multi-threaded and the second one is
called faster card scanning and I'll get
into and I'll do a little bit of
explaining what that means and then
we'll get into this future list and then
I will do a little summary at the end so
some background here on this full GC for
g1 so in JDK 9 we made g1 the default
collector bulgy C and G 1 is a single
threaded in JDK 9 and previously of
course and if you got to this state in
your application where it was going to
have to do a full GC it was single
threaded so you had this sort of
undesirable experience of having a long
GC pause and this is a relatively long
pause compared to what you would see
with parallel GC also known as a
throughput collector the implementation
of the full GC and in JDK 9 for G 1 was
largely based on serial GC which is a GC
that's been around since we started Java
and it was single threaded so it used a
lot of the pieces from serial GC now
that's kind of the background on it
this feature is called JEP 307 parallel
full G c4 g1 so you might be asking
yourself what's a JEP the JEP stands for
a Java enhancement proposal anybody can
write a Java enhancement proposal anyone
of you can write one so if you have an
idea of how we could improve Java or the
JVM you can write one of these and you
could just submit it for consideration
so that's essentially what a JEP is it's
a way of saying hey I want to propose
this new capability or a change usually
larger than something like a you know a
small enhancement to you know a Java
class or something of that nature but
it's a way that you can propose a
feature enhancement so JEP 307 the 307
was the number that was assigned to this
feature capability the key improvements
of course being a multi-threaded mark
compact gc4 or g1 for doing a full GC so
it's the same idea as far as what
parallel GC does one when it does a full
GC so this sounds a little bit confusing
when you talk about a parallel full GC
for g1 sometimes that gets confused with
what's called parallel GC which is known
as the throughput collector so that's
why I wanted to throw this up here is to
try not to enter that confusion into
parallel GC versus the multi-threaded g1
full GC so multi-threaded being the
notion of parallel okay so from an
implementation side it really wasn't as
easy as taking serial GC or it's single
threaded implementation adding threads
to it and then guarding around the
states with with some locks or its
shared state with locks or some compare
and swap sort of concurrency idioms
wasn't quite it's not quite that simple
and it's not as simple as dropping in
parallel GCS implementation of
multi-threaded GC g1 and parallel GC
have vastly different frameworks you
probably have worked with applications
at your at your company where you have
two different frameworks but yet you
have some functionality you'd like to
share between the two but you can't
because they're to just totally
different frameworks underneath of it
that's in essence what the scenario is
with between g1 and parallel GC so some
of those differences arise from the
design or the architecture of g1 versus
parallel GC in g1 you take this Java
heap and you're dividing it up into a
large number of equal sized regions
where with parallel GC and you're taking
a young generation and an old generation
and g1 you take a set of these regions
and they become a virtual notion of a
young generation heap or an old
generation heap so there's some vast
differences in in their implementation
some of the other things are a little
bit more of a challenges it's a little
more difficult to compact the area of
memory where your Java objects are when
you have a large number of regions
versus you have a generation space that
you'd have in parallel GC g one also has
to deal with which is kind of an
artifact or a side-effect of having
these regions and the way that the Java
heap is split up you have this notion of
a remembered set and we'll talk a little
bit more about what a remembered set is
and what its role is but you have to
update these so there's more work to do
because you have a larger number of
these regions to deal with as opposed to
dealing with a an old generation space
or a young generation space and there's
also this notion you having to update a
lot of things because you're moving
objects parallel GCE has to also do this
sort of thing so as you're moving stuff
around or that Java heap you have to
update references when they're pointing
to other things that are getting moved
one of the limitations which we'll talk
about is work-stealing
so work-stealing is this notion of
you've got multiple threads here that
can do work and you have to figure out
some way to divide up this work where
maybe you've given something to one
thread and it goes off and it's
executing and there's another thread
that's doing some work and it's got a
lot more work to do so one thread
completes and it's waiting for the other
one to finish so work-stealing is this
idea of saying hey if you've got some
work over here for me that I can do
because I'm not doing anything give me
some of that and I'll continue to work
on that
that's essentially what work-stealing is
do you see we didn't implement for
work-stealing just yet so and we're
gonna address that a little bit later so
that's one of the things that you're
gonna see a little bit later here is is
we did address that or will be addressed
in a future release okay so here's
here's kind of a complicated chart here
and I'll kind of walk you through this
so this is kind of showing what the
relative performance difference is here
between parallel GC and G one's full GC
over here I can get this thing to show
up my pointer not working let me walk
over here so while we're here on the far
right yes I can't quite see that little
thing okay there it's showing up over
here is there's three different apps
here that we're looking at or three
different types of a configuration of
tests this top one is a test where it's
doing in essence the test is
implementing or enforcing a certain
number of system dot GC so it's invoking
a full GC and it has a relatively small
live set a fairly small Java heap so
it's on the rather small side there's
another one here that's called big Ram
tester it's the least recently used type
of cache stress test it's got a lot of
references it has a rather large live
set in it 90 percents pretty large a 10
10 gig Java heap
so this is a this is a pretty good
stress and there's another one called
tree Fragger which is a fragmentation
inducing type of benchmark so it's
inducing fragmentation and the Java heap
and this was something that was
developed by the folks at Red Hat the
amount of live data that's in the Java
heap is in this sort of what I would
consider a medium size and it was run
with a 20 kick Java heap so if we look
over here at the chart them g1 is this
darker blue parallel GC is this lighter
blue the way to look at this scale is
this is normalized to G ones performance
so anything that's higher here is going
to be something that's it's performing a
little bit it's it's actually taking a
little bit longer so in this case where
it's invoking system about GCS it's
actually parallel GC is actually taking
a little bit longer to do this than what
it is for g1 similar over here with the
big Ram tester when we get over to the
fragmentation the tree Fragger we can
see in this particular case G one's
actually a little bit slower than
parallel GC and this is kind of goes to
that classic notion of with a given
implementation of of some kind of
functionality you can think of different
algorithms that depending on what you're
doing as far as what it's asked to do
for work where it might execute a little
bit faster one versus the other you know
I could write an application where
serial GC could probably execute it
faster than any of the other collectors
would it be very specialized toward
serial GC that's kind of what you see
over here is here happens to be an
application where parallel GC actually
runs this faster than g1 likewise over
here these happen to be a couple of
applications that run faster with g1
versus parallel GC so it's kind of vary
depending on the application
in terms of command line tuning of what
you need for command line in any tuning
info this multi-threaded full GC is on
all the time you can't turn it off
there's no command line option that says
here's how you turn it on or turn it off
it's just on all the time nothing for
you to do nothing you have to turn on
your knob that you can control here is
the number of parallel GC threads so
parallel GC threads is the number of
worker threads that work both in the
case of a full GC but also in the case
of other types of GC events whether it's
a minor collection by young GC or on any
other type of g1 pause whether it's an
initial mark etc we're logging a good
place to start is to start with this the
first one here with the X log GC with
the star this is similar to what you
would have been familiar with with print
GC details thank you a little better
pointer here if you want more detailed
on these full GC phases you can add this
what's over here on the far right hand
side it's not necessary unless you kind
of want to get into the gory details or
if you're a guy like me where you kind
of want to understand what's this thing
really doing how does this work this
will give you that sort of detail for
fine-tuning g1 this is applicable not
only for full GC but also other places
of g1 a rather long sort of command-line
option you can take the output from this
if you're stuck with understanding what
it means you could then turn around and
ask one of the experts at that hotspot
GC use at open JDK and even if you're
stuck you can go there more than likely
somebody will probably chime in and say
hey can you collect this information
using this set of command-line options
depending on the description that you
come with they might have a little
shorter commandlineoptions there for you
in terms of limitations or potential
consequences not mentioned there's no
work ceiling so the time to complete
could be a little bit different
depending on how the work is distributed
to those worker threads that are doing
the full GC and that's going to be
largely depending on what those object
graphs look like so we address this in
jdk 11 this was one of the first things
that we wanted to work on now I'm going
to talk about that a little bit later
you have potentially a higher usage Java
heap usage after the full GC versus that
single threaded implementation and on
average it could be as much as a half of
a region waste per GC worker thread now
that might sound like a lot at first but
consider in g1 you'll have over 2,000
regions versus that number of parallel
GC threads think about something like a
a 16 core hyper threaded CPU think of
that as a system you got 32 Hardware
threads there to deal with even even in
that case you've got 32 you've got 32 GC
worker threads versus 2,000 regions
that's something really to be worried
about that's not a lot of space that
you'll be talking about in terms of
waste so an obvious thing here is the
smaller number of GC threads
unless this waste would be now if you
only had one of these it's usually in
essence you got the equivalent of what
you'd have with the single threaded full
GC so what we did in JDK 11 was we
introduced this notion of a dynamic
sizing of GC worker threads which turns
out saves us Java heap space because we
don't have this wasted space because
it'll dynamically tune itself to figure
out what is the most appropriate or an
appropriate number of GC threats to be
running and it also saves on native
space
a couple of reminders here yes this
multi-threaded full GC is in JDK 10 this
dynamic sizing of these worker threads
is something you'll see in the next
release and then I mentioned we'll talk
more about that a little bit later so
this gets us to the faster card scanning
so what's a card well let me see if I
can explain this for you so this is
going to get into a little bit of some
technical detail so what I've got here
is these big blue squares these
represent what is a g1 region and memory
these little gray spots in here these
little rectangles these are representing
an object that's been allocated in one
of these g1 regions and these arrows are
something that says here is an object
that's making a reference to an object
in another g1 region so here we've got
several of these guys so these arrows
are something that's representing an
object that's referring to another
object in another region okay so when g1
does a garbage collection
it'll evacuate live objects in one of
these regions to an available region
over here so think of this as an
available region it's going to be
evacuate this object to here so it's
moving this object here notice that
there's a reference here that's going to
need some updating so there's something
that needs to track this mechanism or
that these references exist
that's an essence what's called a
remembered set so in g1 you'll often
hear this term or terminology called
remembered sets this is one of the key
pieces with g1 as you start talking
about tuning it and whatnot is
remembered set so this remembered set is
this thing that keeps track of the
inter-regional references so as this
object is getting moved
there's an update that needs to go on in
this remembered set
so the thing that keeps track of this is
a thing called a card so a card in as
essence a little subdivision of memory
that identifies that these sorts of
references will cross into another
region there's a similar sort of thing
that exists with the other collectors so
it's a notion of identifying when
there's references between other objects
and they cross outside of an area as an
assessment in essence what a cards
purpose it is so when you're going
through a GC pause one of these things
that you're doing is you're scanning
these cards so the faster you can do
this the shorter your pause time becomes
so that's what this feature is all about
so as I was just saying GC needs to find
these references in these cards and
these remembrance sets to move on these
move the objects very quickly so the way
that we did this is we took a look at
the code that does this and we saw that
it was rather generic and we replaced
this with with rather specialized code
and highly optimized and in essence we
removed a lot of the branch conditions
in it one of the things that you will
run into as you work with processor
people people that make processors like
Intel is this notion of branch
prediction or a branch misprediction so
the more branch conditions that you have
in a program the more opportunity there
is for mispredictions
so if you were able to eliminate some of
those conditions your code begins to
execute faster so that's in essence the
principle that we applied here so on
this graph the way to look at this graph
is this is a graph that's an essence a
little more complicated what I'm going
to describe here think of this as a plot
of GC pause times so when I overlay the
before and after this is kind of hard to
see but you can see a little bit of a
difference in the shade of green here
where this was after we
these changes of of the implementation
of the card scanning so you can see
there's a rather substantial difference
in the pause times so what you can
expect to see here is you should expect
to see faster or short or scard card
scanning times in JDK 10 in other words
expect to see shorter pauses or you're
going to see less frequent pauses you're
going to see one of those two are
possibly both okay in terms of command
line tuning there's really nothing here
to do you get it for free there's no
command line option there's not any
tuning that you need to do to realize
this for logging suggestions it's pretty
much the same as what we saw before
limitations or potential consequences
I'd be surprised if you're going to see
anything here you should just have a
positive experience with with this but
if you do see some issues here do let us
know no reminders here both g1
multi-threaded full GC and faster card
scanning is in JDK time let us know if
you see of a issues here we'll get into
the future list this is some of my
favorite things couple of things we'll
look at here of things that I'd like to
see us do and things that we've worked
on that you should see in the next
release so one of them is dynamic GC
worker thread sizing so this is the
thing that helps us with the
work-stealing also so manually setting
the right number of GC worker threads is
it's a hard task and it's a time
consuming task there's a lot of stuff to
consider how many Hardware threads do I
have what's the underlying hardware how
many other apps are running on this
system what's the application how many
threads can it have busy at a given time
does the application go through
different phases where it might need
additional threads to deal with GC it's
hard
and we only have a small set of knobs
that you can set but we got multiple
ones so they could be setting parallel
GC threads the concurrent threads
parallel reference processing those
things are all set statically and that
number of threads are going to sit
around for the life of that app the
benefit of getting this right is you
save resources you have faster startup
you potentially realize some pause time
improvements the solution to this is g1
collects all sorts of information
already how about letting g1 figure this
out for us and let it dynamically change
this as the application goes through
different phases you might expect that
as an application initializes it
probably is allocating a lot of objects
and could probably use a lot of GC
threads at initialization time as it
transitions and gets into a steady state
it may not need as many GC threads again
it may be able to do just fine and maybe
even better without having as many well
g1 tracks all of this different
information racing branding from the
actual bytes that have been copied into
GC how many references have been
processed
how many reference objects are processed
how many cards were scanned there's all
sorts of stuff so these two charts will
start with the left one over here so
what we see here is this lighter blue is
before we made this change to do dynamic
sizing of the number of GC threads this
is this lighter one so this is the
before and after this is pause time this
is a logarithmic scale here so this is
pause time this is overtime and you can
see this here as as we're starting and
initializing our pause times are a
little bit higher and then we start to
flatten out here but notice that our
pause times are consistently lower here
over here on the right is the number of
threads that we're using at each DC so
early here in time
we're using a large number and here's
our where we're setting it statically
and they're sitting around for the life
of the app that's what's happening today
here what we're doing is we're letting
g1 figure out what is the optimal number
of threads to be running and we can see
this drops very very quickly
all right so how does this help well
it's an enhancement to G one's full GC
and addresses this no work-stealing
limitation it improves the JVM and the
apps initialization time and it
typically reduces a JVM footprint
because you don't have as many of these
GC threads sitting around that are in
essence they're not helping you they're
reducing memory footprint there's some
additional information here if you want
to take a look at this this is all
documented in and JEP 308 under this g1
ergonomics and this is the in our bug
tracking system this is a the actual bug
number that you see associated with it
now this should be in JDK 11 it's
actually been implemented already so if
you haven't access to an early access
build out there you can try this out
today the one thing I tell you is keep
in mind that disclaimer where I said at
the beginning don't believe what would I
say so this gets us to the next one this
one I'm pretty excited about too so
there's things that called the
remembered sense so we talked about this
role of what this remembered set plays
in g1 the problem with this is remember
it's s can occupy a lot of memory one of
these remembered sets abstractions exist
for every region that's in g1 so you
could have two thousand or more of these
and in some cases we've seen this take
as much as 20% of the total heap in
other words of the memory that's
allocated for the Java heap and for the
JVM as much as 20% of that going towards
this mechanism of the remembered sets to
track these inner regional references
that's a rather alarming number and you
find that the old regions use the most
remembered set memory that kind of makes
sense because those are the objects that
sit around the longest and the longer
something sits around you would expect
other things to reference it so that
makes sense so that's the problem a
couple of the key observations here are
I just mentioned that it maintains these
for all of the regions but they're not
required we don't have to have them for
all of the regions we really only need
them for the regions that we're
collecting the regions that we're
collecting are also known as a
collection set so this is a g1
terminology so if you see this
terminology and something you're reading
about g1 and it says the collection set
that's the set of collections that are
the set of regions that are being
collected on the current GC sometimes
you'll see it called AC set so that's
really where the only place that we need
these remembered sets the other thing
that we can do is if we're maintaining
these remembrance sets all the time we
have to go through this notion of once
we've collected a region and we've
updated the remembered set there's
probably some obsolete entries in there
that are no longer needed and we have to
do some cleanup work there ok is there
some way that we can avoid doing that
well the solution was or the solution is
let's only keep these required
remembered sets around when we need them
and let's just dynamically build that
when we need it and because we only need
it for that set of collections that were
or the set of regions that we're
collecting and that's much smaller than
other words
we don't have these mm regions that
we're ever going to collect on a single
GC we've got a much much smaller number
of those that we're going to collect on
a GC so why don't we disc construct it
for that set of regions the other
advantage here is it also helps minimize
some memory fragmentation so this is a
memory that's being allocated natively
for the remembered sets so what we can
do here is we can construct these
remembered sets concurrently between G
ones remark and its cleanup phases and
then instead of this this live map
calculation that we were doing we also
don't need to do this updating and
maintaining of these obsolete entries
because once we're done with this we can
throw it away because we're just going
to rebuild this remembered set each time
that we do a GC so there's none of this
obsolete entries that we need to do so
the potential consequences of this
approach are is going to lengthen the
time between G ones remark
cleanup phase and it could be up to 30
percent longer on a marking cycle but G
1 has a dynamic notion of identifying
when it should start a marking cycle so
I hop here stands for initiating
occupancy percent so it dynamically
figures out when it should start a
marking cycle G 1 can automatically
figure that out even though it may take
30% longer so it can just starts the
concurrent cycle a little bit earlier so
you might see a little bit higher CPU
utilization during those times when it's
doing this concurrent cycle
well that's one of the trade-offs that
you'll see what I would say is probably
well worth the benefits because you're
saving potentially a lot of memory so
this bounce you gives you a an upper
bound on the amount of memory that
you're going to use because that number
of regions that you need to remember
it's s4 is much much smaller a nice side
effect of this is it also improves
throughput and lowers pauses because
there's less work to do during the
pauses so on this big Ram tester that
was on one of these earlier charts the
memory usage here was about 10 percent
of the maximum heap size after this this
feature was implemented it dropped to
about seven and a half percent so it was
a nice substantial change there's an
industry standard benchmark that I run
very often and then I have some things
that I use to inject some additional
behavior to it that I beat up on a lot
of G C's with and when I ran that on
this this feature on this change its
throughput improved by almost 10% this
workload also has a notion of
application response time in it it's
actually a geometric mean of a couple of
different app app level response times
ranging anywhere from if I remember
right something like 10 milliseconds to
200 milliseconds it had a 10%
improvement in its latency that I was
really impressed with I thought those
are pretty impressive numbers so here we
had a feature that we were able to
reduce memory memory footprint we're
able to increase throughput and reduce
latency that's very very unusual to
realize a benefit and all three of those
areas that usually requires a lot of
work so I was really really pleased with
what I saw with this some additional
information on this is you can find this
in this bug number here
again this should be in JDK 11 again
remember that disclaimer and I mentioned
at the beginning don't believe a word I
say to summarize this JDK 10 has this
multi-threaded full GC you're going to
be able to realize faster shorter pause
times from the faster card scanning a
couple of the new additional
enhancements coming in g1 this dynamic
GC worker thread sizing the rebuilding
of memory remembered sets and you're
going to be able to realize that lower
memory footprint the higher throughput
and lower latency a call to action here
for you go check out JDK 10 I just
released today as you've heard we'd love
to hear your feedback if you liked what
you saw with what I talked about with
JDK 11 there's some early access bills
out there I'll leave this slide up here
when we finish or I'll go back to this
slide I just have one more after this
one and these slides are also going to
be available online from what I hear so
you'll also be able to get a set of
these so one of the things you can do
with JDK 11 for instance is you could do
some early qualification on those
features if you got an environment where
you can do some testing it's kind of a
nice way that you can do some early
qualification so when it comes out as a
release you know you can hit the floor
running and you can do that that upgrade
right away if you're looking for
something has lower latency a project
that we started actually this has been
under under work for several years now
that we just recently open-source it's
called zgc zgc is a low latency GC
that's targeted I rarely see something
over 5 to 10 millisecond pause times
with it or anything larger than that I
have to write something very specific
and really beat on a given area
of pauses within the JVM to get
something outside of five to ten
milliseconds from a stability standpoint
I'm really pleased with what I'm seeing
with this thing stability wise so if you
have an opportunity to kind of want to
take a look at that if you're looking
for something in that lower latency area
go check that out
couple of people I wanted to acknowledge
that we're really helpful in getting
this slide deck together a couple of the
g1 engineers Thomas schatzel and Stefan
Johansson and really everybody from the
hotspot DC engineering team for their
efforts and improving hotspot and also
the open JDK community we get a lot of
contributions from people whether it's
their experience as far as what they're
seeing with with an app questions that
they ask tuning questions code
contributions etc so a huge thanks to
those folks I'll leave this back up
we've got about five minutes here if
some people have some questions yeah
so we're talking about the the number of
you know what's the ideal number of GC
threads is that we're talking about okay
so the question is around is there a
number of ideal parallel GC threads to
use that's gonna vary some based on the
underlying hardware and also on the
application and and what I mean by that
is I'll start with a latter part of that
as you saw in that chart when an app's
going through initialization you see an
awful lot of ABS objects being allocated
and the system is very busy going
through that initialization phase you
expect that it's going to be able to
want to utilize a lot of GC threads
because it's allocating very very
quickly it's doing a lot of work and
then as it transitions into that steady
state where it may not be using a lot of
that CPU I think most apps when they're
running in in production in a steady
state might be using 25 to 30 percent of
the CPU maybe a number of around 25
percent of the total number of Hardware
threads might be a more optimal number
of parallel GC threads at that point but
as it transitions into a stage like it
goes into like a busy hour or a busy
period you might want to see that number
increase it's gonna that's how kind of
how it varies there where the underlying
hardware comes in is you have systems
that have a different number of
underlying Hardware threads you also
have different CPU architectures where
that may also influence the number of
threads that you want to have for
instance on on SPARC systems they have a
much larger number of Hardware threads
than say an Intel or an AMD system so if
you have a very very large number of
Hardware threads when the JVM starts and
you've got a large number of parallel GC
threads and influences the time it takes
for the JVM to start because it's having
to start all of these GC worker threads
also
so it's there's a couple of moving
pieces in there to figure out what that
ideal number is yes it can and and
different CPU vendors say as I was
saying for instance you take Intel's
typically you have two Hardware threads
per core that's hyper threaded core
there are systems that spark makes that
can have as many as sixteen Hardware
threads in a core so it's it can vary a
lot we're in in the Intel case maybe as
you're going through initialization you
might want the same number of Hardware
threads as the number of GC worker
threads in the case of that spark system
because of the difference in the chip
architecture you're not going to want 16
hardware or parallel GC threads for each
one of those Hardware threads because
not every one of those Hardware threads
are going to be executing at the same
time it may only be 50% of that so it's
it's a that's a rather hard question to
answer but those are the two variables
are the two largest variables in is is
the application behavior and the
underlying hardware
other questions yeah
well the question is how many GC threads
are started by default oh I used to know
this number off the top of my head if I
remember right it's in the area of it's
a formula along the lines of five
eighths the number of underlying
Hardware threads if I remember right
it's in that area I didn't hear all of
that it's not a fixed number four you
know every system every you know the
same four on every underlying piece R it
will look at the underlying hardware to
see how many hardware threads there are
and then it will apply this formula it
isn't exactly five-eighths the number
five member right it's but the formula
is very close to that typical region
size it can be anywhere from one
megabyte to 32 megabytes 32 megabytes
being the maximum region sighs so when G
1 starts it'll look at the Java heap
size that you have specified and it'll
say okay
ideally I'd like 2,000 regions okay so
if I have a 2 gig Java heap okay that's
going to be a 1 megabyte region when I
get to you know a rather large Java heap
on the order of 100 gigabytes it's gonna
use a 32 megabyte region and you might
have more than 2,000 yeah it'll always
target around 2 it'll target 2,000
depending on the Java heap size and then
it'll use in multiples of powers of 2 1
2 4 8 16 32 but 32 is the largest size
yeah so what I'll do is it'll take
multiple regions so the term that g1
gives for that so the question is if you
have an object that's larger than a
region sighs what does g1 do so g1 will
treat that region as as something called
a humungous object so this is a
terminology that you'll see in and the
tuning information about g1 that you'll
run across is they'll talk about
humongous objects so a humongous object
is an object that's larger than or it's
an object that spans more than one
region so it'll take multiple regions
put those together to hold that
humongous object yes
I don't know if I could put an actual
number on you know percentage-wise or
what the actual the Delta would be
between say JDK 9 and JDK am some of the
areas where I know there would be rather
drastic improvements are there's a
certain set of family of applications
that will allocate one really large
array or they've got a really large hash
map those are things that are rather
hard for a GC to go through and and mark
because they can be very very deep and
very lengthy to go through and Mark
we've done a lot of work to improve how
we go about marking that or stealing to
be able to mark that in the shorter time
that's one of the big improvement areas
okay I think that's it thanks everybody
for coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>