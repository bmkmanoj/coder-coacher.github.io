<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Complex Integration with Oracle Coherence | Coder Coacher - Coaching Coders</title><meta content="Complex Integration with Oracle Coherence - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Complex Integration with Oracle Coherence</b></h2><h5 class="post__date">2015-06-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/UUZQbGwrPQo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm going to give you a little bit of
background on myself so you know roughly
where I'm coming from not a huge amount
of detail but you'll at least know where
I'm approaching these problems from
we're going to look predominantly
architecture side not its deep
implementations of coherence and and how
to to tweak every last little bit out of
it but a lot of what I'm going to be
talking about today will certainly make
huge amounts of difference in
performance and then generally around
the architecture that's as you use go
herons and have a quick show of hands
those who use coherence at the moment
wow I could put a fair guess in that you
guys probably know more about the actual
details of coherence than I do but I've
been working with it on and off I since
it started back in two thousand or so
with Cameron and such so this isn't this
isn't going to go into a huge amount of
detail and coherence but it's going to
go around the sort of outsides the
integration getting into coherence and
this is based on several numerous
clients in fact predominantly large
banks dotted around the world some of
which have been in production with this
six or seven years going back to about
2000 58 years invade so I'm going to be
a little quick bit of background on
these sort of banking world banking
environment who's who's in financial
services here few of you tacos retail a
proud telco chap so again this will be
predominantly financial services but
we've got a some interesting bits in the
telco side and there's obviously a
fairly relevant outside message is a
message so your you'll pick up the
differences so hitting view a bit of
background certainly where I'm coming
from on this the pros and cons of XML
some of the problems we have with that
going to look at Java binding which is
sort of the key part of this and binding
outside of XML so this is binding with
comma delimited files binary
particularly on the telco side and some
of the sort of more nasty things and as
you'll see from the industry where I
come from and I'm assuming will be the
same with yours XML is is certainly in
there big time but it's it's still not
the lion's share of the problems that we
have on the integration world at this
point so
this is a combined solution that we've
been putting in over the years and
recently we've had some say issues we've
started to move into different areas
getting further and further up as Java
gets better and better and better we're
starting to move up into the low latency
high performance area where
traditionally was occupied by C C++
certainly for deterministic performance
we still stick in the C C++ but we're
now starting to get java up there right
up at the front so we're now looking at
performance into the high hundreds of
thousands per second and while we're
doing this this then starts to move the
problem elsewhere so again I'm give you
some introduction into that at the end
QA I actually managed to grab lunch
before so I'm just going to go for the
beers after this my backgrounds I'm
deeply geek I spend a lot of time with
gadgets programming etc but as I said
before I don't i'm not fortunately now
one of the ones who get sent out to the
bank and sit there 24 hours a day until
the problems fixed I've got colleagues
that do that fortunately but I did spend
my time coding I spend my life on
aeroplanes and my probably biggest
problem is the nexus servers are not
accessible for most planes and so maven
is one of my pet hates as I try and
compile stuff that they downloaded on
the airport lounge and of course it
doesn't work when you get on the plane
but I'm certainly plenty of time in that
and we've got 30 odd large banks as huge
customers and so this is a combined sort
of effort from all of those this is a
real diagram that I borrowed from a bank
long enough ago now that probably nobody
recognizes it I presented it a number of
times and nobody's yet recognized it
it's old enough dates back from about
three years ago this is the sort of
thing that you see on the managers walls
and they tend to color code them and it
gives us a rough idea of the complexity
inside this is not unusual a recent
survey in a bank large European while
worldwide bank should say based in
Switzerland that now is it
about two of them looked at the number
of databases they had they had 49,000
instances of databases give you some
idea of the complexity and their data is
just spread around all of those places
and you see every single one of these
boxes represents numerous different
implementations if we look at this from
a functional point of view I didn't
expect you to need to read those but
we've got things like trading matching
confirmation interface management
payments funding they'll have risked
margin all of these sorts of things this
moving from the top down once we got the
front office where the trades happen we
got the middle office where they
calculate the sort of which trade should
be happening we've got all of the
payments to surf in the middle office
and then finally finance at the bottom
once we've aggregated everything that's
been done in during the day we've added
it all up we make the payments at the
end of the day or the following day or
in some cases several days later so
moving from the top we've got price
quotes and 200 300 thousand a second
coming out of here we've got trades
happening by machines in the hundreds of
thousands per second as we move down we
get more and more complexity as we a
grenade all of those numbers into large
portfolios of derivatives the
derivatives are immensely complex then
finally at the bottom the numbers are
much lower with perhaps sending messages
every few minutes or certainly every few
seconds but these the values on these
are so large they're in the billions of
dollars we have one client that puts
through more than a trillion dollars
through their systems or through our
systems every single day and they've
been in that for 10 years imagine the
amount of money that's gone backwards
and forwards if you miss one of those
messages the interest alone on just one
day of that is it goes even with today's
very low interest rates goes way way way
into the hundreds of thousands of
dollars so it gets quite important about
guaranteed message delivery so if we
just isolate a small part of this taking
out some of the spurious parts again
this is just looking at one particular
transaction we look at all the different
types of message flows that go through
these now these this is what we call a
it's a distributed transaction but the
long transactions these transactions the
very worst thing that we would do with
this is actually use distributed commits
on this and what typically happens stuff
will happen at the top it'll go through
get sent down we'll have maybe later on
that day few hours later the settlements
could happen as to say as much as two or
three days later so these are these are
long transactions and if something goes
wrong it's it's absolute hell trying to
wind it back up the ultimate goal for
every bank here is what's called stp
straight through processing so there's
no human interaction that's the perfect
scenario for a bank stuff gets traded at
the top automatically goes all the way
down and is settled at the bottom that's
the absolute goal that's the sort of
nirvana of every single bank that's what
they pay their literally hundreds of
millions even billions of dollars to try
and achieve that's what they're trying
to aim for what's important here though
you can see the colors we look at the
top it's predominantly very low latency
high performance so we get we certainly
have no time for brackets here so
there's very little XML it's typically
fix and other standards in that sort of
area and we get into the CSV flat files
very typically inherited from databases
because they're the typical export from
a database then we get into proprietary
internal formats very typically XML by
this point and then finally as we start
to send these out to various other
places we some of the modern standards
or XML but a lot of them are still
traditionally 30 or 40 year old
standards like Swift so that's one bank
that's one transaction in one bank then
that goes across to all of the different
offices in every single bank around well
not every single bank but all the head
offices around the banks around the
world then you've now got to communicate
all these together but you know what a
bank does it deals with other banks so
that might be the infrastructure in one
bank they'll had have have head offices
in each of those locations but they deal
with exactly the same thing in the other
banks so we've got all the communication
going between all of these so you adding
then all of the institutions every one
of these at the bottom represents
exactly the same thing at the top and so
again we add yet another pile of
integration
so we've got all of the integration in
the bottom in terms of message types
we're looking at tens of thousands of
different individual message types about
one bank alone if they're doing prime
brokerage which is where they treats the
middle office and back office payments
for another bank most of the large banks
do this they'll have thousand two
thousand different clients those clients
are hedge funds that do the front office
trading and they send all of their
trades to the bank but every one of them
is not an IT shop they do all of their
work on Excel spreadsheets and they st.
comma delimited files to the banks the
banks have to integrate these comma
delimited files into their standard
formats into XML into databases and then
report them to all of these regulatory
bodies here and more and more
governmental regulatory bodies are now
insisting that the bank's explore this
so this creates should we say a
nightmare for somebody who actually
cares about it for those of us that
charge money to go and sort out the
problems it's it's absolute bliss
because it's a never-ending problem and
it'll certainly be there until long
after I disappeared in hopefully many
many decades from now so it's it's a
great amount of IT work to keep us busy
looking at the XML XML is is definitely
the sort of ongoing solution at the
moment this fuzzy bit on the right that
you can't really see looks like a sort
of sorcerer on its side is looking at an
F pml derivative and I've zoomed out and
I've zoomed out in out in out in out
this roughly 3,600 elements in there so
i zoomed down to that and this gives you
a little bit more you can start to see
the elements now and as we zoom out a
little bit further we start to see the
some of the relationship between these
and then finally at the top there I've
given you sort of a zoom out of I was
zooming in detail of what the sort of
thing was we're looking for what's
important here is you see a lot of these
as 021 which represents obviously
optional what this actually means is
that most of this stuff the 3600
elements are optional it's like
somebody's thought of what are all of
the possible things or ways that
something could happen or something
could go wrong or something could be an
exception and I want to provide a field
for that it's like the sort of ultimate
this will capture every possible thing
and in fact that's what it is every
single derivative trade that has been
done that caused the nasty mishaps in
2008 it's not a result of this obviously
but they were all defined using this
standard FP ml is financial products
markup language and that's basically
what all of the derivatives are written
into interestingly there however when
you present this to the the IT people in
the bank's they know they have to use it
and they think it's our my god it's
going to be a complete nightmare but
when you actually take the average
typical trade it actually only looks as
simple as this that's an example
obviously they get much much bigger but
for those taking pictures perfect but I
can give you the slides afterwards make
your life much easier but feel free to
take pictures there they're not
proprietary the basically the date is in
there as you can see if you're swapping
some information a currency in an amount
it's basically in there and what all of
these extra fields are exceptions what
happens if somebody doesn't pay what
happens if the payment is like what
happens if the interest rate changes
these are all a little sort of what if
cases and that's why they're optional
and the optional fields come off that
but your average trade looks like this
so it's actually not quite as bad as you
might imagine so it's XML the future no
probably not well definitely not I'd say
because again we have lots of lots of
issues particularly around performance
it's it's very very overly verbose for
for a lot of the things that we need a
lot of the new standards are using XML
FP mlo mentioned before fix ml fix ml
came out six or seven years ago and it's
still only used by about thirty percent
of the areas using xml again because it
has too many brackets there's no point
in parsing all these things when you're
trying to get be the first one to get
the trade iso 20022 is not really xml
messages it actually is a standard it's
usually implemented as xml but in fact
it can be implemented as I saw it as a
SN dot one so it doesn't actually define
a an implementation and then we're left
with this huge amount of other types
the predominant one again is csbs and
that's the one I want to concentrate on
just purely to give you an example of
the sort of things we do I'm going to
give you a another brief example of
binary standards as well which was
introduced recently by telcos what we're
trying to achieve here is to get this
huge amount of data into a coherence
cash why do we want to do that well I
don't the banks do the banks pay us to
do it so that's that's that's why it
works works quite nicely but the
nightmare is as you can see coherence
works beautifully well there's no
problem with that but it's getting these
tens of thousands of different types of
messages into coherent so that we can
actually use coherence for what it's
good for in terms of querying it in
terms of exporting distributing the
information around the various different
nodes we've got and that's really what
the headache is the vast majority of the
time we spend is actually getting the
data into it not actually configuring
coherence itself the other problem with
XML in fact with any language is that we
have business rules give you a very very
simple example you imagine a comma
delimited file with credit card
information in it obviously there's the
security issue behind that but again a
credit card transaction has to occur
before the expiry date so you can make a
simple rule to say the credit card
transaction dates in column G must be
before the expiry dates in column X so
this is a simple business rule but even
in XML you cannot define this there's no
standard there's a thing called schema
Tron but that is bound to xml and that
again restricts us so there is no
standard to do that yet today so the
standards organizations publish these
standards xmf pml iso 20022 etc and
again iso 20022 doesn't have to be XML
so I'm not going to bother to read these
you can see the horribly complex and as
you can see from the bottom one ird 57
means interest rate deposit derivative
and that's rule number 57 these are
mandatory rules if you don't respect to
these rules and implement them your
message gets rejected or should do or
some will gets into trouble and someone
gets fines and these banks get finds
tens or hundreds of millions of dollars
so it's well worth implementing them we
look at some of the Java binding
technologies this this is the way we
went at the end of the day if you're
going to put messages into a an
in-memory database the in-memory
database cash grid whatever you want to
call it the names change and you'll find
me using them sort of interchanging them
if you're going to put it in there it's
Java so you might as well put it in in
Java he might as well leave it in Java
if you put java objects into coherence
you can query them you can get them back
out if you put XML in typically what you
do is you bind the XML and you but the
XML in as as java objects so we took
exactly the same approach and this is
exactly what we do originally going back
quite a number of years we took castor
because Jack's be didn't exist at the
time and they were restricted to dt DS
in those days before the schema came out
so we took castor was the first one that
managed to get the schemas out and as
you probably remember those of you most
of you look old enough to remember at
least 10 years back when XML was coming
out we had dt DS first then we had
schemas and then for those of you that
work in finance fpm L came out and FP ml
is so complicated that it used to break
all of the tools and so XML spy used to
break instantly and you'd have nine
releases within two days just trying to
keep up with the latest version of FP ml
so it's really pushing the edges but
what we did was to take this and we
started to use it for financial services
standards so we started with the XML and
then we thought I on we can do that with
comma delimited files we can do this
with some of the others we can actually
take a comma delimited file we can bind
it and we can bung it straight into our
cash so take something like a comma
delimited file two examples believe it
or not they're actually real examples I
had to take some of the fields out in
case anyone recognize them you know what
it's like with Google these days you put
a you take any random number put it in
and it find something on there so they
are actually real bits of comma
delimited file you take these but the
data in them is still the same data as
you saw from that piece of XML so we
look at the bottom the XML
yeah I've color coded these just to make
them obvious so you can see them but you
can see the amount the currency the
trade days when our trade goes through
the system from front to middle to back
office it's the same information now
there's information in the front office
that may get thrown out such as who did
the trade what time the trade was done
or something some of that information is
important some of it isn't when it gets
to the middle office what's important is
the counterparties the people who are
actually paying the actual finance
behind it so the risk behind that and so
you can assess the risk of what you
might be buying the commodities do we
actually own the commodities we're
selling what are the what are the rules
behind them if we're shorting a trade or
something like that and then finally
when you get down to the settlements you
need to know who you are paying and how
much you're paying and you need to tail
this up but at the end of the day in a
bank if you're buying a commodity you're
buying a currency you're selling one of
these things the information as it flows
through sixty seventy eighty percent of
it is the same information as it flows
through so if you go think back to that
diagram with the transaction with all
the different colors we got fixed we've
got a FP ml we got internal proprietary
formats csbs and Swift the vast amount
of information that we got going through
the system is is the same so now we
start to create a repository of metadata
behind this and we can now start to map
these to our different standards but
interestingly we can actually take the
comma delimited file and all we need to
do is to tag a particular column as
having a type and we can now start to
model that so we can take a csb as you
see it in excel here as it's imported
just into excel this unfortunately is
one of the origins of many of the comma
delimited files it's probably the worst
case it's one of the my mind what are
the only reasons Microsoft has survived
is because they're so predominant in the
Excel and area in this sort of
environment but people export these
things and obviously want to import them
but the data in there is absolutely
critical unfortunately they take Excel
way too far and they start to define all
sorts of other things
rules and information which is human
readable but not machine readable if we
export this listed out we get something
like this and again this is just a quick
demo a sort of an example that allows
you know that you can actually see main
credit card expiry date amounts currency
etc it's a sort of information this
however is exactly the same as if we'd
represented it in XML if we model this
and what I've done here is I've actually
created an XML equivalent of this and
I've imported it into oxygen which is
like XML spy just another one of the
ones that I use think of the spreadsheet
now it has an optional header the header
may or may not be there doesn't really
matter it has an unbounded number of
rows ideally at least one so one too
many so we see at the bottom here we've
got one too many rows an unbounded
number of rows each of which have more
elements and this is the one that I've
expanded that has strings dates integers
floats all the usual sorts of things and
at the bottom and if you go back to this
nip back here you see the bottom very
typically you have other stuff that
typically appears out of spreadsheets
just to annoy you and if you code this
you find somebody has on the 10,000th
and five hundred and twenty fifth row
somebody's changed an integer into a
float and it just so happens you only
looked at the first thousands and
assumed it was an integer so you wrote
your code around the integer and that
10,000 etc one is is a float so this is
the sort of thing you always get
something awkward at the bottom so it's
it we need to cope for this as well so
again going back to this standard we've
got a in this case a footer which has a
string and an integer and it contains
information but this is the sort of
model we want if we can model this we
can read the kama delimited file in
using a parser and we can generate the
parser because if we know the model we
know the separator we know the
delimiters and we know the line
separators we can generate a parser that
passes this and fills this model in this
is what we did a couple years ago we had
a company called c24 this effectively
used castor
and created a java binding tool that we
could use for all of these situations
now it's it's free to download you can
use it for all of these situations and
the the payment part is obviously when
you start to use proprietary libraries
or as we say standards iso 20022 FFP ml
doesn't stop you downloading at pml and
importing it yourself so this is what
we're using for this so if we take this
we take the comma delimited file what we
do is import it now if you import the
comma delimited file in Excel you'll end
up with a little wizard and you'll have
to say these are the limiters these are
the these are integers these are floats
etc and that's exactly what we do we
import it we get something similar to
the top and then we get through and we
further qualify this and we can go
through in much more detail now the
bottom here you can see I've started to
provide a regular expression to define
what the expression for the credit card
is now maybe it's spaces maybe it's
dashes and then of course I could modify
that and put bracket dash space ? etc
but what this means now is if somebody
sends me the comment delimited file a I
can pause it I can put it in the Java
but i can also validate each and every
single field to make sure not only that
it's a date but to make sure the content
of the file has actually got what it
should have in it this makes it much
more much safer once I've imported the
file put it into coherence and if I'm
then using the coherence query language
for example I'm trying to extract
information out it means that the
information i get is usable because if
someone puts a credit card number in
which is missing one of the bits or for
example it doesn't do the mod 10 check
which all credit cards have a little
checksum in them it also means that it's
going to fail so I the best place to
actually indicate that something's
failed is when you receive it not when
it's got into your internal system and
you've started to add up all the numbers
and then you find something's actually
finally failed so input the file put
lots of information into it and this is
the sort of result of what we get
we move that across and this generates
Java code now this Java code is
basically using the internals of castor
you'll notice that it's everything's
fully qualified so we've got
java.lang.string you know I think yeah
horrible why does it do that if you had
a column his name was string you'd have
a lot of awkward problems if you had a
column whose name is class or if you had
in Excel sorry in an XML schema as
happens all the way through these
standards an element called class
unfortunately that then starts to screw
up a lot of the code generation so you
have to be very very careful it's it's
not quite as obvious as it might seem
and the things that as you expand these
out you start to learn so we fully
qualify them of course you'll often see
date somebody will put date up there and
of course date will conflict with Java
dot util dot dates whereas we may need a
separate class Hall date so everything
is fully qualified it makes it a little
bit ugly when we load the code into to
read it but it works a lot more securely
this is generated code so we don't
typically work with this what we work
with is the library that this generates
but it's sort of up there just to show
you that we go from this comma delimited
file to this code and of course the
basic types integers Long's etc we we
have extra information to see whether
they're actually sets changed etc so we
can lipid whether they've been modified
so we call this a complex data object so
we we inherit another class this class
gives us a lot more features and it
allows us for example to do xpath
navigation so there's an XPath navigator
with inside the object so take a comma
delimited file loaded up into Java we
can now go to the root of this and we
can now navigate the comma delimited
file as if it was XML without actually
having to go and turn it into XML it's
also incredibly performance because
you've now got this large comma
delimited file now yes if it's a six
hundred megabyte or a five gigabyte
comma delimited file which let's face it
is not unusual we need to slice it up we
can do that in per line we can do it in
back
so yes it don't typically hold it all in
memory but it allows us now to do things
like use XPath to to do aggregate sums
etc but it also means that when we put
it into coherence we can use the
coherence query language we can extract
stuff out but we can also extract
information out of groups of files and
execute XPath than them to get for
example again to do transformations and
export the details but because we can
now navigate the comma delimited file
from the roots remember going back it
has a root the root has three elements
has an optional header an unbounded
array of rows and a footer at the bottom
in this particular case we start at the
root we can navigate through the entire
tree and we can export those as as the
original CSV if we know the delimiter
switch we obviously do as xml tag value
pairs Jason we can generate HTML
documentation we can also do customized
serialization which actually makes quite
a good performance so in comes the
commons delimited file very quickly
without doing anything else we can turn
that directly into XML we could also
treat it as if it was that XML without
having to actually convert it into XML
we can export it into Jason so it's it
it's very useful for being able to take
something and then export it put it in
again typically its adherence we're
using coherence as a sort of an an ESB
or a central cash within our
environments typically all of these
trades in our going into a coherence
cash and we can now query these out but
if we're clearing them out and then we
maybe want to send them across to to
another application we can convert them
even convert we just export them as JSON
or as a web service or XML etc without
having to actually ever do the
transformation so as we move the data in
we need to do quite a bit of typically
we don't just put everything into
coherence what we need to do is some
sort of filtering on these so along the
outside we typically got mule
Hugh's camel weblogic whatever it's
called now websphere again the clients
have all these different things we've
got a spring integration spring batch et
cetera so we've got something which is
enabling is to read the messages either
pole them or to be triggered or to
receive them from some sort of mechanism
spring batch to decode the the zip files
to batch them and then to send them into
our system we have a multi-threaded
engine which basically decodes these and
now writes them as Java objects directly
into the coherence cash so this is what
our typical very very simplified diagram
looks like this if you want to spread it
out into a into a bus have to be sort of
careful in a sort of implying that this
is now your ESB but this now is your
central caching layer so if you wanted
to for example put trades in here you
imagine this coherence less spread out
we can pick them up and query them and
take them out in any other applications
so this potentially is now your your
centralized bus now when I say there's
lots of different banks using this there
are plenty of banks I'm certainly not
going to name them here but I will name
them afterwards at the cost of a beer
but out of about world's 20 largest
banks is about 15 or 16 of them which
are our customers which use these sort
of things now it's not just coherence I
have to add that there are other
solutions here there's things like hazel
cast gemfire giga spaces terra cotta etc
which equally well fit in this space and
it would be unfair not to mention them
because they're widely used there are
many banks which use multiples of these
technologies because they're just so
large they just buy different bits and
people change their minds so the same
architecture works so a lot of the what
I'm sort of talking about now is exactly
the same architecture using similar if
not some almost identical technologies
another interesting one that we we've
been doing earlier this year is where
the telco the message volume here give
you an idea is 88,000 messages a second
which is the average
of 24 hours a day 7 days a week 365 days
a year that's 88,000 every single second
now that's just the average so you can
imagine at night it goes down somewhat
but in the morning rush hour when
there's a global event that happens or
the equivalent of thanksgiving or
christmas new year and things like that
like goes way up into the hundreds of
thousands per second in fact almost
every day it hits 200 300 thousand a
second now you imagine processing these
if you have code that processes so we
said the averages 88,000 if you can
process 300,000 a second and for half an
hour you get 500,000 a second you now
have 200,000 per second for two hours
which is 72 million of these things to
sit and wait and process afterwards and
that's assuming that you actually have
any resources left because even though
they goes down it's not going to hit the
bottom it's still going to be sort of
coming down so think of that many
messages to be able to store them you're
needing hundreds of gigabytes of RAM so
we need to be able to process these
things at lightning speed so the goals
that were looking at it between one to
one-and-a-half million messages a second
now that's not just being able to
process them that's being able to
process them understand them sort them
triage by triage I mean deciding which
messages we do and don't want and also
store them and you've got to do that at
one to one-and-a-half million a second
so I'll get to some of the issues that
that created so we take a standard again
going back to the one that we had up
there the RFC 2865 which is the radius
standard used you've got the 2g 3g two
and a half g 4g LTE etc etc all these
different you a voice and data you've
got to put all these in it's a binary
standard it's actually pretty
complicated although if you you know any
coder can sit down there and probably
code most of it in a couple of days most
of that will be reading and trying to
interpret the little bits of nuances of
the of the standard but there's lots of
little things in there that say if this
bit is set then the following field will
be there if it's not set than it doesn't
exist and so ever
else moves up a little bit because the
binary needs to compact it you'll have
integers which are filled bits 7 through
to 13 for example in a 32-bit word so
there's a lot of the little bits and
pieces you need to do a lot of binary
manipulation imagine now however if you
were to put that into coherence how
would you query it how would you know
whether your your data is there what we
can do however is to effectively bind
this we do it exactly the same way we
define it as if it's a model and this is
this is the actual definition of the
radius 2865 spec including right the way
down as you'll see here we got unsigned
for byte words all sorts of bit fields
and when you click on these you go into
more and more detail and looking at the
complexities of which bits start bits
and bits or zech sores and and such so
we go from this this is an actual debug
and again I've had to take out a few
little things and modify a few bits
because he's actual 3g data from an
actual client modified enough I think
believes statistically that it will
never be found out who it is but I sit
there with a little dosbox dose what
they call it these days Oh command
command line window see how old i am now
so it sit there put this stuff out and
I'm literally sitting there taking these
and I'm checking them against the
standard now some of these are obviously
bytes that you can see quite easily some
of them have got little bits and fields
inside them some of them are actually
masks but what we get out of this once
we've modeled it because we understand
the model we can now read the binary
standard we can actually create getters
and setters out of this so we can now go
directly into coherence with a binary
message and we can actually do using
coherence query language we can do a
select whatever from messages where
value dots getversion ID where version
ideas bits 5 to 12 and the getter does
the masking and the shifting to return
the value 5 coherence is none the wiser
so it basically returns the values
you're getting out of it on top of that
we can now apply this to the
ESB so the mules the camels the fuses
and the spring integration etc we can
now apply the same logic and so using
spring integration we can now do simple
triage or filtering our messages to
decide okay this is voice message goes
off to this cash this is data goes off
to this cash this is somebody on roaming
goes off to this cash that's important
you can't just bug everything into one
big cash and then sort it out afterwards
you've got to do some sort of splitting
up front to be able to effectively slice
the the data and decide where you're
going to put it because it makes your
queries significantly more performant so
we can now do this inside spring
integration or mule or camel whatever
whatever takes your fancy but it's
really what's important is you can do
this on binary you can do this on comma
delimited files and that's that's really
the advantage we're getting out of this
so the CSV s and binaries are incredibly
efficient as one reason why we're not
going to use with very unlikely in these
areas to move to XML there's no point if
you think about it it's effectively
sending the header with every single row
so you're at least doubling the size and
then add all the spaces in the comments
and and all the other little bits and
pieces and the lovely little brackets
ago at the end and it's at least twice
the size of a comma delimited file i
comma delimited file is already using
binary coded decimal so that's already
inefficient by a ratio of sort of six to
six to eight so two-thirds of
inefficiency of the top bits that you
don't use so you get into binary that's
where it starts to get really efficient
and that's why the telcos use binary
because it's super efficient your phone
doesn't have or didn't have the
horsepower to send the messages to the
cell towers of with the data so we need
to compress it interestingly there
however this Christ performance issue
because there's absolutely no way that
you're going to take those binary and
small messages convert them into lovely
Java objects complex data objects that
we use before where you've got instant
strings and doubles and floats and bits
and pieces with all the getters because
there's no way you can do that at one
and a half
in a second and if you do that at one
and a half million a second it generates
one and a half million as seconds times
the number of elements you've got as
garbage and that means the garbage
collectors coming in so regularly it's
it's basically killing the performance
so out of this we were getting roughly
sort of 20,000 messages per core which
is sort of good you know on a laptop we
could eight or four calls and hyper
threaded with IO bounding is that are
even on a laptop i could probably get
four or five times performance so i
could hit a hundred thousand a second
out of that but again remember if you
don't quite meet the peaks everything
that's above the peak ends up in a
buffer or accused somewhere and there
are laws around different countries that
say you need to be able to process that
data to get unfortunately things like
terrorist actions you need to be able to
follow find out where people are that
sort of stuff there are laws that
require that the telcos the MNOs
actually provide that data so we cannot
afford to have stuff sticking in queues
apart from the fact it's a lot of memory
so we we changed and so this is where
we've now really severely moved off the
caster and we've now introduced the byte
buffers and this is a result of the
requirement for performance so what
happens now data comes in this could be
a comma delimited file it could be
binary we basically just loaded up using
niño straight off the socket straight
off the the desk wherever it may be and
we put it into a byte buffer bytebuffer
reserves or its Patti old now 1.4 it's
been around for long time but I seen it
used a huge amount it's basically just a
wrapper for a byte array you could use a
bite or as well but it provides a few
little interesting bits and pieces there
are other implementations of it as well
but I generally prefer to try and stick
where they can with what was sons now
Oracle's Java so little demo of the sort
of code you can do on it it just allows
us to get in shlongs bytes and things
out and we can move up and down it we
can we can sort of limit the amount of
size we've got we can also slice it and
create another copy we can also pass it
around and it becomes very efficient
what we can do however with this is we
say
here's the file or here's the line of
the comma delimited file if you want the
data out it's here but we don't bother
to pass it until you actually need it so
we do what we call lazy pausing so now
the data is coming in in some cases we
need to sort of pre scan through we need
to look obviously for things like end of
rows things like that we can do that
very very fast and very efficiently
certainly more efficiently than pausing
every line we can look for the line ends
we can look for the limiters we can look
for certain things and it's a careful
sort of management of how much of that
do we do up front versus how much do we
do when we actually use the getter now
if you used no getters whatsoever you
could take literally a gigabyte file you
can load it into memory well as fast as
niño works which is sort of mini second
time because effectively all it does is
map it into memory and pass you back it
just reads it in off the network card
and passes you back a pointer but we can
start to pass these things that
incredible performance where the the
performance starts to get slightly
slower is when you want to get the data
out but when typically when you're
putting data into a coherence cash
you're querying a few fields you might
aggregate sum of those fields and take
out where this and this and this which
are typically looking for three or four
fields and don't forget we've already
sorted it so we've already put it into a
hears the voice data here's the data
data and his the other bits we've
already partly done it now we can go
back and get the little bits and pieces
out so the performance now we've gone
from roughly 20,000 a second per core
this is non scientifically measured on a
laptop it is goes into production but
you don't get to mess around with the
performance figures on these things to
comfortably over a million a second per
core and that's quite a significant
performance increase and that's taking
three to four fields out typically
obviously the more fields we take out
the slower it gets but it's a good 50
times faster that's where it gets
interesting because we can now take the
getters so they get the Gator now just
simply uses data which is the byte
buffer
gets the field that it wants performs
the and either mask in this case right
ships it and that gets our number of
elements or the data that we want the
interesting thing is that the after a
while the compiler pics in and this
stuff absolutely flies the real problem
we had now because we're starting to
move the problem elsewhere as we can
pass this stuff now two million two
million a half a second perk or so on a
in a typical decent Linux box we're
hitting up in the 5 to 10 million a
second which is more than adequate
problem is that when we hits the ESB the
ESB is dynamically filtering these
things so you hit something like spring
where we've got a filter where we're
saying I want to get the duration of the
call for example this is dynamically can
or put in and uses reflection so
something like an expression like
payload duration less than point one
second so we want to find all the cutoff
calls we're now passing these things in
at a million a second or several million
a second certainly way more than we need
to for the performance but again
sometimes things get cut off and we get
massive piles and one goes it's it's
good to be able to catch up the problem
is this typically generates reflection
if we put reflection in there that takes
about seven hundred and nanoseconds
again non scientifically measured that's
700 nanoseconds is pretty much the same
time as it took us to pause the message
in the first place so it haves the
performance just with one single call
init so what we did we started to look
at the options of compiling the queries
so now we can take the queries we can
compile those so from java 1.6 they were
very kind and they put in a effectively
put the compiler into java now what we
can do is generate a piece of code we
can compile that it generates naturally
the class file or bytecode when we can
load the bike code in as a one-off so
there's obviously a little bit of time
the very first time you kick it off but
you're not going to change queries that
fast
because they're normally human written
queries that go into the census a few
fractions of a second to compile it we
loaded up we can do a cluster for name
and execute it this runs at native speed
this was superb so now we can basically
take these messages we can actually
dynamically sort them and filter them at
native performance give you a really
sort of brief overview of how the
compiler works so we do a basically a
tool provider we get the built-in
compiler there's plenty of code on
Google to find this out does you
basically override the file handle it
just to create yourself a little in
memory file handler you can create the
file anywhere you like so here we are
just using print lines for an output
stream pass that into the compiler
compiler takes quite a lot of parameters
just as it does on the command line you
remember that before we had maven an ant
and that awful Eclipse thing and
IntelliJ and all those sort of things we
used to come write it on by hand
basically this is all the same stuff so
we have the minus d2 where we deploy its
we have all the parameters and then this
Chuck's out all of the information and
all of these fields that it throws out
do you remember when you you had a
syntax error on the command line it
would put two little arrows underneath
to say this field is unrecognized all
that information comes out of here so
this is what the the Ides use internally
now and this basically just passes pass
or fail and it puts the output into
wherever I said minus D and it gives the
output and then when I'm running the
code I can basically do a class stop for
name the very first two lines there you
do at the top of your loop obviously
because you only need to do it once and
then from that point onwards you can now
access the the mechanism directly what I
did in this particular case this is just
a this was just the very early code that
i was trying it out on i just used a
little interface which i called an
accessor and then they should see from
the implementation of this when i create
the code the accessor basically is i
decide whether it's get duration gets
start time get n time get whatever I
want so I can have a double excess or
and in Texas or a string or byte array
accessory cetera so I can basically use
the 1i once
code here is the thing that's decides
which one I'm getting out of so we can
load this up we get native performance
now I demonstrated this to the guys in
spring because we do a quite a lot of
work with the guys in spring and they
said yes we've been looking for a case
to to actually do this we've been
thinking about it ourselves so Andy
Clemens who does the spring expression
language has now put this inside spring
as a prototype it will be coming out at
some point very soon again remembering
this is the sort of stuff that we're
doing before we get into coherence the
more we can do before we get in the less
we need to do inside these are the sorts
of performance he was getting these are
his slides for accessing properties we
went down from 300 and this is 45
million operations from 300 and 900 31
milliseconds down to without checking 29
milliseconds again some of the checking
can be down front that's question
so it's really noisy behind yep yeah so
two approaches one most of the most of
the tools at the moment so again spring
fuse camel mule etc use reflection
because you type in something it's
loaded in from your whatever tool you've
got to see the graphical interface or or
XML or something it's loaded up and it
uses reflection but it reflection is
very slow by slow when you when we get
into this sort of Rome so what the
spring guys have done and this this will
very quickly follow on with most of the
others I'm sure as the requirements come
up what they're doing is optionally
saying if this is hit multiple times
then we'll actually will take the hit
and we'll compile it so you can launch
off a separate thread you can compile it
and then from that point onwards you can
go to the compiled version so this I
have the live reason again I'll put you
in contact if you're interested with the
spring guys if that's what you use
obviously with mule as well and this
this I think will be the way things are
going because you can't now we've got a
performance bottleneck we can really
pass them unbelievably fast and now
we're stuck with the ESB s that just
don't process fast enough and again as
we're feeding that ultimately into your
coherence database data store what we
need to be able to do is obviously
triage that as much as possible so it
gives you some sort of idea again
invocation of methods from two seconds
down to about tenth of a second again
it's just sort of early early trials
again these are not my figures these are
handy Clements so what we're getting out
of this is the ability to do dynamic
rules an insertion from these massive
number of different sort of data stores
literally sub microseconds so rates of
one over a million a second why this is
important is because if and when things
fail in a bank I I was on a call this
morning as I was busy writing these
slides with a
well with some colleagues who are taking
data from the Chicago Mercantile
Exchange and they have FPGA cards which
are Hardware network cards which do the
timestamps and then merging this with
the tick data and they're synchronizing
them it takes them pretty much six to
eight hours to process this data now if
something goes wrong you might not need
this performance on a day to day basis
but when something goes wrong you really
don't want to be waiting five to six
hours to catch up because if it goes
wrong again a second time your windows
gone to be able to catch up before the
next day starts so this is why you need
this sort of performance you need to be
able to do this and the bottleneck is
continually moving down if you can
process massive files you can sort it
out and you can put it into coherence
the quicker it's in there the quicker
you can query the quicker it can be
distributed around the various other
different nodes that you've got so at
the end of the day where we're now
managing to process a lot of this data
and where we can actually reduce the
amount of information that goes into
coherence because we can filter out the
stuff that we don't need in there we can
actually create aggregates on the fly
much much faster we can put the
aggregates and the information and the
time windows and things into coherence
and we can use coherence to distribute
which is now what is now effectively a
very small fraction of the original data
we had in the first place we can filter
that out if you're taking literally
88,000 messages per seconds at a
continual rate and you want to put those
and you want to have at least two to
eight hours worth of data in a cache
that's a heck of a lot of data you're
going to need terabytes of cash and
that's terabytes of garbage collection
terabytes of distribution terabytes of
network traffic the less you put in
there the more efficient it's going to
be in the quicker you can get your
queries out so this is exactly the sort
of performance that we're getting out
I've left eight minutes that's a very
last thing we're working on here is the
distributed CEP distributed rules most
of the CEP engines and rules engines to
large companies recently acquired work
on single machines and they can only
really treat the data that they can see
at one time
we're getting now into big data big data
my definition of big data is something
that doesn't fit on one machine and so
now we can start to distribute that but
if we're looking at something putting
that on coherence or one of the other
implementations we can now start to
distribute those queries and execute
those queries in a distributed manner
knowing where the data is so that's
we're now looking at processing rules in
a distributed environment at the sort of
same rates again compiling those rules
and we're getting sort of pretty
phenomenal abilities to be able to do
queries on moving averages and things
like that when something pops so two two
moving averages change above certain
criteria then execute trades for example
so I'd like to thank you very much and
open up for questions
me too
say that
just the
yeah we're most of us majority the stuff
goes into into a MapReduce that's not
the only thing we have multiple layers
so effectively you've got all of the
data arriving from all of the cell
towers and then you have to tell you
that up because with most of this is in
Europe that we're dealing with so as you
walk around with your cell phone you
have nano towers and certain micro
towers and pico towers and all these
different sort of size towers and your
phone call moves from one to the next
and some of that information arrives out
of sequence at the central error so
you've got to tie those up and put them
into sequence so you have real-time data
where you maybe want to capture certain
where somebody is at a particular time
and that may be under a European law
that says if the police or security
services say we want to track this
number they don't use my numbers but we
call it a phone number for now they have
to within a certain number of seconds or
very small number of minutes find out
and report every single call that has
been or data and position that that's
being used then you have slightly less
important stuff where people are the the
telcos want to provide offers so if you
opt into four offers if you happen to
walk near a certain shopping mall or
something you get an offer saying the
new Apple Apple's got stock of something
rather in a store and so that they they
get money for that obviously from from
the vendors and then finally they want
to do stuff this is where they put it
into am a producer Hadoop system where
you might take lots and lots of data
over several days or sections of several
days over several weeks feed that into
Hadoop and start analyzing for example
what pages people are looking at as they
travel into the into work so people
getting the caltrain up what sites are
they reading if you can work out what
sites they're reading you can pre cache
the data at the cell tower and avoid
paying the expensive cost of from the
cell tower through to the server because
it's it's data on demand for example so
if you can pre cash all of the data from
from the TV companies and as the
caltrain moves up and down if the data
is there for you so it saves the tow car
lots of money
and all of this feeds in multiple layers
you have real-time stuff you have some
semi real-time and then you have sort of
archival but the archival stuff also
needs to be fed up as part of the rules
so you have rules that come from
multiple layers as well same in finance
you've got real time data actual sort of
trading data with your accounting and
then you've got settlement data at the
bottom and all that applies to the risk
of what the risk is at any particular
point to the value of any trade or any
bank which is becoming more and more
mandated as as the government's get into
more and more into the banks any more
questions total supposed to raise your
hand he raised his hand going yeah this
supposed because I was literally just
tinkering well wrote most of them this
morning actually they're uploaded to the
Java one site so there should be
available there's my email so just email
me or semi or 3 i'll send you a copy
with with pleasure as a PDF
so questions about the obvious
difference between moving but sort of
keeping up with the super high
performance and at the same time the
unbelievable values the high values yes
it is a problem but when typically when
you look at the banks will work with the
banks is the people in the front office
don't speak to the people in the back
office so it's very unlike or the middle
office they all treat each other as
aliens and they that doesn't tend to be
a huge amount of communication between
them so it's extremely unusual to find a
single system used across what you get
certainly is my previous roles as chief
architects in banks was coming up with
an architecture or a sort of a an
architectural global architecture that
could be reused but it's reused in
slightly different ways for each of
those areas it says why for example in
JP Morgan Chase we we created investment
bank markup language which is IBM L with
a very similar architecture from what
you've seen here and that's that's an
architecture that we could use because
we could do the job of binding the the
trading low high high frequency low
latency side we could reuse the same
architecture as we went through but you
don't find the same instances of kit or
processes being used across the
different areas the front office is
typically one building middle office is
another building and back office is
normally a couple of hundred miles away
into somewhere where no one wants to go
so as an architect you're looking for an
architectural pattern that can be reused
with slightly different performance
criteria sort of the in each case it
does it does pose a problem but it's
it's one of those things you have to
take into account and that's that's why
we used the the Java binding technology
because it coped with everything we
could throw at it from both the high end
so we can actually Java bind the fixed
messages and the front office and we can
java bind the Swift messages in the back
office the vast majority of the world's
Swift messages go through this system we
go through exactly this this product
the world's largest users of it use this
as do the central banks so it's it works
any more questions one will take these
two then we'll of other words sorry
garbage collection using this garbage
collection happens it's there's not
really much to say I mean any one of the
things we did find I mean the early days
this we design this around XML and Swift
and we didn't need the performance and
we had more than adequate performance
with the ability to do several thousand
per second per core we left plenty of
space where we're using a fraction of
the CPU power that we had in the
machines so there was never any great
issue with the garbage collection we
could run a relatively lean machine as
we started to get further and further up
towards the high performance that it
starts to create more and more problems
which is why from this year onwards
we've been looking at this we gone for
the complex data objects to what we've
called the simple data objects which are
these byte buffers and the byte buffers
they still create garbage but they
create a fraction of the garbage that we
have before so it's one of the main
criteria is avoidance of objects so
we're looking at every time we look at
the code and again it's code generator
what we do is we write the code we want
to see then we write the code generator
to generate the code that we want the
nice thing is as you improve the code
generator it works right across the
field for all of the code you're
generating so you get improvements
across the board it's one more question
so do have we had instability issues as
any software product yes biggest biggest
issues we had certainly in the high
performance cider when we get exceptions
because in multi-threaded environments
the exceptions tend to kill the thread
so we need to be very careful about
handling that and making sure we don't
throw again this is some of the changes
we've made to the new code generators to
avoid throwing exceptions you have to
take a lot of the checking out but that
comes at a cost obviously a lot of it
comes down to the architecture if you
can if you can check your messages as
and when you receive them and knack them
I send them back as you've got them it
causes many many less problems we did
have a statistic it was about 20 to 1
ratio or improvement in terms of errors
if you can validate your messages at the
boundary so if you think of messages
coming in if you validate at the
boundary and reject the boundary we had
a 20-fold improvement in in terms of
throughputs and sort of lack of errors
internally because an error that may
appear not to be a narrow such as a date
being put the wrong way around because
Americans have this habit of mixing up
the month and there the date it sort of
looks valid but it goes in and the dates
the wrong way around and it's valid as a
date syntactically but it's semantically
it's the wrong date because it's it's
it's just wrong so that causes problems
further on down when you calculate the
difference between two dates and it
becomes negative in your trade gets
reversed and you know that causes all
sorts of problems and try and try to
find the cause of that it's a headache
so validate on the outside and validate
with the with the rules and the
performance or sort of maintenance
reductions we're about 20 to 1 i'm going
to thank you very much if any questions
offers feel free to email me and i'll
gladly send you the slides up front
thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>