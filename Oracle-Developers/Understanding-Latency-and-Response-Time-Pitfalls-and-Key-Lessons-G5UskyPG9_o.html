<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Understanding Latency and Response Time: Pitfalls and Key Lessons | Coder Coacher - Coaching Coders</title><meta content="Understanding Latency and Response Time: Pitfalls and Key Lessons - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Understanding Latency and Response Time: Pitfalls and Key Lessons</b></h2><h5 class="post__date">2015-06-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/G5UskyPG9_o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi everyone we're going to talk about
latency and responsiveness or response
time I'm guilty am the CTO of Azul and
and I'm a lettuce latency addict the
other name for this talk is right here
and I know I know what I'm going to
raffle this off now so a while back i
started calling this the oh talk
laughingly because when i look at you
guys as i speak at some point during
this talk there will be some you know oh
we do that i have that and my numbers or
wrong effects and all that and I
predicted that will happen there's a
couple places in this and ever since
I've told people that I call this though
 talk there's been an interesting
phenomenon people have started saying oh
 in the middle of my top so we'll do
that yeah here's a ticket to the 22
tonight's party and yeah needs to be in
the right context maybe and we gotta
figure out that he's at the first the
most oh shits what do we do there my
favorite oh but just display long
but it needs to be like when they really
matter to you don't make them up right
and I'll figure out some fair way to
randomize this later the people who
actually say oh will come here
later we'll do our an actual raffle okay
because I don't know how to pick so if
you have heard about me follow me around
or something I i like to rant about
latency this is an example of my ranting
blog i tweet something and then I
explain in detail why I'm right but this
is the blog called latency tip of the
day but I do other things i'm the
co-founder and CTO vessel systems I've
been working on garbage collection for
over a decade and doing some interesting
things there this is evidence of me
working on garbage collection
this is an actual trash compactor I've
had one for many years it's job is to do
small garbage collections to delayed the
big full GC to the weekend so I can take
the bag out then here it wasn't working
right fragments were coming out the back
and I had to debug it so I took out my
trusty GC book and I thought that was
funny 10 years ago I really need a new
picture at azul we make JVMs and we make
JVMs that dramatically improve latency
we actually have to je viens once called
zing it's all about better metrics just
we've solved garbage collection pauses
are gone all that stuff is not something
you have to worry about anymore that's
what that's about we also make up javeon
called zulu and that's just a binary
distribution of OpenJDK I call that the
vanilla JVM it's you know format for
windows for linux it's free if you come
to us later and ask for enterprise
support will be happy to give you a good
price that kind of stuff so that's what
we do but in this realm of zing where we
set we basically sell a JVM that solves
a latency problem we measure latency all
the time right because that's how people
decide that this is good so as a result
we we learned a lot about what people do
wrong when they met your latency and how
hard it is to decide whether you want to
measure and and have to set up a good
experiment and stuff like that so over
the years I've looked at hundreds of
various latency logs and and and started
you know trying to coach people through
the process of what is it you're trying
to show how do we set up an experiment
how do we know if it worked so this talk
is really a result of a lot of that
experience it started off as you know
you're doing it wrong and it came into
more here's all kinds of ways people do
it wrong and here's some things you
could do right and here are some tools
that you could use so hopefully it'll be
constructive as well some philosophy
when we talk about latency and by the
way when I say latency I mean latency
your response time
or round trip or any of the things
measured from point A to point B what
are we asking so generally when we talk
about latency we care about behavior of
latency latency is the time it took
something to happen one thing to happen
but you hopefully do more than one
operation a day so each of your
operations even if they're doing exactly
the same thing has a separate amount of
time that took to happen and we actually
do need to measure again and again and
again and again and again to get a good
set of data and what we're actually
asking is how does latency behave across
all this how does it behave how do i
wanted to behave we're not asking how
long did that one operation take in
that's all I care about or here's the
list of billion operations I did today
go study that right we're trying to
establish some sort of statements of
behavior across them and we really care
about behavior or you're here hopefully
you care about behavior when we talk
about behavior saying something like the
common case where it's X the average was
something that's not a description of
behavior behavior is a lot more than
that so to give you an example of why
it's a lot more than that let's test a
little bit about what you guys care
about you're here so hopefully you care
about latency in your system or in
somebody else's system but let me ask
you this how many of you actually care
about the worst-case behavior in your
system and it's okay to say no that's a
valid state how many actually care about
your worst case okay good amount how
many of you know what the worst case
requirement for your system is it less
than the number of people that care
that's a good indication how about the
four 9s how many occur about the 49 is
not just the worst case okay some good
amount there now these seem like okay
extreme I need to know what the worst
that'll happen very unlikely things
happen but let me reverse that for a
second who here cares only about the
fastest operation that happened to
day and nothing else you just want to
know what the fastest was nothing else
matters there's some actual business
applications for that but that's very
rare so none of you right hmm well yeah
because you run run run and you publish
the best one right exactly benchmarks
good one how about how many of you only
care oh sorry I'm one one step ahead of
myself and talking I should see where my
slides are so very few of you Karen of
you care just about the fastest thing
how about the better half how many of
you only care about the good half and
don't care about the bad half anybody
here doesn't like you all want to know
is what the good half was I don't need
to know what the bad half is anybody
with that kind of need of nuh okay let
me point out that that is what the
median measures when you look at the
median number it is showing you what the
good half is and it says nothing
absolutely nothing about the other half
same could be said about the ninety
percent thing so the ninety percent
operations that meet requirements but I
don't care about the other ten percent
it's okay to fail ten percent of the
time it's usually pretty rare so when I
say I like to rent about latency this is
one of my rants that says you know
what's wrong with this picture
everything is wrong in this picture
let's zoom the picture out for a second
this is a pretty graph it's got a lot of
percentiles in it over time forget the
fact that the math behind it is wrong
that lines the 95th percentile where's
the bad five percent it's not here this
is the chart you so manager you show
management when you only want to show
them good results this is the feel-good
chart this is that I don't want to know
about bad thing so don't tell me that
they happen I know they could have
happened but I'd them that's how you do
it you do a chart that's pretty like
that so maybe you have a taste for where
I'm going with this
I think we all and I still do by the way
I've been talking about this for a
couple years and I still think that I
engage in latency wishful thinking
latency wishful thinking works something
like this we learn some math in school
and that math has those nice curves in
it and statistics and standard
deviations so wouldn't it be nice if the
world were studying for latency looked
like that so we could talk about it in
simple terms and this usually translates
into us measuring a few numbers like an
average a 90th percentile maybe a couple
other percentiles and say that from that
we can get a feel for what the rest
behaves like there's some shape to this
we can project and no we don't need all
the data it's a very common thing and we
tend our intuition goes there all the
time you have to fight it another way to
look at it is if I've measured and
covered ninety-nine percent of
everything how bad can really that one
percent be I mean yeah I understand it's
worse than what I measured but how much
worse it's not going to be that bad so
let's look at actual distribution from
actual systems to give you a feel for
what the problem with wishful thinking
looks like this is actually a set of
data from a trading system and the
y-axis here is a milli second so it's
very nice it's 50 micros of 70 micros
there it's nice and flat so how very
healthy system people have worked on a
lot and for the purpose of our
discussion it doesn't matter if that's
50 milliseconds or microseconds or half
a second or whatever that is that the
message is the same and from this nice
flat well engineered graph can we tell
what happens above the 99 in some
percentile because look you can project
that it's straight right well obviously
I'm baiting here and and if i zoom out
you'll see some bends to this things
happen at some point but that's not all
if i zoom out some more and more things
happen and in this specific chart that
high point up there is 50 millisecond
where this point is 15 micro second
that's a thousand to one ratio of
occurrence in this chart this is
naturally occurring latency behavior
so as you start looking at more and more
percentiles more and more things show up
if you don't look there you don't know
about them and I like to say that these
shapes tend to evolve to match
requirements whatever you state as your
requirements the ones you actually
measured I'm once you actually
experiment the ones you actually say
need to be better we'll end up shaping
this curve in this specific case these
were the requirements note that these
requirements ended at four nines so what
happens right after the four nines
things go really bad because nothing
said don't do that I find that if people
start looking at charts this way if you
show this to the CEO there'll be a
requirement right the only reason it
wasn't a requirement is nobody was
looking there I also find through a lot
of experience here that you can do some
archaeology in these charts as I said
they tend to be shaped our requirements
and from looking at this chart you can
tell that the benz tend to happen at
even number of nines I've noticed that
across a lot of data sets when corners
are shape changes happen they don't just
happen randomly they happen at some nice
number of nines in a mature system it's
a natural law what's not really natural
lights a result of humans working to
requirements see that point right there
at some point some engineer was watching
the 39 s number that's not an accident
they worked until it got to the three
nines now I told you that the wishful
thinking math doesn't work so let's take
this data set and look at actual stats
from it so this is an actual
distribution from an actual system these
are the actual means and actual standard
deviation computations off of those if
we apply our high school or early
college math but along with that number
we have a measurement here that at the
five nines we have a result that's a
hundred and eighty four standard
deviations away from the mean if you go
back to your basic statistics the five
nines are supposed to cover to
completely sit within four and a half
standard deviations from the mean if
you have anything near a normal
distribution and you could be somewhat
forgiving so maybe six eight I don't
know standard vision but that's a little
far right so what does that tell us does
it mean anything wrong is wrong here
that I do wrong math is this wrong data
or measurement answers yeah this is just
not a normal distribution that's all it
tells you so here's the flash news I've
been looking at charts like these for
years I've never seen one that's normal
I've yet to see a normal distribution
from any response time or any latency
chart coming out of any software system
they all have outliers that are many
standard deviations away from the mean
they always have them and you know if
you have one that doesn't that's a
perfect system but I've yet to see one
so the actual takeaway here and if you
have one takeaway for the whole session
is standard deviation is absolutely
meaningless as a metric for like 'no see
if you ever see it on a piece of paper
next to your results the first thing to
do is to erase it it's distracting it's
misleading at best okay I know that's a
big statement but you know unfortunately
that's how the world really works here's
another interesting way to look at the
wishful thinking part this is from a
trading system it's about 10 minutes
worth of data those red lines are the
worst case spikes in any one second
interval that green line at the bottom
that's the 99th percentile if I stop
measuring at the 99th percentile I
wouldn't see any of that red stuff which
is actually the important signal the
other interesting data point here is
that in this data set the average is
higher than the 99th percentile and that
actually happens in probably one out of
10 or 20 logs I look at that the average
is higher than the 90th percentile the
other thing is that this max is not a
little bit far from the average or
typical it's 30 thousand percent higher
I don't like to call these kind of
pictures jitter because calling this
jitter is like calling card
arrest the skip of the heartbeat yeah
you're skipping one heartbeat just for a
really really long time the ratios here
are the same okay so intuition could get
us in trouble and we need to do some
unlearning because we all tend to kind
of say average and spread and this looks
like jumbled together in a nice way
here's some basic intuition we all have
we all tend to think that load somehow
affects latency well load and latency do
have a tiny bit of a relationship
between them but load is probably not
the dominant factor in playing serie
behavior this is a chart that's 45 years
old from an IBM kick server document it
describes how we expect rich systems to
react in responsiveness to load but when
we look at these when we say response
time what response time are we measuring
is it the average the mean no one of
those percentiles in is load really a
driving factor in this behavior because
if I plotted each one of those I might
see a different curve so let's look at
something that's more common this is a
common output again i just want sampled
something on the web that serves my
agenda but this is a common load
generator type behavior where you have
load ramping up this is this light blue
line and spiky black line is response
time measured over that benchmark and I
don't know what this system does exactly
but I could tell that these things right
here are not correlated to load and
that's very normal to see in actual
systems the way you actually look at the
data we have points with high response
time at low load that are much higher in
response time than the response on it
lower load at higher load if we wanted
to find some correlation if we flattened
out or averaged out this graph we'd
probably find that the bottom of this
black follows the bottom of this blue
but the spikes are certainly not
something that is there this is very
common I call these hiccups
in hiccups are usually there in all
latency systems a few other measures if
you measure them long enough and they
could happen because of noise usually
they happen because of accumulated work
that you need to pay for for example
maybe you got the cpu for the last 10
milliseconds and now it's somebody
else's turn your fast for a while then
you have a blip in your fast for a while
or maybe you need to reindex the
database a flash a lot of or flush a log
file or garbage collect or power save
mode or whatever it is I don't know what
caused these spikes but a lot of times
you see them coming as a result of
accumulated work in or periodic events
that just happen these hiccups are
typically strongly multimodal that's a
natural behavior of all systems which
I've seen measured and they don't look
anything like a normal distribution they
just look alike a lot of different modes
and in of operation you have a good mode
that's typically your average your
median that's why you usually look
that's where your engineers spend their
time profiling to go faster but they're
usually some somewhat bad mode in
another terrible mode and how many modes
depend on your system that's how real
systems behave now windows happen not at
all you're great but I haven't seen one
of these if they happen you need to
understand how often and how big in
order to understand how latency behaves
then figure out if that's something
that's acceptable to you freshly the
most common way for us to deal with
hiccups is to do this let me teach you
how to do that we'll start by taking a
shovel and doing some math with it true
story happen more than once actually I
like to ask people what they know about
their data when we first talked to them
because I like to get good detailed logs
to analyze them because I suspect
they're all kinds of issues with them
now I don't like to accuse people of
that up front so I try to figure out
what data they have so commonly I'll ask
what do you know about your behavior
they'll talk about some percentiles or
averages or something and then I'll ask
them for some data they didn't give
like when they're done telling what data
they have asked for something else this
case I talked to a guy and he said I
have an average of this the max of this
a 95th percentile of that so I say do
you know some other percentiles what's
your 99th percentile and you say he says
wait a minute let me look this up I
don't have it but I'll get back to you
in a second e take puts the phone aside
comes back after 20 seconds is my 99th
percentile is X so at this point I'm
happy because usually to do this they
need a raw log of data with all the data
in it and I can mix go ask for that
because I want to analyze what that
looks like so I say so you probably have
a log and he says now I don't have a lot
so I asked him how did you figure out
what the 99th percentile is then anybody
wanna guess yep he had the average he
had the standard deviation and three
sigma from the average is where the
nine-year point seven percent Alex we
all know that's true right well that's
the wishful thinking part right so you
take perfectly good data with actual
hiccups and actual numbers you compute
two numbers from it an average and a
standard deviation you throw all the
rest of the data away and then you
answer all questions based on those two
numbers that's an unfortunate common
practice anybody recognize that yet so
this is always wrong it's always always
wrong I've never seen it to be right and
what you can do is actually look at the
behavior this is what I recommend people
try to do plot the entire thing don't
look for a number or two or three take
your entire latency set and plot it for
example like this for any percentile
there has to be a number and the
convenient think about this kind of
format with a percentile here and of
latency here is that you can also plant
requirements on there that yellow line
is a way of saying I have a requirement
I need the ninety nine point five
percent to be no bigger than 30
milliseconds I need everything else to
be no bigger than 100 milliseconds
that's an actual requirements from an
actual telco application and if you plot
that requirement you'll figure out that
you broke right here if the blue line
crosses the yellow line from the bottom
then that's why it broke now why is that
important other than knowing we failed
that picture really helps to focus work
in attention we don't have this picture
and somebody says we tried the test and
we failed give it to the engineers make
them make it better what do you think
they're going to do what are they going
to work on well nine times out of ten
they're going to work to improve this
point it's the easiest one to do let's
pull out the profiler let's make our
code faster let's figure out where we're
spending time and if we make that better
everything will get better unfortunately
this is already ten times better than it
has to be your problem is over here and
you most likely are looking at a
trade-off of making this better my
making this worse maybe I mean if you
can make both of them better great but
trade-offs are very valid ways of
meeting requirements so just by knowing
that you're failing here you can focus
your next two weeks of work on the right
problem and if you don't look at this if
all you said is hey you know I failed my
max filled my percentiles you don't know
where to look the other thing is that
systems like these if you start looking
at them have shapes that you start
recognizing cool things i'll use this
all over the slides over and over again
and you'll start seeing patterns perhaps
but this is a three-mode system a good
mode a bad mode and a worst mode the
transitions between modes tend to be
nice smooth humps because if you measure
it right you'll see all the numbers
between any two modes spread linearly on
a log chart linear looks like this so I
talked about requirements I keep talking
about requirements and in requires our
why we're measuring stuff to begin with
other than academic reasons we have no
real needs to measure stuff without
knowing what we're trying to achieve so
stating requirements is important and
you know these are obvious points but
I'll go through them one by one you need
requirements to describe how latency
should
behave saying I want it to be faster is
not a requirement I want it to be fast
it's not a requirement what does it need
to be and saying I want it to be 20
milliseconds it's not a requirement what
needs to be 20 milliseconds the average
the median the worst case it's very
different things so you need to
disfigure out what behavior you want and
you need to say you know how do you know
that you made it requirements are
pass/fail criteria there's a yes/no
answer there's no ninety percent answer
to these things and different
applications have different requirements
don't go saying oh my buddy here has
this requirement let's use his
requirements requires should be driven
by your actual business needs what you
actually are supposed to do hopefully
you can figure that out or somebody
could tell you that or you know product
managers customers contracts whatever it
is your measurements are supposed to be
designed to tell you whether or not you
meet requirements and if not where the
failures are or maybe if you are how
much Headroom do you have hey this is
very common sense very simple
common-sense steps but often people
don't follow them as a simple logic step
and they end up doing an experiment that
has nothing to do with their needs that
happens way too often so let's go
through how you could establish your
comments I I often like to conduct an
interview either with somebody else or
with myself in the mirror to establish
what the requirements are so let's go
through some example steps most commonly
I'll ask people what are your
requirements and they'll have something
like I need an average of X it's a good
start averages are kind of funny and
meaningless terms but I usually
translate average to common case or
typical because that's what people
usually mean so I'll say okay the
typical you want is that you want
typically to be nice and snappy to 20
milliseconds or whatever number you said
what's the worst case that's acceptable
to you and the most common answer to
that is we don't have one I don't know I
haven't thought about it we want it to
be fast
right any one of those so here's a
foolproof sequence that will get your
worst case requirement out of ending
buddy including yourself ask is it ok
for this system to freeze for five hours
in the middle of the business day I
guarantee you that unless they're
running Hadoop the answer is no way in
hell and at that point you do this trick
you say let me write this down cannot
freeze for more than five hours and at
this point somebody will correct you and
say that's not what I said I needed to
not freeze more than X right now a
simple warning here is this is typically
an overreaction right this is the I wish
everything was like this and you should
negotiate backwards it's you know are
you sure we're talking about something
they'll never happen what if that
happened only twice a day are you okay
with that what do you actually have is
the worst-case requirement where if that
happened that's just done enough I don't
want it right and at that point you'll
get some more reasonable number right so
we have two numbers now we have the 20
milliseconds typical and a two second
worst case scale that to whatever your
application is that's not enough yet
next question is ok I have these two how
often am I allowed to do one second how
often am I allowed to do half the worst
case at this point I usually have an
annoyed person on the other side and
they say didn't you just tell me this
won't happen at all and you know I told
you a different number they say look we
have requirements here there's a worst
case there's a common case under what we
wrote so far I'm allowed to do 1.9
seconds half the time if that's not ok
let's say what's okay and you know at
that point we'll start looking at
percentiles and other things you can at
that point they'll take the lead usually
unless this is you and you're talking to
yourself and you'll end up with a set of
steps a common case a worst-case and
some steps in between often people go
too many steps
this is actually the 50 and ninety
percent else here are redundant but you
need a common case whether that's 50 or
an ID I don't care because that's the
same thing you need a worst case because
otherwise it's okay to stop at five
hours and you need one other point
between them that has multiple nines in
it I highly recommend it has at least
three nines 29 is a joke are you okay
with one percent of our requests to your
web servers not coming back I'll
recommend you that each web page today
has a hundred such requests in it which
means every web page will experience
that right so you usually want three
nines or four 9s in that thing and once
you've established that you have at
least something to aim for another very
important thing is to remember that this
is over some given period of time we're
not you don't usually mean over the next
10 years including the year that I'm out
on vacation so this is over some amount
of time usually the worst amount of time
so it'll be something like over the
worst hour of the day all this has to be
met right or each one of these needs to
be met for any one hour period of the
day oh note that sometimes you fail
different ones at different hours you
need each hour to meet all of them or
each day whatever you decide on these so
this is what requirements need to sort
of evolved to what the numbers are where
the steps are or even if they're shaped
exactly like this is not the point the
point is to capture what you actually
need and to think none of the common
things but of what you would consider
breakage what would you consider
something you would invest more money in
fixing so that's how we talk about
latency a little in you know we talked
so far about only latency but lengthy
doesn't live in a vacuum this spectra I
talked about before Israel because there
is some relationship between load and
response time yes even at low load you
will have hiccups but you are pretty
much guaranteed that if you overflow a
system with work everything will be
really really bad okay so when we ask
about
when we look at a system and say how
much can you handle the actual question
should be how much can you handle while
meeting my requirements if you didn't
say requirements and you asked a
hardware vendor how much can this handle
they'll say look it can handle this much
so many widgets per second went through
my system I don't care for anybody
screams nobody said there's a
requirement for no screaming if you ask
the user still say that's where it went
from acceptable to unacceptable that's
where it broke if you ask a sysadmin or
somebody that's doing capacity
management will say I need to be here
because I don't want to be anywhere near
that other stuff and there's an actual
number you're trying to establish which
is the capacity carrying the carrying
capacity of the system before it breaks
how much can I handle sustainably
without breaking any requirement that I
stated so I define this term called
sustainable throughput that's the actual
number you want to establish if you are
measuring for capacity it's a
sustainable throughput is the throughput
you can achieve while everything still
works if you fail it's not interesting
to know how fast you go down a cliff
after you fell off the cliff now I could
go down at terminal velocity but it's
not safe this is not sustainable speed
this is an interesting experiment to do
with software because luckily was
software we can do that and it doesn't
hurt too much but taking the shiny car
for a joyride running it into a pole
going back to the lab getting another
car running that into the pole doing it
10 times measuring the shape of the
bumper differences between them is not a
useful way to spend your engineering
time and unfortunately that's what most
people do when I give you a nice shiny
toy you're going to go see how fast it
is right and then after you see how fast
it is you see well what's the latency
when it goes that fast I guarantee you
that it's going to look bad because your
saturated the system the actual
experiment you're trying to setup
is how fast can I you drive a red car
without hitting a pole and in
establishing that speed it is useful to
crash the car a couple times to
establish the limit but then you're
supposed to back off and play with other
things
maybe change the tires maybe train the
drivers see if you can do any better
most of your experiments are not
supposed to be about hitting poles most
of your experiments in actual systems
are not supposed to be about saturating
the system you're never going to run
like that in production it's not an
interesting behavior it's simply a way
of establishing a limit so don't do that
one other take away from this day do not
do that think of the red car in the poll
every time somebody tells you look at
how fast this goes and doesn't throttle
it and doesn't measure legacies so if
you do want to compare things here's a
good way of looking at how to compare
things multiple scenarios so this red
line is some sort of requirement set
latency percentiles and Layton sees
these other lines are different
scenarios maybe they're different loads
maybe they're different configurations
maybe they're different systems or
settings whatever they are and you can
compare the behavior of multiple things
to see which pass and which fail and
which does better and which doesnt it's
valid to say hey let's raise the bar and
make it easier it's valid to say look we
could do that good so let's change the
behavior requirements maybe but this is
a good way to compare scenarios and yes
some of these are red Falls that crash a
car I'm sorry red cards the crash
against the pole some of them are not
showing this in practice a little bit of
bragging rights here this is how we
compare zing are a cool product that
takes away garbage collection puzzles
with other products that have garbage
collection pauses you're looking at a
system that's a messaging system it's
supposed to deliver a certain number of
messages to a lot of endpoints how many
endpoints is actually a benchmark
fifteen thousand ten thousand etc in all
of these tests the system was able to
deliver that many messages to 15,000
endpoints but in some of the tests
people weren't happy that's a 15,000
endpoint test that's not happy that's a
10,000 endpoint it's that's not happy
that's a 5,000 here's a 1,000 it's happy
the difference being sustainable
throughput in this system
the red car hitting paul is 15 to 1
which is why you don't want to spend
that much time here now we do that
that's hot spot by the way we run the
exact same set of tests on zing and we
get four lines that you can't tell apart
down there 1k 5k 10k 15k and all of them
pass remember this system can handle
15,000 endpoints at that rate but can it
handle it within your requirements is
the key question and when we show wising
is good here we don't say we make the
system faster we do say this system will
be able to handle this much more load
while meeting your requirement in this
case 15 times as much load meeting your
requirements that's why I like to shape
things this way because if you are
trying to compare capacity carrying or
latency behaviors you need to look at
things comparatively like that okay so
so far I talked about some philosophy
and how standard deviation and stats
don't work well and hiccups and examples
and requirement settings and how not to
test things maybe how to look at here
come the bad news everything I said so
far is useless if the data you put into
these nice chart is crap and
unfortunately most of your data is crap
let's talk about how there's this
problem that I noticed a couple years
ago and I noticed it because we wasted
two two months of engineering chasing a
non-existent problem because of it and
ever since everywhere I look it pops up
so I call it an accidental conspiracy
none of you mean to do this but I want
to see who here actually thinks they do
here's how the problem happens a couple
of scenarios that cause it the first one
is load generators we build a load
generator to see how well our system can
handle stuff we set a set of
requirements let's say we did that right
and the low generator basically does a
scenario it sends a request measures the
time to response and some more requests
maybe I have one of those maybe have a
thousand of those in each one of those
clients we're running a scenario
and each request will happen after the
previous response came in and we waited
for however long we think waiting needs
to happen to run the scenario this works
perfectly well until or as long as each
request comes back before you're
supposed to send the next request out so
you were able to run the scenario that
you're supposed to run but what if one
of those requests doesn't come back in
time what if it takes longer than it
would have taken for me to decide to
send another thing most of these testers
are unable to send a next request may be
the first one was a log in the next
one's an operation you're inherently
stalled or maybe this is TCP so by
waiting and only sending the request
when you're ready you effectively ignore
a bunch of time when the system was
misbehaving and as a result you
basically coordinated with the system
and backed off and collected not enough
data or not the right data another
example of this in practice is in ma
during time within servers so it's very
common for us to see stuff like take a
timestamp before an operation taken
types of Africa operation record the
length of the operation and let me show
you this in code instead of text this is
a read operation in Cassandra and it's
taking a start time during operation
taking a stop time the end time and
taking that Delta and just logging in in
some statistics bucket that later will
show up on somebody's screen so as a
bucket will get averages and medians and
standard deviations unfortunately but
also percentiles and maxes and the rest
of them right so what can go wrong with
this I mean obviously we have to do that
to really bad things can go wrong here
the first one is imagine that that
operation stall for I don't know five
seconds during that five seconds nothing
else happened and we record one large
five second operation but we don't
record the fact that they are you know
5,000 operations that are waiting to
start and when they come in here we
think they're really fast I we don't
record
the fact that they're not in the timing
window but even more importantly this
specific case is a read-write database
that is often used in a right mostly
mode 95% rights 5% reads what if the
stall happens not during a read we won't
even see that one big result both of
those are examples of a mission there
are some bad things and we didn't record
them so you know how bad can this really
get right yeah I know we're dropping
some results but what can that due to my
numbers let's run some hypotheticals
here imagine a perfect system and I'm
going to test it with one client threads
this applies to any number of client
threads but I'm going to ask you two
hundred things a second and it's going
to perfectly respond in one milli second
each time it's got plenty of headroom
plenty of time everything is good I'll
take the system and I'll stall it for a
hundred seconds after a hundred seconds
so I weighed 100 seconds hit ctrl Z on a
keyboard count off 100 repeat now before
we actually measure this and talk about
stats let's talk about it as people do
what a fair description of the system
would be if we look at this we could say
that on the left side the average is 1
millisecond by definition we can also
say that on the right side the average
is 50 seconds why 50 because depending
on when I come in there it'll be
somewhere between 0 and 100 average is
50 that makes the overall average 25
seconds that's a fairly simple
straightforward way to look at it we can
also look at percentiles half of this
will be good and then it'll start
getting bad the 99.99 percent of this
thing is clearly terrible at close to
100 seconds these are the common sense
ways to describe the system probably a
fair way to describe it so let's go and
measure them with a low generator or
with a monitoring system like I said and
see what numbers will pop out of this
exact scenario on the Left i'm going to
have 10,000 measurements that show 1
milli second each on the right i'm going
to have one measurement that shows 100
seconds let's do math
average of this is 10.9 milliseconds not
25 seconds the percentiles come out like
this % the 99.99 percentile reported on
this behavior will show a 1 milli second
response time this is a system that
didn't run it all for half of the wall
clock time but that's out but it'll come
out of your tests nobody yeah okay hey
there you go so what happened here well
if we test 10,000 things on the left
we're supposed to test another 10,000
things on the right they're both the
same amount of time we're not supposed
to say are you ready for my test you
know I have a teenage daughter you know
if I go we want to see if a room is
clean I don't you know I studied the
spot check it right you know so the
10,000 results that were supposed to be
measured when we dropped all but one and
had we actually measured all those we
would have gotten exactly the numbers we
describe the system commonsensical e
right coordinated emission is the act of
erasing those results unintentionally or
intentionally they're being erased and
when you look in your latency logs it's
very easy to spot this go look at find
the largest latency in the log look at
the timestamp right after it or right
before it and if you have coordinate
admission there'll be a nice bad gap
probably the length of the lunch latency
and unfortunately this happens a lot a
lot so a common question I get from
people is ok so it's wrong and all the
numbers are wrong but I can still use
this as an engineer for intuitive
meaning right I'll test it and I'll see
if it got better or worse at least I
understand all the numbers of crap so
let's answer that one let's say I took
this system and I improved it
dramatically instead of stalling for a
hundred seconds it will answer every
question but it will take five
milliseconds instead of one to do it now
let's measure
that I have 10,000 results at 110
thousand results it's five that's what
the percentiles looked like so I spent
three weeks fixing the system and I
tested and it says whoa the 99.9
percentile got five times worse you did
the wrong thing throw it away go back
this is simply to demonstrate that
intuition is not going to come out of
these numbers either yep now when i said
that i discovered this accidentally out
of spending two months of engineering
time we spent it on that we were trying
to improve the average behavior of a
system that we eliminated outliers in
and everything was great except that the
average got a little bit worse 23
microseconds worse but this was a
latency sensitive application so we
spent two months looking at profilers
looking for stuff to improve finding all
kinds of stuff to move the average down
and only two months after that I thought
to ask for the original raw log file and
I saw the gaps in it and we've when we
filled in the gaps it turns out that
before we ever started we were actually
eight percent faster on the average we
just didn't know it because the other
system get the erase all the bad results
and we didn't if you erase all the bad
results you're only looking at the stats
of the good stuff that half of the good
stuff to ninety percent of the good
stuff whatever it is that makes you feel
good but doesn't tell you what reality
really is so this is bad you can try and
correct it and find out that you're
cheating twice so I won't get into that
here but you know things get really bad
with this and unfortunately I see it in
many many systems for example anybody
here use jmeter oh
because jmeter has this built-in that
over there is the output of jmeter on
the test I just showed you that's how it
would report that behavior go back do
the control Z test and see for yourself
the bottom is actually what a core
corrected output would look like we we
actually played with this a little bit
have it an intern work on a fix for
jmeter for some specific scenarios leave
and put it on github but the problem is
very much there anybody here use key
value stores for anything ma'am cash
days Jake ashes how do you know how much
you need how do you know what capacity
what do you stress this this stuff with
anybody use YCS be the Yahoo clouds
something or other benchmark that people
used to play heard memcache and J cash
and HBase and other things well this is
output from why CSP and I'm going to
show you some sanity checking tricks
that you could do easily in this output
this is a single thread run at a given
throughput they wanted about 50
something thousands per second and and
then you get some stats out of the run
it's a design bed park to see how what
your percentiles are under certain load
so in this specific run we can see that
there was a 26-second max latency this
is not accidental this is Cassandra
running a 20 gigabyte eat which tends to
have 26 second pauses every once in a
while maybe so there was one here and
that 26 seconds actually was one point
two nine percent of the total run time
of benchmark that's big enough that we
could do some sanity checking with it
because if I shave one percent one out
of that 1.29 I'm left with 0 point 29
and I could say that the 99th percent of
this system can't possibly be better
than point two nine percent of the wall
clock time which turns out to be 5.9
seconds but the benchmark is reporting a
5 milliseconds 99 percentile since
thousand x off of reality if you use
this to capacity plan your memcache d
or your Cassandra or anything else it
will show it can do that many but don't
believe the percent of right you're
living with much much worse percentiles
than that some more examples from the
real world this is an actual system
measured before and after correction
this is the first chart i'm showing you
where you can see the shape of
coordinated emission with your eye
whenever you see those vertical sharp
rises like the top chart that's not
guaranteed to be coordinator mission but
ninety-nine percent chance at it that's
a joke but I usually usually that's a
cause the cases where you can see a
vertical rise is when the system
actually has you know sometimes it
answers fast sometimes his answers slow
because that's the length of the
operation so there will be two levels
but if it's the same operation you will
always you're supposed to always see
that nice smooth slew the bottom is a
correction of a this is a jmeter output
the bottom is a corrected jmeter long
one of the tools I talked about lets you
correct those logs without writing any
code j hiccup has a secret mode but you
can feed a log file through and it'll
out put another log fell out and all it
will do is correct for a chord in a
mission and that's what that output
looks like these guys had a requirement
for better than one second 99.9
percentile and they were passing it in
the lab and failing it in reality and
this is you know just seeing that the
lab was lying so it was wrong by seven x
this last one is a Wall Street example
it's a really really cool high frequency
trading up nine micro second tipple
cooperation across the measured thing
and this was the shape you see the
coordinate admission pretty clearly here
but we had an interesting problem here a
Singh is really good at eliminating that
kind of outlier but we had a hard time
explaining to the bean counters why you
would want to spend money to fix your 89
problem so I actually flew to New York
and sat down with these guys and help
them put instrumentation in their code
and when we took the uncorrect and
actually ran the right correction on it
we found out that that's the actual
shape that's a thing was here the other
JVM was there and this is a 2500 x error
in reporting that would cost this
business if people did it all the time
which is why I go around preaching right
but the correction here was an
interesting correction e because the
code looked a lot like the cassandra
code measure in the process from A to B
and putting in the correction was an
interesting challenge I actually built a
tool as a result of that so what do we
what can we learn from this the first
one is whatever your test system is test
the test system don't believe them don't
take the output and show it to your boss
you'll get embarrassed later there's eat
there are easy ways to test the test
system the easiest way is the sanity
check test I just showed you control Z
is very easy to do if you take a system
and freeze it for some amount of time
you know happen and then you're whatever
a reporting system acts like that didn't
happen or the percentiles aren't right
you don't don't believe anything else it
says basically right just don't waste
your time looking at stuff that already
lied to you don't ever look use or
derive from standard deviation okay
hopefully you got that one always
measure the max times this is the one
value that's usually hard to hide from
it is possible to hide from I showed you
one case that it does but it's harder
like from an external measurement system
it's really hard to not see the max it
is easy to say I saw the max but I won't
look when it's happening but the max is
a really good sanity checking I showed
you the case worth just from the max
value of a bench market I could tell you
the problem is there without looking at
the rest of the data because it's
impossible for that max time in that
percentile to exist in the same reality
and measure percentiles measure lots and
lots of percent else so you know I used
to talk up to here and then just say
sorry go home and
it's sad but I've built a bunch of
things since then and that's what I'll
talk about next first one is something i
call HD our histogram HD our histogram
is the tool that lets you plot these
kind of charts it's not a tool it's a
library but it lets you collect data
that you can use to plot this view
because this view is very useful for
understanding how latency behaves
without it you're blind so what does it
do why do I need it why did I have to
build it turns out to see this kind of
view you need both a good dynamic range
and a good resolution at the same time
why good dynamic range because I don't
know what your results are going to be
there might be seconds or microseconds
or milliseconds and they might span a
wide range why good resolution if i use
a logarithmic histogram this bottom this
top quarter of the chart would be one
pixel and it would be kind of hard to
see what the shapes are there so HDR
Instagram is simple a simple thing it's
by the way this is up on github it
basically lets you do a dynamic that
cover a configurable range of time with
configurable resolution for example I
could say I want to cover all the values
between a microsecond in an hour and I'd
like to do that with three decimal
points of resolution you make a
histogram and then you just dump a bunch
of values into it it's a fixed size data
structure that works pretty fast it has
built-in optional compensation for
Gordon intermission if you just record
the values they'll be there but you
could say I want to record this value
and the expected interval between
measurements was X so if it's bigger
than that autofill all that linear thing
that was missing my tools use that
option a lot it's open source it's
actually public domain on github and
it's become pretty popular in the last
couple years it's fast it was built with
low latency people in mind so it's got
fixed cost both in space and time once
you make it it creates a data structure
of fixed size that never grows you can
record a trillion results into it it
doesn't need to grow it's just counting
things a nice interesting
we laid out buckets that don't change
it's also really fast on this laptop I
can record 200 million results into a
histogram per second that is faster than
I can measure time on this laptop so if
you can measure time you can afford to
actually store that result in a
histogram internals kind of look like a
floating point number that's really
really big it's got a mantissa part a an
exponent part it's just how it deals
with both resolution and dynamic range
it's a new type of data structure now it
has nice tools for iteration you can
iterate as if it was linear and whiff is
like a logarithmic or it's like by
percentile so you can get output that
looks like this with all these intervals
and that output is very useful for
plotting these charts at on github
there's this excel sheet with this data
in it and this excel sheet drop plots
this kind of relationship which is not
trivial well I mean it's easy to compute
but you need to know about it which is
why the output firm HDR histogram has
this peak precomputed column over here
which feeds straight into things that
want to chart it so this is a good way
to measure stuff report it and visualize
it that's the y-axis and x-axis now sgr
histogram has a nice logging feature I
added a few months ago you can basically
take Instagram that you're recording
really fast into and record intervals of
histograms losslessly record histograms
not take the 99 percent of every second
and later try to average that that math
doesn't work okay but if you keep the
entire histogram with it counts you can
later add up histograms each compressed
histogram tends to be a few hundred
bytes fact it's usually below 200 fights
now / interval which means they're very
very small in size and you can take this
log of lots of intervals and go back and
say yesterday between eight and nine am
I want to look at the distribution of
latency and you just take all those
intervals add them up as histograms and
look at that put it's very good for very
useful for looking at long-term
logs and behavior and looking back in
time there's a bunch of tools using it J
hiccup uses it that's a tool i built but
some monitoring tools people have built
are doing it they're currently ports to
c c sharp and to go that i didn't build
so that means some other people are
using it for stuff I think it's really
cool that code a hail the guy who built
the metrics packages that guy reported
it to go so that's HDR histogram it's
just a class and you can use it in your
java class for stuff latency utils is
something built on that that was built
to deal with this problem remember the
problem of recording time if you record
time within a process and a freeze
happen somewhere in the process
recordings may not reflect it or may
reflect it with dramatic Gordon
intermission so H sorry latency stats is
built to basically give you a bucket
that corrects for that without you
needing to do anything about it and the
way it does it is magic the actual
principle is since we get all this
timing recordings we can actually
establish the expected interval inside
the bucket looking at the recent rate
and you can time cap that into a moving
window with a time cap there's an
underlying positive factor in the
process that says if I notice the
process froze for everything I can tell
things about it for example to detect GC
pauses or controls ease or anything else
it's an observational positive factor so
you could say you know basically if
multiple threads didn't see any movement
that was supposed to happen at the same
time and you have enough consensus it'll
say if it's a pause and if it detects
deposit you tell all the buckets that
are recording latency that there was a
positive this length so they can fill in
the missing gap with whatever interval
right they were seeing and altogether
that basically gives you a way to see
both the wrong recording and the
corrected recording for any of those
that is really built to replace what I
had to fly to New York and spend a day
with people for
right so that the scenario i showed you
actually have to do that by hand i went
and wrote this afterwards and now people
can just drop this in and get the
results the right way now the last tool
running a little over time here but is
there a talk after us by the way in this
room okay so i'll run a little longer if
guys if you don't mind but feel free
otherwise j hiccup is the last tool in
the sequence jacob gives me pictures
like this that's actually an excel sheet
that comes out with Jacob and Jacob is a
pretty simple tool it's it's very easy
and very simple and silly almost it's a
java agent that adds a thread to your
application whatever your application
does and that thread goes to sleep for a
millisecond when it wakes up it says how
long has it been and if it's more than a
millisecond and it was as well i came in
late that's a hiccup I was supposed to
run and I didn't let's record that these
are basically the plots of the hiccups
observed both in percentiles in overtime
and i like to use that format obviously
but as a tool all this does is record
the fact that your process experienced
hiccups it doesn't know why now you
could do that by adding an agent you can
actually connect to a running process
from the side as an agent and as a
background operation it's extremely
cheap again this is open source in
public domain now we use this a lot both
for testing and for production time
logging because you can go back to
yesterday and look at the hiccups of
your system so if somebody says there
was something weird yesterday you could
figure out if there was a hiccup
recorded with it and it's very useful to
triage with this you can actually run a
control hike up that runs with an idle
load so it could see if there was some
hiccup that the whole system saw you can
at the same time run a hiccup on your
jvm if somebody says there was a problem
in both of these had a hiccup then it's
the system you don't have to look at
your code or the JVM you know something
happened to the system but if the JVM
had a hiccup in the system didn't well
go look in the GBM and if both of those
take up logs are perfectly happy and you
have a lot of a long-latency you know
your GV m in your system we're running
fine go look elsewhere look at your code
look at the network look at somebody
else just the triage a this is very
useful now examples of it in use are
here for example this is a I often pull
it out and when people say I want to
start a test and I want to see no worse
than 5 millisecond result on there I
asked them to give me a Jacob log of
what they think is an idol system
running for two hours and often I'll get
this picture that has 20 millisecond
blips on their Idol system at which
point I say we don't need to do the test
I'm going to fail the test your system
has a problem let's fix the fix the
system first anybody want to guess what
the most common cause of a blip that
looks like that is ok it's crontab
somebody has a job that runs 20 minutes
after the hour runs a few threads for 30
milliseconds nobody saw that top didn't
record anything but you'll see it here
right on the right you can see that a
clean system can be had after you've
taken out the bad things and Linux could
run so that glitches aren't bigger than
about half a millisecond that's a good
system to start testing on now it
depends on what you're testing here if
you're talking about a human response
time app both of these are good but if
you're looking at a latency sensitive
app you just have a bad idle system
there there's no point in trying to beat
that you can also use it to just compare
behavior of collectors this is CMS sorry
how parallel GC vs g one running the
same one gigabyte eh cash and the same
thing just look at the patterns for some
interesting stuff you can use it to
correct log files remember I told you it
has a nice secret mode for doing that
this is taking a log file with
coordinate emission and fixing it to
actually give you the right behavior so
it's useful for a bunch of things but I
built this for a reason anybody you can
you guys read what Charlie Nutter said
there it's an honest description of why
Jacob exists
so let me demonstrate that on the Left
we have hot spot with CMS running in
each cash and all right we have zing
running exactly the same thing can you
see how much more beautiful zing is well
at this distance you might not quite see
it because they both have spikes and
they both have curves let me highlight
the scale and then we normalize a scale
and at this point let me say that being
a thousand times better is hard to show
in 500 pixels okay now this is you know
human response time tine of bad stuff
low latency guys often say I don't have
that problem so let's show that in their
scale here's a 20 millisecond glitching
up here is the same kind of trick in the
same kind of thing with the messaging up
same so so if I do one thing to show why
is good what you know why use our
products I use this all the time just to
show before after picture and it's easy
to show to a bean-counter it's easy to
show to CEO it's easy to explain you can
do this with your code and to show why
you did something good you know as well
so takeaways for the talk standard
deviation bad if you have if you haven't
yet decided what your percentiles and
Max are and stated them you don't have a
set of requirements you're probably
doing premature testing and that's often
a waste of time measuring throughput
without latency requirements is a waste
of time it's not interesting to repeat a
test running a red car into a pole the
same is by the way true backwards
latency measuring latency without
thinking of what throughput you want to
test it out is also kind of a waste of
time I guarantee you that if you do one
operations per day everything's going to
have a nice latency behavior that day
okay but you want to know at what load
you're supposed to actually carry and
meet your requirements mistakes that
seem tiny and irrelevant in measurement
technique can completely change the
meaning of your graphs and they could
lead to very bad business decisions
meaning I will deploy the wrong number
of servers for black friday
and I will miss black friday that's bad
business decision right j hiccup HDR
histogram latency utils or cool tools
and zing is also a cool JVM so that your
takeaways for the day since we don't
have anybody after us i'm happy to take
questions at this point questions yeah
yeah yeah
right so I'll repeat the question so my
mic will amplify it the question as I
understand it is we talked a lot about
the max but how do you define the max if
you know how do you know you've actually
tested it off you only tested a hundred
million things you only have so many
nines I'm it when you look at this from
a requirements point of view you're
basically supposed to say I need to meet
this thing with in this amount of time
and it's easy to answer whether or not
you did I'm not talking about the
theoretical worst case I'm talking about
the observed maximum in whatever tests
you did and whether it's a million
operations or a billion operations it's
there with these tools you can get
precise number of nines forever many
things we want you can't get more than
six nines out of a million things but
you can't get six nines out of a million
things right so yeah I mean the max hat
is basically the maximum I saw and I'm
talking about not designing to the
parameters obviously you could try to
design to them but simply measuring
whether or not they happen and that's an
easy thing it's a pass fail question
right yeah so
so I have some correction techniques
recorded intermission right tools i have
like HDR histograms the net utils make
certain Corrections there are other ways
to correct for them as well the key
thing is to understand the problem
exists and then figure out the best
correction is to not have it happen for
example build in asynchronous slow
generator and you won't be missing
results but if you can't avoid that for
example if you use TCP you cannot avoid
back pressure then you now have a
situation where you know that there are
dead times in the system that it behaves
badly because that's usually where the
ten-time is and you weren't measuring at
that point you pretty much have to
project reality into the gap to project
the nearby reality make a best guess
think about it this way um seeing that
didn't happen is the worst possible way
to report right that's truly hiding your
head in the sand projecting bad things
into it the worse you project into it
the clothes the more the better your
report is you could say hey I don't know
what happened there so the worst thing
happened all the time it's finally a
little going overboard in most of these
cases you could say hey this is a very
clear gap there was a part of the system
froze what am I missing I'm missing five
thousand results how big would they have
been had I asked the question while they
wouldn't all have been the max right at
each point there would have been
different so in HDR histogram if you
give me an expected interval I just fill
in the gap from your value down to 2x
the interval and stuff anything below 2
X 2 interval won't have any corrections
the other ones will just get a nice
linear drop it turns out that the linear
drop is not you don't need distribution
or modeling within that because all
we're doing is doing counts like you
know all these would have happened at
that interval you know whether they're
spread linearly or not doesn't actually
affect the results so it's a pretty easy
correction again the best correction is
to not have to correct don't miss your
request and you won't have to correct
for them yeah
so it knows I didn't talk about how to
build systems I'm just talking about how
you measure them to answer to what you
just asked is if you have a magic system
that reacts to the fact that something
went wrong and works around it measure
from outside of that I don't care what
the components are doing if you can work
around a failure show that your work the
run to fail the external e measured
behavior is supposed to be done right
now if the external measured behavior
says oh look there was a pause it went
away for 10 seconds the next request go
the right way but you kind of missed the
fact that it took three seconds to flip
over and you didn't do anything for the
three seconds you're not measuring it
right and if you do measure it right
then you'll get the right result I did
there's certainly a lot of systems out
there that have a nice faltaron
capability of routing around things my
favorite designs are these you know
monett honest kind of item Putin
consensus-based flowing forward things
where hey some of this stuff could be
lost and used to get and still get the
right result out the other side you have
five things computing whoever pops up
first it's right and those are great
because they survive glitches and
components but then you do that on a TCP
connection and you know Blum you have ax
or something that God's glitches and you
can't get away from that
you make less infectious
yeah so I have a lot of experience
looking at what JVMs behave like okay I
have looked at a lot of systems are not
JPM's to but I spend most of the time on
JVMs and obviously I have my pink
glasses on and to me all the problems
look like garbage collection right
because that's what I have a garbage
collection gun but but there are lots of
things that make jvms pause and lots of
things that make system spots there's a
talk by john from azul systems tomorrow
the title of which is with GC gone what
else makes the jvm part and we any list
17 things right and guess what not just
the jvm pauses Linux likes to pause
transplant huge pages is a beautiful
feature that will pause your system for
800 milliseconds every once in a while
turn it off if you still have swapping
on don't come complaining to the guys
about the glitches on the JVM right and
there are several other settings that
are best turned off before you start
measuring it that the reason i do that
20 milliseconds you know that that
sanity check give me a j hiccup blog for
your idle system for two hours it's
because most of the time they come back
failed and we just saved ourselves a
week of working on something that
wouldn't if worked right so i like to
use those tools for sanity check I have
a solution for garbage connection
glitches to demonstrate that solution I
need people to measure them so I've
built ways to measure them but this is
useful for people looking and network
glitches at C++ applications at file
systems this is just about latency and
responsiveness and how to look at it
happens to be a Java one so we can talk
about GC pauses yeah
we're from the surface it might seem
that
always always very few people i know can
actually have a lab that demonstrates
reality and in fact there are lots and
lots of things they're hard to model
like you know most people in low
jenner's will model load is a constant
arrival rate and unfortunately the world
doesn't behave like that the world has
some other distribution I haven't spent
enough time looking at what that looks
like to tell you what it really looks
like but I'm pretty sure that it's not
arriving at a constant rate in trading
you often have spikes where 500 things
happen at exactly the same time not some
was sown in distribution or like it just
happens and if 500 things can't happen
at the same time 499 of them are going
to wait in line and you'll see keeling
related latency and they should show up
in your stuff an interesting thing I'm
actually working on another talk where
I'm looking at the various core reasons
latency exists and looking for
signatures oh this one looks like
queuing and this one looks like GC and
this one looks like constant wire length
you know and you can tell because they
have different shapes I'm not sure you
know how well you can tell but I can
tell you that I've been looking at those
kind of shapes a lot and my eye spots
things I can recognize VMware in a J
hiccup chart it has a signature has a
perfect line at 50 milliseconds of
glitches where they exactly in line now
other glitches will be over the place
but there's this line at 50 and that's
the quantum for the vmware scheduler and
every VMware system that's slightly
oversubscribed will have that exact
signature in it now I only figure that
out after seeing a lot of them so the
first thing to do is you know start
plotting look at the chart you know use
that as a way to visualize it's it
really helps
oh I couldn't hear you well yeah we make
most of our money because JVMs cause a
lot of this right and we have one that
has is less of it it so garbage
collection tends to be a very dominant
pick up generator it's back the two
tends to be bigger than anything else in
a regular hardware software system so
when you have a GC pause it's usually
bigger than scheduling bigger than power
savings bigger and lots of stuff I mean
it not a lot of things freeze your
server for three seconds you know it
just doesn't happen that much I have
seen a power-saving setting that did
that but you know and if it's happening
periodically a higher with load hey
that's what it is and we like to remove
that and sell people product space than
that but you know other things also
cause glitches and their magnitude your
sperrys yeah
that was me this morning yeah yeah I
think that if you kept all your Java
heap sizes to 640k everything would be
okay unfortunately we need more than
that yes the size of the GC pause will
grow with the size of the heap in JVMs
that pause in je viens the don't pause
you can use any amount of memory you
want it's one of our big selling points
whether you use it to hold more things
or to run faster because memory is a
great way to not do work or or just to
buy time between whatever work you do
memory is very useful and today's
servers come with hundreds of gigabytes
of memory that's not an exaggeration a
244 gigabyte server and amazon costs two
dollars and eighty cents an hour that's
less that's about a penny an hour per
gigabyte so when people start talking
about you know fitting in small heaps I
make fun of them afterwards there you
save three cents good for you yeah yeah
but yeah in reality look if you can't
use zing for whatever reason then you're
left with having to work around the
limitations which means either live with
the pauses you remember you can just say
hey let's move the requirements to where
we meet them fine or shrink your heap or
don't grow it or do really cool
engineering I've seen people do exactly
those things where every x is GC pause
they route around it right most caching
engines do this where they cluster and
you set an SLA and then if a GC pause
happens it drops out and then it rejoins
and it resync beautiful beautiful stuff
that completely isn't needed of G's
he actually worked but you know it's a
way of breaking things work yeah
all right are you
so
free and just
so whether or not the other results
arrive depend a lot on the load and the
generators so for example jmeter when
used in a normal scenario typically
won't generate missing results later but
if you use a constant food throughput
controller then it will its job is to
make sure that this many happened for a
minute per second I'm actually showing
the effect i call it cheating twice why
CSB does this so why CSP actually does a
Constance we put test and it's got this
the cut is very simple in it this it has
a loop that says what time is it how
many times how many requests should have
sent up to this time based on the
throughput am i behind if the answer is
yes and the request repeat and if not
you go to sleep so
yep
so production is tough because we never
know what the traffic is so the best we
can do is just project into the dead
times in a constant load generator the
effect that you have from arriving the
things arriving late is that instead of
recording them bad you record them good
the same problem exists for production
so you froze remember the Cassandra code
you froze 10,000 things came at you but
you didn't start the clock on them you
finished your fries look 10,000 hot
things and when they're hot they
actually get processed in shorter time
than when they arrived one at a time and
you record 10,000 beautiful results for
things that were in a queue right so
that's an example of not correcting
enough right because because you still
have the line twice right that's a tough
one I'm not claiming correction is
perfect and saying it's better than
nothing the ideal would be to know to
actually track things you know time step
when they truly arrived and when
unfortunately you want to know when they
truly were meant to be sent so as long
as you're talking to clients with non
blocking protocols you you know you'll
see when they come and arrive in your
cues but if you talk to anybody through
TCP even if you measure on the wire if
you didn't do work there's back pressure
and they didn't send requests now you're
going to say hey nobody asked me
anything and they're going to say I
didn't ask you anything because you
weren't answering anything you know
they're right right so your measurement
system generally in production can't
tell if quiet happen because there was
no work and later there was a spike or
if people were waiting you can try and
build when you know more about your
system you can try and build things that
will guess better at that for example
you know there was a glitch and there's
a correlated spike maybe we could do
something about that but you're running
blind you lost information
at that point
it's so let me use this as a as an
example and talk about it in context may
notice that in in here I had two
separate lines this blue and green line
that actually represented two levels of
conservatism of Correction both both of
them were under correcting when I go in
and I corrects if I tend to act and the
conservative side of Correction because
of credibility I could say look we know
that at least this thing happened it was
at least this bad but the reality is if
you're reporting to yourself to make
decisions you want overcorrection not
under correction but not somebody says
my 99th percentile is X you don't hear
that is my 99th percentile is it you
know it's at least as bad as X right you
actually hear that is my 99 percent are
at least as good as X right so if
there's something to correct you want to
overcorrect when you're not sure so that
that statement would be true and you can
make an actual safe decision based on it
that's an argument for overcorrecting
the utils is its Java code so you can
tweak it quite easily the hard question
is over correct by what I'm not a great
statistician that's why I believe this
stuff yeah um right so um so the HDR
histogram is very simple code you give
it data it records it and if you tell it
to correct with an interval it'll do the
linear thing if you want to overcorrect
give it half the interval they don't put
twice the number of points you can
decide to to overcorrect the problem
with doing that is you know knowing how
far and i don't have a great answer for
how far latency utils has actually got a
lot of nice pluggable things in it so
for example it has an interval estimator
but you can plug your own in the default
is look at the last thousand things and
cap
I think either one or two 10 seconds so
no longer than the last 10 seconds last
thousand things but you can put in your
own thing for example if you actually
know what the arrival rate put it in
similarly the positive textures are
pluggable the current paws detector is a
consensus-based you know it basically
puts at least three threads to sleep
over need to see no movement in order
for them to believe that there was a
positive anybody moved that wasn't a
pause but you could decide to say hey I
have a GC log parser and every time the
GC log there was a pause said it was a
positive just say there was a pause the
reason your detector could be anything
you want so for example right now the
only things I can detect in latency
utils are observable process white
pauses but if you're running a database
and you know the database pause based on
indication is your software and it
didn't pause the JVM it had some other
glitch you can make a positive effect
that will tell these things to correct
for that right I don't have magic for
everything fortunately and there is the
key problem here is we have missing data
and the missing data is all bad as a
result what we do is if we if we do the
regular thing we basically report the
percentiles of good stuff and the
percent of the good stuff is meaningless
information 99% of stuff that was good
as good that's I showed you the you know
system was freezing and yeah yeah I
don't have the here's how to solve it
and the problem will go away thing for
you unfortunately this this as i said
this used to be kind of the skys falling
I'm just telling you about your problems
and I put a few tools that help a little
right there's guys still falling okay
I'm gonna get off the Mike and wrap up</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>