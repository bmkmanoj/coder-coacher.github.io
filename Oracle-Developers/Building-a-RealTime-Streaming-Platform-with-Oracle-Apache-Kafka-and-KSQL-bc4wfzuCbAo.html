<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Building a Real-Time Streaming Platform with Oracle, Apache Kafka, and KSQL | Coder Coacher - Coaching Coders</title><meta content="Building a Real-Time Streaming Platform with Oracle, Apache Kafka, and KSQL - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Building a Real-Time Streaming Platform with Oracle, Apache Kafka, and KSQL</b></h2><h5 class="post__date">2018-02-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/bc4wfzuCbAo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I ordered list of rows event so one of
them happened another after that
and another after that and the question
of when do I need to collect and store
them and process them and do other
things with terms of events the idea is
that if you can actually model a lot of
things maybe even every sink as a stream
of event and by getting them into one
place in on one platform you can start
building applications that react to
those events and do stuff with those
events and this makes your business more
agile in different ways and this is
basically what my entire talk is trying
to substantiate this claims that you can
actually model a lot of data is streams
of events that it makes sense
and that it brings practical benefits to
the business but you may say I don't
actually have those dreams of events and
maybe Oracle DBA or traditional data
architect and I have is a bunch of
tables don't worry
we do have you covered we do think that
you may not know it but you actually do
have stream of events all we have to do
is uncover them so let's look at some
examples of those events and see how you
may already have a stream of those
events so when most people think about
events they think about it as something
that the application reported usually in
JSON format because this became really
really popular in a lot of applications
these days and something like you have a
website and you're selling stuff in a
virtual store and someone came in and
decided to view a product and he clicked
on quick view of a product and this
application reported an event someone at
this time this user viewed this product
from this page fantastic
I have an event a collection of those
from all the users across all my
products can be seen as one stream of
events other sources of events maybe
sensors and my favorite example is maybe
connected car where you're driving
around and the car reports things about
maybe where you're but more likely how
your battery is doing a lot of fad or
most connected
our battery electric cars battery driven
and optimizing the battery is the main
competitive advantage of the companies
building these cars they want to collect
a lot of information on how the battery
behaves under different conditions
especially temperature so your car will
be constantly collecting what is the
temperature and how fast the battery is
draining something like that and you'll
have this stream of events getting
reported over usually a cellular network
continuously from all those cars
obviously there is a lot of examples
sensors are now everywhere if you have a
home security system it has a lot of
sensors a lot of fare your home and no
a/c heating systems I'm sure there is a
good term for that in proper English
they will report streams of events I
heard some people's teakettles report
streams of events all the kind of things
happen so tea kettles that tweet are
absolutely streams of events and then
another very traditional source of
streams of events is your application
log applications always write stuff to
notify all activities sometimes errors
if you want to go and find easy source
of events in your organization go and
look at someone's application look at
the log files and you can see this is a
stream of events right someone access
this page someone access this page
certain access this page one event one
thing that happened after another is a
traditional stream of events what you
may not expect as a stream of event is a
database because as we said we have a
database but how do we actually is it a
stream of events and so that's what I
want to show you now you think that the
database may be a static table but there
is a whole other way to look at it so
how many of you our Oracle DBA have been
through Oracle DBA training ok good
server it's funny like those events
collect people from you know some
developers some architects some people
who know Java some people who are really
into node.js it's really hard to find
the common ground and it's worse for the
joke like I used to know that if I go to
DBA event you make fun of Java
developers and storage means
if you go to a java event you really
want to make fun on your students it
means that you have this thing on your
mind right and I I'm totally at loss
here so I guess the only thing I can
safely make fun of is myself or expect
self-deprecating jokes anyway so those
of you who don't know how Oracle works
underneath is that when you do all those
updates and to work hell you sink you're
updating a table and you are uplifting a
table to memory and at some point the
table will actually get persistent to
disk so if you stop Oracle and start you
will have the same table again but what
happens in reality is that not every
update immediately gets persisted to a
table in disk
it actually first gets persisted to what
is called a reader log or if you use
another tray database it may be called
transaction log right ahead log bin log
etc etc and this is a sequential
collection of we didn't update he didn't
insert he didn't delete and if you crash
the database and start it back up it
will look at everything it hasn't in the
database files on the disk and then we
look at this read log and we start
saying okay here are some transactions
that happened some inserts and updates
and leads that happen but did not make
it they made it into memory but did not
make it into the database files so we'll
have to basically it's called a riddle
because we're redoing those transactions
again from the log so this log is the
real source of truth for the database
which means that we have a log of events
so here is how you want to think about
it suppose that you have you know your
bank and you have someone's back account
in database table account ID balance and
someone goes and does deposit and you
have an update please add $50 to the
background I'm widely simplifying if
your DBA don't shoot me on the spot and
you keep you keep doing those changes
and the state of the table keeps
changing with it so you keep editing or
keep removing and every time you do this
action the state of the table changes so
if you do a query you will only see the
latest but if you look at the River log
you'll see a stream of updates coming
into the database so you can see how you
have the
trim but is that you can convert into a
table by just adding stuff up and you
also have a table is that you can
actually by looking at the data it was
underneath the databases change log you
can actually convert it back into a
stream which is a very very popular
thing to do in Kafka land and in stream
processing and so we have an entire blog
post exploring this a bit more because
it's it can be just like the matrix it
can be a very hard concept to wrap your
mind around like a table is actually a
stream a stream can become a table but
it's super powerful once you get it it
can make it means their applications can
look at the stream of event and maintain
a cache that is identical to what's
going on in the database and we'll talk
a lot more on how we can use this
duality to do cool stuff in Kafka so
here we go so we have this stream of
events in the database I hope you I
convinced use it even though you think
you have databases what you actually
have are streams of events but now we
have to get these streams of events into
Kafka why do we need to get these
streams of events into Kafka because you
can actually do cool stuff with that the
first thing is that suppose that you
want to do real-time stream processing
you cannot actually do it if you don't
have you know a stream of events to
process so there is enough places you
could get a stream of event for example
you could go to your developers and say
every time something someone click
something in your website please make
sure an event gets reported to a Patrick
rafter and technically it's easy I am
we're talking about something like maybe
five lines of code there is not a lot of
magic going on but we do know that the
developers can be reluctant to just go
and change their code because someone is
in the organization just said so they
have priorities they have things that
they're busy there's always some risks
is also always opportunity costs so they
kind of need a good reason to do it well
on the other hand if you go by yourself
and just connect to the database read
rogue and starts grabbing data you can
write kind of a B or C or example
applications that uses the same data
that you wanted to get from the
application the same
things that people do in the website
they purchase says the return zones that
you can get it as a stream of events and
show hey Here I am
I built this really cool a dashboard
that shows in real time everything that
goes on in our store we don't have to
wait for the big database it'll load in
the data warehouse
you know it's 4 a.m. and then the
reports come out at a.m. you have this
ongoing ever updating view of the system
are executives love it now I think it
will be so much easier if you just
report a few extra events into Kafka so
I can make it even cooler it's a lot
harder to say no to that so we kind of
look at this change capture capturing
changes from a database and the gateway
into a real-time stream processing
because it's a very powerful source of
streams so obviously that was nice yeah
now you know why but there is the how
how the hell do we do that
and so Apache Kafka shows up with a
farmer called Kafka Connect which as its
called
it allows you to connect it to a lot of
different databases and I'll talk about
it in a second but one of the different
connectors that allows connecting
includes is connectors to a lot of faith
familiar systems and we're going to talk
here about to connect the Golden Gate
connectors that allows you to get
changes from Oracle but the bazoom is a
project by Reddit that actually allows
you to connect to Postgres and MySQL and
Sandra and Mongo and the list just keeps
growing they keep adding more and more
of those databases and then there is
also other people who are also writing
those CDC tools pretty much every change
capture tool you can think of these days
will be able to report data into Kafka
because Kafka calif became the de facto
standard of world we put a stream of
events so you have this channel and you
have change you capture changes into
Kafka and what this gives you and this
may be their most important slide in
this entire presentation I said it and
immediately cameras go up you guys at
home should have been it in the room you
should have seen it and but the idea is
that once you collect
streams of events into
kafka you enabled data integrations that
were actually really hard before so
getting that data from a relational
database to an applicant from an
application to relational database is
easy that's what we always did writer
with applications did inserts and
updates to database that's like the
traditional way of doing things getting
data from a data warehouse from the
database into the data warehouse or into
Hadoop that's also something that
especially in the data warehouse most of
us solved something between 10 and 20
years ago so we know how to do that but
there is a lot of other lines that are
either really hard to do and you have to
do them over and over again every time
one of your developers is like you know
the netherworld is not really what I
wanted I want my data in elasticsearch I
want my data in Mongo my application
will be really efficient if only all my
data was in Cassandra they may be right
ins they may be wrong but in the current
organizational atmosphere it is
increasingly hard to tell a developer
don't use the tools that you think is
the best tool to use which is very good
for for pretty much everyone it's
amazing for innovation but you don't
want the same developers to always spend
I don't know 20% of the time building
integrations okay I want to use
Cassandra but the data is actually in
Oracle and in Mongo how do I get data
from Oracle and Mongo into Cassandra so
all those lines of communication makes
life a lot harder so but once you have
everything in streams of events in Casta
it's a lot easier to just connect
everything it really lowers the cost of
integration by a lot it doesn't mean the
cost of integration no longer explodes
as people bring in their pet databases
but even cooler what if you have an
application that needs to get updates
form an Oracle database in real time you
definitely don't want it to query Oracle
million times a second that would be
ridiculous that will be be incredibly
expensive but you also don't want it to
query Oracle once a day because it will
look at stale data and you'll just get
wrong results you could have triggers
but as the DBA portion of the room knows
we hae triggers as
as in the room knows it's really hard to
get your DBA to edit rigor because the
other portion of the room really hates
those triggers so judge that'll capture
into a soon events that every
application every micro service can
subscribe to and get notified in real
time oh this guy changes address we need
to recalculate the cost of insurance
this enables those really cool new use
cases of which I'm absolutely not going
to talk about today it's a totally cool
different application we are going to
talk about getting data moving data
between databases so we have CAFO
connected and we have a lot of built in
connectors and this is a very partial
list we have over 50 of them and if you
want to write your own we also have very
good guides on how to write an example
one that is just pure C and then all the
extra work you have to do to make it
production ready there's always a catch
right it's like 15 minutes to the first
version and another two months to get a
real version it's all this bad is two
months but there's always this thing
about production but a lot of stuff is
already ready out there and production
ready and you just need to install a
connector at the confirm the confluent
connector hub most of them are open
source and now don't ask me for an open
source version of Golden Gate I can tell
you right now we don't have an open
source version of Golden Gate you may
find cheaper versions of Golden Gate if
you go to Golden Gate competitors but as
far as I know there is no open source
equivalent that just makes everything
free well one could wish but nope so
this allows you to do this data
integration stream events into a Kafka
via connect and stream events out and
that's where you can get events out of
MongoDB into elastic out of Oracle to
Hadoop all those different kind of
integration are suddenly a lot simpler
than they used to be but as everyone who
has experience with it healing and data
integration knows this is not like you
cannot actually do it like you cannot
you it rarely works that you can take an
event out of your database and just
insert it nearly nearly into another
database because you'll inevitably find
out the
the types are different there is this
one extra column there is a security
this is PCI certified but other one is
not PCI certified so you have to live
out on the credit cards this there is a
lot of things that go wrong in the
middle but before we get to that let's
talk a bit about the scale part of it so
when people think about especially
modern data platforms and especially
when we say it's a distributed real-time
platform you always think about scalable
like Google scale Facebook scale and so
like I don't actually have this scale
like hello some of you may be working
for Facebook with Google but most of us
really are not but but that's only one
dimension of scale the really scary
scale here is the number of projects
your organization is running the number
of different data bases that developers
need to make talk to one another and the
rate at which this is growing so it's
more about the university of data and
the need to store a lot of events from
those sources because who knows if they
will be useful then actually it's not
about being Google and the nice thing
that cover connects is here is that the
white boxes that you install those
worker nodes and then you can run pretty
much any connector on them
so today you're integrating and
literally today we're integrating Oracle
with elastic but tomorrow someone comes
to you and says hey I actually have
Cassandra I don't have elastic can I get
it outta there and you tell him here is
the rest api go knock yourself out get
the data in there I don't even have to
be involved and this is really powerful
and this is something that when I talk
to my customers a lot of time the story
here is that I got some data from my SQL
to cough cough for my own use case that
they needed for an application and now
everyone comes to me and not of the DBA
and says hey how do you have all the
data from SQL in Kalka can I come and
grab it and unlike the DBA the database
Kafka was made to scale to pretty much
an infinite number of consumers because
it's not you're not running a heavy
query you're not using all those CPU
resources you're not taking lakhs
you're not using latches you don't
music says all you do is basically say
every time you have a new event send it
over so as long as you have connections
to Linux and this can be five thousand
ten thousand you can grow there and when
you're out of those you can add
additional machines because that's where
the distributed part come out you can
scale out so this is what I mean when I
say at scale and other the covers and
they said there's workers that are
running tasks every tasks basically goes
out and pulls events from a database or
pushes it out to a database and the nice
thing is that cover Connect managers a
lot of the concerns about reliability
and scalability for you so for example
imagine that one of our worker died
Kafka will make sure that all the tests
that's running this worker will
magically make rate to this other worker
so this is something that this is what
we means that it actually takes a lot of
the concerns away from you you don't
have to worry about oh my god I have
this scripting part I wrote that gets
data from this database to the other
database but what happens if it crashes
this is kind of the Kafka magic and
another thing that it takes care of for
you is that a lot of time when we write
those scripts that get data from place
to place they lose the schema
information but this is actually
important information you want to know
what are the fields in my data what do
they mean and there's an integer or a
string or timestamp or what the hell is
going on so this is another thing that
CAFO Connect will manage for you under
the covers and another very good topic
for presentations that I'm not going to
talk about right now the things that I
mentioned earlier is that you very
rarely actually want to get data as is
from place to place you want to do some
processing in there so kapha connect
shows up with a single message transform
which is a set of plugins that allows
you to basically take a single event do
something to it and put it back in place
for example take out the credit card
numbers because the people on the other
database are not allowed to see it
things like set a partitioning key
because you know how the data is going
to get partitioned in the other system
things like store lineage where did the
data come from maybe some rules how are
people downstream allowed or not allowed
to use it and so you can do a lot of
this kind of processing magic on the fly
but note that this is simple even single
message transformation or simple message
transformation it means that you cannot
modify you cannot do things that act on
large sets of messages so example the
easiest you cannot do an aggregate you
cannot do a moving average because that
involves more than a single message you
came out to do a join what do you do if
you actually want to do a join or
aggregate I mean that's not an
unreasonable thing to want to do in your
pipeline right we really really want to
do it and this is work a school Yoel
shows up so everything I talked about
until now who is part of the Apache
Kafka project this is still apache
license open-source but by confluent it
is not part if you go to capital Apache
dot org you receive everything I talked
about except this if you go to
github.com slash confluent inc slash SQL
you will see Casper so this is the SQL
engine there is a lot of other ways to
process data in Kafka but I figured that
if you're an Oracle event you guys will
appreciate some fun SQL so the rest of
the presentation I am going to show you
what can you do with some SQL on top of
Kafka so the idea is that if you know
SQL you can do a real-time stream
processing this is the promise we make
it easy and like everything else I
talked about its scalable and its fault
tolerant and you don't need to do to
deploy complex event processing systems
and you know really a order clusters so
the busiest thing you can use cast well
for is just look at your streams of
events in Kafka like just say hey I have
this click stream I'm wondering either
do we even get anything from Mozilla 5.0
now website let's take a look let's see
the last five messages in which case you
will put a limiter and so this is just
allows you to explore our streams a bit
but if your streams are really large
with tons of
this will just overwhelm your screen we
will see messages just floating you want
to be able to do that much with it the
really powerful thing starts when you
start creating tables from streams we
will talk about it a bit more in a
second
and then you can actually do a bunch of
data analysis on the fly so suppose it
you want to know every time you have a
stream of credit card authorization
requests and if someone tries the same
credit card three times within five
seconds which is usually not something
inhuman we do it kind of indicates a
boat you want to mark it as suspicious
maybe you won't immediately block your
account but you'll definitely flag it
hey hey some something weird is going on
there
maybe you'll automatically SMS the
person hey are you trying to buy this at
that store because I'm getting repeated
authorization attempts this is all you
need in order to do it you do a select
card number you do count star you have a
five-second window you group account
number and you want to have more than
three or physician attempts in a five
second window is it going to be fast
because hey you may have a lot of credit
cards that's where the scales come up
you can just run it on a lot of machines
at the end of the day if one machine a
process is small enough subset of credit
cards this can be ridiculously fast
another thing you can do is do some
monitoring if you have a stream of
events from an application and you want
to know about errors this is how you do
it you want to know about errors that
have more than one error in a one minute
window you can get the list of those
errors that happen so frequently this is
also good for IOT data you have your
home security device if it keeps sending
errors in every minute you may have a
problem so you can do those and then
popping them our most common use case is
not as much as generate alerts and just
transform one stream into another which
is what we're going to do where today
you basically hey what you have a stream
of actions but you really want your
application to only get notified when
there is an action by a Platinum user so
you create a new stream of events which
is
the platinum users and you make your
applications subscribe not on the
original actions but on the new VIP
actions so we talked about like three
for use cases when this is a really good
fit but as you know even though I always
think that every new technologies that I
worked on absolutely cure cancer and is
good for everything ever and will miss
all every problem and make everything
easy we all know that in the real world
isn't as nothing is perfect so some
things you probably don't want to use
SQL for is for ad-hoc queries and Kafka
doesn't really have indexes seok SQL
works very well and processing data that
arrives into the in the future but if
you want to just look at data that you
kept in Kafka for Mance's and it's a you
know 14 terabytes and you're writing
this query and you expect fast results
this is going to take a while because
it's going to be exactly a full table
scan of external bytes of data so be
careful doing that and because of this
lack of flexibility because we only
process data that arrives from the point
we wrote a query into the future and we
it also means that things that require a
lot of slicing and dicing of the data
like you know Connect tableau and
thoughts visual and using your data in
15 different ways not going to be a good
fit also we don't have a JDBC driver yet
so you're going to have to use our api
s-- to run the queries and tableau
doesn't really know about our api is so
even if we had indexes this integration
wouldn't work but as is it's just not a
good fit but this is kind of what we
imagine you're going to do you're going
to get data into Kafka from your
applications or from databases or from
log files you're going to run some cress
ql queries to process this data and
you're going to push the results into
another system that absolutely allows
you to do slicing and dicing and this is
what i'm going to show you in a second
so when we look at this pipeline it
basically has three parts right it has
the extract part the load part and in
between the transform part I'm going to
talk about them in this order so
starting from
getting data out of Oracle into this is
basically the pattern we are building
here note the blog post in light green
on the bottom this is basically taking
you step-by-step on how to do it like
how to get the data into Oracle we're
using swing bench if you're familiar
with it it's a load testing tool that
runs on Oracle and it has this fake
order store orders schema so we're going
to generate data using swing bench we're
going to get the data from the Oracle
reader log into Kafka using Golden Gate
and the blog post talks you into detail
how to do that as well we are going to
do some processing using SQL and then we
use kapha connect to get the data into
elasticsearch and draw some pretty
pictures looking at how we get imagine
the swing match worked how do we get
data into a from oracle using colonnades
the usual way you have Golden Gate and
Golden Gate has connect handlers it you
can download the connect handler for
Kafka and it will get data from Golden
Gate to Kafka this is not a Golden Gate
talk so I'm not going to dive into a lot
of Golden Gate details I've spent the
last ten years of my career trying to
run away from really understanding
Golden Gates and so far I've been
successful so don't make me do it but
you can see we have an entire blog post
on how to install Golden Gate and get
that get your data into a Kafka but then
suppose you've somehow made it what is
really important to validate that it
works so you don't want to insert some
rows into Oracle make sure that you have
the right topic existing in Kafka
so you licit Opik and here we go and
then you want to start consuming events
from Kafka remember it's a consumer
right now it's not the query so you just
listen to the stream of event and you
say we want the events that are getting
published right now onto this topic and
we use JQ to format so nicely and if all
went well this is what you're getting
you'll just see a stream of those I had
this table we have an operation this is
an insert this is the time stamp we got
a login ID a customer ID and a logon
date so and you this is one event and
you'll just suppose to every time you do
an insert into a table you are supposed
to get another one of those you do an
update you will get opt I type you so
this is really really cool and then the
other parties that you want to get data
node from Kafka to elastic and we use
kaffir connect so so far we used Golden
Gate and now it's time to use kaffir
connect and we basically say we want an
elasticsearch sink connector we were the
connection URL is the elastic search URL
we we have the data comes from this
topic goes to this elastic search index
and in between we told you about the
transformations that santim data doesn't
exactly fit we are changing the
timestamp from whatever it is that it's
kind of a binary format that we could be
holding Kafka into a formatted string so
those events go like that into elastic
and the way I ran it used to confluent
load and use and you define and you
think and you give this JSON I just
described with the sync definition and
then you take a look at what you get out
of elastic and in this case we can see
the elastic returns data all you can go
to the elastic UI and make sure that you
can see the index in elastic but best of
all if you installed Kabbalah you can
actually create all those nice reports
and do all kinds of cool slicing dicing
and data analysis in elastic and this is
without any processing except the simple
replacement of a timestamp so this is
like the simplest the pipeline possible
you can all do it in like 10 minutes but
let's do other cool stuff yeah you can
you can just really impressive stuff in
10 minutes by the way so when we look at
them example swingbridge schema we can
see that we have three tables that we
are going to talk about they have more
but we're going to talk about three of
them we have the local table every time
someone logs in you get an event someone
logged in this is a stream of events we
have orders every time someone tries to
buy something you have a new order this
is a stream of events the customers on
the other hand most of the time it
doesn't change it's kind of a static
table of customers every once in a while
you get a new customer or a customer
changes
he's a address or class or language and
then you get an event it doesn't update
so this is what we'd call it slowly
changing in dimension right it's it's
still a string of events but a very slow
stream of events so the customers were
going to actually model inside cask UL
as a table we'll try every time we look
at it we'll just look at the current
state everything else will model the
streams of events and let's talk about
how we do some data analysis so the
first thing is we want to say let's look
at the D string of changes that comes
from Oracle database and model it as a
table basically create a small cache in
memory with an exact copy of the table
in Oracle so every time we want to know
hey what's the customer name we don't
have to go back and query Oracle for it
and here you have to specify all the
columns because we're using JSON if you
used Averill's and this would have been
automatic for you I think the blog post
actually shows you how to eopns
automatically and then let's just see if
it works let's select few customers and
you can see a bunch of customers I'm not
explaining the sequel because I kind of
assume you all know SQL if anything is
unclear kind of a wave your hands I'm
also running fast because I know how
much time I have left the next thing I
want to do is to take the logins so
logins I do want to model as a stream of
events I don't when I do it from here
when I did a select I don't see all the
updates and insert that happened in
order to get Carl Greene to be an
occasional customer I may there may be
five updates before that but I don't
care about them I want to see the latest
state of events when I'd look at the
logins I don't want to just see the
latest logon I want
see every single lovin by every customer
so you model it as a stream of events in
exactly the same way and again we can do
a select and we get the last five events
very simple but now the really cool
stuff is that we want to enrich events
and I don't have much time to convince
you that enriching a stream of events is
actually a hard problem with pretty much
everything except case QL if you try to
do it in spark on flink
you will find out that you have to
create this in-memory cache yourself and
you have to dual hash join or you'd have
to go and call the database every time
no matter what you do it's going to be a
bit complicated we make it basically
just a joint you have no idea what
machinery we're doing underneath but I
can go into you it's a lot of machinery
what we want is it
remember the Logans they have customer
ID which is nice but we want the
customer name and we want the customer
class so we do a joint and we get a
customer name and customer class and now
this is what the result will look like
what if you want to do more what if we
just don't like this formatting at all
we can do more we can concatenate the
first name to the last name to get a
full name and instead of how long
they've been customer in as a timestamp
of when it is join you actually get we
do some more advanced a casting and the
calculations and you know how many years
have been a customer and now you can get
this process stream of events and the
nice thing is that note that we're
creating a new stream of events now we
have real opens and reached as its own
stream so we can go and query go to
Logan and reach and get us all the
customers who are prime and have been a
really really loyal customers so you get
this really nice a subset of customers
just based on the latest data and this
is really powerful you process events
from one stream you create a new stream
you grab events from the other stream
this is basically how we build ETS with
a lot of small micro services in between
so we have those input topics we did a
create table we created the stream we
join them we got a new stream as a
result and we're dumping it with kafir
connect into elastic as a result we have
really nice graphs about how long
customers of bins are how to correlate
the frequency of logins who is how long
someone have been a customer you can
really slice and dice the data this data
this free process data in all kinds of
cool ways how many customers do I have
from a which type etc etc you can create
very quickly incredibly impressive
dashboards and remember the whole goal
is to very quickly do something really
impressive with the data so you can gain
political capital for your next project
right so this is like the easiest way
now to successfully get buy-in to your
next project look what I did in just a
drag give me a budget but of course
joins is not enough usually we also want
to aggregate the data so here we
aggregate every the order table a pair
our how many orders and what is the
largest orders that we got from each
customer type sorry in each order status
per hour and when you so basically again
created we create a table nagas stream
we want a summary and when you do a
select from the table you can see that
for each hour and each order status we
have how many orders existed and because
all of them are one order the maximum
and total are kind of similar but I
swear that it doesn't have to be this
way and then because this is a select
from a table but you can also get used
cup of normal capitals to just show you
the stream of events straight from CAFTA
and because the stream of events
straight in Kafka you can a once again
get this back into elastic so in this
time we got a stream of orders and we
did it aggregate we collided it per hour
and we created a new topic which we
stream into elastic in order to generate
are really cool graphs I think by now
you kind of got the idea graphs heat
maps are where it was I think about five
years ago I don't know if there are
still considered cool or not I think now
it's flame graph
everyone talks about so just to
summarize because I have three minutes
left we got we got events from Oracle
we got events to work and using swing
bench we got events out of Oracle and
into Kafka using Golden Gate we used to
SQL to do really nice analysis with just
few simple queries the results went into
elastic where we could easily visualize
them into cool graphs and in about an
hour - we have something really cool -
short team if you get caught in a crunch
you know it's demo day you have to show
some things that you did this week and
you don't know exactly what you do this
week where did all the time go and of
course this was our example but it works
in the generic case data can come from
pretty much anywhere you can do all
kinds of cool transformations and if
elastic is not your data visualization
of choice you can get data back into
work head or you can get that enter data
warehouse or wherever it is you know how
to visualize from that's where the data
should go if you really liked it and you
want to follow up others in my book
there are some free resources out there
if you go to if you need to want to use
case QL you go to github if you want the
entire conference platform including
Kafka and the schema registry and a
bunch of connectors but not a golden
gate connector because that's belongs to
Oracle and you go and do and download
the conference platform if you want to
read more about how to do it remember we
had a bunch of blog posts on how to use
Golden Gate and I were to use case QL
and how to build this entire pipeline
and if you have questions and don't feel
like paying us for support we actually
have a slack community so you can go in
we have a channel for connect we have a
channel for SQL we have the channel for
I mean you being what the hell am i
doing nothing works
I got salt lies at the conference this
never happened to me before
so you can go and ask questions we have
3,000 people in the community the
chances of getting an answer we're
actually ridiculously high and you can
also ask me on Twitter but not about
calling it that so yeah say thank you
all for being here I've like one minute
so maybe one questions that I'm around
all day so you can grab me at lunch or
something yes
okay this is a huge I have an entire
presentation about reliability in Kafka
but in super short every piece of data
that you write is duplicate to three
machines and written to disk so you need
three machines to have a really serious
disaster and at least three sets of
discs to die in fire if it's read tens
and you need like a I don't know eight
three times twelve sets of disks to die
in the fire and so it is pretty safe but
it's you have to configure a lot of
stuff correctly which is why I recommend
going and searching for reliability in
Kafka and getting a long list of stuff
you have to get right because it is
actually fairly easy just like in Oracle
to accidentally mislay your reader log
and lose all your data yes that's a
fantastic question so he asked if there
is any kind of ordering system to
messages in Kafka and that's I'm sorry
that I didn't make it clear this is like
the biggest thing in Kafka and what
really defines a stream of events is
that the messages will always be
preserved in the order in which they
were written and there will always be
consumed in the order in which they're
written and this is like key to the
whole thing right because obviously if
it's my bank account and I deposited 500
and then I withdrew 500 it's a very
different experience and I'm out of time
and Google actually worked
thank you everyone again and I'll be
around for questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>