<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CON7718   JPA in Reverse   Pushing Database Events to Java EE Applications in Real Time | Coder Coacher - Coaching Coders</title><meta content="CON7718   JPA in Reverse   Pushing Database Events to Java EE Applications in Real Time - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>CON7718   JPA in Reverse   Pushing Database Events to Java EE Applications in Real Time</b></h2><h5 class="post__date">2015-12-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/KCNtw107zuU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm Randy Stafford and this is John
Philippe LaRoche and our topic today is
a jpa in Reverse we call it pushing
database events to java ee applications
in real time I know the titles a little
bit of a mouthful but you have to put in
a few buzzwords to get your talks
accepted here at javaone so you know you
should give me credit for not using
micro service or cry or cloud ok so our
agenda these are almost like sections of
a pattern we want to start out talking a
little bit about the context you know
that the problem that's occurring in
this context and we want to evaluate a
number of known solutions to this this
problem and you'll see what it is in a
minute and then we want to highlight a
new solution that we've been using for
about two years now that actually uses
database replication technology to
refresh caches in Java EE apps and then
we'll have some lessons learned what are
the what are some of the consequences
we've discovered and some of the nuances
of using this approach and we'll finish
with a little Q&amp;amp;A if we have time left
I've got quite a bit of material so
we're going to try to get through this
but if you do have a question please you
know raise your hand ok so to start with
I use the word java ee applications in
the title but I don't mean I mean
enterprise applications written in Java
obviously you know you have the old
style with you know web containers
servlet containers and ejb containers
and so on and more recently we have
microservices architectures with
polyglot persistence and you know
different persistent stores /
microservice potentially I this talk
applies to all of these styles and in
particular experience has shown that you
need cashing in front of data sources
typically in enterprise applications
that are written in Java and I'll get
into why you need caching there so if
you look at the interactions in an
application without caching the
application goes directly to the data
source to get the data it needs and even
the queries are relatively fast maybe 10
milliseconds if you have a thousand of
these to render a page that's a ten
second response time and and I have seen
this I've done quite a bit of
performance optimization of object
relational mapping applications in the
field in my career and I've seen HTTP
requests that caused thousands of sequel
statement executions before the page is
rendered so this is this is why caching
gets used if you put in caching maybe
you have a backing distributed cache
between your data source and your
application server maybe you have a
local cache in your application server
you can get much faster access to data
may be a round trip to the backing
distributed cache costs one millisecond
obviously a local cache in the in that
application servers heap with on
serialized data is zero milliseconds the
the data access is instantaneous so your
10 second page response turns into a 1
second page response it's the same
number of data accesses this this is why
oh and let me just mention that this is
not the only reason why people put
caching into applications there are
other equally important motivations for
example if the backend system is down
you can continue to operate while
accessing data from cash that's that's
one example and scaling to higher
traffic loads so maybe maybe for an
e-commerce situation where it's cyber
monday your database tends to exhaust
first under really high traffic load so
there are several motivations for
adopting caching and this is why caching
has been part of Java ORM since 1996
eclipselink was the first Java ORM it
had level to cashing in it from the
beginning it was basically ported from
small talk or M named top link and top
link had level to cashing in it
hibernate has had level 2 caching for a
long time so these are just a couple of
grabs of the documentation from
eclipselink and hibernate on their their
l2 caching features
also this is why we have in-memory data
grids and you know I borrowed a cheesy
marketing slide here but with in-memory
data grids you get an extra tier of
architecture between your data sources
and your application servers whose job
is to cache data so the need for caching
for all those reasons responsiveness
availability and scalability has driven
you know level 2 cashing &amp;amp; 0 rms and has
driven an entire market of in-memory
data grids and in fact in-memory data
grids are our mature enough technology
now that gartner says that they're on
the slope of enlightenment in the hype
cycle and will will hit the plateau of
productivity within two to five years so
this is not I mean this is prevalent in
memory data grids are here to stay there
are sort of at the same level of
infrastructure as databases and
messaging systems and they've been
around for 15 years coherence was the
first and they're adopted across many
many different industries but there's
still one problem that affects your
ability to cash and that is let's say
you have some invaluable Enterprise
database and you want to inject caching
into an application that uses this
database maybe it's a new application
maybe it's an existing application
that's not performing well enough and
you want to inject caching into it well
what if there's some untouchable legacy
application writing to that database I
see a couple heads nodding how many of
you have this problem I soom maybe
that's why you're here how many of you
are using an in-memory data grid okay
good how many of you are using an ORM
with level 2 caching okay good good all
right so this this is the problem that
that we want to focus on today and the
next section we'll go through the known
solutions to this problem before we
brought database replication technology
to bear on it and I'll let JP go through
the the first two of those known
solutions and it didn't advance there we
go
okay so the first one is the introducing
expiry logic so in this typical case you
have the cash and expiry will will
happen at the cash level so in this in
this sequence application does it get
then it's now so actually is the first
set goes to the share database to the
database to load that that information
then the subsequent access you you
access the cache data at some point that
that I will expire set so that the next
get will trigger and under select again
going against the sheer database then
it's put to the cash so what happens if
like in the previous slide there's
something up in in from the legacy
application actually updates the data on
the sheer database so then you're going
to be exposed to some inconsistent data
so in that case at T 0 the data was
loaded then you do ab subsequent get at
a t2 so you're getting the cached object
expiry was not trigger yet but the
update was done at t1 previously so
bottom line you get stale data then the
expire goes and then a tea tree you're
actually loading well I that I expire
and then it triggers the another the
other sequence for actually reloading
the data but until t4 bottom line data
you obtain is is is not up to date so
comparing what's in MDGs in murrieta
great verses or am so in I mdgs you get
generally standard features it's a
standard feature that's built in the
product some may have sliding expiry
sliding expiry is you might
let's say that might be useful in a case
of session caching let's say you touch
the data so it research the expiry in
rms while eclipselink allows it via
custom add cash a notation I brunette
delegates to second level cache provider
so they both offer that solution merits
and mmm it's the end Emirates buy it
simple to to implement or use that some
degrees of caching benefit with we saw
it you still have some caching but
you're still prone to inconsistency and
you could still work with a shorter
expiry but then you're less benefit from
the caching so the other strategy our
solution is periodic refresh so you
still have the cash but between the cash
and a share data base you have some
refreshment can ism it's actually to be
a bit more aggressive aggressive and
start reloading that data from from
behind so in that case simple sequence
again there's been preloading let's say
of the cash and then they get your
access the cache data and then at some
point in time they will be at the
refresh refresh trade or level another
reloading of the data again say what
happens when you're from your legacy
application you have data data you're
still exposed to some level of stale
data that case happening you know in
between the initial load let's say and
before the second refresh is triggered
you can access your still exposed to
some stale data so again comparing
what's in mdgs and war ends
it's depend of the product it's not
offered in all I mdgs so some I needed a
feature or integration that do periodic
refresh Ryan of it and again I guess
we've seen and Randy seen a lot of
custom implementation which of often has
a cost of some maintenance level raise
your hand if you've done this I'm sure
your followers are very happy that
you've done this so I the RM is
generally not offered out of the box and
again you could toric Lee use this
approach at risk a vile thing or Emma
capsulation duplicating or M logic so
actually what you're going to code at
the refreshed read who you might happen
actually to actually duplicate that code
in that little trend logic so I'm going
to yeah maybe sorry merits and demerits
so more benefit from cashing than with
expiry I think you can do more
aggressive and still benefit from it
short refresh interval so you have
actually you're less exposed to the
stale problem i can use audit tables for
efficiency why it means is there's some
database that have audit table so
basically you can actually within the
database level be informed of if there
was an update or not to another table
and it just comes down to you can
monitor a little bit more efficiently
asking that audit table prior to see if
we need to actually reload the data or
not they merits again we saw that
they're still still possibility custom
machinery required we have for the
Refresh ahead then the question is where
is it deployed where is it running so
that complex if I is probably the
architecture and you have to support it
make sure that it doesn't you know that
is fully available stays up so more
complex to make scalable and highly
available well that tread of course
makes makes a more request to the
database so actually if you have
multiple client notes that are actually
or let's say you in your cash deployment
you have multiple nodes then you can
multiply those select happening so that
can be an issue thanks GP so I'll talk
about the last two known solutions that
we've seen there's one I'm just calling
this triggered messaging um I have heard
of I've never actually seen one of these
where you basically put triggers on
tables that you care about and those
triggers call stored procedures that on
cue messages in some messaging system
that are consumed by something that
invalidates or refresh is the cash has
anyone done this okay yeah wow I've got
quite a few people have done that great
so this does happen out there and this
is these known solutions are kind of
going in order of worst to better right
so um the thing about triggered
messaging or the prevalence of it this
is generally not available in an imdg
I've never seen an in-memory data grid
that offers something like this out of
the box nor is it generally available in
an ORM obviously it requires custom
development the triggers the stored
procedures the message consumers you
could theoretically use this with an ORM
but again at the risk of duplicating
logic or in violating encapsulation
hibernate for example what hibernate
puts in its second level cache is
actually this cache entry object it's
not it's not your pojo it's it's an
instance of org dot something dot cache
entry and it basically represents a row
of the table with you know simple types
and so you're if you were going to do
something like this to refresh a
hibernate second level cache you'd have
to instantiate in that class and put
your instances in there so you you're
treating hybrid eight as a white box
unit you know what is putting in its
cache and and you better play nicely
with whatever it expects to be in there
yeah so obviously we've heard of
implementations of this in the wild
they're about a half a dozen people that
raised their hand to this one okay so in
terms of evaluation this is event-driven
and it's it's near real-time you know
the the trigger and the stored procedure
and and the message on cue that make the
cache consistent with the database
except for however long it takes the
message to get delivered you know which
I don't know maybe maybe that's a couple
of seconds and the another merit of this
approach is it's scalable and highly
available due to reliance on messaging
infrastructure that's that's pretty
proven technology demerits of this a lot
of complexity there's a lot of moving
parts triggers stored procedures some
kind of messaging system some kind of
consumer and your messaging system has
to be callable from stored procedures
you are the people that raise your hands
are you doing this with Oracle AQ yeah
or is anyone is there anyone who's not
doing this with oracle AQ okay so what
is your stored procedure calling okay
okay so db2 has like an mq interface
yeah okay so it's going to be something
like that it's going to be some
messaging system that's callable from
within a stored procedure I already
mentioned custom machinery and then of
course you need to administer the
messaging system and the destinations
you're using and so on so those are some
of the demerits of this and the final
known solution is database change
notification that this is an old name
for a feature of Oracle database and to
be honest item I don't know if other
databases have this feature it's now
called continuous query notification in
oracle database but basically you can
register a java listener with a JDBC
driver and when you register the
listener you pass a query and the JDBC
driver will call back your listener or
whenever the result set of that query
would change because of a transaction in
the database which is kind of cool and
so you set all that up and then your
listener just gets notified and puts
data into the cache the prevalence of
this as far as i know this is only
available with with oracle database i
have seen implementations of this in the
wild we've built these for some
coherence customers out there obviously
it requires custom development it's not
generally available in anything that in
any imdg I should say but this is out of
the box in eclipse link if you use
eclipselink ORM that this this is like a
documented feature of eclipse link so in
terms of evaluation this is the nearest
real time you can get without being
synchronous somehow because the Oracle
continuous query notification feature
uses Oracle databases fast application
notification which is like a custom wire
protocol between the database processes
and the jdbc drivers so there's no
there's no message on queuing it's this
is this is as consistent as you can get
it has fewer mover moving part
than triggered messaging but it's still
custom machinery and your listener may
need to query the database after it gets
after it gets notified depending on how
much data came in the notification
versus you know what's trying to do to
the cash like updated or whatever and
it's complex to make scale scalable and
highly available because it's all you
know bespoke bespoke custom code okay so
those are the four known solutions that
that we've seen or that I've seen and
we're ready to move on to the really the
database replication based solution but
has it has anybody else seen any other
solution besides those four to this
problem yes
the same idea this aging part when you
subscribe to us you specify that I want
to have all the result of that farming
to be pushed to me that database then
you can do whatever you want with those
results and what was the name of that
database again rethink okay gotta thank
you I didn't I wasn't aware of that okay
well I'll tell you I'll good I'll now go
into another way that we're trying to
attack and solve this problem and it's
it's based on Golden Gate I apologize
for mentioning a product name here at
javaone I I can mention hibernate but I
can't mention anything commercial anyway
Golden Gate is a real-time heterogenous
database replication technology that was
acquired by oracle in 2009 when I say
heterogenous I mean that sources and
targets can be different brands of
database so you know you can replicate
from sequel server to db2 or actually
more likely db2 to sequel server etc and
in concept what Golden Gate is doing is
its tailing the databases transaction
log and every time a transaction gets
logged by the database Golden Gate
transforms that transaction log entry
into a database neutral format that they
call a trail file and then Golden Gate
pumps those trail files to other
machines where they're consumed by other
Golden Gate processes and applied to the
target with and then initially the
target was at some other relational
database the whole the whole Golden Gate
product was set up to replicate data
between relational databases this is a
proven technology industrial strength
technology with all kinds of high
availability solutions and scaling
solutions and monitoring packages and so
on it's written in C but it has a Java
adapter and this is what allowed us to
solve the problem of
refreshing cashes in Java so here's that
here's just a diagram of what a typical
Golden Gate deployment looks like you
have some source database up here and
there's a Golden Gate extract process
running that writes this trail file I
talked talked about and that trail file
can be pumped to different places over a
network for example different data
centers so if you've got some kind of
multi-site architecture or active
passive or active active you can pump
the trail file to both sites and it can
be applied to a target at each site so
that's what's shown here those are they
call it a remote trail but it's the one
that got pumped across the network and
it's can normally consumed by a
replicate process to be put into another
relational database well Golden Gate
also has a Java adapter so the the here
the trail file getting pumped across a
network and on this host labeled data
integration server there's an extract
process running Golden Gate extract
process that has an embedded JVM it's
kind of an interesting technical note
they launched a JVM inside the c process
and they talked to it over J&amp;amp;I so when
when transaction records come through
the trail file they they feed him into
this Java framework in the JVM by Jay
and I and then the Java framework can do
whatever it wants and in our case we we
thought well why not make the Java
framework put stuff into a in memory
datagrid so this is how this feature of
coherence was born there's there's a
feature called hot cash I didn't invent
the name it's probably some marketing
guy but a hot cash specializes the
Golden Gate javed apt or framework to
replicate data into coherence and hot
cash uses jpa for the mapping and I'll
go into more detail on on the mapping in
a second but it's fundamentally a
mapping problem so this this is a usage
of JPA in reverse so to speak entity
hydration is driven by database
transactions not by application code
calling the JPA api's so as a result you
get sort of real-time or near real-time
event
ribbon cash refresh from database
transactions using pretty proven
database replication technology and the
jpa standard for object relational
mapping this is simpler and more
out-of-the-box then the triggered
messaging approach and it's just as
scalable and highly available and
monitor bowl and so on because you know
on one hand you have mature messaging
technology but on the other hand you
have mature database replication
technology so this has been around for
two years now JP was kind of the
trailblazing adopter of it but it's now
in production usage in quite a few
mission-critical systems out there in
the world so fundamentally it's a
mapping problem here's what Golden Gate
sees this is an XML representation of
what Golden Gate calls an operation so
like let's so what I did was I I added
Barack Obama to my contacts database so
I've got a table called contact and I
inserted you know a row in there and it
had all these columns and so that
resulted in a transaction log entry in
in my database which was turned into
this trail file operation trail files
are really binary but there's a there's
a tool for spitting them out in XML so
you can see what's in there so the
question is you know given this
insertion of into a contact table 2
which cash and which cache entry should
this correspond that's the mapping part
all right so so Golden Gate can deliver
you the data but you have to decide you
know where to put it so I've talked
about object cache mapping for quite a
while because i've been at around
coherence for eight years now and
there's some interesting questions how
do you take a domain model and map it to
an in-memory data grid like and so you
know with with jpa JPA gets you from a
table and primary key and row to an
entity class and an entity ID and the
entity state and then hot cash ads in
additionally given an entity class there
there's one cash that goes with that
class
and the entity ID is the cache key this
is kind of obvious kind of default but
this this cache per entity type pattern
I call it is a long-standing pattern in
object relational mapping this is what
hibernate does this is what eclipselink
does that they out they all use a
separate cache coherence terms it's a
named cash I think in hibernate and
gemfire terms it's a region but they use
a separate cash or region for each
entity type so this raises an
interesting question given a cache per
entity type how do you handle
relationships between entities in cash
as anyone struggled with this awesome
you can tell me about it after all right
well so I mean as you know when you're
using an ORM your objects refer to each
other by by pointer references right and
and the ORM s give you proxies and
things so that you can lazily fetch
objects and sort of navigate an object
graph all by point of reference and that
that's all very nice and very convenient
until you need to store until you need
to cash more data than will fit in one
JVM heap so when you get to a like an
in-memory data grid scale and you're
caching you know tens hundreds of
gigabytes of data even terabytes of data
you really can't let objects refer to
each other by point of reference anymore
you know they have to refer to each
other by identifiers and currently with
with hot cash that this is how things
are working you know if an address needs
to know you know who's the customer that
owns that address it either has a like a
foreign key reference to the customer or
vice versa and JP will go into more
details on this so there's a huge
mapping problem here with cash refresh
and that's why jpa was chosen in this
solution because it's standard you know
mapping metadata okay so that I kind of
already made this point but the normal
JP a control flow the application or the
service is calling you know find on an
aunty man
commit on an entity transaction and that
causes sequel select statements or
update statements from from the JPA
provider what we're doing is we're
tailing a transaction file a transaction
log file and we're getting a trail file
operation record and then we have we
have to build an object we have to build
a jpa entity from that trail file
operation record and stick it into the
cache stick it into the right cash at
the right key to be specific with the
right relationship information so the
whole thing the whole control flow is
reversed and that that causes some
interesting nuances you know when when
you consider this cache per entity type
pattern and it's sort of one at a time
operations that are coming at you from
Golden Gate it gets tricky to maintain
all the relationship information in the
cache in the grid and that's what JP JP
is going to talk about for quite a while
here so golden gate is in control and
everything is reversed so I'll turn it
back over to JP and we're right on time
you've got a half an hour maybe 25
minutes for Q&amp;amp;A and he'll go into the
details and give you a demo okay so like
we talked about Randy talked about its
you have tables then everything goes
into the cache but then your your I
would say stock what you have is
actually a you know a bunch of caches or
maps with key and values and I'm
guessing since we're talking about
domain object models what you're
probably interesting in at Wednesday
when you consume that data is probably a
classical object model so what I would
like to add also is sometimes you're
stuck with some sort of legacy database
schema and you know how it is sometimes
you you have to move on to production
and you try to get the collaboration of
your your dear dbas and sometimes you
know it's a bit difficult so you have to
be
more I creative and when I'm also its
its induced by the the key-value format
it's not does not exactly Maps the usual
usual object graph that you would like
to navigate so what happens is you might
want to or you might have to do some
sort of transformation with that data
that sits in the cash so i have a few
examples and these are real life
examples so please don't question the
database schema you this is what we have
to deal with there's probably other ways
to to to modernize this but this is
usually what you're given so from the
database model you have in that case
address address link and customer after
reverse GPA going through golden gate
and hard cash we're using the one to one
which again is kind of the standard you
will have the the corresponding cache to
holding the entity corresponding to the
initial table what you might want in
that specific case is probably
navigating from a customer to the
address objects so in that case just one
to two because we're going to work with
this a little bit so it's the way it was
model eyes it's simple is that when
actually let's say the address of a
customer changes in that case going back
to you have the legacy application
probably call center you know modifying
or getting the call from the client and
to modify that database so actually what
will happen in the database is you add
an address and then the address linked
table is just a sequence actually that
is there's an address link record that
is added just linking the same customer
but with the new address ID so
let's say this is what you have and you
want to retrieve the address so dealing
with the caches you have the plane
caches you get a customer object from
the cache then you have to queried your
address linked Kash to get with the
customer ID to actually get the address
ID that is associated with it and
probably adding some sort of constraint
to since it's a sequence you have to get
the highest the most recent address link
up entry so then you get you quote you
get the address from the cache based on
the phone address ID that's tree request
to the cash I understand that it's fast
but it's still a bit you know cumbersome
any better option so let's try to pre
resolve the G address from the address
link so that would be nice how are we
going to do it so we use currents live
events again it's not not nice where
product page but it's it's a nice nice
feature of it so using live events to
intercept insert events on address
address linked Kash and you can invoke
an entry processor from interceptor to
do a concrete safe update on customer
cash ok what does it mean bottom line
are from the Golden Gate you get an
insert like I said you get a new new
address was that added and then also an
address link was added to actually link
a new address with the existing customer
so you have the address linked Kash so
naturally goes it's given to Golden Gate
the trail file and high cash it will be
inserted in that cache to recurrence you
can listen easily to events happening in
cash so what we're going to do we're
going to intercept that insert that
actually put that is done by hard cash
and then what we want to do we actually
will trigger any entry processor to go
and update the customer object in the
customer cash
so actually we will set the address ID
so now we don't need to go through the
address link object will have this
information right within the customer
object again doing that just mention the
entry process turrets guy is done in I
would say a transactional safe way so
now you have the customer object in cash
toss the address ID and you still have
to let's say you want the address to
query the map sorry to get the address
from the address cache but it's easily
to implement if you want to get address
method within the customer and to hide
actually that logic so what's facing it
when you're accessing that customer you
just you know ask ask the gate custom
get address method and then you have it
I just want to to add that from the GPA
part the address ID is transient of
course because you're dealing with it
within the data grid and what if you
require non lazy loading a red dress
that could be a possibility well it
still you could still address object
could be pre queried at address laying
insertion time so when we went to to
that sequence we could still even at the
this time we resolve the address that's
pretty much up to you dependents a use
case do you systematically get the
address information or not when you're
accessing the customer object depends
bit of the use case so another case is
composite view so again it's true to one
on one you get pretty much everything
from the tables to two caches the thing
is what you want to consume at the
client level or let's say from the real
domain object that you're interesting in
maybe you want an aggregation of all
this so you don't want to go and you
know let's say billing account go and
query all the cash
is for in this case payment credit
historian bill keep in mind that we have
that granularity probably because at the
database level we have some sort of
billing process that is triggered you
know in batch every night so we have to
have those all those separated tables
because of legacy i would say batch
processes but probably when you're
consuming this that you want a probably
aggregated view of this let's say
billing summary in your application so
in this case working again from the
tables we have 11 mapping tu caches and
but what we want at the end let's say we
have a billing account info details
object we want all this information
already there this is also to avoid
useless calls to the caches and probably
for performance reasons you you want
that aggregated you so again we use the
live events and entry processor
combination so a lot of things happening
but I want to stress out the fact that
coding wise this is a not very it's not
a large footprint so and probably also
that this is happening you know although
it's safe through because it's an old
shoe entry processor it's concurrent
safe update to the billing account
object but probably you're not happening
at the same time due to billing
processes so it's pretty much like the
same one before it's just that we're
pushing we have I would say parallel
processes of actually modifying the same
objects what's interesting and this is
actually the cash let's say payment
credit history and bill they're
participating in actually enriching this
billing account object but when you're
consuming it you probably don't care
anymore about the payment credit history
and bill so just for a little i would
say optimization and space optimist
we call those staging caches so they'll
they'll use in the transformation
process at the data grid level what you
can discard them so true an eviction
policy you can pretty much keep those
caches relatively with a small footprint
yes yes in that case it is but it could
also end but in this case I would take
convey aggregated information so it
mostly for display and reading probably
the I would say the update process and
again in this special use case the
modification of this is done through
back end processes correct probably
automated due to billing cycles so in
that case it's pretty much only it's
only read data but that could go also in
in READ&amp;amp;WRITE mode that are other
impacts but in that case it's it's
specifically composite objects for for
reading correct so talked about staging
caches and i want you i went to a
specific good on time ok so there's we
said with there's the GP behind the
scene while technology so of course you
know about embedded so let's say we need
composite objects why not you know do
embedded objects with the payment and
all this so it's it's also possible and
then it comes to what's your specific
use case in our use case we we also want
to access other objects actually for
other purposes so it comes down to since
they're embedded when you build the
billing account object it's all goes in
the cash so there's no caches for the
other objects there's no de peak so
if you really want to do the 121 then
the embedded doesn't work for you but
again you're in that classical let's say
customer address that could be a natural
fit maybe it fits your purpose so
preserving parent-child relationship
info so that's another interesting one
so object where friends are not
preserved across caches so you cannot
just keep an object reference so how do
you do that so to keep relationship
information between objects kept in
different Maps or caches I like Randy
already mentioned the preferred approach
or the standard is to hold the keys in
the parent object so you still have some
information about that association
referential integrity needs to be
preserved as child collections gets
updated so then that's really the
challenge is what happens dynamically
when that you add or remove children so
in that specific case billing account in
the model behind is a praiser actually
for a wireless cell serv system so
billing account for associated to
billing account you can have multiple
subscribers so again with the one-to-one
you'll get the billing account cash
associated with billing account table
and subscriber for a subscriber table
what you really want as a model is
probably one to n relationship from
billing account to multiple subscribers
so again it's pretty consistent it's
using the live events with the
combination of the entry processor and
it so it's pretty much the same chain so
but in this case we're actually reacting
on insert and delete so we have to keep
up-to-date with the children keys so in
this case the entry processor will
actually handle updating the set
removing or added Keys identities of the
subscribers within the billing account
object again in that case there's a GP
annotation that works a bit like the
embedded board for collection its
element collection you could embed those
within the billing account object again
it comes down about how to use it if you
need to separately query you access
subscriber objects then they need to be
in their own cash so this this is why
the embedding those children within the
object might not do it for you but it's
still possible again pretty much about
the the use case how is it consumed how
your model is consumed
I I'm not i might add this just a little
warning you have to understand that
what's happening behind the scenes there
is a addy entry processor level there is
actually serialization and
deserialization happening so let's say
you have huge sets and worst case those
updates come in batch the case would be
we have the case in in a billing for
example at night process runs and then
for let's say billing account you have
hundreds of Bill information that is
coming and that could be an issue so
there are alternative to do that so
again it's a bit about the use case you
have that might not do it for you but
again it's specific case if you have
really huge sets and the outcome in
batch at the same moment bottom line
Europe you're going to you're going to
have to what you're trying to achieve
actually to hit the same set correct to
update it and that the problem is that
when it comes in this is pretty much
done in a civilized manner so you have
to hit the same object and all this is
all those updates are queued but this is
a very specific i would say issue that
can arise i have one odd case and it's a
a change to database PK so you're
dealing with maps and we've seen in
weird cases actually the and maybe you
have you've seen those but actually the
peak a primary key on a table there's an
update to it when dealing with maps of
course you have a key and value so and
this is how it's identified changing the
key means it's you're not dealing with
the same actually entry so what happens
in that case you really have to be sure
that you map the PK attributes within
the object and you get you still get
notified by an
update from the let's say the event
listener but that case what how it has
to be materialized it's actually a
delete and insert of a new entry so this
has to be handled manually but again
with live events you have an entry
processor you have the kid to do it and
again this is based on I would say from
the terrain experience and it's it's a
bit of an odd case i would say general
recommendations a good on time so okay
it's always good so you can be creative
on the GPA level and but the thing is
that's that's not the main purpose here
it's rly to to have materialized that
object model so you can use it so i
would say let's say you have a large
project and we're usually not dealing
with one or two tables you probably have
some sort of legacy domain on database
key mine it's about you know maybe 40 50
tables and then you're you have to to to
tackle that that GPA i would say a
configuration so my rule would be keep
it simple and move on to to make sure
that everything is going at the next
steps that is this is running smoothly
again you're usually given with that
table schema so you don't have the
option to really modify it when you're
actually dealing with a project that one
wants to use hot cash that's another
little information make objects
evolvable that's i would say that's
current specific but the thing is with
currents actually you can handle a
change within the domain objects as i
mean as java class definitions of course
additive is supported let's say you have
you add attributes maybe you didn't map
everything you just want to as it goes
you know as new actually use case for
four
consuming that add data model comes in
you have to change them and actually
current supports that and even without
without down the down time when it's
additive there's a rich actually feature
for that and it can support multiple
version even at the same time why Cannot
I go go quickly with today's produce
initial simple mapping and test the
whole chain Golden Gate to cash consumer
iterate refine and use currents live
events to massage the data to actually
fit the the end domain model that you
want to actually consume and again don't
forget queries maybe you can cannot
achieve it to entry processor or the
example I had was with the let's say you
have a huge set again Cowen's is a very
performing product and you can query
based on certain constraints as long as
you add indexes and sometimes it goes as
fast so it's not initially this is I
would say a design pattern but it's not
necessary to use this the the queries
might do it for you performance wise
we'll do a little demo to there were a
lot of moving parts we talked about
database extra Golden Gate hot cash
coins data grid so I'll do a little demo
and to end to to actually see how this
goes so I would say these are the usual
suspects so you have a database and the
extract Golden Gate actually it will
handle the trail file then another
Golden Gate holding the hot cash which
handle the transformation from the trail
found actually the creation in the
object creation and then the currents
were actually those cash are reside and
where those updates are made so that's
pretty much the flow that Randy already
talked about in that case we don't use
by the way you can imagine that that
could be on two different sides
city extract the original extract
generating the trail file and updating
it and the target extract receiving it
it's not necessary but usually I would
serve prediction implementation usually
these are separated I would say the
source extract and the target extract
what we're going to use we're going to
go back to our little model of the
address customer and the address link so
this is what reside on the database I
and its materialized within the cash as
the corresponding while one-to-one
mapping of address address link and
customer cash so what we going to do
let's say we trigger some sort of
modification database level again the
example was well someone receive a call
at a call center to update its account
its address and then probably on some
sort of portal we we want to read to get
actually that updated in real time
update transformation nostale cash no
stale data and the thing is it's going
to be is going to happen very fast and
you won't see much so we'll have to pick
a little bit in the debug file just to
give you a bit of story of what's
happening out the golden gate level and
also at the currents that are level
since we talked about event interceptor
an entry processor maybe that though
these are new concept for you so I'll
just show you a little trace what is
happening there okay
soon
ok
powerpoint slideshow mode
oh it's okay
ok
so just to tell you what's in it we have
custom information the address
information and the address link so
actually we have that's the view of the
data grid we have the at the bottom it's
just a little dashboard to see we have
three caches corresponding to those
three entities other information so just
to show that that that agreed is
actually running so so going back
actually just showing information about
that customer information maybe I just
go to the GP just so you see the entity
might be a bit useful actually
so in that case we like I talked about
we have the customer object and we want
to easily access navigate to its address
the associated address so in that case
this is typical we're going we're doing
just GPA annotation we see from this
little information that we're associated
to that's the name of the entity but
that's also used to actually identify
the cash that it belongs to so we have
mapping basic mapping but i want to show
you an information is this one actually
this is transient feel so address ID
doesn't belong to customers actually
this is how we handle it we handle it in
the data grid so this is how we will set
set this information and we just want to
see let's say for example when we access
the g address i want to show you i'm
sorry
having you mean having the address ID
yes so in that case if you want to
navigate the address from the customer
we have that gate address method which
actually will query to the cash to
actually return the real address
reference so going to this in this case
we just have this customer with this
address which is actually fully pulled
from the address cache so if we go to
the database and we'll just trigger a
little update so actually we're
inserting the address and the address
name link
so I'm expecting to have a new address
in this subject so now we have Montreal
igniter and that that change so same
customer just change the address and now
we see actually the address ID is 3 how
did that went through so picking quickly
at the hard cash level just to see
actually the insert operation from
database coming but being materializing
to operations to data grid and this is
you can see that from the hard cash
adapter with in golden gate so we got
our insert we get a new transaction from
the database it's an insert just here
and then it's materialized as an entity
operation on the cash so this is the
address that came in and after the
insert on the address name link and the
insert operation on the corresponding
cache what happened in the data grid so
we triggered that even interceptor and
an entry processor this is a basic debug
lines though so we just these are traced
from the event interceptor and a trace
from the entry processor so we just
picked up the address namely insert we
got the customer ID and also the address
ID and then we can invoke it on the
customer yeah one minute so I just want
to summarize real quick this need for
cash and validation remains a real need
even even in 2015 you've got legacy
databases out there like see apps
writing to them thank you for we wanted
to share this solution that we've come
up with using Golden Gate and coherence
it would found it to be pretty powerful
it's been in production at jp's client
for two years now under pretty high load
500 gig of data
and being refreshed however frequently
it gets updated so we've had two years
of figuring out how to work with this
and we you know have to have a backlog
of things to plow back into the product
now thanks for enduring my pseudo UML
and thanks for taking an hour I don't
know if we have really time for
questions formally but maybe you can
catch us after we're done I actually
have to run down to moscone for another
talk of four o'clock so I'm not trying
to be rude but I do have to pack up and
get out thank you again please fill out
a session survey sometime or rate the
session on your way out if you would</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>