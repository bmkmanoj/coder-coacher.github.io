<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Interactive Data Analytics and Visualization with Collaborative Documents | Coder Coacher - Coaching Coders</title><meta content="Interactive Data Analytics and Visualization with Collaborative Documents - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Interactive Data Analytics and Visualization with Collaborative Documents</b></h2><h5 class="post__date">2017-10-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/mWDeEXs_CMQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so thanks for choosing to attend here so
I'm joined my name is Hasan Chaffee and
I have with me Daniel who is the main
person working on or leading this effort
from Oracle labs so what I want to first
start doing and start by doing is
reviewing some of the foundational
concepts in particular what we mean by
data science and also what are some of
the tools that data scientists use and
talk about some of this project that
we're working on in support of that so
by by show of hands how many of you guys
are how many data scientists do we have
in the room today okay
so you so we have about one or two but
you know so it's the hottest job and you
know everybody should be wanting to be
the data science if you look at any you
know job ranking it's the number one job
in America today so so so the issue with
that data science is a term it's kind of
like if we remember big data as a term a
lot of it is more of a kind of a
marketing thing a lot of you guys if
you're dealing with data and trying to
analyze data are already doing what's
considered to be data science but let's
let's try to define the term of data
science and data scientists so so at
least according to some definitions it
involves finding useful information
hopefully a lot of your or some of you
are interested in that using non-trivial
amount of data large amount of data
right so the the information you're
gaining is not is based on data not
based on you know you talking to to some
friends they're telling you information
so it's data-driven and the other key
differentiator is that you're using
systematic means to do this so you're
not just you know opening up an excel
looking at it plotting some graphs
although that you know could be argued
to be some kind of data science but
typically you have a set of tools at
your disposal that you can deploy and
you're able to reproduce your results
and and
and have ways to programmatically derive
this insight from your data so how is
data science typically done so first
step is data collection and we're not
gonna talk too much about this in this
talk but it's really a big deal it's
it's the starting point
again if you're obviously you know
there's only two words in data science
it's science and data so if you don't
have data it's very hard to do the
second part right and actually the first
word is data right and typically that's
where a lot of the challenges is getting
this data curating it and typically what
you want to do is combine both your
internal data private data with some
public data so you have to also be you
know have some proficiency in terms of
finding relevant external data and
mashing it up with internal data once
you have your data then you want to
start exploring it so that involves
typically cleansing the data starting to
build models what we mean by models is
basically try to build an understanding
of what the data is telling you and then
create a machine representation of that
insight and so that's your model so if
you think you think of it so you're
teaching you're essentially explaining
or building a way to represent a
particular insight and so one example is
the fact that we notice that people that
have certain attributes like certain
kinds of movies right so you create a
model that represents you know the your
your knowledge of which movies to
recommend based on people's attributes
and you basically test this these
assumptions in that model against some
real data and it typically also involves
visualizing the intermediate results and
that's that's your main activity you're
spending a lot of time doing that it's
an iterative process you're going to try
different models different things and
then you build a product typically
called a data product that essentially
delivers these insights and applies
these insights to solving a problem and
the problem could be
you know recommending new products to
our customers or it could be attributing
a risk score to people they're applying
for loans it could be attributing a risk
score to people applying for visas it
could be all those sorts of things and
then you deploy that and then there's
more data that gets generated based on
applying this model and then the process
starts in you so it's a to getting a
loop and a continuous process so that's
the the process now what tools are
typically required and one can think of
broadly two classes of tools one is
back-end tools and they're obviously it
starts with data so you need to have
tools that allow you to manage to store
to query this data and that involves
both a more traditional relational tools
like the Oracle database but also new
emerging no sequel unstructured tools so
now you have your data you can query
your data then you need some efficient
computational engines to to run
analytics against this data and so these
are things like spark and then on top of
that you have typically built-in
algorithms and built-in frameworks and
methodologies so that you're not
rewriting everything from scratch so
basically the way these tools mature you
start with a framework so in the case of
spark you have spark that's just a
general framework or tensorflow that's a
general framework and then people start
building these kind of toolboxes that
are pre-built there's a set of
algorithms you can apply but in all
cases these frameworks allow you to
innovate and write new code because not
not there's not all problems don't fit
into a category we can just deploy a
toolbox so you need the ability for you
to customize the algorithms by adding
your own code so that's on the back end
side then on the front-end side you need
typically a way to quickly and
interactively execute questions that
essentially have a rapid prototyping
environment you need to be able to
visualize have visualization tools you
need to be able to support or typically
you need support of modern
languages a lot of a lot of the the
tools now are targeting emerging
languages and in this day and age you
want to be able to deploy your your your
investigation and leverage the cloud
because you want to be able to tap into
vast amounts of compute on an elastic
basis so having the ability for these
tools to be able to deploy to the cloud
is very important and so you can think
of this as these front-end tools as the
equivalent of an IDE but targeted at the
data scientist so this area the area
we're coming out of is is focused on
software engineering in dealt with
writing you know algorithms and code and
debugging these and so the IDs we had
dealt with that and allowed you to
essentially set breakpoints and print
the values of variables in memory but if
your task is to make sense of large
amounts of data then we need new tools
that essentially allow you to debug your
analysis by looking at the results you
know so in that case just debugging just
having a debug statement that prints the
result of a single variable is not
useful you need to be able to look at
essentially millions of billions of rows
and make sense of that and so that led
to this evolution of these tools and the
rise of this notebook technology and so
that's what we're here to show you a
little bit of and talk to you about and
so this is not a new concept the early
tools actually think they date to the
late 80s or so forth like the first tool
was Mathematica there are there are
earlier examples of this some people
would say that if you guys have ever
used low tech low tech as an example of
this but basically the Mathematica had
the first instance of a notebook and
there the whole concept of a notebook is
that you you have this environment where
you write your code and you see the
results as you write different parts of
the code so the
and the results are essentially put
together and it allows you to also add
comments and add make it look like a
document like a living document that
also is executable so now the
explanation of your experiments are in
the same document as the code and as the
results of these experiments so that's
that's really when made it appealing in
the math oriented types of environments
so Mathematica and you can think of R as
a still coming from a linear algebra
perspective but you know more quotient
oriented and the other key feature is
the embedded visualization now this has
evolved and the recent trends are to
have tools like Jupiter which is
essentially the rebranding of ipython so
that's kind of the notebook environment
for Python and then on the JVM side
we've recently seen the rise of Zeppelin
so Zeppelin is you can think of it as
Jupiter but more for JVM based languages
but we also noticed that most big data
vendors most most people that are in
this big data space are starting to
provide their own notebook environments
so it's it's quickly becoming kind of a
required feature if you're going to be
providing big data solutions to provide
these types of notebook environment so
data breaks which is a company behind
SPARC cloudera has announced and I think
delivered a a workbench if you look at
the the the various cloud vendors sure
has a notebook service so everybody's
kind of presenting notebook services a
lot of it is open source notebooks
wrapped up and so again when we see this
evolution there's a few dimensions we
can look at one is in terms of the
programming language as these
architectures these notebooks evolve
they go from being focused on a single
or dedicated language so in the case of
Mathematica obviously it's just you can
only write Mathematica code our studios
are you put
when it started was mostly focused on
Python but then as you see the
architecture is evolved there's a
recognition that you can't just get the
whole job done with just a single
language so you have to have your
notebook architecture supporting
multiple languages and have the ability
to essentially deploy the right language
and the right framework for part of the
job because the job includes many
different steps from data acquisition
cleansing manipulation querying to
deploying various different frameworks
you may be interested in doing a deep
learning type of framework versus the
more of a traditional statistical
regression model so you'll have to deal
with different tools and there's no
single tool that can do everything so
it's very important for you to be able
to support these things in one
environment on the application side
these notebook used to be these desktop
oriented things where single user has a
single instance it's running on their
desktop and then it's moving towards
more of these cloud oriented offerings
or browser based our offerings where
collaboration is also a key feature that
that needs to be solved collaboration
and requirements that are deal with kind
of sharing and the ability to restrict
access and also the ability to tap into
the execution resources in an elastic
way so that's the kind of the final step
was before again I'm running this
notebook environment my desktop or my
server now I need to be able to have
these notebooks and then be able to
execute them in cloud resources just in
time so basically the notebook allows us
to have the notebook itself is the code
and then when I want to run it I can
instantiate some resources and attach
those resources and run and run the
notebook so at Oracle Labs we've been
you know noticing this trend and our job
at Labs is to notice trends and and
engage with them and and work on
technologies and products that that
finally feed into our product portfolio
so we think that this trend of notebooks
and notebook environments and
essentially environments that target
data scientist from code all the way up
to business business analyst is very
important and if I were to make an
analogy it's kind of like if you're an
Internet company or like Google the
importance of having something like a
web browser let's say you know or a
mobile offering it's extremely important
for for an internet company like Google
because they want to control the
experience of people interacting with
the Internet so it's same same thing for
a company that's focused on data and
data management we really need to invest
in in these types of tools so we've been
working on building our own notebook
technology our starting point was
Zeppelin the Zeppelin interpreter
architecture so we inherit a lot of of
their of their capabilities but we
focused on on the use cases in the
enterprise so first we start we built it
all using our Oracle jet technology
which is a JavaScript front-end tooling
technology that comes with a lot of
powerful visualizations Daniel
demonstrate some of them we focused also
on the enterprise so things like
security and the ability to control
access to the different notebooks and
sharing and then we've also worked on
integrating with various Oracle
technologies in addition to the open
source technologies so you can take
advantage of we'll talk about that later
our graph analytics graph visualization
and so forth we also are working at labs
on this next-generation language
execution technology called growl which
allows you to run in a polyglot way
multiple languages and have them
interoperate very closely together and
so that's also something we we've we we
use a notebook to support so with that
I'll let you know we'll show you how it
looks like and and
go over some of the features we give the
computer went to sleep unfortunately I
skipped over walnut yes ok yeah so and
then another another project we're
working on as the ability to buy the
ability to the ability to to to deploy
different kind of languages so walnut is
is the integration of gras which is our
next-generation language technology that
allows us to run multiple languages
inside our execution environments namely
the starting point is a database so we
are able to run things like JavaScript
for example now and and we actually now
deployed this on OTN so you can try it
out we can run things like JavaScript
code that you graph from NPM deploy to
the database and invoke it from within
sequel at performance in excess of their
performance we achieve with PL sequel so
for example so that's and that's also
integrated in your book so in a notebook
you can write a JavaScript function hit
enter it will deploy to the database and
then in the next paragraph you can
invoke it from within a sequel statement
okay are you ready online yeah okay so I
take over from you um I'd like to show
you in a quick few minutes the data
studio the Oracle of sailor suit that
we've been working on and I started
writing our own Java 9 interpreter so
it's called release last week with the
Jay shell so finally we have a poll for
Java Java 9 and this is actually the Jay
shell tool builder if you're familiar
with Java with the gel which has a few
more information few more powerful
features than the normal J shell API
such as it if you run in command like
one time version it will create a
temporary variable here it will give you
the result we're using Java 9 version
181 and then it creates these questions
that you can say reference later on and
it also has certain commands such as a
help feature and all these kinds of
things
so so what you see here is the data
studio and like us on earlier mentioned
we can we support multiple languages
right so in the first line of these
these code blocks these paragraphs you
usually define the language you want to
use and then what happens is if you
write a command it will send it to the
server the server will find the correct
kernel or interpreter which can execute
the code and then sends it back to the
to the client to for visualization if I
scroll down a bit to show you a few more
examples let me quickly go to this
example here so so another language
which I use in this notebook just for
demonstration purposes is the shell
interpreter so what I'm doing is I'm
downloading a file from the German
election I'm German yep
from last week and I don't want it set
into a temp directory and then I used
the Java interpreter to which from this
file I said maximum lines 51 and printed
it out so what you see here is we sent
this command to the server and we get
this back from the server so we get this
the columns separated by tab and then
new lines for new words and what you can
then do is based on this what we do is
we pass this information out of it we
say the first line is the header line
and then we get the different headers
and then we get the data assigned to
these headers so we see here it's the
header line it has area ID name state
registered votes so on first and then we
have this data block which gives the
values for this for the header and based
on this what we can do is we can take
this as input and then we can visualize
it in a nicer way like as an earlier
mentioned this is all JavaScript
okay JavaScript excitation toolkit
called jet or COO jet and it has all
kinds of different visualizations like a
table like a line chart like a bar chart
I can go through a few here so you see
like all kinds of customizations sorry
for this pop-up I'm not connected to the
network
and the other thing is what you can do
here I just wanted to highlight a few
things from the J cell actually from the
German ein shell which I kind of liked
I'm playing around with last week so um
I was played playing around and they say
the shell allows you for example to to
write a code which is here to write a
function for example it's called lazy
which returns a variable that hasn't
been initialized so it's not not yet
possible to execute this lazy method and
that's exactly what the J shell is
telling you here so when you execute
this it tells you oh I created this
method but however I don't I don't find
this variable so unless it's not unless
it's declared I can execute it and then
if you if you're under it tells you this
exception if you then define it
afterwards it also tells you okay I
updated the method lazy and now I can
execute it and then if you execute it
you get the value back and finally the J
shell also has some kind of history
function so if you run it the history
you see like what you've executed before
so this is just a quick example for the
J shell
oh yeah I can spend a minute on the
implementation of the gel so this is the
Zeppelin interpreter we supposed Apple
in and essentially what it takes as
input is just the command so whatever
the user types in and then additional
information like context information
that might be required in this example
it's not and all I'm doing is I'm
passing it into the J shell tool builder
read the output from the shell and
return it to the to the client okay for
the next step of the demo I want to
introduce quickly the graph technologies
that we are using so graph technology
basically the basic gist that we will
present data is a graph so you see here
we have a table where we have like four
accumulations and then what we can do is
we can have nodes and edges of these
technology so out of these tables so
what we know is entities become vertices
and then the relationships become the
edges and then we can capture these
interconnections between data entities
and and that gives a better insight into
the data so if you try to find out in
the table well I'm not some some person
is related to another person it's kind
of hard because you have to go through
the columns you have to see a match if
you visualize it as a graph it's easy to
see a connection between one node to
another node so what we did at our
collapse we were working on the graph in
an analysis framework called PGX our
graph analytics which is fast and easy
application for graph analysis and
datasets and it's already integrated in
a few products there was a talk on
Monday which also mentioned where we got
into more detail into it so the idea of
this graph analysis is that there are
two major approaches one is the one
approaches to compute
computational graph analytics so we
wanted one argue wasn't on it you want
to see okay
like patreon for example from Google
what is the most important pages what
other pages are referencing your page
and then the other approach is to graph
pattern matching we want to run similar
things such as sequel on graph
you wanna say give me this note which
has a relation to this edge into this
other note and in projects we flip our
both approaches and we also support to
write your algorithms in an own language
which I will skip because we don't have
time so quickly if you example it
because I'm mentioning it in the demo
afterwards about Picchu all it's a pet
imagine graph query language it's not
sanitized yet but we're working on it if
you're interested to read more about it
you can look into the paper from the US
2016 and there's a quick example of what
it looks like I can go to do some later
on so what you see is just similar to
sequel what we're doing is we select
certain properties from notes from a
graph and then the match gloss is
telling us okay how does the graph look
like
so in this example we have a person who
is related to another person who is
related to another person where these
entities by this friend relation so this
example is showing us the enemy of my
enemy is my best friend and then we have
additional work tells us where we can
say okay and in addition the name is
amber and she lives in New York and
she's older than the person we're
looking for
and what you've seen the first time is
recursive past query pattern and just
defining the the relation what we're
looking for so the enemy of my enemy is
my best friend which can be recursively
matched in the query later on I'm
skipping this okay so what I want to
talk about now is that an example of how
we use data science to to analyze
certain things so what I picked is the
stackoverflow firm if you're familiar
with it which is a huge programming
developer community and was just several
sub forums but I'm most interested in to
the Stack Overflow firm which is the
main source for developers to get the
information from and so what we're
trying to do is we try to find diff
topics like okay what what are people
talking about these days about
programming languages how did they
evolve
who should you talk to if you have a
question regarding a certain category
topic yeah who are the experts so that
we did is we looked at archive.org stack
of a flow uploads the data set once
every few months to a character arc and
we picked it and if you want to look
after them it's available it's dumped in
XML files so we had to do some
pre-processing and which I'll talk later
and then what what I want to show in the
second part of the demo is this is a
quick way to to model to analyze the
graph using topic modeling so I want to
look at the graph and then I want to
assign certain questions and text to a
certain topic and see how they relate to
each other I can skip this for now okay
so like I said the dystek overflow form
is an XML format and we had to do some
pre-processing so you have this post XML
with the text and the users so what we
did is we took the XML files we load it
into spark and then we created a graph
out of it and yeah and then in the lay
that light on the demo I will run an
algorithm on it which will generally
take the questions take the texts that
are assigned to the questions and try to
map them into certain groups yeah and
the structure of the graph after
extracting the XML files is that we have
users we have tags we have comments and
posts and posts can either be questions
or answers and then we have also
declared okay a question can be an
answer can be the accepted answer to a
question okay let me quickly jump into
the slide into the demo
so that this again is again is using our
data studio where we have here this this
markdown so we have a marked on Twitter
so you see that here we do is we pay
some markdown code and then what it
outputs is HTML and in the in the first
of what we did is we loaded the stick of
a flow graph which we previously talked
about into our engine into our PGX
engine and it has certain properties so
if i look into the next step every node
has a parent ID as a display name has
last accept answers or day its creation
dates and all kinds of things so to get
a quick overview over this graph what we
did in the first step is we use our
property graph query language to select
all the labels and labels describe a
note just like all the navels and see
the count of it and how many how many
labels are in this graph so what you see
here is that 68% of the graph our users
31% are post and then there's only 0.4%
which are text I think 50,000 yeah
50,000 text 3 million 3.5 million posts
seven point six million users and then
what we did in the next step is we
looked at the relations okay how do they
relate to each other and what what note
is having in relation to another note
and for this we use pgq Allegan where we
select the source called n in this
example it has an edge to another note
called m and then we group by both an E
and M so that we get that we can count
all the the relations so what you see is
that in this data set we have posts that
have attack to another okay it's a check
here which is 4.3 million and then we
have user that posts posts and post that
are answers of posts or I accepted
answers of posts since this graph has
nearly 50 million endnotes we did some
quick processing so we moved all
questions that don't have a Java Tech
since we had Java 1 and then we did some
time further pre-processing that means
we will move all vertices that are older
than 2015 because we're only interested
in the recent three years and we removed
all users that don't even have a
reputation of 10 so if users right
certain questions are answered if they
get points you started with one point
and if you don't contribute then it's
not necessary for analysis and then we
also remove the vertices that don't have
any relation like questions without
answers which reduce our graph to two
nearly three million elements I think
and after this pre-processing what we
can do is we can for example just see
okay what does what does the form look
like based on the text so what we did is
we just select all the text and group by
the tag name and count how many texts
are are in this form what you see is
that related to Java most people talk
about Android these days and a few
people talk about spring swing Eclipse
hibernate and so on that's a much more
visual way of seeing that information
right in conveying that information as
opposed to just showing a table and
people have to stare and you know it's
very easy can you show the table I mean
the result set is a table and then it's
just a matter of just you clicking the
word cloud and you get that you get that
visualization yeah so to continue with
just analyzing the graph before we did
do anything we just list the all the the
users by the reputation and reputation
is a property of the user so in this
example we we haven't filtered out the
non Java related questions that's why
John skeet here although he is a Java
developers here on top one I think most
of his post about c-sharp actually so we
see that if you have general questions
about programming we can we can go to
John skeet and ask him he knows a lot of
things and then what we can now do is we
actually actively just look into our
graph we search for the most active
users based on just written posts
so we have this PG girl Simon again
where we say okay give me the user from
this graph where the user has done
something with posts has written a post
here where the user has relation to
another post and then we count and what
that is is that we see that Erin for
example here has written over 3,600
questions or answers related to Java on
Stack Overflow yeah the different
representation here is just that we see
Erin have 3,600 answers and med program
or exam 2854 and then what we can also
do is we can look into the questions and
see okay what is the most what is the
highest go off a question what is what
does a question look like so what we do
is we select the title of a note which
is from type question which you see here
in the Mitch and then we are good by it
by a certain criteria for example the
dance account or the score or the view
count and what you can see here is the
syntax here which I highlight here
allows us to to have these dynamic forms
to interactively allow the user to
choose between different criterias here
which is useful if you want to hide the
code you want to share it with someone
else who has not it was not familiar
with programming itself but you can
change the criteria and then see the
results are calling me and that's our
initial foray in terms of bridging
between the data scientists the coder
that's gonna write these notebooks and
then they're gonna leave some holes for
more of these so you can quickly take a
notebook and turn it into a mini
application where you say okay I'm gonna
write some queries this is my you know
sets of quarries but I'm gonna live in
leave enough holes or give enough
selections so the user can essentially
not have to worry about writing the
queries can just select some things and
and while we don't talk about our road
map yet but road map is really about
taking that concept of okay well you
have no books there's gonna be some code
there but then how do we take those
notebooks and uplevel them and allow
other more business users to you know
interact with that artifact and so we're
thinking about having this concept of
pipelines pipelines are essentially
widgets that are connected
and the notebook one notebook can become
an instance of such widget but if the
notebook has defined input and output
then the notebooks can start being
connected so then you'll have people
that essentially can author these
widgets as notebooks and then connect
them so and then then you're bridging
you starting to bridge the gap between
more high-level business or analysts
that are more familiar with just a you
know connecting things together like an
init to a data pipeline all the way down
to writing code all the way up to people
just want to look at a dashboard and
just kind of select some some input from
some boxes yes yeah so we're working
with the with that would would would
those team so so obviously you know
Oracle is reorienting towards more
choice right we want to that's our whole
strategy we want to have people have
choice in our cloud be able to use both
open source
you know projects as well as some of our
own differentiated technology so we are
working with with with those teams to
also provide the data studio in that
environment so you'll have a choice of
having your simpler notebooks but also
converting those and what we're seeing
here is actually a layout if you notice
up top it says Jupiter so that's a
Jupiter layout of this notebook we also
have other layouts or Zeplin oriented
and so the whole vision is you're
starting as it may be you are starting
as a jupiter user will come in
you can have your jupiter notebooks or
your zeppelin notebooks and then if
you're interested you press a button it
converts into a data studio notebook and
it gives you some extra features like
all these extra visualization we have
right so that's really our our strategy
you know as is basically provide this as
an alternative
yeah the machine learning notebook in
not in the initial version but in
subsequent version the plan is to
include this functionality in the
machine learning notebook as a different
notebook type and I think that will also
evolve long term too even if there is
demand to have something like Jupiter as
well I see the I see there I see that
there will be plat platform kind of play
where the platform is allowing or store
these different types of notebooks is
controlling access you know who has
access to what notebook but then you
when you have access you open up a
jupiter notebook it will open up you
know in a jupiter instance right
and so we would fit in that platform so
i think i think you know there is this
notion of choice there's this notion of
not a single you know people are just
comfortable with different kind of
notebook and will offer over the long
term all of these options in one
platform so so ml is starting with
zeppelin but in subsequent versions will
have also data studio as an additional
choice not a replacement and every
interpreter they write in zeppelin on
and jupiter we can easily integrate into
our system so we person compatible with
interplay interpreters yes all these
things that you're seeing where he runs
the code it's a extension to the
zeppelin architecture so we can take any
zeppelin notebook easily bring in it so
the goal is if you have a zeppelin
notebook you should be able o imported
and executed in this environment yeah
so so take a pup so we already supported
tinker pop API in our product and
actually there we don't have an example
here but there's a PG the PGX shell
allows you to or the what is the DOP the
OPG shell allows you to interact with
our data access layer which is tinker
pop compliant right so we have a tinker
pop thing but these eql is actually much
more powerful than than what tinker pop
allows you to do so we have both again
yeah okay thank you I skip you through a
few sections here and I'm just going to
show some visualizations from a local
jet so what you can see here is what we
did is we we group by the users and the
post to a specific tag so for example
here if we say we group by text first
and then by the users by the amount of
answer they have given to the users we
see that if you have questions about
swing for example you go to chemic our
or mad programmer for example and we can
change hopefully the visualization I
don't know if that works offline to say
okay we go the other way around
now what I'm doing is I'm grouping first
by the username and then by the text so
what you see is here that chemicai which
I pointed out earlier he knows a lot
about doing but also about J tables for
example or J frames so every time you
you get the result you can highly
customized that you can say okay this is
my columns I want to show this all the
numbers I want to show the series I want
to show so what we also did is we edit
in graph is validation on top of it so
jet doesn't come with the graph
reservation yet so we've got our own d3
based graph is relation which in this
example shows you that there are two
questions because we have a limit of two
the point to the Java 9 Tech and I can
quickly go through it what you wanted to
do is we wanted to find the communities
within this forum so we want some
algorithms on it's like PageRank like I
described earlier and then we run in
foam app on it infom app will
essentially take the graph and see
whether or not there's if there's a tech
to a question with another tech could
also be related to another text that are
not necessarily on the same question so
you want to see okay if someone has a
question about hibernate for example and
someone knows is not about hibernate
what what else doesn't know about so we
wrote our relax map which I don't show
yet here and we run it and what what
relax rope is returning us is community
IDs since it cannot predict the name of
the community so we have the community
ID and when we for example look for the
tech named Android we get the community
seven hundred seventy two thousand and
then what we can do is we can see all
the text that and the same community so
I ran this before for Android and what
we get is we get the Edit Android
asynctask the recent screens navigation
drawer which are all components from
Android but you might not you might not
see it in the question itself and then
we add this I did the same with the Java
nine tag just to see okay what what do
people ask about if they if they ask
about Java nine and then we see that
people have questions about jigsaw about
Java module there's two some errors in
it like the high DPI which belongs to
Android and then finally what I want to
end with this is just try to find
experts in certain communities certain
topics so again we use particular for
this we asked for the users that have
knowledge about the topic ID 360,000
which i think is hibernate in this
example and we see that dynam from the
Stack Overflow community answered three
hundred ninety two questions about it
and if we look into this and he may not
directly answered something about
hibernate but he may have answered
something about a related topic to
hibernate because that's that's the
point here yes I think actually it's an
example about spring so if I look into
if I asked for the username again and
the topic ID I get the list of questions
he answered and the red text so we
answer two hundred twelve questions
which edits Bing tag but you also are
so like seven questions with certain
outer wire attack on the dependency
injection text so you see essentially
just by running this infinite algorithm
you see you get different groups and
then you can easily point out okay these
texts belong to these texts even though
they are not in the same question and
this can also help you if you deploy
this as a program to alter assign
different text that are related to this
tag or help you for the search yeah and
then finally I just point out a few
questions that he has answered in this
community which is like how to disable
spring security the connection of the
database dies or the start thread at
Springwood applications that you know so
okay great so so yeah so with that that
ends the nice the top portion I think we
have a little bit of time to take some
questions so we're we're going to
continue investing in this we're really
interested in more enterprise use cases
so a few guys are see this as useful
please get in touch and we're happy to
work with you guys on this so thanks
again for your time
yes yeah so we're we're we're almost
there please leave your your card with
us so we can email you we have basically
just going through legal approvals where
we're done with that and now it's just
kind of a final packaging and released
so we'll make it available on OTN it
will be in more of a developer oriented
unsupported version because that's what
we put our labs artifact that but it's
also making its way in various products
like we mentioned actually the first the
first product where this may show up is
in our graph cloud service which we're
working on which will be available in in
in somewhere next calendar year
basically so we'll have we'll have this
this is also being built as a as a
platform in a sense that we are putting
extension points so another key partner
of ours is a financial GPU the financial
global business unit and so they're
taking this and and essentially
rebranding it and
enough extension points and we evolved
this to have enough extension point so
that people can essentially create their
own distinct studios or data studios
based on these using this as a building
block and we're gonna actually keep
working on this and at some point we
will have an SDK where people can
essentially once we have widgets create
their own widgets and kind of create an
ecosystem where partners can can add
their own functionality into this stuff
yeah so we already have integrations
with cytoscape irrespective of this so
our graph technology integrates with
side escape or we have a side escape
plugin we also have been working with
Tom sorry time sorry supports our or our
RPG X technology and our graph
technology and and there are already
customers that are working on deploying
Tom Sawyer and combination with our
graph technology so that's already kind
of working ok any final questions all
right thank you guys for your attention
and your time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>