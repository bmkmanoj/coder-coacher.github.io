<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Writing and Testing High-Frequency Trading Engines | Coder Coacher - Coaching Coders</title><meta content="Writing and Testing High-Frequency Trading Engines - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Writing and Testing High-Frequency Trading Engines</b></h2><h5 class="post__date">2015-06-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/iINk7x44MmM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right Who am I I'm an Australian
living in the UK father of three I have
a recently popular blog called vanilla
Java and I'm third or four java of tag
on Stack Overflow for answered questions
and effective ones have more questions
on Java than anyone else and I've spent
the last six years designing and
developing and supporting high frequency
trading systems trading systems for
hedge funds and for investment banks are
now working as an independent consultant
okay
so what are the key drivers that you
have in a high-frequency trading system
the key drivers in a high-frequency
trading system is you have something
that's very event based you have an
external source of information usually a
market or a number of markets or clients
and you want to deal with all those
advanced as quickly as possible and so
you inevitably your core training is all
part of your system is an event-driven
system some of the key things that you
want to make sure your event-driven
system is that your producer is not
slowed by your consumer a lot of systems
where there's a slow consumer it's
acceptable to bank up the produce it may
be acceptable to bank up the producer
but in this case you can't you can't
tell the market off can you slow down a
bit I are having trouble keeping up and
it's very important to be able to record
everything that you're doing not only
from a business perspective to see
whether you really did do a trade or why
you did a trade but also when why why
did it take as long as it did in
particular because unlike a lot of
systems if you run a little bit slow
that's okay it's not good but it's not
bad too bad in in a high-frequency
trading environment you don't want to be
running slow at all you just want to be
running if you're slow if you're not
fast enough you will be losing money
you're better off switching it off so
classic example is that in it's quite
standard in a lot of enterprise
environments to have a second data
center and in the second data center you
take over if your primary fails but in
high-frequency trading that's not
acceptable
your second data center may be too slow
which case there's no point trading in
your second data center you're better
off not reading at all because otherwise
you would just be losing money another
thing is it's kind of different is that
a lot of lot of systems horizontal
scalability is a key way of scaling your
system because generally the issue is
high throughput you have lots of
independent requests and you need to be
able to deal with those requests as
quickly as possible but you want to be
able to scale horizontally so that if
you say for example you have twice as
many servers you might have twice
handled twice the load but in low
latency it doesn't work that way if
you've got twice as many service you
don't get half the latency you usually
get like three or four times the latency
so you cannot scale as easily
horizontally instead what you need to do
is you want to reduce the what you're
doing you want to make the systems as
simple as possible so they're doing as
little as possible so their latencies
are as low as possible
another thing differences in
productivity a lot of systems and you'll
hear this talk about particularly
higher-level languages is how easy was
it for me to add some code but in
reality for low latency it's how easy is
it for me to remove things it shouldn't
doesn't need to be doing so in fact is
no point adding something if it makes it
a great deal slower and you can't then
take out anything that's redundant in
terms of what it is doing so in fact
it's much more important to be able to
understand everything that's doing so
that you can take things out
okay layering also traditional design
principles that you have multiple layers
in your application each layer deals
with a single concept or a small number
of concepts and this allows you to
abstract out and deal with separate
concerns at separate layers and this
makes it easier to test and develop and
also to reason about problem is that
adding all these layers adds latency and
also adds complexity you often lose a
lot of your opportunities to simplify
what the system is doing by adding
layers so again in a low latency system
you tend to put in the minimum number of
layers the layers are still good but you
have to be very cautious about how many
you put in you want to keep them as
little as low as possible and often it's
typical to see a very short call stack
so you might want to have a call stack
of less than 10 or 20 for the for the
entire that the deepest ever gets where
say you might have in Tomcat server you
might see something closer to maybe even
a hundred that would be inconceivable in
a low latency system it's also about
taming your system your system is there
to do a job and generally it's very
specific as well you're not interested
in sharing your workload with any other
processes your server is there to make
money for you and it's not there to be
nice to other applications it is there
to do its job as best as it can and
that's it and so it basically takes over
the system you're not interested in
virtualizing your system you're not
interested in playing nice with other
processes so for example you're critical
cause are usually busy waiting they just
run 100% all the time they never contact
switch they just run on the same CPU
that you've dedicated to that thread
that's it it's not there to do anything
else because that's the job of this
machine and in particular busy waiting
in a low
latency contacts can improve performance
by two to five times so that's a
substantial improvement by using busy
waiting the reason why busy waiting
helps is you're not giving up the CPU
you're not clearing all your caches and
your instruction caches and all your
branch prediction every time you deal
with a request a good way of slowing
down a benchmark is just to sleep for a
millisecond between tests you'd be
surprised how much slower not not only
does the millisecond add a delay but all
the code after that millisecond runs
substantially slower and measurably
slower because if the caches aren't
warmed up instruction cache it the CPU
does a lot of tricks with the sisk
instructions to turn them into RISC
instructions and all of that is lost so
yeah you'll be surprised how much slower
a small test can go if you sleep for a
millisecond in between doing tests again
you want to keep you want to keep the
locks to a minimum ideally love free
coding that's not always practical but
you want to keep them to an absolute
minimum and again if you've got a busy
waiting thread single-threaded there
isn't anything for it to share other
than the messages coming in and out of
it so there's no need for those those
critical threads to have any locks
anyway and also it's useful to make use
of direct memory structures so as
opposed to using the heap for for your
bulk of your data you put the bulk of
your data off heap and you run that
continuously through memory so you don't
have headers which add basically
overhead to the how well your caches
work but also you can control your
layout completely and so you know
exactly where all the data is all packed
together that can also minimize your
garbage and we'll see some examples of
that coming up okay a latency profile so
what is a typical latency profile of
what I consider a well-engineered system
looks like well a latency profile is
basically you take the timing
of how it handles each event you sort
them and the one in the middle is a
typical or median your average is
usually about 20 to 30 percent higher
than this then you have 90th percentile
which is your worst 10% it's the top end
of your 10% the 1990s your worst one
three nines is your worst one in a
thousand and four 9s is one worst one in
ten thousand and as you can see this
goes up quite dramatically
you will get and so one way of was some
some people have asked me how how did I
get into high frequency so what I did
was I would instead of measuring
averages I would measure ninety nine and
three nines so I get the system to do
what they asked for but on the three
nines so in fact the system was running
ten times faster than that were they
asked for but that gave me gave me an
excuse to tune the system far further
than they had originally perhaps had in
mind by by reporting the three nines and
then on the next project did the same
and so on eventually stepped up to
projects that are in the you know 10
micro second type range I think I
covered all that so yes so the short of
this from the graph is that the worst
point one percent is you can expect it
to be about ten times your typical even
in a well-tuned system if you do this
graph and then and often in are not so
well chain system you will find that the
three nines is far worse than this
it can be much more than ten times could
be hundred or even a thousand times and
I actually will in my demo be showing it
on this laptop it will be worse than ten
times because this laptop isn't quite
it's not a really low latency system so
this is from a real trading system that
I developed five years ago and my
approach for reducing the impact of GC
was to produce so little garbage
that the ebin size was larger than the
garbage produced in a day and the reason
this was acceptable was that we only
traded we traded less than 12 hours a
day so in the middle of the night I
could take a hit of a full GC but then
run all day just slowly filling up the
eating space and as a result of doing
this of course there's no collections
snow mining collections there's no full
collections
there's no GC pauses to worry about so
that's not even an issue you might think
you know GC pauses that means there's no
jitter not at all your systems actually
can experience an enormous amount of
jitter that's got nothing to do with the
garbage collections lower level Jabba so
is something so then the obvious
question comes up if you're doing all
these things should you really be
writing in Java at all and the answer is
I believe so because you can use Java
this way even if it's not often used
this way and the nice thing about is it
integrates very well with natural Java
now the thing is you don't want to be
writing all your code in this very
contrived way you want to be writing you
want to be reusing libraries that
weren't written with this consideration
you want to be using libraries that just
pretty much off the shelf and you don't
want to be constraining itself and in
fact in this previous example this was
running in carafe with I POJO so there's
a note it's running in an OSGi container
and at least half that garbage is from
the OS giant container itself but but
that's okay so so even though you can
write your core systems in a low
low-level way you don't have to write
all your code and generally my I think
you should expect to write maybe 10% of
the actual code run in a very low
latency and efficient way because 90% of
the time you will spend it about 10% of
your code which means the rest of your
code can be run in normal Java and
integrates very nicely with low-level
Java
okay so what sort of latency reporting
should you do often we see people report
or maybe averages and averages are very
nice but really all they are is often
just an inverse of the throughput which
it gives you a fairly optimistic figure
the also you see standard deviations
unfortunate thing about standard
deviations is that it assumes a normal
distribution and as you can see this is
nothing like a normal distribution and
you can get Layton sees that according
to the standard deviation model
shouldn't occur at any time in the
universe life or the universe at any
point so it's just just nonsense so
standard deviations give you a very rosy
picture of what your latencies will
really look like so what you need to be
looking at is your nines I don't really
take too much notice the typical a
ninety actually look at the two ninths
as a starting point as I mentioned
before three nines is what I'd like to
be has a good figure and you also want
to look at what is the worst in your
sample you can't measure the worst
because you need basically an infinite
number of samples to ever determine what
your worst would be but you can at least
report what was the worst you got in
your own particular run
I forgot what the point of this one was
other than what it says on the screen so
so often if you get disturbances in your
latencies this doesn't show up in
through ports and it doesn't show up in
averages either but when you start
looking at a histogram or your latency
issues you can see real issues oh and
the reason for why Golo garbage now most
people think of again the most important
pauses for is to worry about is the GC
pauses so you might think that oh well
if you don't have any GC pauses you're
done you know there's no there's nothing
to worry about well the thing is that
actually you don't want to be filling
your caches with garbage either so even
if you're not pausing and pausing isn't
an issue for you your code will run a
lot faster if you can stay within your
l1 cache your l1 cache is about three
times faster than your l2 cache your l2
cache is four to seven times faster than
your l3 cache and importantly your l3
cache is shared so if you have multiple
cores and you want them to all run
concurrently without getting in each
other's way you want them to stay within
the l2 cache the significance of this is
if you're producing 32 megabytes of
garbage a second which is not an
unrealistically high figure a lot of
systems produce more you're actually
filling your 32 kilobyte cache with
garbage every millisecond all right so
if you want to go and access something
that you access to millisecond ago it's
gone because of the garbage you're
producing and you actually find that
once you reduce your your garbage your
produce you speed up the operations that
won't even slow down by GCS and you can
get a substantial improvement in
performance
sure
there's there's an there's two l1 caches
one for the instruction one for the data
the garbage obviously doesn't have any
effect on the instruction cache it's
only has an impact on the data cache so
yes so there's two 32 kilobyte caches
and that's shared for the whole core so
both CPUs so if you've got two CPUs
being used on the same core then in fact
they've only really got 16 kilobytes of
private sort of private cache so again
you you're critical cause you may want
to say I'm not gonna share this with
another CPU I'm going to effectively not
use hyper-threading I'll just use one
CPU out of that core and then that all
the resources that core available to
that thread it's often recommended that
you explicitly turned it off and in fact
for if you're doing very intensive
floating-point it's generally considered
that you should turn it off I don't
because often I'm working on machines
where I don't really have what I
consider an unlimited number of course
so what I find works well is you you
have hyper threading on but you just
don't allocate anything to the second
half of that core so you have one power
busy waiting while the other ones idle
or just not running anything and the way
you do that is you isolated the core
using ISO CPUs in Linux and then you
just don't assign anything to it so
effectively that core is only running
one thread which is the one you assign
to it and then for the rest of them like
your junker CPUs anything that might be
doing everything else that can be hyper
threaded I purposely decided I don't
care too much about what they are doing
sure
okay so it gives us some examples later
of what lower level Java looks like but
basically low level jet by low level
Java I mean things like your recycling
your objects rather than letting the GC
to it you're not using things like big
decimal using double or long you're
you're avoiding doing a lot of the
things which are considered okay in
natural Java like creating lots of
objects and creating strings and cutting
them together and producing a string
buffer you you're actually creating far
less objects then you would naturally do
if you're just writing normal jar what I
call natural Java no its promote that's
right you're you're limiting your what
you do to a subset of the api's and the
subset of the code which will which runs
fast and then you don't use anything or
you avoid using anything which runs
slowly and that means actually avoiding
things like like file dot last modified
now you look at file dot last modified
you think oh well that's that takes no
arguments and returns are primitive that
shouldn't be producing any garbage
well actually unfortunately it does it
it takes the string you have it the
first thing it does is call a native
method but in the native method it calls
back into Java and converts the string
in the path using utf-8 encoding or
whatever the platform encoding is
creates an encoder for that turns it
into a new byte array then passes the
byte array to the actual underlying call
and then checks the last modified so if
you're lucky you might get about four
objects created from a from a method
that looks like it shouldn't be creating
any so that's a method you wouldn't pull
too often and in fact in the reason why
I bring this up is in the OSGi container
i had to change the directory watcher so
that it wasn't polling
things like last modified and sighs and
exists on a regular base quite so often
because that was producing at one stage
it was producing twice as much garbage
as everything else in the system so
ended up having to change that I have
some examples of what I considered so
low level Java later okay so recycling
is good this is one of the things that
they generally don't recommend you do
because what having objects that live a
long time and then die in the Tanyard
space is a bad thing but if you do this
approach you recycle the objects you
keep them for the life of the
application they never die which case
that's taking them for a very long time
and so you just keep reusing the object
so you pre allocate your objects you
perhaps create a few more objects on
startup than you need but you reach a
steady state hopefully in your warm up
on your startup before you even start
running your system and at that point
there's just no more objects to be
created or cleaned up another thing to
avoid is the kernel the kernel generally
slows you down in a lot of ways and it's
very common in these systems to use
kernel bypass network adapters if you do
IO via the kernel you can be looking at
delays of maybe 30 microseconds which
most of the time that sounds quite good
but with kernel bypass you can get it
down to two or three microseconds and in
the overall scheme of things that can
make a very big difference for a low
latency system isolating busy waiting
CPUs can make a big difference in terms
of the amount of jitter and I've got
some examples of that and another thing
I do is use memory map files for storage
the benefit of that is that's just a
memory right so you just write out
it just goes to memory there's no OS
call may
if your application dies at that point
the data will still be written to disk
provided your OS doesn't die so here's
an example of jitter so these two
programs all they're doing is polling
nano time so I've got this little loop
it just keeps calling nano time over and
over again and calculating the
difference between the nano time calls
and then I count how often that delay
between Cole's was greater than one of
these divisions and as you can see the
bounded one it was still getting even
with busy waiting in isolation it was
still getting delays of ten microseconds
between one call to nano time and
another quarter min a time and it was
occurring and this is per hour by the
way so is occurring roughly thirty times
an hour so every two minutes however you
can see that if you just busy wait and
you don't isolate your call you get a
whole range of delays going all the way
up to above two milliseconds and in fact
if you don't busy wait you actually get
some in the five millisecond range plus
range that's between consecutive calls
to nano time so this is without GC
pauses without cache misses I think this
is just the scheduler and interrupts in
your system that you can turn off
disturbing your your CPU and in fact you
can see there's a massive spike here at
100 microseconds and that's the minimum
time unit so what's happened is that the
processors be threads been in stopped
and then in the very next time units
here oh you're still busy I'll I'll wake
you up again and this is happening three
times a second so you get substantial
jitter in your system just from the way
the scheduler works and non-maskable
interrupt what free coding again one of
the problems with locks when you start
to look at your two nines and three
nines your yo is that there's there's
this you run into more pathological
cases where your locks
really slowing you down or that causing
the thread to stop running and give up
go back to the system then come back
again and all of that whenever you go go
to the OS your thread goes to the OS
then comes back again notice that
delaying that call but everything that
that thread runs for quite some time
afterwards will be substantially slower
than it would have been so it's it takes
a long time to wake up again and sort of
get up to speed yes so another thing I
recommend doing is avoiding using
bigdecimal there are basically most of
the operations you can do in bigdecimal
you can do in double as long as you're
happy to have about fifteen or sixteen
digits of precision and in almost every
case even the u.s. national debt can be
represented without confusion and that's
just an estimate anyway because no one
knows precisely what it is at any moment
so to the sent I mean so yeah so you can
do it what what a lot of training
systems do is they use long instead so
that instead of representing dollars
they'll represent cents and that becomes
an integer and it's a long now that has
its own headaches because you have to
remember that oh well this long has two
fixed decimal places this one might have
six another one doesn't have any hard
way you know when I do some operation on
these like multiplying them how many do
end up oh I have eight now or is at
least with double if you can get there
to work it handles all of them and you
just need to use appropriate rounding
whenever you use the value so what
objectives do you want from a low
latency but library you want want it to
be as lightweight as possible you wanted
to do only what you need it to do and
nothing else
you make full use of the hardware so
that work at the limiting factor is just
the speed of the machine you using and
the performance characteristics of the
applique of the library is a key
requirement of that library and what I'm
leading to is a whole bunch of libraries
that I've written for different areas
they're all open source first one is to
do thread affinity so this is the thread
binding that I mentioned before you can
just google these the job that's used
quite quite a few people use this this
Java Chronicle which is getting more use
that what that does is it allows you to
persist an IPC between processes in a
very low latency way and if you don't
want the persistence and you just want
direct access to memory you can use Java
lang that allows you to create massive
regions of memory of one client that's
just creates a 70 gig continuous region
divides it up puts a lock at the start
of every notional record
so there's thread safety operations in
there as well for for native memory and
the last one which is more cool than I
use it but I haven't heard anyone else
use it where you just you can generate
code on the fly and it will compile it
and give you back as a class so you give
it a string it gives you back a class
and compiles it for you
the one of the things which is nice
about it is if you're running in a debug
mode it will write it out to a directory
you specify in the right place so that
if you step into this code you generated
it will just magically be in the right
place and the debugger will see it so
you can step into generated code and
that can be kind of handy when you're
trying to debug it because
debugging generated bytecode is a
nightmare
which ones that sir
the third one typically they are fixed
rank length records so Chronicle is not
fixed length it has an index but
typically they're just fixed length
records they tend to pad them out the
records just to make them simple to work
with and the reason why this doing the
work yourself works at all
is because you're dealing with large
amounts of data that are actually quite
simple they are very simple life cycles
they have simple sizes very simple
structures and that's where managing
yourself can actually work if you start
to get into complex data structures with
complex life cycles it's the amount of
work you involved and testing involved
just goes up by order of magnitude
compared to writing in natural Java and
then at that point you sort of say well
is it really worth it but at this point
if you've got a very simple data
structure very simple data simple life
cycle then that can work very well so
the purpose of Chronicle one of the
things that's key is that it's designed
to allow you to log everything now and a
lot of perform even non-performance
systems that a very common pattern is
well I will write the minimum of logs
because it slows down my system all
right so you tend to turn debugging off
because that will slow you down even
generating the debug this will slow you
down so you have some sort of checked
I don't generate my debugs unless I'm
actually going to write them whereas
what this is designed to do is to be so
fast at persisting but that is no longer
such an issue and the sort of rates that
you can persist that are in the tens of
millions so we're talking about between
say 100 and 300 nanoseconds on a decent
box to to log a message and there's
often very small compared to what the
rest of the application is doing and so
then logging everything such as lots of
time stamps to work out the exact
timings of what happened
to a particular event or all the details
of the event become practical so yeah so
very large messages can be up to 10
microseconds but usually you don't need
to love those there's usually some other
way of condensing the data so the sort
of performance you see between 1 &amp;amp; 3
gigabytes of data in bursts and
obviously you can't sustain that because
you're you rarely have a disk subsystem
that's that fast and in fact you don't
need a disk subsystem that fast because
as long as it's just a burst of say 3
gigabytes then you'll have periods of
time when the disk system can catch up
so here's an example of the code this is
for writing out text so so this example
is from a unit test to have it's going
to write out 10 million lines of
date/time with an ID just made-up ID
which matches the index the entry
another ID which is also made up and
then I finish it and writing this and
reading it again on this laptop takes
1.7 seconds that's to do 10 million
entries here's another example that's
from a unit test and one of the things
to note because we have guilty ninh here
is that it avoids a thing called
coordinated emission by it doesn't
actually write out the time stamp it
writes out the time that that entry
should have been written so if there's
any delay in the producer that's not
going to be in your favor instead it
takes a more conservative time stamp
which is the time that I should have
written that entry not when I got round
to it but you can see here for example
in just busy weights and one of the
things that's potentially unrealistic is
that it spaces them out very
even me although a lot of trading
systems this is still the case because
to get this data you have to get it from
a source and that source will only
deliver data to you at a certain rate if
you have a one gigabit line for example
you're only going to get a hundred
megabytes of data a second in so you can
only do so much with that so your data
is going to be fairly evenly spaced out
anyway so reading this entry looks like
this again we're busy waiting to get
each entry and then when you do get an
entry we read the data back in again it
puts the actual timestamp and then
there's a third process that looks at
these compares the timestamps and gets a
latency distribution so this is the sort
of numbers you get on this laptop and
what's interesting here is that you're
seeing a small percentage of very long
delays 100 microseconds for half a
million a second and this goes up quite
dramatically as soon as you go up to a
million a second so now it's it's it's
not happening that often but enough that
it's notable double it again and
suddenly it's getting up to 2% and you
went by the time you get up to 5 million
at messages a second 21% of messages are
being delayed by 100 microseconds so you
see you can see that your jitter can
ramp up dramatically but the thing is if
you look at just your averages this
average looks fine right I wanted to do
five million in a second I defied
million 1.01 seconds if you looked at
that number you think that that's pretty
good then you look at the jitter
distribution and it's actually enormous
right because at five million a second
on average it was taking point two
microseconds which sounds brilliant
point you microseconds the twenty one
percent of the time the message has
arrived hunt over a hundred microseconds
delayed right so this is we're looking
at the distribution of your latencies
isn't
because they can show up problems that
you just would never see with just a
plain average or plain throughput test
obviously if you're not running this on
a laptop you get better results but even
half a million is pretty good to give
you some context opera feed a burst rate
has 120,000 events a second and opera
feed is one of the highest rate feeds of
any market data in the world and you
wouldn't necessarily run it all through
one thread okay so this is just another
example of another thing that's
interesting is that it obviously there's
there's a bit of overhead in just the
fact that you're using a real file
system if you don't use a real file
system use tempo fast you can get higher
throughputs but again these sort of
numbers are usually much above what most
people need this of course if you run at
this rate your latencies will be
terrible you will see enormous spikes
where it'll catch up and then it'll
drift off and then it'll catch up and
drift off at at 30 if you do thirty five
million a second on this laptop which is
what it can do no sorry this is another
box if you do thirty five million a
second on a server which is what this
can do you will see latency sometimes
ranging into the seconds so if you're if
you want something that's latency
sensitive you obviously don't want to be
driving at that kind of rate though this
is very low heat so this is running at
30 with 32 Meg heat right so we've got
one more library here is affinity I've
shown you before the chart that shows
how much difference you can get by
reducing the jitter if you isolate your
CPUs and Java affinity library makes it
easy to describe your layout so you can
say I would like to have
I want to dedicate whole core to this
thread and I want this other thread to
be running on a different socket or the
same socket or you can say I want it to
be on the same core as this other thread
so you you describe your layout that you
want and then it looks at the CPU
information and then says okay I can put
it here here and here
as mentioned before java.lang is the non
persisted version taken out of Chronicle
it wraps unsafe to make it safer to use
without losing too much performance and
it supports a very large memory spaces
you can allocate continuous regions are
much larger than 2 gigabytes and I've
clients who do it's also designed to be
very low-level serialization
deserialization and that's designed to
be G syllable GC list so you can
serialize and deserialize objects
without creating any or where creating
objects is you've not necessarily option
you can use object pooling to reduce the
objects being created this is a runtime
compiler what what that's actually used
for is if you want to low reload
strategies on the fly and while the
system is running you may have a DSL to
describe your strategy but ultimately
what you want to do from that is
generate some code that implements that
strategy as efficiently as possible and
you can do that as a part of a your
release process for your strategy or
what you can do we've done in the past
is actually just have a DSL that can be
loaded in it run through this it's
expensive takes a couple of seconds but
then once that's done you've got a
completely generated piece of code that
just runs through and you know it can
run as very efficiently
so do I advocate use Chronicle for
everything not really because it is very
low level you're getting down to every
bite it's very fast but it's quite
tedious to co2 and generally you want to
hide away all of this so here's an
example which I'll be running it has
we've got two gateway processes they're
both writing to a chronicle there's a
processing engine that pulls those jobs
off processes those events rights to
another Chronicle which is in turn
picked up by the gateways and any other
systems that need to monitor what this
system is doing there's an interface for
messages coming in and an interface for
messages coming out so in fact what
happens is that you've got two
interfaces the metadata here describes
the entire order profiling and timed
history time stamps which messages it
went through any details you want to
record about the event and then there's
the actual commands themselves which
could be objects these are objects that
are recycled so if you want to retain
any data out of them you have to take a
copy but if you don't need to do that
you can they can be reused again and
again and again this is a very simple
long life cycle the object exists you
call the method by the time the method
returns that object can be reused so so
as a processing engine looked like it
implements one method in this case that
takes the arguments it creates a report
based on the argument and then sends
back an event with the same metadata
object but of course appending more time
stamp information and sends back the
report okay so now going to run it
praise the demo glance
okay so I have a PE mein which is a main
method a main process that implements
the processing engine and fire that up
and you may even hear the fan starting
because it's busy waiting on one of the
CPUs now this I've added the base GC now
one of the things you'll notice that the
heap size is 32 make and I've put
reverse GC which is not very much
information but it's enough to see when
it does a garbage collection and as you
can see it's done I've triggered a full
collection on startup and then that's it
so now I'm going to start up one gateway
to drive messages going through the
system and there's one thread for
writing messages and another thread for
reading them their independence so that
they're not influencing each other and
it's sending it through at a rate of
half a million a second so the Gateway
is writing the message it's being picked
up with a processing engine writing back
a message and then being picked up by
the Gateway at a rate of half a million
a second and as you can see it's
persisted all the messages in and out so
it's actually persisting a million
messages a second to do this and as you
can see the throughput was good but also
the latencies are actually surprisingly
good cuz you don't always get numbers
this nice actually so we got the
round-trip time was 0.6 microseconds 90%
of the time it was 0.6 it was actually
quite flat and then the start to see the
higher nines coming up and the worst one
I got was a hundred and six and I'm
pretty sure if I run this again it can
be closer to a thousand but so I got a
good run this time if I run this on ace
decent box on a server it's much more
consistent the PE main what did it look
like yes the PE main just gave me some
updates and there's a slight pause there
at a hundred thousand because that's the
warmup and as you can see neither of
did a GC with a 32 Meg heat so it's gone
through 10 million messages either way
no no collections no so what it does is
it uses Chronicle which is used a memory
map file so the memory map file is
shared between two processes one process
writes into memory the other one just
picks it up and then reads it from
memory and then it does the reverse kind
of going back again so this is how you
get both persistence and IPC in one
you're not reading anything that hasn't
been at least notionally persisted to
the OS and and the kernel isn't involved
kernels only involved where a new page
which can happen quite often it's not
too much of a hit where you do get hit
particularly in earlier versions of this
library is where it has to map in a new
block of memory and that was
particularly slow but in the latest
version that's been that that's actually
done in the background now and get much
lower delay it happens asynchronously
and it's generally quick the thing is
that randomly it's very slow and that's
what part of the reason why you don't
want to be doing it in process is that
randomly you will get massive delays
into the milliseconds even with a
machine with SSDs so you don't want to
see that impact and so essentially what
happens is the buffers just fill up a
bit more than they normally would but
then they drain quite quickly to solve
that
no there aren't any locks it does
thread-safe operations in terms of doing
a put ordered int and also get volatile
but apart from that there's no locking
going on basically what happens is that
it writes out your data entry go back to
the code
okay so what this does is that the very
first line reserves some space the
significance of this is that it decides
that that that much continuous memory is
available then it writes in some data
which is just pure right straight out of
memory if each one of these turns into a
single machine code instruction yes all
of these turn into a single machine code
instruction and then finish what it does
is it goes and puts it into the index
because as I said Chronicle is indexed
which means that there's an index
allowing you to have variable length
messages there's only when the index is
updated is does it go and check the data
so when it's doing the reading it does
the flip side of that it's busy polling
the index waiting for some day to come
in right so to start with it's zero so
keep saying that all zero zero zero is
still zero zero zero oh there's a value
there right now because there's a value
there there must be some data so it goes
then it reads the data only after
checking the index first yes it is so
it's it's yes it's atomic it just does
writes an integer
does it put does it put ordered int in
some unsafe in reality
I've never tried the client much to be
honest but I would expect to see
significant differences actually because
the client mode doesn't compile the
codis aggressively and but unless you're
running on 32-bit Windows you probably
wouldn't need to use the client mode
so what else safe to say okay so here's
an example the bottom line is what
actually get on a I 7 desktop and as you
can see the numbers are a bit faster but
also quite stable I was going to say
that the 77 is a lot better than what I
get in the laptop but for some reason
that run was actually good and it was
only 106 this is something I ran earlier
this is a 600 which is a bit more
typical of this laptop and so so we get
down to the questions yes
there the question is what is there
what's the benefit of switching from
long term double to long the main
benefit is not so much performance it's
the fact that you don't have
representation errors to worry about
because with double you cannot represent
point one exactly and this can lead if
depending what you do with the
calculations can lead to cumulative
errors that become undesirable whereas
at least if you use long few
calculations you know you won't get any
representative representation errors and
you won't get any cumulative errors so
that's that's the main reason for
switching to long as an alternative to
say using big decimal now interestingly
enough Java seven big decimal will use a
long instead of a big integer if a can
and that improves the performance a bit
but it's still substantially slower than
using double or long
yes and that may even be a good thing
because it's shared between the FPU is
shared between your CPUs on your core
but yes so you can get mixed results as
to whether using floating point along is
faster but I don't think it's consistent
across all architectures I don't think
there's a general rule you would need to
test it to find out what is fast to feel
or platform yes yes yeah and the thing
is with the high availability is that
certainly in a high-performance system
risk is a general part of what they do
day to day they're much more aware of
the risk both in terms of business and
IT and often they're inclined to say
we'll just one one box and if it fails
we'll fix it just leave it at that in
some environments they want to run two
systems concurrently and both active but
if they do they're still going to be in
the same data center because you don't
want to be further away from the
exchange because it's just not worth
being further away than the exchange and
so for example Chronicle supports TCP
replication for that reason is that you
want to be able to take you you know
continue from where you left off the
problem is it depends on the failure
because if your system actually fails it
could obscure the last few transactions
you did or take them with it so in that
case you often have manual intervention
you ring up the exchange and say well my
last trades what's my position and so on
so you continue on from there but it is
often important not just to just
continue IT there's usually some manual
intervention and some coordination with
whoever is managing a risk externally
yeah so you always care about the
latency which of the other technologies
I'm thinking a flight recorder yes well
I would consider flight recorder but
haven't actually used it myself in
production so yes it's one I would
consider what are the technologies well
I use trove fairly routinely
for collections of parameters I tend to
dabble with the job Ellucian stuff quite
a bit that tends to get used either
things like some some of my clients use
struct some use some of the parses that
they have although generally what
happens is they end up writing their own
custom parser but to start with it's a
java Lucian's good place to start for a
lot of those things they have
collections that recycle objects as well
things like fast map and fast lists they
work by recycling their entries quite a
bit
yeah yes you would print compilation
that's actually more useful than the GC
logging because the problem is that GC
locks are buffered and if you're only AG
seeing once a day you may not see any
output for a couple of days so it's
actually tells you that something's
wrong and then it's not like it doesn't
happen because you might put something
in your code that every so often is
creating an object or your say for
example you're failing to get a TCP
connection every time you fail to get a
TCP connection it creates an exception
go to create a new socket object and
before you know it you can actually
trigger a GC collection just because of
things that were outside your control so
these things can happen so it is worth
monitoring those things as a diagnostic
that something is wrong but and print
compilation generally I do that more in
development just make sure that all the
critical methods have warmed up by my
warm up code it's not something I've
ever monitored in production in terms of
Diagnostics as you can see with the
metadata the whole purpose of that is to
trace the timings through the system so
rather than use some sort of external
profile that does in a generic way I
know my critical path and so I want to
put key timing points and all that all
the way through the critical path when I
write and read to each Chronicle for
example in any key stages in between and
so when it gets the very end I have a
list of time stamps and that is often
hopefully associated with a business
object the significance of that being is
that when you get fills or messes you
can then tie back to the the run the
time stamps that contributed to that
fellow misses very easily because it's
all together in one place so you're
doing a lot of redundant copying but the
whole thing is you design a system so
that redundant copying is acceptable in
terms of recording time stamps all over
the place yes
there's the thread affinity so all that
does is it wraps J&amp;amp;I je n'ai sorry and
that calls set affinity for for a thread
so it that yes yes you're going you're
going to a system called that will
actually do the binding of a thread to a
particular cpu and to even relate the
cpus it's actually a linux command line
so it's not even something you can run
once the system has started you're
actually changing a lot of the the line
you put I sell CPUs equals and then you
list all the CPUs you want to isolate
you have to change your irq balance so
that it won't put interrupts on those
CPUs as well and then once you've got
that you should have a CPU that's 100
percent idle unless you're using it of
course and then when you fire this up
and you're busy wait you go to a hundred
percent busy and in fact you can
diagnose problems in top because now you
know which thread is running on which
CPU and if it's not a hundred percent
busy and I had one client there was 16
percent busy on a busy waiting thread so
they knew they had a problem straight
away and then they diagnosed it back to
a lock that they didn't need took out
the lock and it jumped to 92 percent
busy and the sort of continued on from
there sorry
yeah so in that CPU is there to do a job
for you and it's not there to do
anything else so you're not going to
share it with anything else you just
gonna run it hot all the time that's
this that's the approach yeah you can
use yeah it's a question you can use J
and I to access collection server pro
primitives yes well the two approaches
you can use is to use something like
trove which is a collection of
primitives and you get very similar
performance to what you can achieve with
j'ni
the problem with J and I is that you pay
a cost for every time you go in and out
of a j'ni method whereas if you stay
within native Java code you're not
paying that overhead you're just paying
an overhead of the fact that it's in
Java s done perhaps not pre compiled in
the same way and so you don't get quite
as much benefit as you might imagine you
get benefit in J and I if the bulk of
the code it runs for long periods of
time with NC right if you're jumping in
and out you can you can spend more time
jumping in and out then actually
anything that you could possibly save so
the you just have to be very careful
that the overhead is not quite so large
sorry
that's correct so one of the things that
unsafe does in the hotspot compiler and
the hotspot JVM and it's not true of all
JVM is that it treats them as what calls
intrinsics
so if you have a look at the code it
says it's native but in reality what it
does is it substitutes it with a machine
code instruction so it's not actually
using j'ni for those methods and there's
actually you can get a list of all of
them there's a entrench if you get the
source you get intrinsic star H it has a
list of all the methods that are treated
as intrinsic s-- and they get replaced
with machine code instructions so you're
not paying the J&amp;amp;I cost in that
situation sorry
that is there is a cost and the cost is
usually in the range of about over of
around one to three gigabytes a second
is what you can actually do but the
benefit is that you get no impact on
your JC so you can run for very long
periods when you do TC that memory
doesn't count sort of in terms of Europe
GC cost you can do that and this is
where we you don't have to but this is
where if you have a complex data
structure like I said earlier you start
to say well is it really worth it right
doing this but you can do that you can
put complex data structures in as well
because all complex data structures can
be decomposed into primitives because
they can be serialized no you can if you
know the offset of whatever it is that
you want to change you can just change
that offset from an offset but again is
that easy to calculate or not and often
it's not easy to calculate but if it
that's right so that so there's pluses
and minuses to using and if you've got
see a lot of applications they've got
maybe a very simple data structure with
a fixed length of fields but they've got
a loss all of them and then that's when
it makes sense if you've got a complex
data structure like a tree or something
has a complex life cycle then yeah if
you go to simple hashmap with a very
simple key and only has one value class
then that can work quite well if you've
got multiple value classes for example
then it starts to get more complicated
and then you start to say well actually
I'll get my GC to do the work thank you
under Linux the simplest thing you can
do is to run perf you can run perf and
then an application and it will at the
end give you the statistics on things
like what your CPU hit rates are and a
number of other counters but I usually
look at this CPU hit hidden miss rates
there's one for Solaris performance
analyzer has one built in and that's a
bit more granular it can work out where
your misses are occurring which is even
more useful but usually AB not got
passed just using perform some sort of
micro test well I'm surprised that
questions you come about it
what disruptor does is it works entirely
within memory and it has the benefit
that it's nots persisted and so you get
you can get higher throughputs but my my
claim is well if you can get 10 million
a second do you need higher throughputs
but that you can get higher throughputs
with it the downside of the disruptor is
you could lose visibility you don't have
a record of everything you did or any of
the timings of what actually happened in
the system so what some clients have
ended up doing is they put Chronicle
behind disruptor to record everything
and then decided that maybe they didn't
need with disruptor because really all
they need it was the recording that
works particularly well if you don't
have network capture infrastructure in
place what's some of the top high
frequency firms have is they have a
recording of every packet coming in and
out and they've got teams to do all the
correlation and cross-checking and they
trigger dummy TCP packets to go out to
time or the intermediate events and that
case you don't need any persistence in
your course system and then it doesn't
make sense to have Chronicle you would
just have disruptor and that's what they
do in fact so if you because I know
Madden Thomson reasonably well
he tends to deal with the more high-end
high frequency trading you've got this
the money to spend on that kind of
infrastructure
whereas I tend to deal with more the
clients who can't afford to have whole
teams looking at TCP packets too so it's
nice to have a solution where it just
falls out at the end that you've got
that kind of recorded information
because part of well a few reasons one
is that it's easier to find Java
developers there are less and less C and
C++ developers out there the other thing
is that it integrates much better with
Java so if you want to step from
non-natural java to low-level java and
then back again it's all transparent
your java profiler works your Java
debugger works it all works nicely
together and still cross-platform you
can still compile all of this stuff
compiles on windows that runs on Windows
and then runs on Linux so you don't have
any of those recompilation issues that
you have to deal with in writing and C
you don't get caught it so so it's more
the fact that you don't need now a
separate skill set just to handle the C
portions it's not that you won't things
like kernel bypass often to get the best
out of your kernel bypass you might want
to write some stuff in C anyway because
even though they have support for Java
you do lose a few microseconds and doing
that and if that's important to you
you're going to have to write something
low-level if you've got GPUs in your
system quite likely you'll end up having
to write some C code because Java just
doesn't support GPUs the same way so I'm
not saying that you couldn't do it but I
would encourage using low-level Java
before considering writing in C
it will be um it should be somewhere
I'll put it on my blog if nothing else</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>