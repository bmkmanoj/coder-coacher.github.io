<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Performance Tuning Where Java Meets the Hardware | Coder Coacher - Coaching Coders</title><meta content="Performance Tuning Where Java Meets the Hardware - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Performance Tuning Where Java Meets the Hardware</b></h2><h5 class="post__date">2015-06-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/nhL2ZRj82Fw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you all for joining us this
afternoon my name is Darrell go and this
is my colleague Charley hunt and we're
gonna be talking about Java performance
tuning where Java meets the hardware so
a few years ago Charlie and I were
working together on performance analysis
and looking at low-level instructions
and so on and then we met up last year
and said we really should do a talk on
this topic so hopefully you'll find this
interesting so the outline of what we're
talking about is you know the process
you go through from source code to
machine code and then we'll just talk a
bit about GC because I think that goes
with the territory and then so Charlie's
going to cover those two topics and then
I'm going to talk a bit about
disassembly because that's one of my
favorite things so with that take it
away charlie
Thank You Darryl so code generation is
our first topic here I thought it was
worthwhile to take a little bit of a
maybe a couple of minutes to compare and
contrast the differences between how
optimizations or compilation works
between our Java programs and our C
programs so in a java program we write
our java source code we put it in a dot
java file we compile it with java c
turns out in the java bytecode that's
stored in a Java class file we then run
java with our Java class file starts our
java application executing in a java
virtual machine in contrast with a c
program we write our source code on a c
source file
we take our CC compiler we compile in
Lincoln that creates an executable for
us and that's where we execute our C
program the major differences here is in
the optimizations as when those are done
in Java it's done while the application
that's executing within the Java Virtual
Machine in the case of our C program
it's being done a compilation time so
it's worthwhile to take just kind of
brief moment to think about that
if we take a closer look here what
happens at Java Sea time you get a
rather trivial example here on the
left-hand side of a Java program not
really doing anything all that
interesting other than allocating a
thousand element array sticking a value
in that thousand element array and then
summing up the values nothing very
exciting there but if we take a look at
what's generated from for java bytecode
java bytecodes fairly easy to read and
understand so we can look at the Java
bytecode on the right hand side and we
can clearly see where our integer array
allocation is occurring we can see our
for loops in here with the go twos and
we can see where the invoke virtual
bytecode is being called here to print
our value at the end so it's fairly
straightforward to understand the
benefits of being able to take this Java
bytecode and put them into a and load
them into a java virtual machine this
java virtual machine is able to inspect
and take a look at the underlying
hardware and be able to make various
different optimization decisions
specifically for that piece of hardware
that we're executing on even within a
family of hardware so you could take
something like a xeon if you're
executing on say something like an ivy
bridge or a Haswell versus something
like a Nehalem different sets of
instruction sets that may be available
to you on more recent versions of the
xeon so you can do specializations and
specific optimizations for specific
instruction sets the other things that
you have is you have the runtime
information available to you you also
can optimize code that's used so if
there's portions of the application that
never get executed you don't have to
worry about optimizing it or you can
make optimization decisions about
something that's never being called
we're going to see a few examples of
that later on Darryl has got a very nice
example of virtual methods which i think
is a really interesting one to take a
look at and you also optimize the
important parts of the code so let's
move into the next stage of JIT
compilation and what sort of happens so
I'm gonna stop here for just a brief
moment and talk a little bit what's on
the right-hand side of this slide so
what's on the right-hand side of this
slide
is it showing what looks like a profile
I think most everybody recognizes that
that looks like a profile or some kind
so this is a profile that that's called
Oracle Solaris studio it's unfortunate
that Solaris is in the product name of
this because it also works on Linux this
is one of the brilliant marketing
decisions and branding decisions that
was made by Oracle back in the Sundays I
used to refer to this tool as one of
sun's best-kept secrets because it's an
incredibly powerful profiling tool it's
a tool that I like to use that gives me
an enormous amount of information and it
gets me the information that I'm looking
for the quickest there are other tools
out there that are similar to what the
Solaris performance analyzer this
Solaris Studio tool and that would be in
things like intel vtune or AMD code
analyst so if you're on say the Windows
platform you could use something like
intel vtune or if you're on the linux
platform you feel more comfortable and
using intel vtune that would be
something that would be very comparable
so there's various different ways you
can look at a profile with this
performance analyzer that's part of the
Solaris studio what you see on the right
hand side is they function view of this
particular program this profiler has
multiple different views or modes that
you can look at a profile one of them is
called a user mode another one is called
an expert mode and a machine mode and
I'll show you an example of the user
mode and machine or a machine mode and a
couple of examples here and I and I'll
call that out when we're looking at
something in user mode versus machine
mode and I'll explain what the
differences are so that's kind of what
is on the right hand side here is this
profiler called performance analyzer
part of this Solaris studio tools so
here we have a profile of this program
that's executing and we're looking at
this thing in user mode and when we're
in user mode anything that executes as
part of the JVM is lumped into this
entry that we're seeing here called JVM
system Darryl and I were talking a
little bit earlier today and he made a
very interesting observation and it was
something that I hadn't thought about
when you're monitoring something at an
operating system one of the
you tend to focus on is how much time am
i spending and kernel CPU in other words
how much this time
and your general reaction of that is
well I really want my program to be
executing my application rather than
being executing in the system or in the
kernel code you could look at JVM system
in the same way I really want my java
application to be executing rather than
executing code in the JVM so in user
mode again you'll see Java level methods
of your application and anything
associated with the JVM whether it's
garbage collection JIT compilation etc
is going to be lumped into JVM system so
a profile of this particular program
that's on the left hand side here is
showing that we're spending the vast
amount of time in our our method called
test and I think we looked at the body
of this I don't think that's any too
surprising there's another tab in in the
performance analyzer tool called the
time line and if we look at this again
in user mode in the user view we get a
timeline of what this looks like as it's
executing and each one of these ticks
that you see here can tell us a stack
trace of what was executing at that
particular point in time and if you
click on one of these you can get what
that stack trace looks like and you get
a different coloring for each element in
the stack trace and we've also annotated
this to show that at the beginning here
on the left hand side you can see here
is the amount of time that it's been
taking launching the JVM if we switched
into machine mode what you'd start to
see here is you'd see additional entries
here in this Timeline view for the
internal threads of the JVM and you'd
start to be able to identify what's
going on as the DVM was launching words
spending a time if that's a core sort of
question you want to have answered I can
answer that question for you for
probably 99% of the applications JVM
sorry time is dominated by class load
time that's always been pretty much the
case so you can also see it was once
this JVM initializes you can see where
it begins to start to execute the
program so this is a nice view that you
get with this tool to figure out what's
happening at a given point in time as
the application is executing
if I now switch into the machine mode
anything that was executing inside of
the JVM shows up here as an entry in our
list of methods that are being executed
one of the things that you'll see here
is an entry called interpreter so any
part of your application that's
executing within the interpreter shows
up as a line called interpreter one
extreme sort of an example that you can
run into is in the JVM there is a space
called the code cache this code cache is
a thing that holds your generated code
your assembly code that gets generated
if you run out of code cache space JIT
compilation is disabled which means that
your program thence is going to start
switching back into executing in the
interpreter so what you would see if you
had a profile of that you'd see a very
large amount of time spending in the
interpreter here also when you think
about an application through it's going
through its initialization phases to the
point where it's JIT compiled everything
or most everything you're going to see a
fair amount of time spent in the
interpreter you'll also notice here that
our our entries start to look a little
bit different it's a little bit
interesting here that we're seeing main
taking the vast majority of our time as
opposed to tests that we saw in user
mode so that's a question of me may want
to take a little bit a closer look at if
we switch over to the Timeline view and
we're going to be looking at this in
machine mode we now start to get a
little bit more information about what's
going on and we've annotated this with
what's happening so this row here the
1.2 row here is is illustrating what's
happening with the java application and
we can see a pin point at the time where
it's executing in the interpreter and we
can see where it's executing the test
method and we see a rather large
transition start to happen and we also
notice that the depth of the call stack
seemed to change so there's clearly
something happening within the
application and what the JVM is doing
here what actually is happening here is
main is taking tests and doing
in line of that and doing an
optimization on it so it makes it look
as if our main is the one that's taking
our time so if we look at this in kind
of a different way as we execute in the
interpreter we JIT compile and we inline
it you can see what starts to happen
over on the right-hand side we added a
little bit instrumentation to the
program to tell us you know how much
time are you spending here so you can
see as this thing transitions out of
interpreter into JIT compile code
another tab that's available in this
tool is called the disassembly tab so
I'm looking at this disassembly tab in
Machine View mode and what I get here is
the generated assembly embedded with the
source code and this starts to become
pretty interesting and you start to
identify things that maybe you wouldn't
expect sometimes you see things that you
expect other times you don't it also
helps you correlate the generated
assembly code with the statements of the
Java code and in this particular case
it's kind of interesting that we see
this instruction that's taking the most
amount of time let's see if we can
understand what that's all about so if
we start to look at this generated code
we can see that this integer allocation
this array allocation actually is being
transformed and optimized into an
allocation of the array on the stack and
we see some prefetches that are
happening in here as we're starting to
do something with this array it turns
out that the most expensive instruction
of what we're doing in this simple
program is the amount of time we spent
zeroing the array remember the Java
specification says that when you
initialize an object that that space is
zero we're spending an enormous amount
of time or in this particular trivial
example if you will that's where the
bulk of our time is being spent okay so
moving on to garbage collection so
garbage collection think about this as
imposing a tax on your application
potentially it offers you the
opportunity for some low-hanging fruit
through either
command-line option tuning or through
some code tweaks where you're trying to
reduce either object allocations or the
amount of objects that you retain for a
long period of time so you don't spend
as much time in garbage collection but
what can we do from a tools analysis or
a tool standpoint with this type of a
tool and looking at GC if we look at a
profile in this user view mode we have
JVM system here again and largely for
most applications vast majority of your
time that you're going to spending in a
JVM system is going to be GC especially
once you get to this point where you
surpass that phase where you get into
steady state of the application and you
finished the vast majority of the amount
of JIT compilation you expect would
expect and I think it's reasonable to
expect the vast majority of time and the
JVM is going to be spent doing GC prior
to that it could be things from JIT
compilation running in the interpreter
executing safe points etc if we switch
over to the timeline in user view we can
start to see these gaps of where the
application thread is has stopped
executing and this pause is due to the
GC so we can see this in the timeline
view if we switch to the machine view we
can start to see what are those threads
doing inside of that JVM so here in rows
1.3 and 1.4 we can clearly see that
there's activity going on and those two
JVM threads and if we were inside the
tool we could click on one of those
stack traces and we could tell by
looking at the stack trace that those
are indeed garbage collection threads
the one interesting thing to me when I
looked at this was the bottom row here
it looks like there's a pretty pretty
established sort of frequency of which
something is going on in the JVM it took
me a little bit of analysis to figure
out what this was doing it so I looked
at the stack trace of each one of these
I clicked on them and it turns out it's
the watcher thread the watcher thread is
responsible for various different
periodic tasks if you will as they're
called in the JVM one of those would be
a periodic task because we're running a
profiler here is collecting that profile
information so as the profiling agent
gathers this information stores it in a
buffer somebody is going to go out there
to that JVM and ask for that at some
frequency there are other types of
periodic tasks besides this notion of
collecting the profile samples but you
can clearly see associating on this
timeline that gap in the application
thread and what that's associated with
what's going on inside the JVM GC
Diagnostics this is rather
straightforward stuff that I think most
of us are familiar with you can get GC
information this is at the the highest
level a very high summary of what's
going on it's very analogous to what you
see with verbose GC the command line
option here print GC maps directly to
verbose GC I recommend to people to use
print GC details to get you more
information about what's going on with
each of the generation spaces in
addition here one of the key things to
take a look at is you get CPU times here
at the bottom of each of these
collections so you get the amount of
time that you spend in user land it's
this time and then the real time the
wall clock time one of the things you
can pay particular attention to is
comparing user time associated to the
wall clock time to get an idea of how
much multi-threading are you able to
take advantage of here how much scaling
or parallelism are you achieving and
when you see those two very close to
each other it tends to suggest you've
got single axis single threaded activity
going on inside the GC from there you go
say ok what can I do about increasing
parallelism or reduce that amount of
time I'm doing single threaded type of
GC work this time is another one you can
take a look at when you see high sis
time it's something we want to try to
eliminate in a garbage collection most
of the time this is related to the JVM
swapping swapping in GC java virtual
machines don't get along very well
because you're visiting a large amount
of memory so you're going to be swapping
a lot of this stuff to and from virtual
memory with that I'll turn it over to
Darryl we'll do a much closer deep dive
of taking a look at the hardware and
looking upward towards the software I
think you'll enjoy this
thank you so hardware performance
countess this is like RIT for me this is
my bread and butter this is stuff I love
and but for many other people have got
our cane a bit weird so every chip has
got a whole bunch of performance
counters on it and these count events
this count things like instructions like
cycles cache misses all sorts of things
and most processors have you know
hundreds of these things you can count a
subset of them at once like maybe two
maybe four and what's useful about them
is they tell you all sorts of detail
about what the process is seeing so this
isn't like what the OS is seeing or what
the code is looking at this is what the
actual process is hitting and what it's
a stumbling on so that the typical ones
that the easiest ones are you know
instructions and cycles so number
instructions pretty straightforward how
many instructions they do number of
cycles it took to complete those
instructions more interesting ones are
like things like load from loads from
memory when you load data fetch data
from memory and do something with it
and typically when you get down to the
hardware there's a whole bunch of weird
stuff that goes on that most of us don't
really want to know about and but the
hardware folks do want to know about
that and so they've got counters for
weird stuff and we won't talk about that
today now that the the common sort of
scenario that the common metric Isis
reported all over the place is either
IPC instructions per cycle or CPI cycles
per instruction and these are fantastic
metrics because you say houses you can
sound really knowledgeable because how
this is you know 0.7 IPC or whatever but
they're kind of a bit meaningless
because as a JVM as a virtual machine or
as a compiler or even as a programmer
you can pad your instruction count you
can take a lot of instructions to do
something really really trivial and that
gives you great IPC hey I've got IPC of
10
actually you're not doing any useful
works this is sort of like a an
interesting metric to look at but what
it really tells you is that if you
aren't getting if you're getting low IPC
you're not issuing many instructions
then that's bad but if you're getting
sort of high IPC lots of instructions
being issued that's either really really
really really good or something's really
bad somewhere that's giving you lots of
instructions you don't need to use so
it's not a straightforward metric but it
can be useful now on Solaris and on most
OSS there's some mechanism to get hold
of the performance counter of
information on Solaris the CPU track
that you run an application under CPU
track and it reports back the counter
data yeah once a second or at the end of
the run or on every fork or exec or
whatever and so this is a little program
another little toy program that does
some stuff and it when it exits it
reports the number of cycles it took and
the number of instructions it took and
this took 2.4 billion in snow sorry 25
billion instructions at I'll start there
again this took 25 billion cycles and in
those 25 billion cycles issued 20
billion instructions it's giving you an
IPC of 0.8 now that's bad an instructor
CPU the processor should be able to
issue one instruction every cycle at
least if you look at your xeon or
whatever it's probably about 4 or 5
instructions per cycle so less than 1 is
like you know pretty poor marks in the
test so there's something wrong and that
what is going wrong is the interesting
question to ask and the interesting
thing to dig into so the causes of low
IPC are basically there's three of them
I suppose long latency instructions so
some instructions like divided square
root are take a long time for a process
to execute so long time being like 20
cycles or something
and in some instances you can't do
anything else until you complete that
square root operation so you do the
square root and then you have a whole
bundle of dependent stuff that you're
just waiting for that square root to
complete to in order to you can start
that happens it's typically not that big
of an event and what is the big event is
fetching data from memory generally
speaking if you look at look at a
profile from an application you're gonna
be spending most of your stall time
fetching data from memory so that's the
kind of area you've got to really dig
into and understand and that's kind of a
topic for the next few slides and then
there's a third thing which is bad
things happening that's my catch-all for
today about stuff other stuff that
happens I don't really want to go into
that but you know I find an interesting
topic one of the most common situations
of sort of bad things happening is raw
hazards so most applications pull some
data and do something to it and store it
back to memory and then they might a bit
later say oh I need to use that again
and so I'll pull that back in again and
store it to memory so processors chips
are optimized to deal with the situation
where you store stuff to memory and then
you fetch it again a bit later they have
a sort of a raw bypass because that's a
read after write thing and they have a
bypass that said I just dealt with that
I'm gonna pull it back in again
it's our special hardware for doing that
that hardware works really well most of
the time except if you change the format
of the data so as an example situation I
often hit this in is you've got some
data which is in a buffer and you want
to pull that data from the buffer and
put it into an integer and so one way of
doing this is to load four bytes from
the buffer store four bytes into a
temporary variable and load an integer
from that space and that causes a
problem for the chip because you're
storing bytes and loading integers and I
said hey I can't do that I'm gonna have
to wait till that data gets out to the
system before I pull it back in again
and so that's an example of where bad
things happen to the performance and
most of the time you won't notice this
sometimes your code can be such that
you'll just trip over this significantly
so the big one is memory let's talk
about this so here's an example of code
which is doing some pointer chasing
again their total code is you know an
extra six lines or something there off
the top of the screen essentially I've
got an array of shapes or a poison a
list of shapes and pointed chasing down
and you can see that in this code there
are two hot points the first hot point
is where I access the shape and take its
center of gravity and the second hot
point is where I update the center of
gravity to in theory move it closer to
the center of gravity of everything so
it's kind of a sort of gravity model
type thing and I've got this list of
shapes and accessing this list is taking
time now I've already revealed the punch
line that it's gonna be memory so but
it's the journey to how we get to that
punch line is the interesting bit so I
profile the code and I dive down into
the disassembly and how many people like
x86 disassembly okay fantastic I'm not
actually I I couldn't read it it's not
my favorite thing to read it's part of
my second favorite thing to read but
that is my problem this bit of code here
if you look at the address EA somewhere
in the middle there it's reading from a
bit of memory storing that into a
register and then that's the tail end of
the the arrow at the point of the arrow
there's another read where you can see
this is using the same register EDI and
it's reading from an offset from that
register and then you get three and a
half seconds of time that three and a
half seconds to time is the pain that's
your reason your application is running
slow
and that's what we're kind of seeking to
optimize here so you'll notice that
unfortunately most tools will report the
time on the instruction after the one
that's causing you the pain now I call
this the innocent bystander effect
because it's like you give somebody here
hold this package and then you scarper
and the person you've left holding the
package is the one who gets in trouble
for it so you've got to look at this and
do a little bit of interpreting now in
this instance you can see the line
numbers in the square brackets the line
number for both the instruction and the
innocent bystander afterwards is a
hundred so both of those will get mapped
onto the correct line of source in some
instances the next line number might not
be the right one in that case you have
to do a little bit of thought in order
to go yeah it's this line it's not that
one
that one's the innocent line that's a
good reason why it's useful to look at
this view and not just take it as gospel
that the profile is giving you the
exactly the right information you need
because sometimes things like getting
the line number correct can be tricky
for the tool saying right there's
pointer chasing going on and there's a
cache miss which I'll talk about a bit
later
anyway the instruction costs a lot of a
time and that's what we want to diagnose
now as I said there's a performance
counters on this chip and if you look at
any machine you and with the appropriate
tool it will give you a list of a
hundred performance counters to
investigate on Solaris we've got a
studio performance analyzer which
helpfully maps these event counters onto
what's kind of like a JIT some generic
names for them now the generic names
we've picked come from Pappy which is
the performance api tool which is an
open-source product and they've defined
a whole bunch of names for the standard
sets of events you might find on a
processor and so this event here is
cache misses
and I will talk about kasia shortly but
anyway that's basically where you go off
the chip and fetch data from memory and
pull it back onto the chip so when I
profile using that counter it indicates
that the same point in the code where
I'm seeing the time is the point in the
code where I'm seeing the l2 cache
misses and this is good I have now got
enough information I've got confirmed
diagnosis that this is the problem and a
half way to fixing it
maybe so moving on let's talk a bit
about what happens when you have a
reference to an object and so looking at
this you have an object in memory
somewhere and you have the address of
that object and the chip says well I've
got this address and it's to an object
somewhere but the address the chip uses
is going to be a virtual address meaning
it's not the location in memory where
that data is held is actually an
identifier for that location in memory
so the location in memory is a physical
address and programs you and I write all
work on virtual addresses the reason it
does this is so that you can page that
memory out to disk and then page it back
in later put it in a different place
the physical address has changed but the
virtual address is the same so it all
works very nicely same with context
switches and things like this to make
this magic happen there's a structure on
the processor called the TLB I have no
idea why it's called TLB it stands for
translation lookaside buffer which is
it's a fantastic name but it conveys
absolutely no meaning so you look in the
TLB to find the physical address you've
got a virtual address you look it up in
the TLB and that gives you a physical
address and then the hardware looks up
the physical address in the chips and
then it pulls back the data from that
physical address and works on it and
that's fantastical works very nicely
there are things like TLB misses which
I'm not going to talk about in the
slides but essentially the TL B's are
very small structure
on the chip they can hold let's say 128
conversions translations have you want
to call them and when you get to 129 it
has to throw out one of the hundred and
twenty-eight so you can get things
called TLB misses and that it means that
it wasn't in that 128 it has to fetch it
probably from cache probably from memory
and then it gets even worse there's
another structure called a TSP and I've
forgotten what the TSP stands for today
but it's more complicated doesn't matter
and that is another structure in memory
that you go through and look up these
address mappings in so most of the time
we deem TLB misses but they have a cost
small cost or notable cost but that is
just the kind of the metadata stuff what
you actually want to do is get data from
memory so your program runs along hits
an address of an object it says okay I
want to fetch data from this object so
it goes ok lookup and TLB got the
physical address I'll pull the data from
memory and it has on chip a cache so any
data this recently used get stuck in the
cache and it will stay there until it
gets kicked out later on by because it's
got new data it wants to put in fetching
data from memory is really really slow
that's your big cost there's this
analogy about you know if it takes a
second to fetch of data from a register
on the chip it takes you know a minute
to get it from a cache on the chip it
takes a week to get it from memory it
takes a year to get it from disk and
it's that sort of order of magnitude it
really is the processor goes says oh
this data isn't isn't on chip ok it's
gonna take me a month or whatever to get
it from memory and it's sitting around
waiting during that time and when your
program hits these things
that's what typically slows it down so
going back to the the contrived bit of
code that I set up earlier so I was
doing pointer chasing on
list of shapes and each of those shapes
has a list of points and each of those
points is a separate object so in order
to pull the points in front of shape I
have to traverse every link in the list
and every time I traverse a link every
time I do a reference or a point to jump
or whatever it's an opportunity for that
data to be in memory so if you see all
these arrows in this diagram those are
all places where you might well get a
cache miss and a cache miss is 200 400
cycles it's a big deal and playing
whack-a-mole with them is great fun so
you can look at this diagram say look
lots of potential cache misses so how
can we optimize this well the obvious
that way of optimizing it is to instead
of having lots of points this is a
pointer point I have an array of points
and that's great that's reduced the
number of arrows on this diagram which
ought to lead to improve performance now
what I've done here is I've got an array
of x coordinates an array of Y
coordinates an array of Z coordinates
and suddenly because I'm now traversing
those I hit the first coordinate and I
pull in the whole lot so that's much
more efficient and then going back to my
CPU track tool I can see that
interestingly my number of instructions
has gone up from 20 billion to 24
billion I haven't figured out why that
is it's yeah stuff whatever but my
cycles has dropped from 25 billion down
to 17 billion and my IPC has gone up so
that's kind of good I'm looking at my
own PC and say well it's better than it
was my cycles is down it's more if it's
going to be more efficient hopefully
some of those instructions they increase
they're not just spurious noise stuff
but it's a good sign but again an IPC of
1.4 doesn't necessarily mean I finished
it just means there's got an IP C of 1.4
some of those instructions may or may
not be useful so if you look at the
profile now you can see that the time is
spent in the same places in the code but
there's less of it that's spent there so
there's some further opportunities that
I could that I could potentially do
something further do something more to
improve this but for the moment I've
stopped that's kind of improving my toy
benchmarks is left as an exercise for
when you go home and have enough
invested on the train so in that example
I took a linked list of points and
converted it into an array of x
coordinates an array of Y coordinates
and array of Z coordinates what's
interesting about that kind of
conversion is it's not necessarily
always the best one to do so here's
another example where I've got an array
of points on the left-hand side and each
of the points is a class an object with
an X and y&amp;amp;z coordinate and on the
right-hand side I've got arrays of x
coordinates and Y coordinates and set
coordinates now in this instance the
array of a separate arrays of x y&amp;amp;z
coordinates is not the best way of going
because I'm randomly accessing these
arrays I've got some nasty X or type
formula in there that's supposed to
generate a pretty random stream of
numbers so what's happening here is that
I randomly access these arrays and where
I've got three arrays I access the x one
and that's a cache miss X I access the Y
one another cache miss an X access the Z
one and it's another cache miss so I've
got three potential cache misses going
on so in contrast if I set up these as
an object when I pull the x coordinate
in I immediately get the Y in the z
coordinate at the same time because
they're all co-located in memory so this
is much more efficient now in terms of
performance the numbers on this
particular code that I threw this
together with didn't make a lot of
difference it went from three seconds to
two point eight seconds
no that's not particularly exciting but
I'm sure I could contrive an example
where it made a huge difference but it
might take me more than my allocated
budget of 20 lines so what we're looking
at there is that you want to co-locate
data because in memory it isn't just a
when you say load me an integer it
doesn't just pull me back pull back an
integer from memory it pulls back the
integer plus the surrounding stuff in
memory so memory is actually arranged in
terms of lines called cache lines each
of these cache lines is 64 bytes in size
on most hardware today sometimes it's
bigger sometimes it's smaller but
generally speaking 64 bytes is about
what you get so it means that when you
fix when you pull an integer in you pull
in another 60 bytes of data and if you
can put useful stuff into that 60 bytes
of data then you do better for it so the
other thing that goes on is that if
you've got two threads or two processors
and they both try to access the same
cache line to write something to it then
they can't both write something at the
same time because the update is at a
cache line boundary you don't just write
an integer to memory you write an entire
64 bytes to memory at a time so this
means that if you've got data structures
that are shared between threads you've
got to make sure that they're padded so
that one thread can access one set of
data structure and another thread
accesses a separate data structure this
is called pool sharing so I'm going to
talk about that next so here's an
example where I've got a shared array
and this is one that Charlie put
together for me
you can see this shared array of shorts
right at the top there and then I've got
two objects that I'm going to run one
the first one top one just says okay I'm
going to access the an index in that
array and if I got many threads all the
threads will access the same adjacent
element in that array the second one is
that I've
okay I'm going to pad it so I'm going to
multiply the index by 32 so the first
thread will access this one the second
thread will access is one on the other
side and they're far apart they're not
on the same cache line so in terms of
performance this is what happens so on
my laptop which is in my bag over there
it didn't make a lot of difference I've
got to one CPU two threads it with full
sharing it's three seconds without full
sharing this 2.8 second smidgen faster
not a lot faster then I tried this on a
nother machine which is a neelam with
two CPUs 24 threads vastly different
system and it took 63 seconds when those
full sharing going on versus four and a
half seconds when there wasn't then this
is with you know 24 threads
larger number of threads more overhead
which is why it's not didn't scale as
well didn't didn't perform as well
without false sharing but very different
impact there so what this is is that a
code that what runs well on your laptop
and runs really nicely with two threads
may suddenly run like a dog when you
take it to a big machine and there's got
24 threads hanging around and they're
all trying to compete for the same
resource so it's something to watch out
for because it means that the actual
hardware you deploy on may have
different performance characteristics
from the hardware you've been developing
on and so that's kind of something to be
very cautious about now I'm interest
really about diagnosing this because I
know that there's a problem but what I
really want to do is find evidence that
that is the problem I'm looking at and
therefore the fix I'm proposing is
fixing the actual problem not something
that's kind of happens to work so
there's quite a complicated series of
sequence of operations that goes on when
you want to write data to memory so when
you do a store it's
okay have I got this line in cash if I
haven't got this line in cash it goes
and looks for it in memory and then it
pulls it back in but it also says I need
to own this line this line of data
before I can do a store so it does
something called a read to own now if
anyone else owns that line of data
it says them wait I want that line of
data you invalidate your copy and so
I've got the unique copy I'll write into
it and then you can borrow it back again
and so you what you'll get is invalidate
requests meaning some other call came
along and said I need that data you get
rid of your copy of it these were these
will have weird peculiar names on
various bits of hardware you should be
able to track them down what they what
they actually are by looking at the PRM
anyway so I profile using the
performance analyzer on these Hardware
accounter events and I can see that
where the time is spent I get a lot of
these events this is in the run run a1
and then in run or two where I've
avoided the fault sharing thing I don't
get any of these events so I know that
slow code and these card were counter
events go together and so therefore
that's the root cause of the problem and
I know how to fix it just padding stuff
it's trivial but identifying is the
problem
you also notice on the slide that with
this is a multi-threaded code so use of
times like astronomical it actually runs
for I know as I said 63 seconds but
because I've got 24 threads hanging
around for 63 seconds there's a lot of
thread time of multiply 63 by 24 comes
out to about 1300 so that's what it
looks like in aggregate if I dive down
into the disassembly again this is
essentially that both routines generate
the same code and you can see that in
one routine time spread kind of evenly
there's no big Peaks in and in fact most
of the lines are highlighted in yellow
saying hey this is a hot line and the
other side where there is false sharing
and there's actually two lines which
spend a lot of time and only one of
those lines is highly
yellow because it's like 800 seconds you
got to look at this there's times being
wasted here so it highlights the the
important bit of the code to look at and
it also shows you that the hardware
counter events correlate with that time
so you know that very definitely this
time is due to this kind of event
so to summarize about memory you've got
you've got to make basically make memory
density import count you've got a stuff
data together that's going to be used
together and you want to push data that
isn't going to be used is rarely going
to be used off into the wilderness so
that won't get in the way and if you can
avoid pointer chasing so much the better
and when you've got multiple threads
think of how your data structures are
going to use that going to be used by
multiple threads now Charlie suggests I
took a look at polymorphism because this
is a fun topic I like it and it's subtle
is significantly different from what I
see when I look at C++ code so suppose I
got family of shapes so I've got a
circle square a rectangle a triangle
they all kind of live together happily
in the same house and then I run through
my list of shapes and I could do some
arbitrary rather pointless comparison of
who's got the biggest area and I count
looked and it doesn't really matter what
I'm just going through my list of shapes
and I'm touching them all and now I can
look at various scenarios suppose I've
got one shape if I got this uniform
homogeneous environment only had
triangles live in and or if I've got
this environment where I've got
triangles and circles or her gots
triangles circles and squares and so on
I can make this arbitrarily complicated
how does this impact performance so this
was coming from a C++ background this is
kind of super quite surprising to me if
I've got one subclass then it's fast but
I've got two it's slightly slower and if
it's got three it's kind of plateaus at
the
two seconds so what is going on here and
I love the fun goodness the disassembly
view I'm sorry this is you know x86
again by the end of this talk I hope
you'll be very familiar with it and
really love seeing it so this this here
is it finds it says what type of shape
are you dealing with and in hex for do5
2d 78 is triangle and so are you a
triangle and it's only expecting to find
triangles here because it's only made
triangles so if it finds something it
isn't a triangle something's gone wrong
so this is really just a yeah belt
embraces sort of thing where it's saying
okay if something has gone wrong
I'm gonna have to deal with it but
otherwise it's a triangle and so just
zips around this loop making a very
redundant check every time and that's
why it's so fast that's why it only
takes four seconds if I put a triangle
in a circle in there so first of all it
says
are you a triangle and if you are a
triangle we can go and do this bit of
code otherwise said I you're a circle
but I want to just check that before I
assume you're a circle so it checks that
and assume you're a circle all's well
and good and it calculates the area of
the circle or the area of the triangle
so you've got two branches going on and
it's pretty fast code now that took
about ten seconds now if you have three
shapes or four shapes or n shapes you
get this kind of generic template and
this is kind of what if eclis see in C++
because in C++ you've got a whole bunch
of objects it's very rare that you can
actually say I've only got one of these
subclasses and so this is a place where
Java actually can often do better than
C++ because it's saying well I've only
made triangles of course it's a triangle
so this one just says well whatever you
are I'm going to call your virtual
method and look up what you are and call
it and so that's very nice for very
brief and but interesting I hope way of
looking at what happens when Java hits
the code is the hardware so
to me the JVM is fantastic abstracting
away the details of the hardware that
you're running on but I believe as
developers it's useful to look at the
underlying hardware and see how that
impacts your code because sometimes it
might we'll explain something that's
been bugging you for weeks about why is
this bit of code slower than it should
be I hope you found that interesting and
I've got a couple of slides about other
talks given by our two groups so charlie
is going to be talking this afternoon
very shortly about garbage collection
done on Wednesday about garbage
collection and I think those were very
interesting and then for the studio side
of things we've got a whole bunch of
talks about various topics but if you're
interested in the performance and
analysis tool we've got the screenshots
from today the project lead for that
we'll be talking tomorrow between 3:00
and 4:00 I think next door so if you
just hang out in the hallway for 24
hours so you'll find out more details
about the tool from that so with that
what questions do you have
so the question is about the code cache
and if you exhaust the code cache space
and you drop back into the interpreter
are there anything that can help tell
you if that's an issue or are there
means for the JVM to be able to deal
with that in more recent JVMs the later
Java 7 updates and the later Java 6
updates there is the capability called
code cache flushing so the idea there is
that you're identifying the least
recently used optimized code that's in
there think of it as kind of like
garbage collection for the code cache so
it basically is a way to try to allow
you to be able to continue to put things
or into the code cache that are
optimized there are some plugins out
there for things like visual VM that can
tell you the occupancy of code cache
visual VM so there's one that I'm aware
of that's Kirk pepperdine develop that
will give you the occupancy of of code
cache that's one of you know I'm sure
there's other ones out there that are
available yeah
so the question if I'm understanding it
is does the performance of
virtualization change when you move to a
virtual environment virtualized server
is that the question
if there's other things executing as far
as I know I would suspect that to be the
case there was something that both
Darryl and I discovered within probably
the last day and a half is so we
actually we were running a Solaris VM on
Mac with VMware and in the Advanced
Options section there's a little area
that says hey do you want to expose
Hardware counters what we don't know is
number one is the hardware counter
information we're getting accurate and
is it being isolated to our virtualized
stack we don't know so I don't really
know the answer to that question if
you'll if you get isolation and there
quote accurate then I suspect that the
information is going to be feel fairly
useful but if it's aggregate if you're
getting the aggregate across all of
those then so I'll add on to that that
you know at least in Solaris where I'm
most familiar and in Linux
the you won't get performance counters
bleeding between applications in the
same OS but unless the counts is defined
in such way that they have to but in
some but if you're on virtualized
Hardware it's really going to be hard to
tell but
so same OS you should be fine but once
you had virtualization into the mix it's
kind of like it's a miracle it works at
all other questions yes
so the question is so we had the example
where it was a false sharing example
where you see much much faster execution
on well I should say you the impact of
the false sharing is much greater when
you have a larger number of Hardware
threads or and or the multiple CPU
sockets and the question is what do we
do in that case do you want to answer
that sure the full sharing is false
sharing is I like it because it's very
straightforward to fix it's just bad
stuff so if you're dealing with you know
one element here and the next one in the
next adjacent integer slot you just move
them so they're far enough apart they're
on separate cache lines of course
sharing is one of these its annoying it
comes up every so often but it's a it's
a good one to fix where you really hit
scaling problems or things like you know
real mutex locks where you're saying hey
I need multiple threads to update this
one variable and they can't all do it at
once so that's the more of the the
problem with writing scalable code and
there's a lot I could talk about in
about scale of writing scalable code but
I'm happy to talk to you afterwards but
like agar prowess
um I does does a Java land issue I think
more supporting transactional memory and
has well yeah it's I don't know an
absolute answer to that my speculation
would be just based on past experience
and working on the hotspot JVM my
expectation is that there is
optimization work going on in that area
that's about as much as I can tell you
know I whether there is or not I don't
know the answer to that but my suspicion
would be just based on past experience
that there is yeah my reading of the the
specification for Haswell HTM stuff was
this looks like the kind of thing you
could use to get rid of some locks and
critical code which would be a very nice
feature but it's something that would
happen in the JVM rather than up in the
user stack I guess that's it yeah I'm
sure if it's possible we'll see I think
an interesting sort of question that
goes along with us and maybe Daryl you
know the answer to this is what sort of
implications does that have on the tools
such as something like analyzer and
getting you the sorts of information
that you're interested in yeah that's
something that puzzles me at the moment
and one day I'll know the answer to it
the reason it's puzzling is because you
know transactional memory is based on
the idea that if it works you kind of it
just does the instruction it said it
worked and but it's like this sort of
showed Ignis cat if you then send a
profiling signal into the middle of it
that's almost certain to cause it to not
work so it's like it's only going to
work when you're not looking I don't
know it should be fun okay
any further questions well thank you
everybody for coming I hope there was at
least one thing in here that was useful
for you and if you're in an environment
where you're trying to squeeze out as
much of these Layton sees as you can if
you're not using the tool that's giving
you this level of information I'd really
encourage you to invest some time into
looking at one of these tools and seeing
how your application behaves but I think
you'll find some really interesting
things show up and it'll probably
generate some really interesting and
challenging questions thanks again for
coming thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>