<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Jetty 9: The Next-Generation Servlet Engine | Coder Coacher - Coaching Coders</title><meta content="Jetty 9: The Next-Generation Servlet Engine - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Jetty 9: The Next-Generation Servlet Engine</b></h2><h5 class="post__date">2015-06-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8QCC4HaSy58" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">this session will be about jd9
and you know subtitle is the
next-generation server engine we'll see
why is this before starting how many of
you use Jerry but probably not almost
all of you so how many of you use Jerry
in production like a server only like
not embedded not for tests just a server
ok
just few ok how many embedded or some
other form like tests cool thanks
all right so this is me I'm Simona
birthday I I'm a long time open source
contributor I started with JBoss in 2000
and I went all the bake to MX for J
Jerry comedy a bunch of other projects
I'm currently the lead architect at
Antalya wet tied which is the company
behind Jerry the provides commercial
support to Jerry I'm also the comedy
project leader comedies are is another
open source framework is sponsored by
intaglio web tied and it's a web
messaging framework think of it as JMS
Java messaging service on the web right
I'm also part of my job is also doing
JVM tuning so I'm kind of an expert on
you know performance on the JVM and
tuning garbage collector and stuff like
that this is our agenda we'll see Jerry
introduction we'll see a little bit of a
web history and evolution how the web
changed over the past 15 years we'll see
the new web protocols that are coming
forward and we'll see the latest Jerry
features and performance and we'll take
a quick look at the future for HTTP 2.0
in particular so the jury server
container basically jetty implemented
the server specification since the first
version of the specification 1.0 that
was back in 97 currently we have 4
version of Jerry seven eight nine nine
one so Jerry seven is implementing the
server to point
five eight and nine going go to server
trick zero and nine one implements the
newest server 3.1 specification which is
you know something that I exited like a
couple of weeks ago maybe one month ago
or something so it's a very new
specification we will talk about this a
little bit later and a few slides so
Jerry is mostly known as for this
component based architecture like you
showed me right you mostly use jelly
like embedded because it's very easy to
embed but it's also great for production
too you can just install jelly use that
as a production server it scales really
well I'll give you performance numbers
later on but it's just great as a
production server too so don't be shy
use it alright so let's go for some
history and evolution of the web so
let's roll back time until 96 like
escape navigator has the best moment
like see the peak here was 96 and then
it was just went down and and died other
things that happened at here where Java
was born in January and HTTP 1.0 was
born in May actually at that time in 96
jetty was already one year old because
Jerry was started to be written using
java 0.9 and it beeped 0.9 so it was
like a precursor of all the time so it's
probably one of the first Java server
HTTP server out there ever so where we
date back but we are very modern too and
we'll see that so let's stay back in 96
time the HTTP one two one zero protocol
was designed to serve this kind of pages
one html5 file 600 bytes
no images no nothing just the HTML okay
so this is really important because
that's not even close to a web page that
we have today all right however the HTTP
protocol was designed for these pages
let's go further on and see what
happens during those years right between
96 and today what happened so the first
thing that happened important for
server-side was that over time it was
discovered that the type of a year that
we were doing in particular blocking i/o
was limiting our performances right so
we went from a model of blocking i/o to
a new model that basically every server
today is uses which is a synchronous i/o
also called new i/o or non-blocking or
you know something like that so one of
the problem that came up at those times
was the sea tank a problem there was how
do I connect tank a client to a single
server and have the server be able to
handle requests send by those 10k
clients okay at those time in 96 in the
90s this was a very hard problem because
everybody was using blocking at your
model to solve this problem we had to go
a synchronous alright so Jerry 6 was one
of the first server containers that used
a synchronous i/o as the basis for being
more scalable over over you know under
load in particular we push this concept
of a synchronous i/o even further we
have created a library called the jetty
continuations and this library was a
portable way to suspend and resume
requests basically we introduced a
programming model and we gave this
programming model to developers way
before server 3.0 where people could
actually say okay I mean I get this
request now I gotta do some processing
but I want to do this processing like
you know maybe a synchronously and then
later on I would like to resume the
request and then write the response back
to the client okay
meanwhile you know try to be as scalable
as possible jetty was able to do this we
went even further on based on this
library we created the comedy project
which is a scalable one messaging
framework as I told you so on top
of these facilities we have created a
project that actually abstracts and uses
this is the basis for you know being
really scalable will see some benchmark
numbers in few slides so what happen is
that the experience that we had by
deploying this library to customers to
users to the jetty community open-source
community the feed us back regarding how
good Jerry continuation were doing for
them and how scalable they were actually
this concept have been then incorporated
in the server 3.0 specification now it's
a standard there is a standard way in
servlets that you can suspend the
request do some synchronous processing
and then write the response back down
alright the server 3.1 specification
which is the most recent one expand this
concept even further because it not only
allows you to do a synchronous requests
and responses but it allows you also to
do a synchronous input output ok we
server 3.0 3.0 you had to use input
streams and output streams and if those
were blocking you were stuck there I
mean until data were coming or you could
actually write to the string your thread
was basically blocked there right with
the synchronous i/o your application
developers web developers have an API
where they can actually say ok I'm going
to write as much as I can before
blocking whenever I'm not going to write
anymore because it's blocking ok the
client doesn't read my stuff so I'm
filling all the buffers and then I
cannot write more because all the
buffers in the line are filled then I'm
going to stop take that thread put it
back on the thread pool so that it can
serve other requests and then please
call me back when you're ready to write
again all right another thing that
happened is that we moved from single
core CPUs single threading models to
multi-core like I have 4 cores in my
phone right now it's like I even have
multiple cores in my fridge or you know
it's there everywhere
not only that but in order to take
advantage of CPUs becoming more and more
efficient in going parallel the JDK has
been enhanced with atomic primitives JDK
5 introduced it
you know the atomic classes so now we
can that map directly the atomicity that
we get in the java programming language
maps directly to CPU instructions that
do operations atomically so with this in
with these new two two things we are to
basically rethink the locking that we
we're doing injury injury within surgery
to take advantage of these new features
right so basically we have rewritten
completely the core of Jerry 9 taking
this into account we have done like a
lot of work in trying to reduce the
threat context switching at a minimum
possible by still respecting the server
specification we have done a lot of work
to try to reduce a problem called false
sharing inside the Jerry data structures
how many of you knows but what is false
sharing ok just view so it's basically a
cash problem that happens in the CPU the
limits scalability to a certain point
all right why complicated requires
another session alone but we add this
problem because you know it was easy to
program in a certain way but when you go
multi-core you have to pay attention to
details that before you did not even
know that they existed
all right we replaced all the users of
our internal data structure to use
concurrent data structure provided by
the JDK when those were not scalable
enough for us we wrote our own
concurrent data structures also we have
taken out all the synchronized keyword
in the Jerry quarry oh and replacing
those in favor of atomic state machines
so we still guarantee the correctness of
the behavior that synchronized would
you know provide for us but we're
knowing do that we're not doing that
using synchronized keywords we're using
atomic data state machines so all this
work that we have done in order to be
really modern even if we date back in 95
gave us a big performance boost like jd9
one is 30 percent or even more faster in
throughput than jelly 8 even if it even
implements more features because he has
to implement more features the jury that
the server 3.1 features right which are
heavily based on Io so we had to modify
the internals of jelly to actually you
know expose these features to
application developers because the
service specification required us to do
that so big performance boost okay where
we're following the times now let's go
early before fast-forward to today
this one is the same page that I show
you before the w3c droid page that I
show you before in 96 this one is tutele
2013 page it is made of one HTML file
for CSS to JavaScript 27 images for a
total of 34 resources and 138 kilobyte
basically we add a 34 time increase in
number of requests and responses that we
need to go to the server and to download
stuff down and at 250 times increase in
the data that we actually download okay
now the fun thing is that this page here
is not even close to an average web page
that we have today this one is the CNN
page 172 resources for 1.2 megabytes of
download just for the home page right so
you get a call 172 times a number of
servers out there in order to compose
just a single page this is today's web
right it is completely different from 96
web
I'm whoops
I lost the mic there's one in blood
oh good in the back yeah so yeah
today's pages are like this which is you
know probably what many of you create
today right my good back okay so um the
web exploded right and can we still base
what we are doing today on assumptions
protocols technologies they were
designed and they were actually in 96
but are there still valid today you know
this is an important question to ask
well we can do a lot better right and
we'll see how so the problem is that
HTTP is an old protocol it was not
designed to serve today's web pages it's
not bi-directional doesn't do any
multiplexing HTTP is duplex only doesn't
do any resource correlation and we'll
see in a second what is this so and we
have to do a lot of hacks in order to
fake be directionality we have to employ
something called comet techniques any
anybody here heard about long Pauline
yeah that's a hack basically in order to
get B directionality on top of HTTP
right and we do that because there is a
limit in a jiffy
now browsers hope and multiple
connections the MGP specification says
you can only open two connections to the
same domain right browser open six they
they don't respect the HTTP
specification they just open six now any
of you is doing domain sharding here
yeah domain sharding is a technique
where basically you put your images in a
different sub domain so that you trick
the browser and you say okay you
open six connection to domain.com but
then I put my images into images
domain.com that's the difference of
domain and the browser opens another six
connection to that domain and then you
put your CSS into CSS the domain the con
so the browser can open another six
connections over there right that's
called domain sharding and we only do
this because HTTP doesn't support
multiplexing that's the only reason we
do it because we do not have to do that
if HTTP supports multiplexing it renders
our deployment our old web application
way more complicated we're just working
around HTTP limits um there's a nice
thing that happened in HTTP 101 it was a
feature called pipelining so these
people basically sit around the table
and say we have a problem we need to do
some kind of you know kind of result
between results correlation and
multiplexing let's introduce pipelining
unfortunately when this was deployed it
did not work
it had semantic problems basically every
browser especially the dust one have it
disabled and almost all the mobile
browser have it disabled as well all
right so doesn't work we try to fix the
protocol with a feature but we couldn't
all right well it turned out they they
thought it would you know fix the
problem but it actually did it so HTTP
is old not only that if we go back in
time in 96 and we try to try to you know
click on the w3c page and download that
with a 96 computer with a 96 browser
with a 96 CPU and a 96 internet
connection you know we'll probably
render the web page in one second say
now if you go to the same page today we
have 8 cores 6 megabit connections to
basically whatever sites we want super
powerful computers and everything but
still that page comes down in one second
so basically 15 years but have
but our user experiences a human's
either didn't change or it's actually
worse the CNN page takes eight seconds
to complete like you have the unload
after two seconds but then the moment
all the things pop up properly you got
eight seconds that's a long time right
so people you know people that was
really interested in the web tried to to
do something about it and you know they
tried to find alternative solution to
HTTP because that's the problem so two
two new protocols came up WebSocket
whose use case is only be directionality
and speedy which is a new protocol to
transport more efficiently HTTP over the
web so several containers or you know
they do circuits they used to do only
HTTP but that's not enough anymore for
today's web server containers need to
become polyglot which is a common term
these days programmers are becoming
polyglot you know now super containers
do so let's see this to protocol in a
little bit more detail so that we can
understand better where we are going
WebSocket so web circuit was an effort
initiated by browser vendors they wanted
to give developers the capability to
have the servers to push down data to
the client without having the clients
actually requesting something to them
right that was the primary use case for
it of course solves up the common
techniques no long polling and more and
it has been standardized finally as a
wire protocol is a JavaScript API within
html5 and lately with a jsr in the java
world now you can write web application
based on WebSocket like exactly like you
can do with servlets okay there is a new
package called Java X dot WebSocket like
there is Java X dot servlet so it's
it's a standard API that eventually all
server containers will implement now the
key thing is that Jerry seven was one of
the first container to actually ship
WebSocket back in 2009 like four years
ago
so four years ago we implemented
WebSocket we gave developers server-side
developers our own API it was not a
standard API it was organized Jerry
WebSocket but still they could program
using WebSocket we gained a lot of
experience by deploying web socket to to
our customers and jt9 one with 39 1 we
finally implement JSF 3 5 6 so now we
can offer a standard api to application
developer state so the key point is that
we were early adopters we were doing
WebSocket when nobody else was doing my
socket and in those four years we gained
a lot of experience a lot of use cases a
lot of performance fixes in in order to
get WebSocket really well how does with
WebSocket work works in this way you'd
run some port 80 or 443 which is really
cool because you don't have to call your
sis at mean to ask him to open another
port in the firewall all right he uses a
mechanism that was introduced in HCP 101
called the HP upgrade mechanism it works
in this way you make a plain old HTTP
request but you specify these two
special headers connection upgrade
upgrade and you tell what is the next
protocol that you wanna speak and then
you get some WebSocket specific headers
here but they're not very important the
server gets this request understand that
the client will like to upgrade things
and the replies with a special response
here called 101 it doesn't send back at
200 ok it sends back a special response
said that says one-one switching
protocols I'm good with the I can speak
WebSocket and in these special headers
do and then this one is basically a
computation based on what the
I ain't so the client and the server at
this point know that either pier can
actually speak WebSocket alright the
client speaks workshop and the server
speaks WebSocket
what about intermediaries in the middle
right these guys are the problem because
what they see is they I'm seeing an HTP
request here okay I'm going to pass this
along with maybe a lot a few others in
between right oh go to the server then
they see a request coming down and say
oh this is an HTTP response
I'm sorry response coming down HTTP
response maybe a lot semadar down to the
client then the client sends a WebSocket
frame and the intermediary look at it
you say well but this is not HTTP I can
understand WebSocket is a binary
protocol so it says ah that's garbage
and closing the connection all right
this still happens there's a trick that
I'll show you later on how to fix this
but let's go on on website at first so
what about performance do we actually
get the benefit from using this new
protocol now this is a graph that shows
a comedy chat application using jd7 over
the HTTP protocol okay so it works in
this way it's a chat so one chopped user
sends a message to the server and this
message gets broadcasted to a number of
other clients we measure the time that
it takes from this message to go from
the originator one to all the other
clients all right and then we take a
mean so Jerry seven comedy lollu habla
20k client 20,000 clients connected to a
single server all of them sending a
total of 250 k messages per second
that's about 50 K requests and responses
per second the latency of these messages
the medium latency is below 200
milliseconds here 200 milliseconds so
it's pretty good you can do a lot of
stuff with this kind of latency and
you're still doing 50 K requests and
responses per second so not bad not
really bad now let's go to the WebSocket
graph it's this one now this line here
is not 20
a client it is 100,000 clients and this
number is the same rate 50 K messages
per second but this number here is 4
milliseconds not 200 right the only
difference is the protocol I am using to
transfer data from the client to the
server back to the client that's the
only difference that it is all right so
the WebSocket protocol being the binary
protocol is much easier to generate much
easier to parse so much so that it gives
you this performance boost right plus of
course is be directional that's the
super cool thing about all right can you
actually use WebSocket yes it turns out
that basically 73% of the browser's
today like worldwide browser support
WebSockets so you have it baked in
already in all your clients not even the
best one but even almost all the mobile
clients support that already so you're
good to go
your only problem are intermediaries the
missing part here is of course Internet
Explorer because he supports WebSocket
since version 10 so there's still a
large part of Internet Explorer and user
on 8 &amp;amp; 9 so you know did catch up
eventually but it's already there of
course we have server side we have there
seven eight nine nine one standard
GlassFish raised in Tomcat 7 and alpha
thing but on cat 8 is going for the jsr
as well so almost everybody now does
WebSocket
so our conclusion our WebSocket big
performance boost super cool but does
have limitations one of them is that
WebSocket is a very simple protocol it
says this is a frame WebSocket frame you
have 23 bytes that follows but you have
no idea whatsoever what are those 23
bytes right compare this with an HTTP
response for example you can look at
and you see content-type : image / PNG
and then binary bytes you know exactly
what the binary bytes are in WebSocket
this metadata about the bytes that
you're transferring back and forth is
totally missing there's no specification
whatsoever so it is a very low-level
protocol that's why we always suggest
that it is a lot better to use
frameworks that exist already instead of
baking your own protocol on top of
WebSocket you know try before writing a
WebSocket web application from scratch
look for frameworks around that do what
you need to do especially we recommend
to use WebSocket comedy if you have to
do web messaging right because it's a
very scalable framework er I show you
the graph before you can connect hundred
K clients to a single server send 50 K
messages per second and get four
millisecond latency between these
messages so it does scale right
intermediaries there are still some
problem I have a war story about this I
was talking to a guy that was using
comedy in India and I could connect from
Italy to his server in India without
problems using WebSocket this guy in
India could not connect to his own
server in India because it was probably
passing through a transparent proxy that
was cutting the connection because it
was not understanding WebSocket so this
kind of things happens now if you have
to write your own WebSocket web
application you would have to take care
of falling back to http if WebSocket
fails that's a lot of work believe me
because I've done it so again go look
for comedy or frameworks that do this
for you because you don't want to be
fooled by this intermediaries that you
have no idea that they exist
all right so very low-level protocol but
it has limitations all right they're
going to be to go away eventually
because all the internet physical
infrastructure will eventually upgrade
to understand what socket so all the
many of us will eventually upgrade but
you know you gotta take a little bit of
care these days let's go to the meaty
part of this presentation which is
speeding
so the real cool thing that I love about
speedy is that it's a live experiment
how many of you have a gmail account all
right most of it did you know that
probably how many of you use Firefox or
Chrome as a primary browser like okay
almost all of you so did you know that
what you are using speedy to connect to
the Google server for Gmail since I
think three or four years now you are
not using HTTP anymore
using that those browsers so the cool
thing is that speed has been designed to
be a better HTTP from ground up and has
been designed these days where the web
is different from the 96 times Google
Twitter if you go to twitter.com you're
using speeding if you're going to
Facebook using speedy WordPress speedy
this one is our own website web tied to
comp we use a speedy through jetty I
came here I took a place to stay through
Airbnb using speedy as well they are
everyone is moving to speedy like every
single day it's like it's a rash move to
speeding right your place to say to be
on the wire but it's totally transparent
for web application you take your wire
you deploy that into jetty you change
nothing in your web application you just
configure Jerry - tell Jerry speak also
speeding please
done no change on the developer side
so see
you just have to change the
configuration of jetty to enable a
connector the speaks speedy instead of
HTTP or also speed instead of a ship it
if you are not deploying your wire if
you don't if you deploy your wire into a
server that doesn't support speedy then
it doesn't support speeding I said again
embedded you have jet embedded
no well you can do it either way if you
stole Jerry as a standalone server you
can configure Jerry and letting speak
speedy if you do it embedded same stuff
you just you know attach a connector to
speak speeding no different whatsoever
I mean it's so it's actually what the
core is the same exact API so there's no
difference so what we have done in jetty
7 &amp;amp; 8 those T those two versions of
jetty were actually HTTP servers so we
hammered in speedy in order to make them
speak speedy but multiplexing was not
really the base of those two versions
because they were designed and optimized
for H is beyond so in Jerry 9 we have
totally Rilke tech that the NAO
are in the core of Gerry Hanley and we
have separated the well for for one it's
very easy to plug in multiplexer
protocols and for second we have totally
separated the semantics of HTTP from the
transport level right the semantics is I
want to do a head request to this
resource right now how do i transport
this semantics request I can use the
HTTP wire protocol to do that or I can
use a speedy to you know transfer the
same type of request ahead to this
resource I can just use a speedy how
does it work right because it will have
the same problem that we spoke before
about WebSocket if I have the client and
the server speaking speedy but
intermediaries may not understand a
single thing
about speeding so there's a trick here
instead of using a plain text connection
we use an encrypted one using TLS right
so the intermediary says a bunch of
bytes coming in say oops
they are encrypted can look inside sorry
just forward them to the server and the
responses from the server is encrypted
as well so you know the server the
intermediate will say oh this is the
responsive encrypted bytes I have no
idea just pass them down right and this
way we can get rid of the intermediaries
problem and this is valid for WebSocket
too right speedy defines a new framing
layer on top of TLS when you put in HTTP
requests and responses and of course the
good thing about speed is that it
carries the HTTP semantics so
differently from WebSocket where they
just put inside a bunch of opaque bytes
HTTP defines a semantics for what you
are transporting over right so we can
get transparent transport of HTTP on top
of a different protocol two features
stand out for spinning one is
multiplexing which is built-in finally
the blue pipe is a TCP physical
connection to the server and the green
pipes are requesting responses right so
speedy defines a thing called same
string which carries the HTTP request as
you can see here these packets have been
sent at different times and the response
err responses are carried by a another
speedy frame called soon reply and you
see that the replies are coming out of
order this one was the first request
being sent because it's the most one the
most advanced one but its response is
actually the last one okay so you can
get resources and you know requests and
responses out of all them now this is
really interesting because imagine the
CNN page 172 resources how would you do
it
or before was open six connection to
cnn.com send six resources six requests
right wait for time to come back then
send another six wait for them to come
back and another six and so forth until
172 right well they're not all on the
same server but you get the point
with speeding you open one connection
you show up and run 72 requests
concurrently in 172 green pipes and
you're done
okay this has a lot of benefits and
especially makes a lot better usage of
TCP connections uses way less resources
on the server and you know well
responses can be sent out of order so
it's good because if you get you know if
you want to get the five icon it's
normally a very small icon very small
file it can go back very quickly but
then you know you have maybe some
information that you want to generate
based on a query that you do on a
database for that particular user that
may take time to generate right but it
doesn't matter because it will now slow
down the request for the favicon all
right the key point here is reducing the
round-trip so I don't want to do this
six wait six wait six wait thing all
over again right because that's going to
slow me down a lot the other outstanding
feature the speedy has is called speedy
push so Jerry is actually one of the
first server that implements the speedy
push is probably the only one that
implements a speedy push and the
implementation that we have done of this
functionality is totally automatic again
you just deploy your wire you turn on
speedy push and Jerry you're done you
don't have to change a single thing in
your web application so let's see how
does it work it works in this way when
you request a page to a server right you
get a name our HTML page and you get the
page back then the browser starts
browsing the page right and then what it
finds you know first line says Oh
there's a CSS that I gotta ask to the
server oh there's an application
javascript file in a script tag in the
HTML that are gonna ask to the server
game right then you get these two back
you gotta parse them as well so you're
parsing the style of CSS and it turns
out that inside the file there's a
reference to an image right so get into
another request to the server and get
the image that's three round three four
four files right because these two can
be done in parallel on two connections
so three round-trips anyway now what if
we could build a cache that binds
together the first request the index dot
HTML with the resources that are
actually depending that are dependent on
that page that are present in that page
there isn't an inner end concept of a
primary resource in HTTP and secondary
resource the secondary resources are the
one that are depending on the page that
you are downloading right what if it can
be the cache of this information right
this would be a speedy push cache that
we build in Jerry actually so that when
you do
speedy push what we can do in jetty and
what we do in Jerry is this you ask for
the index dot HTML page and then the
server knows that you not only need the
HTML but you also need the JavaScript
the CSS and the image so you're doing
basically one request and you get four
responses back because that stuff that
you need eventually right we have a
bunch of heuristics in order to not push
data that the browser has so we're being
smart about it but HTTP 2.0 will be even
smarter okay HTTP 2.0 takes these
concepts to a next level support of
browsers 50% 55% of browsers out there
means the majority right the only
lacking partial is of course a interest
Laura they're always late but eternal
Explorer will 11
will support speeding microsoft has
already said that there will be a speedy
three implementation in internet's
plural eleven so Microsoft is you know
joining the parking late but it is
joining the party because they have
realized that the new web is speeding
all right
HTTP is going to die yes
speedy push is a is defined in the wire
protocol as of as how do i transport the
push stuff that i want to push the
implementation it is dependent there are
servers that don't push there are
clients that don't accept push the
strains right chrome and firefox the
latest version do accept push the
strings so if you have a server that can
push they will be getting this push data
and for work we have a demo in a few
slides and i'll show you the big
difference that it does yes
say it again if it is possible from for
a server to understand if the browser
supports PD yes we that's actually what
we do in geni we we have a connector
that folds back to HTTPS because
remember speedy runs over TLS if it
doesn't support speedy so absolutely it
is the way he runs yes
what about push no no this is not making
this information is not travelling from
the client to the server but the server
has the client has a way to say I don't
want the push data and it can operate a
thing called presenting the string that
the speed is string if it doesn't
support speedy so eventually the server
knows but it may know a little bit too
late it has already started to push data
and then the client says hey I don't
want it and you know the server can say
ok our you know I wanted to push you a
big file I pushed you a little bit now
I'm going to stop because I know that
you know you're not accepting it so it's
something that comes later on yes
no no yeah the green pipes that I show
you this once let me go back for a
second over there
this is green pipes when the
request/response is finished they go
away right there I mean if you have an
idle connection it will just be blue not
no grains the server and the client can
agree on the maximum number of
concurrent connection there is a
mechanism in speedy to send
configuration options both way and they
can agree and say I can accept that most
hundred concurrent connections so it's
configurable
yes
no no the intermediary the intermediary
will see the TCP open and it will
forward that and then you will see a
like a server al client'll or TLS client
alone at the point on the intermediaries
out of business because it has to take
those bytes do not even look inside them
and forward them over so the the
intermediary cannot look inside what's
inside so it will just work
exactly exactly what was I was telling
the the trick of using TLS the works in
speedy will work exactly fine for double
for WebSocket too because it will just
say to the intermediaries Dom but you
know don't even think that this is HTTP
because it started as HTTP you remember
the upgrade you know forget about the
upgrade if when you're just looking at
our encrypted bytes you don't even see
the initial upgrade HTTP upgrade Wow one
second I'll take questions later but I
really wanted to you know go to the end
of the presentation but you know stay
with me so let's go back to the most
interesting part here the demo ok so
let's see the difference that speedy
makes right so you see here fine all
right so this one is a is a page that
shows um next to this one there's a page
that shows the behavior of speedy in
well shows the behavior of a page in
three different cases under HTTP
normally under speedy but without push
so just multiplexing and speedy push all
right the page the follows is a page
that is made of a big image made of 20
smaller images that compose the page
together right so we have a typical case
of a primary resource with 20 correlate
to the resource secondary resources to
it right ok so I think
let me find my mouse you go so this one
is the is the HCP behavior right so I
took a quick trick here I had to slow
down the local host interface because
otherwise the effect will not be visible
at all because it's local Oh sis too
fast and too fat pipe but it eventually
shows so there's a 200 millisecond delay
between like a round-trip delay but in
order to show you what happens so pay
attention
after I click because something strange
happens
all right did you see how at the
beginning it was like plop plop plop and
then pop became faster at the end that's
a TCP feature called slow start TCP
connections open start when they're open
they're cold when they're cold they can
transfer only a limited amount of data
every time you use this connection they
get hotter and hotter and hotter and the
pipe that they can transport becomes
bigger and bigger right so if you're
sending small requests over six
connections or maybe 24 connection
because you're doing the main charting
basically what you're doing you're never
getting these connection work enough
right so you're still like running a car
in the second gear you're never going to
the fifth gear right so if I now reload
this page the connection has been made
hotter
yeah it's you know it's going a little
bit better right let's go back and see
the same page but using speedy okay
quick see the difference it's like it's
perceptible it's you know I don't have
to convince you you can actually see
that it's way different this one is a
new connection so it's cold but because
we shove the 20 connection immediately
those that connection because hot very
quickly and of course because it's hot
it can download back the data much
faster and also concurrently right for
the first one I had to do six back six
back up to 20 right here I just open one
connection 20 20 back right so if I
reload this page now the connection is
even hotter in some way you know it's
way faster
okay that's only multiplexing in action
now let's go for the magic we go back
and now we do speedy push now pay
attention because I have a question
after
click-click who's thinking is faster
than speeding good guys who thinking
slower all right so you're wrong it's
exactly equal to speedy and this one is
the first request and Jenny did not have
the cash for right it's with this
request that the jeddak was able to
fulfill the cash right so now if I
reload this page click you don't say it
right so now now chrome was playing bad
with me but because some time was
crashing but let's click here boom this
is the kind of difference that we want
after 15 years of web right we don't
want
click-click click-click click-click
click-click we want this right so that's
the difference one second questions let
me finish the presentation I have plenty
of time JAMA one is finished unless you
want to go to the rap half but I'll be
out there for questions for for you all
right let's go back to the presentation
here I was able to go back to the
presentation here we go
all right so word on Gerry 9 1 which is
our latest release it was published last
week and at its polyglot speaks HTTP web
socket speedy and we're working on
making it
speak fast CGI to well I have a word
about this as well it is implementing
the server 3.1 specification it's jt9
one is great as a production server 2
it's a not a monolithic server you can
start only the modules that you want if
you don't want WebSocket because you
don't use it you can just start Gerry
without the website
module it would just work with HTTP and
whatever year else you have chosen to to
have Gery served for you right there's a
bunch of configuration possible it's
very easy to customize we have changed
the way Jerry start in 91 to make it
very easy for people that embeds jetty
that repackages jetty into their own
product or even install Gerry as a
standalone server in their machines to
be really easy to upgrade for example
the jetty release to a minor version
afterwards right so you have noticed how
throughout this presentation I was
continuously repeating jetty was one of
the first server to do person that right
was one of the first Java server ever
written one of the first to do niÃ±o one
of the first to do WebSocket one of the
first probably the only one Java one to
do speedy and so forth so we have this
this key concept in our project which is
where we want to be on the edge of the
web right we want to be exactly at the
front we want to take the wind that it
takes when you're surfing the the wave
right so early adopters benefit from the
fact that we are going that edge they
can have an edge right now on
competitors because just a move their
website to speeding right your website
will behave like the demo more or less
and I have proof of this there's for
example Yahoo moved the part of the
internal server to speedy they just
can't enough the times to show pages
like at an office is a lot it's like
come on it's a it's a very different
user experience right late adopters will
benefit of the fact that we have pushed
these features this web page features
early and we gain the experience on them
right so our experience implementation
stuff is probably very stable it has
great performance very easy to push 30
to 70 Kay
Qwest responses per second on a single
server and remember these are not HTTP
requests you can find any benchmark out
there that says oh we can do 200k
requests per seconds but those are
probably simple HTTP requests you put
the response in a byte array and then
you just show the biter right down to a
socket that's not a server request the
server request has to come in to the
server container do all what the
specification says you have to do with
that guy you have to you know anoint the
request and the response it with all the
servlet goodies because otherwise
they're not good so there's a lot of
work that needs to be done by a server
container in order to function properly
as a server container so these are 70k
requests and responses servlet requests
are responses per second okay so it's a
very scalable server very easy to
connect 250k clients to a single server
I actually went up to 450 in the latest
benchmarks that we did you know the word
cannot be in stable some of them was
disconnecting but you know 250 was easy
it's like okay just you know fire up as
many client as one connect all these
guys to the server no problem at all
right so you can actually get a lot of
benefit using Jerry because instead of
using one giant server or 20 small
server you can actually use less of them
or a smaller one because it does scale
very well we have tested the server 3.1
implementation for synchronous i/o and
we were really worried but actually
turned out is on par with several three
zero on performance wise but it under
load it will just use less threads to
run but the performance throughput wise
is on par with three zero so it's it's a
very good result believe me I know the
internals and we were really worried the
server 3.1 was way slower than three
zero we were really worried but
we figure it out and it's important so
intaglio does provide commercial support
sense so it's not the open source
project that you download and then you
know you're on your own
we can do all the hard stuff for you and
you guys just concentrate on your
business side right we do the
configuration for you we use all the new
features you have a special use case
that you want done implemented into
jelly we are there for that performance
this is the same crap as before but the
difference is the scale now 50k messages
per second is here this one is under K
messages per second so it does scale a
lot better and this is HTTP a
synchronous HTTP with server 3.1 we were
able to connect before we had to stop a
50k clients but now we could connect up
to under K clients to a single server
before having the server be unstable so
well remember this this guy are singing
20 K requests and responses per second
so they're not idle connections right so
it's it's a really good result a quick
run through the servlet 3.1 API for a
synchronous i/o this is your servlet
with a service method first thing you
have to do is you have to call start the
sync this is from server 3.0 you make
your request and response is basically a
synchronous then you have to create a
grid listener interface implementation
grid listener is a new interface that
has been defined by super 3.1 and you
have to pass this read listener to the
circle industry calling separate
listener and then you return from the
service method you don't do anything
else all the rest will be done by the
great listener grid listener
implementation has two methods that are
called by the container one is called on
data available it will be called when
there is data to read and
you have another method call on all data
read which will be called by the
container when you have read everything
okay
now I skip the implementation for a good
reason that I'll show you in a couple
slides symmetrical to a read listener
there's a right listener it only has one
method here called on write possible you
get the output stream you have your
stuff to write you pass that to the
output stream now the output stream
could write one byte maybe two bytes
maybe the whole of it maybe the whole
weight - one bite you you don't know
right but you have a method on the
upstream to query the output string hey
and be finished are you yeah you know
did you write all what I passed you if
not you have to exit from this method
and then the container will call you
back when a write is possible again
otherwise you can loop and you know
write even more at a certain point you
have to figure out when you're done
writing and you have to call a sync
contacts complete you're not supposed to
read this this is a single echo servlet
written using blocking i/o server 3.0
style
well even server 2.5 style that's
basically five lines of code this one is
the same implementation using your
synchronous i/o right it took me three
attempts and four hours to write this
guy this one is probably one attempt in
30 seconds and because you know just if
you have a cool idea you just go there
and say control space and it will
probably write all this while loop here
for you like you don't even have to
think about it now this stuff here is
way more complicated Greg Wilkins the
jetty project founder look at this and
say Simona you know what I don't think
it's right and I said well it's correct
then maybe there was a better way of
doing this and so he went on I wrote a
totally different implementation almost
this long actually probably
exactly this one but it was completely
different I was doing stuff from the
read methods he was doing stuff from the
right methods what I'm telling you is
this is you don't have to go a
synchronous i/o in order to be scalable
if you use Jerry right you can scale
even in this way because we are doing
all the synchronous stuff for you under
the covers right if you really need to
go a synchronous i/o then be prepared to
you know look around Google a lot
experiment in and stuff because the API
is not really for the junior developer
I'd say I mean I'm I'm on this business
since a while and it took me like three
attempts to write it correctly so it is
not a simple API having said that it is
a synchronous so you actually get the
performance benefit of using less
threads last slide and then I'm for
question we have a bunch of new features
in Jerry 9 1 we have totally rewritten
HTTP client again we have totally
removed synchronous blocking HTTP client
it's totally synchronous 70 percent
faster than Jerry AIDS we have an HTTP
proxy a speedy proxy we are working on
fast CGI and at this point Jerry will be
able to serve PHP Python Ruby by calling
a fast CGI server by proxy into a fast
Jer server so basically you don't have
to put a patch in the front
Jerry in the back or nginx in the front
and Jerry in the back in order to serve
mix it content like PHP and Java right
you just install Jerry we do everything
for you
last slide small word on HTTP 2.0 there
were two proposals but eventually speedy
will be used as the word as the base for
HTTP 2.0 so all the stuff that we allow
you to do now it's the stuff that you're
going to be able to do in the future
with HTTP 2.0 so basically we are
time-traveling you
into the future right because you are
capable of doing future things now
there's a there's a sixth draft for HCP
to the zero out already they're actively
working on it it will probably take I
would say at least one year at minimum
to be kind of standardized at the
protocol level it will take a little bit
more for browsers to catch in and do the
implementation especially internet
explorer of course it will take a while
for server to update it will take a
while for intermediaries to upgrade so
the other key point here is that be
prepared to stay on SSL for a while
right because that's the way you have to
like get the intermediaries out of the
picture right eventually the whole world
would update maybe our you know
grandchildren Oh see that so big
evolution in the web WebSocket wicked
fast speedy super cool super performance
in there with speedy push all of this
it's now already you can use Jerry do
stuff of course call us because we are
you know we are the most experienced
guys in this stuff so we're there to
help we can save you a lot of googling
around by doing the stuff that you don't
need to do because it's not your
business right we do the Jerry stuff
because we know it like from inside out
you do your business stuff you save a
lot of money a lot of time lot of
engineer time many time you know if you
call us because we do stuff that you
will be able eventually to do it but
like we do it in one tenth of the time
right so a bunch of references and go
for questioning us</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>