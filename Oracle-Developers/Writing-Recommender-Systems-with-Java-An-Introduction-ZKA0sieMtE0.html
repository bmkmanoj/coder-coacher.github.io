<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Writing Recommender Systems with Java: An Introduction | Coder Coacher - Coaching Coders</title><meta content="Writing Recommender Systems with Java: An Introduction - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Writing Recommender Systems with Java: An Introduction</b></h2><h5 class="post__date">2015-06-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ZKA0sieMtE0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Alan Barsky it's a pleasure
to be here thanks for coming and I would
like to talk about recommender systems
in Java and about how to write them and
that would be also a practical demo
showing how things work in practice
so like the agenda is the first part of
the presentation will be more of a
theoretical background and so there will
be a couple of formulas but we don't
just just go straight if we see what
types of recommenders they are what what
form the input data can take then we
will take a closer look at how to
compute recommendations with my hood my
house I'm not sure how you say it in
English
I guess my couch is okay but anyway it's
not crucial to say it properly and yeah
and then like I would just mention how
how to do things with spark and how to
write custom recommenders okay so let's
start and yeah and I have to introduce
myself
sorry I almost forgot about that and so
that you act like you know who's talking
to you and so yeah so day to day I am
the co-founder and software engineer at
subdermal it's a one of the best
software houses in Poland and that's of
course because I work there and and yeah
so I also we are we develop various
systems within Java Scala groovy we are
a type safe consulting partner and we
don't stop mainly for the u.s. that's
part of why I'm here yeah in the
evenings a I write a blog I invite you
to visit it I write about the various
technical subjects recommenders as well
but also Scala ibeacons docker various
stuff and I also am also doing some open
source projects assignment the original
author of hibernate universe which is an
entity holding library and I wrote my
choir which is a library for doing
dependency injection in Scala in the
cell libraries of the framework and
that's an important
sanction yeah and a couple of others so
how did I how they start writing
recommenders so one of our clients is
the app studios and they have an app TV
and we were writing in a back-end for it
and the app tier is basically like a
social TV application so you have you
know the TV schedule and we import
favorites from Facebook and now basing
on these favorites of course you can do
recommendations right so you can have
some shows recommended like what's what
what kind of shows you will probably
like right so this can be both movies
and TV shows like serious and we are
doing this recommendations basing on
Facebook favorites and like internal up
favorites and there are some business
rules here like you probably wants to
recommend shows which actually air in
the next week more than some show that
was airing two years ago yeah yeah yeah
yeah sure so yeah like that's just how I
started looking at recommenders but it's
like all like it generalizes to any
domain yeah yeah so some talk of
background about recommenders so when
the right thing I recommend err the
first thing we have to do is we have to
actually identify what an item in a
system is right so it can't be TV shows
it can be products in a shop and we can
actually do the recommendations in many
ways right we can recommend items to
users so that's like the canonical
example they can recommend items to
items so that would be for example
finding similar products with a given
product is so we like you go to Amazon
and you see that users who bought this
product also what these are the products
right so here we recommend items to
items you can recommend users to users
so like a dating service for example
right and he would also have some other
business rules like how you mix the
users and
and you can also recommend users to
items so like let's say you want to you
have some stock of shoes that you wants
to set right and you want to see what
you guys are most likely to buy these
shows so the first step that we have to
do is we have to actually identify
what's our item that we want to
recommend of course and like which way
the recommendation is going so then once
we've identified that we will have some
form of input data and that comes in a
couple of flavors so the two most common
are that we either have a ratings so we
have three item two pools we have some
user IDs right we have an item ID and a
rating which like it can be one to five
stars can be one two three one two ten
like domain dependent right but we have
some kind of rating sometimes now
intuitively when you know on the
practical side and the recommenders just
accept a CSV five CSV file for example
or something like that now where you
have this tuple defined so the other
form of input data is when we just know
if a user interacted or likes an item or
bought an item right so it does have two
item to post so we don't have the rating
and so this kind of input data is called
is called eunnarae because it's even
worse than minor right in the binary
case we know that something either
happened or not and here we only have
information if the other bought the
product or likes the product right if
there is no user item to put we don't
know if the user have hasn't seen the
item at all or if the user doesn't like
the item right so we only wanna know the
positive binding so we want to know the
the positive side and that's actually
the most common form of data where you
have unary input yes so the sources of
input can vary you can have implicit
input like a click stream like you can
record the fact that somebody read an
article a watched a video on YouTube
right and and actually like in a search
engine
like when you have top ten results if
the user clicks on the third this
actually gives you more information than
just clicking on the third right because
you also know that he didn't click on
this first two and it's probably
important why he didn't click on the
first all right so this input data it
can be enriched that way sometimes we
have explicit ratings like Facebook
favorites or explicit ratings of movies
on the Netflix right we rate them one to
five and so on okay so how do we
actually do the recommendations so the
most common way is first we try to make
a prediction
so given a user and an item which we we
try to predict what rating will do that
will the user give to an item right and
once we have all the predictions ready
we can sort the predictions and just
take the top end and that will be our
recommendations because that's a bit
harder if we don't have the rating scale
and but then we can like see how close
he has to to a one or what's like what's
what is he most likely to to buy for
example and salt on a basic on that okay
so there are two basic kinds of
recommenders the first one which I'm not
going to talk about here content based
recommenders so this when you are able
to describe your items like your items
in a shop with some features right so
you know for example a movie we can
describe the movie using a lot of
features like the town who stars in it
and is it like a what kind of movie it
is and and and so on so for each item we
are then able to define a vector of
features right and each feature it's
going to be either either 0 1 it can
describe how frequently a term is
present in a document using tf-idf and
and and so on so but in this case the
crucial thing is that we can somehow
look inside the items write what they
are and then once we have that it's
actually to do the recommendations we
compute the future vector of of each
item and then we have a user so we know
what like we know what his history and
we can compute the user a vector by
summing all his the the item that he
liked about and then the actual
prediction is we take the cosine between
the two vectors rights of the vectors
are like vectors in a very big
dimensional space so we can calculate
the cosine and the smaller the angle and
the more likely the user is the like the
most similar the the item and the user
are okay so that's content-based
recommenders but we're actually going to
look at collaborative filtering so we
are going to look at the case when we
don't have the insight into the items
right yeah the items are kind of non
inspectable right we just know that
usually users like something or not and
we will try to see what if there's any
structure in there right so we don't
know what the items are we don't we
can't look inside we don't know what
there are features and and let's like a
much more general method so it doesn't
look inside the items but it tries to
find some structure just basing on the
fact if somebody likes an item bought an
item and so on so it's called
collaborative because it's based on data
from many users right and we try to like
capture the taste let's see so the first
type of collaborative filtering is user
user clapper of collaborative filtering
and actually when we will read about
recommendation systems you will find
this sound quite often I think this user
user of cobbler filtering and item item
so first ID in the user user case we try
to measure similarity similarity between
users right so we need some kind of a
metric and which tells us how similar
users are and then the our prediction
will be the weighted combination of the
ratings of other users right so the more
similar another user is the the bigger
the weight to his rating of an item that
we want to see if the given user is like
by oh not so an important thing when
doing recommendations in the genera we
must make sure that our domain is scoped
so there's agreement so if we try to do
recommendations based on like let's say
political and food tastes that's
probably not a good domain because if
two users agree politically they
probably won't agree on food like
there's like no indication is no
correlation here
and and the other way around as well
right food tastes probably don't
influence the political views but maybe
they do but it's and another interesting
research area so we must make sure that
the domain in scope so that there's
agreement among the users about things
right and also importantly the
preference the preferences should be
stable so it's not like like if I like a
movie I probably like it in a week and
in a month right it's not something that
changes day to day okay so how do we
compute user of similarity so there's a
lot of different metrics and we don't
really you don't really have to know the
math behind it but I guess it's good to
know like what the what it means roughly
and so a popular metric for example is
Pearson correlation and it just takes
two users so a and you you our users and
we take like the we try to see how far
how different their ratings right so the
they are with the bar on top is like the
average rating and that's because you
know some users if they get before it
may be a very good rating right and
sometimes if a user gives a four it may
be a very bad right right so one user
may like give a three as I moderate and
therefore is a very good and another
user may give five as a moderate and
four if the movie was particularly bad
right so we have to somehow normalize it
so that's why we saw there with the bar
is the average rating and that's how we
normalize each user's ratings we
multiply the differences and we multiply
and if the users so the users agrees
will be like it will be positive
the SEC really negative so it will make
this some smaller yeah yeah there we the
bar is an average of ratings of all user
yeah yeah okay so now that we have this
matter in this similarity metric we can
actually try to do the recommendation so
to do that we don't look at all the
users we just look at some users that
are a most similar so we take a user
neighborhood so that's another term
that's going to pop up quite often and
so typically we say we look at somewhere
between 25 and a hundred users that are
most similar to the user for which we
want to do the recommendations right and
it usually works fine we can find you
then fine tune that parameter later once
we have some method of evaluation
finally that's like the formula for
prediction again you don't need to know
it like it's all implemented for you
in market and other systems that help
you with recommendations but again it's
it may be good to know more or less
what's happening so once once we do the
prediction the wao is the weight of
similarity between users right so we
want to see what's the predicted rating
for user a and item I so we take again
the so like before we have not
normalized by subtracting the average
rating now we have to edit back right so
we get the right amount of same scale
and then we wait a the we wait the
ratings of the item for which one to do
the prediction from other users weighted
by the similarity and we do that sort so
the sum here the N is only for the users
from the neighborhood and that's that's
how you do the predictions then you sort
and you have your results and so item
item club or filtering is the other kind
it's quite similar to user user but it
works with the items which is probably
not very
rising due to its name and here we have
to define an item similarity so here we
don't compare users to compare items and
we look to items which are most similar
to what the user has already found right
so before we are looking for users who
are similar to the given user and
waiting their ratings and here we are we
are looking for items which are most
similar to the items the user has it's
hard to say which I am which one is
different so in some cases the user user
will be better and in some cases the
item item will be better named Matias so
it's you have to find that usually it
does doing experiments yes also another
algorithm which I will talk about later
co-occurrences where we just count how
often it items go together it's like
very it's a very straightforward
algorithm but it also works quite well
so the final part of the theoretical
background is so at some point we have
to actually see and we have to actuate
so we need some way to check if a
recommender is good right so we have
written our recommendation algorithm and
now we want to know they have various
parameters like we can take different
metrics for comparing users or items we
can pick different neighborhood sizes
and we can take user user or item item
right so that's a lot of choices so how
do we tell if a recommender is good and
in this manner it's a very tough
question Dino is the answers
unfortunately and so the like the most
obvious idea is to take a recommender
take the training data that we have
right now we take a user we remove some
of his favorites preferences some of the
items that he bought and then we try to
compute recommendations for that user
right and we see if the items that we
took out from the model actually
reappear so there if they are
recommended back to the user right so
let's that's it like leave one out but
we can like take out ten or a hundred
depending on how many a ratings the user
had in his history right so that's like
leave one out method and we can also
extend it to a like numerical metric by
calculating the mean absolute error
which is like if we have ratings of
course if we have ratings we can
calculate how much the predicted rating
differed from the real rating right from
the items that we have taken out of the
model and we can simply calculate I mean
error another metric is a mean squared
error where we square the results so if
if the difference is bigots it it
punishes the implementation more so it's
a numerical metric we can use if we have
ratings if we don't have ratings we can
use something called precision and
recall so again we take M we take out
some items from the model and we see if
they are recommended right so now we
cannot compute two values one one is
precision so we can see what percentage
of the items that have been recommended
back actually in the in the list that we
have taken out right so how many good
items are in the recommendations when a
good item is something that the user
already likes or something that you have
already bought now in a similar way we
can calculate recall and recall is the
percentage of items that have been in
the in decide that we have taken out and
I recommended back these are like two
values that you get that you can
calculate and you can compare different
recommenders basing on that numerical
values there's like one fundamental
problem with that is that it's very hard
to measure novelty right so so far the
all the evaluators that we have seen in
fact punished recommenders which try to
recommend novel or new items right
because we all we have always taken some
items from the existing like dataset we
have had them
right from the recommender and we try to
see if they get recommended back right
but maybe we have a great recommender
which always recommends new items which
the users haven't yet seen but they
would like it a lot right we don't know
that it's like impossible to check like
what the users will like or not and so
yeah that's like a fundamental
fundamental problem with a with the
evaluation and that's why the best
method is to do a be testing on the
humans simply and in fact that's a very
often how recommenders are evaluated so
like maybe the in the initial phase you
do like the metrics the precision recall
or the mean squared error but then when
you actually deploy it you can do some
AP testing to like fine tune the
parameters yeah okay so the question is
how would they be testing work so you
deploy to recommenders like two
completely different 100 different
parameters right and you direct half of
the users to one recommender half of the
users to the second of commander and
then you have to measure some kind of
metric again right so you can measure
like a how many users click the
recommended items or how many of them
buy the recommended items like so how
good are the recommendations petpak
depends on the domain okay there also
ways to increase diversity and
serendipity and serendipity is an
interesting word it like doesn't have a
translation to many other languages like
I'm from Poland and as there is no
polish translation for the words it's
actually hard to understand what it
actually means and but I think I got it
anyway diversity so at least when I go
to Amazon after buying a book and what I
see is like a recommendation list with
books on exactly the same subject that I
just bought right so the diversity is
very small and that's not really good
like I just bought the book and why do I
see 20 books on the same subject that
I've just bought so it it may be good to
like willingly
increase the diversity for example when
you build the top and list
right and you see that the top three
items are very similar to the fourth
item that wants to add this similarity
here we can measure using our metrics
maybe it's not worth adding that that
item right we want to keep the
recommendation diverse so that they are
really engaging to the user right
similarly with with serendipity maybe it
doesn't make sense to to recommend items
which are really popular right maybe it
doesn't make sense to put the most
popular movie on they recommended to
watch this because the user probably
knows about the movie anyway right so
maybe we want to a and we want to
encourage the user to somehow explore
our products items whatever we recommend
which are like not the most popular ones
right like yeah that's always a good way
to the recommendations is to simply
display the most popular items to
everybody right that also sometimes
works our that creates a loop because
the most prefer items are recommended
and then they are even more popular
because they're recommended and it's
like with pop songs on the radio and
yeah so there was a Netflix challenge a
couple of years ago where they had the
million dollar prize for the best
recommender and the to measure which
recommended does better or not
they use the mean error metric which I
showed you before and actually they
winning algorithm a won by predicting
the low ratings more precisely right so
a it wasn't really the maybe the best
one because you know you don't really
care if the algorithm it makes a 2 star
1 star prediction accurately you only
care if it makes the 5 and 4 sub
prediction accurate right so you have to
watch out with the metrics as well ok so
now going to the practical part we will
take a look at a mahute my hood has two
implementations of recommender systems
one run on a single node and one runs on
so we will take a look at the single
note scenario and actually it's very
often enough to just use a single note
and only if you have a really a lot of
data you you have to go distribute it
and go to Hadoop and so the single note
case it works like fine until you have
like 10 maybe 20 million data points
where a data point is like the
preference I can enter it in the CSV
file write the user item pair or the
user item rating tuple if you have a
couple of hundred million then you
probably have to go in scale out this
very if you get interested in the
subject there's a very good book a my
Houghton action it's written by the guys
who wrote my hood so they probably know
what they are writing about yeah so
that's what you know so in the examples
I'm going to use a free dataset called
movie lengths you can download it from
the internet it has a couple of
different sizes right so I think I'll be
using the one on 100k and that means
that it's this 100 K and a preference
values as input so it's and actually a
lot of the if you read about
recommenders you can see that this data
set is referenced quite often you know
as a reference point because it's very
easy to use okay and so let's let's do
some coding idea okay so I have I have
the data set down downloaded here you
can take a look and so these are the
entries right so go to the top so that's
like the user ID that will be the user
ID that would be the item ID in this
case it's a movie ID this that's the
rating and here they also include the
timestamp and of when the preference was
was recorded so when you do some more
advanced recommenders you may also want
to look at the temporal aspects so like
the
initially I want to like action movies
but then I got older and I started
liking romantic comedies or something
that I know maybe that's right relevant
in some way and so yeah so that's our
input data that we are going to use and
we also have some supporting data so
that we can actually see the title of
the movies and I have some code ready
for that so I have the movie database
but I just read in I just read in the
movie titles that's you dot item yeah so
here we have the mappings right so
that's a comma separated the CSV file
which uses pipes and commas and that's
the movie ID we have seen and that's the
title of the movie so then when we
actually do the recommendations I have
an author class here it's a very simple
class which just takes in the
recommendation sorry the recommendation
that we have produced and prints out a
nice you know this movie in what that
title got this rating okay so let's
let's write a very simple recommender
okay and we'll make it actually I just
realized that I'm writing in Scala not
in Java but hopefully that won't matter
like it's all thus creating classes and
invoke methods so sorry I probably
should have done it in Java but movie
I'm doing day to day scale and I just
didn't think about it for some reason
okay anyway so first of all we need to
read in the model so there's a file data
model in my hot class which
automatically parses a CSV files so we
have to give it a file and I will just
type and pass users Adam W that's me
projects my presentation 100k away
Donbass right so that's the file that we
have seen with the actual ratings okay
so we have the model so we have like the
training data right so now we have two
so first we will take a look at user
user collaborative filtering
recommenders
so first we have to define how we will
measure you use a similarity so let's do
that so have a user similarity object
which will be person coloration
similarity alright so that's a class
that implements that implements the
similarity according to the formula we
have seen before and we have to pass in
the model then we define the
neighborhood hood new nearest and user
neighborhood yes we have it okay so now
we define a when doing recommendations
what's the neighborhood of users among
which we will look for recommended items
so we will look at 25 nearest users
right and we pass in the user similarity
and the model ok finally we create the
recommender generic user base
recommender ok so we are doing a user
user collaborative filtering so yes so
we are using the user base recommender
and we have to pass in the model
everything in the model neighborhood no
how did it figure out I wonder this from
neighborhood I don't know but user
similar okay now we can now that we have
the recommender so the recommender when
we actually try to do the
recommendations it looks at the user
neighborhood and it tries to recommend
new items for the given user right so
recommender dot recommend so now we have
to pick up a user so I have picked up
user 925
it's a popular random number which I
have printed here and and we are doing
and we will get 20 recommendations
recommend shoes okay and here I'm going
to use my printer class to print them
nicely okay yeah
so what's the meaning of 925 that's the
user ID for which we are going to do the
recommendations okay so in the in our
data file we have some existing
preferences for the chooser and basing
on these preferences and on the
neighborhood of that user we are going
to compute the recommended and the
recommended items okay so what is it
read I don't no idea
that's right one maybe it will compile
yeah
25:25 here yes so that's the user
neighborhood so that's the size of these
are neighborhoods so when computing the
recommendations we have to we have to
define like at what number of similar
users we are going to look at when
trying to looks in our items so 25 is
usually a reasonable thing to start with
I have to import one scholar Java
compatibility class and it's okay yeah
so let's run this yeah that's some
logging and we have our recommendations
here right so user and 925 most probably
will like this movies right and here we
have the predicted ratings right so we
can see some known movies and we can see
some movie that are not so unknown this
actually by a Polish director so very
good movies number they are actually
good and platon quite an well-known
movie right Shawshank Redemption also
quite well known movie so yeah so these
are like the recommendations for that
person we can actually probably there is
somewhere here fi which tells us
information about the users new user I I
have so yeah I was looking for it before
the presentation so we can see that you
know 925 is a Salesman it's a she and
she has 18 years old she has 18 years
here so yeah we can find out that
information if that helped us in any way
ok so we had done some recommendations
and now we can use another recommender
so let's say for example that now we
want to try the item based recommender
so let me just comment it out for a bit
and now we will create a user base
recommender so now instead of a user of
similarity we have to define an item
similarity and that's going to be
Pearson correlation similarity so this
in fact the same similarity and that
have you used before
implements two interfaces right it
implements both the user similarly and
item similar to interfaces and that's
all we talk we don't define a
neighborhood because we are only
comparing items not users so we can
recommend there is no generic item base
recommend right but it's almost the same
but different different class okay so we
have a different recommender we can try
to run that and most removal to get
different recommendations this now this
time yeah like the top of I don't
remember what was on the top before but
now it's definitely different I'm not
sure what jaws 3d is doing here but yeah
and okay yeah so we have like two
different recommendations right we can
also try to use a different metric so
here us in the color the purrsula
Croatian sorority we actually go to the
implementation we can go like to the
interface abstract item similarity and
item similarity so now we can take a
look at what are the implementations
right so let's yeah there's quite a lot
of different implementations emotionally
that's not magnified but hopefully you
can see there's like Euclidean distance
similarity a log likelihood similarity
Tanimoto coefficient similarity so these
are all different ways to calculate the
synergies between two items right we can
let's use the tiny motive example one
right that's a well-known one of them
Tanimoto correlation similarity
coefficient sorry sorry yeah so and we
run it again we get different results
most probably yeah
again we have a different movie at the
top right okay so now we have all these
different recommenders how do we
evaluate them
so there's support in my code for that
as well okay like that and so let's
let's let's for example use the user
similarity I will comment it out okay so
now when
actually why did he remove the impulse
taste okay
so now when we want to evaluate the
recommenders we have to somehow a
abstract from the model right so when
evaluating recommenders we will take
some items out we will build a model
basing on the rest and we will see what
gets recommended right so the model is
not fixed right now right we will be
removing some items to doing
recommendations saying what's the value
of the metric we'll take some other
items out the recommendation and so on
so we have to provide my hood with a way
to build a model to build a recommender
giving a model and it's like what we are
going to do so we have the Builder which
is a new recommender builder recommender
builder and we have to implement the
built recommender method in which we
have the data model fixed right so we
can just copy that code that we used to
create our recommender and instead of
model we have to use data model okay so
now we have to find a way to build a
recommender given a data model right so
during evaluation this data model will
have some items taken out okay so we
still need the big model so to provide
like the seed and now we create the
evaluator waiter and now it's going to
be a fun plus named average absolute
difference recommender evaluate yeah
there's only one much of that yeah so
that's I guess as long as a class thing
can get evaluator dot evaluate build our
no I will talk what's the parameters and
I will say what's the premise in a
second and that's going to be our score
right so we are no longer doing
recommendations but we are going to
print the score and that's going to be
the score for the recommend okay print
line so what happens here is we
instantiate an evaluator so the
evaluator will use a give some kind of
metric here
it will use the average absolute
difference so the average mean error
right now they mean error metric and so
it will use that to actually compute the
score and then we have to pass in the
way to build a recommenders to the
evaluator that now is an optional data
model builder so instead of giving a
recommender builder we can give a data
model builder it doesn't matter that's
our starting model from which we take
the data then we have to define what's
the percentage of data that we will use
for training so here we are nine percent
of data for training and ten percentage
of data to evaluate then we can evaluate
and then we can define how much of the
data that from the data that was
evaluate wants to actually use to
validate the results again we can run
that and we have to return the
recommender so this okay
and it's running it's doing some
computation it took like two seconds so
like the score is zero point eight to
nine so like that's a number right it
doesn't have to mean anything to us so
what it means is that like the average
predicted score was off by point eighty
nine from the risk of right but we can
use it to compare our to compare the
recommenders right so if we now use the
Tanimoto coefficient similarity and we
run our computation our evaluation will
probably get a different number it would
be very unlikely if you got the same one
and it's taking a longer time to compute
them two seconds so maybe it will
eventually Claire mini seems like a bad
choice for similarities metric all right
we got it when they eat it oh so it's
better right so the average error is
smaller right and we can also manipulate
other parameters like we can use the
person again but use a neighborhood of
100
so let's see like with the neighborhood
of 100 maybe it's going to be better
let's point 83 right so it's almost as
good at with the tiny motor 1 and you
know now using so we have some
parameters which we they can kill here
we can use the item based recommender
and basing on that metric we are able to
somehow evaluate our recommenders yeah
it's randomized yeah so now if we rerun
it it will be a probably different
number yes point 80 tonight now it was
point 83 may about this close like it's
randomized so you know you should expect
not a not identical results but roughly
the same ok so now a couple of other
things that we can do with the
recommenders let me just remove the
evaluation stuff and so sometimes you
need to include some business rules so
like in the recommending TV shows right
we wanted to probably recommend shows
that are airing currently and that are
not also right and and my hood has
support for that as well so we can we
can add something called anhydrous color
and it's used to risk or the it's used
to risk or and the disco that mahute so
internally Emma hood gives each item
some score right and higher the score
the more likely the item is to be
recommended so here we have an
opportunity attraction to actually
manipulate disco right so we get an ID
of the item and the original score and
here we know we can like check in the
database if for example if you are doing
an e-commerce recommendations if the
item is available right so that we don't
recommend unavailable items or we can
check if the item is relatively new if
maybe if you want to promote it we can
like return double the original score or
something like that ok so you can just
read written there or the original score
we also have the opportunity to filter
outside
some items so let's say we will filter
out all items and that are odd right
yeah even sorry it depends on on the
similar on the similar geometric so you
can do operate like double the score or
square the score or something like that
but you shouldn't be really tied to any
specific numeric value so it can be it
can be like one can be a lot but it can
also be very long it again it depends on
what kind of metrics they use and how is
sometimes also how much data you have so
it's not normalized annually and Andy
and we can simply pass the idea of
scorer to our recommendation and now
when we run it we should only see movies
that have odd IDs which is maybe a
strange requirement but we can do that
so why not right so we can see that all
of these are odd numbers right and okay
so so so so that's it for the demo now a
bit more about what happens when we
actually have more data so what I've
shown here it works on a single node and
as I said like the practical limits are
probably like 10 million data points
again you should test it now it depends
on how many items you have as well and
if you have if you have a very small
item space maybe you can use more data
points yeah but it's like the riot
roughly the size so what happens if we
have like 100 million data points right
and well then you have a couple of
choices one of them is to use the multi
node mahute implementation which is
based on Hadoop that implementation it's
like it's a hard job right so you
provide some data to it and it pre
compute recommendations for your for
your users and at aura commanders
available get an item item based
recommender which implements a different
algorithm to the one we have seen so far
so what does it computes at the
co-occurrence map matrix so it
in the matrix is like items buy items
and we compute how often the items go
together right amazing on that you know
we see that the user has like a given
item we can see what what are the items
that most often go with each item right
it's quite a natural idea to the
recommendations
it also has support for a matrix
decomposition which I will talk about in
a second well what what is it so yeah so
I won't do a live demo because it was
taking a while probably have to actually
compute something and it will be only
like bash screens we all know that bash
is unsafe so what I'm gonna do it and
yeah so like running the recommended
drop it's very uneventful you just pass
in the data and after some time after
some time in HDFS you have the results
and the result is again a comma
separated values fire okay and yeah you
can again support some similarity class
and so on you can also try this Li run a
demo to implementation on Amazon so on
EMR which is like the Amazon Hadoop and
as a service and so it just works out of
the box without any problems and also
what what I have done is if you have a
reasonable number of items so in our
case it was its like tens of thousands
of items not maybe not hundreds but tens
you can also compute the co-occurrences
for a lot of data points so currently we
have about 600 million data points on a
single node right you can implement the
same algorithm as on Hadoop but on a
single node so doing most animal
computations and using a displaced cache
so we use map DB and so here like we are
not using my hood we are using ideas
from a hotel going from a hood which I
like probably generally known algorithm
what's but we are we don't have all the
additional complexity of hadoop added
and so sometimes you know it's a it's
worth actually looking into a custom
implementation especially after
experimenting with a
with a single note setup also that's the
question it when you add more data
points and when you calculate
recommendations based on more and more
data points at some point adding more
data points what won't add any value
right there's like there is some barrier
at which you have enough data to get all
the structure that is right so it it's
it if you have a recommender which does
recommendations basing on one hundred
million data points if you have two
hundred it doesn't mean the
recommendations would be any better they
can be like exactly the same right
because all the information about the
relations between the items is or is
already included in the 100 million data
points so sometimes you have large set
of data a good idea may be to sample the
data and just take a subset of the data
points and the recommendations basing on
that yes okay so the question is what
takes time is it running the recommender
or building the model so like in Hadoop
case there's only the building the model
step right because you the result is
actually all the recommendations so
looking at the actual recommendations is
like looking up in a hash table right in
in the single node the majority of the
time is spent actually doing the
recommendations because it looks like on
it looks at each user or each item
depending which recommender we use okay
so master exact composition is also a
gaining popularity as a weight of the
recommendations it's a bit more
mathematical and so we can look at the
existing data that we have as a matrix
where the rows are the users and the
columns are the items right so we hit
that a that's our user item matrix and
there are mathematical methods to factor
that into a to other like two or three
matrices which when multiplied give back
the original matrix and
so that like two basic methods one is
called alternating these squares where
we compute two matrix to me to two
matrices X &amp;amp; Y here and SVD singular
value decomposition where we compute
three matrices and here's like the nice
picture so the dark blue are the ratings
right the users buy items and we try to
decompose it into three matrices and
into two matrices depending on the exact
algorithm so when we have to like we try
to decompose it into smaller matrices
where we have like the users and the
latent factors so for each user we try
to find a value of a latent factor so
the latent factor is like some decisive
feature of of the items right that we
want to somehow extract from from our
rating matrix right so these are like we
try to guess a de factos that really
influence the choices users make right
so for example such latent factor maybe
if a movie is an action movie or not
right so it may turn out that that a
latent factor like that that's like the
decisive factor which influences the
decisions of the of the users most so we
try to create X smaller matrices for
each user we try to come to compute the
latent factors and for each item and
yeah then we can do like we try to do it
that way that the original ratings are
there and the rest is like the the
predictions let's let algorithm it's
also available in my hood
finally there's Apache spark which is
like very popular right now I think it
is gaining popularity fast Apache spark
is kind of an in-memory Hadoop so it
provides streaming computation mostly
memory it can be data from HDFS there's
a library built on top of a spark which
is called MLA lib and it contains some
matrix factorization code which we can
use though recommendations it
contains the implementation for the
alternating squares algorithm but it's
like an active development so it's
probably will get better and again we
can use it when we have way more data
then can fit into memory so we can't use
the single node recommenders which I of
course much easier to use then you then
you know a whole apache spark
installation and cluster okay so for the
end a couple of links the sources for
the what I have shown are on github
yahood is an open source project an
Apache project so it's very easy to use
the same for spark okay and yeah I have
some as a bonus I have some stickers so
there are two kinds of stickers a one is
an I'm proud of my called sticker right
so everybody's part of that code I guess
yeah so they are here you can see the
frito to take some and it's actually the
motto for our code review tool and which
we use at a company we have developed it
for ourselves but we try to see if it's
going to be useful for anybody else so I
also have some voters for a half year
license here for that they also over
here and if you are into Scala or if you
want to be up to date with Scala news we
have a scholar newsletter which goes out
every week so there are some stickers of
that as well and maybe you have some
questions relating to recommenders yes
okay yes
no not really but I sometimes they are
hard to understand like they may you
know you don't get some numbers so like
you can try to guess that you know that
this factor is only high when the movie
is an action movie so it probably means
the factor is an like action movie
factor right but it's hard for me to you
know that's not I at least I don't know
about any automatic methods to find it
it's like with narrow networks like you
know when you have the middle layers and
the middle never run so they sometimes
you can see that like when you do like
image recognition you can see that like
it tries to recognize the shapes or
something like that so it's a bit
something that yes
and for color yeah so like the mark this
single note and so yes the question is
about what what if there are no ratings
so yeah so in the example hide ratings
about the mod a single note that works
in in the case when you don't have
ratings almost exactly the same so
that's like you can't use all metrics
all yeah all similarity metrics when you
don't have ratings but in general uses
the same way the co-occurrence
accounting algorithm essentially
influenced by matrix the alternating
least squares it also will not
influenced by matrix if you have you
know Roy data is simply 0 1 matrix yes
so sorry and moving as data if you
Google movie lines I'm sure it will pop
up I oh do you mean I have many of them
no no no it's like these are just
supporting files so these are like split
into smaller data sets for testing but
is this like one main data set which is
the UA but I don't even know what the
rest of the files are so did they look
the same maybe they assaulted in a
different way or something like that
that's like just one data set yes
multiple dimensions I'm not sure I
understand what you mean why would you
want to do that also you want to combine
a couple of metrics okay so the question
is can you combine a metrics yeah sure
you can like the easiest way would be to
write your own metric and just call the
other metrics so yeah it's like it's a
class name that you pass in so it's as
long as it's on the class path sure yes
and no so it's like by convention it's a
user item so I mean this data right by
convention is user item rating optional
time time optional but I'm not sure if I
always transform into that form and just
give it to my hood maybe there is a
configuration file somewhere but I don't
think so yeah that's like you can only
specify the delimiter so it has to be
that way or you can construct the data
model yourself by reading from a
database or it's a very good by the way
we do is we I just dump the data from
the database if I in like in an offline
process
okay thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>