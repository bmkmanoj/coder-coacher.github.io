<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Extreme Continuous Delivery at Unruly | Coder Coacher - Coaching Coders</title><meta content="Extreme Continuous Delivery at Unruly - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Extreme Continuous Delivery at Unruly</b></h2><h5 class="post__date">2015-06-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/FXszf296eS8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you for coming we're going to be
talking about our summer extreme
approach to continuous delivery at
unruly and like other extreme
programming practices we've kind of
turned this one up to 11 as I hope you
found it interesting and I'm Alex I'm
probably fine on Twitter that's a zero
no no and yeah I've been a developer at
unruly for just over two years now two
years to the day in fact is it really
i'm benjy and benji webbe on twitter i
clean a ton relief for about five years
now unruly is a marketing technology
company we're working on advertising
tech like a distribution technology and
tools that help brands compare how
popular their content is online as their
competitors how much people are talking
about their stuff and we simply rapid
growth for the company well I started
over 10 of us and then we grew up to 120
within two years we're now over 160
across 13 offices now I think so we're
growing pretty quickly we've had some
challenges with continuous delivery over
that time some of which we'll talk about
bit later this is going to be the way
our talk is structured so first we'll
talk about the way in which we work so
these are principles and practices that
we do every single day then we'll cover
why we think that works so that's more
the people and the culture because you
can't just drop process in and expect it
to work you need people to buy into it
then we'll have a short five-minute
break for questions and then if we have
time we'll go on to some of the actual
concrete examples of problems we face
particularly on the infrastructure side
and with the development of code as well
so some people think maybe continuous
delivery at a small company or startup
is easy because after all how much can
actually go wrong keeping up at any
users but it still requires a careful
and considered a page and now we're a
bit they go as well and
our real things that we can break we
have adverts on publisher pages with
millions of impressions or tens of
millions of impressions every day if we
break their pages then people are going
to get very upset we have systems that
processing now significant amounts of
money so we do have to be careful so
well what we're going to be talking
about may sound a little bit extreme we
really think it's the best thing not
just for delivering value but for
keeping things working so how do we work
how many people are familiar with
extreme programming almost everyone
that's great extreme programming gives
us set of practices like test-driven
development like pair programming how we
pay learn all about production code like
continuous integration and we're going
to be talking about how some of these
practices fit in with the way we work
but probably the single most important
aspect of the extreme programming is
this idea with feedback loops nearly
everything we do is about getting
feedback rapidly so test-driven
development is the shortest possible
feedback loop we can have for finding
out whether our code actually does what
we think it does having a customer in
the team here we can ask questions this
a short feedback group for finding out
for building a light thing pair
programming is the shortest possible
feedback loop for code review and
hopefully you'll see that the way we
approach continuous deployment is also
about a short paid back loop so the
first edition of the XP book by Kent
Beck says this quote plan releases once
a quarter penetrations more frequently
this was more along the lines of
introducing XP to a company that doesn't
do it and we actually don't do either of
these things which is fine XP says one
of its core values is that you can
change it or modifying it when it
doesn't sue you or when it doesn't work
and it's not religion so what we
actually do is we we have planning
cycles of around a week around two to
three weeks depending on the team but
these are just prioritization of what
work the rest
business thinks is valuable for us to
work on in the next two or three weeks
these aren't hard deadlines was to
release things we actually aim to be
releasing five six times a day on a good
day into production and delivering value
so our goal is to deliver value as
quickly as possible and to minimize the
time from having an idea so getting the
value from that in production how many
people have read the continuous delivery
book by Jess humbling David Farley not
too many okay and well this this is kind
of introduced at least the name
continuous delivery lots of people with
doing it before then but in this book
there's this idea of a value stream
which is what I was just talking about
what all the steps it takes from having
an idea to release thing that and
getting the value from it in production
and but in many organizations this can
take a long time even in this example
which was kind of the example of doing
it well I think we've got several weeks
for the whole cycle to happen we like to
make that faster and so in some
circumstances we can do this really
quickly so for example so on one
occasion we had a request come in from a
sales person saying they had an ad
campaign which was a big ad campaign
worth a lot of money and it was
contingent on a feature we didn't yet
have a developer jumped on that
conversation soon as they got in the
office and they found out of it all
about what was needed what was the cost
going to be and then they went and spoke
to a stakeholder where do we need to
move but then i went i spoke to a
stakeholder and found out about whether
it was actually high priority than other
things that we were working on based on
the cost and based on the valley which
the stakeholder could evaluate in this
case it was more important than the
other stuff we were doing so we actually
started working on it first thing in the
morning and all the team were in during
the morning we deployed
an increment of that production we got
some feedback from our internal customer
in this case the sales person in the
picture was full and we made small
changes as a result of that feedback we
deployed again towards lunchtime and at
that point it was fulfilled the
requirements we won the ad campaign we
made some money now we don't always work
in this way in fact we don't often work
in this way because it's quite costly to
keep context switching what you're
working on but our normal practices
enable us to work this quickly if we
need to more normal process as Alex
alluded to just now we have kind of two
or three week planning cycles and at the
beginning of each one will get senior
stakeholders from the business together
and they will prioritize our work will
give us will will give them a set of
candidate stories that we could be
working on and I'll put them in a
priority order for us to work through
then throughout the rest of the time
will be coming up with new things that
we could be doing for the next planning
cycle so I thinks that people are asking
us to build will be discussing the cost
of them discussing how valuable they are
so that we're ready for the next
planning cycle and then on the
implementation side on a given day will
be picking up the next highest priority
story of the queue and beginning work on
it and then throughout the day and when
we're working well each pair of
developers will be deploying maybe five
or six times a day to production working
in really small chunks of work it's
probably worth mentioning our definition
of done at this point to some teams work
is done when tests are passing or where
they've integrated it with the code that
the rest of the team is using we take it
a couple of steps further than that it's
done in continuous delivery it's done
when it's deploy a bull to production we
say it's done when it's deployed to
production but even more than that we
only say it's done when it's delivering
value in production so we'll be
deploying increments of the work to
production all the time we won't have a
story signed off as complete until
we've measured it delivering the value
that it was space T we can do this quite
often because well if it's designed to
increase some metric for advertising
then we can measure that is it actually
doing that if it's designed to save the
money we can see whether it's saving us
money if it's designed to witness the ad
campaign have we actually got the ad
campaign is anyone using the features
were building this stuff is actually
quite easy to measure is it delivering
the value that was intended the reason
that the work was prioritized in the
first place and because we're delivering
continuously the value is there to use
be used by the business as soon as we
finished it so talked a bit about how we
we plan the work which is the first half
of the value stream the second half is
what's often called deployment pipelines
this is another excerpt from the
continuous delivery book by david fatty
and Jose humble and then the deploy
pipeline typically you have you'll
commit some code some automated tests
might get one unit test then later
integration tests acceptance tests and
then you'll have a choice of deploying
it to different environments may be
progressing it to uat and later to
production aburrido Reticuli kiss
metaphor it was interesting to hear when
we were am at pipeline conference
earlier in the year David Farley was
saying that when they came up with the
metaphor of a pipeline they were
actually thinking about kind of an oil
pipeline which is what many people seem
to assume they were kind of thinking of
a cpu pipeline but the idea of an oil
pipeline which is kind of the initial
impression a lot of people get we don't
like because it implies that it takes
some time for stuff to get from the
input to the output maybe things get
stuck in the middle so we're not too
keen on that we kind of prefer the
metaphor the gate who are sluice gate so
we have our development environment on
one side well our tests are not passing
because we haven't finished stuff then
the gates locked we can't open it when
they are passing there we can unlock the
gates unlocked we can open it and let
our changes through to production as
quickly as possible we want to make that
pipeline as short as possible
so that we can release stuff really
quickly which is the other half of
getting the value stream really quick
short so as part of doing this one of
the first things we found that we had to
tackle was the problem of automation
this isn't a new idea nearly everyone
does it so if it requires human effort
and you find you're doing it repeatedly
more than a couple of times then either
script here or find an application that
does it humans are far better things
that are repeated then sort of computers
afar but repeated tasks than humans are
so these aren't just things like our
integration of acceptance tests these
are actual like performance tests
running in production as well so keeping
an eye on everything is happening and
delivering us back information this is
the information that we look at when we
determine whether something's delivering
value in production or not yeah so we
run into a one problem where our test
started taking longer and longer
particularly our ad units which have
selenium webdriver test which open up a
browser click around and those were
taking a really long time so we sat down
and we thought do we actually need all
of these tests to pass before we deploy
and we actually decided no they're not
critical we'd like to know about them
within a certain amount of time to with
it in an hour or so but they're not they
shouldn't be a blocker to us being able
to deploy so what we've actually done
for those tests is we by and large made
nagios check set out them we call this
nag DD nag for Ned Yost but also you get
text magic nagging you to fix problem
and what this meant for us was that
these long tests that were blocking our
build and making it harder for us to
deploy increasing time between deploys
and this actually cut it down and
recently we've actually started
packaging all of our integration tests
and running them constantly against
production anyway to deliver some more
of that feedback value which we so love
many of you probably familiar with the
process of Testament development and
where you write a failing unit test for
some new functionality then you write a
smallest amount of code necessary to
make that test pass and then you do some
refactoring and see your tests go green
again and at that point you should have
a choice you can go back and do some
more refactoring which you should and go
around that metal loop a little bit or
you can write another fading test and
add some more functionality but there's
another choice in our post this and
we're doing this often enough that it we
appear it does fit in this loop and that
is to deploy to production so when I
test to green we can deploy and then
you'll notice after when we deploy we
don't while we're waiting for the deploy
go and write another fading test instead
we wait for the deploy to finish we get
some feedback from the deploy which then
informs the next thing that we do and
that's really important so that feedback
might be going to speak to the person
we're building the feature for finding
out if it's progressing in the direction
they want it might be simply saying we
haven't broken something in production
if I be sitting we haven't had a
performance regression in production and
but regardless we deploy our changes we
look at the result and then we use that
to inform the next thing that we do so
this the fact that this is synchronous
is kind of counterintuitive for many
developers developers like to make
things asynchronous because then they
can do more things but there doesn't
necessarily mean you're delivering more
value and the fact that is synchronous
we wait for it to finish so that we can
get feedback it also gives a strong
motivation to keep everything really
fast because otherwise we're sitting
around twiddling our thumbs and nobody
likes that the extreme programming book
has the idea of a coffee break build
that your build shouldn't take longer
than it takes to get a cup of coffee and
for us that means that deploying to
production shouldn't
longer than it takes to get a cup of
coffee otherwise it gets very
frustrating and the synchronicity gives
us a strong incentive to keep it that
way seven people would say they do
continuous integration nearly everyone
that's great maybe use some of these CI
tools as you might find it surprising
then that we do continuous integration
without the CI server continuous
integration is a practice it's not a
tool and there's there's often a lot of
confusion about nest people hear about
CI tools that using a CI tool therefore
they're doing continuous integration now
we do use a CI server for some things we
have steam steady it is useful in some
circumstances but it's not a core part
of our process for all of the things
that we're building because we're doing
things synchronously we don't need a CI
server to make this stuff asynchronous
we can run our build deploy tests from
any workstation from any machine it
doesn't mean we don't spin up other
environments in vagrant and ec2 as part
of the deploy process but we're
orchestrating it from our workstations
and because we're waiting for it to
complete anyway which has the added
advantage we can deploy from anywhere
even our laptops there's no chance of
the build server becoming a special
snowflake which is the only place you
can do your build we're forced to make
it work everywhere we also don't do any
branching a lot of teams will be feature
branching where they start working to
feature they'll create a branch that
lives for the lifetime of that feature
now on there's your back we don't see
this as continuous integration because
branches are by definition keeping your
code isolated from the rest of the code
that's kind of their purpose there's
kind of an implicit lunch that exists on
everyone's workstation when they're
working on a feature but this won't be
longer lived in like 45 minutes an hour
after which we'll be deploying that co2
production measuring it into master as
part of that process and yeah we see
this as real CI because we're not just
integrating that code with the rest of
the team
we're also integrating that code into
production with our users and data in
production so your love of the way
people use these tools is more
continuous isolation than continuous
integration because they've got their
feature branches they're running their
tests on their feature branches
repeatedly so the list are checking the
there things aren't broken on their
feature branches but they're really not
integrating the code with the rest of
the team or less to play into production
so since we don't have a CI server we're
faced with the problem of who decide to
get what gets to go into production we
actually only have two environments we
have development which is our
workstations and production which is
open to everyone you can hit our servers
and that's open to everyone on the
internet so in a lot of places it's the
business the guests to decide to deploy
whether it's the person who filed the
feature request whether it's someone
else in the building who gets the final
say on whether this change is allowed to
go out we've actually unruly we've taken
that responsibility back from the rest
of the business and it's the developers
who are working on the code who decide
to get when it gets to go into
production and this might not work in a
lot of places for us and the way we work
it does because we're focusing on
delivering small changes small change
sets into production as quickly and
frequently as possible which means
there's less fear that we might break
something horribly and deployments are
no longer an event they are a complete
non event there let's deploy okay so how
we do this is we have feature toggles
and this has a bunch of other names this
might be but branch by abstraction I've
heard some other names for it but and
these are used to isolate features from
people who don't need to see it so for
example if we were developing a feature
the for the sales team no one else needs
to see that features will isolate that
feature toggles can be quite
sophisticated they might be an if
statement in
a template somewhere checking that a
user has a role this moves us nicely
onto this idea of user acceptance
testing in production and as I said we
only have two environments dev and prod
we have a global organization which all
of our developers are co-located in
London but we have satellite offices as
Bengie mentioned all over the world and
in particular for the places like
Singapore and here in San Francisco the
time difference can be a bit of a pain
to get feedback so these short feedback
loops for value that we like are
lengthened by the time difference and
there's not really anything we can do
about that so the way that we can we
tackle this is we are deploying these
changes that we want feedback on into
the actual production so our users don't
have to do anything special then I have
to VPN in anywhere they can just look at
the application as they normally would
and this is really great for us we love
this we have this mantra cause there's
nothing more production like them
production itself which kind of
encapsulates the way we think if we want
feedback on a feature and then we deploy
to production and find out where this
really shines is performance feedback so
for our stats processing platform we
frequently we used too frequently run
into problems where I miss a
misconfigured query or a badly written
bit of code and degrades the performance
and because we deployed that bit of code
quickly on its own and we saw the
performance impact immediately we were
able to quickly reverse it I say reverse
Benji will touch on this later and as
such we are a fan of Canary deployments
deploying a feature if it doesn't work
roll it back that canary is dead and we
have a lot of dead Canaries but we have
a lot of good code in our code base
because of that but for this to work
it's essential to have an accessible
customer so this touches I'm always will
be earlier having a customer embedded in
the team whether it's a product manager
whose representative for the rest of the
people who are using that app or for us
because most of the business is
co-located with us it becomes a matter
just going upstairs and tapping them on
the shoulder which is again really great
for us it tightens that feedback loop
things not just continuous delivery of
continuous deployment really everything
we're continuously deploying changes all
the way to production which means that
becomes such a non-event first or even
happy to deploy after we started
drinking beers on a Friday it's that
much that little risk the change sets
are so small that say where we too vague
something it's very unlikely cuz we know
what we're doing with when it's just a
small set of changes we know would be a
just going to have but if we do play
something we know exactly what schools
do it so it's really easy to fix it and
redeploy which so people often ask
what's your lower back strategy if this
goes wrong and the basic answer is we
prefer to call forward rather than
running back so if we we can deploy
really quickly to production we know
exactly what will have broken something
Sweeney just deployed it and it's a less
than an hour's worth work so we just fix
it and we deploy at in the very unusual
scenario that we don't know what's
caused it it's a matter of averting the
last commit pushing naps to get hub and
playing that and then we still know that
the latest code is the code this in
production which is a really useful
simplifying assumption first so
so and those are the sort of techniques
in our toolbox but as I mentioned
earlier it's all about getting buy-in
and having a culture that supports this
way of working so the thing we talked
about most throughout this talk is
getting value and getting information
quickly and getting tight feedback loops
through eliminating asynchronous enos a
synchronicity is increasing where
possible and the thing that ran in that
we ran into most first of all was silent
and here is a so you've grown up as a
business you've scaled up and the first
thing you might think to do it right we
have these ops concerns as creating
opportunities to create dev team we have
loads of testers let's let's have to
test two teams and customer here so this
keeps the developer team quite isolated
from everyone else and the real killer
is that if you're handing off work to
other teams this is a problem so this is
the pushing it off it's not my problem
anymore and so we thought what would be
the best way for us to fix this and we
initially started by moving on to
splitting these teams by product not
projects we were quite firm believers in
the no project mentality we just have
product boundaries so here we might
split into parts AB&amp;amp;C so we have maybe a
BA a test a product manager a developer
of your lucky but we found again this
wasn't works you've just shrunk the
scale you're still handing off work and
the solution we came to this was quite
nice is that oh we value general ism
over specialism so you'll notice in
these pictures that there are a lot of
developers and then there are a couple
of people who aren't developers what we
do unruly is we have a couple of
specialists so for example a specialist
for a user experience in you I work
reliability and scalability and an agile
coach with product managers and and
product managers which will get on to
later
but it's it's not their job to do a peep
so if a story has a piece of you I work
it's not passed off onto the UX
specialist it's there they almost work
in a sort of consult consultation sort
of Rob so they will at the start of the
story they'll come in and they'll
discuss the story with the developers in
most of the cases they're actually pair
with the developers on this work so it's
not a question of passing off work then
it's that the specialists are helping
the generalists do what they do better
rather than handing off work to someone
else and this is us embracing one of the
core principles of XP which is
collective ownership so everyone chips
into everything then so we did this with
our development team we thought great
we've solved this problem but we hadn't
we had a team of product managers which
almost acted as the buffer between
development and the rest of the business
so business would have a requirement
talk to the product team policy and
create stories stories go into dev
features come out for the rest of the
business as far as they were concerned
it's stories come in features come out
which we found was quite painful and we
still had these bottlenecks between the
business and the product team products
and development so we folded those into
this this group that we were creating
that we now call product development and
the product development team it's not
just developers it's taking the idea all
the way from its inception to delivering
value in production and we do this with
product strands so product managers keep
a lid on everything with regards to
interfacing with the rest of the
business but say a developer will have a
particular product strand which is a
vertical slice through the business of a
particular feature so whether it's User
targeting for our ad units or a new ad
unit that we're developing one developer
will own all of that slice of
functionality not do it all themselves
they will pass allow stories between the
Reston's developers so it's all
distributed evenly but this way
it sort of makes sure that the
developers have an idea of what's going
on from the inception of the story to
delivering value a collective ownership
it's a bit more extreme than many places
collective ownership suffering around
code everyone in the team is owns all of
the codes that anyone's free to work and
leave any part of the code base but we
take that further we have the team of
journalists collectively owning
gathering requirements or new valuable
features that we could be building in
the future we collectively own
operations as well we're responsible for
making sure that the things we build
keep running in production and even use
the support to some extent Aloha we do
have people who work within the team who
specialize in that another has another
specialty an aspect of this is that
developers on call when things go wrong
because we have this responsibility that
will get woken up if we deploy something
that will break that's overly
complicated it means we can have the
freedom to deploy as frequently as Lee
as we like to use the technologies we
think and most appropriate and I think
really think you start running into
problems if you separate the
responsibility of looking after things
from the decisions of what should we be
building because you need to have those
two things together we'll have questions
in a moment if that's okay and I
mentioned before we do pair programming
and that is as short as possible
feedback loop for code of you and that
means we remove code review as a
potential blocker and for our deployment
pipeline because all code is loop is
reviewed at a pointer which is written
we don't need to have the code being
blocked on being deployed until it's
being reviewed by someone else and we
also have a strong culture of
self-improvement so we're continually
trying to get better at doing this so we
have team retrospectives where we're
looking at how we're doing compared to
our values but also we have a concurrent
stream of development tasks that are
working on
often to do things like speeding up the
deployments as it gets slower because
well we're constantly adding tests we
need to constantly be making it first as
well and then we give all developers
twenty percent time to do whatever they
wanted and they do lots of interesting
stuff in that we don't impose any rules
and what they can work on but it does
often lead to people scratching their
own itches and working on things that
make for example our continuous
deployment better so we have had testing
tools come out of that we've had things
that led us track dependencies for what
needs to be deployed to production as a
result of a set of changes as we found
that really helpful so we thought it
pause for questions of what we done so
far we do have some more material on
challenges as we've grown that any
questions on safer yeah question my dear
uncle so you have 30 developers now
presumably they had different areas of
specialty where feature comes out and
something goes wrong maybe it's not
noticed by someone who's very tech savvy
how do you wrap that call for my
developers or begin engineer you has to
a get called first developer freely
there could be a wrong route so it's so
basically up to each team to decide how
they wanna call the waiter works they
know what they need to provide the rest
of the business as a service and then
it's up to them what things are worth
waking him up what things and who's
oncol most of the teams I think are now
doing kind of a a few day rotation on
call and they really do rotate it round
all the general lists in the team and we
we do hire completely full stack
generalists they were happy to work on
any part of the system and know all the
system because we're doing hair
programming because we're rotating round
least once a day who's paying with whom
everyone gets to know the whole system
anyone can be feel comfortable to fix
any part of the system as well and we
talk a bit more about splitting the
team's later but the teams each team is
quite small still and so there any
responsible for a limited set of things
and then they'll only be on call for the
things that they own and then of the
other teams on call for the stuff that
they own Summer meets a decision which
teams around as the team set up their
own monitoring they set up their own
checks for the things they were deployed
it was very much thing a blurring
between tests for things before they go
into production and tests in production
which is our monitoring system and so
they'll set up these checks in
production for things that they want to
know if it got long and if they go wrong
with a little let them know and then
depending on how severe is it might page
someone out about sounds the question
yeah now the question when do we remove
the feature toggle do on it and so when
the so say one of them was one of these
if statements we are talking about that
only exposes it to a certain subset of
our users when the customer who's
requested that is happy with the feature
will add another small commit on which
is removing that feature toggle and then
we push that out to production yeah
pretty much Maria unless it's something
something might so the example the
springs to mind is a new checkbox on a
particular bit of the application
platform and once we're confident that
it's working and that the customer
thinks is working in the way that it
should be we don't we still run all the
tests as normal but we don't have a
specific tests for the removal of that
feature double by and large now the
question there yeah weaver hat
a short question first I'm just going to
ask so we've heard a lot about put those
work
have crosses problems so
so this is an yeah well we'll be talking
about some of the challenges as we've
grown in the next section yeah and
second half is basically all the things
that went wrong along the way a question
down
initially so yeah if every commit this
production do developers I hear about
committing I guess it's not necessarily
every commit that goes with everything
that's pushed goes to production and
we're only talking about a few minutes
or an hour of work being pushed anyone
and when people join the team it takes
them a while to use lists often coming
from places where it takes like six
months again released and it's a big
shock but they're pairing with someone
who's used to it so people I think
everyone is when the team is deployed
something on their first day so you get
used to it very quickly another question
I say you want to change your
which one month of that again the next
section is actually on infrastructure
they DJ to bathe changes so kind of
databases are particularly interesting
is off so we use DB deploy barrel
they're also other tools I think that's
one called fly away which I first heard
about at this conference but they meant
to largely the same thing so the idea is
if you're on your laptop or workstation
you're bootstrapping the application
from the start you start with an empty
database you run DB deploy with its
hundreds of the skin and migrations
which just bring the database up to date
and then gradually ingest data so it's
in a suitable rep curve and production
for testing purposes generally with
databases we they're being synced back
to our development environment regularly
and the next laterally when you start
the thing up here automatically runs the
skimmer migrations since the last
natural of you the databases that are
small enough and we try and do nightly
restores of from production on to the
workstations but for the ones that
aren't big enough we find that a scheme
were done and a bit of recent data is
enough to enough to confirm that
everything is working the way it should
do and that the tests of passing and in
the way they should be but
infrastructure is actually the next
thing we'll talk about should we move on
to that now and we can have more
questions video sounds like a good idea
to me so infrastructure challenges since
we operate full stack this means that
the developers also owned all of the
operational requirements sometimes we're
asked are you a dev do you operate
devops in a DevOps way are you dip ups
we normally say no we're much more
closely aligned to this idea of no ops
in the week we don't have an ops team
the ops team are the developers and vice
versa so we started off like most small
companies with a beloved little box in
the office pankor figured maybe some
shell scripts are best but as with all
companies who grow up discover we have
to
go into the cloud eventually and so
we've scaled up from what were one or
two machines hidden in a closet
somewhere to several orders of magnitude
more now in the cloud we use AWS but
obviously other providers are available
so a phrase that gets bandied around a
lot is infrastructure as code so this is
this idea that through your
configuration management tool of choice
we use puppet but there are tools that
are less slow available to you as well
we have them so we try and apply our XP
principles as we would with production
code to our instruction code as well so
some of these are really easy so pairing
is easy we just work in the same way
that we do collaborative ownership was a
bit harder as was TDD and these were
both things that we ran into problems
with but again it's about creating that
feedback loop and so deploying these
infrastructure changes into production
and observing the changes so we had this
idea of snowflake servers snowflakes
every snowflake is unique and we were
quite guilty of having unique servers
that we weren't actually sure were
rebuildable and when we start scaling up
rebuild ability is an absolute must just
from the sheer fact that the more
machines you have running the much
greater chance that one of them just
dies we use AWS as sometimes they just
turn them off without them us asking
them to which is nice of them it tests
our set up anyway but what this means is
that we've had to focus on making these
machines rebuildable so we actually take
this idea and we call it continuous
disposal so where possible even if we're
not doing anything we're tearing down
our machines when we building them so
this again this is not a new idea
several big companies Netflix is one
that springs to mind is when you operate
at that kind of scale you can't not have
this sort of thing in just for this for
testing your fault tolerance and
resiliency
we were caught out several times by
assuming that we've got this machine
we've gots configuration under puppet it
must be buildable right run and that was
entirely wrong so this the
aforementioned machine it was a database
server died and everyone run around
basically with their everything's on
fire because they couldn't rebuild it
and we were we've never been bitten by
that sense because no one wants to go
through that pain again and this is I
heard this sort of comes from agile if
it's painful do it more often and you'll
make it less painful for yourself so the
way we actually start deploying our
projects in production now is so say we
start with we have this app that we've
been told to develop and it needs a set
of requirements so what we do is we spin
up a machine that has the requirements
so we've largely lose job use Jarvis
this might be Java tom cat or something
and then we deploy what we call a
walking skeleton so if this was a web
app this would just be an endpoint that
say returns hello world but the
important thing is you deploy that
production straightaway and and what
this means is that you can iterate and
deploy frequently because something we
ran into was that we would start a
project right a load of code the code
would be working and then we try and
deploy it and for some reason wasn't
deployable or there was something wrong
with the machines we haven't thought
about it from an ops point of view the
thing that springs to mind is when we're
trying to build one of our our bidders
in our bitter exchange architecture not
turning up the you limits and this has
bitness several times but this comes
back to this idea of having this
feedback so deploying something and
rapidly iterating over it to deliver
value each debt because anything is
better than hello world so at EDD we
touched on and our opinions like so unit
testing for us is not useful so we there
are a lot of frameworks to test your
chef will test your
but for us we run an almost completely
homogeneous stack we've run on centos 6
and our workstations wrong fedora and
almost all of our apps are written in
Java that while this simplifies it for
us it means that there's very little
branching or logic in our configuration
code which almost entirely removes the
value of unit testing what we're much
more interested in is accepted assisting
so building the machines and checking
that they're actually doing what they're
supposed to and we divided these into
two states internal and external so an
external acceptance desk and this is
implemented at asian specific this is a
framework called bats or bash x the
batch acceptance test suite or something
but it runs a set of shell commands and
just a search on non zero exit status so
here we're just testing that our
analytics product redirects to SSL
entirely simple massive value that we
could though that just delivers even
something as simple as this so where we
were normally trying to provision these
things almost in the dark adding these
tests in really helped us out with
reasoning about what's in production and
then we also have this module testing
this was written by one of our
developers in their twenty percent time
which is a java port of the service pack
framework this is more reason about
what's going on inside the machine so
which service is running which ports are
running things that wouldn't be visible
from the outside user but can obviously
have a massive impact on the state of
your application so the biggest problem
we ran into was this notion of shared
infrastructure so we started off with
one team and that team had had some
puppet masters had some Ned Yost and
graphite servers but then when a team
grew and the team split and eventually
one of those teams grew and split again
we've now got three teams who are
largely responsible for their own
disparate stuff but we still have the
shed and configuration management the
shared alerting
and what happened was that if there was
ever a problem each team would basically
assume it's not our problem one of the
other teams will fix it and this is an
absolute sure fire way for something to
go horribly wrong and it did twice one
of our alerting servers fell over and
then we suffered a massive outage that
night and no one knew about it until the
next morning so what we have done we
actually created this cross team team or
cross team squad and that we
affectionately called for a squad after
the now sadly defunct DevOps poor at
Twitter account but its purposes we have
a representative from each team and it's
their responsibility to sort of collate
the requirements for their team and
prioritize them in order then for three
days each iteration which translates as
two pair days from each overlapping team
we cross team pair on these issues and
try and resolve them so what we've done
is actually introduced this idea of
cross team pairing to get shared
ownership of this code back and we found
that this has been really useful in
particular when all the teams were off
doing their own thing their architecture
their architectures started to vary
massively and by bringing it all back
together we've started to consolidate
everything so there's this phrase which
is reduced variate tsinghua reduced
variance increase me which is it's
actually a phrase from an actual
manufacturing discipline called six
sigma and the idea being that focused
first of all on making everything nearly
the same and then focus on improving
things all together it's going to be
less effort oh I'm the long run to make
sure everything's the same and then
improve everything then try and improve
local hot spots and the reason this
works is if you have lots and lots of
machines you can't really afford for
them to be disparate or configured
manually or have
some form of them disparity between them
and the fact that the time between
failures becomes less important than how
quickly you can recover and the last
thing that I'm going to talk about is
this idea of Phoenix workstation so
Martin Fowler talks about Phoenix
service and the more commonly known as
immutable infrastructure so this idea
that if you're going to do something
that touches a production machine
instead of modifying it in place you
tear it down and rebuild it we've
actually taken this and we've extended
it to our workstations so because we
pair we don't have personalized
developer workstations we have shared
pairing workstations and we actually do
we call it crumb coat deletion so if
we're working in this continuous
delivery way delivering small small
chunks of value along the line we
generally find that we don't leave
outstanding bits of code overnight what
cron code deletion does is there's a
cron job that runs at midnight and
because their shared workstations they
have all the projects in one directory
and what it does is it trashes this
directory completely and recheck sell
everything from version control so if
you've left some changes overnight that
you haven't pushed or left some open
changes if you come back the next
morning they're gone we found this has
been it feels like a sword over our
heads sometimes but it's it's a good
motivator for delivering small amounts
of change and in particular if we're
rebuilding our workstations quite
frequently and also checking out our
code it means that you can move from
pairing workstation to pairing
workstation continue working seamlessly
but it also means that our development
environments are almost exactly the same
as our production environment so Benji's
going to talk more about less on the
infrastructure side and more on the the
economics of probably scaled up yes so
obviously when i joined though only
about three of us and a development team
and now tech teams around 30 i think so
we've had some challenges as we've grown
we in order to maintain
the working practices that we like so we
found that when we got to about 10
people on a team that some of our
practices started to break down
particularly thing a slight collective
ownership because people in the team
that size people can kind of pick and
choose a bit what they want to work on
avoid working on parts of the system but
they don't like and and various other
practices as well become harder as the
team grows because communication starts
to get hard so we decided we need to
split the team's the first time this was
really easily we had to set for
applications to separate products so we
split the team across along that
existing product boundary and then that
our normal practices where the team owns
the product they are in coming up with
new valuable things to add to that
product they are in keeping a product
running it really separates cleanly
between the two applications they just
look after their own stuff oh it's that
worked really well for the first time I
but then we need to destroy the team's
again because we're going further and
this time we'd one out of products at
the time we now have more and in fact I
think we have each team owning at least
two products now but we saw this as an
opportunity to take advantage of
Conway's law Conway's law is the idea
that organizations tend to produce
systems that reflect the communication
structures within their organization I'm
and we had quite a monolithic system at
the time with one of our applications
and by we want to display the teams we
wanted each team to be able to deploy
their stuff independently without
affecting other teams to be able to
gather requirements independently
without affecting the other teams and so
on so we have to decouple those we have
to introduce API boundaries we had to
I'm separated about and have a
well-defined interface that we could
control the chain job so the behind each
interface the teams can work
independently so we were improving our
architecture at the same time that we
were splitting out the teams I'm an
advantage of doing it this way but if
we'd started off with lots of tiny micro
services
I could play this would have been travel
would have just dividend up between the
different teams but there were some
advantages to doing things this way we
know more about where the conceptual
boundary is by an assistant once is a
bit more mature so c'est advantages of
doing here's both ways that said we have
started to shift towards building things
as smaller services and so each team's
daiwa sponsor for quite a few set for
applications and many of which
independently deployable and that's one
way of tackling the increasing deploy
speed as you add features if you've got
smaller services you can deploy them
more quickly if they independent but
they just have trade-offs and as we
started to move to smaller services
there's more complexity in the
interactions between them there's
behavior emergent in the interactions
between them and it's harder to test
that it's harder to be factored stuff
across the different service boundaries
so it says by no means are silver bullet
but we were tending to trend in that way
another problem or challenge of the
smaller services is a dependent disease
so you change some code might be used by
more than one service at what in how do
you know what needs to be redeployed as
a result so we've actually got a tool
that mentioned earlier someone built in
a twenty percent time that says I have
changed that and it gives me redeploys
all the things that were affected by
that change so we we know we've got the
latest code in production once of a
mistake we run into with this we started
versioning artifacts and to say that we
could have different versions welling in
production and deploy them separately
but we ran into problems with that as
well where we had unexpected issues in
production due to not having tested all
the different versions of our code and
how they interact together x we didn't
really know all the versions that were
running in production and we realized
that having different versions in
production was again a way of isolating
things it wasn't really continuous
integration because we had different
versions that were isolated
so we've moved much more back to
redeploying everything that's affected
by a change still means if we've got
good conceptual boundaries in our
applications then we don't need to be to
play everything I don't make a change
but some things require deploying more
than others another issue about MC we
switch to get maybe 3-4 years ago and we
kind of took the typical get advice that
everything should be an assignment repo
so you have really sweet lots of really
small repositories but the way we work
we're usually working with thin vertical
slices of functionality that we
implement which might be involved
touching user interface API is databases
and we're finding that we were adding
this tiny slice of functionality but it
was requiring touching several different
code bases and then we lost the ability
to track atomically that change across
those we post other ways of working
around that with that sub modules sub
trees and that we've actually just
started shifting back towards bigger get
for posit Ares who don't have all that
much code compared to say Facebook and
we can quite comfortably have all of our
code and a single git repository we
still have quite a few now but we've
started coalescing them back together so
we can track the changes or atomically
another challenge as we've grown is that
it becomes harder to share our practices
show the way we work Alex mentioned our
architectures deviating sorry there's a
few things we've tried to encourage
cross-pollination between the teams we
have a quarterly team rotation where we
send out a survey and we encourage
developers to state preferences for
teams that like to switch to and then
we'll encourage at least one person to
switch between teams every quarter when
it's appropriate for the team if they're
not in the middle of an impending
marketing deadline and we even had for
quite some time a team lead rotation
in one team where the person responsible
for making sure that the team was
working effectively was rotated so
different people had an opportunity to
try that out and they found it quite
helpful to understand both let the
business valley as well as all the rest
of the things that we're doing and we do
other things the lots of companies do
that we have an internal Tech Talks
every fortnight where people share
things that they've done in their twenty
percent time or cool new technologies
they've become available for or will
probably do talks on things we've learnt
at javaone when we get back other
scaling challenges a one-time state in
our applications is something we've
ralindi one of our applications in
particular have maintained stated in
memory cues and when we deployed the
deploy process had to wait for those
cues to join before it could restart the
production application because they was
with non-persistent Keys just brings the
data being stored in memory and that was
fine when there were a few users but as
the number of users increased it started
to take longer and longer for these kids
to drain and it was holding up how to
play posters so we've had to spend
invest quite a bit of time in over
moving that sort of non-persistent state
and in some cases as splitting up the
architecture as well to allow us to
restart individual components without
affecting the whole system we have to
continually invest in improving the
speed of our deployments we're always
adding new tests for every feature we
add and that makes things slower because
every test has some sort of cost to it
unit test is very small but with
acceptance tests user interface tests
with test hit the api's this can be
slower and so we have to every so often
we have to be profiling each test suite
we're finding the hotspots making it
faster in some cases we reach the limits
of what we can do there and we have to
look for boundaries where we can split
things up into separately deployable
artifacts but so that we can keep our
synchronicity our coffee break deploys
we
you have to be spending a fair amount of
time on make things faster reliability
is another issue as well it's kind of a
pain when you've got point one percent
failure rate of your tests and 100 tests
but if you've got a few thousand tests
it becomes pretty much impossible to
deploy because you'll have something
fading on each each test run yeah so we
have to manage those test failures and
approach that we've taken we have a
little library of ju nibbles which will
actually be open sourcing probably
shortly after the conference it's
pending approval at the moment and one
of them is a quarantine rule for a unit
where we can annotate a test is under
quarantine if it's paling
nondeterministic li and what that does
is effectively deletes the test for
purposes of our deploy gate but it still
executes the tests when we deploy and if
it fails it will spam us with a message
telling us the failure information if
first if it fails nondeterministic Lee
if and it will still fail the deploy if
it fells deterministically so you know
for certain you're poking something but
if it fails and then passes it lets the
deploy continue because the test is
pretty much worthless because we don't
we can't trust it it sends us the output
so that we can analyze it and then
either delete the test if it's not
delivering enough value to warrant
fixing it or fix the test and under the
last challenge I wanted to talk about
something that's ongoing this idea of
collective ownership versus freedom we
value collective ownership is the only
way that the way that we work can work
that it does start to break down in some
circumstances we also value giving
developers lots of freedom to choose the
technologies that they want to implement
things in but if everyone chooses their
own favorite tech and whatever the
current hotness is then you end up with
quite a lot of different technologies
and used by the team and then it becomes
too much for everyone in the team to
know
to a high enough degree to be really
comfortable with owning it so this is a
trade off and one of its struggled with
and we've actually started going back
the other way and consolidating things
because we introduced for example
Skarloey and a couple of the teams we've
got several people he really likes Carla
and that the trouble is with several
quite diverse text acts it becomes
difficult to collectively owned a system
so we've started migrating that stuff
back to Java it's Java is now less
painful with Java 8 anyway so the strong
scale advocates are less less in
disagreement about that just wait summer
we do continuous deployment continuous
delivery but we do some things in
particularly odd ways the things that we
most get pulled up about are these idea
of having a synchronous deploy leading
to no no CI server we find it's useful
for if it has libraries that we don't
control we're actually to meet up in
London last week where that price talked
about or how they use it is they have
this idea of having progression tests so
their CI server builds the latest
release candidate of libraries they're
using so not that's being currently used
in production but it builds their
application against the next release
candidate to see if it does break so
they are forewarned of things that could
possibly go wrong and with the next
couple of versions which we found quite
useful and will probably be adopting in
the near future as well that's with
third party dependencies that things
that you might be broken when you pull
them in in the future like XML libraries
are etcetera but and cron code deletion
yet this raises a few eyebrows when we
talk about it and some people don't like
the idea of working and shaping their
code only to come
the next day and find all of their hard
work on I've been bitten by this is
really bloody annoying but it means that
you've been working in a way that's not
really gelling with the way that we work
as like as as a whole you can actually
get it back from backups but it's
sufficiently much of a pain that you're
still annoyed play it's easier to just
work in this way than it is to try and
recover the backups so it's more of an
incentive that way but the key the key
points to if you want to take something
away from this is the idea of how a
keeping our pipeline short through
having through removing a synchronicity
whether that's melding teams together
and reducing handoff between the
products and dev and ops or whether
that's having our builds on our
workstation and deploying from our
workstation removing the asynchronous do
there this way we're getting feedback
fast so we're getting if our deploys a
slow we know about it or something goes
wrong a production we know about it and
it's these things these metrics that are
really key to delivering value for the
business which at the end of the day is
what it's all about if you can tell your
stakeholders that that your feature is
now increasing revenue by X percent that
makes for happy stakeholders with a
pretty much out of time we've actually
so some of these anecdotes and so in
particular this shared infrastructure
but some of the other things like doing
large-scale architectural transitions
we've actually can contributed to this
book called build quality in its a
selection of experience reports from
teams that are doing continuous delivery
and/or DevOps it's available on lean pub
quite a small and that's quite
reasonable fee and the proceeds go to a
good course thank you for listening</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>