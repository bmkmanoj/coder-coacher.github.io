<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Caching In: Understand, Measure, and Use Your CPU Cache More Effectively | Coder Coacher - Coaching Coders</title><meta content="Caching In: Understand, Measure, and Use Your CPU Cache More Effectively - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Caching In: Understand, Measure, and Use Your CPU Cache More Effectively</b></h2><h5 class="post__date">2015-06-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/EAUlxpdj3fY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Richard I'm gonna be talking
about CPU caching and I think to begin
with quite surprising the high levels of
insight and I think that's kind of an
interesting running theme throughout
this talk where there are you know a lot
of things we're gonna be I'm gonna be
covering which you don't directly change
what's happening you kind of just like
I'll let the CPU do its work okay so
just a brief overview of the structure
are we talking about why you might want
to care about CPU caching performance
what's becoming a bigger issue over time
talk a little bit about hardware
fundamentals not very much detail but
hopefully enough that you can understand
what's going on and then we're talking
about how you can measure things which
gives you a way of baselining tuning
optimizing understanding what's going on
and then we go through a few kind of
principles and examples of how you might
use this and how it affects and works in
different blocks of code so first of all
let's have a look at some of the
motivation so to put it very simply you
know everyone's kind of aware of Moore's
law the idea that well transistor ate
some mass of the increasing and also CPU
clock speeds are increasing happening in
an exponential rate everyone's kind of
aware of it but the thing that people
don't really talk about is that over the
last decade the clavicle ock speed per
annum increase of memory is only 9% a
year ok so once you've got this very
very sharp exponential curve on the CPU
side there on the memory side of things
it's much more shallow and that puts you
in a very difficult position with
getting data and out of memory and
getting across it just just getting out
of the data out of memory to here with
so the kind of simple background
hardware solution to this problem is to
put a cache there so we're all familiar
kind of vaguely with the idea of how
caches work you kind of demand some data
if it you check up and have whether it's
in the cache if it's present there's a
hit
so when we talk about and if it's
absence then there's a miss and the data
gets looked up for main memory now the
point about CPU caches is they're a kind
of an economic trade-off okay so fast
memory is very expensive way way too
expensive you to have it throughout your
computer and you're having a little bit
is entirely affordable and then trying
to make use of that is what what you
want to end up doing so over time kind
of Hardware evolved so this is a Sandy
Bridge level diagram it's probably about
right for Ivy Bridge and house well
things are a little bit more complicated
so if you see at the top you've got a
bunch of physical cause each of those
physical cords if you've got
hyper-threading switched on might have
what's known as two Hardware threads
associated with them
servicing each core you've got a level
one cache which is split between
instructions which gets executed and
data related to the program and then
servicing also core specific if your
level two cache and then there's a
shared level three cache overall of the
cause okay so if you if you think about
what's going on there at the higher
levels of caches because they're called
specific you that they're dedicated to
each core and your level three cache may
well have contention between accesses
from different cause now again the
reason you end up with multi-level
caches is that that same F cannot say
Mecca Nam ik argument of having a small
amount being affordable and then
breaking it out and just be applied time
and time again so you know 40 50 years
in the chip design you end up with
multi-level caches so when your CPUs
going along and executing instructions
happily eventually it hits a point where
it can't continue to execute because
it's waiting for data from main memory
or somewhere along so you can actually
access and compute something and that's
known as a stall and the point is that
stalls displace potential computation so
if you want your program to be kind of
running fast and you've got a compute
bound workload you might have
already have an awareness of kind of
operating system-level metrics on
processor utilization like h top top
vmstat just simple things like that and
you'll know that you've got your CPU
running at 100% usage but the point is
that measurements like that aren't
capable of showing you what's actually
happening underneath they just know that
something is executing they don't know
that the the actual pipeline and the cpu
level is stalled and not continuing to
work there's a kind of visibility
impedance mismatch there now at the
hardware level there's a bunch of busy
work strategies which try address CPU
stalls so one of them is out of order
execution which has been on you know
even fairly mainstream processors for
like 10-15 years now and the idea there
is if you've got some instructions that
relate to some data and others that
relate to other data and the data of
some of them is available you can change
the order of the instructions and
execute them so you made so you make the
best use of the available data and the
other thing is simultaneous
multi-threading or the Intel buzzword
which has been annoyingly chopped off
the bottom the screen there it's called
hyper threading and the way those
strategies work is basically you have
two Hardware threads which are
associated with a call and then if one
of them doesn't have the data to
continue executing it that's the other
Hardware thread executes so the latency
in clock cycle figures on there are
specific to this laptop here but you'll
probably have a similar distribution of
clock cycle licensees over you know
other two other processes so the point
is when your data's in register in a
register you can already just access
that and go ahead and use the data so as
you go down the cache hierarchy it gets
increasingly expensive and you'll see
that there's just a huge huge gap
between data being in level 3 and in
main memory
I should caveat that these latency in
clock cycle figures are like the
official Intel this is how fast it runs
figures which means in reality it's
probably that there's there's a lot of
kind of ins and outs as to why it might
be slightly worse than that ok so the
other thing to think about when you're
Mark's CPU caches is that data doesn't
get moved around the cache subsystem in
terms of the data that you want or in
terms of the way things work in your
program you get these things called
cache lines now cache line is the atomic
unit of transferring data between things
in in the cache subsystem so it's like a
fixed sized block of memory and if you
imagine your whole memory is just
blocked up or conceptually imagine it's
all blocked up into sequential cache
lines so a lot of modern processors have
64 bit 64 byte sized cache lines but you
can see on other more esoteric
architectures it kind of flips between
32 and 256 bytes the other thing to bear
in mind is that because cache lines are
purely a hardware consideration your
program can never say hey I want this
cache line hey do to move this cache
line around you need to write your
program in a way that encourages the
hardware to move them around efficiently
so the other thing about this talk I was
going to say so the picture on the left
there's a guy called Ulrich Draper who's
written this very note well-known and
really interesting our tour board what
every programmer needs to know about
memory which in some sense is very
ironically titled document because it's
got a lot of things which nearly no
programmer needs to know about memory
but some people do and I guess the key
point I want to make is you don't need
to know everything perfectly about the
underlying hardware details just a few
general principles as to how things work
and I think you can go quite a long way
with that ok so key architectural
takeaways from this bit of the talk
modern CPUs have large multi-level
caches the cache misses call stalls and
stalls are just leaving your CPU idle
and not really doing anything ok so the
other thing that's important to
recognize is whenever you want a kind of
performance tune or optimize some kind
of program you want to have some way of
measuring and understanding the
underlying behavior so you may well be
familiar with kind of box level tools
like ISOs a VM Statoil
H top execution profiles which tell you
where inside your code base you spend
time actually
ting memory profilers which tell you
what's what way your allocation rates
are garbage collection logs heat dumps
you name it there's loads of tools but
if you're talking about understanding
the interaction of the CPU cache we're
going to be looking at a slightly
different technique which is to use on
chip counters so before I say how you're
going to measure things we first want to
say when would you want to measure
things so what if you have a problem
like don't look at CPU caching behavior
first I had a really simultaneously
interesting and slightly depressing
conversation with a lady who works for a
software vendor who had a consulting
engagement with a client where the
client was telling her that there was a
huge performance problem in him with the
software because it was context
switching too much he was having
terrible CPU cache performance and it
turned out the software was actually on
another machine entirely and there was
like a network connection between it and
all sorts of other things interacting so
kind don't look now first there are lots
of other things that might kind of cause
you performance bottlenecks or
performance problems which are more
likely to be kind of top of the list
like a network in your database waiting
on an external service to match iog see
all the rest of it but when you've
decided that your program is
bottlenecked by a computational task
something that's actually using your CPU
and you know where in your program that
bottleneck is then you can start
thinking about CPU caching okay so
performance event counters modern CPUs
have these additional registers which
let you count events that happen that
are related to the CPU itself so an
event might be there's a level 3 cache
miss there's a level 1 cache hit there's
a branch mispredict there's I think you
can get temperature monitoring
information you can get like literally
hundreds and hundreds of different types
of events can be kind of recorded and
extracted out and the other thing to
remember about these registers is
they're what's termed is model specific
registers
that means they're not standardized by
x86 they may well differ between
hardware vendors and they may well I'd
better they will differ between hardware
vendors and they will differ between CPU
models so Intel in general seem to try
and aim to keep some concept of
backwards compatibility but it's not
exactly precise because it might be that
architectural II their CPU design
changes and that totally affects what
you weather those events are even
existing or continuing to happen under
the new design so you have to bear in
mind that when you're looking at this
world it's not exactly the right once
run anywhere Java that you're used to
it's kind of a little bit more like a
kind of a Western scenario where the
rules are quite questionable and it's a
moral no-man's land don't worry too much
about the details there's plenty of
tooling which goes down and deals with
all the innards so in terms of kind of
the measurements you want to start
having a look at one of the key metrics
of CPU utilization at the lower level is
instruction retirement rates so an
instruction gets retired when it's
executed right through its a completion
so it ignores branch misprediction
ignores you know actually instructions
which might be in the process of being
executed but aren't completely executed
and the key point is that instruction
retirement is a bigger is better time
metric now the theoretical upper bound
on your instruction retirement rate is
the clock speed of your CPU times the
number of available cause so that's like
your kind of upper bound you'll never
get there it's like a very very much a
theoretical upper bound and the point is
that you can look at this instruction
and say you're stalled when you're not
retiring instructions you're kind of
wasting your CPU away and the other
thing it lets you do is if you've tuned
some of the other measurements which
we're looking at which I'll cover in a
sec and you see that there's a
correlation between them getting better
and your instruction retirement rate
getting better you know you're actually
fit
seeing the problem rather than tweaking
another number which you don't really
see the impact of okay
now in terms of cache misses themselves
there are a bunch of what thing a bunch
of different configurations you can use
to measure your miss and hit rates so
you can look at every different cache
level and you can even at level one
where there's two different types of
cache one for instruction one for data
you can look at those separately you can
measure hits misses
there's read and write rates and
obviously you can start to calculate
ratios and volumes so there's also if
you look at in terms of tooling you
might want to use there's a bunch of
open source profilers so Linux kernel
itself exposes this information if
you're willing to play around with it
and there's also no level things like
read MSR write MSR if you want to look
at open source tooling I think Perth and
liquid I like the kind of top two noted
open source things there's also a bunch
of proprietary tooling all of which has
like different features and different
useful stuff I'm not going to cover any
of the proprietary tooling but that's
they're unavailable so in terms of good
benchmarking practice obviously there's
a lot of JVM Jarvis Pacific benchmarking
good practice like doing warm-ups
measuring and ruling GC out of the
equation things like that making sure
you're actually doing the CPU work and
that's where you're actually bottlenecks
but there's also a bunch of ideas which
are specific to CPU caches so one thing
you need to bear in mind so if I just
explain what this graph is you've got a
eat that each of those lines is a misses
two hits ratio so if you're at 100 that
means every time you try and read data
you miss the cache if you're at zero
that means every time you try and read
data you hit the cache okay and you'll
see there's a bit of noise in the middle
and I'll come to noise in a sec where
you can ignore that on the left hand
side you've got a note you've got a
reasonably flat bogit line on the right
hand side you've got a reasonably flat
but a graph
both of those are me running benchmarks
they're both running the exact same
benchmark but the difference between the
left hand side and the right hand side
is that the left hand side has a much
larger working set it's got bigger data
so it blows the cash out and you
actually still need to read things in so
you need to kind of be very careful to
make sure that the actual amount of data
that you're working with is
representative of how your actual
programs going the other thing to bear
in mind when trying to have a look at
these measurements is the concept of
variance so you see on the left and on
the right those kind of parts the graph
look like random noise that's because
they basically are random noise and if
you ever see a graph that looks like
random noise when you're benchmarking
you're probably benchmarking random
noise and you need to go back and have a
look at your benchmark obviously good
orbiter maths take your standard
deviation or variance make sure it's
reasonably low and you know you can
reproduce it and everything is good but
I've point out the variance figures
specifically because when you're looking
at measuring cash hit miss races hit
miss ratios a lot of these things are
kind of statistical properties the
probability of whether something is in
there or not in there and because it's a
statistical property you need to be very
careful that you're not just fluking the
results okay so key takeaway from this
situation is you can measure your
caching behavior you can baseline you
can understand what's going on under the
hood and that means in the words of
Charlie Sheen
you're winning so let's have a look at
some principles and examples I'm going
to cover some sequential code first and
maybe some concurrent situations later
so firstly we need to think about kind
of what prefetching is so your CPU
obviously doesn't have that kind of
pipeline to main memory entirely
bottlenecks all the time so when there's
available space there it'll try and
eagerly load in the next bit of data
that it can find and the CPU has a bunch
of different strategies which it uses to
identify
what the thing is that it wants to
prefetch and bring in so modern CPUs
have what's known as a data cache unit
which does this and like a really common
simple strategy is trying to prefetch
adjacent cache lines the one that you're
reading it starts looking at patterns of
memory access there's a lot of stuff
under the hood that goes on now the key
point is that no matter how good that
prediction is and how sophisticated it
gets it's never going to be perfect and
you need to try and give you a CPU a bit
of a helping hand so it can do its job
as easily as possible so what do we mean
by giving us a bit of a helping hand
there's a kind of general concept called
locality of reference which kind of
splits down into a few different ways of
giving it a helping hand so one of which
is temporal locality so temprid temporal
locality means you're referring to the
same bits of data in a short space of
time and what that means is stuff that's
already cached can be reused
another thing is spatial locality so
that means referring to data that's
close together in memory and sequential
locality is a special case of spatial
locality where your reads or writes are
arranged linearly and that means that
when it's streaming its prefetch it
meets the CPU heuristics for what to
read okay so there are also a few other
general principles so compressed dupes
is on by default in the JVM but if
you're in a configuration where it's not
and you can use it obviously big heaps
can't you should have a consideration of
that and also trying to use data types
which is just as small and compact as
possible because that means you don't
need to transfer as much data avoiding
big holes in your data I'll cover what I
mean by that I'm trying to make accesses
as as linear as possible so let's look
at like a really simple trivial example
you're kind of mutating some value of a
primitive array now in Java primitive
array elements get laid out sequentially
in memory so if you're accessing element
I and you're looking at element I plus
one that's going to be the next value
bear in mind this only applies to arrays
of primitives okay if we go into a
situation where we iterate over a
primitive array
get a bit of a skip you're kind of
jumping a bigger gap and it's harder to
see what's going on for the CPU so if we
have a look at what that ends up looking
like you can kind of see if you look at
that blue line as it's increasing but
there's a bunch of flat plateau bits now
each of those flat plateau bits
corresponds to me increasing that skip
exponentially okay so the low flat bit
of blue is when skip was one the next
plateau is two then four then eight I
carry on incrementing it and the red
line starts going through the roof as
well so what this is kind of showing is
how you if you're jumping through bigger
holes in your data you can easily see
from the measurements how your miss rate
is going for absolutely through the roof
multi-dimensional arrays become a bit
more complicated even for primitives so
the way multi-dimensional arrays end up
working in Java is their arrays of
pointers to other arrays which can then
be located at arbitrary other places in
memory okay so what that means is if you
jump if you're within that in array you
can still guarantee that they're
sequential for primitives but if you
jump to somewhere else it's not at all
predictable as to where that jump will
be going to so what some people start to
do is realign their accesses around
arrays to make a single big array of the
size multiplied by you know the width by
the height and then they can use a
little formula like that to index into
the specific point in the array that
they want to go to now if you're doing
that kind of approach you really need to
be careful about the way in which you
define your striding alignment so on the
left and the right it's got basically
the same code but with just this
striding alignment different so on the
left it's going incrementing by column
and on the right it's incrementing by
Rho so incrementing by column means when
you come to get to the next element you
kind of got to jump over
to feature the next element but if
you're going by row you can just go
sequentially as you can see from the
Miss rates you're getting like 90 to 86%
misses by destroying in my column and
like way down by like 30 or so for going
by row okay
so there's also a bunch of other general
kind of data layout principles that go
beyond this so if you have access to
primitive specialized collections like
the new trove or GS collections you
might want to have a look at using those
guys because that gives you a nice easy
friendly Java API that's a bit more like
you know a normal list but with the
primitive specialization of an array
arrays consistently beat linked lists
for a load of tasks primarily because
linked lists end up with a load of
pointer chasing so you've got to go and
find the next element the linked list
then you've got a rel offset element
from the node to the element of the
linked list and those kind of accesses
are not at all predictable for your CPU
whereas for an array even if the object
that you're referencing at that element
is not predictable at least the
reference to the pointer for that is
more predictable hash tables tend to
dominate search trees again because
search trees have horrible properties
where again you're doing pointer jumping
to go down the tree to find where that
next element is code bloating so there's
the I don't know how well-known it is
but there's this kind of semi famous
benchmark done using GCC where GCC move
was compiling some code and there was a
version with o2 level of optimization
and O 3 and the O 3 the more heavily
optimized version of the code ran 17
times slower and the reefs in around 17
times slower was because it had very
extensive loop unrolling where the body
of the loop was pulled out multiple
times and the increase in size of the
methods then totally blew the
instruction cache on the CPU so if you
do those kind of low-level trips in your
code base like manually loop unrolling
be careful because there there is a
of CPU impact the other very much things
very mind I'm not going to cover these
guys specifically but there's a hell of
a lot of research and publish material
out there so if you have a
domain-specific problem you might want
to just have a quick literature review
and have a look through those custom
data structures cuz there's a load of
stuff which people have worked on
already and they've already solved the
problem and there's a few examples which
have quite nice cache locality
properties now I made some reference
earlier to the fact that I was talking
specifically about primitives but in
terms of the way object layout works in
Java it's not necessarily our friend on
this topic now if you've got an array of
objects each element in that array then
references an object which goes
somewhere else in in memory and that's
not guaranteed to be the next slot in
memory it's not guaranteed to be to have
any kind of alignment properties and so
you can easily end up with those
situations where you do a lot of pointer
chasing all over the place now it is
somewhat of a Java weakness hopefully at
some point in time there'll be an
implementation of some kind of value
types or IBM have a packed objects
proposal something that allows more like
memory layout control and understanding
of these kind of things but at the
moment you just have to deal with that
and your garbage collector can also not
be your friend in some of these
situations so it can copy your objects
around it can compact them it can do a
lot of things that move where objects
are in memory which means even if you
had good locality of reference when the
object was allocated you might not by
the time your program comes to use it
later it's not entirely a bad thing
though because thread a thread local
allocation buffers do tend to help there
are alternative approaches I say
alternative I don't necessarily mean
good some people have begun to use
things like sun miss gun safe to
directly allocate blocks of memory you
can then manage the mapping of that
memory and manage how you write your
code yourself to utilize an unsafe
allocated memory now I'm not saying
never do this but for like 99.9% of
people never do this because it's it may
be slower it's very error-prone it's
definitely a lot more hard work you know
if you work in the kind of low latency
trading space I know this kind of thing
is fairly commonplace but for a lot of
other areas it's it's really not worth
it I also have a library on github which
I'll just briefly mention which
basically lets you define an interface
and then it does all the unsafe stuff
under the hood so you just write normal
Java code you don't worry about the
allocation I still won't necessarily
recommend people use this it's up there
if you want to play around and
experiment with but I would view it very
much as an experiment rather than
something I would ever recommend for
production use but yeah so have a play
maybe let's have a little mention of
data alignment so we say that an address
is n byte aligned when the address is
divisible by n okay and we say
something's cache line aligned when it's
byte aligned to the size of the cache
line you also get something called word
aligned which means it's aligned to the
word size of your CPU now Java loves
doing words alignment so any object you
create with with with knew will be eight
byte aligned any quarter unsafe to
allocate memory will also be eight byte
aligned byte buffer dot allocate direct
if you ever use that method ends up
backing on to the unsafe allocation so
it's also eight byte aligned now there's
also a kind of trick
if you do end up using unsafe or a byte
buffer directly which lets you do
alignment to a certain amount so
specifically if you want to make sure
your data begins at the beginning of a
cache line and ends at the end of a
cache line you can over allocate by the
size of your cache line
find out the address where your memory
begins and then offset all your accesses
by the difference between those two okay
your address if you allocate memory
through unsafe you've just got along
with that address in and if you're using
direct byte buffer and you really really
don't like yourself it's got a field
called address which contains the
address of the data which you can
reference but I again clarify I'm not at
all recommending this or responsible for
the harm you may do to your co-workers
and loved ones there is good reason why
you might want to do cache line aligned
access so the experiments here were done
using Java and using direct byte buffers
and comparing the performance of data
accesses beginning at the beginning of
cache line with data accesses that
straddles two cache lines and depending
upon a bunch of factors such as the size
of data accessed in the amount you are
getting a kind of three to six times
performance speed-up on core to Joe's
about a one and a half on Nehalem and
Sandy Bridge so if you've got large
amounts of data there is a compelling
performance advantage for doing reading
and writing that's cache line aligned so
just recap for this section the key
point on it at home is you want your
data in memory to look like cheddar and
be thinly sliced when you access it
rather than looking like m and tar and
having huge numbers of holes in it
that's the key pointer so I'm gonna have
a bit of a look now at translation
lookaside buffer so I briefly mentioned
the thread-local allocation buffers I
know the acronym is very similar those
are two very different concepts be
careful not to confuse the two
especially not when having drunken
conversations in a pub so just a quick
recap for virtual memory I'm sure
everyone at some point in time in their
life and you how virtual memory worked
but it's always good to make sure we
still do so basically the problem that
operated one of the problems the
operating systems have to solve
is that Rams finite and it's fragmented
so different blocks of it get allocated
to different programs but your program
would love to view memory as being a
single contiguous address space with
infinite memory and we also want to
support running multiple programs at the
same time because multitasking is quite
useful so kind of virtual memory
provides a mapping between a virtual
address space which the program works on
and the physical address space that may
have some elements in RAM may have some
elements on disk page tables are the
things that define the mapping between
the virtual address space and the
physical address space so everything
that has an address really has two
addresses the address where the program
thinks it is and the address where the
operating system knows it is this is an
OS managed object and when you get an
entry missing in the page table you get
what's known as a page fault and the OS
also manages bringing in pages from disk
now the problem with page tables is
because you need to manage this mapping
and because a lot of accesses need to go
through the mapping controlling the
amount of memory access that's required
on page tables got very very expensive
to the extent that when kind of early
operating systems came through having
virtual memory was an incredibly bad
feature for your OS performance wise now
hardware vendors caught on to this and
said hey we need to solve this problem
and introduce the translation lookaside
buffer now a TLB is like a CPU cache but
for your page table entries and it tries
to minimize the amount of memory
readwrite bottleneck you get on doing
page look at page lookups when you get a
multi-level CPU cache you end up with a
multi-level TLB cache and also the hit
miss readwrite rates all that kind of
stuff is also measurable through on chip
performance counters as we also saw
with normal memory access the other
thing you get which is frankly awesome
is what's known as the walk duration if
you if you want to have a look at what's
going on have a look at the water Asian
because the walk duration takes into
account the the overhead of actually
looking things up it actually tells you
how many clock cycles are getting eaten
through doing TLB work now there is also
this thing where kind of early hardware
support came in that whenever you had a
context switch between running one
process and another process you need to
change the whole address space and it
flushed all the TLB entries out in order
to affect this change and that's a very
very expensive flush now on modern
hardware the cost of this is
significantly reduced by putting an
entry in the TLB for every page called
the address based identifier which is
just a number that says it's associated
with this address space now the
interesting thing about page sizes
themselves is there's a kind of
trade-off between the size so if you've
got a bigger page you've got less pages
for your memory and you've got quicker
page lookups but if you've got bigger
pages because a lot of things are only
transferred in in terms of the pages the
atomic unit you're wasting a whole load
of memory and potentially you're also
hitting a point where you end up doing
huge amount of disk paging to read pages
in now fortunately because of this
trade-off the page size is adjustable so
I think on modern Intel trips you can do
4k 2 Meg on one gig if you're reading
the manual sometimes you might see 2
megabytes referred to as huge and one
gigabyte as merely large I don't know
why that is the case but just just a
warning there you can sometimes get
quite significant speed ups by changing
the page size so like you know tariffs
I've seen measured values in the 10 to
30 percent range now obviously 30
percent isn't an amazing win but you're
talking about literally flicking a
switch
to get it so it's quite quite a cheap
win if you're wanting to use large pages
your operating system might only make
certain numbers of your pages large
pages so if you're on Linux you might
want to look at /proc /mm info to find
out how many are used if you're using
Java that flag on bullet point four is
what you need to pass into the JVM to
tell it's actually use large pages I
don't know if anyone in the audience
ever encountered a situation where it's
oh wait if you benchmark this kind of
thing you might find that you need also
to do a complete clean reboot of your
machine to make sure that you can
actually have accesses to those large
pages and my gratuitous picture of
ayaats at the moment is just to bang
home the point if you ever do
performance tuning on oracle databases
they love large pages just as much as
larry loves his large yacht so the TLDR
for translation is bearing in mind that
virtual memory and paging have a big
overhead in running your program and you
can change your page size to address
this ok so I have a look at some more
kind of concurrency examples related to
CPU cache in now so just just to recap a
context which is what happens when you
change between two things which you're
executing on your computer at the same
time so you can have context switching
at the thread level you can have it at
the process level and if you're running
on a hypervisor you can also have it
between virtualized operating systems
now you need to bear in mind that the
cost model for how expensive a context
switch is as in how much time it takes
varies quite significantly between these
three variants so for example with a
thread because you're both sticking
within the same process you don't have a
lot of the translation lookaside buffer
overheads because you don't need to
switch the address space of the process
but if you've got processor
context switching it can be a lot more
painful and virtualized OS well if
you're talking about you know how much
this kind of thing costs as in when I
say cost I mean how much time it takes
you can want to view two different types
of costs so there's a direct cost which
is what the OS actually has to do to
implement the context switch so you know
there's a lot of details under the hood
but the simple three bullet point
summary is it saves the old process
state it figures out what the new
process is going to be and then it loads
the new process state in your scheduling
cost might not be context might not be
context might not be constant time but
the direct costs don't vary that much
I'll saying indirect costs on the other
hand can vary hugely so there's a
translation lookaside buffer reload for
the process or virtualized OS context
which is you ended doing a cpu pipeline
flush because you're jumping to a
totally different area you get some
horrific potentially horrific
interference between the locality of
reference between two different threads
okay
so there's also some existing attempts
to quantify exactly how expensive and
how slow context switches are so this
paper are a reference here is a few
years old so the actual hard core
numbers I'm sure are completely wrong on
any modern hardware but I think the
ratios and the principles still apply so
the methodology that these guys used was
they wrote a kind of benchmark which
they could tune how much data they were
reading and writing and they could tune
how many threads they were running at
the same time so they could have a good
way of measuring on different amounts of
data read and write I'm measuring on
increasingly large numbers of contexts
creating lis large amounts of
context switching and they found that
their direct costs were very very
continent nearly constant the entire
time but the indirect costs range from
being basically zero to being you know
several hundred times larger than the
direct costs and they started to defined
that the indirect costs become the
primary factor when the working set of
your data was bigger than your l2 cache
I'm not sure off the CPU they were using
had an l-3 cache so you can't really
conclude that's a hard thing for your
program but what I'll really say is a
lot of people have a situation where
they have some kind of app server a
servlet container something running you
know worker threads or JDBC pool things
like that how many threads you run on
your thread pools on your servlet
container things like that can have a
huge difference in terms of performance
as to how at the both the latency and
throughput levels that you can get and I
think especially when you look at the
cost of context switching if you're
sitting there with a two-for box running
four thousand threads you might want to
just have a quick look at what's going
on and it sounds a bit crazy but I've
seen a few people doing this kind of
thing you can get context switching
numbers very easily from your operating
system without even looking at the
underlying CPU caching behavior but
don't think that it's cheap just because
the OS doesn't need to do stuff because
there's a lot of knock on performance
costs for you okay locking so in Java
there's kind of broadly two types of
locks there's the old type of lock with
the synchronized statement and then
there's kind of the Java five newer lock
so the locks I'm talking about here this
only applies to the old style of
synchronized key word and which is that
a lock requires what's known as kernel
arbitration so if you've got two threads
both saying hey I want to win this lock
and goes down to your OS as to who we
now this doesn't cause a context switch
exactly it causes what's known as a mode
switch but you do get two of them cause
you've got to go into the kernel then
deal with the arbitration and then jump
back out again
and also if you're doing too much lock
contention where you've got your
programs kind of constantly locking and
and you've got a lot of a lot of threads
all trying to win those locks you'll end
up with a lot of context switching where
your OS is trying to say hey you have a
go at making progress you have a go at
making progress and you'll end up with
context switching too much those kind of
things have a significant cache
pollution cost under the head under the
hood so even if you have a look at your
lock contention problem and say well you
know the system time for my CPU you know
you know the amount of time spent in
kernel space isn't too high you still
need to bear in mind that there are
knock-on effects of the user space you
can try not free algorithms or just
minimizing your lock contention or in
some cases it's just worth just really
value ating your architecture and see if
you can use some of the existing
concurrency utilities in Java which have
already been optimized by very smart
people to minimize this kind of problem
the other thing to bear in mind is
what's known as GC safe pointing so
intermittently your garbage collector
wants to pause the program that's
running and a safe point is that the
point in the program's execution where
it's fine for the GC to come along
interrupt safe points are used by other
things such as thread such as execution
profilers but that's another matter
all-together
now safe points under the hood do cause
a page fault so you need to bear that in
mind if you're looking at for example
things like the TLB warp duration and
doing execution profiling at the same
time you're seeing to bear in mind that
that's GC related and you can also get
context switching about context
switching problems between your mutator
or program threads and your garbage
collector the other thing is if your
program thread is
in order to do some GC work you might
not have your operating system put it
back reschedule it on the same core as
before so all that nice cache locality
that you'd built up on the level 2 and
level 1 caches which is specific to that
core is just kind of thrown away I don't
want to talk about the Java memory model
here's I was gonna say it's like a talk
in itself but it's like it's like a
lifetime of learning in itself so
there's there's a standard JSON 133
which defines the Java memory model and
gives you guarantees about visibility
and ordering and what certain threads
can see about what other threads are
doing now if we just look at for example
of volatile keyword in Java when your
JIT compiler comes along it'll probably
insert if you look at the dumped
assembly the LOC prefix which is a
prefix on x86 instructions which gives
certain ordering control at CPU level
now in terms of the hardware that ends
up having an impact on your cache
subsystem because if you're saying this
writes to memory has to be visible on
other threads then you need to ensure
that your CPU caching level there's a
coherency between the view of the
different CPU of the different layers of
the CPU cache and modern muck processors
use variants of the messy protocol in
order to implement this I want to talk
about the messy protocol but basically
this whole thing has a cost and it has a
cost
which if you look at your execution
profile of you of your of your code so
if you just run it through a profiler
you'd see there's some time spent in
that method and if you look at a cache
profile of view of the world you can see
that that cost is is visible in terms of
the cache coherence one of the kind of
major dynamic languages on the JVM for a
while had a situation in which any
construct
to call caused a volatile right to a
variable when profiling code and saying
why is this significantly slower than it
looks like it should be this volatile
right is not at all visible to the
person who is doing the optimization
effort that's quite a difficult
situation to figure out what the
underlying problem was the other thing
I'm going to talk about briefly when it
comes to concurrency is false sharing so
when we talk about cache lines obviously
different chunks of data can live on
that cache line because it's 64 bytes
and share that cache line now sometimes
that's good because it means that your
data gets loaded in not so good because
sometimes certain data just accidentally
lives next to other data and if you're
doing writes to kind of volatile
variables for example and you're in a
situation where you have two threads
writing to the same cache line even if
they're not writing to the same variable
you can have this situation where each
thread once the kind of has to force a
coherence situation has to force a right
back on that cache line so there's a
diagram to give you an idea of what's
going on so if you've got two threads on
a CPU both write in to the same cache
line and both forcing data to be written
back now one of the available solutions
which has been kind of used up till now
field padding so if you've got a class
because there are certain rules that
happen about the way fields in classes
get ordered in memory you can put a
bunch of other fields around a value
that's volatile and being written to in
order to pad out that field and make
sure that no two instances of that field
are on the same cache line so bear in
mind I've added seven fields here which
are eight bytes and then set the 64
bytes which is the cache line size the
thing you need to bear in mind with
field padding based solutions is that
fields in your class
get reordered okay so they get reordered
so the bigger types get pushed towards
the beginning so all the long so we put
together all the inter we put together
all the short so you put together if you
want to again this has got a very
low-level technique that I wouldn't
recommend for people unless you're
absolutely sure you need to use it if
you do want to use it something that
gives you really good visibility into
what's going on under the hood is a
github project called Java object
layouts by aleksey shavelev who's you
can just say here's a class dump me what
the object layout will be in hotspot a
better solution is hopefully coming in
Java right with what's known as jet 142
so a jet is a jar for enhancement
proposal a JDK enhancement proposal so
that's something that's a proposed
change to the open JDK project that lets
you give you a new feature or a bug
fixes
now what jet 142 gives you is is an
annotation called app contended that you
can put on variables that lets your JVM
manage the field padding you can then
provide a single flag to the JVM at
startup to say I want fields padded this
much or I want it disabled or enabled or
all sorts of things so they're
supporting the codebase in open JDK at
the moment for it and it should be
available in Java rate so that's
something to have a look at the other
thing to bear in mind goes from the
subject of false sharing is it's also
possible to have false sharing problems
in your garbage collector so the
concurrent mark-sweep collector in
hotspots splits up ram into a bunch of
different cards and it keeps track
intermittently of what blocks of ram
have been written to so it doesn't need
to rescan the whole of your memory
remark phases you now the problem is
that in hotspot itself there's an
optimization that's been applied that
says instead of going hey
has this card already been written - if
so market because this needs to be
something that gets done on every single
write done by a Java process the
optimization is just say hey look when
their servers are right we're just gonna
write the card table and sometimes that
means because the card tables block
together in memory that you can get
false sharing problems between different
GC threads trying to write to the to the
table there is a parameter use
conditional card marking which disables
this behavior and says hey duty if
statement you need to use with care
because that optimization was originally
an optimization and for most people will
be faster but you you it's just another
thing to think about when you're when
you're looking at this kind of stuff so
basically on the the concurrency front
there's a lot of Barbie where's context
switching is very expensive it's a lot
more expensive than most people view it
as it's a lot more expensive than you
necessarily have obvious visibility
around file sharing is a problem which
some people encounter and what you can
have you know alleviating file sharing
problems can give you significant speed
ups inserted in each cases and obviously
all those things that make concurrent
code safe like visibility atomicity and
locking they all have costs and
understanding those costs can be quite
hard so just a big too long didn't list
on the whole talk let's bury my caching
behavior is you can measure those
effects through modern specific
registers and there's a bunch of common
problems the solutions which you can
apply so there's also a few interesting
links so my blog there's stuff about the
the ORAC graph a PDF document what every
programmer needs to know about memory
which if you've got plenty of time on
your hands I recommend you guys read
it's really interesting the company I
work for who do kind of performance
tooling software core J clarity we've
got a friends or
this which is a mailing list if you kind
of want to ask questions or talk about
anything performance related we'd love
to hear your say on things
Martin Thompson is a guy whose blogs
very regularly and with very very high
quality so his blog is also highly
recommended he talks a lot about what
what he terms mechanical sympathy which
is being programming in a way that helps
your hardware and also there's a lot of
job related stuff and the concurrency
interest mailing this which is a kind of
Java related interest round concurrency
also has intermittent discussions about
things like for sharing contended
padding and a lot of that the costs on
visibility and and locking so I'm done I
don't know if anyone has any questions
no okay well thanks for coming along and
listening guys</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>