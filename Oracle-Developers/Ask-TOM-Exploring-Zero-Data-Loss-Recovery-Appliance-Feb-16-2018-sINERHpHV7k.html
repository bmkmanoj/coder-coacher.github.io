<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Ask TOM: Exploring Zero Data Loss Recovery Appliance | Feb 16, 2018 | Coder Coacher - Coaching Coders</title><meta content="Ask TOM: Exploring Zero Data Loss Recovery Appliance | Feb 16, 2018 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Ask TOM: Exploring Zero Data Loss Recovery Appliance | Feb 16, 2018</b></h2><h5 class="post__date">2018-03-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/sINERHpHV7k" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">to see the interest in our technologies
so we're going to be focusing on backup
recovery technology area and we'll hit
different topics every month under this
group of topics today's topic will be on
Recovery Manager and flashback
technologies and the format that we're
gonna have is I'll start with a short
presentation just to outline these two
technologies and and then we will take
your questions on chat so you are muted
during this session so please use chat
myself and my colleagues will be on the
chat session and then you know we can't
discuss any SRS or licensing issues you
need to speak to your supports on your
account team for those topics but we're
happy to talk about your use cases you
know technical questions you have
product questions those kinds of things
all right so let's go ahead and start
and and we'll see how this goes
so probably many of you know about our
man
our man has been around for 20 plus
years it's hard to believe but it was
introduced back in Oracle 8 okay so it's
been growing over time the adoption has
definitely been increasing and we have
probably you know 80 90 percent of
customers that we serve a use are met
and as you know that Ironman is the
backup tool for the database right and
it has capabilities for back at the disk
backing up to to tape or to tertiary
media through a media management
software and we have Oracle secure
backup which is our integrated backup
software or you can choose to use net
backup networker well they're the sorts
of various backup software applications
in the market and so these software plug
into our men to provide the interface to
the media for the backups now our man
has evolved over time and all
covermore the functionality as we've
gone through the different releases but
we have you know not just traditional
backup in recovery but Armand has
evolved to become much more
comprehensive in terms of the
functionality and use cases and they can
address for example we have many
customers using our mend duplicates and
our menu store operations to do database
cloning and a new platform migration ok
these are very common use cases
customers have test dev environments
they need to move from one server
platform to another server platform over
some period of time and so Armen enables
the customer to do this because at the
heart of our men is a as a backup and
restore engine so the data can be
restored to any other server the data
can also be converted to a different
Indian platform and that is our cross
platform migration support as well in
our net and we also support obviously
other technologies within Oracle H a
more high availability area such as data
guard our men can be used to backup both
the primary and physical standby
databases and your backups in this
environment can be used on either site
so you can have because the backups are
physically identical it's the same copy
the database on the primary and standby
you can use those backups
interchangeably that's one of the
beauties of the way that Armand works ok
we also see our menus to do refreshes of
the standby say for example your standby
databases is lagging behind maybe the
network was interrupted or your or your
renew stream was was have an issue when
the standby comes back you can actually
use an armed and incremental backup
taking on the primary and apply that on
the standby to catch up the standby much
quicker than you can by doing a way to
apply ok so this is a sort of a
incremental backup a restore if you will
then the our men land but it's a very
interesting use case right to catch up a
standby database in this manner we all
to work with real application clusters
to be able to do parallel backups across
multiple nodes to be able to spread
workloads or allocate specific channels
for the backup of a store across this a
big nodes so it's very flexible and the
way that you can use it for your
particular needs now we also have some
some more backup destination support
okay for very interesting use cases one
very popular one that is has been
available for one to two years now is we
call back up the cloud storage service
okay this will be a topic of our office
hours in in two months
we'll talk more about backup service how
you can use it it's a very quick way for
customers to get a low-cost easy way to
get a backup stored in our cloud
securely and it's off-site right and the
backup can also be restored to various
locations not just back to the original
location that can be restored to another
on-premise location another data center
for example it can be restored to a
database cloud instance for example if
you want to start spinning up cloud
instances and doing tests up and so
forth on our Oracle cloud so very
flexible again very low cost and we will
cover that and in the April office our
session now the other interesting backup
support that we have is for the zero
data loss recovery appliance and this
will be a focus of our session next
month
the recovery appliance is a new internet
system developed by our product team to
provide very very tightly integrated
very low data loss zero data loss in
fact protection support okay for your
all your core databases within your
environment it is meant to scale it's
meant to consolidate all your backups
into a very scalable performance system
and it's a it's redundant right and the
storage and hardware is built by our
product team who's also built X data so
we leverage varies
similar technologies underneath for
running these backups across many many
databases in some cases our customers
have thousands of databases across the
enterprise
backing up to multiple recovery
appliances right so there are various
interesting features and capabilities
within the recovery plans will cover the
next month just keep that in your mind
as we go through and ask obviously ask
your questions on any of these
technologies in this session if you come
up so again the history of our man right
it's been you know 20 plus years and you
can see I'm starting over late
these are just some of the features that
we've introduced and just to give you
some food for thought and maybe serve
some questions for the chat is that you
know we started you know back with
Oracle eight eight nine nine nine with
me you know kind of base level
functionality doing parallelism for the
backup in the store we introduced
duplicated in 9i which is you know very
effective to the cloning and the storing
of the bankers to another database
server block mean recovery online
mechanisms to get a particular block or
a set of blocks that are found to be
corrupted within the database to be
repaired very quickly that was also a
nice sort of online value feature from
from the arm inside you know moving on
to ten gene 11g in the sort of 2000s mid
2000s area we have the fast recovery
area fast incremental backups
incrementally updated backups so the
incrementally updated backups is the
mechanism to take an image copy to disk
backup location and then take your
incrementals thereafter to essentially
update or roll forward if you will those
image copies so this eliminates the need
to take another full backup to disk and
you have a current copy that's always
available and that leads to the next
point which was switch to copy this is a
mechanism by which we can switch the
database files to use the copy instead
of the production files on the
production storage location ASM disk
like like data or or just your local
data file location right it's a very
quick way to do that should essentially
do this restore instead of actually
storing all the data back from the
backup we also introduced the mechanism
to offload your backups to the standby
database a lot of customers like to do
this in the data garden environment as
you know that backups can be can be
taxing on your systems for you know
large databases it can take a lot of
time for example and we found that
customers can use a local standby to say
back to offload if you will the backups
so that again the backups are physically
interchangeable can be used between
primary and standby and and so this off
flows the primary yet still gives them
completely cover ability from the backup
locally right and so that native Guard
integration if you will if you will was
introduced around this time dream now so
going into today's Oracle 12c and now
12c release 2 in our upcoming 18 seat
release number of features here that we
added such as table level recovery the
cross platform backup and recovery
support that I mentioned before for a
common use case customers need to move
from one OS platform to another of the
time we also had enhanced active
duplicates active duplicates is a
feature of duplicates command that
allows you to create a clone database by
simply connecting with the source
database so no longer do you need to
take a backup you simply connect to the
source and Arman actually acts as the
conduit to copy the files and put them
and put them and get them fixed up on
the destination server side right and we
can do this in parallel across multiple
channels etc so it's very nice way to
create a clone database without a
heading to take a backup first we also
have as I said the standby database
synchronization we can take an
incremental and catch a standby that's
maybe behind very quickly and also a
multi-tenant support you know
multi-tenant with 12c with a big sort of
market
in a feature introduction if you will
into the day-to-day side and this allows
you to do that we call a a container
database which can hold multiple
pluggable databases so you have
individual databases within a larger
container database and we can back up
that entire container database to to
hold you know which will contain all the
data for those individual pluggable
databases you can then restore
individual pluggable databases as needed
basically a consolidation approach right
for your database side and to be able to
then consolidate the backups very
effectively yet still get all the
recovery features in are meant that a
pluggable database level okay so the
second part of this session will be a
flashback and flashback has been also
around for a number of years introduced
first in 10g and we have a number of
individual operations if you will under
for the flashback technology family and
so we have kind of two groups one being
is error investigation which is on you
know querying the data as of a point
item passed so we have commands such as
flashback query to see what the you know
what a table looks like as of time
yesterday or two days ago
for example this might be for auditing
reporting for any kind of or any issues
that you see them that imitate its data
itself right see them compare the
versions of the data over time this is
where flashback version query comes into
hank very handy where you can see kind
of the changes in rows and table between
two different times in past and then we
also have flashback transaction query
and which is the ability to look at a
transaction and history of a transaction
ID and the rows that it affected over
time okay so it's kind of even more
interesting if you will
auditing method to see specific
transaction details over time now on the
recovery aspect so flashback can also
act on the data itself - we
or to undo certain changes and we do
this with a number of operations you see
your flashback table which gives you
back a version of a look of the table
back to a point time in the past
flashback drop yeah but you know
customers actually connect to a
production database this was the test
database and they drop a set of tables
that they were intending to do for
retest kind of exercise so flashback
drop is a you know the mabel's database
to essentially move a table from the
recycle bin back into the actual
database sort of the original database
structure itself ok so we have kind of a
concept of recycle bin in the database
flashback database operates on the
entire database to rewind the entire
database to a point time in the past and
we do that through a different set of
files and you set of files we call
flashback logs these are kept in the
fast recovery area essentially logging
sort of the before change images as
changes occur so we have log we log
these sort of block images and then the
flashback log files and then we replay
those we search back into those
flashback log files upon flashback
database operations so you can get back
to the point time and past and then we
have a flashback transaction so DBMS
flashback transaction back out allows
you to take a specific transaction ID or
a set of IDs and unwind those okay you
can unwind those take a look and then
commit the changes if you really want to
undo those you can also cascade option
which allows you to all the dependent
transactions to also be undone that are
related to that transaction so it's
pretty flexible kind of recovery
mechanism at a transaction level as well
so just the you know another look at the
commands themselves to do like flashback
queries like star as of you know kind of
clause here
flashback versions query has the Select
star versions between clause then
flashback transaction query using select
star from flashback transaction query a
key
and then you supply the transaction ID
and we'll give you on the court
essentially of all the changes that
we've been affected by that transaction
on rows and whatever tables that were
that we're out touched and these queries
are based on your undo so as long as you
decide you undo for a retention time
that you want to do this query in the
past then we'll be able to serve you
know service those requests and then at
the flashback error correction level as
I mentioned we have flashback database
flashback table flashback drop and then
flashback transaction back so it's a
pretty rich set of features this is very
effective for human error logical
application type of errors where you
don't need to mostly do a physical
restore and recovery using a backup but
you have something caught maybe within
the last few hours last day and you want
to just unwind those changes very
quickly alright so with that said I do
see a number of chat questions I've
already started coming up and so we'll
go ahead and start taking these and
we'll also highlight some of these on
the audio as you go through and you know
yeah Kelly Marko you can also get
unmuted to highlight some of these
questions as well if you would like so
just taking a quick look at some of
those questions now I see a question
about our man duplicate okay and then
one about plenty of archive logs alright
so let's just go with that then since we
have this question already
it's about arm and duplicate restore for
OMF /sm files rmn is skipping already
restored OMF ASM files from a failed
duplicated attempt and then the question
is
I thought that when a set new name is
run our men will come up with a new name
for the data files ok how was our
medical get able to identify identified
files from a previous field did look at
and skip them even after a new historic
control file is stored and said your
name is executed after the duplicate is
we want ok so that's a good question
there might be and I see this is in 12
102 ok so that's also more later release
that we have the 10th unit does work as
you as you described that that the set
new name essentially redirects the names
on the control file for the the
duplicated database for the location of
where those files will go okay and so if
you have a set new name after a new
control fucker is stored and said
meaning that's executed after the
duplicate is rerun we should be able to
pick up that that noon that set new name
now one of the things is that in that
fail duplicate attempt one thing that
might be good to do is to is to clean up
those files yourself ok you can you can
do a you know you can have a new
location specified a new ASM disk room
certified for example trying to set a
new name on that one and see if that
goes through ok
but if then if the file name matches and
you're using ASM unique ID files right
we will skip those files that are
already restored and so it it really
depends on kind of what more the
previous state or and what happened
after that failed duplicate attempt and
but I would expect that the send you
nation if should allow those files to be
redirected if you will to a new location
right so if you can try doing that
standing in like a new disk group or
basically a new location
maybe underneath the existing date of
this group pointing to a brand new
location with no files in there and do a
duplicate and see if that goes through
then there's probably some issue with
the original foundlings themselves that
have only been restored that's the best
I can I can get you to if you want you
can also obviously we'll we can follow
with the offline after the session okay
we also have a question about the best
what is the best in bug feed solution to
clean up archive logs and the standby
database we're seeing that our men are
kind of logger attention with effort has
some bugs and various versions used to
recommend scripting the cleanup is the
safest okay right so I'm not
specifically aware of the these bugs
that you're speaking to I mean we
definitely fixed and improve things over
time this is the arm and archive log
retention setting with Fr a so this is
probably the configure deletion policy
for the archive logs that's probably
what you're referring to but those
configure eight configure settings
should work as expected on both the
primary and the standby database you
know for example if you have a you know
if you have a primary that's you know
sending backup sending um you know
primary going to a standby and you're
taking backups off of that standby
database for example you want the log on
the primary to be deleted when you have
applied that right so you do deletion
policy of applied on standby
on the primary database now on the
standby you would configure this to be
archive log deletion policy to two
backed up for example backed up one time
and that would not in none of the delete
commands or FR a clean up commands would
operate on that log until that backup
has been done okay from the standby
database side now if you do backups on
the primary and the standby for example
then you could do a policy on the
primary called you know configure
deletion policy
you applied on standby backed up 1 times
for example that could that what that
means is that the deletion policy must
be satisfied for both but it's been
applied on the standby and that the log
has been backed up on that particular
database which is the configuration set
so you would do that on primary and
other thing obviously on the standby
we just set it to deletion policy -
backed up you know end times for example
and that would that should take care of
it
so if you're not saying that and you
followed a doc there's maybe something
else going on but there's something we
can take as it's not a call to do s are
sort of triaging or you know kind of
debugging but but certainly try that out
and and again you know following up
afterwards there's still other questions
about the functionality let us know
yeah that is a follow-up questions also
on the duplicate right right right yeah
so so so yeah why don't we you know this
this particular question I think you
know we should deal with this an SR okay
and that's probably the best place to do
this is support can I could treat it
properly versus just through our kind of
session now that we don't have you know
all the information available but but
yeah it definitely file an SR and follow
up there with that but I do see your
your scenario we are trying to convert
the active standby database to snapshot
complete the you know do the clone from
that snapshot and then put that snapshot
back to a normal standby database right
that should be that should work okay
that's my expectation that should work
and then when you're saying the other
case is the Armen duplicate from active
database you have a failing active do
table a gap we don't know this is going
to be a support case that you have to
take up there's maybe some other
configuration or initial agent
parameters or or something else is not
set up properly there can you say
there's a smaller database and that's
working okay so please follow up on the
nests are on this one as well
okay okay great
so we do have you know there's some
questions from the product team that you
know we'd like to add you know ask as
well keep chatting and keep asking your
questions but I think it's interesting
for us to hear some feedback on your
product usage and your environments one
one question we'd like to ask is how
many if you can in you know in your chat
tell us a number of databases that you
have in your environment that are using
Ironman and that's and whether they're
using disk disk tape or a combination of
both in your environments if you could
give us a little bit of data points
there it would help us to see kind of
spectrum if you will of usage
you
okay thank you silver
and thank you serve okay
great yeah CDL array is a very good
topic next month we'll have office hours
on GDL array I will cover that more in
depth there okay
now mm-hmm now the question we're
interested in is kind of your top three
challenges you have in your backup
recovery environment you know the top
three
you know pains or challenges that you
deal with using our men and something
that you know give us a bit of data
points on on your uses as well so top
three challenges in your in your backup
environments well
you
okay Thank You Patrick that's
interesting to see the virtual disc of
ritual you know tape libraries there as
well being used and along with physical
tape
okay you have a question here from serve
fre in another fest mounts if you want
to keep backup in fre and innocent fest
amounts faster what do you recommend
okay
so not sure exactly the question if
you're asking you want to keep some
backups for short-term period or you
know a faster recovery on that Ferrari
and then you want to keep a set of bag
with longer period and another another
location like NFS mount so it's our men
so that the arm and design is to keep a
backup for a database for a retention
period right so you set up a retention
to like 7 days or 30 days and we keep
those backups around okay and there's
two device types that you have discs and
you have SBT and I that's typically take
or a third-party media right and so if
you if you have your backups there's no
if you will there's no ability to have
different retention periods for
different sets of backups okay in from a
configurations perspective we have
customers who want to say delete certain
backups after 30 days and for databases
and they want to delete you know for
example some backups on tape after a
certain amount of days right there is a
way to Duluth the arm and delete
obsolete command to say I wanted to
delete these backups on this channel on
the disc SP disk channel type for
example that are you know older than
seven days recovery window and I one
delete these backups that are older than
30 days on SBT you know you know commit
type of command so you can actually have
you know scripted delete certain backups
from the totally on the disk side or on
the svt's channel side and you can't
separate that out for within a
particular channel as you see here right
so what I would recommend here is to
look at you know potentially look at a
you know salute you know like for zdl
array this is
sort of a engineered recovery appliance
that does allow our men to back up to
the appliance and we do have retention
sets on the recovery appliance for the
disc backups and then you also set a
retention for example on the tape side
right so within the product our zeolite
product itself you can have different
tensions for one database and then your
second question is about do you
recommend recovery catalog database or
controlled file should be sufficient we
typically recommend recovery catalog
database as a best practice recovery
catalog has has been around for a number
of years okay it is a way to centralize
or consolidate all your back of metadata
or history into a into a single location
and it's in a database so you can
actually back it up you can put data
guard on it so you can protect it
through you know H a and outages and in
that respect from one database server to
a standby database for a server for
example and so we you know we typically
will say use recovery catalog database
if if you have just a control file you
can use our man you could have you know
your backups and your history retained
however you'll be limited by how many
how many records that control file can
in hold there's a parameter that we call
control file keep record time and you
know if you decide you want a longer
retention then that period I think it's
typically set for seven days that can
increase the size of the control file
considerably plus the backup records and
the backup records in the controller
called transient records they can
actually be recycled if non-transient
records need to be created such as data
files new data files added it such as
archive logs for example recorded these
are non-transient files and can actually
push how to recycle the backup so you
may actually lose backups older backup
history by using control file with a
recovery catalog database you have no
such limitation you can go very very
high
attention yeah on another option if it
can add Tim for the previous question
can also be the cloud so you can use the
fr a for the quick recovery and local
backups you can do you can do an image
copy for example this is the same thing
we use in in our cloud database
instances we have a local image copy fr
a cat for seven days with incremental
merge and then longer 30 day retention
are sent to object storage you can do
the same for compromise you can do a
image copy to the local disk and then do
a longer retention backup of that every
image to the cloud yeah thank you Mark
great and we did get some other data
points about some of our attendees
setups as well here so that's that's
definitely good to know and we also have
a question here about restarting failed
duplicates is that it safe to restart a
failed duplicate by connecting the
auxiliary completing Academy open via
this room you might want to fill the DB
ID and DB name the same as production
and it will maker up the catalog okay so
silver on this question again if this is
an actual issue when you didn't do it we
start that should be followed with an SR
the duplicate command was enhanced and I
believe that was either
11200 maybe level 204 or 12.1 where we
put in restart ability within the
duplicate command itself so restarting
duplicate should not corrupt the catalog
and I understand that your concern is
that the DB ID and DB name are the same
as production we should we shouldn't
have you know that shouldn't be the case
with at the end we do a need where we
create the new DB ID for the clone and
that is then registered and if the
catalog so we shouldn't have a case
where there's any corruption of the old
pack of entries right if you've had seen
this issue that is a obviously a serious
issue and needs to be looked at through
an SR
I also want to mention that in the past
customers have used it was a failed
duplicate they've been able to restart
by doing arm and restores so being able
to do the normal arm and restore data
files and be able to restore the rest of
the data files recover those data files
and then do the need at the end similar
to what typically would have done so
that's another option
like I said duplicate has what was
enhanced when we started ability in the
11 204 12c kind of lease time frame ok
great we have about 15 minutes left from
this session we welcome any other
questions we have on this on the screen
here as you see and also if Kelly or
Marco do you have any other questions
for the attendees at this point know if
anyone is using the cloud storage as a
backup target just 7 ad FS anyone that
may be also interest in the next session
ok yeah so please feedback on the chat
if you're using backup service for your
data for your draining your backups
I think another interesting question
would be if anybody is already using the
recovery appliance
yes
all right we have G healthcare I see
that silver mentioned that they are
using recovery appliance great
you
okay
so with just the remaining time as you
know as we have you know thinking of
more questions or any chat I've asked
Marco to give us a quick overview of 18c
arm and new features 18c is to be
announced released very shortly and so I
think it might be interesting for you to
see kind of what we have we have done
and will be available in 18 see ya on
can you can you go up to the next slide
team yeah okay yeah so yeah so I did see
which is basically I think today GA is
released officially on accelerator right
add some interesting features and arm
and new features in Armen and
specifically in please added areas that
you see here so enhancements around
around multi-tenant databases
duplication and duplications or data
guard and the support of cloud archives
storage right so there are two different
types of archive storage in the Oracle
cloud one is the standard object storage
and there is the archive storage for a
longer retention currently we only
support object storage with 18c will
introduce in support for the cloud
archive which is for longer attention
it's a it's a lower cost storage on a
price per terabyte basis but it's a snow
where we store if you want that is a
four-hour delay to get it back I'm
covering this now because we don't have
a slide now specifically on these if
you're interested in these topics you
can come as they mention to the April
office hours and we'll cover more
details around this can you go to the
next night yeah okay so I ran
multi-tenant database is the new
featured 18 C is the basically
preservation of the backup history so
when you restore it EDB that from a
backup that was taken
either as an on multi-tenant database so
before you convert and non-muslim
database in a PDB actually or when the
PDB was flagged on a different container
database and you moved after the backup
you can go ahead with public stuff what
happens today is that indeed the history
is lost
you cannot restore from a backup taken
when a database one non multi-tenant
after it's converted into a PDB now with
18c this is preserved so you will be
able to restore a PDB into it either
difference in DB or a new CD be you need
to use the XML data that is generated
from the unplug operation if it's from
one city be to another or then there is
this procedure that can be used to
generate also this this data if the
database was not a multi-tenant database
and the new command is what you see in
the example here when you can do restore
pluggable database pdb name from free
plugin back up and that's that's the new
then you feature and obviously you can
recover and then open it so can you go
ahead Tim yes ok active cross c DB pdb
duplication that's also new obviously so
you can do an active duplication of a
pdb between different containers
can you go ahead see the example command
you start arming ok connect to the root
of the source databases target and then
connect to the axillary instance and
then you can use this new duplicate
pluggable from active database oops go
back one second ok
duplicate pluggable database and pdb
name to the new city B from active
database and that will allow you to
duplicate directly from one city
to another can you go to the next I
think it's perfect last one and it's a
new recovery standby database from
service for data guard it's a one single
command to refresh the stand Bay
everything is refreshed control file
data file and in case there are new data
files that being added they are restored
and with a single command you start
arming when connected to target to the
physical standby database and then go
next you run this command recover
standby database from service and give
that primary DB name and this will
basically roll forward your physical
standby and on yeah basically one single
step everything is done without the need
of backing up restoring etc that's also
new and yeah I think that's all we have
yeah Thank You Marco so yes this last
note was automating I want to describe
before about can't you give a stand by
using an arm and incremental this is
basically automated with one command now
and so it's you don't need to find the
SCN that's needed etc for the
incremental parents and you can simply
run this command it will catch up with
the most recent data in the primary
okay so we're going to take any other
questions on the track and give it a
couple minutes and if not we'll just
close the the session then
now the one thing I do want to know too
is for the pluggable database feature we
do have a flashback pluggable database
that was introduced in 12 release - okay
so you we can do individual flashback
pluggable databases within a larger
container database so please look at
that as well as you survey the flashback
technologies
okay well I'm not seeing any other
question this moment and so I'm gonna be
closing these office hours at this time
thank you again for attending we hope
this was informative team please join us
next week we're gonna be focusing on
zero dead loss recovery appliance and
bring your questions for that as well as
any other backup recovery questions that
you have and we thank you so much for
attending</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>