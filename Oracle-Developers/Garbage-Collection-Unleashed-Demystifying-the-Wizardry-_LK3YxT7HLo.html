<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Garbage Collection Unleashed: Demystifying the Wizardry | Coder Coacher - Coaching Coders</title><meta content="Garbage Collection Unleashed: Demystifying the Wizardry - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Garbage Collection Unleashed: Demystifying the Wizardry</b></h2><h5 class="post__date">2015-06-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_LK3YxT7HLo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay garbage collection unleashed
demystifying the wizardry all right this
is the title now you're going to find
out what I'm actually going to talk
about lawyers your nerd for the next
hour I've got like a fanciful title with
an IBM it's called the runtime architect
basically i'm in charge of core BM
technology for the jvm really the how
how we're going to get things done I've
had a lot of different roles over the
last 18 or so years with in runtime
technology and so 18 years should not be
viewed as making an expert but what it
does do is it it says that I've seen a
lot of battles and experienced a lot of
different crazy things so you know this
is going to be say stories along the
lines of things I've seen things that
have happened and you know what we've
done to you know kind of react to them
and so on there's my contact info if you
want to shoot me a follow or an email or
something like that you know visit booth
5112 no marketing types there it's just
developers in fact i have booth duty
immediately following this talk so if
you have any questions or you want to
berate me or anything like that you'll
feel free to swing on by i'll be there
for four i guess about three hours or so
so i won't be able to escape all right
so what are you going to get from this
talk and the first thing i want to say i
want to start it right away and say look
this is not a tuning talk okay there are
a lot of tuning talks out there already
there's lots of blog posts log entries
use this command line option use that
one use the following you know verbose
log mechanisms or tooling in order to
debug your GC pause times those are all
great and important and useful this is
not that okay instead what I wanted to
do is kind of take you through a bunch
of different war stories not really
stories but more experiments and show
you how some of the crazy behavior that
can actually happen you know based on GC
technology that you're using the
platform that you're on and really going
to give you a bit of an explanation on
that the whole point being that I'd like
to build on your deep knowledge toolkit
so excuse me so you know expectation
being that this is not the thing that
you use as low hanging fruit it's not a
matter of Oh double your nursery size
and you'll get better performance but
more a matter of if you're sitting there
writing code you're sitting there
designing something or you're debugging
a problem
you know what are some of us say the
deeper bits of knowledge and they're
just really useful to have baked into
you ultimately the goal is really to
just put ideas into your head concepts
things for you to think about and things
for you to look up and read about later
so exercise left to the reader or viewer
in this particular case that hopefully
you'll pick something up and be able to
say jeez I want to learn a little bit
more about that I heard about that I
know about this and when you see
problems they might be related to this
ultimately we're kind of trapped in the
same room for an hour so hopefully I can
be entertaining right and a half of your
eating lunch so you probably going to
pass out halfway through from like the
other the the lunch coma that's fine too
don't worry about it I just got told i'm
an optimist since i mean i think that
fifty percent of the people will
actually stay awake or fifty percent of
the people fall asleep like which way do
you think I'm being an optimist anyways
um sorry I'm fighting a bit of a cold
here let's just get up to speed I'm
going to spend don't worry I'm going to
spend ten minutes on a bit of remedial
this is really about level setting
terminology when I say words what I mean
if you get an understanding of okay look
he said this I now know what he means
some of its fairly basic don't worry
about it this is not GC 101 we'll get
through this fairly quickly so first of
all when I say compaction simple concept
right you got a heap you got some
threads allocating into the heap you've
got lots of objects all over the place
and of course as usual you have
something like free memory being
reported to you as some value but we all
know that free memory is not really big
one big contiguous chunk it's actually a
bunch of little chunks all spread out
throughout the heap so when you try to
allocate something that say consumes a
lot of the free memory occasionally what
happens is that it cannot actually fit
like we have in this particular case
that object from that thread is not
going to fit in the heat so what do we
have to do right we were on a GC and we
also run a compact this compact will
pick up objects on the heap and slide
them down typically there's other
technologies to do this as well copying
between semi spaces and so on but in
this particular case the simple concept
or understanding is that we're going to
pick objects up and slide them down to
create one nice contiguous chunk so that
now
we can happily allocate that object into
the nice you know big area all right so
that's what I mean when I say compaction
doesn't mean necessarily this particular
technical approach but it means an
approach that accomplishes this all
right generational collection what do I
mean by this conceptually what happens
is when I say generational conceptually
we take the heap and we split it into
two pieces okay on one at one part we
have something called new you'll hear it
referred to as nursery space new space
allocate space sometimes even referred
to by some technologies that collect it
and I'll get into that in a moment like
scavenging but we have the new side and
on the others on the other half ish
sometimes three quarters sometimes one
quarter depending on how you configure
it we have something called old space
tenured space and so on there's a
different you know names for it but
again old is typically what's used all
right we have threads and they will
typically allocate always into the new
area so nursery allocate space and so on
eventually these threads allocate the
space fill it up and there's no more
room to put objects in here all right
what happens we run what's known as a
local collect otherwise known as a new
collect nursery collect and so on it
will focus strictly on the new area now
the whole point of this is best bang for
buck right we're going to concentrate on
a smaller area of the heap hopefully
effectuate a faster garbage collect and
cut down on the amount of pauses that
you have ultimately most objects die
young so again this is the best return
on investment biggest bang for buck this
is what we mean this is part of
generational collection technology now
this it typically in a lot of cases will
be a compacting collector as well a lot
of the different vendors will do this
Oracle does it IBM does it you know a
lot of all of the major VMs will do this
this will compact the area down to just
the live objects that were found in
there and then we proceed on our merry
way threads are allowed to allocate yet
again all right eventually it fills up
we do another one of these local
collects but eventually we don't want to
keep objects in this new area all the
time right there's only a limited amount
of space and we've got this whole old
area that we could move things to so a
lot like your crusty old professors in
university
you couldn't get rid of because they had
been there for a while they had they had
received tenure and so what we do is
with objects that have survived a
collection long enough or many different
collections in the new area we
eventually say it's a waste of time to
collect you we're going to kick you out
and put you into old space and tenure
you all right so generational clinic
collection will typically involve a lot
of ten uring moving to an old area and
those objects are only ever gotten when
we do a global collect okay so
conceptually from a generational point
of view you have two types of collects
local which is on new objects and global
which is on everything there's kind of
no in between all right when I say stop
the world what do I mean stop the world
is you have Java execution Java threads
running they eventually have to do a GC
operation of some point that means that
all the threads that are Java threads
will stop execution completely and
totally right they will stop this is
either cooperatively in the sense that
we ask them to stop and they do politely
or we use some operating system physical
facility to say no you're done stop
either way they stop GC activity
proceeds when GC activity is done they
are then allowed to resume okay so when
I say stop the world this is exactly
what I mean concurrent sometimes people
get concurrent in parallel mixed up and
I've got think two more charts on
remedial so we're almost through it
concurrency is when GC operations can
occur at the same time as jab operations
now this can occur in not instead of one
of two ways but up to two different ways
one is to have dedicated threads like I
do at the bottom running GC activity the
same time that your Java threads are
actually running another possibility is
that Java threads actually will stop
themselves and run a bit of GC activity
to help okay then resume job activity
this isn't all necessarily preset at the
same time you can have different threads
participating you can have some threads
that do not participate at all this is
typically triggered through something
like allocation have hung gris threads
for the heap will go you know go after
more GC work to be punished right you
consume heap you must try to return it
however it doesn't preclude dedicated
threads or threads that just don't do
anything on GC activity
this in part goes along with parallel
when we say parallel GC we mean there
are more than one thread that can act
run GC level operations at the same time
okay that's either through concurrent
mechanism or just through multiple
threads during stop the world but when
you hear somebody say parallel GC they
don't mean parallel with the java
application they mean parallel amongst
themselves IE more than one thread all
right now I did say I was done with
remedial i do want to cover allocation
because this is actually an important
point to understand before we get moving
with the talk excuse me so you know
you're designing system and you know
allocation is really just you know the
most basic resource contention problem
that you have in the JVM right
everything else is scalability and so on
you've got lots of different there's a
resource we don't want to introduce a
global lock and so on and really it all
starts at allocations so if the JVM
doesn't solve this for you you can't go
and solve all your other problems
because quite frankly just wouldn't
scale so you have the standard problem
of allocation right all these different
threads try and allocate off the heap
now you know typically and it depends
the measurements that you kind of look
at or find some of these measurements
are actually say that the average bytes
object size is between 50 and 70 bytes
so you can imagine how unbelievably non
scalable this is because we've got you
know a situation where it's not really
easy to find free memory here it's kind
of scattered all over the place you
might have a list or some mechanism but
ultimately you have to introduce some
sort of lock right to say look you know
one of the time please because this is a
really complicated resource to manage
right we're just talking about memory
and allocation here you can imagine your
scalability is basically zero right and
in fact it's funny back in the old days
we're all proud of one of our older
implementations and then we tried it on
a multi-core machine and you know
normally you expect scalability from
your perspective in a graph to be like
this I can tell you that the scalability
dropped below once it hit you know more
than one core and more than one thread
so yeah
so you can get it wrong right even the
experts can get it wrong um so what do
you do right it's really silly to be
going for every seven bites through this
lock for all these threads it's just
ridiculous so instead what we do is we
say well if i acquire that walk one i
just go grab a whole swath of memory for
mice for me to allocate oh right now
this private buffer that you can go out
after is called the number of different
things all right different vendors will
call it things like a threadlocal
allocation cash threadlocal buffer
threadlocal heap which is a bit of a
incorrect statement that's what the out
of the IBM term and in fact I'm going to
tell you it's slightly wrong the point
is is that I can go out I can get some
memory and say look I'm going to be the
one who's only allowed to allocate out
of this so I go in through the
allocation I say nope I'm going to grab
this buffer for me I'm going to grab a
whole chunk of memory I need 50 bytes
I'll get 4k all at once and bring it
back that doesn't mean other threads
can't see into it if I allocate an
object I can allocate an object and
stick it into a static and say have at
it everybody can see it no problem it
just means that no other thread can
allocate out of that chunk ok so you
know we go out we grab this buffer this
is now assigned directly to the thread
when it goes to allocate don't bother
going through the lock I've already
allocated that memory I almost hit the
wrong key there and then we can actually
populate ourselves lock free ok nice and
scalable alright
that's all there is to GC folks I'll go
out and implement your own now I I I
used to get chastised by my by my
superiors way back when because I used
to just say look do you see you know
garbage cause it's just pointer chasing
right all you do is follow pointers
what's the big deal right and people
said look you have to actually make it
sound a bit more complicated because
quite frankly people think you don't do
anything here I were I learned my lesson
on that one all right going to talk
about finalization a little bit here let
me remind you so we're in first of all
don't use finalization okay just on
however sometimes you're forced to okay
either through design decisions that you
or somebody else has made you're using
some sort of an application or rather
framework that's going to make use of it
you have to deal with it and in fact the
java class libraries themselves going to
force you into doing this when you have
things like file handles and so on right
they have a finalizer on them so you're
kind of stuck no matter what you're
dealing with these types of things I
want to use finalization here to kind of
demonstrate a bunch of problems when the
JVM don't be alarmed about it being
finalization as much as it being okay
we're demonstrating potential things or
problems that you may not be aware of in
the JVM platform as a whole that maybe
you should they don't happen all the
time but again deep knowledge toolkit
right things to kind of build on things
to learn about and what have you and
it's of course a bit of a word joke here
okay let's take a code example can you
see that at the back is that okay like I
mean yeah thumbs up thumbs down I'm not
shirking to yeah good all right awesome
I was really worried about that some of
the rooms have really small screens
pardon me what yes can I raise it is
that better yeah okay good sorry I tend
to turn away from the mic didn't give me
a clip either I was actually told not to
roam from the podium which is a bit of a
mistake if you seen you speak before
I've knocked over a podium once I've
spilled water on a laptop once did not
get like it i just learned to hang on
for dear life really right so let's I
apologize if I'm getting a bit loud just
somebody tell me to quiet down let's
take this really simple piece of code
now I've deleted a lot of it but really
conceptually this is going to illustrate
the problem that I'm trying to you know
show you so you've got a really simple
inner class finalized by object it'll
print finalized are called and then
you've got a main method here that we're
going to instantiate that object but
then we're not going to do anything with
it ok so we assign it to a local
variable we instantiate it assigned to a
local variable then we ignore it ok we
do a bunch of other work create some
garbage this that and the other and so
on but we don't do anything to this
object then we eventually call system GC
run a GC and run finalization this will
run finalize errs on any object that is
eligible finalization so not all
finalized is just the ones that are
actually garbage and ready to go ok so
quick recap ok got a simple finalizar
here we'll get a print off we got we're
going to instantiate something that
we're instantiate this object and we're
not gonna have any references to it so I
have a question will this object habits
finalizar run who thinks yes ok who
thinks no that would be everybody
excellent can't say maybe although
actually maybe way me a multi-threaded
environment of garbage and it is
possible but ok so so we got absolutely
nobody that thinks that this should be
finalized let me ask you a different
question should that object be garbage
collected if it didn't have a finalizer
if it was would you notice so so then
why can't sorry so it's not being
referred to know users of object beyond
this point sorry
it's okay i don't i don't read comments
either don't worry hey right oh wait i
was supposed to not say things i didn't
want my boss to hear right that's what i
was told by the okay let's run an
experiment and find out now i'm going to
run an experiment which actually has far
more complicated code than this runs in
a loop and it really tries to force
things badly you got to play a ton of
games of twisters but conceptually this
is what i'm going to do so you're just
going to have to trust me on this one
you has trust me right alright so let's
run got ahead of myself here excellent
okay let's run an example so we'll
basically run that code and this is on
the IBM JVM so who so we'll let this go
for a bit here we'll see what happens
hmm turns out everybody might be right i
behold Oh what happened there well hang
on maybe the IBM JVM has a bug all right
let's run let's run let's run the Oracle
JVM hmm does that talk about I roughly
about five seconds or so right 5 10
seconds so we'll give them 20 or so
now you're gonna have to trust me folks
and I can't release the code yet I this
was kind of last minute code it's IBM
those lawyers I'll eventually be able to
release it's not a problem I'm not
really trying to hide anything but that
was very interesting okay they both ran
and the whole point here is that that
object the question of could you of
garbage collected that object and nobody
would have noticed the answer is yes you
could right nobody was referring to it
it doesn't matter that in your source it
looked like it was still being referred
to it was eligible for garbage
collection because it wasn't being
referred to anymore so the same thing
happens with a finalized pelagic keeping
a local reference who cares right it's
garbage you can't talk to it anymore I
can run the finalized I can garbage
collected which means I can run the
finalizer all right scary stuff who
thinks they have a bug in their code
right now let me show you an example
because trust me I see these i see i get
this reported to me at least once a year
let's take something simple like this on
the left we have a fine eliza ballabh
ject and I've omitted a bunch of code
you can return the instance variable for
the file output stream and the finalizer
will close that thing now there's
probably a bunch of other functionality
here but whatever it exposes the file
output stream on the right we're
actually using it we're going to create
this fine Eliza ballabh ject we're
actually going to go get the stream and
there's no more uses to object beyond
this point and we're going to try to
write to that stream well what happens
when the finalizer on the Left gets run
you're doomed
that's a little mortar I get this
reported to me at least once a year
believe me and I would argue this is
actually slightly poor design you
shouldn't be exposing your internal to
the finalizer that handles cleaning up
your stuff when you're actually exposing
your internals these are the kind of
things you need to look at more
importantly you might say finalisation
ba think about weak references think
about soft references these things
happen all right the order of things
okay this one's a bit more convoluted
and I do play some tricks here so just
kind of go with me on this one we're
going to create to finalize abab Jex
okay they are on the right they will
print out their ID okay we will create
two of them one called one with number
11 with number two it prints out when
they get their finalizar run I'm going
to nil out first the number one guy okay
the do work and do some work there hints
but those things will generate a bunch
of garbage and cause a bunch of garbage
clucks to occur do work does a lot like
a thousand do some work does like one or
two okay so we have create two objects
finalize Bob Jex nil the first one out
create a bunch more garbage and garbage
collects Neil the second one out do a
truck load more work what should be
printed out we should see you know
possibly one possibly two possibly both
possibly neither all right hey probably
I want to hear the guesses but you
probably have some guesses most people
would say look I should see one and two
in some order right it doesn't really
matter so let's run that and find out
now I'm just going to stick to the IBM
sorry that's the Oracle side
right anymore ah yes the border so most
features to say we should see one end to
write in that code fairly obvious
especially since you kneeled out the
first one first why you unleashed all
references and not playing tricks you
are literally that is the end of it
there's no more references to that
object it's eligible to be garbage
collected and finalized so if we run
that but we only get to now what
happened all right so who thought that
that's what they would get consistently
I can run this like a hundred times we
will get to every single time who
thought we would just get to no one all
right see now and then there we go okay
let's go back to generational collection
for a moment okay first thing happens we
allocate fine Eliza ballabh ject one
ends it ends up in the new space then
like I said we allocate a truckload of
objects to cause a whole bunch of GCS
alright so we GC and GC and GC like mad
and eventually that object because it's
being hung on to is going to get tenured
so it gets moved to the old space now
this is finalized about object one then
we allocate the second one we do we
allocate some objects around at some
garbage cause a few more GCS but then we
know it out that one can be finalized
why because we've actually run a nursery
collect local collect scavenge this is
your generational collector folks that
you're running knowing and loving and
it's only going to be able to talk about
finalized about object to not one right
because the collections are actually
local I can't tell you I cannot tell you
how many times I have people come to me
confused why they've nailed two
different things out and only one of the
resources has been cleared up right you
are being caught by the wonders of
generational collection okay now this
isn't a knock against generational GC I
mean you know we internally at IBM my
team was in fact the ones who introduced
it to our JVM was a generational
collection and it took a lot of time to
buy in and nowadays I mean you just
don't run without
a generational collection are mostly but
these are the kind of misbehavior or
moments of misunderstanding that you can
get again think about referenced objects
again think about memory resources and
went to collect runs and you not freeing
up the amount of memory that you think
you should or you're not getting the
finalizer or something and queued on
your reference cue that it should have
been that should have happened you can
possibly have moments like this just
because you've got rid of the references
in a certain order doesn't mean they're
actually collected in that way all right
last one with finalization before I go
on and makes make fun of some people
other than me all right here's a tricky
one out now if I was being evil before
this is this is really an evil example
because I'm actually misleading you
completely here if I have a thread and
it refers to two objects now i'll call
these object anchor points okay they're
regular old objects you know they
composed of a number of different things
hierarchy of so on but they're just
regular objects in the finalisation in
here or anything however they refer to
finalize abab Jex okay again we're going
to have one in two in here so you know
from a code point of view this is what
it looks like fairly simple we create
these anchor objects we're going to
allocate to finalize abab Jex one and
two we're going to nil them both out
okay no gc's in between we're just going
to nil and we're going to do some work
to cause GCS and then try to run
finalization okay what should we
actually see the finalizer for one and
two i would say looking at the surface
this code you yeah you probably see one
and two right so I'll run the example
you can probably guess what's going to
happen here not one into this is the
second hardest thing to do folks
actually type in front of a live studio
audience it's like just nerve-racking to
now I'm pretty much trust me on this one
you go back to that code I am running
that code okay like you know there's a
few syntactic things and so on but I'm
running that code I'm not playing tricks
on you and I think the one thing that
you didn't see though is what these
anchor objects consisted of right so
looking at the source you
expect one type of behavior and of
course you don't really see what's
happening under the surface and this is
one of the misleading aspects of using
frameworks and so on what is really
going to happen to you right what are
you getting out of this and what I
misled you through this picture because
this isn't actually the size of the
objects being allocated in fact it was
more like this from an anchor point of
view ok so the first anchor object new
to allocate some big giant wad to
basically hold on to the final object
the second one new to allocate a little
smaller ok static variable just a little
counter and so on what does that mean
how did that affect what we had here
well I think this diagram speaks volumes
right do you think you can fit the big
one into the new area you cannot it will
go right can you i allocate this no
chance right but i can an old space so
in some cases you'll get objects went
directly to old space in particular this
anchor object which is holding on to
your final I zabal well guess what when
you do a local collect right a nursery
collect you're not going to be able to
tackle that giant object the smaller one
is going to go in there and you'll be
fine right and that's why we get that
finalizar but this one's just going to
be delayed we see this again all the
time all right objects that just you
know maybe made it to 10 your space how
the structure of your heap is what
technology that you picked this has an
effect on the behavior of your
application these things happen like I'm
not making you know I'm not making this
up you know I'm happy again i'll be
releasing the source you know probably
within a week or so once i get clearance
but this is very simple code i'm not
even a java programmer really i just do
run times right it's like being the
stunt man and not the actual actor but
the point here is that these things
happen right and people don't understand
but i but i know that the same way I
allocated it in this order how come my
collection didn't get it you have
moments like these think if it did fit
into the nursery or new space but you
ran us a local collect and it didn't
generate enough free space so it could
have physically fit in if there was
enough but there wasn't might end up
going to the old space as well these
things can happen
alright I'm sorry somebody was coughing
I just missed that i miss the question
part yeah well you're a bullet so the
question is can you tell the size of new
and old yes through either you setting
it from the command line okay or verbose
GC and the third one would be verbose
colon sizes will actually tell you your
default settings which may be variable
there's be some jmx beans can also
handle this as well for you there's
other other ways to do it but most
certainly well certainly all right
enough with finalization all right
what's this gave it away okay gave it
away in the previous slide right what's
this it's an iceberg all right icebergs
have a number of different interesting
properties based on the picture I'm
showing you what is one of the
interesting properties right you cannot
see everything that's actually happening
right on the iceberg what you see from
an iceberg is just a little tiny bit
it's usually a very pretty bit but it's
not the whole story right there's a lot
more happening underneath and underneath
the covers so this happens a lot this is
the kind of out of memory problem that
we see fairly no it's fairly common and
I'm sure most people in this room have
hit this at least once and I just wanted
to kind of quickly go over it this is
not your out of memory problem like a
full heap okay we know and love this
problem right we recognize it we're just
out of memory you try to cram another
object in there GC runs there's no room
and you know you throw an exception and
this is the IBM sort of you know result
that you get and some proposed you see
along with some Diagnostics here it's
fairly chatty but I will say that you
know that there is a lot of information
here again Oracle has the same one this
isn't uh you know who's who's verbose GC
is you know more more bountiful but you
know it's pretty obvious when this type
of error occurs so I'm not going to
really drill down into this right you
have a GC end you know type event you
talk the 0 free memory 0% you've got no
more room and then of course at the end
you thrown out of memory
right that if you didn't catch and do
something right we know this problem we
know and love this one right not what
we're here for instead there are
different kinds of out of memory
problems that you can get all right and
too many people are too quick to
conclude that it's one type without
actually looking at the information or
what they're doing in their system so
let's take this heat for example plenty
of times you will see some objects
actually holding some native level bit
of memory outside of the heat right a
bitmap some data store some database
connection that you're marshalling data
through and so on so this native memory
is real of course in consuming on you
know resource memory resource on your
machine so that's not so bad but you
know if you keep adding up on these
things and I've drawn this kind of huge
and lumpy for a reason it's we're
filling up the screen and eventually
you're going to run out of native memory
before you actually run out of heap
memory okay and you'll get an out of
memory error now you know I'm kind of in
the mood for jolly rancher now we can
thought big green thing got a hankerin
but yeah so you know this one is a bit
less obvious right because you'll get
the out of memory and you'll see a lot
of people say oh geez you know I got to
expand my heat / stuff like that which
is actually the wrong thing to do right
because you're already out of memory and
you can't expand it any further so you
really have to watch for these things
right you know again gcn but you've
actually got free memory and not just a
little bit like eleven percent and you
still throw a note of memory error and
these are the kinds of things you have
to watch out for now this is native
memory I'm talking about but every other
resource and your machine is the same
way right sockets file handles thread
handles and so on okay so I had you know
you can run out of these things some
operating systems are really good about
it they just whack your process and say
sorry too many threads goodbye and
you've got a nice little message and you
know you can actually debug it so this
type of out of memory and this result is
actually really good and nice to see but
sometimes and I've been doing a lot of
exam I've done three examples so far on
the screen sometimes you end up with
some examples that maybe don't work out
so well and this was my example that I
wrote
some operating systems and JV hams don't
really handle certain out of resource
problems as well as others and of course
this is nerd culture right so you can't
have this you know on your machine
somewhere without you know a friend
either online or otherwise you know
making some some helpful hint or or
comment thanks Anton is that tone here I
don't think so consume Anton I are
friends so although i'm using him here
against without his permission I'm sure
he's fine with it he's a good guy follow
him ok so boring text only chart the
whole point here is that you know don't
jump to conclusions out of memory
stories and be aware like you know when
you're looking at your system it isn't
just about how much memory and the
frequency of your GCS you know you want
to when you're doing dumps find out it's
like oh look i'm using this many image
objects i'm using this many file objects
and there's many threads and so on and
you got to keep tabs on these things
it's hugely important there's plenty of
throttling facilities from my operating
system point of view that exists make
use of them make sure you know I mean if
you're running simpler you know agent
type stuff that's fine but you will see
the out of memory story when you're not
actually out of heat memory a fair
amount all right 25 minutes ago
sharing memory all right now let's take
a completely different problem set and
let's just how some on this this this
type of stuff is really kind of nerdy if
you thought it was in Europe I just said
that you probably thinking the other
stuff is pretty nerdy Ryan what are you
talking about this is even worse okay
but this is another one that really pops
up and this is not necessarily behavior
as much as it's actually performance
related and it really confuses people a
lot so I'm going to gloss over yeah I
apologize I know the codes at the bottom
for the back I really apologize folks
but there's not a lot there and I'll
explain it the believe it or not from a
performance point of view we actually
see these problems pop up and they are
the kind of things that I'm going to
gloss over the cause and why and what's
happening to you just remember that if
you're like oh jeez this sounds like you
know situations that we have in the
large there's further reading to be had
so really kind of take this home and go
yeah okay there's some other stuff I
maybe should learn about or read up on a
little bit this isn't the first thing
that you necessarily go after but it is
something that actually happens and if
you have a big enough code base this is
probably happening at least once if you
work twice and causing you pain that you
think may have been cured by some sort
of global lock story which is killing
your scalability of noise sorry all
right two threads one field so I'm going
to run an example we're going to time it
okay we're going to run it like a
billion times in a loop and I got two
threads and they're gonna access the
exact same field now that feel does not
volatile or anything like that it's just
a field and I'm going to basically you
do the code that you see down below i'm
going to fetch the field I out plus one
and stick it back into the object okay
no atomic operations know anything like
that just a regular old I think it's an
infield okay actually it's a long i'm on
a 64-bit machine and so long either way
so i have one example i can run the same
example with two different command with
two different options one is same which
accesses the same field and one which is
different which accesses the same field
on a completely different object now
these objects have a ton of fields I can
tell you they have like something like
128 or something like that a huge amount
now who thinks does anybody think that
the time taken for these loops to run
through is going to be different for
both versions okay few people are
non-responsive or everyone's asleep so
you get that it's going to be we're
gonna run two different versions of this
bench one that uses two threads on the
same field of the same object two
threads on the same field of two
different objects all right all right
window get you back to the top okay so
we'll run the same this is really daring
of me to write because I'm using you
know timing based benchmarks and so on
you'll see a little bit of extra time at
first the JIT warming itself up and so
on getting going but okay we've got
sometimes hear about these are in
milliseconds and there's two threads and
they each print out how long it took
them to do the iteration something like
a million times I think way more like 10
million anyways about 2.3 seconds or so
for using the same object same field now
same field but on a different object
even not yes thanks dose faster right I
don't think anybody's going to disagree
with me there over 2x faster so why is
that what's going on with our system why
do we have that so again I'm going to
gloss over this this is not a deep dive
into this but it's really just to give
you a sense of what's happening and why
having the same field being accessed by
multiple threads all the time is not
such a good idea you might say well
throw a lock up around it but that just
makes it worse really in some ways but
the real problem here is that you know
this is true for cores and CPUs so take
this X point i'm gonna use cpu is just
make the picture nice and big but this
applies to different cores and same cpu
as well so when you have a cpu and it's
accessing some memory okay memory is
actually really far away and really slow
to access so when you actually do
something like you know field 1 equals
field 1 plus field to you know the CPUs
happy to do that right to the field
except that it says well why would I do
that memory is really far away and its
really slow to access and I could make
this a lot easier and faster to access
if I threw a cache on the front right if
I got some memory a smaller amount than
actual main memory that's nice and close
to the CPU I can do this faster I'll
just read
value in and I'll just go at it and make
all the pluses I want and everything
will be perfect so what it does is it
says fine I'll just take in my cash i
will read that location since you're
accessing it and I will do all the work
there this is great it actually makes
things faster okay and this is like a
first time access there's no like warmup
period it just accesses the field goes
in I'm gonna bring this into my cash and
it just uses it nice and fast alright
perfect until you introduce another cpu
accessing the same field right and it
has a cash too and it gets the bright
idea hey look i should read this value
into my cash and actually use it there
too and it'll be really fast
unfortunately when you do something like
that both cpus or cores will look at it
and go hey we're using the same thing so
we should talk right this is really good
right this is like know a couple having
a really good communicative relationship
right this is there everything you want
a marriage right because they're on the
same motherboard they ain't leaving each
other right this is you know it's till
the end of time right so you should talk
although maybe not but anyways point
here being is that you've destroyed the
performance of your cash right all the
performance that the cash is trying to
gain you is just shot because now these
two things have to talk every single
time potentially depending on your
architecture and this is why when we
write to the same field of the same
object from two different threads no
blocking know anything it's actually
slower than if I just pick some other
field somewhere else and have the same
thread going okay because they're not
busy communicating they go off that's a
piece of memory I don't care about I
don't have cached why would I bother
right big enough code has this all the
time you are kissing performance goodbye
by doing stuff like this alright so when
you're designing stuff right now really
have to think about it is this object
going to be accessed by a lot of threads
frequently is it going to be written to
reading is not so much a problem because
you're not modifying the values because
we're writing back to it right now they
gotta talk changed changed changed all
right let's do something different then
all be smart I'll write my code so that
one thread operates on one field of this
object and one other the other thread
operates on a different field of the
object okay so same idea slightly
different not the same field anymore but
the field field two right thread 2 will
operate on field two thread one will
operate on field one I'll get that right
anyways picture right there very close
but two different fields so we should be
okay right won't be cash in the same
values and of course I'll run two
versions same and different and let's
see what the performance is you can
probably guess it's not the same since
I'm bothering to do this again same
object to different fields
just some jet warm-up don't worry about
that I know it's this one so about the
same as we had before for the same field
that's a little frightening so what if
we do that it's as if it was you know a
different object right we go back this
isn't a matter of this looks the same
like if you if you weren't if you didn't
realize it actually had these these
batch files different the first one was
false sharing same field i think it was
called you would think i'm running the
exact same code because it's the exact
same times practically that are
happening so what's going on right
didn't I just give you an explanation
that said that this would not happen
well I sort of did because I didn't tell
you the whole story right see if you use
smart says hey look if you're going to
touch a field here I bet you're going to
touch fields around here to pretty soon
right so these these things called cache
lines right and for a bunch of different
reasons efficiency and so on when the
cpu says i'm going to cash this
particular field that catches caches the
surrounding area they're on very hard
boundaries okay so this is a sliding
window it's actually 64 byte chunks on
sorry 64 bytes on my cpu here that I'm
running here but what that means though
is that by cashing that value there from
this thread I'm also cashing the value
next to it okay and CPUs talk in terms
of cash loins right I've done something
to this cache line so when you have
another cpu referring to the other field
you have the same problem as we had
before there has to be communication
even though they're not touching the
same field because they're on the same
cache line the way caching works it's
basically the same problem right so the
whole bright idea oh that's no problem
one thread on one field montreux and the
other new problem right well problem
okay and again stuff like this happens
and I'm going to even build on this
problem even further than you'll see
this in a moment and this is like really
wacky you're going to get really angry
because you know your first reaction is
okay well fine let's just make sure the
fields are pretty far apart right in the
source allow me to blow that one up for
you
true statement you ever get a pizza
without the crust okay crazy amount of
code here okay and I've deleted a lot to
fit on the screen and make it readable
two classes shared and shared sub class
which is which is a subclass of shared
there's a whole bunch of padding fields
there's like a billion padding fields
making it way larger than a cache line
size of 64 bytes believe me all these
padding fields are Long's okay and i
have three fields of importance here uh
wrong version of the slides right there
supposed to be another red arrow here
field three okay so there are three
separate fields in this text right now
like I've said the amount of padding is
way larger than 64 bytes okay so should
be on different cache lines and their
ins and I'm basically going to run okay
good I'm going to run the same code idea
and measure the timings okay so I'm
going to access one thread is going to
access field one and one field is one
thread is going to access field to these
should be extremely far apart we should
have no problems accessing that okay and
then I'm going to run another the same
code but tell it to access the third
field again very far apart lots of
padding and so on sorry not drawn to
scale but you get the idea the point
here is that if you go back between
these two and i talked about cache lines
and so on you would expect the time more
or less for accessing the set the access
in the second field to be the same as
accessing the third field okay and for
anybody who thinks there's paging
trickery going on here I've ensured
they're on the same page if you don't
know what I'm talking about don't worry
about it but that is not the thing
that's going to happen here sorry okay
so accessing set the second field access
and the third should be the same time
but of course here comes the demo to
show you it's not and the the title of
my batch file gives it away to
all right 1.1 1.2 seconds that's for the
second field let's take a look at
accessing the third field exact same
code that a third field access was
significantly faster what's happening
right not can't be true Ryan I mean you
just told me how things worked
everything are you lying to me why did
that just happen okay let's go back to
this code here's what it conceptually
looks like you know from a memory layout
point of view right field 1 32 bits it's
an int bunch of padding the pad fields
field 2 so on and so on right all the
way down to field three alright when the
vm loads these classes its spots
something that it can do it says hey
look i can take field to pack it into
the slot for we're next to field one now
you're saying wait you can't throw an
extra field into us the superclass
that's not possible well sure it is as
long as i don't let you access it right
I can totally use that storage pm's
being helpful right look at that we're
not extra snow now I created complete
pouting I can get rid of that Hey look
everything's nice and dense and on the
same cache line less gc's better
performance hooray but what you typed
isn't what you got right and surprise
back to cache lines sweet right so don't
do it right don't do it and and just to
kind of blow your mind a bit more I'd
have shown you this is what happens
there are other ways now you'll get the
same version every time from different
vendor but the point is is that there's
different ways to organize things
another way it might have done it was it
might have said look I loaded shared
first right and I'm smart enough to
actually stick field one at the end when
you had it listed at the beginning
you're not getting what you want me just
because you typed it in as long as we
simulate the hot behavior folks that's
all that matters right so even if you're
trying to solve this problem of let's
rearranging the fields and make them far
apart and so on you can't win we'll find
you it will catch you and get you I
promise don't do it right don't do it
yeah all right at 10 minutes I got one
last example this one's pretty
convoluted but I just had to show it
because it's cool and the fact that you
can do it from Java a lot of you won't
we experienced this problem necessarily
but this is one of those things that
look it's kind of cool and you know go
learn about it because you know it's
something just interesting to read and
learn and understand your architecture
and the fact that you know picking up
your code and moving it from one you
know architecture to another not even
necessarily you know say x86 to powerpc
but even just upgrading your cpu's just
just changes things right completely and
moving up can sometimes make things
worse right like little cache line
things like oh well increase my cache
line size select know maybe that's not a
good thing because your code was relying
on it being smaller right so here's my
crazy example just to kind of show you
something so small data big problems
right let's take a gigantic byte array
like just some gargantuan thing okay
colossal now a reminder on my machine I
know my machine it's 64 byte cache line
size and I'm going to talk about just a
bunch of cache lines within it not every
cache line within the byte array just in
just very specific ones in fact I'm
going to take and I'm going to populate
these in a moment but i'm going to take
ones that are every 4k apart so I'm
gonna take a cache line size 64 byte
chunk on my byte array at the beginning
skip 4k talk about another 64 byte chunk
skip 4k and so on all the way down the
line and my humongous byte array and
what I'm going to do is the same example
I've been doing a lot which is I'm going
to read this is how the loop is going to
work I'm going to take the first slot of
the first cache line or first when I say
cache line using 64 byte jump because I
can't necessarily guarantee that the
object is allocated on the right cache
line but the first bite of the first
64-bit chunk I'm going to read it out
plus one and put it back in single
thread this whole test two single
threaded okay so read the first slot of
the first 64 by chunk out plus one put
it back move to the next chunk do the
same thing on the first index move to
the next chunk do the same thing on that
index and so on go back to the beginning
and do the second index down the line
and so on until I've done if you do some
math bites all 64 bytes rinse and repeat
go back 0 0 0 0 1 1 1 sorry all the way
around for you folks but basically same
idea okay so there's the recap I'm going
to do my example is going to run up to
10 different 64 byte chunks okay so 640
x 640 bites should be enough for anybody
right yeah get into yeah I was terrible
i know sorry that's i've been saving it
for 51 minutes when you're laughing are
you laughing at me or with me yes be
optimistic right it's how I've made it
this far right so really simple right
we're gonna go down so I'm going to run
a number of iterations we do like we
just keep skipping through it left and
right I'm going to show you the examples
actually going to print out you know
what it did with just one 64 byte chunk
that want to do with 264 bite chunks and
three and so on and timings will print
the time out and I'll print the Delta as
well so you'll just keep seeing it just
runs in a loop continuously somebody
lefting a punchline come on whatever way
i will let it warm up for a bit the
first run is kind of like just ignore it
a yeah the deltas again it's not precise
mouth is very simple Java code so
that was a good one but I let it run for
one or two more just to show you it's
pretty consistent that iteration 3 was
perfect man would stop it there that's a
good one all right okay if you look at
this again two weeks but let's look at
iteration for right you can see the
number of entries being the number of 64
byte chunks we're talking about 1
through 10 the time it took for each one
that's kind of interesting but what you
really care about is a die as I add one
more 64 byte entry how much more time
does it take me to run through this loop
you would expect that to be pretty
consistent right add more more thing it
should be +1 in terms of the unit of
time if you look at iteration for you
know clock jitter so on and this not
it's pretty close actually you know you
end up with about you know 70 80 kind of
you know milliseconds these are actually
in milliseconds expressed but then
consistently and you can look at
previously in iteration 3 I didn't stop
the duration 5 but you probably saw it
in iteration 2 as well iterate entries
nine in ten so if we have nine and ten
64 byte chunks the Delta exploded
relatively speaking of course so you
know we're getting up to 640 by switch
should be enough for any about everybody
and it's not okay and I'm not playing
any tricks here this is a single
threaded there's nothing else going on
I'm just accessing literally 640 bites
and somehow this exploded why all right
this is the last one folks and we're
just going to wind it down all right
cash capacity is actually less than you
think right is the whole point you the
CPU actually has a cash in each Korres
cash I know my cache of per core and
system running one thread I only care
about one core is 32 k for the l1 so
that's way more than 640 bites right so
how come all of a sudden my performance
jumps and it has to do with the cash so
the cash is actually divided up into a
number of sets okay and I haven't drawn
this necessarily to scale but it's
divided up into a whole truckload of
sets you might have heard something
called set associativity this is what
we're talking about here now what
actually happens is that cache lines
when you actually read it into a CPUs
cash go into a particular set every
single time that cache line will go into
the same set okay there's a bunch of
computer math based on a dress and so on
it will always go to the same one not a
different one
the same one in that set there are a
number of entries and so it'll find the
appropriate one through some simple you
know least recently used type algorithm
so when I read this one in it will go
into its appropriate cash set when I
read the next one in it goes into that
appropriate set based on the times how
many entries do you think are in that
set before it overflows eight okay after
eight your performance jumps because we
have to be evict one and then when we go
back to read it again through the loop
we've got to put it back in and evict
something else and you've written it so
it's even worse right it's not just a
look up and read since we're writing to
it we got to write it back to main
memory all right this is always a really
cool example for me because you can
actually do this in Java you might think
when on earth does this happen right and
crazy like nobody i'm not gonna how can
I even tell it's like the GC it's moving
objects around I don't even what what so
you're right okay you're right where we
end up seeing things like this now again
deep knowledge toolkit right go read
about this but we do see this when
people actually start laying their
objects out end-to-end and arrays and
start accessing that way where they'll
actually loop over the arrays like if
they're actually doing a loop if you're
just doing linear processing look that's
going to happen you're going to fix
stuff but you don't care about it's
still being there right what we are
where we end up seeing it is where
people actually go through objects in a
list and they go okay I'm going to look
at the field the field the field the
field and loop back looking for status
or computing some value constantly and
so on and although they're separate
objects because they're being referred
to say by an array the way the garbage
collector will work will actually copy
the array and then look at each of the
objects linearly and place them into end
in memory and then you end up
experiencing this exact problem okay
this is a very hard one to detect but it
is something that's kind of interesting
just to stick in your heads right why
not sleep on it right something you can
say hey I know something about this now
if you didn't all right before I get
forget really quickly Numa who knows
what new moon is really ok this is
really quick there's no more examples
i'm going to type stuff anymore or
anything like that really quickly
because i've got about two minutes left
not all memory is the same speed from
the same cpu so if you have a cpu
memory tends to be you know relatively
close from a certain point of view so
there's certain blocks of physical
memory that are actually available to
your CPU so they're actually fairly
quick to access but when you end up with
multiple CPU systems right they have
physical memory that's close to them too
so when you start accessing other memory
in other parts of the system there has
to be a communication that occurs this
is not caching okay this is actually hey
this memory is actually closer to the
cpu than that like literally almost from
a physical point of view and in fact
from a physical point of view but it
makes for communication latency so that
if the top one needs to refer to
physical memory or stuff in the other
cpu there is a delay it is slower than
stuff closest to it further if you have
a situation like this it can get worse
and worse and worse ok this is
non-uniform memory architecture the
memory that you're looking at is not
created equal from your perspective all
right now you're saying look I run
threads I don't bet you know what do i
do I don't know where the memory is GC i
allocate stuff and so on again if you
don't know about this you should know
about deployment aspects right if you're
deploying with virtualization for
example and you've got multiple cores on
a piece of hardware you don't know if
one of your cores is ending up on one
cpu and one is ending up on the other
creating performance problems for you if
you're running on a very large machine
with multiple cpus and you're deploying
a single JVM with lots of threads some
of those threads are going to be on one
cpu some on the other if there's a lot
of data communication guess what you're
incurring a performance penalty and so
on and so on ok the JVM is modern JVMs
oracle IBM we all have support to deal
with these types of things look into it
again if you're going on big machines be
aware there's also platform utilities to
handle this type of stuff taxa task set
Numa CTL are examples and again be aware
when you're virtualizing stuff this can
actually be really painful now you know
how you virtualize it so on this can
cause you problems right if you're
accessing creating a situation where
you're having to access memory at two
different banks it's an issue from Java
you can't control it but it does affect
your performance be aware there is
reading on the subject I recommend this
right it's fairly terse read it's huge
read but it's you don't have to get it
all
to start getting it so you know all
works paper is is it's pretty much the
canonical memory paper for developers
this is there's a bunch is like five
papers that i always recommend reading
one of them is this just do it okay and
that's that hopefully I haven't you know
hopefully I kept you entertained for an
hour but you know even better hopefully
you've got a little bit more knowledge
and a few more things to think about go
yeah maybe I didn't quite get that but i
actually have now been exposed to things
that are interesting thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>