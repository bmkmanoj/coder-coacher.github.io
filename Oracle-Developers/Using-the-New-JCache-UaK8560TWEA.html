<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Using the New JCache | Coder Coacher - Coaching Coders</title><meta content="Using the New JCache - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Using the New JCache</b></h2><h5 class="post__date">2015-06-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/UaK8560TWEA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone I brought Oliver I'm
Gregg luck and we finished Jay cash yes
we did
and we're going to take a bow so we've
been presenting I think every year
I know last two years we presented it
yeah it's coming it's coming coming
that's here now it's been available
since about March so we were going to do
something a little different today
instead of just give you this slide so
we're actually going to code okay there
might be a few slides but we're hoping
hopefully enough time going to be a bit
more interactive so people can throw
questions at us if the questions are
really complicated we might have to stop
but you know give you a chance to feel
the API talk about it there's certainly
going to be people who'll go well that's
really weird why did you do that
and none of these decisions like even
though this thing's been going on for a
long time
none of the decisions about the API are
really done quickly yeah how many miles
we've traveled just to meet and hours
we've been on the phone it's one of the
surprising things is that it certainly
seems simple but some of the the
concepts are incredibly elusive and
we're surprised that the elusiveness of
them for example one thing that you'll
notice with the with Jay cash is that it
doesn't define resource constraints and
you might say to yourself well well why
is that and we had it in there for a
while
and it turns out that a resource
constraint is with respect to a specific
store and where store agnostic in the
architecture so yeah it became through a
challenge so a lot of things we sort of
had to wave our hands and say well yeah
how much is in the cash well that's up
to the implementation you sort of
configure it well get going we go
through and I guess I guess just kind of
like in terms of what we were trying to
achieve
we were try it we were trying to take
you know
maybe 95% of the api's that you'd want
to use and we looked at whether or not
we should try and standardize
configuration and so you know back to
this back to this resource issue if we
just if we defined a number of
standardized topologies like local then
process cash like distributed cash you
know with multi-tier stores that we
could then have actually defined
resource constraints but because this
this is a still a fairly rapidly
evolving area we thought that that would
be very short-sighted and it also means
that some existing caches out there that
you may be using wouldn't be open be
just wouldn't go to anything in the spec
wouldn't go to be compliant so we had to
keep it pretty simple to start with even
though it is there's a surface area is
pretty large okay so the idea is that in
your code you can use JK it'll give you
about a 95 percent coverage we have a
standardized way of reaching out to
proprietary features of it given
implementation using using and using
unwrap methods on cash manage ocation
motion tree we can show that - yeah I
will show you that but but the thing is
let's say you wanted to the whole point
about this was to try and create a
standard that you could use kind of like
JDBC if you go to change your
configuration what you can expect is
that you won't need to change the code
if you're using J cash because we have a
TCK we guarantee that all
implementations work the same what you
will need to do is change a config file
and we thought that was a pretty good
deal yeah swap out the jar change your
config file and and you're done
and then to know everyone's you yeah so
okay so we have the usual arm safe
harbor statement but this may actually
not last year and the previous year this
really was quite important because we're
finished yet but as we finish now the
spec is version 1 so I guess this
applies to the last slide while we're
doing it so I don't applies to Oracle -
right yeah ok it's good Oracle it just
means me ok so um
introduction to J cache so I said we're
gonna do a few slides get started but
let's have a quick show heads who is he
last you when we're rambling on a
vacation no I told you that I told you
no we're trying to decide this morning
whether to include sort of slide - told
you what Jay Cash's versus just diving
into the details of the code like you
know solid I told you they don't
different the audience okay well it's
sorry not let me ask a question right
your hand if you've looked at any of the
Jay cash presentations or Jake age
documentation a few more people who's
read the spec come I can't have my own
and often tiny more specs it took like
now Brian this is going to shock you who
in the audience has created their own
complete implementation of jaycation and
the interesting thing is those people
didn't read the spec so did those
implement a fear we could go on I'm
fascinated by the Airways we we as
maintenance Co spec lead to maintain the
official list of who passed who is
actually comply yeah so you have to get
past us as a gay if you have an
implementation and you want it listed on
the JCP and we will show you where it is
then contact us and we can list it up
there so
okay so intro to jaycation okay so
really we want we want it as you said
it's you know caching for the Java
platform that came through 107 we
ratified it in March basically since
about I think December last year hardly
want underware any changes we were
mainly writing spec yeah we got we got
finished we got finished in December and
then there was just there's just kind of
the process of dividing and so forth so
it actually kind of took until March yep
so basically you know the motivation is
Co its standardized the API for the Java
development really have cool caching
concepts so when we talk about a cache
entry like everyone knows what that
means when we talk about topology
everyone knows what that means because
we define those terms in the spec
it's really important that we have some
cool way of communicating these things
now one one so we had one person in the
room that had read the spec I'm sure the
for people that have implemented the
spec have also read the spec actually
there's Yanni over there Yanni raise
your hand
the only cost monopolist over here was
was the Oracle rep on the spec before
Brian so thank you we got there take you
about yes oh so we've got extensive
Javadoc however if you actually want to
beyond the Javadoc you if you really
want to understand the behavior please
go and read the spec we went out of our
way to make it readable using Australian
English right this Australian examples
work throughout there's wombats and
kangaroos a sort of stuff and like it is
a long spec you think a J cash spec is
probably like 15 pages think it's like a
hundred and thirty pages there's all
sorts of API stuff in there and this you
know concise definitions of like by
value of by reference or tool we're
going to talk about so and if you find
it if you do want to read it or you do
think like you have insomnia and you
want to go to sleep and should read it
and probably to sleep it the finding
mistakes or you have any questions like
please send us information like if we
can only improve it through people
reading it and the community working
with it with us yeah I mean for example
for example awesome from hazel caste our
guy was was implementing was
implementing a spec and he came he was
looking at he was looking at at closed
versus destroy and we kind of rely like
I realized it looking at it I
communicated with you we kind of
realized that actually we probably need
to get back into the spec and just
clarify it slowly
yeah and that's a perfect example and
yeah so these these things happen and
yeah each implementation you know loves
to implement their own or what subtle
way so you know coherences the notion of
close us earth ago to close the edge has
cash as an ocean of clothes in their in
their sort of native implementations
caching implementations but in Jake ash
which was standard
what that means so and so they just to
finish that story actually destroy means
that you close the case and you remove
all of the contents that are in it
requires as a topology clothes
regardless apology clothes means that if
it's a distributed apology quite closer
on in process pretty much means the same
as destroyed clothes and a distributed
apology essentially means that the
client detaches detaches from the data
but other other clients might actually
still be tuning into it yeah okay let me
just make a comment on the motivation oh
yes
the really really super super big
picture motivation of what I was trying
to achieve in creating the spec was if
you look at web scale architectures
you'll see that that caching is is
always or as a significant pillar of
what they do it's always backed in
whatever whenever one of those guys
Facebook Twitter Google Yahoo looks to
build something a high capacity service
they always add casing into it now in
spring and Java EE and back to
enterprise software that has not been
the case caching is as a technique has
kind of had a little bit of a dirty
reputation as if everything else worked
properly you wouldn't really need to do
it so by creating Jay cash what we've
attempted to do from a 30,000 foot view
is kind of say actually it's okay to do
it it's Orthodox you should be thinking
about it when you design your
application here's a standard way to do
it and then it gets baked into these
these frameworks like spring and EE yep
so what was a unfortunately we missed
the Java EE 7 cutoff actually is quite
liking we did that and there are a few
reasons why we did it but so we didn't
appear you know J cash didn't appear
inside of Java EE but it's it's quite
important that it didn't because
otherwise it would have been a spec sort
of on the side of the rest of GE there
was not enough time for us to
investigate how to integrate the the
caching API with the rest of je because
we don't really want you know Java EE to
be this you know just collection of
stuff we want an integrated platform and
so
you know we'll talk a bit more about the
motivation later so when you look at the
platform see you'll see the specs is
retargeted Java six and so that's a
strange thing to do but a lot of people
are still on Java six right he's on Java
6 yeah right so it's about 30% yeah last
year was almost was probably always 90%
so the spec is written with Java is is
targeting Java 6 so people can implement
it with their existing apps but the TCK
and the reference implementation is Java
7 so it's fine you can still build an
implementation and you can just testing
us Java services just to re-emphasize
that the three implementations that will
tell you about of JK's that exist now
you can take your Java 6 application and
you can add J cache to it alright you
don't have to wait for spring or EE it's
designed to be used with the JDK yep so
yeah that's why we did that so for the
one person who's read the spec you'll
you'll see this you'll see this table
this is I think probably if anything
else hot walk this is probably the most
important table most important piece of
information and we get asked this a lot
and I'm gonna when I should when we get
into some code very shortly I'm going to
ask some questions based on this so you
know why isn't it just map so well the
cool thing is that it looks like map and
some people say why don't we extend map
and there's some reasons we didn't know
extending that because map has some
constraints that are efficient aren't
efficiently implemented in a distributed
environment map is not designed to
really cross a boundary our process
boundary where azmuth imagines that
you're in process yeah so where the
recursion that we couldn't do that so I
think what you and yannis were part of
the decision to do no no no I had many
painful conversations up and then Oracle
building but it's it's actually you know
fundamentally a great thing but still
when you look at cash nor the API in a
second it's key value base so that's why
I'm about the QE base you can do atomic
updates which is which is also fine but
one thing what's very subtle difference
is
entries may expire right and entries may
be evicted
another thing that's that's really
significant is because map map is is
conceived as an in process data
structure when you put something in a
map you're putting a reference in so if
you put something in a map and another
thread is holding a reference to that it
can mute or even even the can't thread
after puts things in it can actually
mutate either the key or value in place
now what's the behavior when you
actually mutate a key in a map it's
pretty bad it's undefined yeah it's
actually also undefined in cache so it's
kind of the idiom of using map is that
you put things in but if you're mute if
you're gonna mutate the value that's in
there you know explicitly that you're
doing it most people don't
so it's simpler to actually think for
cash where in particular it could
actually you could ever you tiered
stores that might that might actually
have see realized objects in we're
changing we're changing a reference
wouldn't work either anyway or it's
distributed or ITIN reference but you
don't want to have side effects we
actually went with the default storage
type of store buy value which not store
by reference right so when you're using
a cache by default you have to assume
we're making copies of the keys and
copies of the values or actually we
don't define exactly how you do it it's
it's that way we defined it if you read
the specification is if you put it in
and then you actually change the thing
that you put in it doesn't change what's
stored in the cache right there's a
multitude of techniques for doing that
so some are reference and still they
value the the notion of expiry and
eviction is this critical because when
you place something in a map you expect
to be able to get it out unless someone
else has removed it if you place
something in a cache and you go to get
it out it may actually not be there so
completely valid implementation of this
of this of this specification is one
that always loses data or never accepts
data always returns null it's perfectly
valid implementation that would pass the
TCK I think the way we write the TCK
that almost almost passed out okay so
you can just do a null implementation
sugarhouse so you think that's really
weird but in a cache like you're trying
to keep recently at most reached
recently used values or frequently used
values and the underlying implementation
may need to make decisions about how
much memory you have
you know evicting stuff on the fly and
all of that configuration is outside the
scope of the spec right and and you know
some of the implementations I think you
guys do it as well is now pre fetch
stuff so you know coherence can pre
fetch stuff as you know as entries look
like this time to expire and just on
using them a lot instead of just
expiring them and then going to some
back-end integration like you can you
know go on pre fetch and say I'm going
to get you a new fresh copy so all
that's the semantics is the ape the spec
lets you do but it's not specified how
you tell it okay if you're wondering why
why cases do what they do that the key
thing to remember is that what they're
aiming for its performance that's the
key thing so other obviously is a bunch
of other things so supporting
integration I briefly touched on that
you know connecting up to back-end
systems with a data user typically like
a database but may not be supports
observation we all look at that the
notion of entry process with a whole
section on that today and statistics
okay so you can go here the Jets this
was all done through JCP we use GTV 2.9
was it ended up a 2.9 yeah so everything
is like completely opened all the issues
we've raised all issues we've closed the
discussions in the forum like we try to
do absolutely everything in the open all
the source code for not only the spec
the reference implementation examples
and the examples I've got going to run
through today which are new we're going
to put them up as well so you everything
we have here you can get soul up on
github there's like three or four
hundred issues as we were discussing
things recurring them as github issues
so that the fully discussed there's also
a Google group you'll see a couple of
issues there where people asking
questions about the current spec so it's
all completely transparent okay
so if you want to grab grab the speck
it's really easy it's in maven central
you can just include that in your app so
let's actually get started at some let's
see if we can write some code
everyone likes code so my first thing
the first example everyone's aah Wow
nice it's on I can't even see that and
actually I keep looking this way because
this guy's got an 11 inch smack like
size it at all it's nothing to a sized
man okay yeah so my max bigger than your
Mac it is it's rather that's hidden okay
so there's a few convenience methods in
the spec now this is just one way of
getting you know getting to where you
need to get to actually get to a cache
the implement the specification lets you
say you know if you have if you know
what implementation of a cache you could
just say new cache you can do that
but for the most cases caches are named
and they're like you know resources in a
system they may be injected into your
application through whatever injection
your container or or framework allows
you to do but if you want to just you
know write main and say hey give me a
cache and this is what you have to do
you basically you need to get a caching
provider and J cache was designed to
support multiple caching providers so
you don't need to tie your application
to a single one you can have multiple
ones from the same vendor from different
vendors and they all can live together
happily they can live together in the
same application actually so there's
there's the sensible defaults if you've
got multiple caching providers sitting
in class path and used to occasionally
education provider it'll just return one
randomly so we provide a way if your ad
fully the fully qualified class name of
the provider it will it will specify one
so the point is these are as simple as
they can be if you have a more complex
world we also accommodate that yeah but
at the same in a simple case you could
just do that right so let's just run
this and of course once you get a
caching provider you can need to get a
caching manager and then once your cache
memory you can get some caches but
let's just run this i'll come up dude
so that's hello the first the first
issue this is what everyone sees until i
start to configure this this is what you
will see when you download the gauge ok
you just use j cash and just start this
this is what you're gonna say you have
to add a provider yeah my to choose a
shoe shop now hazel caster vitae you can
use coherence you can use a reference
implementation so the reference
segmentation is there all the source
codes there and my old like my old
project the education is also nicer than
it so you know being vendor friendly we
have a maven file here which has all of
the different providers that we know of
so there's you walk around the exhibit
hall as people claiming to have
implemented JK which actually haven't so
there's only three that actually have
done it so far yep so I haven't included
the reference implementation because we
don't actually like ship it anywhere you
can download the source and just make it
and a word of warning about the
reference implementation we wrote the
reference implementation has a reference
implementation basically for something
to run our TC k against it is absolutely
not a full implementation you cannot run
our production it has not does not have
the concept of resource control and
actually it's not even thread so if you
know if you have one thread of this yeah
so but it passes a TCK so so i'm this k
something I'm gonna use coherence but
the cool thing is you can actually just
use any of these and you know Greg you
know mucking around before so so you
uncomment the ones who are now each each
provider has a different way of doing
this so terracotta eh cache basically
uses an adapter so they've a separate
library which wraps the H cache has a
cast implement it directly like here
you're gonna be nice and after you stop
the coherence why are you gonna show
this dot you know so yours now if you
want you just all you have to do is
uncomment they say yes yes right so one
difference is you'll see that for key
here into the two jars same thing for
terracotta because it's an extra wrapper
what we decided to do it hazel cast is
just add it into the core and it's not a
rapper
built on a lot of local services but a
sister you know at this level it just
means it's one jar Robin too doesn't
know how much so so now when I run out
don't get any exceptions but it you know
Karen starts up and it's gonna try and
find a cluster and because it's
clustered it's on and so yeah we can do
the so the hazel cast being patchy to
open source is actually also on maven
central you'll notice that's it you just
swap now can I just I'm just gonna make
one comment about hazel cast Hauser cast
is a in-memory data grid there's never
really been a house of casters never
attempted to be a local in process
anything so while it's possible that you
could have a cluster of one node and use
it in process you will get shockingly
bad performance if you do that we make
no attempt to do any local optimizations
so if you're like looking to compare
performance and things like if you're
looking at hazel cast like like for
education in process case so you get
really fast performance coherence has
got a local mode as well doesn't that
yeah so the default we're using today's
or local right so for us we're basically
a distributed in memory data grid so if
you're going to do a performance test
with us and other implementations just
make sure you do it distributed
otherwise you'll say what the hell's
going on with this yeah yeah you want to
make sure when you compare any of these
things make sure you're always like
comparing the same sort of topologies so
so let's let's go on to let's switch
back to let's switch back here because
this is an important slide gonna run
through the hierarchy yeah these are the
essential concepts everything you need
to know to keep the core concepts on one
slide ok so so this this nice little
piece so basically your application can
use this convenience class called
caching and you actually can get caches
from it we showed you how to get a cache
provider but as I said like your
application may have the caches or
passion for
or cash managers injected into them but
in this case we're actually using
cashing to get the cashing provider the
the cashing helper class will actually
manage all the providers and manage the
class loaders it uses the standard Java
server service load of stuff this is all
the cool stuff that you only wrote I
think we even touched it yawning they're
really good
he attacks it yawning I did I must it
still has your name on it though yoni
and then cashing providers are really
the implementation so this is where
actually all we did I think the main
change we did was to which we you and I
didn't get to at that point was
supporting multiple multiple patient
providers and then cash providers can
have you know many cash managers and the
cash manager is usually used to have
many caches now this notion of manager
in between is actually important because
it allows you to do some scoping when
you're running in a container so when
you're running in a full-on EE container
usually it's that excuse me the cache
manager is bound to your application or
it's bound to your application class
later but in a java SE application you
know you don't don't really worry about
this
so the caching provider like containers
frameworks may come with a caching
provider built in and then your
application would have a cache manager
or caches injected into it from there so
so metaphor a metaphor if you're
thinking of sign oracle database is that
is that a caching provider is kind of
like a version of Oracle where you go
the binaries on your filesystem you fire
up an instance that's that that's your
cache manager and then the individual
you know tables would be your caches
that's right so but most cases what
actually shows you the previous example
you don't need to go through all of it
you can just say caching get cache and
it will find the default cache on fire
and the default cache manager and off
you go
yeah so once again it's incredibly
simple if your environment is simple
right so it's as complex as you need it
to be to make your stuff work so we
talked about implementations there's the
reference implementation just gonna grab
that play around with that the reason
the reference implementation is
important is because when you
a question about whether cash is
behaving correctly you should look for
the reference implementation I'm not
saying that it would ever be completely
perfect but that's where you go when
you're implementing a spec there's a
couple of other implementations that are
in process and finish band have got an
implementation up to 0.8 of the spec
there's a couple of other people are
talking about implementing it I think
most of the that isn't a space blow up a
minute over time as they implement it
Ron and I as mainotes leads will notify
the JCP and that's where you keep track
of those yeah again all in the open okay
so let's let's that use that's using
some caches so step two here so we now
have our cache manager and the next
thing we actually have to do is actually
configure a cache now one of the things
about the spec is you don't actually
have to configure caches you can show to
a cache manager say hey give me give me
cash products or giving a cache part
catalogue and you don't necessarily have
to create that cache because you may
with whatever your provider is
pre-configured outside of the system so
you may have some XML file that defines
a whole bunch of caches and their types
and all the integrations and expiry and
light sort of stuff all of those sorts
of rules so you can then just say to the
cache manager hey give me this cache but
we wanted to make sure that you can
build these things on the fly so what
you can say to a cache manager and every
implementation must support this so you
can say to a cache manager hey create a
cache and in this case we can say you
know cache manager credit cash now in
terms of configuration the
configurations obviously you know if
you're using asel castle using coherence
or using eh cache the configurations can
be very specific in fact every provider
has their own type of configuration but
instead of forcing you to have to learn
all of these and it makes it hard to
swap if you just want to use a regular
old cache we have this standards
provided spec provided mutable
configuration class and every
implementation must support it so when
you say out-of-the-box you say mutable
configure
raishin here and I'm gonna sew on it
store by value and I want the keys and
melly types and so on you know you give
that to coherence then it will respect
that and create a local cache or if you
use my the coherence flags you can say
hey mutable configurations are always
distributed cache there's no ok it'll
it'll do that so we must respect us
because we work this way we support
dependency injection containers yep out
of the box so then we can create it and
then obviously we can you know put some
values out so my question nice crate to
pry yes that's about all I know so you
know my question here is you know what
should the value be returned when I say
from my cache you know get a you today
my I want everyone say g'day mate so I
you know just get a mate is normal eg I
was like what are they gonna say got a
well it depends on the translation so
what is returned g'day mate not oh it
could be null oh yes it's not a map it
could be no by the time even between me
brings up a question what are the
default expiry policy well it doesn't
even matter might even the default
experice policy was eternal it wouldn't
explore that's right we yes unless you
were out of resources unless you're out
of resources so even when you tell the
cache don't ever expire anything you
know through the configuration don't
ever expire anything the implementation
can say no I'm not going to let you run
out of memory I'm absolutely gonna
remove stuff and I'm gonna remove stuff
as in what I please which is which is
normal
pretty much every case implementation
rather than run out of resources will
throw things away you don't have to
configure it that way but by default
that's the way you know how they're
gonna work so between putting doing this
port and doing this get the value may be
lost now if you're using map that
behavior would really suck
all right you with map you used to
expecting the values there right so and
also you think well this is silly like
this is like this little piece of
example here but if you imagine this is
running in a really big system and some
other processes just pounding data into
the cache and you think well I may just
put three values in why is my value not
there some other process could have
actually overrun the cache and forced
that value to be removed so you must be
conscious of this all right don't just
say all I've got a couple of maps in my
application I'm just going to swap them
out for caches now the API is the same
everything I'll be great I'll just
compile power will run but be conscious
of the behavior of the cache when you
configure it right okay so we can run
this now yeah because it so why don't we
subclass map because the cache is not a
map I know your entity's name caches a
map isn't it yes but we also use it
which is a bad idea that price trying to
get away from but we also implement map
semantics so and also if we then you
call infer you then we'll just pollute
the whole API with exceptions as well so
you know inheritance is the easy
relationship if you see if we said cache
is a map then we have to respect that
and that's in locking coherence we
respect that by default and you can turn
it off the point in JK is that we don't
we we don't want cases to be Maps the
usual language that we use we say the
case is a map like but not Maps okay so
did I run that no I didn't run that one
I ran the wrong one that's right I think
it all these other ones that are running
and stop these hey guy I was right did I
make you
okay so I can of course this is pretty
simple
I can now run it now in this case we
haven't got the eviction and the default
that says it'll be fine we're gonna
we're not gonna throw it away but I
could run this several times and Michael
and then sigh don't be gone so you can
imagine how hard it is to test this
stuff so in the TCK we made some
reasonable reasonable expectation that
when you put stuff in it's gonna be
there yeah and as a simplifying
assumption yes otherwise it like there's
no way to say but we basically say
reasonably when you put something in
there and try and get it out it should
be there but it Doretta CLE it doesn't
have to be I just like to make a point
you're probably getting a feeling from
this this seems incredibly simple that's
the whole point of JK it is an
incredibly simple API you can have the
most mind-blowing like complex
sophisticated infrastructure distributed
infrastructure behind it that can be
running some of the world's most mission
critical systems but the API itself is
super simple so you learn the API you
can use it for anything from in-process
systems to large coherence and hezekiah
systems alright so we're back there
where we up to now well I'll you know
just sort of go cache dot and sort of
look at some of the math you have so
there's obviously we're going to talk
about listeners shortly we can do entry
process we're going to talk about
so we've seen port we've clear so you
want to remove all the entries from the
cache by the way time check we've got
about ten minutes left
no no no we don't want that so we talked
about clothes contains key very much
similar these things similar to what you
see with map
yeah apart from listeners get get all
port get input getting remove and so on
okay load a whole bunch of things
iterate the things that you would expect
to see replace okay let's go on
so we're gonna leave a little bit of
time for questions all right correct
five five five we've only got really two
bits of code to show left yeah okay all
right one of the fundamental differences
between map and n caches is we decided
that we wanted to allow custom atomic
operations to be performed without the
requirement for locking right so this
seems this seems like a weird thing but
if you imagine you want to you know
change a value in a cache what you want
to try and avoid is going backwards and
forwards to the cache from your
application and entry processes is a
concept that's came out of coherence
which we put in call entry process and
coherence at all interests and caches
you guys have implemented all of
implementations now have the concept of
entry processor in Java right you might
think of these lambdas right so this is
pre Java right but the actual lambda
stuff still works and we've tested so it
allows us to in a distributed system to
completely optimized and remove
round-trips
so let's well here's a really good
example so if I wanted to you know
increment a value let's say I had a
number in the cache and I want to
increment that value in J cache you
could say increment some string is the
key here and you pass in an entry
processor in Java 8 you could use a
method reference that sort of thing we
could you could write a custom lambda
and then have some parameters versus
using something which actually this API
doesn't exist going lock getting entry
updating it unlock you know we didn't
put this in when you consider the
network's there you've got an applique
typically you get an application server
you're an in-memory data grid that's
across the network
if you lock then you do an operation
across the network bring it back to
serialize it do something put it back
you're locking it for a long time that
that's going to hurt performance with an
entry processor
the the code for the entry processor and
the data are actually running in the
same node across the network so you
basically you've got the code you've got
the data and the code and essentially
when you invoke an entry processor
you're telling you're telling this the
in-memory data grid to execute that
entry process are against that code so
because there's no network nothing
across the network inside we actually
lock it perform the operation and unlock
without it ever leaving that node so
it's a super fast logic and a lot like
logically do that logically and so it's
that it's locked for the very shortest
time possible yeah so in terms of round
trips you basically send the entry press
across the wire logically lock it do the
update and return the result versus in a
distributed system you'd have to like go
over lock get bring it back do the upper
in the client so we say logically
because there's different ways of
implementing it yep in hazel cars we
don't actually lock the entry we we have
a queue up things on our partitions
right now so we test a lot of this stuff
with Java right even that's Java six we
need Java it was coming along so you can
absolutely use lambdas you know you need
to be a bit careful around serialization
and the lab has been available in the
server side if you're doing this in his
tributed mode but this is really a
killer feature of everyone's of use
memcache T right the notion of just
doing processing in the cache like just
doesn't exist this is really really
amazing scalable yeah this is this is
the killer feature of a memory data grid
is that a you get 10,000 times
performance ever anything else so we can
we'll actually we have a simple example
here so what I wanted to do is you know
update you know convert one of our
entries to uppercase so I can say invoke
against a you that's the key and I can
also provide a whole key set so I can
say invoke all and provide a whole key
set and so do this against all of this
stuff all right so single and batch
operations and you'll see IntelliJ has
converted this into a lambda for me okay
all right until IJ is smart enough to go
hey
but here's the actual whole code and
what we do is Bo the implementations
what they do is when this you know entry
process arrived wherever this entry is
we pass the entry in which is mutable at
that point and any other arguments that
you may have had and then you then we
execute the code so if the entry exists
then you know set the value to be upper
case of the existing value and notice I
can execute an entry presses against an
entry that doesn't actually exist it's
not that cool I can basically say hey
invoke against some key that doesn't
even exist in the in the in the in the
cache and when I get there I'm
guaranteed that the entry processor has
complete exclusive access over the entry
which doesn't exist so no other entry
processes can change it no one else can
do a put over the top of it and then I
can like update it so you want to do a
really super fast counter inside the
in-memory data grid you do something
like this so if it exists do something
if it doesn't exist create the value and
then I can return anything okay
so of course we can run this so it lets
us do things like custom foot we're
almost done
of course I ran the wrong one
I must remember to do this good idea
that then I did don't know I see that
Larry Ellison to do it yes
so now of course my entry is g'day mate
okay so I have a question ah so what is
a return now so the question is so an
entry process is is so question is why
did the interview process or return now
well I just chose to return now I can
make it return anything but the reason
is that what that that an entry process
that does set value it will update the
entry in place now you as the calling
thread you don't need that necessary I
need that value returned to you you may
not need to know at all if it filed you
might just need the exception to be
thrown so the return value of entry
processor is anything that has semantic
value of your application often times
it's going to be nothing at all so I
could change this to say it's I want the
current value and I've entry gate value
and I can say something like current set
the entry to be that and then I consider
its current so this now the entry
processor so you know back to thinking
about this is an Oracle database the
entry process there is a stored
procedure stored procedure in your cache
right so I can return entity doesn't
have to be string here it just so
happens to be that's what I to find that
a return type to be no no I can make it
I can make us anything I want I'd say
it's generics yeah everything the whole
of JK is generic okay so imagine I could
run an entry press that goes over
mutates a value and then returns a
different part I may have a cache which
is uh let's say I'm cashing a product
but I want to just update one value
inside that product and maybe your honor
I want to return the the previous value
I don't return the whole product I don't
want to like so typically in a cache
you'd do something like I'll get me the
whole product I don't want the whole
product I just want a piece of the
product so I can use an entry press to
go over and just grab that and bring it
back and the beautiful thing is when you
use things like method references I can
actually just pass in the method
reference so the attribute that I want
that current
another interesting thing about entry
processor is that the whole thing as
atomic if you're performing multiple
operations within an entry processor
there's no visibility to other threads
until you complete so you can get you
can create you can only update it
depends on the implementation so in this
case it's all local but in a distributed
implement yeah distribute the
implementation there are may the
implementation may require things to be
serializable like like yeah but yes it
might be yeah first over the code to be
on all of your notes of your memory data
grid the simple way that everybody
pretty much supports is if you package
all all this code up stick it in a jar
and then when you start up your memory
data grids there that's one way another
way is dynamically across the network
look polish the commands on slims on the
implementation like jinny might be used
to do a genie in java spices yeah yeah
another question I guess you mean is
invite synchronous or a increment this
blocks until the result comes back yes
that's fine
so it doesn't block the entire cache and
these semantics are actually not defined
by the specification so for example if
someone was calling get on a you the
implementation may choose to block on
that the implementation may choose to
not block on that it's an optimization
that you choose that the configuration
stage is so one it's guaranteed that if
two threads try to call invoke on a you
at the same time only one is executed at
any important first and there they queue
up well Q is one way of implementing
that semantics you can eyes locks and
that performs they're natural cues so
but the semantics are but I could invoke
against a you US and F are on three
different threads and the spec doesn't
say whether they all happen sit
concurrently or they happen sequentially
or depends on implantation just this up
brings us to a really interesting point
as what is the report what is the
guarantee consistency level in in in the
spec so we we did actually spend a lot
of time thinking about that and what we
came up with is essentially that the
guarantee is strong consistency but
implementations may choose to support
additional consistency models which we
chose deliberately not to define we can
talk about what some of them are like we
consistency eventual consistency but we
don't think that this is kind of mature
enough to go in and actually define them
all yep this is not a database as a
cache the spec actually had JTA
transactions in it until pretty light
into last year and then we decided to
actually remove transactions from
version one of the spec they make they
may come back in into a later revision
of aspect it's in the notion of world
rollback so let's say if there's an
exception inside of it the spec says if
there's an exception the entry won't be
mutated the spec actually compared to
any vendor implementation what you'll
find when you read the Javadoc of J
cache and then if you go and go on and
actually read the spec dock you'll
you'll find that the specification of
behavior typically goes way beyond what
any any vendor implementation error has
it's a very much a tightening and a
reading redefinition of the behavior and
you went through that mental process
yourself De Niro's yeah what to do what
to do when there's an exception
Kenneth what if I want to return an
exception like there's like a huge
variety things but also like the spec
doesn't say well if I've got a cluster
and the cluster goes away and I'm doing
invoke and the cluster is on there what
happens so different implementations
choose how to risk like well I'll I'll
take all of those invoke and I'll block
or I'll fail with an exception or I'll
guarantee that occurs on a different
server like the general generally
speaking we're very very clear about
what the contract is and if we if we're
not defining something we're very clear
and we say without it's that we're not
defining it and
and anything outside of that we've
missed and you should tell us because
it's going to fall into those two
buckets like we will define it clearly
or we were explicitly so we didn't
define it and there's a reason for that
I mean so far so far we don't have
anything like that I really and I think
no one's brought one of those to our
attention yet
we'll see probably next week we we've
got more questions but we're almost out
of time so I think we just continue to
the end of the presentation and then
we'll take available questions before we
get kicked out by a B and then we can
take more questions out in the hall so
there's only one other example and
that's listeners so the spec defines the
ability to find listeners and this is
unlike this is unlike what you see with
nice kill solutions and most of the most
of the open source solutions that
outside at the Java space don't really
have the concept of listeners and this
looks really complicated and this is one
thing where were you know talking about
making a little bit simpler in a dot
release but basically when you define a
listener you basically want to know when
things change in a cache whether it's
created or updated deleted or expired
you want your application to be told
about those because you may want to make
some decision if you there's a specific
piece of data in the cache and it gets
expired you may want to know about it so
this is the observer pattern but
designed to operate against a
distributed system or a local one yeah
or like a one one thing you'll notice is
that unlike with a normal observer you
can't create an instance of an object
and say okay you know this is this is my
listener you know case set listener the
thing is you may be distributed to so in
fact in this along with loaders and
writers we actually have a factory that
factory serializable so you pass in a
factory and then the implementation uses
that factory you're actually creating it
so the listener when it needs it and
that that is that is kind of a pattern
that you'll see again and again
throughout the spec so the notion is hey
I want a listener but I want the
listener to be in the grid or I want the
listener to be in the client or actually
want the same lesson to be in both
places or I want to listen on elements
filtered elements in two different ways
and on different servers so you can't
just say add listener alright so you
need to give it a
more informational so where where does
the listen where does the listen happen
so it's all in the spec basically very
elusive concepts in a distributed system
so we spent a lot of time honest yeah so
hence the configuration of them isn't
that pretty but we know we can add some
very simple helpful classes so in the
case where you only want to locally we
can we can add one way of doing that
that's that's clean but this this sort
of allows all of the implementations to
provide a lot more capabilities to
developers so you can register them
dynamically and we're just going to run
this so you can see so this my listener
here is really simple and it's just
implements that created listener
implements created listener and updated
listener and one thing you'll notice
unlike this sort of usual listener event
processing you'll see in in the standard
Java platform every method in the spec
that either lets you load or write or
listen is designed to work in batches
right because if you put a hundred
thousand entries into into a cache and
you have a listener on it you don't want
a hundred thousand events back right
well you do want them but you don't
necessarily get them coming chatting
backwards of course across the wire so
every on advanced you know on credit on
will give you back an iterable and then
you can either write over them and the
implementation can then choose to bash
them up we don't we don't know how big
these things are gonna be some some
implementations and now you know are now
storing data yeah we J cache could the
size of a JK could be running into the
terabytes yeah and like you go get key
if we went get keys and just return a
collection of keys you'd explode yeah I
you know we support iterable matching so
that's why this sort of looks a little
weird you know why can't I just like put
my thing in so why can I just get one of
it
so hence that's it and you know running
we can we can see this running
and you can see this is the event output
and you notice even the even the entry
processor which mutated the entry cost
an event so it doesn't have to just be a
port that caused the event right and
could be a result of the entry process
of that coolest event okay so let's wrap
up probably are we haven't talked about
annotations yeah which is your least
favorite topic well actually no Yanni's
least favorite topic favorite topic of
mine no I'll do this yeah I don't know
why these Oracle guys don't like
annotation oh we love them it's just
hard to implement it's not because
spring got there first is it no well
spring implements these that's the
really cool things bring actually things
4.1 has support for these so spring for
that one's got full support for J cache
also for spring the most important part
of that is them supporting our
annotations they've had their own
annotation types for awhile that's kind
of the list we'll just show some code on
the next slide so you know let's take
look we got a blog manager there
essentially you can put an annotation so
that when that method is called it will
invoke a case operation right so one
example would be on where's this one
here so get blog entry when you call
that what will actually happen is that
it likes you go and check the cache for
up to see if it's in the case if it's in
the case you'll actually execute the
method so to really I actually really
like this idiom Bryan doesn't yeah he
doesn't I do I like it it's just lots to
do so this this type of thing it's
braked in the spring for one in the
reference implementation we've got
adapters for CDI and also for juice
which you can just go and grab and use
with juice or Java EE 6 or above yep
okay the future we always like to
crystal ball a little bit so we've had a
few requests actually the requests have
had so far I've been basically Java dot
fixes so we actually don't need to do a
maintenance release just to fix Java a
typo so that's one thing we're going to
do but obviously we've found like some
feedback has been how you would like it
make it easier to do
add a listener or easier to do a
serializable entry process so we can add
helper classes around this trivially
without starting a new jsr and so on and
there's a couple of things that we need
to do what we'd like to do in the TCK
that make the TCK easier but again we
don't need a major maintenance release
for that either
but we'd probably wrap all of that work
up together I mean so far it's good
we've got three implementations done
since we released the spec and we don't
feel the need to do a - release no-one's
no one's been screaming yet yeah so jake
has - now our idea is basically to
enhance it to support Java 8 - Joe 8
platform adds a whole bunch of cool
things lambdas and streams so stream
processing your cache entries lambdas
and of course if we inherited map we
would have that automatically but there
would be some real problems with that
because of the way that map assumes
stuff being it and then some of the
compute compute lambdas just wouldn't
work so by the way we actually tried
this well Gregg sat down for some time
yeah if you got a look at my blog you'll
see some labor there so so learn yep
kind of almost running low on time the
other one is that an unsolved problem in
Java EE is a standardized way of
actually doing session clustering so
maybe something might come out of server
for for that so whether that's in J
cache or whether and that's in the
server the new services for spec but
obviously there's some stuff in Java EE
alignment to look at and that's one of
the nice things is that now that J case
exists and it is a JDK spec other got
other guys like serval if they wish can
go to go ahead and use it and they might
be able to do the job without requiring
any changes from others we should we
should wrap and take questions outside I
think yeah that sounds good bye so we're
gonna wrap and we're gonna go and stand
outside so that the room can be used for
next session so thank you for attending</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>