<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Java Memory Hogs | Coder Coacher - Coaching Coders</title><meta content="Java Memory Hogs - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Java Memory Hogs</b></h2><h5 class="post__date">2015-06-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/gz4LFnOstes" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Nathan Reynolds my email
address is Nathan dot Reynolds at Oracle
comm how unique
I am the EXA logic performance architect
but my son just died there we go
I work at Oracle and I'm here to present
to you on Java memory hogs so ram is
cheap go buy more ram and problem solved
class dismissed I see I'm gonna have to
try some other tactics to get rid of you
guys all right well if you're here to
learn about how to tune your GC or get
better GC performance you're going to be
very disappointed if you're here to
learn how to chase down memory leaks and
fix your memory leaks you're going to be
disappointed as well still no one I'm
gonna have this try harder all right so
I've got 68 slides 60 minutes and let's
spend about 10 minutes on this one slide
that you haven't ever seen before all
right so a little bit of introduction I
work in a team Oracle called PSR if that
stands for performance scalability and
reliability we are my team is dedicated
towards optimizing Oracle middleware
including a little bit of the JVM but
more importantly WebLogic so people all
the way up the stack fusion apps you
name it
my particular role is to optimize the
software for Oracle X illogic and if you
if you're not familiar with the term
it's basically a rack of computers that
we sell an engineered system with no
single point of failure and we've done a
lot of work to make sure that things
work very well out of the box for you
all right so here's the presentation
agenda so a little bit about some
background so my boss asked me to
optimize the software for X illogic and
I started thinking well all right what's
going to have the most impact on all
these applications and I thought well
hotspot and WebLogic so I started
thinking about that and immediately I
thought well I could go in and optimize
JIT and help improve get a little bit
but I figured it was already well
optimized yes there's a lot more work to
be done there but
I didn't think I would make that much of
a impact and then it dawned on me that I
had heard that Java programs are
typically 2 X dot of C++ programs
I couldn't verify that on the internet
so I'm not sure of that one but I
figured maybe that would be a good place
to look for optimizing so I would didn't
want to just focus on one or two
applications I wanted to be able to
cover all applications because we
already have people working on each of
one of those individual applications so
my problem was well how do I get a
variety of heap dumps because I need
heap Dunstable to analyze the memory
across all these applications and so I
asked my team and I got one or two and
usually you don't hold on to heap dumps
because Sue's the utility of them is
done you throw them out because they're
very large so I stumped for a while but
then what I realized is we have a bug
database and in that bug database people
upload heap dumps so I just need to scan
through the bug database downloaded well
analyze them and voila
I've covered all the applications across
Oracle unfortunately just some caveats
with this analysis that I should be
upfront with most of Oracle products are
servers and so this is gonna be very
skewed towards a server application
however I'm hoping that client
applications would also be applicable I
did throw in some client apps I took
Eclipse IDE heaped-up from there and
some other client app programs that I
had and just add them to the analysis
just for the sake of it most work
clothes some workloads are represented
more than others and so that might skew
a little bit the results some workloads
just aren't represented at all I mean
they're more mature products they don't
have memory issues and so that people
are just aren't uploading the teams
aren't uploading eep dumps to the bug
database we also have to ask ourselves
well why did the person upload the heap
dump to the bug database everything's
hunky-dory no problems no not the case
it could be that we're at out of memory
or there's a memory leak and they're
trying to chase down something wrong or
there's a memory aggression between
version a and version B the memories
hire and they want to try and fix that
and so I'm hoping that by looking at the
average or
median throughout all these heap dumps
we avoid the extremes and look at what
the core problem is across all these
programs now there are some surprising
benefits to this the obvious one is by
reducing your memory it reduces your GC
time but what I didn't foresee is that
by lowering the GC time it also reduces
your average response time because when
GC is running your application is frozen
and that means all your latency is just
spiked during that time and so I'll talk
about this app this optimization a
little bit later and Chris Bailey kind
of hinted at one aspect of it and that's
the lazy initialized hash map an
arraylist optimization and so we took
these three applications ran it with
this optimization and we were able able
to save anywhere from 8 to 13 percent of
the heap and the surprising fact is that
response times came down by 7 to 19
percent so even if your application
isn't memory bound still trying to
reduce the memory that it's using will
help your response times all right let's
talk about some statistics where did I
get all these statistics there's a
product that oracle has called J or Flo
it's there's a command-line utility
which will dump out a nice text file but
also there is a plug-in to Mission
Control and it will go through and do
all these analyses and point out all the
different problems in your program can
you use Eclipse memory analyzer sure I
use it it's good tool and sometimes I'll
use two the two in conjunction one will
identify the problems and one I'll go
and drill in and trying to figure out my
application for more information on J
overflow there was a Java one
presentation last year from the author
of the tool and you can look that up to
see how how well to see how you use the
tool all right
throughout the slides I'm gonna be
throwing up this table if you can't read
it I'm sorry I gave you a warning that
you might want to move up but let's go
through this table because I'm not
always going to be reading out the data
the first row tells us how much of the
heap is used by this particular thing
I'm talking about in this case twenty
four point four percent on average of
the heap is used with a standard
deviation of twelve percent a median of
twenty five percent and a maximum of
eighty-three percent
ish the second row tells us how many
bytes
is used by the statistic in this case on
average two hundred forty five megabytes
the third row tells us how many
instances are in the the heap on average
for this particular thing that I'm
presenting about two million objects now
the bytes are always multiples of 1024
the instances instance counts are always
multiples or powers of a thousand now
what about these dashes on the min well
J overflow isn't going to report
everything it has a reporting threshold
of 0.1 percent and when I cannot
determine whether the there's a zero or
somewhere between I just put a dash
because I don't know but it's going to
be very small now with statistics we
always want to compare the mean to the
median to see where the outliers are in
this particular case the outliers are
towards the zero end of the spectrum for
the percentage but for the bytes they're
towards the higher end because the
average is being pulled up in the bytes
cased all right heap usage our first
statistic well the first row isn't that
interesting because all heaps use a
hundred percent of their heap space duh
okay but the next two rows are a little
bit more interesting the average heap is
about a gigabyte of what I analyzed with
the largest heap being about seven six
and a half gigs the smallest one being
three Meg's and actually wanted to check
out what that was and it happened to be
just a simple GMS sender client some
very popular objects are string char
array which i'm not double counting when
i say string these are independent char
ways outside of string string buffer
stringbuilder object arrays again not a
part of any collection ArrayList and
hash map
all right let's talk about strings watch
out for strings these consume about a
quarter of your heap on average that's a
lot of space so you need to watch out
for them one thought I had was well how
many people are encoding numbers into
their strings well about 0.4% of the
heap is that way and it's kind of
scratching my head whole who would be
silly to do that and I thought well XML
parsers of course when you're parsing
through you're going to be putting
numbers in your strings so the average
string size including its headers the
object headers array headers and
everything else inside there that Chris
pointed out earlier is about 126 bytes
that means that the average string
length is about 45 characters but that
it's that calculation is assuming that
we're using on j2k before 7 update 6
because at that point a couple the
fields are removed from string to make
them make strings cheaper now here's a
nice little chart and finally we get to
see some red
I'm from Oracle and I've been at Oracle
for 11 years and red just looks good so
what am I trying to say here well I took
a histogram of the strings by their
length and so the first row says strings
of length 0
we've got about 20,000 of them in an
average heap dump why do we have that
many empty strings in a heap is beyond
me but we have them and that's something
you want to want to watch out for lots
of empty strings the mode or the most
commonly used one is for letter strings
so I guess these programs use a lot of
foul language in Java a char is two
bytes and what we noticed is most of the
time the upper byte is 0 this is a waste
of space so a potential future
optimisation in hotspot will use a byte
array to encode the characters inside a
string instead of a character array but
only if all the upper bytes are 0 if
they're not we have to use a char array
and this will cut the memory usage of
most of your strings in half and so the
statistics I'm showing you here is if we
do this optimization
we'll save about 14% of your heap cool
so I recommend that you switch over to
using utf-8 everywhere in your
applications for one this will ensure
your strings will be compressible since
the characters will be encoded as bytes
there's a CPU benefit as well to
encoding as utf-8 the Oracle DB uses
utf-8 as the default character set the
mid-tier spends CPU time converting the
array away from utf-8 sending the data
through the applications tier and then
converting back to you TFA to send it
out as XML or HTML so you spend a lot of
time doing these conversions and a lot
of your profiles will show utf-8
conversions our CPU profiles will show
utf-8 conversions so if you switch to
using utf-8 everywhere though chart at
could return a byte in the middle of
utf-8 character so it's not just a
simple switch it over and use it
everywhere you've got to be careful with
your programs so I'd like to read a
quote from the utf-8 article on
Wikipedia text in Chinese Japanese or
Hindi could take more space in utf-8
this happens for pure ticks but rarely
for HTML documents for example both the
Japanese utf-8 and the Hindi Unicode
articles on Wikipedia take more space in
utf-16 than in utf-8
so if your application is in doing a lot
of Japanese Chinese you might be
thinking well those are three
by characters I don't want to convert
over and consume a lot more memory well
if you're mixing it with a lot of HTML
it probably won't matter you probably
save space all right
next aspect of Strings duplicates about
13% of the heap 14% of the heap is
wasted because you have duplicate
strings in your and this is for an
average program now something that Chris
touched on is that there's something
called string in turn and what this does
is it stores the first string into a
weak hash table inside the JVM code then
whenever you do call string and turn
again you get returned that first string
and hence you can D duplicate so do you
want to do this for all the string
constants in your classes I wouldn't
recommend it because the JVM already
does it for you every time you load a
class this would be more for your
dynamic strings but you want to be
careful string in turn does have some
performance cost to it it's not cheap
it's not too terribly expensive either
but it's not cheap so you don't want to
put this in your performance critical
code because it'll slow down your
application also if you're running on
pre hotspot 8 then it will increase your
permanent generation space or perm gen
fortunately that was removed in hotspot
and it's no longer concern the other
thing that you want to be aware of is
that the string in turntable is a
constant size it will not react resize
like hash map and hash table will and so
you need to tune the size or your
performance will suffer we ran into that
and prompted the JDK team to increase
the size for server applications and
I've got the parameter there for you we
increase that size to about 60 K in JDK
7 update 40 if you're wondering how big
is this table you can use j CMD and it
will tell you how many strings are in
the table and the size of the table and
so you can and monitor your applications
that way hopefully in the future we'll
have a hotspot optimization
not sure if it will go in or not where
it will dynamically resize the table and
then this parameter will be pointless so
one idea that we had to get rid of
duplicates is to get rid of a the cart
charm rate inside stream since strings
immutable we can just reuse the same
char array everywhere and applications
won't know the difference
and so one trick we thought was if
you're comparing two strings and they
are equal well let's just make use the
same char array same thing with compared
to if is going to return zero meaning
that they're equal well let's just use
the same try rate in the save memory
there we're not quite ready to check
that in there's some questions that we
have about its performance and some
other issues so that may or may not go
in another thing that we're working on
that has showing a lot of promise at one
is to have a background thread go
through and deduplicate the char arrays
in the entire heap
so as GC promotes these strings we we
check that and start duplicating them
however a well-placed string in turn
will perform better because string in
turn will not only deduplicate the char
array it will duplicate the entire
string object itself saving more memory
for you however XML parsers and other
places where they're dealing with a lot
of strings really fast and performance
is critical those may not be a good
place for string in turn so obviously
there was a little bit of dental
counting between the two optimizations
compressed strings and deduplication of
strings and if we don't double count the
amount of memory left over after those
two optimizations are done is six
percent so this is a significant savings
that you could possibly get from your
applications hopefully soon
alright let's go talk about a raise
first one object array so the 6% that
I'm quoting here for the average usage
of your heap does not include any of the
elements stored apart that there are 8
points - this is just a raised object
arrays and these are these object arrays
are not in the collections these are
part of the program's themselves they're
creating an object arrays outside of
collections and using them and so this
is a kind of a concern although I'm not
sure what to suggest for an optimization
here however of those as Chris Bailey
pointed out many of them are empty just
not used one percent of the heap wasted
on empty object arrays again these are
not from collections and I highly
recommend that you go through your
program and lazily create your object
arrays now many times programs will
allocate these in the constructor so
they can make the member variable final
you get maybe a 10% plus remind us
something performance boost off of that
by declaring them final but if they go
empty that's going to cost you in GC as
well so it's a trade-off that you have
to carefully measure in your
applications now not only object arrays
but maybe string arrays or XML nodes
arrays or just any type of reference
array we were finding that a lot of them
about one person one point six percent
of the heap has nulls
in these arrays just filled with nulls
scattered throughout the array and in
fact by the word sparse I mean more than
half of the array is consumed with nulls
which seems kind of pointless why are we
holding on to a lot of nulls
so I highly recommend that for your
programs you need to go through and
compact the usage of your object to
raise byte arrays when I first saw the
statistics of 98.4% of the heap space
used I thought I've got have done
something wrong here
there's no way any application could be
running with that much actually there
are applications that run and they're in
design that way it comes from coherence
server and that's an in-memory data
cache a distributed cache and they store
the objects as byte arrays so of course
most of the objects will be byte arrays
because that's what they're trying to
store so for byte arrays I highly
recommend that you use direct byte
buffers if you're going to use the byte
array for any i/o the reason for that is
the hotspot whenever you go to do a
pile-on it has to copy that byte array
over to a direct byte buffer anyways so
why don't you save yourself a memory
copy and just dump it straight in also
hotspot has done a lot of work with
intrinsics to make sure that gets inputs
are going to be almost as fast as if you
are accessing the array so the average
size of these arrays is about 439 bytes
with about 2.7 percent coming from the
array header itself so we definitely
have some things that we need to deal
with with headers now you need to be
very careful with any type of object
pool especially byte array pools what
you're doing when you create a pool is
that you're betting that you can write
and maintain code better than the years
of work that have gone into hotspot GC
development and JIT escape analysis if
you don't have some piece of knowledge
that those layers could never obtain if
you don't have that key piece of
information then you might just lose
that race in the long run a pool can't
perform better if your code is creating
a lot of temporary byte arrays but again
just going out and changing those up for
pools may not be the best situation or
solution
for your system definitely measure
measure measure so sometimes many times
object pools are a very common source of
memory leak issues as well as excessive
memory usage and so in short expert
implementation is needed and you have to
be dedicated for the lifetime of that
pool to be maintaining it and making
sure it's performing well all right
duplicate primitive arrays now these are
interets or char arrays or well just
nothing that's a reference and what I
found is that eight point three percent
of the heap is wasted on duplicates now
I'm not sure if that's a test artifact
because a lot of times we will put a
data in the database and have all the
users log in as the same user execute
the same request pulling the same data
through so I'm not sure if this is a
test artifact or not and you might be
careful that when you're testing your
own applications so are these arrays are
these cherries part of string string
builder string buffer no these are
independent of those now what I found
very interesting is that I found one
heap dump where a double array had a
duplicate which is quite amazing since
it's really hard to get just two double
values to be equal bitwise equal but let
alone an entire array and that was quite
surprising I found another heap dump
with a duplicate float array so just
because it might be a real number array
don't toss it out and say oh they'll
never be a duplicate I found one so
there's not really much the JVM or JDK
can do here for you you have to go
through your program and do duplicate
these arrays empty primitive rays this
is where the array is completely filled
with zeros why are we allocating these
again I mean doesn't make sense to me
but so it's suggests to me that we're
pre-allocated but never using them again
lazily create these arrays if you
haven't gone too
some Landa or some other presentations
seems like lazy is the right way to code
your program so make them don't make
that don't let them do any work until
it's absolutely required the same can be
true with allocated memory zero tail is
what means there's some data nonzero
data beginning of the array but the rest
of the array is all 0 and about 5.3
percent of the heap has arrays that are
sized too long and are being
underutilized that's wasted memory just
filling it up with zeros so you need to
properly size your arrays and where
possible trim the length of the array
once you start removing elements
obviously that can have a performance
impact so be careful interets I bring
this out because I see about 3% of the
heap using interets I don't call it any
issues necessarily with interets but
just a caution watch out for them as
you're going through your heap dumps
collections so here's a nice breakdown
of all the different collections I found
in the average heap I'm not sure if
you're able to read the numbers but
hashmap takes the cake at about 7.5
percent of your heap just consumed by
hashmap this doesn't include any of the
keys or values being stored in a hash
map as Chris Bailey pointed out it's
just the hash map the array inside the
hash map in all the entries and
everything else built inside that hash
map to create and and you make a useable
hash map so one thing I also notice is
that 20% of the heap is spent just on
collections that means well we're using
a lot of data and putting into
collections which can be a good thing
but with that much of the heap being
used on collections it seems like we're
overusing collections and so let's get
into some of those problems
the first one is unused collections
about 5% of the heap is wasted because
the collection is created and never
during the lifetime of that collection
does it ever have
value added to it and we could tell that
from a heap dump there was a nice little
trick we figured out so hash map is used
in so I've got breakouts for hash map
with one point for an ArrayList for one
percent hash map is used inside hash set
linked hash map uses it as well it's
used quite a lot of places so now back
to the optimization that I was talking
about earlier what we did into JDK 7
update 40 and JDK is that we don't
allocate the internal array when the
object is in during the constructor
instead we're gonna allocate it when you
force us to whenever we call put or an
add on the collection we do preserve the
initial capacity so if you give that
it's not a waste of time we do preserve
that and we will use it when we create
that array however that only saves the
memory from the internal array what
about the rest of the class and we
haven't figured out a way to not create
those classes even though you're asking
us to so I highly recommend that you
don't create the collection until the
last moment possible again as myself as
a programmer I'll go through and say oh
final hashmap final this final that all
of my members because I know it'll give
me a performance boost but if I don't
use them I'm wasting my memory so it's a
it's a weird trade-off you've got to
play their small collections go ahead
so the question was is if you create
your hash map and then you have multiple
threads hitting your hash map are we
going to introduce a race condition here
hash map isn't designed for concurrency
so you're gonna have a race condition no
matter what we did with this
optimization it's gonna be terrible
however concurrent hash map Doug Lee did
smooth some optimizations there and he
did take he did watch out for that
concurrency issue right in the
concurrency case so obviously if you
you're gonna put this in a synchronized
block and then you don't have to worry
about it so what is a small collection
it is a collection with four or fewer
elements and so the bytes the percentage
the numbers here I'm presenting is the
overhead of just having the collection
it may not mean that you're wasting
space but you just have that collection
there so we kind of need with that few
elements it kind of doesn't make sense
to have a collection if you already know
ahead of time what the keys are at least
a few of the keys are gonna be and it
would make a lot more sense to put the
key make a field inside your class
instead of having it inside the hash map
might cost you a little bit and
performance though or just get rid of
the hash mapping completely and you
might think well why don't I just go use
gold sacks instead and get rid of this
problem no you're just trading one for
the other and if you or maybe I
shouldn't write my own hash map that
might work well then we can't help you
from the JDK point of view or the JVM
point of view and so that would that
could actually hurt you in the long run
so here's a nice little breakdown of all
the different small collections I found
in the heap hashmap taking the case cake
it about 1.5 percent of the heat being
used then concurrent hash map and so
forth and so on
small sparse collection so small
collection is having four or fewer
elements sparse means that the capacity
is two times or more than what you need
for example if I have an ArrayList and
I've only got four elements in it and my
capacity is eight or more I'm wasting a
lot of space and so the two-percent here
is just that extra wasted space meaning
these the collections just weren't sized
appropriately for what they're gonna
hold and a nice little breakdown with
concurrent hashmap taking the cake now I
have to ask the question a concurrent
hash map is because we're going to be
hitting it concurrently with a lot of
threads but we only have four elements
in it so it most we're going to be able
to get a concurrency of four out of this
thing why are we having a concurrent
hash map with so few elements it doesn't
quite make sense and concurrent hash map
is a lot heavier than hash map than
planed hash map in terms of memory so it
doesn't make sense to be using a
concurrent hash map in all cases you
really need to make sure that you block
your synchronized block is a problem and
it makes sense to use a concurrent hash
map and just don't throw it in
willy-nilly all right array lists about
two and a half percent of the heap is
being consumed by the just array lists
now I've got two more tables here
shallow and diff
shallow is just the ArrayList instance
itself nothing else nothing nothing of
the other machinery that it's using
whereas diff is all the other machinery
so most of the cost for array list is
all the other machinery inside ArrayList
such as the object array so what I was
able to pull out from this statistic is
that the average size of an ArrayList is
about seventy six point two bytes and
that gives us about twelve point one
elements per ArrayList now the default
capacity of an ArrayList is
so on average your ArrayList is not
sized properly and you'll need to set
the default capacity appropriately now
does that mean you need to do a grep
through your code or replace everywhere
with your with constructors with 12 I
don't think I would recommend that not
with the discussion about small
collections that we just had so here's a
breakdown of array lists and all the
different problems with them 1% of the
heap is wasted because of its unused
it's just not being used another point 7
is because it's small in well if we got
rid of the ArrayList somehow we could
save that memory small sparse 0.3 is
being wasted because it's just to size
to large empty but used 0.1% of the heap
and then the amount of space being used
by array list holding box numbers 0.1%
and then the other is not necessarily
poor uses of ArrayList it's just
everything else including actually good
uses of ArrayList got to talk about hash
collections all right hash map I already
showed you 7.4 in one slide 7.4 sorry
7.5 one slide 724 a little bit of
rounding going on here so what I was
able to find out that the average size
of this hash map is about 257 bytes and
that yields us about 7 entries per hash
map with a load factor the default load
factor that means the ideal size for the
initially capacity would be nine point
three elements hash map requires the
tail size to be a power of two and
that's for performance reasons when
trying to hash into the into which
bucket so the current default initial
capacity is of 16 is actually perfect
for the average case but for the
outliers it's not
all right so I'm not sure how much I
want to spend on this because Chris
Bailey talked about it quite a bit each
entry at least on hotspot costs 24 bytes
now to be able to point to that entry
I'm not assuming there's any chaining
going on here we need 4 4 bytes for the
reference from the table again I am
assuming 32-bit or compressed hoops here
with a load factor of 75% that gives us
29 and 1/3 bytes per key value pair you
add to your hash map that's quite a lot
of overhead if all you're just doing is
adding intz and doubles to your hash map
and also I give you a nice little
formula that you can use to compute your
hash map but it assumes your capacity
and size are the same so I started
thinking about well how can we improve
the memory usage of hash map well open
addressing is another way but depending
on the size and the capacity it turns
out that it's about the same amount of
usage so it doesn't really buy us
anything
another way is separate chaining with
lists heads and that actually costs more
memory so if any of you have some really
really smart ideas on how to improve
hash map please by all means come hack
on the open JDK and help us improve
hashmaps efficiency so here's a nice
breakdown of hash map and some of the
problems with it now large sparse means
that the that we are above the default
capacity of the hash map so for some
reason the capacity is larger than 16
but sparse means we're using less than
half of that and so that tells me that
0.4% of the heap is wasted because at
one point we needed the hash map to be
larger but now we don't need as many
elements when we took that heap dump
concurrent hash map the average size is
1.4 kilobytes now compare that to hash
map of 257 bytes that's quite a lot of
memory that we're talking about there
fortunately as I mentioned earlier Doug
lee has gone through and introduced lazy
initialize for concurrent hash map and
has taken care of the
currency issues within allocating these
segments and the arrays inside but also
he's gone through and decreased the per
entry overhead and I don't have those
numbers I didn't have any heap dumps to
represent that here but it should be
much cheaper now so here's a nice little
breakdown of concurrent hashmap linked
hashmap so linked cash map lets you know
what was the insertion order of the
elements into your hash map if you don't
need to know that insertion order then I
highly recommend that you switch over to
hash map because it has a much lower per
entry cost also linked hash map is going
to benefit from the lazy initialized
hash map and ArrayList optimizations
because well it related initialize the
hash map that it uses here's a nice
little breakdown and I'm just gonna
quickly go through it for sake of time
hash set also benefits from the lazy
initialize hash map and about six months
ago I presented this information to the
JDK team and Mike do go went through and
decided he just had to do some coding
that day and so on a Friday he went
through and fixed the problem and I
think he got it checked into seven
update for T but don't quote me on that
and the problem is is that if you use a
hash map for hash set you've got a key
and value reference inside your entry
but you only need one of them and so he
fixed that and got rid of one so we
saved four bytes per entry
here's a nice breakdown of hash set hash
table
so hash-table was replaced back in jdk
1.4 that was so last decade why are we
continuing to use hash table maybe as
chris bailey said there's just people
that are hash table people's I don't
know I highly recommend that if you are
a hash table person to stop it now it's
a bad habit the reason for it is that
hash map has a lot of JVM intrinsics to
make it run faster so if you do a get
it's not the bytecode executing it's
actually an intrinsic executing which
will perform much quicker hash table
doesn't have it also hash table as you
all know is synchronized and many times
the synchronization just simply isn't
needed hotspot tries to mitigate that
but again it's still gonna have to check
to see the thread it has to check to see
if it all ends in the lock every time it
hits it but in any case even if you just
replaced all your hash tables with a
synchronized hash map you're gonna get a
performance boost out of it now here's
the breakdown for hash table
so all those pie charts I threw at you
here's that same data in a table just in
case your a table person stead of a pie
person personally I like to eat my pie
ins to beat my tails well so the most
significant problem is small collections
there's not much we can do to help you
there we needed to go through and
reevaluate why you are making this life
choice of using a small collection the
other the next problem is unused and
we've mostly solved the problem for you
and will will reduce that quite
significantly but we still needed to go
through and reevaluate your unused
collection problems other topics rarely
used fields what in the world am I
talking about here a rarely used field
isn't a dead field meaning a field that
you never use and is just sitting in a
code wasting space that's not the case
and in fact if the JVM can prove its
dead when you're loading the class then
it won't even use it in the instances so
that's taken care of what I'm talking
about is a field that he's used maybe up
to ten percent of the time in some of
your instances the remaining ninety
percent
it's just null or zero just sitting
there to do nothing that suggests to me
that we're not being we're not deriving
these classes off enough we have one
base class maybe it's doing too much and
maybe it would be better to create a
subclass off of it that has those fields
whereas the base class does not I
realize that's not always easy to figure
out in your applications when you're
gonna need one or the other however
we're wasting almost seven percent of
your heap because you're just not using
these fields they're lying around
hopefully in the future maybe JDK 9 or
10 or 11 whenever we can get JEP 159 in
where we can read I Namek lis change the
classes on you behind your back then
maybe we can do some optimizations here
to get rid of these fields at least
temporarily for you so here's a
histogram where I took the number of
classes they had say one field or two
fields or three so forth and so on
that had rarely used fields in them
that's quite a lot of classes with
rarely used appeals so this is not just
a simple one class here there that's got
a problem and it's just consuming a lot
of memory this is systemic across the
board so you probably have a lot of work
ahead of you just to go through and
figure out you're rarely used fields
what I find very surprising is that
there were two classes that had 344
rarely used fields in them that is a lot
of fields that just go unused this
suggests to me that someone is using the
kitchen sink empty pattern a little too
excessively over sized fields these are
fields where the upper bytes are zero or
in the case of a negative number they're
filled with FF the hexadecimal FF
in not just 90% against this but all
instances across the board so that makes
the profiting the problem a little
easier for you now what this means is
you might have a long when actually you
could have gotten away with an INT or
short or even a bite and so when I was
thinking about this before I really
stumbled on this I was thinking well if
I'm writing my code I'll think I'll just
put an int here then I'll think about
well maybe I actually need a long and
I'll think it through and go oh yeah I
do need a long or maybe I don't need a
long but rarely would I ever think oh
maybe I can actually get away with a
short or a bite here so you need to
think about how your encoding your data
into your applications now using tricks
like relative borrow a term from C++
land bit fields is gonna cost you a
little bit in performance so you have to
do don't go overboard and just stuff
everything into one long and hope
everything works out well so again
here's that another histogram where I
took the number of classes with
oversized fields in them most of the
time one or two classes had a field or
two that was a sorry a few color many
classes have only a few o'clock one or
two fields that are oversized so we're
doing okay with maybe a few problems
here and nothing like the 344 oversized
fields but still three classes had 49
fields that were oversized so definitely
a big class all right now I'm bringing
up the JDK method from reflection why am
i bringing this up well when I'm not
talking to the JDK team well what I'm
finding is about 1.1 percent of the heap
is used by method instances and
unfortunately these are not immutable
strangely enough so there's not much the
JDK can do to help here or the JVM we
can't go through and deduplicate them
for you so you need to look through your
application and figure out why are you
holding on to these methods and it'll
become more apparent that maybe you
don't need to now to bring in some other
data from other slides the rarely use
fields those account for about 18% of
meth
memory usage so maybe the JDK team has
some things that I could do here but
it's going to be hard for them to fix
the problem
now that the API is public is can and
they it's gonna be hard for them to rip
out the functionality on the other flip
side about 86% of the methods have
really a rarely used field in it so
definitely some issues here to think
about so why am I still talking about
method when I'm not talking about the
JDK team here well here's a list of
fields and how often they are rarely
used in a heap dump and what security
check cache does is if it's null it
means that the method has never been
invoked during the entire lifetime of
the method instance it doesn't get GC it
or cleared later it just means it's
never been vocht
so that then raises the question well
why are we holding onto methods that are
never being called why create them in
the first place again get back to making
your collection lazy or make lazily
creating your methods so let's wrap this
up a little bit so I've got a bunch of
optimizations up here deduplicate
strings is the number one optimization
that we can take care of duplicate
primitive arrays is not what at one I
can take care of so that falls onto your
shoulders to look at one I didn't talk
about was compressed object headers and
if you attended the Chris Bailey talk he
talked about object headers taking about
twelve bytes on a 32-bit platform or
even up to 24 bytes on a 64 byte 64-bit
platform in hotspot the situation is a
little different for 32-bit we use eight
byte headers for 64-bit we use sixteen
byte headers but if you use compress
references we use twelve I'm not sure
how that works out in code so object
headers are a significant problem and we
have some ideas of how to prune the
object header or get or reduce it but
it's going to take a lot of effort
because we've got a lot of information
in that object
from locking to hash codes to all sorts
of things and we've got to figure out
alternatives to those yet still stay
performant small collections are costing
us compress strings or tell anything
else here that I haven't talked about
yet unused high byte fields that's the
oversized fields I just renamed it and
didn't update the slide box number
collections didn't really talk about box
numbers I threw it in here 0.3% quite a
bit of significance there so as you
remember I talked about how I thought
Java programs took twice as much memory
as C++ programs well if we did all of
these optimizations and did them well we
could save 63 64 percent of our heap
space so now Java would have the upper
hand on C++ and it wouldn't be likely
that C++ could do some of these
optimizations but then again I don't
have any real data there to compare the
two languages but so be it also if we
save that much memory then think about
your data centers or in Oracle Oracle's
case multiple data centers and if we
could cut your hardware in half because
you don't need to run as many machines
that's a lot of savings I mean you're
gonna replace your machines every three
to five years if you're a typical
company so that's half your data center
costs right there saved power AC all
sorts of other things half or at least
more than half so definitely worth
millions just for a single data center
so if you've got it a data center like
I'm thinking you're gonna save quite a
significant of money over the many years
by doing these optimizations one more
thing sorry if you look back at most of
these optimizations most of them have to
do with getting rid of null and 0 it
tells me that we've or just holding a
lot of nothing in our heap dumps and we
just need to get rid of nothing
so someone took a raw heap from the JVM
they didn't dump the heap into another
format they just took the raw bytes
right out of the JVM and they didn't
they compressed it with a standard
compression algorithm the heap
compressed down to 1/3 the original size
and that portion is represented by the
gray section on the left or what I
called difficult so it I call this part
difficult because I feel like it's going
to be difficult to do better than a
compression algorithm not impossible but
it's going to be difficult now a good
portion of the difficult space is going
to be due to what remains after we do
these optimizations for example if we
take care of objects headers we're still
in a way I have envisioned we're still
going to be using about 7 percent of the
heap on object headers strings about 6
percent rarely used fields about 3
percent so another 18% and stuff that I
just didn't bother to categorize now the
right side wow that got washed out is
supposed to be blue it's blue on my
screen thanks so the right side is
what's optimizable obviously the JVM and
JDK have a lion's share of what can be
done
however this segment down here the 17.3%
that's you guys we can't do anything
with that nothing so if you want to
optimize memory that's what you can go
after this next segment here is what
really programs should deal with like
lazy creating your stuff but the JVM and
JDK might be able to help out here so we
might be able to shave off some of that
5% now why did I say space the final
frontier besides being a Trekkie
well the final frontier for figuring out
where to optimize memory is this 3% that
I haven't identified yet if we can
assume that the compression algorithm is
as best as we can do so there's not much
left out there to go and find this is it
so this is all you need to look at
inside your
locations if you're an average
application alright so I threw a lot of
ideas at you of what you can do and to
go through your applications and I
compiled them into this optimization
task list just so just for easy
reference so you don't have to flip
through all the slides one thing I
didn't callout throughout this is that
you might want to compare your heap dump
for your application against the
averages that I've quoted in this in
this presentation that way if your
application is using more than the
average you know you've got something
really actionable to focus on you should
focus there first if you're taking the
app average if you're just the same as
an average well you've got some things
that you need to work on and I've given
those to you and we now pause for
picture taking
I'll give put my best smile on and one
more slide and while everyone's taking
pictures does anyone have any questions
they'd like to ask in front
so at the very beginning I called out a
tool I used and I'm going to go back
there for reference so you can write
down the session and all the way back
that's it J overflows what I used that
will do all the analyses I showed you
and tell you how much your application
is using in your programs
Wow it's a little noisy so how did I
figure out if a collections being used
or not well iterators are fail fast
right how do we implement that into the
j2k well inside the collection we have
what's called a mod count variable and
every time you do an update or change to
your collection we increment that it
gets initialized to zero in the
constructor and if it's never been
incremented we know the collection has
never been touched so that was the trick
it took us a while to figure that one
out good so the hashmap optimization
meeting the lazy initialize hash set
with the four bytes
that's a good question does it help I
don't know for sure you've got B doesn't
help because we've got to align and it's
already a 24 bytes structure it may not
help but hey if we got rid of it we got
rid of it if we can find something else
then then we'll help
now the other place that will help is if
we go through and do if we compress our
object headers down to 4 bytes instead
of 8 then the offset will definitely
help us there a question any other
questions am I missing please
so the question is does the garbage
correct collector only run when there's
a shortage of memory and can it run
continuously in the background
concurrent mark-and-sweep
does run concurrently with the program
as well as g1 has several parts where it
does run concurrently with the program
so that already happens and does it only
kick in when it's running out of memory
yes once you fill up your completely
young generation GC will kick in and try
and wipe out as much as possible of that
g1 does go a little bit ahead of
schedule so instead of wait until young
collection is done it starts doing its
mark phase before that point so that
hopefully by the time it's done with
this marked phase the young collection
is exactly full we pause the application
right then and we can quickly clear it
out a lot faster because we're able to
predict when it fills up other questions
please
alignment waste how much right right how
much waste do we have from alignment I
don't have an exact number but there's a
nice way we can calculate that if we go
to heap usage we have sixteen and a half
million objects on average in a heap
with a eight byte alignment that means
you're going to waste about four bytes
per object we can do two alignment so
for x 16 million that's about 64
megabytes so that would be about six
percent of your heap from waste due to
alignment
so you waste memory when you derive
classes how I obviously have to have the
class definition oh yes
so there's internal padding so what
you're getting at is if you have a bunch
of lumps in your base class or sorry a
bunch of duck bytes in your base class
and it's a ragged end then when you go
to your derived class we have to pad to
the next 8x8 alignment just to be able
to put that long in and get it properly
aligned and so yes definitely you'll get
some waste there too didn't think about
that that's a good point please
so the question is do most enterprise
code 80% of the code is frameworks or
application tier or jdk in fact do I
really need to worry about my
application code I mean it's only just a
little bit well we probably about 20% of
the heap is yours to claim just a raw
number but more importantly it's how
you're using the underlying stuff for
example yes ArrayList or the collections
are consuming 20% of the heap but that's
really because of the application and
what the application is doing or maybe
the application tier and frameworks are
doing so you need to really look at what
you're doing and how you're contributing
into this problem of using extra memory
and frankly there's things that you can
do I mean even in strings deduplicated
strings that's something you can do and
can save with a lot of memory that the
frameworks won't be able to do for you
more questions please
so the question is could test artifacts
be skewing these results and can we
assume that these are coming from QA
environments and not production
environments I can almost guarantee you
very few are coming from production
environments usually our production
teams don't file as many bugs so
percentage-wise no
right right and hence each users hitting
it now in my team we try to simulate
large user loads and so we do try to
replicate a production environment as
much as possible however we never
considered duplicate memory and so that
might be a test artifact so but it does
lend itself well to when you're doing
your own testing make sure your tests
are as close to real world as possible
because it may send you down red
herrings and you might spend months
developing features and not optimizing
memory in your test you shrunk your test
by 50% and production you did nothing so
real work
it depends on the optimization for
example the compressing the object
headers that is going to be a lot of
work and I don't think we'll see it
anytime soon maybe three or four J
decays later we might see that one as
far as say D duplicate strings by the
background thread that one right now our
current plans maybe some update inside
j2 k-8 but again we have to go through
the run wringer to make sure that that
feature is not going to hurt anyone or
at least understand it well enough that
we don't hurt anyone yes it'll
definitely be really snow today well we
probably won't quote a number of how
much it'll save you but we'll definitely
say look out we are duplicating the
character rays inside your strings now
one thing that John Rose and Mike to go
and I were talking about after earlier
in the week we were thinking about what
about breaking the identity of strings
so no longer you can say you can make
sure that two strings you can't create
two different strings and make sure that
they're never right entity or they're
never equal bit why our reference equal
we're thinking about breaking that in a
later jdk as one of optimizations so
that no longer we do we have to get rid
of the char right we can get rid of the
extra duplicate string as well and then
pile on some other fancy tricks of
having a string but then the character
data is right after the string object so
we can get rid of the character array
header as well so there's a lot of fancy
tricks that maybe we'll get to
eventually the string equals and
compared to optimization for
deduplication the background thread
works so well right now we're going back
and trying to evaluate well if we have
that in place how much could that save
us the problem with that one is the two
strings actually have to come together
and be compared and many times strings
just live in their own little worlds
separate from all the other strings and
never get compared so it may not save us
very much and so it may not even be
worth doing because it is going to cost
you some performance after you see that
they're equal
we're going to still be executed aside
the equals operator just to do the
deduplication of the character raise its
associated cost you learn with
performance so we just might avoid that
and put that in a background thread for
you good question
other questions oh sorry so the if I
understand the question by doing
background thread deduplication we will
be able to improve cache performance or
we're gonna struggle with cache
performance right so for that
deduplicating thread is going to be
experiencing a lot of cache misses of
course because it's going to be pointing
off to all sorts of different places in
the heap but for your actual application
threads they're actually going to be
improved performance because now you've
got multiple strings play to the same
character right and able to get better
cache performance there's a question
back here okay please
so can the JDK you do UTF sorry utf-8
versus utf-16 just behind your back
automatically not unless we do a major
JDK release on it and say from now on
your strings are all utf-8 however we're
very concerned because we have the chart
app method and you can walk through the
string character by character we might
just completely confuse your application
and cause it to crash so it that would
cause it that could cause a lot of risk
to it right right and that's why because
if a code point comes through and your
application doesn't expect it you're
gonna be in trouble
next question
alright well thank you for coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>