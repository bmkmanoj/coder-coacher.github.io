<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CON7547   Beyond the Coffee Cup: Leveraging Java Runtime Technologies for Polyglot | Coder Coacher - Coaching Coders</title><meta content="CON7547   Beyond the Coffee Cup: Leveraging Java Runtime Technologies for Polyglot - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>CON7547   Beyond the Coffee Cup: Leveraging Java Runtime Technologies for Polyglot</b></h2><h5 class="post__date">2015-12-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Wv3VcJL5OdI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome everyone this is beyond the
coffee cup leveraging Java Runtime
technologies for polyglot my name is
Daryl Meyer I'm from the IBM canada lab
in Markham which is just outside of
Toronto I'm a member of the IBM runtime
technologies team and I've been working
on compiler and optimization
technologies for about 20 years now on
various different kinds of compilers for
the last 15 years or so I've been
working on the Java just-in-time
compiler technology that's part of the
j9 Java Virtual Machine and for the last
two years I've been leading a I guess
it's not so secret anymore project to
open source that compilation technology
that we have and make it available for
other purposes other than Java so
standard legal stuff so lawyers can
disavow anything that I'm saying so a
few months ago my colleague mark stood
Lee went to JV MLS and made a fairly
bold announcement and that is that IBM
is going to be open sourcing a runtime
technology tool kit and I really
encourage you to if you're interested in
this sort of thing I really encourage
you to go listen to that talk you can
find it on YouTube there's a link here
in the deck as well if you want to find
it that way but you know we really laid
out the what it is that we're trying to
do laid out the motivation for why we
want to do it and why it makes sense for
the technology that we have and the
environments that we want to get into so
as a result of that of that announcement
we kind of expected a couple of
different reactions so one way it could
have gone was oh great we've got another
runtime environment that we have to
worry about and you know there's the
waters are already full already so you
know what I'm going to do about that but
the other way it kind of gone was you
know there's actually quite a bit of
enthusiasm around that kind of an
announcement and fortunately since that
time we've actually had some fairly
positive feedback around the direction
that we want to take this technology and
you know how we want to get this
integrated into other runtimes out there
so so in marks talk he actually did go
in a fairly good detail as to why we're
doing things and I don't want to cover
that in this talk at all but i do want
to hit on some of the high points of why
we want to take the technology that we
have and and an open source it so we
really want to be able to experiment
with leveraging our you know 15 16 year
investment in j9 and and we really want
to do this in a way that can facilitate
taking that technology that we have
developed for java and integrating it
into other environments as well one of
the key messages that that we want to
hit on a lot is that we really want to
be compatible with these runtimes and
their communities there's a lot of cool
things that are happening in a lot of
these different languages and these
communities and they don't really want
to tolerate any sort of disruption and
you sort of disruptive technologies we
don't really want to tell different
communities you know you have to change
your environment to match this
technology we want it to be the other
way around right wheel the technology to
be flexible enough so that it'll Bend to
however it needs to be used in these
different environments and the other
advantage of that is that it actually
will work well with a different
technology with the different things
that make those language makes that make
those environments great in the first
place so for example if you have
extensions or different kinds of
additions to the language all that
should just work right and the other
thing is that's really driving this is
you know we really want to have a very
simple consumption model right if it
takes a lot of effort to grab some new
technology and then port your runtime to
it that's not really a great story in
our mind the way that we kind of want
things to be used is that it's got to be
very you know sort of pick it up and you
can mix them relatively straightforward
changes and you can just start using it
and there's be lots of examples that you
could follow if you want to do that sort
of thing
so we want to unlock the vm part of the
j9 Java VM right so for the last couple
of years we've had some development
teams working on looking at what we have
in our JVM technology and figuring out
how we can actually create a language
agnostic toolkit from that and what we
can do with that technology once it's
out is reintegrated back into other vm
is and this is including the j9 JVM and
you'll see in a sec that we've actually
done that so the kind of kinds of
technologies that we're talking about
here are things like memory allocators
and thread libraries and os port layers
and the garbage collector and JIT
compilers all the cool stuff that make
up modern runtimes these days so all
that sort of is really fair game for for
this sort of experimentation but you
know pulling that style out is really
only half the story right I mean it
doesn't really make sense to do that if
we can't actually use it if it doesn't
integrate well with anything so we also
had to conduct a number of experiments
to see how feasible and how easy it
would be to take these different
components and then integrate them into
other environments so we happen to
choose a number of different test beds
for this sort of technology we picked j9
we pick Ruby MRI the reference
implementation we pick C Python and both
late we've done some work with with C
psalm as well so we really wanted to
gauge the effectiveness of the of this
kind of an approach by by doing that
kind of an integration and one of the
things that we realized fairly early on
by doing this kind of you know this kind
of investigation was it's not a small
task at all it's actually a fairly
involved piece of work and it's not the
kind of thing that we can do on a side
branch right so what we really found was
that you know in order for us to do this
work we really had to work directly in
our head stream so alongside all the
other developments using this technology
we had to work along with it so that you
know really put on a number of different
constraints on what it is that we were
doing and what we could do and what we
couldn't do but they're actually worth
some benefits for
doing that as well right so one of the
benefits was that you know as because
we're in head stream we're going to go
out with whatever product happens to be
going out so you know earlier this year
we shipped an IBM GDK 8 and it's got a
lot of cool features in it's got a lot
of good quality and you know what came
along for free there was you know a lot
of the stuff that we've been working on
here and you know similarly looking
forward if we're talking about jdk nine
or beyond you know a lot of the stuff is
underpinning that as well so when we're
looking at the technologies that we
wanted to pull out of j 9 the kinds of
things that we've been looking at our
new utility stuff thread libraries OS
port library things common vm structures
and tooling things and you know once we
kind of have that out you can use those
pieces as building blocks for
integrating some of the other
technologies that we have so things like
trace engines things like garbage
collection technology things like adjust
in time compiler technology and you know
it's really up to the environment as
well to sort of pick and choose what
components you really want to use right
so if I'm only interested in integrating
GC technology into my application to my
runtime and I don't really want to jet I
can just build a GC or the other way
around if I just want to build a jit I
just want better performance let's say
then I'll just grab the jet technology
and go with that so I mean here are a
few proof points it's rochelle look like
the kind of things that we've been up to
right so IBM has this tool called health
center it's sort of a graphical tools
traditionally been used in java that
really allows a user to sort of monitor
the state of a running java application
and you know it has it has an interface
with trace points that will allow it to
basically do method profiling right you
can figure out where time is being spent
in your application so what we did is a
fairly simple experiment was to take
some of the trace point technology that
we had sort of extracted from the j9 JVM
and integrated it into the Ruby MRI
environment and we were actually able to
get trace points working relatively for
free so I mean it is a quick little
snapshot of what that looks like garbage
collection technology you know it we did
want to show that this technology could
be used in other applications as well
we've actually had some fairly good
success with that so what we wanted to
do as a proof point was to take this GC
technology and then put it into Ruby MRI
now the technology that we're starting
with the j9 Java GC technology you know
it's it's type accurate it had does all
the really cool things you'd expect a
modern garbage collector to do with
parallel it's concurrent it scales
really well it's got no tracing stuff
associated with it but the thing about
when we're using it in javas we're using
it in sort of a very type accurate sense
and when we're talking about Ruby the
the GC technology and there is a little
bit more conservative than that and part
of the reason for that is because of a
lot of the sea extensions that they have
that consider reach in and twiddle with
objects and you know unbeknownst to the
GC so the guard any sort of garbage
collection technology there has to be a
little bit careful about that and but
anyways we were able to actually get a
conservative GC up and running and the
good thing about that as well the good
proof point about that is that you know
we didn't have to bend the Ruby VM to
make it very type accurate we actually
went the other way around we actually
changed the technology that we
integrated to make it a little bit more
conservative another benefit that we got
from that as well is that we were we
were able to change the way that Ruby
allocated objects on the heap right so
you know right now in NMR I if you
happen to have your objects are of a
certain size you know there's really
just a point to their that points to the
data that's actually hanging off the
heat and the problem with that is that
it's really hard to get a good
assessment of how much memory how much
heap your application is actually using
so you know with a bit of work and
integrating the tech
that we have we were able to pull those
objects back on to an actual heat so
that you can get a very realistic
assessment of how much of how much data
is actually involved in your application
so once that was done you know we get
some very good things for free so I got
showing here some sample data on the
verbose GC technology that just sort of
comes out for free we can plug into
other tools as well right we've got GC
visualization technologies part of
health center you get that for free and
we've got some more visualization
perspectives from for Ruby as well so we
get that as for free as well under
health center we did also look at taking
the JIT technology that we're working on
and seeing how far we could go with that
and the environments that we chose for
that well we we we tried to pick
environments that really didn't have
technology in there right now and the
you know we picked the reference
implementation from Ruby and we picked
the reference implementation for Python
and traditionally both of those
environments have been particularly
challenging for JIT compilers I'm not
going to say that those like Ruby and
Python don't have JIT compilers for any
implementation it's just up those
particular ones have been particularly
difficult than that in the past and the
reason for that is that you know these
languages are very highly dynamic a lot
of things change even more so than Java
in some in some respects and also when
you're out when we're running these
things there's a lot of you know direct
use of some internal data structures so
when I when it when a jit wants to be
able to optimize code you know it has to
be able to reason about things it has to
be able to prove certain properties and
if it's trying to make decisions like
that but the state of the world can
change because you've got some ski
extension that can do something
completely bogus on you then it's makes
it really difficult to to compile that
code in an effective way and also there
are some design choices when they within
these environment within these runtimes
themselves that make things tough like
for example in Ruby they use such up and
long jump in some cases for doing some
kind of control flow
so the work that we have done to date
here was to really look at how far we
could go with a technology that we've
that we've extracted so far and you know
we're really just compiling you know
methods and blocks and these different
in these languages down to native
instructions we didn't want to have to
make some fairly significant changes to
how MRI works or how C Python works
because really wanted to show that it
can be easily a dot integrated into
these into these environments also the
way that a lot of our compilation
technology works is it really does
support a mixed-mode kind of execution
so you can have methods that run with
the interpreter you can't have methods
at compile that run with the JIT and you
can mix and match those kinds of things
so we want to make sure that any kind of
compiled code that we end up producing
in these environments runs equally well
with the interpreted code that the
theater already does and we wanted to be
able to run you all the extension
modules that that make these you know
language is really really powerful with
their communities and and of course you
know we didn't really do any benchmark
tuning it's basically you know what can
we take what can we plunk in and you
know whatever performance we get we get
you know there's certainly a lot more
work that we can do there's a lot more
low-hanging fruit that we can go after
but there are a couple of success
stories with this right so from a
compatibility point of view you know we
can run rails right so we can we can get
that going pretty you know we run it
it's not an easy thing to get to run but
it but it can run and also from a
performance point of view doing
relatively little amount of work we can
get about a 1.2 x on on many of these
bench 9,000 kernels that are there
available for Ruby I can find ones that
i'll give you like 5x or 10x
improvements as well but you know on
average we're getting about 1.2 x or so
plus we're getting the diversity of a
multiple like a bunch of different
architectures as well write like this is
running on x86 it's running on Power
Architecture it's running on the z
mainframe as well so there are a lot of
good sort of success stories already
with that
the final proof point that I wanted to
mention is the IBM JDK 8 so this shipped
earlier this year it's in some sense one
of our best releases so far in terms of
functionality that was released with it
and performance it did coincide with the
z 13 processor release earlier on this
year so a lot of stuff that we did it
was really it was really for that so
there's a lot of cool stuff there but
you know the real message here as well
is that you know underpinning all of
this because we were working in the head
stream a lot of the stuff that went out
there you know we was really underpinned
with a technology that we had we had
extracted right so it's a it's a very
good proof point for the quality of that
work and the performance of that work
and how well it can actually integrate
with an existing environment so all of
this is great but it doesn't really make
any sense that we just sort of keep it
internally within IBM right so what are
what our proposal is is really to open
source this kind of technology so that
others can start to we can start to
build a community around this and that
you know things will take off from there
so what Mark proposed back at JV MLS was
really to create an open community of
contribution based around these toolkit
of components that can be used to build
a language agnostic toolkit and this
will be a great place for you know
individuals communities corporations you
really to collaborate on the
infrastructure that you can use to build
other VMS and to supplement other VMS
and you know what what we what we want
people to be able to do is to basically
take the building block pieces and
innovate on those as opposed to having
to reanimate and rebuild all the wheels
all the time for every vm that we happen
to come across you don't want to be
reconstructing things from scratch every
time so we want people to focus on
innovating to what matters to them
and this technology will actually become
much more robust on its own though right
because we've got a lot more hands in
the code a lot more eyes on the code a
lot more projects that are being built
from it it's going to get tested in a
lot more different a lot more scenarios
than may currently be happening right
now and bugs will get fixed a little bit
quicker a community will be a great
place to have a lot of collaboration on
best practices you know documentation
you know conferences like this and other
sort of shared learning opportunities as
well and perhaps maybe one of the best
best advantages of this open community
is that for the smaller V ends the
smaller communities out there that
really want to sort of expand in an
experiment with different kinds of
technologies like garbage collection or
technology or tooling that sort of thing
we're really lowering the entry barrier
for those kinds of environments and you
know provide providing these sort of
capabilities that they can take and
easily integrate into their into their
environment so finally I get to the you
know what I'm going to be talking about
today so so marks JV MLS talk really
focused on the wise and the what's of
what we're doing right so we talked a
lot about the proof points we've had
talked a lot about the motivation we
have what I want to talk a little bit
today is about the house and and how
we're going to be doing certain things
so this morning my colleague Charlie
Gracie gave an excellent talk on the
garbage collection side of things and
some of the some of the experiences that
we've had with taking the GC technology
out and then integrating it into other
environments and you went through a
fairly detailed example this morning
about like using C song for that I'm not
going to be talking about GC technology
anymore I do want to talk about the JIT
technology that we have though because
it is something that we've had a lot of
questions about and it seems to be an
area that's that's that's fairly
interesting to a lot of people in terms
of where we are with a jit technology
it's not nearly far as far along as as
the GC technology is at this point so I
can't
show code or things like that I can't
talk a little bit you know abstractly
about some certain things but what I do
want to talk about is you know the the
experience that we've had with with
taking that technology and and making it
ready for open source and some of the
challenges that we've had in that kind
of an environment so one of the first
things that we've got a problem with is
you know are there enough reusable
components in a Java just-in-time
compiler to even build a polyglot
toolkit right what are our chances of
success here so one way of of answering
that is really to start looking at you
know what is the sort of flow that
really worth that we really need to
follow in order to compile a method for
java so in the case of the IBM j9
just-in-time compiler what do we have to
do to compile a java method so initially
you know we have to figure out what what
is it we're going to compiling so in
this case we happen to have a sampling
thread that wakes up every so often to
try to figure out what methods are
actually running on which threads and
you know it's collecting sampling
information based on these different
places where it finds the execution is
occurring and what it can do is actually
analyze the samples that are that it's
collecting to figure out which methods
are worthy of compilation and how much
compilation to throw at these certain
methods and so the ones that are chosen
for compilation get added to some kind
of compilation q somewhere for for
asynchronous compilation and then you
know we've got a compiler sitting out
there as well that basically sits there
something comes into the queue grabs it
and starts to compile it we have to take
the byte code for that method and turned
it into compiler I are the internal
representation that we have it then
needs to decide how much compilation
should he throw at right so in so for
example you know if we happen to know
that this method is really really hot
it's a really important method maybe
we want to throw the kitchen sink at it
and spend a lot of compile time budget
compiling this thing but maybe if we're
in a start-up phase of this particular
application we can't really afford to do
a lot of compiling because it's just
going to be taking time away from
getting the application to start up and
running so there's a bit of a balance
there but that choice has to be made at
some point then you know a whole bunch
of classical optimizations are done java
specific optimizations we can do
speculative optimizations as well on
this and then the flow goes through
cogeneration as any compiler would so we
basically produce instructions from this
IR and then what we do is we publish
this metadata associated with with this
method and what metadata basically is a
sort of supplemental information that's
available with with with a method and it
so for example we would describe things
they're like certain points in the
method you know these registers need to
be inspected by the garbage collector in
order to look for live objects or we can
use it during exception handling right
so if we've inlined methods within a
particular method then you know in order
to get the stack trace looking just
right we're going to have to know what
was in mind and we're in a particular
method so there's lots of extra
information that goes out other than
just the code that's that's produced for
a method and then we take this and we
have to bind it into the vm somewhere so
you know here it is here's a compiled
body and then the the focus shifts on to
the to the interpreter side where now
that you have a compiled body the
interpreter now has to be able to
dispatch to this to this body and not
only from interpreted call sites but it
also has to be able to get there from
jaded call site so previous methods that
we jetted now should end up in this body
that we had just compiled
but we're not done there yet because
once the once the method is out there
you know we really have to monitor
what's going on and making sure that
none of the assumptions for example that
we that we made when we compile this
method have been valve violated so if we
did speculative optimizations perhaps
some of those speculative optimizations
some of those assumptions that we've
made are no longer valid so we have to
have some way of compensating for those
and there are a number of ways that we
can do that same thing if the method
becomes really hot I get all of a sudden
right we're going to have to potentially
consider recompiling it so that's sort
of a typical flow for java what would a
typical flow for Ruby look like let's
say so if you had gone and grabbed a you
know MRI ruby interpreter and you
decided to implement a JIT compiler for
that what might it actually look like or
what would your flow look like so let's
say in this case you now choose method
or blocks based on the invocation count
so you're not using a sampling thread
you're going purely on how many times
the method is actually being invoked so
that's your your decision then when this
count reaches a certain threshold you
can now decide to compile that method
synchronously on the application thread
so you don't actually have some
asynchronous q somewhere you're going to
stop the application thread that's
running and then do the compilation and
then start executing that code to get
this into the jet you're going to need
to convert the Ruby I seek into some
kind of an intermediate representation
you've got to pick your optimization
strategy again you've got to throw all
your op you can throw optimizations at
it you can do speculative stuff Ruby
specific optimizations and generate code
you can publish your metadata bind it
again you know similarly we've got you
know directing the interpreter to call
these things and finally monitoring at
runtime again so i started to repeat
myself a lot there when i was going down
this list I mean there's certainly
differentiating points here but you know
there is an awful lot of similarities
there so well we can start to do is to
start finding some categories of
similarity here right so we have to pick
a method to compile in some way right we
have to be able to actually do the
compilation and on the parts and then
the phases of that compilation you have
some similarities and we have to be able
to dispatch to a new compiled body once
we've actually produced one and after
after it's out there you know we really
have to monitor the runtime to make sure
that you know any sort of state that's
actually being assumed is it still being
consistent so we can use an experience
like this to start to put together the
different categories and the different
tools that we're going to have in our
compilation toolbox right so what what
would the components of a compiler tool
could actually look like and we can
start to find some of these categories
so 11 common category our life cycle
lifecycle management right so the jits
got to start up it's got to shut down
it's got to do some initialization it's
got to do some thread work perhaps all
kinds of stuff around the life cycle
that we can worry about the we have to
be able to choose a method to compile
right and there's a lot of commonalities
that can be leveraged between different
VMs around choosing a method to compile
right so I mentioned two different ways
in my previous example right we have a
sampling approach and we also have a
count and send approach right so though
there's some technology there that can
be shared even an analyzing the samples
like doing that statistical analysis to
figure out is this really a hot method
or maybe it's actually cooling off right
and and how hot is it been in the last
you know three minutes or something like
that so there's a lot of stuff
statistical analysis there that can be
shared infrastructure is a big one lots
of really core compiler II kind of
things that need to be shared when
you're building a compiler right so
things like data structures like CFG s
and blocks and trees and aliasing
information and code caches and other
kinds of very building block things that
you would expect to see with a compiler
a very important one is the interface
between the VM and the JIT itself so the
gin itself does not operate alone and in
particular if we actually trying to
specialized a lot of different parts of
a jet to make it very generic it's going
to need to act it's going to need to
have answers to a lot of questions that
that in order for it to do its
compilation right so to do that we have
to go through a vm jit interface some of
that can be common a lot of that can
actually be specialized as well it
doesn't make sense for you know certain
Ruby questions to be in the in sort of a
generic API but the kinds of questions
are gonna be asked or things about the
environment like tell me all you know
everything you know about this method or
give me the address of this class things
like that you can ask questions about
language semantics you know capabilities
about the target environment like should
I be generating code for a GPU or
something like that object model stuff
GC configuration and other things like
that and then there's the actual
compilation itself on the different
phases that you have to go through so
there's a lot of specialization and
commenting that that can happen with
with with that sort of thing method
dispatch figuring out how to call
another method and sort of utilities to
help you do that if I have to sort of
patch call sites or anything like that
to get me to redirect you know a to go
to be that sort of thing and then you
know just sort of monitoring the runtime
itself right so how do I manage the
assumptions that we've made how do I
determine when one of those assumptions
has been invalidated and how does that
inform the legit that something needs to
be done to this particular method so
there's a lot of specialization and
sharing that can happen from
from that so
so what we're doing is we're creating a
toolkit of extensible compiler
components and our starting point here
is the j9 Java just-in-time compiler so
internally this is called Testarossa the
status architecture or TR and what we're
trying to do there is to really isolate
the Java parts of this technology from
the generic parts to build this agnostic
set of parts and in order to be able to
do this we have to start wearing
engineering a lot of our code but we
actually have a pretty good reason to
believe that this kind of an approach is
going to be successful because you know
the technology itself has been around
for about 15 years has been growing a
lot of different ways for the last 15
years in the java space but it's also
already being used in a number of other
different compiler context right so we
use this in our static compilers as well
right so C C++ that sort of thing it's
also being used in sort of
non-traditional compiler context as well
for some internal projects right things
that do Trace base compilation for
example so it's already proven to be
adaptable outside of the Java
environment in a number of different
ways and the other thing that we're
really being driven by here as well is
that we wanted a very simple consumption
model internally we throw though we
throw the phrase no clone clone make
around a lot and what that really means
is that the ideal most simplistic way of
actually consuming this would be you
clone some vm with some sort of jit
extensions you clone a brand-new
open-source jit and you just type make
and it builds right you don't have to do
a lot of work to actually stitch them
together once you get new versions of
these repos it's a very consumer driven
model it's a little bit simplistic
perhaps but that's really sort of what
we're ultimately shooting for this kind
of thing
and so about the technology itself that
were that we're trying to offer the the
history of the of the Testarossa
technology is really it was really bored
built up from scratch as a dynamic jit
for java and for embedded job actually
and it turns out that that heritage
actually has a lot going for it right so
the fact that we actually started for
java and we're doing this you know
independent of hotspot independent of
any other Oracle implementation meant
that we actually had to produce a
cleanroom implementation so this is all
technology that we've developed in-house
it doesn't have any sort of external IP
dependencies at all you know it's
written in C C++ assembler i think in
total we have maybe about a million and
a half lines of code something like that
and so you know it's a very the IP story
is is great but the other thing here is
that it's an embedded java and we
actually derive a lot of benefits from
the embedded heritage as well and so
things like when you're running in an
embedded space you know fast startup
time is very important right when i turn
my device on i want it to come up right
so fast startup time is important it's
got very tight design constraints in
some cases for memory so you know we
have a very mais early memory manager we
want to make sure that all the you know
every little bit is accounted for and
it's also very flexible to meet all
kinds of different performance configure
our sorry footprint configurations that
you care about right so we used to say
that our java technology used to run or
runs on a watch and it also runs on a
mainframe and that's still true but it's
really a testament to how flexible and
the different ways that we can configure
this technology to get to work and
whatever environment that you want it to
work in the optimizations we've got an
awful lot of stuff in there
and it does a lot of really cool things
so but the framework itself is very good
figura belit is a high level
optimization we've also got a number of
very high performance code generators we
work on we produce code for x86 we
produce code for arm we produce code for
power whose quote for Z so we do all
that very well because it's a dynamic
compiler to begin with you know right
away we had a lot of the recompilation
built into it we had a lot of profiling
stuff built into it very early on we can
do speculative optimizations we have
compensation mechanisms like patching
code we do that you know a lot and and
we have all the supporting
infrastructure in the runtime to handle
that too from putting all the pieces
together point of view this is sort of
what everything would look like and how
they interact with each other so the
main entry point into this technology is
actually this aisle generator so for
your new VM you would actually have to
produce an ion generator that takes
whatever input form you happen to have
and turn it into the internal Testarossa
ir and the flow then goes into a to the
optimization you know it's guided by an
optimization strategy that you put
together you tell it what optimizations
you want to do in what order and so it
flows through that you then go into the
code generators of your choosing so you
can you can generate code for whatever
architecture you want but all the while
here it's act it's constantly asking
questions of the vm right it always has
to go back to the vm to get answers to a
lot of things so if you've got a lot of
generic stuff here it needs answers from
the vm to be able to decide what to do
so there is a fairly rich api that we
have with the with with it with a vm
though where we can ask questions and
get answers that sort of thing
it also works well with a runtime
profiler right so this is where a lot of
the sort of speculative stuff comes into
play so we've got a lot of profiling
infrastructure in that we've developed
to that we're working on releasing and
you know this is where if the JIT isn't
quite sure about certain things so if it
doesn't you know if it can't prove
definitively like the class of this
particular object is something then it
doesn't just give up it can maybe ask
the profiler to say you know well
ninety-five percent of the time this
object was a string so maybe the JIT
code can actually start specializing
stuff for for a string so and that's
what it does and it but it does talk
quite a bit with a profiling
infrastructure to and then they're the
result of the code generation is a
couple of artifacts right we've got the
code itself and then we've also got the
metadata associated with that code and
that's all sort of sitting in a runtime
system that's you know that's constantly
being monitored and operating
oops
so I mean I talked a lot about doing
this kind of specialist specialization
and what we really where we want it to
go and that sort of thing but it's not
really straightforward how we're going
to distill generic functionality from
you know this kind of core compiler
technology and allow the service
specialization for a polyglot so what we
ended up doing was a you know we took a
step back and started looking at the
code itself on all the different ways
that it was actually specialized and
what we found was that there was
actually two main axes of specialization
that we really had to concern ourselves
with one is specialization based on the
compile you're trying to build and the
other is the process or architecture
that you're targeting it actually are
other ways you can specialize but I mean
those two are the main ones that we
found were a lot of the adaptation that
we've had where was was leaning toward
and when we do want to specialize parts
of this compiler technology we do have a
number of goals in mind the first is you
know we really want to be able to
isolate different compiler features
right so what this means is that if I'm
working on something for java if i'm
working on a specialization for java i
don't necessarily care about anything
that somebody else is working on for
Ruby and that person that was working a
ruby doesn't really care about my java
specific extension to this to this
compiler technology so we kind of want
to keep those separate another example
would be if and we see this a lot is
that if I'm working on something for x86
for example I don't really want to have
to burden myself a lot of the stuff
that's happening on the power
architecture or the Z architecture right
I only want to focus on my particular
build for for for x86 so we want to
build in some framework for making that
be able to happen
the previous slide I talked a bit about
the lot of the great things that we got
from the embedded space about the
startup the compile time and the
footprint we don't want to lose all that
all those benefits by by by specializing
in the wrong way right so whatever
solution we want to come up with here we
want to make sure that those values and
those properties are actually maintained
right so we want to make sure that we
still you know have a very good
footprint done a lot of these things and
it's very easy to configure and we also
want to be able to minimize future merge
integration costs of some of these
specializations so go again going back
to the consumption model if I go out and
grab the latest open source technology I
shouldn't have to you know spend three
weeks integrating it because I have to
put if defs all over the place right to
get this to be integrated and then a
week later another another set comes out
and I have to integrate that and then I
have to repeat the whole process again
we're really after a much simpler model
of consumption there and we don't think
that we're done with this either right i
mean the whole point of pushing this out
into the into the community is to
promote and to encourage extensibility
of this of this kind of technology right
so we have to make sure that whatever we
develop is able to be extended once it's
out there so the way that we've decided
to do this sort of thing is to really
work on building extensibility into
certain parts of this in two parts of
this code base and of course some source
reorganization is going to be necessary
in order to be able to allow us to do
this sort of thing but what we've done
is we sort of reorganize things so that
the code and source files are organized
and kind of what we call projects right
where each project contains some degree
of specialization of compiler
functionality
and what we then did was we started to
refactor some of these core compiler
classes into what we call extensible
classes for lack of a better name and
these are actually C++ classes and we
follow a single inheritance composition
model for the specialization and we're
using static polymorphism for efficiency
so it's got a lot of interesting things
built into this to this model and you
know we're really trying to make sure
that a lot of those goals that I
mentioned the previous page are are met
and then what you do is you basically
make modifications to your make file and
you include path that is sort of pick
and choose the components that you want
and where you want them from and and
that that way you allow your
specializations to be used so it would
be describing all the subtle points of
this is really going to take a long time
and it's probably worthy of a
presentation in itself so I'm not going
to do that and unfortunately I can't
even talk about code right now but I can
go through a fairly abstract example
here right so let's say that you wanted
to build a sprocket and you happen to
create one and we push one out into the
open source community and then we also
came along and we created a power
extension to that sprocket and we push
that out and we also created an x86
extension to that thing and we pushed it
out and then and then we did further
specialized the x86 thing for 3d6 right
because make City seven is cool so we
push that out too
so great now out in the open community
there's this compiler sprocket that's
got a few specializations built into it
already so now a company comes along or
some project comes along called you know
the awesome vm project and they want to
be able to build a sprocket but they
don't have to build one from scratch
because they found one out in the open
so they go and grab that and but they're
mainly interested in arm and z
processors and so what they basically
end up doing is they make their own
extensions on top of that and sort of in
parallel with that there's a another
company called my org and they're
working on a multi language runtime and
and they make some specializations to
this sprocket as well and then within my
org there's a c++ compiler project
that's that's going on and then they
come along and they take the
multi-language sprocket and then they
can further specialized it for their own
needs as well so we've got a number of
different ways that this particular
sprocket can actually be specialized but
but how it actually looks you know to
the perspective of each of these
products is something like this so if I
am building a 386 C++ build on my org
the effective collection of all these
different specialization really looks
like this this is actually a class
hierarchy right from that from the from
the open source one all the way down and
collectively these things are a
composition that's really a compiler
sprocket from the perspective of this
particular build that is a compiler
sprocket and you'll notice in here that
it doesn't know anything about Z it
doesn't know anything about the power
stuff it doesn't know anything about the
my my awesome vm project at all it's
just the parts that you need in order to
build your sprocket for your for your
product
and you can do the same thing if I if
they deal their power bill right this is
there c++ power project right so it's
basically all just the power stuff and
only the stuff in my org so that's from
that builds perspective that is a
sprocket so we build some technology
around this this is sort of the main way
that we're doing a lot of the
specialization in the classes we're
leveraging all those sort of composition
features in C++ and then you know
looking at other areas of Testarossa
that that can be specialized the
intermediate language the ir is really
the lifeblood of of Testarossa it's you
know it flows through a fair bit of the
the code there we use a tree-based ir
where the tree basically represents a
single expression or a statement it's
got a single side effect to it so it
looks something like this if you got
those basic java bytecodes you can you
know the the tree basically looks
something like that but the cool thing a
lot about a lot of the IL that we have
is that it's very customizable right so
for whatever product you happen to be
working whatever project you want you
can actually create your own opcodes you
can create your own data types you can
extend ones that are there you can
ignore ones that you don't want and you
can basically build things up like that
and not only that but even like these
tree structures right you can specialize
the different the actual components of
the tree itself like the trees and the
nodes and the blocks and that's sort of
stuff because for example you might want
to add some flags to something right so
all of that can be customized in a way
that that you want and you need for your
product
the main entry point to the jet is the
aisle generator and basically what this
does is it just produces IL from some
input from your vm and these are
typically very very very VN specific
there isn't really in a lot of cases a
lot of sharing that happens between IL
generators because it is very specific
to the input but you know there are some
things that can be shared that like
stitching together trees you're
stitching together blocks that sort of
thing but in general this is really up
to your your vm for for producing the
optimizer itself i said before it's got
a very large set of classical and java
specific optimizations to start with
it's very platform neutral optimizations
both consume and produce aisle and it's
up to your vm the thing that you're
building to decide what packages of
optimizations do i want to put together
we call those strategies how do you want
to organize things right so if i want to
be able to compile things with a lot of
compile time budget you know here's all
the optimizations I want it to be able
to do right or if I you know if I don't
support let's say some kind of a
scheduling optimization I don't have to
build that into my product right it
turns out that this technology is one of
the most difficult areas that we've had
to adapt for a polyglot simply because a
lot of the optimizations that we do the
analysis and the transformation phases
of those actually do have the more
architectural specializations already
built into them and it's it's it's
proven to be a little bit difficult to
remove those so the approach that we're
kind of taking with a lot of these
optimizations is to basically separate
mechanics from the policy of the
optimization so for example with
something like inlining the mechanics
like if you're looking at how the
mechanics of stitching together like
actually consuming trees into another
block of trees or stitching them all
together or fixing up the locals things
like that those are all things that are
somewhat common between different
different VMs perhaps of different
different languages and the policy is
really telling it what to do right so
when do I in line this what is my
learning budget that kind of thing so
we're trying to do that for a number of
different optimizations as well to to be
able to get these features out so
Charlie gave a in Charlie's talk this
morning he talked about you know the
sort of the minimal steps that you need
to do in order to to get things up and
running this is my perspective of what
we would have to do in order to get a
new jet integrated into some kind of an
environment right so and again you know
this is this might be a little bit
simplistic but i think it's it's it's
it's fairly accurate based on the
experiences that we've had before
already getting this into python and
getting this into Ruby we basically
followed a very similar recipe to this
so you kind of clone your favorite vm
you grab the latest open-source compiler
toolkit and then the next thing you
really need to start thinking about is
how do you want to start to trigger
compilations to occur right you need to
first figure out what your granularity
of compilation is right is it a method
is it a block is it a trace what what
exactly are you and we're not putting
any restrictions on that whatsoever it's
entirely up to you but you have to
figure out how to trigger a compilation
and then using sort of the AP is that we
that we have for that and similarly how
do you call a method that you've just
shitted right so and how do you
integrate those sort of those sort of
api's into your into your environment
you then have to build an aisle
generator for your vm now you don't
actually have to develop this for every
single possible input right for example
if you're doing Java you don't have to
implement your support every single byte
code off the bat or Ruby don't have to
support every single instruction right I
mean there are ways of actually
consuming a good portion of it and if
there's something that you don't
recognize your support you can always
punt and go back to the interpreter or
something to Han
that so you just sort of need to get
enough going to get to lift yourself off
the ground and then you can implement vm
specific extensions as you need to the
core technology so if I need to make
specializations to the opcodes or I need
to make specializations to data types or
the instructions or nodes that sort of
thing you should do that and then you
need to work on your interface right the
questions that need to get ants asked
and answered between the jet and the VMS
compilation proceeds and then you sort
of stitch it all together by modifying
your make files and your include pass to
pick up the technology that you want and
then in theory you should just be able
to hit make and everything just works
right and of course over time you can
start to build in more capabilities as
well as as as you want to as you want to
support these if you'd only started with
a subset you can start to build in those
on on demand as you go along okay so
just looking a little bit ahead here so
there are some sort of looming
challenges that are that are coming up
and it's some mean all this technology
is great but in order to use a lot of
this stuff and actually to get a lot of
good performance out of out of it you
really have for every environment you
really have to take a step back and look
at the higher semantic level of the
thing that you're actually compiling
right because if you just not easily
applied you know everything that I had
just said what you're really optimizing
is a lot of the connective tissue in the
interpreter right how do you get from
one byte code to the next and I mean
that actually helps right i mean you are
going to get a fair lee significance
surprisingly significant improvement by
by just optimizing that but if you want
to go even further than that you have to
kind of take a step back and understand
what exactly is that I'm what is it that
I'm actually doing here so for example
in in some dynamic languages maybe the
operation the underlying operation here
is really just an ad right so if I can
prove that
the the operands to this particular
operator are both integers and I can
maybe distill that down just to a single
machine instruction wrap a lot of
speculative code around that and let it
go right so you have to kind of do some
deeper analysis of the the the of the
method or trace to to figure out figure
that out I think you also want to start
using more metadata you know if you look
at if you look at a lot of the stuff
that age it might do if you did a fairly
naive implementation of this you're
going to be spending an awful lot of
time just maintaining the state that the
interpreter expects right I'm going to
be updating program counters I'm going
to be updating stack values I'm going to
be or stuck counters and I'm going to be
storing stuff onto the stock just in
case the interpreter might need it later
right so this is really sort of an
artifact of the compatibility side of
things right you're trying to make sure
that you're so compatible but what
you're really doing in some cases is
actually limiting the amount of
performance you can get so in those
cases you want to start thinking about
using more metadata right to hide some
of those things and and then make some
modifications to the to the runtime to
use that metadata when it actually needs
it right so that way you get the
compiler to focus on you know really
just the you know let the compiler do
what it's supposed to do right optimize
this method not worry about optimizing
the thing of the connective stuff the
other challenge that we've that we've
come across as well is is for tolerance
so some environments out there use
forking is their primary means of
achieving parallelism and I mean it that
works but the the problem is it's really
difficult to manage a compilation
strategy fairly efficiently or crop when
it when it when you have to deal with
multiple processes we can do it it's
just that it's it's a lot more
complicated than it
should be and I mean like a lot of the
technology that we worked on it was
really assuming you know sort of a
threading based model and now that we
after you actually have to start dealing
with multiple processes in order to make
sure that there's not a lot of overlap
between these processes things are being
shared properly we have to bake in a lot
more smarts into that kind of technology
so finally where we are on the road to
open source so if you look at all the
technologies that we've been talking
about and and then are considering open
source the results that we have so far
are fairly encouraging and things are
moving forward quite well and you know a
lot of our proof points are panning out
insurance or where what we have to do
next you know some of our components are
pretty close to being ready to being
open source support the thread the trace
library the G the garbage collection
technology from a technical point of
view they're pretty much ready to go
there's a lot of sort of legal stuff
that has to get ironed out first before
we can push them out the door but from a
technology point of view I think the
first wave is is about ready the
compiler technology that I've been
talking about it needs a bit more time
in the oven there's a lot more stuff
that has to happen to get things
smoothed out and separated in partition
just the right way we are spending our
time split on two different axes where
we're working on refactoring and we're
also continuing to develop proof points
to show that the technology is actually
beneficial so I think we're going to
continue on both of those fronts for a
while and we also we've already started
talking with some of the runtime
communities and some of our partners
around around this kind of technology
and how it can be used in different
environments and and you know just
getting people interested and excited
about the kinds of prospects that you
eat and the potential of this sort of
of Technology so I certainly invite any
feedback that you have on anything that
I've said today though the the project
as a whole you know if you're if you've
got you know good things to say bad
things to say if you want to get
involved and happy to talk with you as
well Project Lead is marked stoodley
he's happy to talk with you or john and
if you've seen John running around here
the last few days but he's always
willing to talk to you to anyone about
about this sort of thing so so at that
point I'm going to take a breather and I
can take some questions I will say I've
got some t-shirts up here for anyone who
wants to come grab one if you want to
ask some questions too
yes sir you mentioned the compiler a
decade I can figure out why zation
strategy based on your trade office of
public
do you use a complication strategy of
having separate lessons
yep that is that is so the question is
do we use tiered compilation as part of
this so the answer is yes that is one of
the ways you can do this it's it's
configurable in the sense that we have
some environments where you choose a
particular strategy like I want to do no
optimization at all or I want to do what
we call a cold optimization right it
just compiles it once and forgets it but
yes we do go through a tiered
compilation and we've been doing that
actually since since the beginning that
was sort of one of our initial ways of
doing things sir
how much do you need to modify
Master methods but
so the question is how much do you need
to patch the the interpreter in order to
consume some of this technology it
really depends at this point from from a
from a purely jit point of view there
isn't really a lot of patching that
we've had to have done typically in some
of the interpreters like if you're
talking about the Ruby interpreter the
the points at which like method
dispatches occur or when you decide to
call another method those are very
localized so you can really contribute
the you know you can really concentrate
your changes that you need to make to
just those areas and and I mean so far
that's what we've done and it's it's
fairly minimal
for speculative optimizations the French
state of
combined but actually
how do you get from the speculative
relation face back to interpret
the Great Prince that opening
right so so the question was around
speculative optimizations and how do we
maintain no frame state between you know
the the digit code and the interpreter
if you have to go back so I mean we've
done a fair bit of work on the Java side
on on stock replacement and being able
to jump between different you know
between different contexts so and the
way that that works is it does sort of
build up a lot of metadata in the event
that some kind of an event occurs that
you have to jump back to an interpreted
state a very safe kind of conservative
state so that kind of technology I think
we're going to have to leverage as well
we haven't actually done speculative
optimizations for Ruby yet but I think
that kind of thing is is what we're
going to have to leverage
nose okay um well I'm go ahead
so the question is why did we choose a
synchronous compilation strategy for
Ruby and not do it in parallel that is
that was purely the simplest way of
doing it right there isn't you know we
we can built in multiple compilation
threads for for for Ruby but it's just
the simplest way to get this technology
working right okay all right well thank
you very much everyone for your time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>