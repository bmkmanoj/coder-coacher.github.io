<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CON2954 - Is Your Profiler Speaking the Same Language as You? | Coder Coacher - Coaching Coders</title><meta content="CON2954 - Is Your Profiler Speaking the Same Language as You? - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CON2954 - Is Your Profiler Speaking the Same Language as You?</b></h2><h5 class="post__date">2015-12-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/v1p5YB5S6XU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you everyone for attending let's
let's get started now I must first of
all admit to everyone I was working on
these slides last night and I did fall
asleep midway through the slides but I'm
putting that down to jet lag and no one
can use the same excuse here so everyone
please stay awake so okay this is this
is a session all about profilers some of
the decisions which we made when we were
developing our own profile accord ex
rebel I'm going to just be talking about
profilers in general so my name's Simon
maple SJ maple I was previously as you
can see from this wonderful picture XIB
Emma now working at the awesome d return
around I have been for about three years
now I'm a developer advocate and IBM i
was actually a developer for around 12
years on websphere any websphere users
jump up in the air and scream your
enjoyment no okay so yeah I a virtual
jug found the virtual jug founder any
virtual jug fans here no one one or two
so the virtual jug is an online java
user group that i founded and created
two years ago now and the idea behind it
is it gets the greater some of the
greatest speakers around the world and
brings it to the people who aren't
living nearby a user group or just want
to hear from more great speakers in
addition to their user group it's
entirely free virtual job calm if you
want to if you want see some of them i'm
also the london jug co-organizer along
with martin Verburg and Barry I'm a job
champion javaone Rockstar speaker and a
rebel labs or thur rebel labs is one of
the one of the content engines as part
of zero turn around we provide a whole
bunch of free technical content very
opinionated as blog posts as well as
article as well as reports so do check
that out as well at zero turn around
calm forward slash rebel labs okay so
we're going to first of all talk about
the tools that are available to you as
people who are interested in improving
the performance of your applications
there are three main types first of all
monitoring tools monitoring tools can
you know effectively tell you where a
problem might exist if a problem exists
typically there APM is which are going
to run in your production environments
it'll tell you if your if your
environment is broken on where that
could potentially be the next step
profilers profilers can actually run
from production all the way down to your
development environment and profilers
are more likely to tell you what is
wrong some profilers may even tell you
how to go about fixing it perhaps it's
more targeted at problems so the
monitoring tools tell you if there's a
problem profilers tell you deeper how to
potentially fix that problem and what
that problem is the other type of tool
is a testing tool and testing tool is
very very important if you've ever
making tests change if you ever making
changes testing tools could include
something like gattling as an example
gattling will allow you to provide load
onto your environment and therefore get
a measurement of how your environment
performs how your application performs
if you were to then make changes to your
application you can rerun that Gatling
test and you will tell if your
application performance has improved or
gotten worse but you absolutely need to
measure everything before you make a
change that's the only way you can tell
if you've improved or not the ability to
benchmark is one of the most important
things when you're doing any kind of
performance testing now of the three
things or three parts that we've talked
about if you're more interested in
hearing about where they all fit
together one of the lab rebel labs
reports which I which i co-wrote is the
developer's guide to understanding
performance problems and this tells you
more about the three different types of
tools which are great candidates to
actually look more in depth to and which
might suit your environment better so if
you just take a photo that maybe Google
that you can download that for free one
of the other things which rebel labs
produces is he
yearly report based on a rebel lab
survey now this report this year was all
about Java performance I can share with
you some of the highlights from that
report now there's a whole bunch more in
the report so do check that out it's
like about 50 or 60 page report so first
of all at what staged people typically
perform profiling or other performance
related tuning this is a really really
crucial slide because traditionally
people very much consider performance to
be a task that is done after you've
developed your code after you have
tested your code it's just one of those
things that gets pushed out to the end
of a release and maybe will or won't get
done but we can see here more recently
that people are more inclined to test
performance from the word go as they're
developing their code all the way
through into into a QA into a staging
into production and so forth this very
much follows the kind of QA style in
that originally it was very much done
where you actually write all your code
and then at a later date you pass over
to to a to a test team they test your
code and so forth and this is also a
style which is now being broken in
performance testing we're looking at
doing our performance testing as early
as we can because we know history has
taught us that when we find issues
earlier they're easier to fix their
cheaper to fix and it's just it's just
better in general so this is a very very
promising graph that also they would
chart and i'll show you performance is a
task that needs to be done from the
start all the way through to production
in terms of who actually fixes the
performance problems when they occur
when we find them it's hugely waited to
the development team that are others
perhaps dedicated performance teams
maybe even operation teams that fix the
bugs those kind of bugs are very much
more likely to be the kind of
configuration bugs that make a change to
your configuration all of a sudden your
your application will start performing
better but it's heavily heavily weighted
to development so if the developers
fixing it anyway it's
very very much more more convenient for
them to fix it as they're writing their
code this is a very very interesting
chart and it shows how long in days each
of these teams would take to fix their
bugs this is going off the average so
for example here we have whoever writes
the code so developer is well we'll fix
a typical performance bug 15% quicker
than the average across all of these
teams interestingly the dedicated
performance team will take almost fifty
percent longer to fix bugs than the
average as well and this is potentially
due to the fact that a performance teams
may find more complex bugs but it's very
very important to realize that it is
cheapest to fix and fastest to fix early
in the process so let's look a little
bit about application performance now
whether we're writing a new application
from scratch or introducing new features
to an existing application and new code
we'll all go through a phase a cycle
first of all if we wanted to say let's
write an application which is a animal
it might start off like a small little
turtle walking very very slowly but then
we would try and find a way of speeding
that turtle up and it's that second
phase where we get a very vast
performance increase let's have a look
at the chart which Alexei shipler if you
don't know Alexei shift lever or you
don't follow Alexei he's one he's one of
the performance and benchmarking gurus
Oracle he's a very very good guide to
follow he also did a session on the
virtual jog about JMM the java memory
model really really smart guy this is
something he just drew up on a
whiteboard this is the curve that your
application will will go through it'll
start here at a as you perform your your
changes it can finish anywhere on this
line depending on on the types of
changes you're willing to make but let's
pull this out into it onto a real graph
so we can see on the right hands on the
bottom we have
complexity how complex your application
code is on the y axis we have
performance how perform at your
application is we start off at a this is
the naive program where we start
typically we've almost certainly added a
whole bunch of bugs with we haven't
really looked into the into the
architecture too much since we just
throw in the code in as we look at our
code we perhaps remove a whole bunch of
GP at work in efficient api's dirty
business logic that kind of thing we get
to point B this is that this is a very
very fast and easy waiting to both
increase our performance and reduce our
code quality that's right and reduce co2
complexity after this the B to C level
this is typically where profilers kick
in the typical profiler will likely make
your code slightly more complex but it
will have a very beneficial improvement
on your on your code performance
anywhere beyond see really you're making
your code a lot less maintainable you're
probably likely doing it because every
millisecond costs dollars probably
looking at the kind of financial
applications here your code will be less
maintainable it'll be harder to harder
to keep in the future so you know beyond
this kind of point you had you must have
a very good reason to improve that we're
talking insane hacks and the code is
extremely hard to look at so profiling
is slightly overloaded the two main
categories here CP profiling or memory
profiling the two are very very
different we're really just going to be
focusing on the cpu profiling or what
can be called application profiling as
well if you just focus on your what your
what your application is doing and there
are two categories of profiling sampling
and tracing you tend to choose as a
profiler it will tend to choose one or
the other I believe York it actually
does both you can choose which one you
want but there are pros and cons to both
and we'll we'll look at them answer now
so first of all we'll look at sample so
what is a sample
if you ask a performance expert they'll
want a snapshot of your jvm how would
you great how would you create a
snapshot of your jvm anyone through an
answer stacktrace yes start tracing a
list of start traces which is
effectively one of these a thread done
you can create a thread dump in a couple
of ways control brake on Windows or kill
minus three send that flag to the JVM
you'll get a thread dump file out and
this thread dump file is depending on
who you are possibly readable possibly
not readable it'll give you a snapshot
of what your jvm is doing at that time
now that snapshot might be useful if you
only want to see a specific type of
problem maybe you've got a thread
hanging or something maybe there's a
resource with two things are waiting on
a deadlock you know you can get that
kind of information but if you're really
trying to understand where your
application is spending most of its time
you need to get a number of snapshots
the more the more the better because
then we can make a better we can have a
better understanding of of overtime
where your application is spending its
time now just one of these can be quite
hard to go three if you're not used to
seeing a thread dump if you wanted to
see a whole bunch you exist in one of
two categories you're either very very
very smart and can read through these or
you're probably more likely in my
category where I would just float for to
sleep after reading a couple and then
not worry about it we don't need to read
these there are better things that can
read these for us computer programs so
this is where a profiler would gather a
bunch of these snapshots and try and
understand what's happening let's say we
had ten snapshots and in every single
one of those snapshots apart from one we
find ourselves in a particular method
what a profiler would do is it will say
ninety percent of your time is being
spent in this method if you only have
four sample counts here which are being
effectively for snapshots which are each
being taken at an interval of 20
milliseconds we unfortunately don't have
that much information to go of with only
four snapshots we can add judgments you
know they don't really hold much water
because we're only working off of for
typical points so what we really want to
do is increase the sample count
potentially reduce the sampling interval
and that way it will give us many many
more snapshots and will have a much
greater chance of being right when we
calculate the percentages and you can
see each of the classes and packages and
how much time is currently being spent
also notice I'm not sure if you'll bear
to see this at the back here we have
calmed out your kit probe this is
actually the your kit profiler profilers
can profile themselves as well they can
see themselves on the stack just
something to note so let's actually have
a look at sampling in action here we
have a ma application flow we start off
in the main method we can invoke foo bar
and the elusive Keyser söze method now
as we sample our code what we
effectively want to do is pick points as
we go through each of these points we
want to make a sample and when we sample
each of these we accumulate that and we
can make observations across that sample
we can't just sample wherever we want to
sample though because we need to sample
at something called a safe point and a
safe point is a point in time where
everything in the JVM stops everything
holds all the threads hope and a number
of bookkeeping things happen in the JVM
for example garbage collection garbage
collection pause is the amount of time
it takes for a safe point at this point
that is when a sample will take place
they're not everywhere you have to every
single thread has to reach its own point
in the code where that safe point would
be a classic example is end of method so
rather than just pick times what we
actually have is a list of safe points
and these are the points at which we
will which we may take a cut take a
sample and we'll use those areas you'll
notice that in this point the elusive
keyser soze met
is never caught so what we have here
even though the Kaiser Soze method has
been called twice there is no there is
effectively no record that a profiler
can look at the data on each of these
samples it will tell you that that
method was never called so one of the
problems with sampling is that it won't
give you a perfectly accurate definition
of where you've been which methods
you've been where you're spending your
time it's an estimation but of course
the advantage of an estimation is that
it's going to be cheaper on performance
it's cheaper to run a sampling profiler
but it's less accurate that's correct
yeah the comment was it's only it's only
inaccurate if you're safe if you're if
if there's the safe point bias is
pushing your data around so the next
question is can that buyers be taped can
a savepoint buyers be tame there are
options if this is important to you if
the safe point bias is important to you
there are options that are available one
of which is JM c java mission control
anyone heard of David Mission Control
this is always surprisingly even though
there's probably about fifty percent so
we're surprisingly few because Java
Mission Control is actually shipped with
a jdk since since java 7 update 40 it's
 for the JDK so if you enable the
flag you can in fact you can just run
JMC which will which will provide you
information you need to enable more
flags to do something like a you know
targeting amount of time and profile
that data but it's free for development
and charged for production it has a
proprietary protocol because it's built
in to your jvm and as a result it can
take safe points outside so it can take
samples outside of safe points there is
another open source profiler called
honest profiler this honest profiler
isn't really backed by any big
corporates so it's really best can do
development and it uses native
interfaces to be able to get around the
safe point bias so we're using native
interfaces yes
no that is that sure is that a generic
name or is that the action in that yeah
yeah and this is this is the actual name
the project is called honest profile
yeah yeah there are other tools that do
use that build upon that yeah yes yeah
so the question is it is hard to know
how much to pay for job mr. troll dovish
but I'm not exactly sure it is very
expensive though that is for sure but
yeah it's Oracle so a big corporate
always going to be a hard to work out
what you need to paste very much
dependent on your environment but it's
out you only have to pay for it for if
you run it in production or if you're
using production data everything from
development entirely free so it's well
worth a look up okay so if we look at
JMC typically historically you really
performance test as an expert it isn't
performance testing isn't really the
kind of thing which is done by a general
developer because historically you've
always needed a lot of knowledge and
information to understand the results
very often with profilers you get a
couple of different sets of results you
either get a basic result whereby you
know let's look at which wich packages
are hot right now java.util is hot you
think right yeah obviously everyone uses
Java util Java new tool is going to be
hot it's very hard to get that kind of
information out but you can also get a
huge amount of information where you
look deeper and this is a case of well
this is all great to have this
information but where do I start and and
this this can be you know when you're
looking at a huge amount of information
unless you are a performance expert yeah
it's going to take time to find to find
that bottleneck it's gonna it's going to
take time so one of the things which we
have been lacking in the industry is a
tool which has been developed for a
performance to all been developed for
developers let's look into tracing this
is the this is the alternative to
sampling now with tracing you
effectively
instrument your code around the start
and end of a method and you catch the
time at start you catch the time at the
end you do your subtraction and voila
you have if anyone wants to note I just
let you know there's a JCP party and
awards ceremony starting very very soon
in 10 minutes that's just a notification
let's just come up I thought it might be
useful to share that let's go back okay
so you've taken that snapshot let's have
a look here's our business method we
have some work in that business method
but we instrument a start and an end we
simply subtract the subtractor start
from the end we have the amount of time
that that work method or the business
logic within that method has taken
pretty straightforward you can of course
if you want a finer granularity cool
nano time depending on depending on how
much detail you want you can of course
have your own API where you call into
profilers and then change that a later
date but there are really three ways of
grabbing this information we've talked
about nano time we've talked about
getting the current milliseconds
alternative you could have this parallel
thread which is running in the
background and there are a number of
reasons why you might want to do this
one of the most important reasons is
because if you wanted to choose to call
nano time if you wanted that depth now
no time performance very very badly if
it is if you run if you hit calls it
concurrently so if you're hitting get
nano time on multiple threads it
performs very very badly so if you have
a single parallel thread which is the
only thread responsible for grabbing the
Nano time you can pull that thread as
when you want to grab another time you
don't have to go and manually find it
this is also important for performance
because getting the Nano time isn't free
it's actually quite expensive operation
will go into some of the some of the
times itself with a parallel thread if
you want to effectively poll the system
time you can either choose a sleep wake
up a yield wake up or just a busy
this is really down to the
implementation of the profiler to choose
but we'll have a look here is a kind of
profiler API and when we start a method
we want to go to this loop variable this
is going to be our parallel thread and
we want to say give me the current give
me the time give me your current time
and the loop method rather than rather
than go directly now into a into a gap
no time we're actually just going to you
know get the the most recent nano time
that the parallel thread has got so if
we look at the parallel thread this is
just a runnable in this while link which
is spinning round it grabs the Nano time
and then goes into asleep simple as that
this spins round every time someone
wants to grab the time they get the most
recent time the most recent nano time
from that from that trip one of the
reasons as I said reading memory is not
free it takes time if you want to get
time it takes time to do that it's
actually quite expensive as well because
now there are a lot of layers to get
through remember we're running in a vm
there a lot of layers to get through we
can also refer here to a really great
blog post I've mentioned ship level
ready really great blog post from Alexi
ship love and it tells you how much you
can trust nano time and how accurate
nano time is and we're taking a few of
the numbers from alexys blog here and we
basically got two numbers we've got the
granularity and the latency we'll start
with the latency if you want to make a
call to get your nano time it's going to
cost the cost on each of the platforms
here 25 nanoseconds to grab the Nano
time on Linux 29 million second at nano
seconds to get out on solaris and just
14 to grab that on Windows now here's
the here's the thing when you grab the
Nano time doesn't just like continually
increment by 1 nanosecond or or you know
point one of a nanosecond it increments
in these granularity chunks if I grab
the Nano time at X time and I get my
result back X on each of these platforms
let's take Solaris the next time the
next
civil increment to that nano time is
going to be X plus around 29 nanoseconds
there's not going to be an X plus 1 and
X plus 2 and X plus 3 we each work to
each of these granular increments now on
linux and solaris they're not too bad
29-26 on Windows we're looking at 371 so
it's very very important to understand
depending on our granularity depending
on our platform how accurate we can be
now another thing is sleep times if we
implement as a profiler some kind of
sleep loop what guarantees does that
give us well if I was to ask for a sleep
of a single millisecond we can tell you
in this is actually from this isn't from
Alexis blog this is some metrics which
we gathered at zero turn around we
notice that on Windows 8 if you if you
perform a sleep one millisecond you'll
typically wait a minimum of 1.7 million
seconds linux and our sacks pretty good
virtualbox in linux it's like tens of
milliseconds if you ask for a single
millisecond wait so pretty poor but
you're still effectively crunching some
cpu so it's absolutely not free in terms
of the CPU cost there is another option
which is a yield of thread now one of
the massive problems which we saw when
we were developing ex-rebel our profiler
is that the window scheduler as you
approach or as you reach a cpu
starvation in your in your in your
environment the window scheduler will
not yield will not awaken threads which
are which are yielding so you might
actually have this point where you call
you know call for a sleep wait for a
yield and you will never be awoken you
will never yield so you will have you
will effectively have threads that don't
wake up and this is only on windows so
what do we do so well let's keep our
variable let's keep our loop but when we
actually perform our sleep what we might
want to say is let's yield unless we're
on Windows if we're on Windows don't
yield effectively perform asleep
these are all considerations from a from
a profilers point of view now let's look
at that let's look at that graph now
this time where we have main foo bar and
what was Keyser söze that elusive guy
that never got caught this time we
replace it with bars because with
tracing we trace we instrument the
entrance of the method and the exit the
method we effectively know how long
we're spending in that method in every
single method there are no methods that
get away from us now tracing is more
expensive but it gives you that it gives
you a much more precise data than
regular sampling now if each of these
circles if the if the diameter of each
of these circles is the amount of time
that we spend in the instrumentation of
the code what you'll notice is in some
cases very very short methods you
actually spend it you can spend more
time instrumenting then you can actually
running that method some profilers what
they might actually do is they might not
instrument those very very short methods
but rather check through the bytecode i
actually just count how many times
they're called now is it important the
amount of time that you're actually
taking to instrument to actually run
through that instrumented code it does
add up if we actually look at what's
happening here if we call main into foo
into bar into bars every single one of
these methods will add a delta of time
to that to that flow and that might be
fine for bars because bars only
experience is one but let's not forget
while we're in bars time still clicking
for main so in each of these calls in
every time we call this instrumented
code we're actually adding it up for
main if we add another branch fubar and
bars they won't they won't get affected
at all but main will take that cost so
depending on how many methods you hit it
could add more and more to that outer
method is it important it depends on the
profiler really depends on you as well
whether you think it's important not
some profilers will actually say you
know what you're spending a lot of time
here can we calibrate can we
move the effect that the profiler is
having so some will actually benchmark
themselves and remove the time that they
spend profiling themselves so if you
wanted you could do that but you know is
it important how granular do you need to
be it depends on you know how loaded
your system is where you're doing you're
profiling you know if this is in
development you're not going to care
about these kind of things if it's in
production well maybe it depends on your
application now we're going to talk
about the performance pipeline here
because if we look at how performance
has historically been tested it's very
much been left till the end as QA once
was QA has now changed so that you would
have a huge number of your unit tests
and they would test your code early in
the early in the cycle you then have a
CI environment and that CI environment
will continually run your tests your
integration tests on every single build
of your code as you continue forward to
production you will have bigger and
bigger tests svt tests load tests and
furthermore now it's just emerging that
now in performance performance should be
given you know that the same benefits we
shouldn't need to wait until until you
know even see I or QA or production
before we do this you wouldn't even
consider putting your code into
production if you don't have unit tests
you wouldn't even consider it but years
ago that was done and the tide is
turning now on performance whereby you
know in years to come we're going to
look back and think I can't believe we
weren't profiling in development as much
who currently profiles in development as
they as they code a few people and you
know it's great but who who writes unit
tests for their applications yeah pretty
much everyone right this is this is
where we're going so we can see that
this is where we're leading to you can
see people believe this is important the
question is are people finding the right
tests the right bugs in the right places
there's no point
in your development environment in
testing for actual you know how many
milliseconds of my spending here because
by the time you get to production you're
going to be on totally different
machines you probably be running across
clusters it's really not going to be you
know mappable from your development to
your to your production however there
are tests that are very valid and are
will give you bugs earlier that you can
fix for example pick something like SQL
queries if I run a test in it generates
a whole bunch of SQL queries it's going
to generate that kind of SQL queries in
production as well why wait to a much
much later before you can test it so
there are four things for big features
we really want in our application
functionality quality performance
security each of these provide our
software mantra functionality make it
work quality make it work well and this
is the way we need to go you can't
develop straight away with security and
performance in mind you first of all
make it work then make it work well then
make it work fast and I'm sorry security
people but that often very gets you know
entirely left alone and it doesn't
really happen but ideally we want to
make it work securely historically
making it work well without agile
without bringing your tests earlier in
the cycle would get entirely lost now
making work making work well is done
very well in your in your development
phase making work fast is now this new
QA which sometimes gets lost sometimes
gets pinched as releases come in we're
going to focus on performance making it
work fast now as part of the report we
released we asked people what they what
they saw in their application has their
big performance hits these are some of
the ones which we which we found slow
database queries inefficient application
code too many database queries these are
you know large constituents of people
that are saying this now many of these
inefficient application code your codes
not going to change between
you know your development and your
production it's the same code so if you
can find bugs you'd rather find in
development rather than production there
are obviously some things like
concurrency issues which you will need
to wait until you know later in the
cycle before you can you can test it
effectively now when we look at schiphol
ABS drawing of where most of the time
you will get your benefit from your
performance testing it really is on its
first section and you don't just
obviously want to rely on your profiling
or your your performance testing during
this development phase because you
wouldn't go into production with our
application that you have only unit
tested for example you do need to have
different profilers different
performance unit but performance tech
tools for each stage of your application
now given that this one is the this
piece here is the area which you know
schiphol have believes we believe is the
area which can deal with performance
quickest and you can get a lot of that
benefit from that's the area which we
with external have also looked into
we'll look at that in a bit but one of
the other areas is whether your reactive
or proactive in your profiling and the
two have you know different aspects to
them reactive effectively dealing with
the problem when that problem finds you
and reacting to it this is very often
the kind of thing that you would do if
you're in a p.m. if you're in if you're
using an APM in production you find the
bug you then react to that bug try and
find it use profiling tools to get more
data fix it and continue proactive is
effectively you know similar to having
your big test suites that run can you
know on every single build you find the
bugs proactively you fix them this is
another area which profile is which run
in your development profilers which run
your CI or whatever that's the proactive
respect so when we look at the pipe
you have different areas from
development with a lightweight profiler
up to your classic profiler which can
effectively run in all stages your
performance tests which take a lot and
much much longer time to write much more
expertise and you're a p.m. and you're a
PM with run in production so the idea of
this performance pipeline is that you
have different tools for the job at
different sections you wouldn't
typically choose the same tool in any in
really any other whether it's QA whether
it's a quality pipeline you wouldn't
tend to use the same tool same with
performance this is where we believe
it's going in terms of the lightweight
profiler you're not going to rely on it
in terms of being your only profiler
it'll find common issues it won't find
all issues what you want from a
lightweight profiler is the ability to
easily find potential issues and provide
that to you in a way which you as a
developer will understand and be able to
understand bottlenecks understand where
the issues lie and give you the
information you need to fix that so
coming up to close enough to the end so
now I want to show you a tool which we
have produced at zero turn around and
this is a tool called ex-rebel ex-rebel
is sits on the lightweight profiler site
so this is a development time tool which
is a proactive tool that will that will
provide you with information about
potential production problems while
you're developing your coat okay that's
the key the potential profiling so lets
me down give you a quick demo so here oh
ok
so here we have the the pet clinic
application the typical spring vanilla
spring application and as we're
developing this consider this is as
we're developing as we are writing our
code for the first time as I go in just
to test my application let's say I've
literally just developed this find
owners bug is fine I'm as button now if
behind this finite is button is code
that we've literally just developed and
ex rebel you'll notice is sitting in
this bottom left-hand corner testing as
we click it's entirely in the background
so very very easy to you know spot
notifications as they appear as I click
find owners that came back pretty quick
we have an amount of information as a
developer if I wasn't using this a
lightweight tool to inspect how my
application is performing I would
probably be ship I would probably you
know put that into my into my code
repository others with them build on top
of it others would add to it and then
only and when we pushed it into a
performance test or further into a
staging would we maybe find that under
load these number of SQL statements that
extra is is showing you here in this
case 37 SQL statements just for this
single click under load that might then
become a problem so with X trouble we
can deep we can dig deeper into this
into this notification simply by
clicking it and we can see exactly
what's happening under the covers so
here we can see that me you make that a
touch bigger so here we can see our
method stacks effectively well call
stacks and we can see that here there's
an invocation to some to some JDBC and
we can see this JDBC call has actually
created an amount of SQL and we can see
that SQL as it's being driven through a
data source who's heard of an N plus one
problem here Kirk I knew you pick your
handle and so an M plus one problem is
quite quite a common problem for if
you're using something perhaps like
hibernate or JP that you haven't
configured an N plus one problem is
typically where you submit an SQL
statement court make an SQL call
to to a database you get a result set
back you get some data back and then you
make further SQL calls based on that
based on you know that results that that
list of results that you've got from the
database if I get a hundred back I might
make a further hundred calls that's
where we get the n plus 1 in this case
as we're the pet clinic our first SQL
statement is getting from the owners
table we get a full list of all our pet
owners including a pet owner ID pet pet
owner name you'll notice that from this
SQL statement we get ten rows of data
back the next thing this is doing and
this is actually because this is jdbc
this is almost certainly you know in a
for loop and other code that's driving
this the next thing we do is we call a
further ten SQL queries and each one of
these SQL queries is digging into
another table in this case it's the pets
table and our where clause is actually
using one results back from owner SQL
query so if we get ten rows back from
our first SQL query we will perform a
further ten SQL queries on the pets
table using the owner IDs we get back as
our where clause and this is a classic n
plus one problem in fact this piece of
code this is actually a vanilla spring
sample code this will goes on to make a
further 13 calls which is why we get a
large number of SQL Cui's now this is
actually only going to get worse in
production because if if if the if the
amount of data in our database is
actually a kind of lightweight amount
compared to our production data this is
actually going to be lower and
potentially a much much higher in in a
production environment so we want to
hear about this early we want to hear
about it in development and what
ex-rebel does is as we're testing as
where you know testing our code as we as
we first write it that's when we want to
get this this SQL information but any
questions over
no cool so let's say for example one
other thing which is which is very nice
about this this this part of the tool is
let's say okay I can see this I can see
this information but actually I'm not
the person that that that maintains this
code I actually want to create a jira
and and pass this code onto onto someone
else who actually wrote this code and
maintained this code what you can do if
you choose okay I'm not connecting to
the internet you can share this this
this snapshot and wonder if i can just
open 115
we can do as you can share this this
this snapshot there's the results that
we get back there we go you can share
this by clicking that you get yourself
your own unique URL this URL will then
provide me with an online version of
this same data I can interact with it
exactly the same I can then pass this
information on to you know add this add
this simple URL into into a jira someone
else in the team who doesn't have
ex-rebel installed can view this they
can see they can they can traverse the
trees grab that information and then
move on to fix as well as SQL we also
support no SQL we also support web
service requests so if you have restful
endpoints that you also want to use egg
travel teste or your performance you can
also do that okay so what else what else
can we do cpu profiling or in this case
application profile is very very
important to understand where the
bottlenecks in your application are
however because we're in development
time frame and development times are
going to very very very significantly to
our production times because we're going
to be as i mentioned running on
different hardware clustering there's
gonna be a whole bunch of different
reasons so if I let's say click on this
guy you'll notice that I'm going to get
an amount of time back here and if I
click on that we're actually going to
have application profiling on on the
request on that get request which we
literally just invoke now from here I
can see two numbers I can see this here
which is the cumulative amount of time
which has been which has been spent in
this in this method in this method call
stack and I can also see as and when
time spent in an individual method goes
beyond certain thresholds we can see in
this case 20.4 percent of time we've
spent in this render model method so you
don't need to necessarily be a
performance expert we can see it
straight away now as I mentioned that
the development and production times are
going to very largely
don't actually provide times here we're
using percentages we're using something
which is you know relative to our
location so here i can say twenty
percent of my time is spent here
therefore if you wanted to perhaps make
a certain area more more performant
that's exactly where you start okay so
it gives you the clues as to where your
application is spending its time where
the bottlenecks exist once again if i
did want to share this I can just get
myself another URL pass this on and bang
anyone can then see this from whether
from wherever they are okay what else
can we see from here let's go through
him ok so with done application
profiling we've done io profiling there
one other thing which we can look at is
let me come back home oops click on here
if I wanted to click add owner let's
just add myself a last name maple my
address UK as I click add owner one
other thing which can can can really
drag a performance environment down is
its session data it's a it's a less seen
issue compared to some of the i/o issues
but if I as I click on that we can
actually see live what's happening in
your in your HTTP session so for example
here the data type passed in addresses
UK name is maple we can see the effect
this is happening on your session data
and you can see exactly where your
session is weighty where it's like if
you have problems for example if I say
actually this application I don't want
it to add anything to my session and hit
save all of a sudden this threshold is
passed and X travel will notify us this
is a problem ok any questions so far
okay so the question is how much
overhead has ex-rebel added on to this
application the overhead that ex-rebel
will provide compared to a regular
profiler is going to be significantly
more but it's not it's not going to be
anything that is in the in the scope of
things in development going to be
significant because you know the thing
that is taking the time is you going
around clicking and doing other stuff
even if we doubled the time or treble
the time you know it's going to take you
a number of you know a couple of extra
hundred milliseconds or something like
that so while the instrumentation that
extra bowl ads is going to add you know
potentially ten fifty percent or on top
of what's already there because it's a
development time profiler the
performance there is an isn't as
important as if it was a profiler that
is in production production profilers
they are much more concerned about the
additional time because when you run a
profiler and production obviously you're
then effects in the end user there's
another question yet
so the question was we saw we saw
ex-rebel caters for restful web services
is there any other support for other
applications in terms of the restful web
service i'll just quickly show you one
thing that how you would actually use
ex-rebel for your restful web services
what you actually have is this ex-rebel
ninja page so that if you have a
headless request like a restful web
service you would actually have this
running in the background as you make
her invitation to your rescue web
service you would see the notifications
through this ninja page the effectively
dummy page this is almost like your head
to your to your request in terms of
other technologies that we support
currently we just support restful web
services and web applications so yeah
yes
so the question is can you run the
application in a remote server or to
have to be localhost so the way you
would actually attach ex-rebel to the
server is actually as a as a java agent
the way it's actually added in here in
the browser it's not actually a browser
plugin at all this plugin this toolbar
is actually being rendered on the server
side so every request every response
that comes back this little widget is
actually being added into the HTML a bit
of JavaScript bit of HTML so there's no
dependence in the browser everything
gets done on the server side as a result
you can you can do it remotely justify
yes
yeah i mean i click classic classic
profilers you know the the J profilers
the York it's the anything obviously
from J clarity as well no no okay I
thought you okay yeah the traditional
type profilers that are designed to run
across any stage really one of the
reasons why we saw X travel as a good
fit for development is because typically
development for profilers is almost
considered an afterthought it's
something well now you typically need to
start up your yeah you know you're a
separate environment where you would run
profiling on that so yeah the typical
profilers would be those kind of like j
profile of your kits you wouldn't
necessarily run where you wouldn't run X
travel in a in a production environment
or you know stage and or anything like
that this is this is designed to run on
a on a developer's desktop
no I mean I would say I mean it's not
going to be it's not going to be like a
huge order out but i would expect j
profiler and other profilers to be
faster because those profilers are
designed to be run in environments under
load and where performance is much more
performance of the tool is much more
critical from our point of view yes
performance is important we don't want
it to run super slow but it's not
critical that the success the success of
a lightweight development profiler isn't
in isn't in necessarily the performance
it's the way it delivers the the
information back to the user and at
being at development time yes correct
yes is for web applications are you yes
in this instance you if you wanted to
you can add load but the results that
you will actually get back is just on
the click that you just on the on the
request that you make so effectively the
data is actually coming back through on
on the thread and it's just basically on
that request but if you chose to you
could yes happy with spring spring in
sight that's a great question i don't
know do you know that JK how this works
with spring in sight okay say yeah and
they're they're complementary
complementary tools any other questions
okay so the one other thing i want to
show you will leave with this free stuff
so if you if you want some free stuff
everyone wants free stuff feel free to
take a photo of this slide if you do
either a jay rebel or in fact an extra
ball trial a free trial fully functional
free trial we will send you either a
durable or an extra or in fact both if
you choose to do both trials so feel
free to to check that out if you choose
to give ex-rebel ago installation is
extremely easy you simply just add a
java agent to your server process once
time you start your server up you're
good to go you'll see you'll see the
toolbar on your application request and
click away see what happens yes so the
question is do does X trouble and Jerry
will work together because they're both
agents yeah when you have multiple
agents they can possibly conflict and if
you choose to have others they can
because typically they do
instrumentation unless they're unless
they're wary of each other and
knowledgeable about each other they can
potentially cause problems obviously
because both agents are developed by
zero turn around we are very very keen
on making sure there are no problems at
all so ex-rebel ang a rebel will work
very very well together in fact what I
can do I'll prove it what we've got what
I'll show you another another thing
which which X which jerebo which
ex-rebel will do is provide you with
information about hidden exceptions so
exceptions which will essentially be
created maybe not throwing may be
ignored and they affect exactly pollute
the logs they pollute your server logs
if you have a problem you'll find
ninety-five percent of the exceptions or
actually you don't care about
them they're not part of the problem so
how can you eliminate that well if I was
to go into my code
and let's find the piece of code that
that we go through on that button click
files here to add a new exception and I
say this is this is a hidden exception
ception exception and let's just do it
let's just do a prince tight race but
we're not throwing it it's just it's
just polluting the logs as soon as I
come back if I was to click that again
two things have happened first of all
gyroball has done the automatic reload
secondly ex-rebel has noticed this is a
hidden exception and as a result
although it says this is a hidden
exception it's no longer hidden to us
because we can see it through a wet
trouble so although this would have been
something that would pollute the logs
ector abba brings it to your attention
even though there is no UI interference
from the exception so that's an example
of that trouble enjoyable work together
any final questions yes yeah i'll upload
the slides to to the relevant place and
it'll be available through the job one
website yeah okay thanks very much for
attending and hope you enjoy the rest of
job</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>