<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Reactive Streams with Rx | Coder Coacher - Coaching Coders</title><meta content="Reactive Streams with Rx - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Reactive Streams with Rx</b></h2><h5 class="post__date">2015-06-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/g-ixzOcMDF4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">today I'm going to be talking about
reactive streams and particularly using
the reactive X framework so reactive X
stands for reactive extensions that's
what our X stands for if you've ever
seen that around and I'm going to be
using examples from Netflix and a lot of
the information you can find at these
two different URLs I'll provide that
again at the end and my my hope today is
to be able to provide to you both
examples of how we use string processing
and reactive string concepts but also
how it works take out any of the magic
from it there is no magic and try and
give you enough of a start into the
details around it so that you can start
to think in this asynchronous model
where you're reacting two streams of
events and you could hopefully start to
see how it could apply to some of the
problems you deal with everything from a
typical web service use cases down to
your your more typical event processing
needs and so I'm going to start with
more of the finite like web service type
stream model and then move all the way
down to the distributed infinite stream
processing type use cases so I'm going
to give a crash course first with what
Rx is what it's signatures are and how
it looks and feels so if we start out
and we compare it to an iterable an
iterable is a pull based model over a
collection of values and as you next
over it you're pulling type T out as you
move along and when there's an error
that occurs it throws the error throws
the exception and when you complete a
stream of items it it would just
complete and the call stack would
continue to whatever's next on that
particular thread in the push model of
observable it each of those three things
are are also there but they're achieve
through event notification instead so
instead of you pulling a type T out of
it you instead on next to it and it's
pushed to you an error since you can't
throw errors effectively in an async
world they are instead propagated to you
just as another event we on error a
throwable to you and then to signal
completion since we're not blocking a
thread we can't just use continuing on
the stack we instead
terminate with a non complete and so you
can think of that as being zero or more
on next events values passed to you and
then if it's not an infinite stream if
it needs to terminate it terminates with
either an on-air or an aren't complete
and that in a nutshell is the contract
of an observable there's a few other
things that I'll get to for the
communicate back up the channel for Sun
subscribing and flow control but this is
how you admit events down the channel if
you put it on a grid just kind of see
where this fits into the programming
models if you look at synchronous
programming we're typically dealing with
either a a single value coming back from
a data I could get data method it's a
blocking call or we might be doing like
a for loop over an iterable for multiple
values we all use this we've every
language effectively has these concepts
if you move to the async side futures
are a pretty effective way of managing
single responses and I'm not going to
spend a lot of time today on the
numerous different ways that futures are
implemented some of them good some of
them not so good
and just move right on to the observable
because I'm focusing more on the string
processing side today the observable
sits in the bottom right of this
quadrant which means that I can use it
to actually achieve all four quadrants
of this and that's one of the reasons
why it's so powerful for how we employ
it in the use cases we're seeking to
solve and so I can use it to do async
programming over multiple values and by
doing that you can always go from async
to sync and you can always handle a
single value when you're capable of
handling multiple which is why we like
this is our common abstraction let's
start to throw up I'm going to use a lot
of code in this and try and walk through
that if you have any questions at any
point time feel free to interrupt I will
ask you to put it to the end if I feel
like it's gonna be too distracting or
whatever but if there's some
clarification or something along the way
feel free to jump in
I'd rather twenty minutes in that would
clarify something rather than you being
lost for the next you know half hour of
it so on the left side here it's the
very bare minimum of how
unobservable the contractor unobservable
to create one and then subscribe to it
so the start in the and the invocation
of it an observable is lazy by
definition it can be cold or hot meaning
it can represent a cold data source that
does nothing until you actually
subscribe to it or hot meaning it
represents some string of events that
it's always firing regardless of whether
you're watching it or not like the mouse
events or stock prices or something like
that so whenever I say hotter and cold
think of cold like a file that's sitting
on your file system it doesn't do
anything until you actually subscribe to
it and then it reads it so like an
iterable and hot represent something
that's just firing events all the time
it just it's whether or not you're
actually paying attention to it it's by
definition asynchronous and push and
then the observer on the right hand side
this is what receives the events and
it's contract is zero or more on next
events and then it can terminate in
either a non error or non complete and
once a terminal event has occurred no
further events can be emitted to it
however and it doesn't have to be async
it can also operate synchronously and we
also in certain cases allow it to act in
a pull based manner for flow control
reasons and I'll be talking about quite
a bit about that later today we also
take that observer which is a fairly
simplistic interface and we augment it
with a subscriber and the subscriber
implements the observer and then adds a
few other mechanisms and you can think
of these as just the the machinery to
allow the bi-directional communication a
plane observer though is only accepts
events being pushed to it the subscriber
allows us to do both the push down and
also omitting events back up the stream
and so there you see the observer the
observer interface implemented inside a
subscriber but it also allows you to
register subscription events with it and
so that when this thing gets
unsubscribed anything is registered with
it it propagates through the entire
graph I'm not going to spend a lot of
time on that today but subscriber is
where
all that happens and then there's also
this other thing called a producer and
the producer is what allows us to start
to do dynamic push-pull mechanisms so
that we can handle flow control in a
streaming system and that producer is
just a very simple functional interface
that accepts a request of some number of
items so when we start to create one of
these we create it with a function we've
given it a type just because in Java a
type erasure if you don't have type
names for things sometimes they can get
messy but this on subscribe is nothing
more than a single type that takes a
call you could think of it really as as
an action of some kind and it takes a
single type which is the subscriber and
when this is invoked then it that on
subscribe function can start the
lifecycle of emitting events so I'm
going to shove all that code up to the
top here just so that you have it as
reference as I go through this and I'm
gonna walk you through from the most
basic hello world up to more complicated
and then I'll move into the actual
string processing elements of the
presentation so the most basic form of
an observable is this one this is like
this is the stereotypical hello world
but spelled out I could literally just
go observable just hello world but this
is what actually is happening under the
covers it creates something with a
function and that function when it's
invoked it gets invoked and you
subscribe to it for each is just a Java
eight naming convention for subscribe
and just iterate over all the results
and so for each is just doing a
subscribe but it only handles the beyond
next and just bill blow-up if you get a
non error and I'll complete it ignores
so you'll see more of that as we go and
in this case it's completely synchronous
there's no asynchrony at all
there's no threads involved this runs on
the main thread and that's the simplest
you can be this now starts to show how I
can have multiple values I can when I
get subscribed to I can omit as many
values as I want and then I terminate if
I need to do error handling I can
propagate an error however I wish and
then I'm starting to show some
scheduling behavior here that subscribe
on is the equivalent of you putting in
side that function all of your threading
machinery like starting up a new thread
putting it on a thread pull those types
of things
let's subscribe on does under the covers
is it literally just says as I go up the
stream just go and put yourself on a on
a scheduler in this case it's the i/o
scheduler which is an unbounded cash
thread pool intended for doing ILO
operations if you're doing blocking i/o
and so in this case this is now
asynchronous when it runs it's going to
run off on some other thread and system
dot out print line will be executed on
that other thread here this is
synchronous but it's showing here how
synchronous something synchronous can
still support unsubscribe so this is
actually effectively an infinite loop
but when I do a take 10 on there what it
does is it when it receives 10 values
from this and then propagates an
unsubscribe up which then will terminate
that loop yes and so the question was
this take do the unsubscribe and yes it
does and so the composition of this
allows you any operator along a stream
can unsubscribe up and the unsubscribe
when it is the contract of it is that
whenever it's fired it gets propagated
up the chain and gives everything the
opportunity to terminate itself and
clean up resources this next one I'm
starting to get a little bit more
complicated I'm not going to walk
through it all here but what this is
starting to show is how if you have
something that needs to deal with back
pressure and flow control you can start
to use the more advanced mechanism with
a producer and you set this producer and
then it doesn't actually emit anything
until it receives a request and so you
can see in this lambda that it has a the
argument are that's a long that when it
receives that request then it will it
the contract is that it can then emit as
many on nexts as it has been requested
that code actually can just be
implemented like this though with cover
methods on it because most of the time
when you're using that doing that kind
of code you have something that's
already represented as an iterable and
this takes care of all that for you so
I'm going to start out with finite
streams
extremes most of us actually deal with
this a lot just in imperative manners
and then I'm gonna move on to infinite
stream processing and then finish up
with an example of taking that infinite
stream off of a single box and then
distributing it across the cluster to
take a look at how the exact same
programming model applies to all three
of these
I'm from Netflix I work on the edge
engineering team and we've adopted the
reactive programming model at the web
service tier over the last two years so
if you use Netflix on any device created
in the last several years you use this
code every time you click a button all
most of our UIs are also implemented
using reactive programming using rxjs
the JavaScript variant and if you're
interested in the the UI side of things
come chat with me after there's lots of
material on that as well particularly by
someone named dropper Hussain who works
at Netflix as well
and whereas everything I'm going to be
talking about from here on out is
predominantly server-side so we we
adopted this yes we are starting to in
some small places we are just kicking
the question was do we use Java 8 in
production where we're evaluating it
right now for the edge systems that run
our API in the next round of refactoring
on that system we're going to adopt Java
8 so if you look at any of my older
presentations we had adopted groovy at
the top of our stack predominantly so
that we could get the lambda support and
now with Java 8 all of our new systems
were predominantly using that yes we
have been working with the reactive
streams team on the definition of its
contract and there's a few naming
signature changes because we what we
already existed so we couldn't break the
world but there's a bridge module that
just converts the types in the spec and
then it conforms with the spec and so
we're we're one of the groups who's been
involved in that and are consider it's
not shipped 100 yet and we're still
working on the TCK but we're compliant
as far as complain as you can be
it's still pretty one no and the
question was whether or not our ex Java
with back pressure is compliant with the
reactive strains back and so that if you
want to know more about that it's
reactive streams org and most of the
information you'd have to go look at the
github project still to get the details
on that so we we went down the reactive
programming route because we wanted to
take our network traffic and instead of
having lots and lots of small Network
calls coming in we wanted to collapse it
into one Network call that could then do
all of its heavy lifting on the server
with lower latency and then shove it all
out the door exactly is the device
wanted but what that meant is that we
now needed to embrace much richer
concurrency patterns on our server and
we needed to have mechanisms for nested
conditional logic with lots of Network
calls potentially dozens and do this in
a way that not everyone had to read Java
concurrency in practice and we didn't
want people having to worry about the
primitives of concurrency which some of
us really enjoy but even those that
enjoy it
stub their toes on it consistently and
so that's what it led us to adopting rx
this particular piece of code is
actually a functioning piece of code
it's simplified though from what we
don't run and prod just stripped a bunch
of features out of it and I rewrote this
in Java eight instead of in groovy which
is what we still use in prod but this
code is actually what would render the
main screen on a netflix UI which is the
grid of movies so what this does vary
top line there you see it handles a
request response so think of this just
as a request response loop but this is
all async and non blocking
so think servlet except that you never
block a thread with this and once I get
into this the first thing I need to do
is I need to go fetch a user so I kick
off that user object that user request
and I observe over it and then I used
something called flat map flatmap what
it does is it basically is saying add
this callback and you'll get invoked
when the value comes back but then
different from a map function which this
transforms from T to R this allows you
to do further async work so it's from T
to observable of r and
whatever I do in this function will get
invoked when I get called back and then
it allows me to kick off more async work
and I just composed the chain and if
that completely blows it your brain
apart that's ok it took me a long time
to figure that out at first but once you
drop flatmap all the async like
functional style programming starts to
make sense
flatmap is like it's the big hammer you
use it all the time
so as i flatmap over at think of this as
like if you use java completable future
think of it as the and then so and then
do this I get that user out of it now
and then once I've got this user back
I've got these two dependent blocks of
code I want to kick off in parallel so a
bunch of stuff on the catalogue and then
that bunt and then I want to go get this
social data for this user as well so I
declare these and just as I'm going
through this recognize that none of this
stuff is actually invoking until the
whole thing runs it's just declaring it
all so declare these two things in
parallel and that first one what it's
doing is saying go get me the catalog
and then when I get back that catalog
I'm actually gonna get a list of lists
so it's a list of lists of videos okay
and we call this colloquial how are you
say that word you lis low lumo and
Netflix a list of lists of movies and so
you get this back and then within it
each list there's a list of videos so
now once I'm inside that I've now got
the video object that I want to do
everything with and there's three things
I want to go off of fetch the bookmarked
the rating and the metadata for it then
I want to zip this all together zip is a
combinatorial operator and what zip does
is it says you've got a bunch of streams
of data flying I just want to have a
function that will wait until I get each
of the value off of each of the streams
and then apply the apply the function to
emit another value that combines these
however I want to do it if Java had a
reasonable strategy for tuples we could
have a default one that just returns a
tuple but because maybe in Japan we'll
get that with value types but right now
we don't really use tuples and so we can
allow ourselves to do something with
this and in this case the slide isn't
big enough for me to show what combined
data does but think of it as just taking
those types and throwing it into like a
hash map or something then at the bottom
I merge the
output from social and catalog and so
the catalog one is going to actually be
a potentially stream of hundreds of
values from all the video objects social
in this case is actually only one value
and what merge does is it just takes n
number of streams and then just merges
them into one stream and that's all it
does under the hood it's nasty and all
fun and all that stuff to do it in a
non-blocking way but from a mental model
just think of it as taking multiple
streams and into one so then when i
merge over that i then flat map again so
that i can just on each of those events
i want to write it out over a server
sent events stream and so i'm just
writing this out over the network as
things are pushed back to me i don't
wait for the whole thing to come back
i'm just pushing it out over the network
as the events flowed to me to enable all
this yeah merge can be as question was
can merge can merge merge many things by
many things are you talking about like n
number of streams it can it can this one
can only merge observables but there's
lots of different ways you can convert
lists and other things into it but
horizontally can have as many items as
main streams merged a list of
observables yes again yep
the follow-up question was can it merge
a list of observables answer is yes it
can so what we did what the the design
change that we went to with this is we
started creating all of our java api s
like this instead and we call them
observable api's and we treat everything
like a stream anything that can possibly
touch a network at any point in time
even if it's almost always from a cache
in memory there's any chance of it ever
being an expensive operation we call it
an observable api and you treat it as if
it's a sync I've got other presentations
where I go a lot into the the reasons
behind this design today though I'm
going to skip that like 50 slides and
just ignore that concept and we're just
going to assume that observable API is
are a happy place to be for this problem
and I'm going to move into the the
stream processing part of it and so in
this particular example I can
actually there's actually six places
where I potentially kick off Network
calls and in two of them with the arrows
pointing at them I can return n number
of values back and I could actually end
up with thousands of videos potentially
let's say I have a hundred lists if
you've ever scrolled through the Netflix
UI like we come up with Jean roofing
else right
like genres are like descriptions this
long customized to like you and your cat
and so we have like everything you can
you can imagine and then they're all you
have a list of all these and then on
each of those I could have like 200
videos in them so that right there is
you know several thousand videos I could
potentially be running through this and
each of three for whoops
each of three four and five then are
being kicked off potentially two
thousand times and behind the scenes we
do Auto batching and collapsing and all
types of things like that that are all
made possible as implementation details
because I'm never blocking any threads
or call stacks so behind the scenes I
can do things like every millisecond
kick off a batch network called it just
captured everything that was requested
in the last millisecond so as we now now
I'm going to take you into an infinite
stream so everything I just showed you
there its stream processing but it's
finite very short live streams some of
them are only one event but then they're
all composed into a stream of many
events and the exact same mental model
for finite and infinite all work
together the only difference is is that
the resource cleanup happens sooner on
the finite and on infinite you
technically never end we run these
things until we shut down a job of box
kills itself or whatever so as I go into
a finite stream I'm gonna go through
this example and fairly quickly so I
could spend more of my time on the
distributed one at the end because
that's where it gets really interesting
so in this example what this is doing is
we have thousands of machines and I've
got a system that has a network
connection every single one of them and
I'm pulling metrics out of all these
boxes I'm letting the metrics just fly
at me and it's just pushing the metrics
at me and then what I want to do is I
want to say take all those and aggregate
them together and output a single stream
that shows me the aggregate
of all my metrics within the last second
give or take and we implemented that
about two and a half years ago in pre
reactive programming and it's this mess
of concurrent hash maps and Atomics and
threads we got it working but in as I've
gone back and rewritten it it's
interesting how much easier it is to
grok the flow of the of what's going on
and what we use is we start to say
alright I've got an observable if you
look at it above this it's an observable
of grouped observable and so this is
where we start getting into nested
stream streams of streams so I have a
stream that's just emitting to me every
possible server in my cluster and then
each of them represents all the metrics
and then what I do is I group by and
what group by does group by is quite
powerful it's the opposite of merge so a
group by does it takes a single stream
you give it a function that knows how to
extract a key from it and then every for
every item that flows through it it it
uses that key to create a new observable
or find one that represents it and then
emits that value down that path and so
let's say you've got odd and even I have
a stream of integers and I say that the
function is is odd and if it's odd it
sends it down the odd pipe if it's even
it sends it down the even pipe and so I
go from one stream to two so what we do
there is we now have grouped it in this
case what we're doing is we're grouping
by the type of metric and so I don't
actually want to know the thousand
servers instead I want to know the
metric from each server so I go from a
thousand machines to let's say 200
streams representing the metric from
each of those thousand machines then
this one is I'm using this to show you
some of the operators that you can start
to apply on top of streams this buffer
what it does is it says as I'm receiving
these events I want to buffer by into
groups of values and I want to do them
actually overlapping so what buffer to 1
does is it says take me give me the last
two values and then slide the window 1
at a time so I always get like previous
and last as I go down the stream and
then what that lets me do is the map
function I then calculate the Delta
between the two the previous and the
last and the state is all just
automatically managed by in a functional
way but just always giving me the
previous sorry the current and the last
value and then I have the calculated the
Delta and then I omit that and then I
now have a stream of these Delta's that
are coming out at the bottom now you ask
why would I do that
another way of doing this is I could
take all the values and then like zip
them all together and I have like a
thousand streams all coming to one and
then like ski then like iterate that
thing to like calculate each time but
that way I'm synchronizing the whole
world on this one point by instead
calculating hundreds of these Delta's in
parallel then when I scan over this when
I merge it down and scan over it instead
now I'm just summing the deltas as I fly
through this and then at each each
element I'm omitting what the current
value is based upon summing the the last
Delta so what skin does scan is one of
the stateful operators in stream
processing it's the functional way of
doing state and so it's a function that
you pass in a stateful object like a
hash map and to seed it you see this
thing with something that you want to
mutate and then it's going to receive
and every value that's given to you each
on next so think of those V dots along
the top the red green and blue those are
the values they're going to pass in to
you and then you apply your function and
then you could think of it as just
you're accumulating state over time and
so the dots along the bottom you see you
can see that I'm accumulating state as I
go along the way the output of this
simplified is something like this I've
taken JSON like that along the top and
then I output what's on the bottom
except now I've summed together four
different streams and from a user's
perspective they don't have to worry
about whether there's one or n in
reality the code the data looks more
like that and
we use this stream processing system to
allow us to act for a real-time
dashboard aggregate the metrics in about
one to two seconds from a cluster of
servers to a UI that then is render
that's like flickering along telling us
what all of our metrics are for our
servers so in a system like that how do
you do flow control this is often the
question that comes into play when
you're starting to do stream processing
flow control otherwise talked about is
back pressure that pressure is one form
of flow control flow control we start to
do a sink can become a little more
challenging so I'm gonna start with it
you the example where you don't need
back pressure in this particular example
everything that is doing is synchronous
and on the same thread so you don't
there's no queuing involved when there's
no queuing there's nowhere that I have
to make flow control decisions because
every event that's propagated down this
is is on the same call stack and it's
preventing anything above it from doing
anything because I'm on the same CPU and
so I'm reading and processing all on the
same thread if I change that code though
to this I need to I need back pressure
and the reason why is I use this
operator called observe on so observe on
is a mechanism for hopping threads this
is used a lot in you eyes if any of you
do Android development where rx Java is
used this is an essential tool because
you put your processing and them in the
background other thread but at some
point you need to observe back on to the
main UI thread so you can interact with
the UI so everything inherently becomes
async but what happens when you jump
over thread boundaries like that is
generally you don't want to be blocking
the other background thread it's instead
you have queues that allow you to go
through that scheduling boundary
asynchronously so async is all about
queues and so as soon as you've got
queues somewhere you have the
possibility to do one of two things
either it's a bounded queue and you blow
it up and you you fill it in you're dead
or it's unbounded and you have buffer
bloat and you run out of memory or you
might have so much memory in your
machine like we actually had an example
where we had a big like 20 gig
keep on the thing and the developers
just notice like why is my data
progressively getting stale err it was
because the queue is just kept growing
and growing and growing so they were
like minutes behind where the data
actually was and so that's the type of
stuff that happens when you're just
shoving data through a system with
unbounded queues or bounded queues and
don't have a way of controlling the rate
at which the data is flowing so there's
a variety of options the first one that
we've all been using forever is you just
block you Park the thread you do call
stack blocking and that was that first
example I gave you where everything was
synchronous but it doesn't take very
long for you need to have some use case
that doesn't work for either you don't
want to actually block your thread which
is often the reason why you're choosing
a reactive programming or even if you
were trying to use that model as soon as
you start to like merge streams together
by definition you either have to block
or or queue because when you're merging
things together you can can conflict and
you've got concurrency as you're going
from potentially two threads to one so
generally this isn't the option that we
can use and so we don't really
explicitly enable this in in our X by
definition is what we're trying to avoid
so the first line of defense to
controlling the rate of data flow is you
use temporal operators so temporal
operators are things that allow you to
batch or drop data according to time and
these if you've ever done data streams
of any kind you're going to be used to
all of these I can sample data so as I
have a data flow coming through I just
tell it every 10 milliseconds in this
case just to give me a value I don't
care how fast it's flowing I'm just
gonna have one value every 10
milliseconds I'm gonna drop and ignore
everything else
throttle first is another similar one
the difference here is that it's just
taking the first value in every window
rather than the last which is what
sample does debounce is familiar to you
I engineers debounce is very similar to
throttle except it's waiting for a
timeout since the last value so it's
often used when you're doing like when
you're typing and you like
new auto the suggestions in his search
box something like that so you don't
want to kick off like a network call
every time you touch something only when
they've like stopped typing and so you
have like wait 450 milliseconds and so
you can see here that it omits the last
value of each burst of typing or
whatever you're doing you can buffer and
so this allows you to batch things by
time so every 10 milliseconds give me a
list of whatever was done there and so
then I can process them in batch rather
than an item at a time
bring it really fancy I'm not gonna walk
through this diagram but I'll walk
through the code this is this one's kind
of fun if I have a system that is
intermittent and I don't it's completely
non-deterministic what the bursts are
going to be I can actually take that
intermittent stream and I'm not going to
go into this multi casting stuff a
publish rough count what that does is it
takes a single stream and allows me to
share it across multiple subscribers and
then it just deals with the rough
counting on when to unsubscribe but what
it does here is I say on the on one of
those I say debounce which means that I
will trigger each time I see a pause
between bursts and the stream so if it
bursts 10 and then has a lull and then
verse 5 and then a lull it's going to
trigger at each of those pauses but I
don't want just the last value I
actually want the whole buffer I want a
buffer of the whole burst and so that
I'm gonna also buffer over it but I'm
gonna pass in this other stream the d
bounced one and it's the marker for when
to trigger the buffers and so as I'm
going along I end up with a stream with
it emitting events like this so every
time there's a pause in the stream it
emits a list of all of the values that
were captured when that debounce
triggered and so this is how you can
actually start to combine operators to
do far more complicated patterns
you can window window is very similar to
buffer except that instead of waiting
for the entire thing to be buffered and
then emitted to as a batch it triggers
the window and then you get all the
items pushed to you on another stream
during that window you can have windows
that don't overlap select every 10
minutes or you can have like ten minute
windows that overlap by one minute so
every one minute you trigger another
window and then you just keep omitting
events down that so windowing is a good
way of processing data that's been
chunked by time or by count here I can
window by count instead now all of these
solutions so far though typically rely
upon one or two things either I'm losing
data or I'm assuming that I can process
it faster in batch than I can one at a
time there's a lot of use cases though
where the neither of those are going to
solve my flow control problem or even if
it would solve I don't want to lose the
data data loss is not something that
works for whatever the use case is and
so reactive pull is something that we is
what we have called this dynamic
push-pull model that several of us
working on rx spent a lot of time
prototyping and going back and forth we
worked with the team at reactive streams
at org from various different companies
and we shared a lot of ideas and came up
with this idea and thus far it's been
working quite well and what it allows us
to do is we we still want to prefer push
and so we push data whenever the
consumer can keep up with the producer
and so it's still an async push based
system but then it dynamically switches
in to pull if the consumer is slower
than the producer generally what you see
is that any given observable subscriber
relationship is one or the other it's
not very often that they actually are
flip-flopping back and forth because
most systems you're either you've got to
fill the queue or you're fast enough to
handle it unless you have a bursty
system if it's bursty then you can see
it flip-flopping back and forth between
the two one of the the things in the
implementation of our X is we want to
bound all the queues but it's important
to understand we're bounding them
vertically not horizontally yes
a burst the question was define bursty
so a bursty strain would be something
where instead of receiving events very
consistently over time you might in like
a 5 millisecond window receive a hundred
events and then go a second without
anything and then burst again with
several dozen and so examples of that
are in the user interface world a user
will sit there and do nothing and then
all of a sudden they pound their
keyboard and then they do nothing and
they pound their keyboard which is white
D balance is used a lot in UIs
so we bound all that we want to bound
all the cues vertically but for
practical reasons you can't bound them
horizontally and that applies in a few
cases the horizontal unbounded nature
applies on like merge and group by and
those types of things were rather
combining or splitting the cardinality
is defined by the developer and so you
still have to keep in mind that if
you're going to merge a million streams
together you're still going to have
memory allocated for the million streams
and so we don't bound you going side
left to right but any one of those
million streams will be able to have
bounded queues on them so each of them
will correctly deal with their their
back pressure
similarly on a group by where you're
splitting there if your cart if the
cardinality of your group of however you
key it is you know ten million items
you're going to have 10 million groups
that are all sitting in memory and so
that is still up to you to deal with you
still have to reason through that but on
any one of those groups we'll be able to
correctly compose through the the back
pressure through the system so that the
overall rate of flow does not exceed
what any one of them are capable of
handling so I'm going to walk through
code piece by piece to kind of show what
this looks like we have the observable
on one side the subscriber on the other
side and this is bear this bare-bones
scaffolding of creating an observable
and subscribing to it when you subscribe
that event goes up the stream from the
subscriber and it propagates up so this
is the first way that of the first part
of the bi-directional communication it
propagates all the way up through all
the composed operators up to the
original observable and then that
observable it can invoke down on the
subscriber set producer this is K think
of it's kind of a handshake where the to
whom are negotiating the mechanism that
they need to then start emitting data
that producer is set down and it I'm
ignoring all the composition aspects
here of like when you have ten things
all chained together but it flows down
the chain and then we need a hook for
telling it how many you want
and so this on start event which is
typically ignored and unless you need to
deal with back pressure this onstart
event allows you give you a hook where
before any data flow starts you're
signaling up how much you want in the
reactive streams org project this is not
needed because it only allows systems
with back pressure enabled whereas rx
allows it to be an optional thing and so
by default if you subscribe to a stream
and actually requests long max value up
which is effectively saying I just throw
it at me I can handle anything you want
and that is what allows you to do
temporal operators and those things
temporal operators always request max
value up I can handle anything you throw
at me because I'm using a different
approach for flow control so in this
case I'm requesting five up that then
allows the that event propagates up into
the producer that producer function gets
invoked and that producer now by
contract is allowed to emit up to five
items if it emits more than that is
breaking the contract and you could end
up with buffer overflows and all kinds
of fun and typically it means that in
exceptions being thrown so the goodbye
contract he can emit up to five items in
the on next as he receives those five
items at the subscriber he can now he's
free as he receives events to then
request more at any point in time he can
request one at a time or you can request
by batch and so you it's kind of like a
micro lease model or I'm saying I am
capable of handling up to n amount and
then as I receive more I can tell you
that I'm capable of handling more it's
completely up to the subscriber as to
one
he can wait until the whole queue is
drained before he requests more or he
can have high and low level watermarks
it's completely up to the subscriber how
he wants to do it at any point in time
the producer is able to terminate with
on complete and on error so that does
not need to have a request that's always
an option and then the subscriber can
always unsubscribe up those those two
parts of the contract are always
available regardless of back pressure
because they're not part of actual data
flow so I'm going to look look at this
now visually with a cue stuck in the
middle so if I have an observable I have
a chain here and this is starting to
show how when observables compose you
just have chains of observables and
subscribers together so think of this
like you have an iterable that's being
subscribed to I'm using iterable just to
simplify it so it's a cold data source
very easy to think through and then I'm
going to observe on another I want to
move to another thread that means I'm
going to be passing the data through a
queue and then on the bottom side it
puts it through this queue and then on
the bottom it's going to be subscribing
to that on and off on another thread so
when I subscribe from the bottom it
propagates that subscribe event all the
way up to the top the source and then he
calls a set producer all the way down
and so this this handshake goes up and
down the chain and then the very bottom
you'll notice that it requests and up so
this is very often the case where you're
doing like some sort of synchronous
processing at the bottom of your chain
on one thread but right in the middle
you've got something that's async and so
you can see that the request from the
async operator up is not request in or
long maxvalue
it's actually requesting the size of
what it's how much it can buffer
internally so it complete it the each
operator is decoupled and capable of
managing different queue sizes and
behavior so the observe on operator in
this case requests up five and that
means that now there's five outstanding
requests between those two that means
now that the observable producer can
emit up to five items and so he's now
put those five items so there's
to be delivered and as they come along
they get dropped into the queue and so
now I've received one of the queues
there's four outstanding requests and
the other one gets dropped in I receive
another on next I've now got three
outstanding requests you'll notice no
more red marbles are showing up on the
top because he by contract he can't yet
he's already he's only got three more
that he's allowed to emit now the bottle
on the bottom side now finally the
consumer pulls one off the queue and
he's now sending it down on his thread
and then he's free at this point to say
I've drained one so I can request
another one back up that then increases
the requested count by one and the
origin is capable of sending more let's
say that the observable speeds up or the
consumer very slow he dumps all of them
into the queue the queues full
outstanding a request is now zero this
is where in a blocking world that thread
would be parked and it wouldn't be able
to do anything you'd be sitting there on
the offer or whatever it is on a
blocking queue in this case that thread
goes off on an event loop to do whatever
it's going to do and it will be woken up
again at some point later or given work
via a request coming in to it and so
it's a completely async process that
thread up in the top left is not blocked
anywhere down at the bottom now as they
start to drain at any point in time the
subscriber now can request backup so in
this case he waited to request two at a
time instead of one at a time and it
just flows back and forth like this now
as I said earlier generally you've got
the system in one of two modes either
you're fast enough and consuming that
the queues are never filled and this
request process is just cycling in the
background and it's just doing
bookkeeping to make sure that the
consumer is fast enough and you're it's
you're still pushing everything through
the system in the quote/unquote reactive
way and you're pushing the events as
they show up merge is it has some crazy
code in it to make it the fast path is
that it never drops anything into the
queue unless there's contention and the
consumer can't handle it at that point
it drops into queues but otherwise the
fast path is it's
push all the way through but it's keep
doing all the bookkeeping of what the
consumer can can handle so that when
back pressure is needed all of a sudden
it starts to fill up queues and stop
sending any further data but in the fast
path you still get all the benefits of a
push based system so in simplistic code
it would look something like this where
when I start my subscriber I request one
and as I receive a value I request one
this is basically you would degrade the
system into effectively an iterable at
this point if you were just like request
one on next one request one and it you
effectively just turned it into an
interval at that point more common
you've got a buffer of some size so
you've got a buffer of 1024 you receive
the values you and cue them and then
you're popping the Vosges values off the
queue somewhere and you're requesting up
in our we're we've played with different
models of waiting until the whole queue
is drain and then requesting up or
requesting like 1024 you request 768 up
at a time so that you've got that 256
buffer that's all completely
implementation details so on a reactive
pull system what's the behavior between
hot and cold so as I said earlier a hot
stream is something like stock prices
Mouse events metrics flowing from a
server there are things that you can't
take tell it to stop it's going to keep
happening it doesn't matter what I say
the the the best I could do if I was to
employ a naive back pressure solution of
blocking it as I just fill up queues
everywhere I'd fall over dead at some
point a cold data source like reading a
file or like pulling off of a queueing
system like Kafka or something like that
it's very effective because then
everything downstream from it can be
pushing through but the actual source is
just popping off the the queue as as the
consumer is capable of handling it the
other one is what if the source just
doesn't support it whether it's because
it's hot or because they didn't want to
decide the strategy there's a lot of
cases where it's a hot source and the
developer just like look this thing is
hot I don't care what you do with it
it's your choice as to how you're going
to deal with it
throttle sample drop buffer it's up to
you and so because of this this is
the areas where in our ex job but we
chose to make reactive pull optional for
a lot of these types of reasons because
there is no one-size-fits-all approach
to flow control is that if the data
source is hot or it doesn't natively
support this this reactive pull concept
we've got these operators that you can
just drop into a stream that then make
it so you can apply your strategy you
want so that when it gets these back
pressure events it's it's connecting
itself in with all the that producer
requests stuff and you can make it so it
literally just behaves as if there's no
back pressure you can throw in a buffer
and unbounded buffer if you want you can
literally just buffer your system and so
what that means I've got a hot stream
that's being observed on another thread
I could just throw this buffer in there
and then it just treats it as it I just
like stick my fingers in my ears and
pretend that there is no problem
oh yeah if you do this on this would
only work if you have a bursty system if
you know that your system is bursty and
you get like these bursts in like a few
milliseconds and then you've got plenty
of time computationally to drain it and
do whatever gonna do after it this would
work it's still dangerous I personally
prefer something more along lieing of
buffer to abound and then drop or do
something else another one is you can
just drop these are the two extremes you
can just drop if you hit that limit the
benefit of this now as your system is
going to be healthy it's going to stay
up it's going to use memory efficiently
but you're just going to drop data
whenever you hit those limits you could
think of it more like this on back
pressure I've got a signal now that
composes through my whole line and then
I start to apply my strategy I'm gonna
come back to some of these strategies
after the distributed model I want to
close out my presentation on so in a
distributed stream processing system
looks very similar except we we need to
handle network connections and so this
is an example that does very basic
anomaly detection in a system that we
call mantas that we're building at
Netflix and the source here is movie
attempts and so these movie play
attempts are coming from a cluster of
servers they're just being fire hose dat
us and let's just ignore for right now
whether these are cold or hot coming
from Kafka or whatever we do that on a
based on the use case if we don't want
to lose data we throw them in Kafka or
something like that or sqs or whatever
if there's if it's something that it's
okay for us to lose some data and we
would prefer to not have the resources
overhead or have lower latency then we
just firehose them at us and then it if
we lose point one percent of them it's
okay so as that comes through that's
stage you'll see that there's two stages
here we're not quite to the point yet
where we're able to like compile down
single instance code and automatically
distribute it so our current API still
have you explicitly showing where the
the network boundaries are and you can
think of these stages as just big blocks
where you're saying from here to here
there's a network divide right here so
just reason about that so in that first
stage where I'm doing is I'm taking the
data and this is running on multiple
servers in parallel I have no idea which
box them on I don't care
I've just got a stream of data it's
giving me these play events and I'm
grouping on movie ID and what that looks
like just conceptually from a
developer's perspective forget the
distributed nature for a second I'm
getting a stream of movie of play
attempts a group on movie ID and I could
end up with like two streams like this
in reality you end up with like
thousands and thousands of different
streams but you end up like that and
then you from that stage you're sending
it off over the network to the next
stage and it looks a little bit like
this so if I have these events movie IDs
1 2 3 4 5 3 4 5 6 7 5 6 7 8 9 coming in
who knows on which box they flow into
that stage one on one of the many
streams that are there we do the group
buy and it determines which of the boxes
on the next layer are handling that key
and it sends them over the network then
to the next stage for that key and that
they can now be processed the next ones
come in on completely different boxes
but they're all aware of which key is
associated to which box on stage 2 so
again we propagate them over so that we
end up with a consistent stream of all
events for a given movie ID on the
correct box and so as a developer you
can now start to reason locally about I
am working with all this with everything
on this one line the
is very similar a lot of any of the
other stream processing systems that you
look at out there pretty common stuff
here what this allows us to do though
now is on the next stage I don't care
how many boxes I'm running on in
parallel I window over that and so now
I've got a single stream of play
attempts for a given movie ID
I may have tens of thousands of these
running in parallel and I window by ten
minutes or a thousand events and so I
want to say it just give me all the play
events in chunks of ten minute windows
or if there's up to if if I burst for
some reason to a thousand let's chop it
there so this one took only seven
minutes to hit that instead and then
within that windowed within that
10-minute window then I flat map over it
just showed you earlier allowing you to
go from one event to many and then I use
reduce now reduce is normally a very bad
thing in an infinite stream because
reduce goes down to one event and so in
an infinite stream reduce will never
finish but because here I'm windowing in
ten minute chunks each ten-minute chunk
is a finite mini stream within the big
stream and so I can reduce over it and
as I reduce what it's going to do is
it's going to collect state from that
ten minutes at the end of ten minutes is
going to emit to my collected state and
so that experiment object which is that
fail ratio experiment that's the state
that I seed into it the play attempts
are fed into it I update whatever state
I want and I return it and it just
iterates there for ten minutes
and then when I admit that at the end of
the ten minutes I filter out all state
full types that are that are not
triggering my threshold for errors this
is all very simple trivial anomaly
detection for a that much code on the
screen and so we filter over and we
we're basically just throwing away any
of these windows that don't meet the
threshold for alerting and then I if I
do meet the threshold
I transferred it with the map function
into a report which then in this case I
just trivially sent an email alert in
real world we're filing this into like
page or duty or different things like
that
and it becomes an alert then we can go
back and look at the data you can see
that the experiment get examples in
there we capture and correlate all the
the data events that signal why we this
happened so long all this what are the
strategies for dealing with back
pressure in a distributed system one of
them as you can buffer often that's not
very useful
you can drop data metrics in all the
metrics collection ones that's what we
do if you miss a metric that the 500
milliseconds when I'm getting another
one from the same box totally okay we
just drop data all day long when it's
metrics like a lot of the operational
use cases we do that we can sample you
can get more fancy with all the temporal
ones this one though is the most
interesting in the distributed system we
don't quite have this one working as we
want it in production yet but we're in
process of doing it where you use that
on back pressure event to then trigger a
scale event we use Mesa under the covers
for this to then scale out horizontally
and launch new basically new worker
nodes and see then you just start to
like move groups off to other boxes and
that's where we're headed with it
because we can launch a new box and
start up a new node within a few seconds
so we'll just buffer the data for the
few seconds and start sending it off to
another machine so we have found this
reactive programming model to scale very
well from finite streams that we use in
the web service request response model
up to far more complicated stream
processing systems that are now starting
to build anomaly detection and
operational systems on top of and we
have come to leverage this abstraction
all over the place and found it to be
very powerful you find a lot more
details on the website which is starting
to come along the RX Java stuff just has
recently hit one dot oh release
candidate and we've been working on it
for about two years
the community has been awesome around
this there's like ninety ish
contributors we've had like 80 plus
releases and we split out all the used
to be the RX Java was the umbrella for
all the other jvm languages just while
we were in rapid iteration we've now
split them all out RX calles its own
thing just over the weekend we quietly
release the first one
that on its new org ID without Netflix
anywhere in the name it's just I Oh dot
reactive X and the new the new website
is starting to become the place where
we're going to collect all the
documentation for all the different
languages in one place and more
information can be found here and if you
go to my speaker deck account there's
other presentations on some of the more
basic aspects of our X Java since ice I
skimmed through that very quickly in the
last few minutes I take questions the
question was about cues which cues are
you referring to
so most of most of the time when I'm
talking about cues here I'm talking
about in memory cues within a box when
in the distributed system when I start
talking about cues like sources for data
we sometimes use sqs and we also use
Kafka so Kafka is actually were more of
our stuff goes for that type of stuff
but sqs is also used in some places yep
pardon me the question is can you back
pressure to Kafka yes it's actually it's
actually the stereotypical like perfect
example because effectively Kafka is a
big iterable you can think of it like
that
a really big distributed interval but
you're pulling off and so I can say I
can have an in memory buffer of let's
say 128 I can request off up to 128 I'm
ignoring all the like aking let's just
forget acting for a second farmy Kefka
support is not because we are X Java has
zero dependencies but it's a very simple
bridge that we've built internally I'm
like we could easily do an Rx Kafka
project we wanted to
the question is does the expanding and
collapsing on top of mezzos happen
magically or do we have program for it
it is definitely not magic that is
distributed programming yeah so we are
building basically a asos framework on
top of it there's more information as
presented at that at the Mays oscon and
we're working towards an open source
release of that some more will come but
yeah scheduling challenges abound with
that stuff so yes the question was do we
use history any of this history is
another library that we the Netflix that
we use for fault tolerance across
boundaries in rx itself it does not have
any relationship of hysterics because rx
is a very low level library that itself
it doesn't actually pay attention to
network bounds hystrix itself uses rx so
as of historic 1/4 which eventually will
be released final is completely
rewritten with rx under the covers and
then in that example I showed at the
request response the very first example
I gave every one of those network calls
is doing it via history command that's
using rx under the covers so I just kind
of glazed over that because my mother
the point of my talk today was not fault
tolerance but every single one of those
network access calls was a history
command the question was is histor it's
an abstraction of this stuff I I'd say
it's an implementation that of bulk
heading on top of rx you could think of
it that way but it's not as clean as it
would be if I was to do it from scratch
B for backwards compatibility reasons if
I was to do a historic like 2.0 it
probably looked much more like just
operators inside an Rx stream
pardon me the question was have we
migrated from groovy to Java 8rx job
itself is always written in in Java like
the core lib it's actually written to
Java six compliance painful for all of
us who actually write the code but so we
don't actually get these lambdas in the
in the writing of rx itself but it fully
supports Java 8 lambdas because of the
Sam types we've made sure that we've
paid attention to type eraser and all
that insanity from the beginning if for
no other reason than to support like
closure and groovy on top of it and in
Netflix production we're still using
groovy today I don't see it going away
for everything at the very top but now
that Java eights out we're starting to
do more of that and then also because
most of our UI teams are more
comfortable JavaScript nazarone is also
something we're evaluating at the very
top of our stack they'll ask questions
all right thank you everyone</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>