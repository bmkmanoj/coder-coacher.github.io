<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>From Java to Assembly: Down the Rabbit Hole | Coder Coacher - Coaching Coders</title><meta content="From Java to Assembly: Down the Rabbit Hole - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>From Java to Assembly: Down the Rabbit Hole</b></h2><h5 class="post__date">2015-06-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/7p1_S-6bWhk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so this is going to be an interesting
experiment these slides did not exist
until I finished them at 5:00 this
morning but it's it's gonna be fun
hopefully I remember what I actually put
in them what we'll dive right into the
rabbit hole now information on me
currently at Red Hat and everybody asks
why is your hat not red it's really
funny but you know I if you've ever seen
the actual Red Hat that red hat
employees get it's it's it's pretty out
there like if I had a full-on red zoot
suit maybe I'd wear that but it's not
really my style JRuby a lot over the
past six seven years but getting more
into just general JVM language work and
JVM hacking and digging and educating as
well and I'm hideous on all the
different services so it should be easy
enough to find a find me and get in
contact so what are we going to do today
how many folks have seen my bytecode
talk that I've done at JavaOne a few
times all right there's a few folks how
about the JIT talk where I show how the
JVM optimizes stuff some of the same
folks not too many ok that's good so the
the I've done a bytecode talk where I
talk about the basics of bytecode and a
JIT talk where I talk about the basics
of how the JVM optimizes and and how you
can learn from it and what I always got
at the end of those talks was people
wanted to see more practical examples
how bytecode applies to the work that
they do the code that they write how the
JVM JIT actually affects the code that
they that they use on a daily basis so
this kind of turns those talks on their
side and says let's look at a few key
Java features interesting Java features
and look at them all the way down the
stack and see what kind of byte could we
get out of them how the JVM optimizes
them and what we get for native code at
the bottom and that's what we're gonna
do today we're gonna go through a few
interesting Java features including a
string switch from Java 7 and lambdas
from Java 8 and see how they actually
compile at the bytecode level and what
the JVM does with them after that so now
the first question that people ask is
why would you want to do this why do I
need to care what's happening at the
bytecode level and really why do I need
to care what's happening at the native
level and so to that I ask another
question who are you as a developer
presumably mostly Java developers in
here probably folks that have played
with some alternative languages in
general at some point we're all have to
be performance engineers we're all
looking at our code and trying to figure
out how to make it run faster how to fix
some bottleneck how to figure out what
the problem is with a production system
of course we're all debuggers we all
need to figure out what code is doing at
runtime and and understand why it's not
doing what we expect so really we're all
of these things and having a better
knowledge of what happens at the
bytecode level what the Java C produces
when it compiles your code and what the
JVM is doing with your code once it runs
it and optimizes it will make you a
better Java developer performance
engineer and debugger all the way down
the stack the other half of this is that
the details really do matter there are a
lot of cool features in Java and other
languages but in every case they have
some hidden costs you can't build an
abstraction without leaking a little bit
inner classes had the cost of creating
extra objects all the time
structural typing in Scala has to use
reflection for most of its invocation
serialization uses a lot of reflection
tricks it ends up being pretty slow
internally to support what seems like a
fairly obvious and interesting feature
and the way that your code is designed
is going to impact the performance of
your applications so understanding what
actually happens at the JVM level gives
you a better idea of what your codes
going to be doing on the machine who's
heard of the sufficiently the the
fallacy of the sufficient sufficiently
smart compiler this is a an idea that
you've given a high level language that
is not quite as fast as some low level
language if we could just have a better
compiler for that high level language
eventually we'll be able to compete with
the low level language and in in general
this is not true or at least it's not
true for a finite lifetime that we live
in today so if you look at this in terms
of the JVM the way this is usually this
is often phrases if you wait long enough
either at runtime or in the longer now
of waiting for better JVM to come out
eventually the JVM will just optimize
everything and I can write whatever crap
I want right that's what a lot of people
think the JVM does do
less things with code and can do a lot
of optimization but it will never be so
smart that it can work around somebody's
been awful code that's doing the wrong
thing or doing bad things so we're going
to assume that we don't have a
sufficiently smart compiler yet and try
to figure out what's actually happening
in our code now so part one of this I'm
going to give you a little primer on the
different tools that we're going to be
using during the rest of the talk a
little bit about how the JVM works and
some vocabulary and terms you might have
not heard before so these are pretty
easy we're gonna be talking about the
Java source that goes in gets compiled
down to bytecode
which is essentially just the binary
version that works across all platforms
and the JVM knows how to execute that
all JVMs can execute standard bytecode
eventually the the bytecode on most
production JVMs will get compiled in
native code and this is assembly machine
code however you want to refer to it
that actually runs on the cpu and we'll
be looking a little bit of this with
some some primer and some help to
understand what's actually happening the
heap is the JVM controlled area of
memory where Java objects live most
folks know the term JIT or have heard it
JIT and äôt basically just mean
converting a higher-level form of code
into some lower-level form of code at
some point in the in the execution where
JIT does it at runtime maybe just before
you run it
maybe after it's run through an
interpreter for a while and ahead of
time always compiles it on the flat
compile time probably offline probably
like a C compiler it turning it into
native code long before it's ever
actually executed so the basic flow of
things in the JVM up at the top you can
see we got our Java source goes through
Java C &amp;amp; JVM and turns into JVM bytecode
and this is where most folks stop as far
as their understanding of what actually
happens running on top of the JVM but
the JVM then takes that bytecode and
will interpret it for a while and as it
interprets it gathers some information
about the targets it sees the methods
that are being called the types that are
being used and eventually it decides
it's had has got enough information for
the the JIT compiler to run so the
interpreter then tells the JIT to go off
and compile this code
and the JIT compiler ideally produces
some more optimal higher performance
native code and then that executes for a
while and you know things may change at
runtime and new type comes in or
suddenly a branch that wasn't followed
starts to be followed may need to back
off to the bytecode interpreter again
but ideally we've optimized stuff at
some point to the best of our ability in
the JVM using inlining and other tricks
we've got some other vocabulary here so
inlining inlining is basically just
inserting the the code of some method
you're calling back into the caller so
you avoid doing the overhead of the call
itself but more importantly you can
treat those as if they were actually
written as a single piece of code and
optimize them as one if there's values
that aren't used variables that aren't
ever read you can eliminate all that
code and reduce the overall overhead and
then the bottom line of all this is that
is optimization an optimization put as
simply as I can put it is doing the
least amount of work possible to
accomplish some goal and that's what the
JVM JIT is trying to do it's trying to
understand the essence of your code when
it runs in the interpreter for a while
and reduce the native code down to only
what's necessary to accomplish that
right so what's the difference between
those two inlining is is probably the
the most important mechanism at the JVM
level for getting good optimization
allows a broader view of the code and
allows doing optimizations across many
calls pulling them all together into one
all right
little flow chart just to kind of
demonstrate what inlining how inlining
actually works so inlining we've got our
target object and our arguments we want
to call will load those onto the stack
or load them into memory or register
somewhere and then we need to check and
make sure that we're still calling the
same target type if we are we can just
run that code directly and you you'll
see later in the assembly code that it's
actually emitted directly in the same
line same execution path as the original
code if it turns out that we're not
calling against the same type or we
don't have the right method we branch
off we do our old method lookup again
and then run that as a separate piece of
code somewhere and this
for instance methods for static methods
is actually a lot easier we always know
that it's going to be the same method
because they can't change there's no sub
classing there's no overriding so we
just load the arguments and we can call
that target directly and so you know you
might be tempted to say oh well let's
just write everything as static methods
well obviously that's then you'd just be
writing C so the tools we're going to be
using today
everybody knows Java C some of you may
have played with Java P Java P actually
takes a compiled bytecode in a class
file and dumps out various aspects of it
as a well human readable format it's
it's mostly readable but you can see the
internals of the class the other flags
here are JVM flags that we pass in print
compilation will show you the methods
that are being compiled when they're
compiled just telling you that the JVM
has decided to JIT this method into
native code print in lining shows kind
of a hierarchical view of all of the
methods that have been in line together
so you can see whether it's in lining
the way you want optimizing the things
you need and then print assembly is via
a separate plugin will actually dump out
the assembly code so we'll start with
something really simple here just a
hello world here is our basic hello
world thanks the Java probably the
longest hello world in the programming
world so what we'll start we'll start
simply with this and take a look at what
the bytecode is so Java P essentially
Java class file disassembler here is the
the command line without any flags and
what you get out of this is basically
the the rough class structure you can
see the name of the class you'd be able
to see if extends something you can see
that it's generated the default
constructor there as Java C does for for
all classes if you don't provide one and
there's our main method but we actually
want to see the bytecode so for that you
pass in C the dash C flag produces the
actual code that's been emitted by Java
C and we cut out some of the noise here
here's the main method and we've got
four byte codes that are being executed
and we'll look at those in more detail
here so the first byte codes we're
seeing or we see a get static
as you might expect there's a put static
that goes along with this and this is
just for doing static field access the
equivalent for instance feels would be
get field put field LDC stands for load
constant it just loads a literal string
or numeric value onto the jet onto the
stack to be passed as an argument or put
into an array or whatever invoke virtual
is for a concrete instance method along
with invoke virtual there's invoke
interface invokes static and invokes
special which is used for constructors
and super calls and then return without
a type prefix on it is just a void
return from a method so we go back to
our main and we've got our get static
here getting the field system dot out
from from java.lang system which is a
java io print stream so we've got our
system out we're going to load the
constant hello world onto the stack and
we can see the string here literally in
the in the bytecode do our invoke
virtual of print stream dot print line
given that hello world string prints out
the string and then returns fairly
simple and and for this for this case
it's very easy to follow what this code
is doing at the bytecode level so let's
actually go to the next next level down
here and see what and see the JVM
compiling this code so we're gonna look
at the print compilation and print in
lining flags now there's a little word
on the JVM JIT I mentioned that it
interprets the code first interprets it
for awhile gathers information about it
and eventually reaches some threshold
may be the number of calls that have
been our count may be the number of
calls it's done may be the number of
times its branched backward in a large
loop the JIT will fire and turn that
bytecode into native code so the classic
JVM had two different modes that it
would run in the client mode in the
server mode client mode did a little bit
less optimization but it didn't take us
long and to compile it could get nice
get some pretty good native code up and
going very quickly the server compiler
took much longer to compile but it did a
much better job of optimizing so when
you run with the server mode things
would start out slow it'd take longer to
why
but eventually you get much better
performance than the client VM and he
worked from two to five times as fast in
Java seven and eight
they've started to introduce the concept
of a tiered compiler it sort of combines
the two the tiered compiler as a first
step goes into a version of the client
compiled code that still has some
profiling in it so it's running native
but it's still gathering some additional
information about how to optimize it
about the types and methods it's seeing
later on then it can also go into the
server level compile use that same
information to get full on performance
so the idea is to get the startup time
of client mode with the eventual
performance of server mode that's not
quite there yet but it's certainly
improving okay so back to our little
hello world example now I mentioned that
there are some thresholds that have to
be reached to in order for it to compile
with the server compiler which is what
we're going to be looking at here you
need at least 10,000 calls to a method
for it to optimize I'm doing a hundred
thousand just to make absolutely sure
that JVM is going to decide to compile
this and we've moved our hello world out
to a separate method so we can see that
method compile and then possibly see
that method inline back into main all
right so print compilation is first
you'll see that you see the print
compilation flag here the other 2x batch
and - tiered compilation are basically
to just get a little bit more
deterministic output from the
compilation logs X batch forces the JVM
to only use one thread in in-line
synchronous with your code executing
compiling that code rather than doing it
asynchronously it's going to turn out
you might have your results print out
before the compiler results print out
and so on I mean turning off to your
compilation just guarantees we go
straight to the optimized compiler
alright so here do we get for a
compilation log the second column here
is basically just the the compiler event
the number of the compiler event that's
run the first one that actually compiled
was string hash code not too surprising
that that would be the first one to
compile considering how heavily had be
used in the JVM and string index of
comes after that the utf-8 and Co
is basically just decoding code decoding
strings and whatnot coming out of class
file so that gets hit but a little
further down here at events 48 and 54 we
can see our holo method get compiled
down to native and about how many bytes
of actual assembly were required for it
and our main method as well a little bit
larger because we've got that loop
that's executing the code so now we all
actually see how it's how its inlining
these things and optimizing them
together we'll throw in another couple
of another couple flags here print in
lining is there but this is considered a
diagnostic option so you pass unlock
diagnostic vm options to the JVM to get
this one to be available for you I'm not
sure why it doesn't seem to have any
performance impact but they want they
want you to be really sure you want to
look at this stuff it's very deep stuff
ok so here we go now we actually see
some inlining happening if we look up
here at index of we can see that it
tried in trying to inline another method
index of supplementary but it was too
big so it couldn't inline that one in
the utf-8 encoder we've got encoder a
loop and it's in line
char buffer array char buffer array
offset decided that these were hot
enough that and called frequently enough
that it was going to inline them all the
way back and optimize them as a whole
now of course we have our our methods as
well down here here's the hello' method
pretty small it just does the print line
call and it was a it was hot it was it
was hit every time that they that this
method was called worth optimizing so
print line got in lined into our hello
and then within print line it does a
print as well it does a print of the
string you provided and a print of the
newline character and so that got in
line too so it all came back into the
same piece of code natively well further
down we see our main method and we also
see that it tried to help inline hello
it wanted to optimize those two together
but because it already had a lot of code
in it it decided this is too big I'm
just gonna let it remain its own piece
of code not going to optimize that into
the main loop we'll just let it run on
its own so you kind of get a structure
of
what the JVM is doing internally the
decisions it's making about how to
optimize this inlining hello and print
line seems like it makes some sense in
lining hello into mein maybe doesn't
make as much sense for the way that this
code executes so the others the other
parts of this output along the far the
far left side the first column here is
how many milliseconds have passed since
the JVM started up for it to decide to
do this so our hello method actually was
compiled about 0.4 seconds into the
execution of the the JVM the B flag is
new I'm not sure what that means it may
mean that it's a normal bike coded
method there are also some flags for
native methods as well oops the % here
is for something called on stack
replacement OSR this main method you'll
you saw it's only called once it's
called by the when the JVM starts up and
then it has a long loop in it the JVM
can still optimize methods like that it
can decide this method is long-running
it's got a loop in it maybe it's maybe a
good idea to back off for a second
optimize it and then let it continue to
run from that point and that's what this
means it means it's compiled main into a
piece of native code branched from the
bytecode interpreter into the middle of
that compiled code and continued
executing so on the execution stack it's
actually replaced the running code not
usually practical unless your entire
application as one giant loop great for
benchmarks though the JVM guys will say
that it's it's it's there for benchmarks
alright so the next level down and this
is the deepest we're gonna go we will
not go into micro code or anything at
the CPU level but we're going to look at
some assembly that's actually output by
the JVM JIT this is the print assembly
flag and I'm sure you're really excited
to see this stuff but we will look well
it'll be hopefully well explained so
we're pulling the compiler flags out we
don't need to see those anymore print
assembly is also a diagnostic vm option
and as I mentioned on this slide here
Google for hotspot print assembly
there's some pre-built dynamic libraries
you can drop into an existing open JDK
JVM to get this flag I don't believe
this flag ships as part of the normal
JVM on its own but it is available in
open source too so there's our print
assembly flag and there it is
everybody knows assembly you can read
wrote this is well obviously that even
the holo world ends up producing much
more code than this in fact this is a
tiny tiny slice of what the native code
for HelloWorld is the server compiler
actually produces 2700 bytes of assembly
just for the holo world the basic hello
world the client compiler like I said it
doesn't do as much optimization for
hello world it only produces about 600
bytes of assembly and in the server
compiler since you know client is is
basically not in lining anything not
doing the optimizations most of that
server output and that extra assembly is
just from the extra optimizations from
the inlining that it does trying to pull
more code together and optimize it so
that's where you get more profiling
generating more code better performance
but considerably slower start up on the
server compiler so we won't look at the
holo world will back off and make this
even simpler so here we have a main that
is going to loop a hundred thousand
times calling this tiny method that just
adds 1+1 let's see what this actually
looks like well first of all if you've
ever tried to benchmark something like
this and see oh let's see how fast it
can add two numbers together Java C is
actually smart enough to see that you're
only ever adding 1+1 in the actual
bytecode generated for the tiny method
it just says well I'm just going to load
two since I know what 1+1 is and I'm
gonna return that so we're not actually
doing an ADD operation here even before
we get down to the native level now if
we see what the compiler is doing with
this ok two bytes yeah that's obviously
pretty simple it's just loading a
literal two and returning it and this in
this case hey why not we'll inline tiny
right into the main method as well now
if we go into the the assembly code for
tiny now we're starting to get to a
little bit more manageable size the
instruction at the bottom there is
actually ret cue the the
return''function that ends the tiny that
ends the tiny function here and there's
a lot of extra noise here that's JVM and
call stack related but the important one
or the important two functions are the
move here which is moving the number two
into the a X register and then the
return at the bottom that returns that
value the rest of this is just JVM stuff
now if we look at the main method it's
certainly bigger like I say the server
compiler in lines this stuff together so
there's gonna be a lot more code and
you'll notice that it doesn't actually
do a call to o2 it well maybe it's hard
to notice that let's see if I can boil
this down a little bit
first of all we'll pull out some of
these little comments that they've added
boil it down to just the actual
instructions and that's still a little
bit noisy what is what is all this stuff
here so we look at these lines the top
three lines and the Toula down further
towards the bottom this is actually just
the normal the call protocol on x86 it's
pushing a memory stack up and down
staving off arguments and so on
so we go into the method push the stack
down leave the method pop it back up we
can remove that it's not interesting for
what we're looking at if you go further
there's these tests that don't seem to
be doing anything test EAX and they're
both testing against some unusual memory
address here oh well these are actually
what's called in the JVM a safe point
this is pinging the JVM to see if it
wants to D optimize code if it wants to
run GC it's kind of like okay I'm doing
all this this loop this really important
loop that adds one don't one do you need
to garbage collect I can stop okay I'll
keep going and then it just does that
repeatedly the reason we see too here is
there's one done on the back edge of
that loop so every time the loop goes
back it says I did another loop do you
want a GC or something all right I'll
keep going and the other one is right
before it returns from the method checks
if the JVM wants to do some JVM cleanup
stuff and so those are not interesting
to us as far as optimization either so
we'll take those out now if you look at
in the mid
we've got something that looks like it's
exchanging a X for a X what's the point
of that huh
that's that's a zero flag this is
essentially a no op at this point I mean
it really is doing almost nothing in the
code and take that out and now we've
really boiled down to kind of the meat
of what's actually happening and we can
actually make this even simpler by
changing these hex offsets for simple
numeric offsets there's our 1 million or
should be 100,000 loop we move 0 into
the SI register and then we jump down to
4 so we'll compare that with our
terminal condition on the loop if if
we're if it doesn't jump out we go back
up to 3 increment it check it again jump
back to 3 eventually we branch out and
return ok fairly simple and this
actually this is a this is a trick I
tricked you here this is the client
version of the compiler code which I
said doesn't do as many optimizations it
is actually if you look at this code
it's not accomplishing anything it's
doing just a loop that increments this
value in a register up to some terminal
value and then finishing the method if
you run this with the server compiler it
actually just becomes this and so all of
that work that we thought we were doing
and on our wonderful 1+1 benchmark boils
down to return I'm done so hopefully
this is art this has been the primer
hopefully with with this little
background now you realize it's not that
hard to understand this stuff once you
know what to look for and that's what
I'm going to try and get to you show you
in the in the second half here so let's
get into the fun part all right so these
are the Java features that we're
actually going to see go all the way
down to assembly through bytecode and
through the optimizer ok final fields
we'll see what synchronized and actually
synchronized and virtual do here we're
gonna look at string switch which came
from Java 7 and we're going to look at
lamb doesn't how they act
they compile all right so final fields
everybody knows final field can't be
modified pretty much they can sort of be
with reflection and things but we'll
ignore that for now they're supposed to
be final or constant and as a result the
optimizing pipeline of the JVM can take
advantage of this in certain cases but
it doesn't always it's getting better
over time bidding it's learning to trust
final and assume that you're not going
to do horrible reflection tricks behind
its back so here's our first simple
example of final Fields obviously the
the static final my string at the top is
a literal value the JVM and actually
Java C as you'll see in a moment can see
that this value is always going to be
the same no matter what my property
there is final but it's calculated on
boot time when this class is loaded it's
gonna go out get the Java home property
and then store it in that field so what
do you think we get for bytecode for
these two lines print out my string
printing out my property hmm some of you
know the trick here so if you look at
the the my string which was just a
literal string value anybody who has
done incremental compilation of Java
code has eventually had a constant not
propagate after being changed Java C
will actually just go and copy that
value out of the code and insert it
directly into the bytecode wherever it
sees it so in the bytecode we've instead
of going and getting that static field
every single time it actually just has
that string literally in the code and so
if you were to go and change that string
and not necess dit they would still have
the old version and this is a little
trick that Java C does to try and make
these literally be constant values of
course the other one is dynamically
compiled dynamically determined at
runtime at boot time so down here it
actually does go and get static that my
property field so we'll we will end up
going back and getting that field every
single time now let's look at this in a
little bit more interesting example
we're gonna just add the hash codes of
these two fields so my string dot hash
code in my property dot hash code and
this is what will actually boil down
and see what actually happens at the
native level so we know that my string
will get inserted into the into the JVM
bytecode directly my property will
always be a field read let's see what
happens at the native level now if you
look at the native code what may be
surprising is that rather than so this
isn't - this isn't too surprising up
here we've got the literal string is
actually in the assembly code again
because it was literally in the byte
code directly doesn't need to be read
from some location in memory it just
goes straight into the into the register
here load that static string but what's
surprising is that this is the actual
Java home value has also been inserted
directly into the assembly code at this
point it's not going off to some memory
location - look at that in my property
field and this is actually a new change
that came along with Java 7 in Java 7
final fields final static fields that
are calculated at class loading time
will also in line they will actually
optimize the same way as static final
literal fields and this is this has
helped a lot of optimizations for lazily
calculated values values that could not
just be a literal string or number like
you know database connection and so on
so this is this is new but it actually
does optimise this down to a literal
load of that string okay what about so
those are statics and statics appellees
have their own characteristics what
about instance finals so a very similar
case this is an instance string and
again calculating Java home property now
in this case I was actually kind of
surprised Java C actually was able to
see through this too and I think this
may have been a more recent change as
well
where I thought that it would always
have to go back to that instance field
and check it Java C saw that it's only
ever initialized to this one string
value and again it put that literal
string right into the byte code my
property again it can't calculate it
can't figure it out until it's actually
run so it can't put any literal value in
there let's actually see what happens
with this at the native level all right
so
as you'd expect with Java see putting
the literal string into the bytecode the
native code does also have the literal
string directly there now the funny
thing is it goes off and it still tries
to calculate the hash code for that
string and this this this actually leads
into like it is the hash already
calculated no I'll go calculate it and
I'll store it but if it already knows
what the literal string is and the hash
code algorithm for Java is part of the
spec why wouldn't just put the hash code
in there so that's a question here's the
plan my property
as you'd expect there's nothing I can do
about optimizing this in well I
shouldn't say that it probably could
optimize this if it saw you were always
calling against the same object maybe it
could only go get it once maybe it could
optimize it a little bit better it does
not do that yet so that's something that
needs to be improved so on the one case
we've got it actually going and doing
the hash calculation in the other in the
in the property in my property case we
have it still going and getting that
field every single time so we found some
stuff that hotspot could actually
improve and this is the hole
sufficiently smart compiler we're always
going to find little pieces like this
where the JVM could be doing a better
job okay moving on to the next one
concurrency stuff a lot of people throw
synchronized and volatile around but
they're not really sure what's actually
happening at the at the JVM level and
certainly not what's happening at the
native level so let's take a look at a
couple quick examples here I've got two
versions of a get time method both just
call system dot current time Millie's
but ones synchronized for whatever
reason I'm just I'm really worried about
that being safe so I threw a synchronize
down there just to make sure just to be
safe up at the top we do print line get
time print line get skip time
synchronized and if you look at the byte
code for these that you really see no
difference on the call side which
perhaps isn't too surprising we load
system out we get the time and then we
print it and that's all there is to it
we also don't really see anything on the
definition of the methods other than the
synchronized flag is in there so this is
not
it's not something that actually gets
changed within the body of the code
that's generated by Java C it's a flag
that the JVM will see on the method
itself and then do so much your logic
what is that extra logic let's find out
what it is so here's the assembly code
for forget time pretty simple all this
is doing here loading the address of the
current time Milly's call into a
register and invoking it get that time
value and then return the value and
that's all there is to it
so that's fairly simple and we wouldn't
expect to see a whole lot more than this
at the native level but what does get
time synchronized look like now if you
actually do the the output of the
compiler dump the assembly dump you
won't actually see the get time
synchronized get emitted it won't be in
that output it actually decides to
inline that pretty quickly partially
because of that synchronization it wants
to try and do some optimizations a
little more eager about it so let's
actually look at the the main we've got
a loop where we're actually running this
code and if you look down at the bottom
we actually just have our plain old move
the curve of the the function address
into the register the call of that
function and what is all this stuff up
here does this have something to do with
locking is this something that says
synchronization entry up here all this
logic well what this logic is actually
doing is it's just making sure that
we're still calling the same get time
synchronized method you'll see that it's
checking up here we've got is this class
actually the concurrency class still
calling the same static method on the
same class has that class actually
changed is what this other stuff does
and so we got some more vocabulary here
two other optimizations that the JVM
will do ones called lock corseting
that's expanding a fine-grained lock
like a lock in the middle of a loop out
to be a larger lock rather than
relocking and unlocking over and over
again and lock eliding if the JVM can
actually see that a lock does nothing or
changes nothing for the flow of the
program it can simply eliminate it and
that is essentially what's happening
here all of this logic up top is saying
am I still calling that same stupid get
time-synchronized method every single
time well maybe I'm not even gonna do
the lock I'm just gonna go straight into
the code at the bottom so it does all
these checks make sure that we're
calling the same method again and that
says screw it I don't need that
synchronized I'm just gonna go straight
to the actual code that does the current
trend I'm Millie's call but if you look
there's a jump couple jumps up here so
it does this check is it still the
concurrency class hmm maybe it's not the
same class maybe something's change
maybe a class loader has been monkeying
with things maybe some instrumentation
is an inserted additional byte code or
something if we look at where this
actually branches to you'll see some
more code okay
well its concurrency class I think I'm
actually going to do the lock and here
is the synchronized lot synchronized
logic that you would actually see if it
hadn't eliminated it but the jaebeum's
actually decided that this particular
lock is not necessary and pushed it off
to a side branch only if we're changing
this code only if we're calling a
different method will it do this
synchronization for us so what does
volatile look like so volatile basically
just forces memory of our operations to
be ordered and despite particular way so
they can be seen across threads and
across cords it does prevent certain
optimizations a lot of times the JVM
likes to eliminate memory accesses if it
can can't do that with volatile x' it
likes to reorder operations can't do
that with volatile x' and so ultimately
it has a similar impact to doing
unnecessary locking in your code if the
JVM could not eliminate those locks it'd
be similar to having volatile zahl over
the place so let's actually look this is
some assembly code that came out of Jay
Ruby and I was investigating performance
issues on object construction running a
bunch of object constructors and trying
to figure out why I kept seeing this
lock operation you know I don't know a
whole lot about assembly but that
doesn't look good
every time every single time I create
just a plain object I was seeing this
lock and made it into the assembly so
and it says lock and its associated with
a field there so I went back to the code
and I realized I was doing this I was
being outsmarting myself here I had a
volatile field that's just an array of
values for this object of a basically
instance variables for this object
fields and in order to avoid doing a
null check I initialized it to an empty
array every time so every object that
was created when an initialized this
volatile field to an empty array maybe
that's not such a good idea
every time it was actually doing that
lock operation and paying the cost of a
volatile right to memory every single
time so that was a nice thing to fix and
that shows a little bit about how you
can actually learn more from the
assembly code or from the compiler
output then you can just looking at the
code yourself all right string switch so
added in Java seven everyone's really
excited about it it is a very nice
feature I would have also liked to have
seen a class switch so we could have
something more like case class stuff but
it's good it's good to get an
incremental improvement like this but
how does it really work this is kind of
a weird one because obviously switches
are mostly numeric before this so what
is this actually doing so a normal
switch we have some variable parameter
that's coming into this switch and a
bunch of constant values and and prior
to string switch you'd actually get
errors if it wasn't a constant value you
couldn't look up a field for example had
to be a static final literal numeric
value from somewhere and there were two
term versions of it there's two
different versions of it in the JVM
bytecode there's a fast version if you
have a narrow range of case values it'll
just put them into a table from 0 to 10
if you have 10 different cases and then
branch directly off of that so it's a
simple quick operation if you have a
wide range of values like you've got 1
in 5 and 10 million they can't obviously
have a table that holds all of those so
it does kind of a binary search so let's
take a look at what these actually look
like in bytecode kind of preliminary to
our string switch stuff so here is a
very simple switch case with very small
range of values 0 1 &amp;amp; 2
if we look at the bytecode you'll see
that it emits a table switch the table
switch instruction is just a table
mapping the literal case value to a
bytecode offset so for the zero case we
want to branch to offset 32 which is
string 0 etc etc for the one case we
branch to 38 which is string 1 and so on
so it's fast it's quick it just does a
jump immediately so we have a different
case here we don't have contiguous cases
they span over a much wider range 2
million 1 million and 3 million and
you'll you'll note that those are out of
order that's that's done on purpose
because the lookups which actually will
reorder those in its own lookup table
and then do a binary search it'll start
in the middle and look and see whether
it's high or low find the next place to
go and then branch from there so what we
actually have is a comparison between
these two the table switch is always
constant time it can just go into the
table at that offset find the branch to
branch target and go straight there
whereas the lookup switch it's going to
have to bop around in that table a
little bit find the actual offset and
then eventually dispatch to it two
different types of switches so let's get
to the point here so the point which
would which one of these would we use
for a string we can't use a table lookup
because hash codes are going to range
over the entirety of int obviously
that's not gonna work for a table lookup
we can't simply use a regular lookup
switch because you can't have two
strings hash codes that are going to
calculate to the same thing and not be
the same string so we need something
sneakier plus even with a switch even if
we're going to have this u hash code
based we still need to make sure that
we've got the same object it's actually
the literal string so the answer is that
it actually uses both of these here is
our string switch with five languages
five jvm languages here we're gonna pick
how we want to greet them if we go into
the JVM bytecode for this this is the
first part that you'll see at the top
and what you'll see is we've got a
lookup switch so here's our lookup
switch these are actually the hash code
values of those strings embedded
directly into the byte code went
straight down the list but you know
suppose there's others logic up here
like I didn't this I store to is
basically storing an integer in the
second local variable but if you look
back at this code I've only got one
local variable here so what is that what
is that second local variable doing and
the instruction before it is I Const m1
it's loading a minus one into some
hidden value hidden variable in the
system all right we'll have to see what
that looks like
all right so here's the next part within
each of the cases for that lookup switch
it does the equals check so we've got
our hash code that's branched to an
equals check and then the body is now if
we see that same variable so for Scala
it's loading one into that hidden
integer variable starting to play out
here a little bit now if you look at the
final part the final piece of code that
it emits for this switch the string
based switch AHA now we're actually
reading this value and here are the
numeric values that we use for a table
switch to do the fast branch to the
target code and if you look at the
previous one for Scala we actually
loaded one into that field or into that
instance very into that local variable
the one goes to offset 67 and offset one
67 and offset one 67 is the greeting
that we had for Scala so that's what
submitted in the byte code for a string
based switch and if we want to look at
this in not not byte code format it's
essentially this Java code if you were
to write it out literally by hand this
is exactly what you would get the hash
is calculated and stored into a local
variable the target is used to branch on
that second switch this is what it's
doing under the covers I think it's
pretty clever not a bad feature to add
now the rest of this this is all pretty
simple Java code I'm not going to go
the native level with this but the
bottom line of a string switch is that
this is basically just a hash table
it's just calculating a hash into some
little lookup table in the byte code
doing an equality check to make sure
we've actually got the right string and
then branching from there so kind of
funny that it's actually just a hash
table implementation written directly
into the byte code all right the final
area here we're going to talk about is
the lambda expressions in Java 8 I'm
sure you've all seen lots of stuff about
lambda as anyway started playing with it
yet okay there's a few folks it's
actually really fun really nice once you
figure out where all the pieces go
you'll never use an inner class again if
I can avoid it all right so this is new
and Java eight again much rejoicing to
have lambda expressions finally added to
Java and the goals for this we're first
of all to have something that's lighter
weight than dinner classes both as a
language construct and execution-wise
something that's not going to be an
object every time a whole class emitted
for it
something's light and simple and quick
we did not want to actually emit a
separate class file on the file system
for every single lambda body anybody
who's used groovy or closure or Scala
knows if you create functions or
closures you get an extra little class
file on the file system large scripts
large pieces of code can create dozens
and dozens of horribly named class files
on the file system that you have to then
clean up and deal with so we wanted it
to just be one class with lambdas in it
produces a single class file and then
finally we wanted this to be optimizable
by the JVM there's certain aspects of
either reflection which would be a nice
little way of doing a function handle or
or inner classes that sometimes gets in
the way of optimizing we wanted this to
be much better at optimizing straight
through and and performing the way we
want a closure to perform so this is the
preliminary stuff for our clue our
lambda or lambda examples here we've got
a list of the jvm languages again and
then a few operations that we're going
to run through them and we'll walk
through some of these so here's our sort
our sort is actually going to sort these
based on the length of the language name
so collections dot sort takes a comp
but the java 8 lambda logic allows you
to use a function a closure to implement
an interface so the comparable here is
being implemented by this lambda piece
of code we take it takes two arguments
in it does a comparison of those two
strings lengths and then produces a
result for the company the sort
operation what does this look like at
the JVM bytecode level so at the JVM
bytecode level it looks a little funny
we don't see any special lambda code or
anything inserted in here what we see is
we've got our load of that list load the
the first argument the zero argument
passed into this method which is a utila
string we've we see our collection sort
call here at the bottom and I've removed
a bunch of the external noise there but
then in the middle there's this
invokedynamic invokedynamic that creates
obviously a comparator somehow but but
what's actually happening here so let's
go a little bit more detail invoke
dynamic is actually used at the JVM
level to create the little lambda object
and then it's cached forever after that
point this is how we avoid having first
of all a separate class file on the file
system we can do that lazily at runtime
using invokedynamic and it's how we
avoid creating a new object every time
like we would for an anonymous inner
class invokedynamic creates at once
sticks it back into the code and then
executes that one object from then on so
we can essentially have no allocation as
a result all right so we're gonna have
to dig a little bit deeper into this
we're not seeing the code we want to see
so let's turn on the verbose flag so we
can see actually what this invokedynamic
is pointing at so the invokedynamic
points if you look up at the top there's
gonna be a list of bootstrap methods
that are also in the class file
invokedynamic for that lambda that
particular closure is going up here to
something called lambda meta Factory
lambda meta Factory is what lazily at
runtime generates that copper
implementation for us if you look in the
middle there's another method some sort
of hidden method that was created within
our lambdak stuff class so we're
creating a comparable we've got this
method let's actually take a look and
see what that looks like so this is
where the body of the lambda actually
goes it goes into a little hidden hidden
static static private method within the
same class and this has all of our calls
that we'd expect to see we've got our
two length calls we've got our integer
compare call we've got our return so the
lambda body is emitted as just a plain
old static method and then invokedynamic
does all the logic of turning that into
a comparable at runtime rather than you
having to create an anonymous inner
class rather than having it on the file
system and creating an object every time
it's all done by invoke dynamic and as a
result we can actually have closures
that don't do any allocation ever and
compile straight through an optimized
straight through much better than inner
classes can so does that actually work
does it optimize the way we want it to
will it blend with the rest of the JVM
ok so the problem here is that in order
to inline code we need two things we
need a consistent target method so it
needs to be the same type in the same
code body every time and we also need a
unique path through the code if
everybody's calling through the same
code we can't pick which one's the
important one to optimize we're probably
just gonna back off and let them all be
slow paths now the tricky thing with
lambdas in the current JVM is that
they're always going to be used with
shared methods
collection dot sort is used by all sorts
of code in the JVM so there's no one
unique path through it this is what it
ends up looking like to the JVM the JVM
wants a straight path a clear definition
of here's a call here's a call here's a
call all three of these go together but
when we've got many callers calling in
to sort passing in their own lambdas and
all the lambdas are even different it's
very difficult for the JVM to see how to
optimize straight through and get all
the stuff to in line all the way through
that whole process so I actually wanted
an investigate well how this actually
affects
how it affects the optics the
optimization at runtime so I've got
another closure based operation here so
I'm gonna map all of those language
names to their first character and then
join those into a string that's going to
be the result of this so I'll have all
the initials of the languages in a
single string the first version of that
does it using the new streaming API so
stream dot map map all the strings to
their first character collect collectors
joining joins them all the other into
one string again the one below is the
ugly version that does it manually does
not use the streaming API but it does
still use the closure to do this
substring so we'll just loop over all of
the different inputs append there
whatever this substring lamda produces
to the string and then pretty then spit
that result out so two versions one that
has my own handwritten code one that is
using the built in code at the JVM level
on the side I've got a little time
method I thought hey why not I'll just
do another little lambda vase thing so I
can pass in a lambda that represents
whatever I want a time here's the actual
test the benchmark for these two
different versions and I don't know if
you're like me but once I started
playing with lambdas I started using
lambdas all over the place I got lambdas
lambda slanted why not I could have just
called these methods but know I'll make
them in two lambdas it's more fun so
there's like four or five land is all
happening here but yes so this is this
is the simple benchmark that I've got
for the two different versions of this
and now here's the drumroll of what
actually happens so if we look at the
code for the streaming version we will
see in the reams of output that it that
it outputs that methods like map do
actually inline in to get initials so
you'll see map inline you'll see the
collect inline so it can get that far
and it can get the first level in as far
as inlining but unfortunately within
collect where it's actually going to do
the calls to these lambdas there's too
many targets too many people making
calls to collect it can't see an
optimizable path through there
so here we're actually just doing a
plain old and Volk interface and calling
out every single time we can't inline
that stuff and this is something the JVM
is going to have to improve on if we
want lambdas to perform as well as we'd
like lambdas by definition are going to
have lots of different targets passed
into a given function that's that's the
value of it you have a sort method that
can take many different sorters
comparables you have a map method they
can take many different transformations
we're going to need a better way to see
all the way through that path and
optimize those but the good news however
is that the manual version which just
does the the lambda calls directly does
optimize like we would like the JVM is
seeing through that lambda call is
seeing through that lambda body and does
optimize it all together here's the code
that we end up with down at the bottom
you can see get initials manually we've
got our lambda apply we've got our
special hidden method and we've got the
substring logic all in line back to the
code so it is it is optimizing lambdas
in themselves there's just more work
that's needed and there's just more work
that's needed at the JVM level to be
able to see lambda passing paths and
optimize those as well so what have we
learned from this the first lesson is
that the JVM is definitely not perfect
there's many more things that can be
done we do not have the sufficiently
smart compiler yet and especially with
the addition of lambda it opens up a
whole new world of improvements for the
JVM things that need to be done we've
learned that every feature has a cost
there is there is no such thing as an oh
one string switch there is extra
overhead that has to happen you are just
doing hashcode and equals calls like you
would expect like you would write if
you're gonna do it yourself so every
feature has a cost and it's important to
know it and you'll be a better developer
if you remember these things that the
JVM can't optimize all of your code
indefinitely infinitely and fix all of
your mistakes
it can't necessarily provide these
amazing features without some overhead
you'll be a developer a better developer
if you remember that and if you aren't
afraid to go down the rabbit hole once a
while and see what's happening under the
covers thank you
it looks like we got a few minutes for
questions yeah
right right so the question is how when
do we actually need to do this digging
down these different levels how would we
know when we want to start looking at
compiler output or assembly output so
the the basic sequence of events that I
recommend for people that try to
optimize an application is number one
your problem is probably memory and
allocation and objects first of all 99%
of the time performance is killed by all
the objects you're allocating either
because of the garbage collection cost
that goes with it or just because you're
burning through memory like mad and the
memory pipeline can't keep up with it so
that's the first thing to look for and
there's tools for you know seeing how
many objects are being allocated
sometimes you can go to this level and
see if objects are being allocated when
you don't expect them to if it doesn't
look like you are have an allocation
problem and GC doesn't seem to be an
issue then maybe it's worth going to
this level the first step would be
profiling to see where you've got
methods that maybe bottlenecking if
you've got a lot of contention if a
particular method seems to be taking up
all the time and then you can start
digging down and seeing ok what bytecode
is being emitted here maybe there's
something goofy with it what does the
compiler doing is it optimizing it the
way I want and then potentially at the
assembly level and some other flags in
the JVM what's happening as far as the
native code and why is this code running
so slow up front
okay so the the the statement was that
hash code can be overridden so it can't
be optimized at the JVM level that's
that's somewhat true hash code can be
overridden but string is also a final
class actually that's what I thought too
I thought how are they going to do this
because I wanted to do this as part of
Ruby's compiler and optimized string
based case winds is what they're called
in Ruby but you know do I really want to
bind it to a specific hash code
implementation and what it turns out is
that it is actually part of the the Java
language specification and then in turn
that kind of applies to the JVM how hash
codes are calculated for Strings it's
actually been in there for quite a while
and as a result the JVM can actually
optimize based on in emit the actual
hash code because it's set in the spec
how cache codes for strings are going to
be calculated it's a good point though
yeah okay
right right okay so the the you may have
noticed that before all of the reference
types in the bytecode there was an L I
don't know why it's L instead of like R
for reference or something linked maybe
yeah I don't know what it actually means
but L in bytecode in the signatures that
are generated always means that this is
some sort of reference type some sort of
objects type and the second question
about the slides these are all new I
will post them on SlideShare and speaker
deck and on to the JVM or under the the
Java one site as well probably a little
ater today you had your hand up right
right there were not any and I thought
there were there there did not exist any
tools but Martine the Verburg from the
London Java group he mentioned that he
is actually working with some guys on an
open source project to parse the print
intellect print compilation print
inlining output and give you a graphical
view of it what it's been released
what's it called all right he's gonna
look it up but there is a tool out there
and I'm want I want this tool this is an
offline tool so it takes the dumps and
you look at them later what I'd really
like is a possibility of passing that
output into like a pipe or something
reading it and and at runtime seeing
okay this compile and it did all these
inlining this compiled and here's the
assembly that goes along with it
but there's definitely more tools needed
the JVM guys just go and look at this
text output and this dump and the you
know drop into gdb and debug the
assembly code and stuff I'm like yeah
that's not for me I need something
little better than that so they
definitely needs to be tools but Kirk in
the back will tell us what it is in a
minute one more question
yeah
arrey bounds checking yeah I wanted to I
wanted to put a lot of stuff in here but
it's it's really deep stuff so I played
with a raise bounds checking null null
check elimination escape analysis a
couple other things the array bounce
tracking is actually pretty clever if it
if it can tell that it's you know got a
constant stride when it's walking some
array and the array is of a consistent
size all the time
it will actually eliminate that bounds
check JIT watch is the name of the
project and you said this is on github
so github it's Chris who codes slash JIT
watch all lower case on github so yeah
awesome tool play with it
Thank You Kirk so yeah you can't
actually see that that go and it one of
the fun things about playing with these
flags is just writing like five or ten
lines of Java code throwing it through
the optimizer and seeing if it actually
does the optimizations you expect escape
analysis for example was incredibly
depressing because it almost never
escaped analyzes allocations away very
poor well that wouldn't say poor
implementation it's a very weak
implementation of escape analysis but
yeah it definitely does eliminate array
bounds checking actually the JVM guys
like tent-like two three years ago told
me if you count backwards in an array
you can force it to do the array bounds
check a little earlier because it'll see
you're walking backwards and obviously
if the first one works all the other
ones are gonna work so there's little
tricks like that that you shouldn't do
but it is kind of fun to play with the
output and see if you can like coax the
JVM to optimize something in a different
way I think that's all the time we have
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>