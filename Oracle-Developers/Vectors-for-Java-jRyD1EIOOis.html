<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Vectors for Java | Coder Coacher - Coaching Coders</title><meta content="Vectors for Java - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Vectors for Java</b></h2><h5 class="post__date">2016-09-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/jRyD1EIOOis" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to one of the panel's
penultimate sessions at JavaOne vectors
for Java
I'm Paul Sanders I work for Oracle on
the JDK on core libraries and the
language bit of hotspot and helping Paul
out on the vector API
I'd say vice-versa and there's also
Vladimir Ivanoff who actually can't
attend us today who is doing some really
cool hacking in hotspot and is saving us
and getting good performance numbers out
of hotspot so just to make sure if you
think this talk is about Java util
vector you are in the wrong room or
perhaps in the wrong decade or both but
there may be a talk where vector is
actually relevant and that is a
deprecation talk maybe in the future who
would like to see vector deprecated it's
harder than you think it does it does
indeed so here's a safe harbor statement
from Intel I will just gloss over this
and the next page Oracle's are shorter
that's all I will say basically you
can't trust each other
okay so this presentation is about
design of high-level Java API s and
their implementation that leverage
modern hardware for high performance
data processing when I heard that slide
I thought maybe I should join
marketing's that almost says absolutely
nothing so then I said more specifically
which we want to design some
experimental api's and their
implementations that can leverage Sindhi
instructions on modern CPUs everyone
know what's in D style instructions are
yes good so that's the essence of what
we're going to talk about so in the
overview I'm going to start out with
some very simple examples and then we'll
slowly build up and get more and more
complex and say what we're doing in the
experiments and say how they're working
out and where we're going to go forward
so it's going to start really simple and
slow and exponentially rapidly sort of
shoot off as in starts talking at the
end of it we'll have caps of propeller
heads about propellers on us all going
crazy okay so going parallel I've talked
about going parallel many times before
at JavaOne with streams and so forth
like that but you can go parallel
the Machine level we've all seen that
done with hadoop mapreduce and apache
spark clusters massive clusters of
machines going parallel you can go
parallel at cause of hardware threads
like java stream API in the fork/join
api and you can even do the level of CPU
instructions and code proces attached to
them and you can leverage this at each
stage if you want so Java streams you
can even you stream like a pea ice to do
so the parallel level at the machine or
cluster level like we've seen with
coherence and thinking finis span as
well and you can even use stream like
computations to leverage CPU
instructions and we'll show some of
those going forward so on CPU
instructions or Co processes as their
most modern ships now support Cindy
style instructions of varying levels of
features we have our neon in
alphabetical order no preference here
Omni on Intel ATX power and SPARC this
was spark was been around for a while I
think it was in the early stage used for
image processing and stuff like that so
that may have been an early mover but it
really hasn't advanced as far as some of
the others have nowadays and then there
are Co presses like the SPARC data and
analytics accelerator DAC so that was
mentioned by John Fowler in one of the
keynotes and Oracle OpenWorld
and then they have a Java API stream
like API that can call into this
coprocessor this is one of the use cases
we're looking at in terms of
investigating for our API can we
leverage Sindhi instructions but also
can we leverage coprocessors like the
spark tax moving on a bit more general
we have these guys like GPUs and FPGAs
these are a little harder tougher nut to
crack more generally but I think for
constrained data problems we could
probably work out how to better leverage
these types of things in the future -
with a bit more of research I think
project smart tree as well as is an open
JDK project which has tried to leverage
Java on this on the on the GPUs more
generally that is a much harder problem
potentially restricting your problem
it's a more data orientated processing
so here's a really really simple example
Sindhi example 32 bits arrays of floats
say I wanted to add
float a number the elements in float
array a and float array B and put them
into results so normally what you do is
you write a for loop in Java you need
stride through it one by one by one and
adding each one at a time striding
through one lane at a time if you're
going to use a 64 bits like say a long
you could do it
two lanes at a time and do four less
instructions but you can go one level
higher you can do 128 bits at a time and
do two instructions or you can take it
even further
256 and did in one instruction like that
so that that's for general concepts so
there's eight lanes you can do in one go
in one instruction you can go even to
five 112 bits on Intel a BX and I think
that I don't know whether that's a limit
or not but that hits your cache line
sizes which is sort of a nice value I
don't know if we'll get ever bit beyond
that whether it ever be advantageous to
do so so what have we got today in Java
and simile so hotspot supports some of
Intel's AVX instructions with help from
intel core engineers they've been
putting them leveraging them into
hotspot but only some of them are use if
you ever looked at the smorgasbord of
AVX instructions out there there is an
enormous set of them it's a very rich
eisah
if you like and there's lots of things
to choose when we're in using a small
amount at the moment there are super
word optimizations in hotspot c2
compiler and they will derive Sindhi
code from sequential code like a normal
for loop and these can work quite well
in some cases and then we have specific
intrinsic four ray copying filling and
comparison we've been working with Intel
on doing leveraging ABX instructions for
these types of things the latest one we
added was a alexa graphical array
comparison and we intrinsic either array
comparison that's important because
people using a classical Sun risk on
safe for that previously and that didn't
work in all platforms a little more on
that today so what do they do they roll
your own some people roll their own
today with 64-bit operations you can use
some
called the VAR handle array views which
I talked about in previous Jaden molest
sessions and a little bit of travel won
last year or you can use unsafe get put
long how many people have used on say to
get put long to try and their squeeze
out an extra extra mileage from your
your data that is dangerous but people
do and unit got to know what you're
doing and then people use gen I how many
people have used J and I to sort of get
it the special access today going off
heap and making sure everything is
nicely aligned so if you use J and ie a
to overcome the cost of J and I often
you might use larger datasets and ensure
that you're marshalling costs between
the boundary of Java and native is low
to mirror it to me relate the invocation
cost of j'ni itself which can be
expensive so some of the problems here
super word optimizations can be quite
brittle if you write your for loops just
right it's really good but if you
slightly go outside that book the way
that indexing occurs and stuff like that
your performance can drop off the cliff
so you have to know what you're doing
intrinsics the point fixes are not
general and it's often quite expensive
to either specific intrinsic at least a
hotspot rolling your own in java is
limited and might not be portable you
might have a bite odor issue is what the
John Rose calls it you might have an
engine problem or an underlying problem
or both and it might not be portable for
example Apache the classic accept
canonical example is Apache
Lusine Hadoop leveraging LexA graphical
array comparison using unsafe that's
great it works well on x86 because it
works on unaligned access but for spark
boom it can crash or really slow down so
you're not necessary writing portable
code which would like to do for people
using Java it's an understatement to say
that J&amp;amp;I is hard to develop and maintain
that's what Project Panama will help
solve I took a little bit that later so
the motivation is we want better Java
support for crunching on data we want to
really improve that use case and that's
data for its regularly laid out in
memory and especially in a predictable
pattern and so this is for big data
applications like Apache flink pachi
spark they currently use unsafe and J&amp;amp;I
or we even use
an interesting library called let
net-lib Java that's essentially a gene a
wrapper around linear algebra packages
our eigenvalue calculating packages and
so forth like that so I think Apache
spark or flink might use these libraries
therefore that this is this is an
interesting use case for Project Panama
as well and as I was reading up on
machine learning applications that seems
to be the one of the hot things if you
got into micro-services you might be
into these instead and apparently in
some cases 8-bit precision can be good
enough when you're doing some of these
calculations that was a surprise to me
so it might mean that operation sim the
operations and bytes might be coming
back into fashion rather than operating
on floats we'll see where that goes but
that's a that's an interesting case the
GPUs where GPUs and traditionally
crunched on red-green-blue data which is
all a bit stuff so we'll see where that
goes so this is part of a bigger picture
it's part of Project Panama which is all
about moving Java closer to the medal
better safer J&amp;amp;I and it's also about
project Valhalla as we'll see later on
in more detail about bringing value
types to the Java platform so I can I
can have an int represented in list int
so generics over values points
represented as values which contain
coordinates and so forth like that and
the these are really interrelated
because Panama would like to use value
types itself and project Valhalla would
like to use very nice efficient layouts
for arrays of value types so we can get
a nice good crunching on that data so
they actually overlap but they also do
separate bits and this really I think
the bright future for the Java platform
there I've never seen it so good looking
forward and the effort and research done
to move the Java platform forward in
these two in these two efforts so I'm
going to talk about two design
approaches one is a low-level vector API
and it's implementation that encodes
specifically whips into the API and one
is a high level eight API which is
vector based in its implementation and
we will be deriving Java bytecode from
run time
compilation of expressions of a
computation instead so one is loam and
we're upping it so we can do more things
underneath the covers and use compiler
techniques that you're optimized even
further so both for general solutions
we're trying to be more general here
what we don't want to do is add point
fixes to the platform such as darts
explicit types on Mozilla Cyndi basic
types in you see these in the JavaScript
API there were bugs open people logged
into the JDK for adding types like this
and we closed them down some I find this
unsatisfactory to add a bunch of types
here we have an explosion at the types
because someone probably wants I want my
byte x 64 or something like that and
this so it's a week and we think we can
do better than this so what we're going
to do is we're going to layer these
proaches and each is going to be built
on the foundations of some basic value
base types and some what we call code
snippets and these will essentially
we'll define operations on registers and
memory for certain bit sizes just check
time I forgot to set the stopwatch okay
let's see can you tell me a little bit
one time is it okay so here are some
value base types we've added to project
Panama almost specifically Vladimir
added to project Panama we have long to
for 128 bits long for four to five six
long eight for five one seven these are
currently value-based types they're
Senshi boxes around fields of Long's
we wanted to be boxes without identity
so hotspot can treat them especially so
I don't care about identity I can
optimize these better and basically we
want to improve the escape analysis of
these types of classes so if we allocate
them the compiler can remove the
allocation so I don't need the
allocation I just want what's underneath
and I want to map that what's underneath
to register in the CPU and these things
are essentially views overpacked values
as we'll see later on what we want to do
is extract extract values from arrays
and put them back to arrays so code
snippets the mantra I have which is not
quite accurate
gets the point across as that codes it
goes like a Java expression and works
like a hotspot intrinsic you can think
of it like that what we want to do this
is an experiment to lower the barrier or
make it easier for us to experiment
further with a vector style computations
if we had to do this at the hotspot
level in C++ this would be hard work so
we want to speed up our innovation here
so the idea is we wrap a few machine
code instructions with a specified
calling convention in a method handle so
we can easily wrap a few AVX
instructions one two three a little
little packet of machine K so it's a
very platform-specific
very unsafe mechanism to jump into a bit
of machine code so here is a slightly
curated example will link to a source
code later on you will see examples like
this in the source code this is slightly
created so what we have here is a little
factory to make a code snippet and
essentially calling a specific
instruction to add to Y and M registers
in a V X so that's essentially the
floats I showed you earlier for floats
in one go we want to add together and
return result for laneways like so is it
four eight
it's eight laneways yes eight laneways
it's two five six bits so what we define
is the method type so we're using these
value type classes we referred to before
we have a method type here it returns a
long for and accepts as arguments one
long for we want to add and the other
long for we want to add like so so we
don't in fact we say we were quite avx2
because this instruction is avx2 only
and then we define the register see I
put question mark here because we're not
going to hard-code what the specific
registers are here this is something we
do later on because each there might be
a calling convention described in this
little packet and each platform may have
a different calling convention we need
to map to and this how this is how it
might look like in code so we have a
floating floating floating a and C at a
specific index we might want to extract
the longed-for value at a specific in
this extract
eight floats into this long full value
you might want to extract another one
from B and then we want to add them
together by calling invoke exact on the
method handle and this is where the
instruction will be called and then we
want to apply it back to the array
that's the basic essence of it yes very
much so so you'd have to for each
platform you'd have to provide separate
code snippets for it to work when you up
the level of the API it's where you can
monkey around under the covers for each
platform using this same technique so
code snippets are about from effort
handle invoke phrase exceptions we want
to catch the arguments are uncheck but
we we check the types of the long falls
and so forth like that and there a bunch
of examples we have in a class called
patch bullet vector utils and
essentially we need to provide two
factories for different platforms or
work that out so here's a specific
example in code you can see from
patchable vector utils we want to load
from a float array too long for values
and then we want to add them up and then
we want to put them back so that's
essentially what I described earlier in
code and each of these encapsulate some
method handle invocation to code
snippets and this is what the
essentially the assembler looks like if
we print out the regenerated method from
c-2 we essentially see we got a we got a
load from memory here we got another
load we're adding them together and then
we're storing them back so it's very
very tight you get exactly what you want
here coming out into into c2 very
precise so here's another example notice
that in the previous example we were
doing two explicit loads from the array
to get the point across but code
snippets are very precise well this is
creating this is creating two
instructions well perhaps I can load
directly from memory instead of doing
two two loads from memory I can use the
add method and load one use one from a
register and use one from the memory
address instead so we can even slightly
change it have a different method for
add that takes in the the array and the
index four right instead of Allah and
instead of two values and then we only
get one load instruction if you can
see it there and when we add from memory
and one register we can even compress
that down this is we're dealing really
low-level things here and this may or
may not map to different point
difference in structure at different
CPUs okay I think I'm done I'm going to
hand over to you okay all right start my
timer here okay
just a dovetail on the previous remarks
about these assembly this these are
actually dumped from print assembly from
situ these aren't a bridged in anyway so
this is actually what is coming out for
for the ad arrays methods so pretty much
all of the assembly in this talk that we
present has been presented either in a
way that's true to form has only been
let's say abridged again just for a
readability sake but it's true to how it
actually appears from print assembly and
I encourage you to check out the
Panama's source code and try it yourself
sometime so anyway to go in a little
more depth we decided to do matrix
multiplication with s Jim so s Jim is a
pretty common matrix multiplication
algorithm seen in Lib Blas 32-bit
floating-point matrix multiplication we
take a naive approach that everybody
who's been in CS class or as read an
algorithms book is probably seen before
going on to a more sophisticated
approach plus a couple caste efficiency
tweaks doing row major reordering or
should say column major reordering one
of the arrays just typical typical basic
stuff we have a baseline version that's
modestly optimized in ways that you
would write it in Java and then some
vectorized versions that are a bit more
aggressively hand op optimized so you're
scalar matrix multiplication is right
here with some additional hoisting to
save you on some cycles here and here
there's not a whole lot to it with Blas
you have some scaling with alpha and
beta variables that come in you do your
matrix multiplication and you pipe them
out to a resulting seam a
Trix there's it's not any more
complicated than that
so so stepping forward into patchable
BEC utils and using some of our
intrinsics we just tweak the kernel a
little bit with our vector primitives
notice that we're now stepping by eight
because this is the length of our hour
long for in terms of float 32 and then
we do our multiplication as well as
extracting we multiply with our scaling
this is another load load sunk load
trick as well as loading another guy out
and then we have a macro instruction
that we've encoded in code snippets so
code snippets leading up to this point
have been primarily single instruction
intrinsic but you're not limited to
single instructions you can encode macro
style instructions so in this case this
is a dot product and intrinsic that I
sort of will gloss over and hand wave
another thing I've left out of this
specific example is in the prior one we
scale by beta inside our kernel and this
one we scale by beta in advance using
vector primitives it's not exactly I
think you can get into a bit of a
philosophical discussion with what
exactly is apples to apples in this
comparison but you could in theory
probably take beta out of this loop and
pre scale it in a loop if you know that
superward will optimize that loop and
vectorize it but you're getting into
kind of this strange territory when you
start writing optimism optimism for
vector after optimizations to take hold
so just for the sake of this example and
for some basic perf analysis I left that
out and did it this way so to make
things a little more fun walking up out
of the data parallel region into a
thread parallel region you can take
streams and just hoist your kernel right
in there and do it in streams parallel
so matrix multiplication falls into one
of those embarrassingly parallel types
of problems as long as you're not
reading from the matrix that you're
writing to and in this case let's just
assume that for presentation sake all
you have to do is take the outer loop
the I loop and
place it with an in stream which is just
an inductive step and attach parallel
and do a for each in the middle and boom
now you're leveraging all your course so
I would like to say that I could keep I
could definitively give you the
performance estimates of doing it this
way I have again I encourage you to run
it yourself I've seen some pretty wild
numbers come out of this so I'm not in a
comfortable position to really share
share those yet but Panama is very much
a work in progress so we'll we'll just
stick with the numbers that we have for
comparing our baseline with super-weird
versus our vectorization techniques so
basically with performance estimates
these will be checked into Panama some
of these the harness for this I think
has been checked into Panama I have to
push up the actual code examples from
this talk standard micro benchmark fair
use jmh we I ran it on an Intel 6700 K
it's my desktop default c2 configuration
so no disabling loop unrolling no
disabling super word just to try to be
fair so note the read it's highly
preliminary don't hold me to this don't
come find me later if it doesn't pan out
for you that's quite the same way be
nice but so as we're there be more
precise analysis to come as we're still
trying to figure out the nature of using
fork/join in this in this in this area
so I leave the multi-threaded analysis
to the reader so just a relative speed
up of and this will be this example
versus this example based example with
c2 optimist standard c2 optimization
fare on an x86 6700 square matrices
assumed in the x-axis so you get about a
1.5 baseline speed-up which picks picks
up to about nearly 3.5 and around 512
1024 and that's when you've saturated
your level 3 cache on a on the entire
processor so you'll see degradation
falling off after that we note that
there is also a missing test in here
because 128 has some problems just the
size 128 so
very much still a work in progress but
we're fairly fairly confident that these
numbers are fairly stable we've run them
a number of times so what this basically
shows is is that just using vector
primitives will get you speed up over
the standard fare optimizations for a
naive matrix multiplication
implementation so when you graduate to
more even more embarrassingly parallel
problems that spend a lot more time in
the vector space where a compiler
probably would not be able to follow you
with optimizations so closely as I'll
show an example later on you'll see a
lot more of speed-up so basically the
longer you can spend in the vector space
the more you're gonna get out of these
instructions so we mentioned code
snippets and if you notice code snippets
the way they're implemented is you give
the Java compiler literal bytes and you
say hey inline that and don't ask me any
more questions and try to run that so
from from a security angle there's some
questions regarding that API but I think
I think the general the general
understanding at this point is that we
will probably not be exposing maybe that
API to the pub yes yes so you know we
eschew unsafe but then we do code
snippets so Co snippets should probably
be viewed mostly as an implementers and
implementers tool which you can use
yourself right now in Panama but with
the advent of jigsaw and the module
system it will probably be or some
equivalent form will be hidden away deep
inside the bowels of the JVM where
people can't get to it or you know
however that will work so the vector api
is kind of the next layer of abstraction
that lives on top of code snippets that
kind of exists in more or less a
one-to-one correspondence with code
snippets in vacations and instructions
so data parallel operations oversize
types and we're we're getting over code
snippets wrapping them up this
this draft I think Paul mentioned Paul
snuck it into his talk last year this
draft was proposed by John Rose the VM
architect in 2015 there are a number of
different prototype implementations
living in Panama that you can check out
and the base type is just vector so you
see this here it's just vector es where
s is a shape shape is the vector size
which is a bitwise size and this is
primarily for correspondence to register
classes so you'll have a 64-bit register
at 128-bit register this isn't set in
stone we have I think had some
back-and-forth conversations on whether
or not it should be vector length so
element length but in this case we're
sticking to bitwise length so the
element type e is just your int your
float your double so in the case of a BX
and your mileage is gonna vary based on
what I say your you're operating on in
the real world but broad support
generally is seen for ant float and
double again there's some there's some
variation there but we kind of have been
centering around those types so a small
example with the vector API so it's kind
of similar to what you've seen before
but this is an ad erase with the vector
API left right and a result you have a
float vector and this is this is
actually a specialized vector so before
I actually we have vector es in this
example it is float vector which is you
could you could consider it akin to an
int stream or a double stream
we've gone both ways with this the B the
the issue with with vector actually I'll
just address this now so without the
presence of value type specialization e
implies a box type and if you if you're
parameterizing it to array and from
array by an element type e you need to
you'll end up you'll you'll end up
asking for big float array or into big
float array which are kind of
pathological arrays that you know
people wag their finger at you for using
so float vector is a workaround for this
right now
so essentially you just say we declare
our float vectors we unmarshal them from
the array in a similar manner that we've
seen before and then we add them
together and then we pipe them back out
into the array it's just a it's basic
stuff and so add arrays is kind of like
a binary reduction of two arrays with
our workload doing this over an index
nothing nothing too exciting but just a
general just for how it works yes
right right right right so yes so the
question is it should be more generic
than just platform independence yes yes
and the nesting the next example is so
you're yes yes no that's a very good
catch though this is a very specific
example set to 256 256 bit registers but
we can actually parameterize and sort of
loosen up the constraint on the on the
shape so yes this is just really
specific in a very tight example if your
platform didn't support 256 this would
break so even with the vector API there
is this risk of over specifying and then
becoming incompatible when you play with
the shape at all and to some extent even
playing with the element type on certain
platforms may preclude the use of
certain mathematical operations or
arithmetic operations so this has really
been kind of a spelunking attempt into
figuring out what is the broadest core
collection of operations that we can
broadcast across the most platforms
additionally there may even be some work
to investigate whether or not like maybe
even query what you can do on certain
platforms because there are there are
you know graduated operations and
complex so there's your base you know
add subtract etc but then there are some
more bitwise fancier operations that
could be available in certain platforms
but not others how do you if you know
you have it how do you figure out that
it's there without sort of sacrificing
the the genericity of your code all open
questions so the vector API as its
defined there like these basic
operations parameterize by ENS so your
ads your moles and your logical
operations and then we kind of step from
those into intermediate operations so
marshaling an done marshaling getting
elements out putting elements in at this
point we're still constrained by the
boxing but the costs for individual
things isn't so bad with in the presence
of auto boxing necessarily there's a lot
of caveats here
working on it and so putting elements in
and then you can notice there are also
some some all horizontal reductions
those would be kind of platform-specific
in the case of things like floats the
order of operation in your reductions
matter so the implementation has to be
explicit to a particular reduction will
you reduce straight wise left to right
or if you do it a different way how do
you make that way explicit or do we care
that much all up in questions still so
then then then arrived more advanced
operations that are in the spec and
these come with their own issues and you
know if you're raising your eyebrows to
this you probably on track so you have
your maps you have higher-order
functions taking in lambdas that are
parameterised by the scalar element type
of the vector so how does that work well
we'll get there maybe so so these are
these are sort of the fully realized
expressiveness of the vector API where I
can tell you right now it would be very
nice to have them but at this point they
may not live they may not live onward
into the result in spec they'll we they
will be realized in a different fashion
so before I go any further I will tab
out and show you a quick example with
more generic style here so there's a
Mandelbrot example based on an example
from github let me close that see if my
windows powers are good enough - okay so
the Mandelbrot example is based on an
example this guy named IDO on Twitter
wrote he has a number of implementations
in this repository I think we found this
as more of a point of interest this is a
Mandelbrot implementation based on Intel
intrinsics if you're familiar with them
they're also another one a number of
other ones based on other Isis fairly
straightforward I can't comment too
closely on this because basically all I
did here was a
transformation using the vector API so
the equivalent vector API code looks
very similar now like what we were
saying earlier in this case what we're
starting from is we can we can create
sort of a template or a a more loose
definition for vectors with without
regards for their their shape but what
this requires us to do is at some point
pass in an object that bears that
information that we're going to carry
forward through the computation and this
is just one way to do it right now so
essentially we we have broadcasting up
here at least on on x86 we call it
broadcasting so from elements we're
broadcasting elements across the vector
we have using a little lambdas to get
around some SI isms right in here but
essentially it's it's still you know
using the vector API at least versus
Intel intrinsics is a fairly
straightforward one-to-one
transformation that said there are other
Isis out there and so we are wary of
overfitting for this for a particular
iced up however this is still fairly
generic at this point so this this would
be a safe API so you wouldn't you
shouldn't be crashing and if some
platform doesn't support it would have
to support it in Java instead and
emulate a functionality so that it would
keep on running or keep on truckin but
it may not be as optimal on all
platforms right right and I think
another another thing to consider here
that we've we've had some conversations
about is that what what does degradation
look like when you are missing the
necessary platform functionality to
support this operation how do you fall
back do you fall back to if you don't
have 256 do you double down on 128 what
kind of you know sometimes people value
performance at the utmost or they want
consistency they want consistency in
their behavior as well so that can
matter as well so these are all
conversations that are ongoing
still very early work but yes so so
degradation is is sort of unaddressed at
this point because we're really focused
on getting it to work at all but it
we've had some pretty good results with
this so let me well you can see my next
slide let me go back to extended okay
and so the result out of this vector API
implementation is the Mandelbrot I mean
it looks fairly Mandelbrot II to
everybody who's seen one before so the
performance of the vector API right Paul
noted earlier there's boxing involved in
the in the extended value types the
vector API right now suffers from boxing
it's a work in progress it depends the
performance for the vector API depends
heavily right now upon escape analysis
functioning correctly so whenever you
have a vector API object returning
another vector API object its implicit
that there's a there's an immunity
there's an aspect of immutability
happening there which deep in my own
brain I sort of assume that to mean that
new is being uttered somewhere in this
process if if everything's boxed under a
fresh object so getting around that with
escape analysis is kind of an
interesting place because it puts us in
this in this optimizations as semantics
realm where oh well if the optimization
works then what we really mean is this
is all going to be on the stack and it's
going to be great ultimately what we
hope to see is value types solving this
problem for us that is the ultimate
solution we could it's possible we could
see a vector API surface before value
types but a full-blown vector API will
be concurrent with the arrival of value
types so as you can see here vector flow
tests well that's no great not great no
good I speak good float vector extends
vector float you know so trying to play
the the stream game also undesirable we
we end up in a place where streams is
now which I
is suboptimal and unsatisfying and but
finally where we'd really like to get
inspectors just parameterize by the
primitive types so value types of the
recipe like we said project Valhalla and
project Panama are different projects
but there are a lot of implicit
dependencies that are emerging between
the two wait I think it was about a
month ago a smaller sort of rough cuts
implementation for value types for
implementers like us was proposed by
John Rose I don't know if we really I
don't know if you really want to comment
on on how that impacts any kind of
timeline shady value type yes shady
value value types in the shade what we
put them in not in the language but
first in the VM and and the bytecode and
then you can code them through method
handles might be a way to get these in
sooner so that people can experiment
with them and then we build up layer on
top the real generic so the values later
on so this may be of interest to us so I
think Vladimir's work heroic work an
escape analysis in situ is will feed
into this and so anyway I think it's
good experience thank you very much for
Adam here we know you're watching right
now so giddy yeah getting the code
snippet types completely out of the box
and getting vectors out of the box but
yes the timelines open on that just not
Java 9 it's not Java 9 yeah it bums me
out but c'est la vie I wanted to say the
other thing about shady value types was
that it would be value types without I
think the proposal was value types
without any syntactic Java
impacts on on Java syntactically it
would all be JVM deep JVM type
optimizations method handles yes
everything would be under method handles
which is consistent with what we're
doing so far so alright to step into the
final leg of this talk which the
propeller hats come out a little bit
remember those advanced features that I
said don't think too hard about these
advanced features and its current
iteration so map take map for example
take you have a binary reduction you
have you want to map a binary operator
over two different vectors and reduce
them the
down lambdas are scaler defined over the
element type II okay so if you get a
lambda if I'm running at runtime and
somebody hands me a lambda and I'm a
vectorizer but I have this lambda which
is byte code that could contain Lord
knows what and I try to convert that
into a vectorizable form I'm probably
going to fail most of the time also
doing that in general would imply
something like trying to crack open the
byte code of serial as lambda and then
reinterpreted in an abstract form and
we're good we don't want to go there not
to mention lambdas or Turing tarpit s--
anyway so you could have a whole world
in a lambda so we want to constrain this
down and make it work so the expression
language is sort of an offered up
solution to this problem that we're also
experimenting with so like I said at the
bottom we don't have a good way to reify
lambdas C sharp has expression trees
Java doesn't have that that seems like
you know bit too big of a solution just
to solve this one problem so how to
inspect a lambda so how about we just
put an ast like an explicit ast inside
of a lambda okay but we use a regular
land up so we can still compose things
okay okay maybe maybe just roll with me
for a little bit we base it on code
snippets in the method handle API and
when you're coding you think of coding
inside the lane meaning vectors have
lanes of elements so when you're coding
you're focusing on one lane going
through you know flowing through a piece
of computation kind of a streamlined way
or something so this this this follows
from this this series of thoughts say
vector operations are simple expressions
okay maybe so expressions are trees
parameterize by an element type okay
method handles which we've discussed a
bit can be combined in the tree like way
just as a show of hands who all here is
familiar with method handles okay a
little bit all right
what who else okay see method handle who
else familiar with method handles the
library the Combinator library these
guys are okay that's okay it basically
what what I'm what I'm getting at is
there exists a library to take existing
method handles and combine them and then
map their inputs and outputs to each
other it's very functional in a way
and we can use this with the victory API
intrinsics so say you have a function up
here oh goodness I touched it without my
laser pointer function goodness
right here we go okay nevermind I'm not
gonna do it
laser pointer right here okay so see
this right here
say we have a lamb that we want to
implement but we want to do it with
method handles we want to do it in a
vectorize form so we take these two we
specify our method type and then we we
instantiate our code snippets and then
we use collect arguments to merge these
guys together so one guy gets the result
of the other guy and then what you end
up is with the machine where you map you
want two inputs and the inputs map down
kind of like the next slide so you had
so so basically what this point
illustrates is is you can you can by
hand take an expression that you see and
then use the method handles Combinator
library to recompose that expression
with vectorized baselines functions so
instead of doing it by hand every time
what if you start with an ast so you
start with an ast that is just a single
expression kind of a templated form and
so say XY points to some expression tree
and we have an ast visitor that walks
that expression tree and then when it
sees an ADD it uses via add PS when it
sees a mole it uses BMO PS and then it
takes the method handles library and
ties it all back together in a nice tidy
way so the behooves us to establish an
expression language with a kind of a in
an Edsel style domain-specific language
in Java it's a little clunky but yeah so
we have we have this interface here with
our nodes that we're interested in and
we compose those together for reductions
we library eyes our reductions and this
is just sort of a working example so we
library eyes these reductions say you
have a binary reduction as before like
with add arrays but in this case what
we're defining in our workload our loop
though our loop kernel that we want to
customize a loop with we we say
left-right dest with our arrays and then
we have our loop body LR and we have an
expression we do add mul like we did
before but this is all in
data in Java what we get back is a
method handle and then we invoke that
method handle and I'm not lying to you
this is the assembly that c2 spits out
so right now the V what you get is the
the loads the adds and the moles and
then the loop stuff here and then there
you go so this is this is um this is
basically what you get
so the you end up in a really nice place
with regards to code gin so the
applications of the expression tree is
you can vectorize without thinking about
individual vectors on the vector level
or super word for that matter so you're
not thinking about programming for the
optimizations it's a higher order flavor
of programming it works in this case you
can focus on loop kernels so there's
some trade-offs to this though control
flow is hard to get right another thing
is when you're using lambdas as
templates and inside of your lambda body
you were to say you were to put in
conditionals you were to put in anything
that was not within the language of the
Etzel that only gets fired off once
really what you're doing is you're
trying to construct syntax trees so that
kind of starts to break down this
natural this is it doesn't feel natural
for larger examples we can get
conditionals in the in the expression
language but they come at a trade-off if
you have deep branching what you're what
you're usually going to say on a lot of
ices is you're going to mask your
operations and so you have to do double
work and mask for for each branch that
you're masking for you're gonna have to
execute both branches with the correct
masks in place so still more thought to
be to put put on that one and we don't
have vector level operations and there
are interesting vector level operations
so blending and shuffling of vectors and
like I said before these aren't regular
lambdas we do put in some debugging that
that allows you to like so peak in
stream you can put a peak in this
expression tree but that will also come
at a
for major performance it but I mean when
you're debugging it's kind of understood
at least at least when you're
prototyping a vector API anyway so the
last thing being is you when you do this
declarative style you have to develop
the sort of implicit trust for the
compiler some people are okay with that
I think I think folks in the functional
realm are a bit more ok with that but
funks folks who who prefer an imperative
kind of neared near the metal kind of
programming it's it's less comfortable
so you know it's it's it's it depends on
if it's your flavor or not so basically
to sum it up a vector API makes sense in
Java we can come up with different
complementary api's very prototype very
much prototypes but so far so good we
see decent perf characteristics emerging
out of these prototypes with a lot more
work to do but we see some speed ups and
versus the baseline being optimized by
the compiler that's good it's sort of a
nice sanity check so I think we're we're
headed in the right direction so
continuing work enhancing the baseline
API as far as the expression language is
concerned we are interested in somehow
finding a dovetail between the two where
it makes sense to do one or the other
attach two vectors higher higher-order
functionality is great Java is going
there it's nice to live where everybody
else is but we can't let that hold us
back too much so I think I think the
vector API is sort of a priority with
the expression languages kind of writing
closely behind it and then lastly we
want to synergize with all the nice JVM
features that are on the horizon that we
hear so much about and we're so you know
anxious to get our hands on
specifically value types mmm can't say
that we're enough I feel like once we
have value types is a phrase that we say
so much once we have value types we can
all retire on the beach and and be done
so anyway if you're interested it's in
the Panama project
I know it's saying it's in the Panama
project is like finding a needle in the
haystack so the JDK test pan my vector
API patchable it's in there there's
additional codes I think there's some
code samples from here that
not be up there and there in the current
form so I will check those in there are
some maven build scripts in there so you
can you can get started
still very you know very prototype I
can't I can't underscore that you know
some legalese right there and then
another caveat is right now these
prototypes build on UNIX systems a lot
of panama and vector a lot of the vector
api depends on the presence of system v
amd64 a b is to be in place we haven't
gotten around the windows yet hence me
presenting on a Windows machine and
showing you code that ran earlier so
Windows is working progress if somebody
wants to come on and like implement that
would be great know that guys shaking
his head now okay fine so anyway anyway
it's all the work in progress and we
love to have more company I think that
wraps it up okay
there any questions yeah
sanitation
are you I so just so I are you talking
about the Mandelbrot Code are you
talking about the like the the vector
code the victory K okay the selection so
you can think of it so the question I
guess is where where the instruction
selection done essentially is it done in
Java or is it done in situ and the
answer is I both at this point so what's
happening in that example is for better
for worse we're supplying a vector
object singleton that has the functions
that you're going to use and then that
rides through that the templated
computation that that we laid out yeah I
you know I right mm-hmm
so I would say that um I can't comment
on I mean I can't say definitively how
how that would look for the vector API
because the current state of the vector
API is generating code that with the
boxing that makes that especially
difficult to to predict but I think what
you'd have is a sort of factory level
API and you'd split out the
implementations and the source code base
according to the style we have in open
JDK or something like that so when you
build a JDK for PowerPC or build a
communicator in tell it was slotting the
right implementations into into the
released image so that's how so you
would go down to the right code snippets
underneath the covers for the right
platform but the API at top wouldn't
wouldn't change so it's - it's a design
question to be decided I call this an
implementation detail is there's purely
just decoding that there as I'm not
trivializing it it's a problem we need
to solve but I don't think is I don't
think it's a hard problem we need
terribly hard problem it's just yep
there's already a difference already in
terms of say windows there's some subtle
differences in the code bases already in
the open JDK so it's it's doing that
more in the extreme than we've seen
before
essentially the Mac has very different
implementation for some of the rendering
of swing and stuff like that I think so
it's not unusual but some of them yes
yeah to repeat to repeat the observation
the Java neo API is also behaves like
like that so it this is we're looking at
coasting this is a way to make progress
fast on this it's something that's an
experimental trick at the moment or
experimental technique to make progress
in this area and it may be something
that stays there and sediments down as a
toolkit for us to use to progress
forward in this in this manner and for
others like yourself and for people an
arm to try this on their on their own
and see how this works out
yes that's why we had the code snippet
saying require a BX - yes it's a problem
so we need to we need to segment this
out based on based on support so that
it's it's a problem that's C - already
has and we're going to emulate some of
that in in Java to restrict the
functionality on different levels yes
there's some runtime things you're right
yes in there's some runtime aspects
which we can code in as constants or we
can static finalists fields we can set
just once at runtime based on the
characteristics so I don't think there's
going to be a performance issue of
regards to that but it's working out the
set of things that are supported and and
having the separate implementations do
that so it's going to be it's going to
be an implementation design challenge to
get the best best way to structure that
can you peep the questions good yeah a
little letter so have we thought about
GPUs yet and streaming the data back and
forth of the overhead they're involved
in that I can oh no project Sumatra
project Sumatra has in the more general
form and they were trying to do Java
streams and one of the problems they
encountered was it was exactly that
major hit so that's one of the problems
of the GPUs and that's why I also talked
about the Solaris Dax data accelerator
it's a specifically streaming basis it
was originally designed to optimize PL
sequel but it's very simple kinds of
operations you can describe and he
basically pointed at array references or
stuff like that and it shares the same
memory as a CPU itself in terms of axes
and it's a much easier problem to solve
going to something like that than it is
to is to a GPU hard problem yeah I think
I think that's a really important
question though we've we've been
focusing on vectors as you would see
them in traditional CPU Sindhi but
heterogeneous architectures are blowing
up right now it's kind of an
understatement and and coming up with a
programming model that is you know far
enough removed that it can live across
most of those is pretty challenging but
you know it's something we're
considering I think thinking about your
computations at more of a high
expression level and maybe abstracting
control flow or conditionals out a
little farther can get you some some
mileage in that area but when you when
you start thinking about the overhead I
mean with GPUs or even you know FPGAs or
anything like that like you there's all
these little really platform-specific
latency characteristics memory
shuffling characteristics that that you
really have to start to think about and
that yeah yeah smokes coming out of my
ears it's a hard problem that using this
expression language at the moment waste
design this is this is a level we want
to get to so we can start trying to do
those forms of
experiments because you the higher the
level the more you can monkey around
under the covers of all sorts of things
an interesting point what it what if we
could make that expression language more
Java like in terms of syntax going
forward what if we could stuff that in
as an ast inside by as a side to the
bytecode and what if the compiler could
crunch on that extra information so
there's all these high-level thoughts
were having about all this going forward
so it's like an extra compilation step
and I think interesting research project
so you noted that that AWS Google is
your that's your observation yeah
that's an area I mean yes I mean I'm not
disagreeing with you however I'm you
know I I trust you yeah big afraid said
once we've got it working yeah that's
the the catchphrase there but just to
summarize to because there's a bit of a
question-and-answer in the audience so
that the basically most people run on
x86 and most cloud vendors are on an x86
so you're targeting that you're
targeting the biggest audience
essentially and I think a lot of
libraries actually assume that at the
moment in terms of performance apache
hadoop and we've seen mostly compiled
best in x86 we're finding that with
we're doing a lot of work to try and
make SPARC better for these platforms
okay so it's a summer race AMD does
slightly better in some areas but not in
others
okay okay I think we're out of time
thank you so much for attending</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>