<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Java MapReduce Programming on Apache Hadoop: A Whirlwind Tour | Coder Coacher - Coaching Coders</title><meta content="Java MapReduce Programming on Apache Hadoop: A Whirlwind Tour - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Java MapReduce Programming on Apache Hadoop: A Whirlwind Tour</b></h2><h5 class="post__date">2015-06-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/UENXnjbnRAY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everybody and welcome this session
is going to be an introduction to Java
MapReduce programming on apache hadoop
i'm going to assume that you have no
knowledge of a dupe no knowledge of
MapReduce but hopefully some knowledge
of Java if you do have knowledge of some
of these things the first part of the
talk might be pretty boring but
hopefully the second part will have
something for you so I'm going to start
by first talk about MapReduce on what
the problem it is that you know
generally as a framework it's trying to
solve then I'll talk about Hadoop and
what it looks like to write a simple
Hadoop MapReduce job in Java then we'll
go into some of the more advanced
features of Hadoop MapReduce and that'll
be sort of the meat of the talk and
finally as a cherry on top because I
guess cherries go with meat will lightly
cover a couple higher-level frameworks
built on top of Hadoop MapReduce so
first on introductions my name is sandy
I am a software engineer at Cloudera I'm
a committer on the Apache Hadoop project
where I work on MapReduce and yarn and
at Cloudera I work on map reduce and
resource management for Hadoop so what
is MapReduce MapReduce is a distributed
programming paradigm and I guess that's
all you need to know so what is the
distributed programming paradigm like
all things we're talking about it's
something that solves a problem and the
problem that it solves starts with this
often you write programs that perform
some sort of processing or computation
on some data often that data is so large
that if you try doing that computation
with the single machine it will take
forever so luckily we have networking to
the rescue and what we can do with is
modern day and age is get a bunch of
computers to work together on a
difficult problem and this is of course
the distributed part so the second part
of the problem is
at distributed systems are hard they
require defining RPC protocols figuring
out how to serialize data need to handle
partial failures and all sorts of weird
states that never come up in you know
programs that run on single machines you
have to figure out how to deploy them
you have to do resource management at a
large scale which becomes very
complicated so that things don't get in
each other's way and cause the
underlying machines too fresh so if
you're in a place where you have to
worry about this every time that you
want to write a program that deals with
lots of data you are in a very ugly
place so wouldn't it be really really
nice if we didn't have to worry about
all these ugly things whenever we want
to write a data parallel program and the
challenge here is of course that we want
a paradigm general enough to be able to
have a lot of different applications and
support you know a lot of things that we
can do but not so general that it
requires users to redefine the same
stuff all the time and have to think too
much about distributed systems so
luckily sometimes dreams become a
reality the central idea of MapReduce is
that you specify your data parallel
program with two functions on that
function and a reduced function and then
the framework takes care of all the rest
so what are these functions the map
function is very straightforward it's a
function that takes an element of input
data and output zero or more key value
pairs and then after the map function is
called on every element of the input
data there's this big thing that goes on
called the shuffle and what happens in
the shuffle is that all the map outputs
get grouped by key then we have this
reduced function which is applied once
for each unique key of map output and
with this call comes all the values that
were you know associated or outputted by
the map functions into the intermediate
data
okay so now with that out of the way
we're going to talk about word count
we're counting sort of the canonical
simple MapReduce program and if you've
you know looked up MapReduce at all
chances are you seeing word count but
we're going to cover it because it's
very good at Allah strating and how the
paradigm works and we're going to
hopefully cover it in a very detailed
way so basically what word count is
trying to do is you have some corpus of
documents and you have some set of words
that appear in these documents and you
want to know for each of the words that
shows up in those documents how often it
appears within the entire corpus so
here's what our map and reduce functions
do in word count the map function is
very simple on each call to it takes a
single document and then it goes through
that document token eyeses it splits it
up into words and for each of the words
that it sees it outputs that word is the
key and the number one as the value of
course we could do this a little more
efficiently by summing up all the words
in the same document before outputting
them but we're not going to talk about
that now because sort of beside the
point um then we have the reduce
function and the reduced each call to
the reduce function is going to get one
of the words from the corpus and then
it's going to get a long list of ones on
one one for each time it appeared in the
corpus the reduce function is then going
to take all those ones of those counts
for the given word and sum them all
together it's then going to output that
same word with that account and there is
our answer so what does this actually
look like in a distributed sense we
start over here with the input data some
list of documents and they could come
from files or a database but for now we
just think of it as a big bag
documents will talk about how to
actually get them a little bit later so
the first thing that happens is the
input data is split into a bunch of
smaller chunks and you see these chunks
is a task a map task and each task is
sent to a machine on the cluster each
task has some subset of the input data
and the map function is called on each
element in that subset you can see in
the third column the outputs of the map
tasks so you know we have a bunch of
words repeated multiple times and you
know that's what we call the
intermediate data after that we have
this magical thing called the shuffle
with all these arrows going on and the
reason we need all these arrows is
because in theory any map output could
be sent to a different machine they're
all sort of reorganized by their keys
instead of the partitions the input
partitions that they came from so the
Stoffel moves all our data around
grouping map outputs by key and then in
the same way that the original input
data was partitioned into map tasks the
intermediate data is partitioned into
reduced tasks and each of these reduce
tasks is in the same way sent to a
machine on the cluster um note that
another thing is doing is sorting all of
our data by key that might not matter
that much in this case but it's very
useful in a lot of applications and this
is something that we should sort of get
free with the framework so the reduce
function is then called on each of these
each of these words and summed up sent
into you know to the outputs for each
partition and then finally output it to
wherever it's going to go which we'll
talk about a little later as well cool
so that's a basic MapReduce
do all the maps have to finish before so
you can start sending over the map data
before the reducers fart start but
before the actual functions start to get
called all the maps app to the finish
because there could be some last piece
of data from one of the maps that needs
to you know also go into the input for
that reduce function any other questions
before we um no it's all within a single
cluster I mean it's across the network
within that cluster but not between
clusters how does it get partitioned uh
so I mean so you'll have like a big file
and you'll basically find I mean
depending on the format of the file
you'll find points that you can split it
at and then you have a I guess a block
size or a you know size of each
partition that you're willing to accept
and you end up with something that's
almost that partition size does that
make sense does that answer your asking
I mean the framework takes care of it
for you if you have some new input
format I you'll have to handle it
yourself but if it's like a text file it
just handled for you
correct yeah so um yeah so there's the
map partitioning which he was talking
about and then there's also the reduced
partitioning on the reduced partitioning
is pluggable so you can define your own
scheme for how you send each thing to a
task you have you know possible problems
with skew there if you do the wrong kind
of thing like for example if you
partition just on a sort of flat
alphabetical type of thing you probably
have some sort of skew where certain
reducers would get a lot more data than
other reducers so you you know sometimes
want to do more and I think intelligent
i think the default is hashed
partitioning
um okay so we'll talk about that soon
but basically the idea is that we have a
Hadoop cluster and this Hadoop cluster
on has a distributed file system on top
of it and the idea is hopefully that'll
be like the central place to put all our
data in and on you know in a distributed
file system each piece of data is
actually you know located some on some
note on the cluster and we do try to
execute the map tasks on the machines
with the relevant data for them anything
else before we keep going ok cool I like
this please stop me in the future if you
have any questions yeah this is great so
what does it do Hadoop is an open-source
platform for storing processing and
analyzing enormous amounts of data and I
should have added on there that it tries
to do it with commodity hardware on we
say commodity hardware we don't mean
laptops but um we also don't mean
supercomputers and a nice thing about
Hadoop which I like working on it is
that it's entirely written in Java one
second so Hadoop is sometimes likened to
an operating system for a cluster of
computers this might be a sort of loose
metaphor but it's also good for helping
us understand what it's trying to do so
on a traditional operating system for a
single computer you know you tip at a
very high level you have two parts you
have a file system that lets you store
stuff and then you have something that
lets you execute programs and you know
change around that stuff that you stored
on traditional filesystem you know this
is whatever the file system and then you
have processes so with a dupe we also
have storage and scheduling but the
storage is the Hadoop distributed file
system which you'll more often heard
called HDFS and the processing is you
guessed it MapReduce um so as I was
talking about earlier hdfs is a file
system that on you know it presents a
single interface but and then it decides
how to
or data on different nodes in the
cluster it's very good for handling a
very huge files and another note about
the execution side is that until
recently MapReduce was the only way to
run distributed programs on Hadoop but
uh more recently with Hadoop too we have
yarn which is a more general framework
for you know distributed processing
applications like mapreduce to run on
Hadoop MapReduce is still you know the
vast majority of processing on you know
most production clusters at least for
cloud air so writing that produced
programs in Java the interface to
MapReduce in Hadoop is a java api and
we're going to go over what it looks
like to actually write that word count
program that we talked about abstractly
in java code so over here is our map
function and in Hadoop we implement it
by extending the mapper class it's
pretty straightforward it takes two
inputs map inputs are always a key and a
value and in this case the key is just
the offset of the document that we're
looking at in the larger file that it
comes from so this is something we don't
care about it all and we just look at
the value which is that text value so we
tokenized the output we split it up into
different words and then for each of
those words we output it and one so
you'll probably notice that instead of
just using Java strings and integers
we're using these classes that are
specific to Hadoop called text and in
tribal will go deeper into this later
but the reason that these are used is
that they can be serialized compact
relief for transfer over the network the
reduced function is implemented by
extending Hadoop's reducer class and it
gets the data types that the map are
outputted as its input so we can see you
know the mapper output is a string and
an integer and the reduce function gets
a string and
iterator over integers so in this case
our reduce again is pretty simple it
takes that word and then sums over all
of the accounts that are associated with
it finally outputting that word with the
total now the less part of any MapReduce
programming is the code that we want to
use to run it so we have to specify all
sorts of things that you know explain
how to you know which mapper and reducer
we want to use and how to send it across
the cluster I'll you know just go over
them sort of quickly we set the map
class we set the reduced class we set
the data types that we're going to be on
outputting with um we set the input
format an output format um and that
handles some of the stuff that you are
asking about earlier which is the you
know computing the splits and doing the
original partitioning as well as telling
Hadoop you know what to do with these
keys and values once we've ran the
MapReduce program another thing of note
is this a set jar by class function
because we are sending our code across
the cluster it needs to be in a jar that
we can actually deploy so if do makes
this very easy by you just tell it the
name of a class and it will find the jar
that that class is coming from and send
it out the last thing to note here is
this business at the top with extending
configured and implementing tool instead
of just doing a regular main function
this is something that I often forget
and it always comes to scream you later
this makes it so that Hadoop can work
some magic when we run it from the
command line it will make it so it can
accept MapReduce properties on the
command line and do some other fancy
stuff okay so input formats
as we talked about above Hadoop accepts
a pluggable input format that tells us
how we interpret our input data and how
we split it up into different map
partitions so the simplest and the one
that we used is text input format and
what that does is it basically takes a
text file splits it up by lines and
gives each line as input to a map task
and we also saw that it gave the byte
offset of that line which is not useful
in most cases but maybe you know in a
few another simple input format is key
value text input format that split line
splits lines on a delimiter you know for
a comma or a tab for example and gives
whatever comes before the delimiter as
the key and whatever comes after it as
the value um thirdly we have sequence
files so a sequence file is a binary
format that's specific to hit dupe you
might use it to a store a small but you
know a bunch of small binary files you
know it's useful for images and it's
also good because it's aware of you know
the boundaries and when you do a
compression with it it can handle that
efficiently by not compressing across
these boundaries um another useful one
is DB input format which basically uses
JDBC to connect to a database and you
know sucks all the input data out of
there lastly of course you can write
your own and there are a bunch of you
know higher-level frameworks on top of a
dupe with their own data formats that
are definitely doing this um we'll go
you know over the output formats in as
much you know excruciating detail but
basically they're the same thing text
output format outputs keys and values on
lines with a delimiter stuck in between
we can output to a sequence file on and
to a database okay so that's how we
start get stuff in and out of a
MapReduce
graham on another thing we're talking
about is serialization so serialization
concerns how we turn data into bytes so
that we can send it over the network
which is something that we you know
often need to do inside MapReduce the
most basic way to serialize data in
MapReduce jobs is by having data types
or classes that implement this interface
called writable and when you implement
it basically you are you know define a
way for converting your you know
higher-level object in two bites and a
way for reading it from bites um so you
implement the serialization by yourself
another option you know which is a you
know often helpful when you're doing
this a lot is Avro Avro is a separate
Apache project that's built to have an
extensible serialization format with
cross language support it's similar to
thrift or Google's protocol buffers if
you know about those and with Evra you
specify the data types declaratively you
know you can nest data types inside
other data types and it handles the
serialization for you it's also
extensible so if you want to you know
add data types later you don't it won't
make what you previously have
incompatible yeah go for it
exactly or the other way around yeah any
other questions here um I mean so you
know the problem with JSON is that it's
text and you're dealing with tons and
tons of data so you know JSON is much
less efficient but you know if you want
to actually you know write your data as
JSON with a writable you could do that
oh yeah thank you um I will do that ok
so that's serialization another
important notion for the Hadoop
MapReduce programmer is counters so
counters are metrics that are collected
for each map or reduced task and they're
also aggregated and resented for the
entire job so you know whenever one of
your MapReduce jobs completes it'll
collect all of these while it's running
and print them out once you're done you
can also you know look on the web UI for
counters for additional you know
particular tasks some example useful
ones it will tell you the number of
total map input records map output
records it'll tell you the amount of
time spent in garbage collection and so
what's really useful about counters is
that you can write your own so for
example suppose you're dealing with an
ugly data set where you know you have
lots of badly formed records and this is
you know definitely something that
happens a lot when you're dealing with
really huge data sets suppose you want
to know how many of your records are
badly formed so you could you know write
your own distributed thing to you know
include this as site data along with
your job and do something messy but
luckily Hadoop makes it very easy for
you and you can simply within your map
or reduced ask when you encounter
something just increment a counter and
then your job completes you know the
value of that counter will be aggregated
and set
you very straightforward um okay so
another important mapreduce future is
the distributed cache the distributed
cache is based on the idea that
sometimes we have files that we need to
sent to every single one of our tasks
multiple tasks are run on each node in
the cluster at the same time and so
using the distributed cache avoids
sending the same file over the network
to the same node multiple times this is
useful for a lot of different things for
example if we're doing some sort of NLP
task and we want to include a dictionary
you know we can send that along to every
one of our of our tasks um another you
know pretty common use is if we're
joining a large table against a much
smaller table that can fit in memory we
can send that smaller table along with
to every task so the question is is this
data that sent across to all the nodes
automatically cleared up after the job
and I think you cannot configure that
it's possible to have it persisted like
between jobs so cashed in that way but
you can also have it a per job any other
questions
so the question is is it a replicated
cash or is it on demand caching um so I
guess I'm not entirely sure what that
means but I'll tell you how it works and
then maybe that'll answer your question
so the way that it works is the data is
written into the distributed file system
with a very high replication factor um
and what that means is that you know the
same data will get sent to a lot of
different places on the file system then
when your map will reduce task wants to
hear we have a nifty diagram that shows
that then when your map or reduced task
wants to access that data then it's its
pulled down from HDFS but it's very
possible that it'll already be on your
local file system because of that
earlier replication does that make sense
cool
uh so the question is is it is it more
of a local file system cache than a
memory kit and that's right the files
are put on your local file system and
it's up to you to keep them a memory if
you want and so how you actually make
use of this in a MapReduce program is a
in your driver program you know you do
this distributed cache add cache file
and you can give it a you are you are I
anywhere it'll put that on hdfs then in
your memory or reducer you ask it for
these local cache files and it'll
basically just give you a pointer to
wear those are on your local disk so
that's pretty straightforward I forgot
to mention another use for it is a
distributing you know additional jars or
libraries that your program wants to use
it doesn't just have to be like text
data cool any other questions on those
tributed cash so the question is when
you say local file isn't that in HDFS as
well so there's the distinction between
files and HDFS which are somewhere on
the cluster and files which are actually
accessed via the local file system and
so I believe even if it happens to be on
the same disk is yours when you actually
I want to access it to distributed cache
it is copied over to a separate place on
your file system any other questions ok
cool
alright so you've written your MapReduce
program and it all looks very pretty but
like any other program you want to make
sure that actually works so how do you
test your MapReduce program like any
other program the first thing you should
do is write unit tests and there's a
separate Apache project called mr unit
which is shown up and that makes this
part of the process very easy you will
have to map modify your MapReduce
program at all and we're going to go
into how that's used a couple slides
after this after your unit tests all
pass you can run your job inside what's
called the local job runner and what
this does is it runs your entire
MapReduce job in a single process you
don't have to you know modify or
anything to do this and this makes it
really easy to profile it and step
through it with a debugger thirdly if
you want to be extra thorough you can
run your job on a single node cluster
this is useful for things like making
sure that you're using the distributed
cache directly or correctly sorry that
all your you know files that are set up
on HDFS are done correctly and Cloudera
provides virtual machines that make it
really easy to set this kind of thing up
um finally you want to test your program
on the real cluster so mr unit um the
basic you know thing that mr unit is
trying to accomplish is you take your
map and reduce functions and test them
individually without any modifications
required so it allows you basically to
construct a map drive map driver object
you give you a wrapper class then you
you know decide some example input tell
it the expected output and run it
through and it will tell you whether it
worked and where it did not so this is
for a mapper this is a similar example
for a reducer it also allows you to test
your job as a whole and you know that's
good for testing how things are shuffled
around and grouped in between phases
okay so that's uh the Hadoop talk pics
now we're gonna talk a little bit about
some technologies that are built on top
of MapReduce in particular mahout crunch
and cascading so but who is a set of
libraries for machine learning on top of
Hadoop on it contains a ton of different
things they're not super unified um but
it's sort of this bag with you know all
these like interesting and very helpful
algorithms here are you know some of the
examples it has a collaborative
filtering and recommendation engines it
has clustering it has some stuff that's
a good for working with text data like
latent Dersch layla keishon all sorts of
classifiers regression blah blah blah
second is Apache crunch so a common
problem with MapReduce is that you end
up writing these map and reduce
functions and if you have a sort of a
more complicated pipeline do you want to
write becomes very difficult to to
manage it all MapReduce is you know a
very low-level API so Apache crunch is a
library built on top of MapReduce that's
based off of a project at Google called
flume Java and it makes it very easy to
write pipelines of jobs in Java um as
well it contains a bunch of capabilities
like joins and aggregation functions
that make it so that programmers that
don't have to write these for each and
every job so here's an example of our
entire word count program written crunch
you define a what's called a pipeline
and you specify a bunch of operations
that will be executed in sequence or in
parallel in this pipeline you define
some input data for example you know a
collection of strings from the text
files
um and then you just run it so instead
of you know three files and three
classes it's one simple file last of all
is cascading which is a more general
higher-level application framework built
on top of MapReduce um it has some of
you know both of what crunch and mahout
have it allows you to write MapReduce
jobs in other languages that run on the
JVM that are not java like JRuby and
closure for example and it contains just
like a ton of functions that make it
really helpful I make it really easy to
do you know all sorts of common data
processing and machine learning tasks
that you know compiled down to MapReduce
so that is all I have and can I add sir
any more of your questions
you say so you're saying where can you
get the slides I think Oracle makes all
the slides available online so yeah
these are going to be up you know up
there okay I'll hang around for a little
longer if anyone has anything and thank
you all for coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>