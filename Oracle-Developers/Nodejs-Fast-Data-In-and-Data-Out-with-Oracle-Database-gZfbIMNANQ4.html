<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Node.js: Fast Data In and Data Out with Oracle Database | Coder Coacher - Coaching Coders</title><meta content="Node.js: Fast Data In and Data Out with Oracle Database - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Node.js: Fast Data In and Data Out with Oracle Database</b></h2><h5 class="post__date">2018-04-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/gZfbIMNANQ4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so my computer clock is just ticked over
to the 7 o'clock in the morning so I
technically well not technically
physically was born in England but did a
little bit of elementary school in
Chicago and then moved to Australia so I
have a little bit of a mixed accent
somewhere along the way I worked out of
Oracle Silicon Valley headquarters for
nearly 10 years so my old joke about
having the American accent because I
watched too much TV doesn't really hold
anymore because I had 10 years to get
the real American accent let's make a
start so you will have seen the safe
harbor statement particularly open
source software there's sort of no
guarantees scheduling is very much
subject to change as used a man comes in
or some interesting problem comes up and
we spend a lot of time on that don't buy
anything based on anything that I say so
I am a product manager I have a
development background I worked on PHP
drivers for some years out of our Oracle
the next group the same Linux group
which I think today just released dtrace
as open source on github DTrace being a
great dynamic tracing tool I now am
working in the Oracle database group and
I have a sort of broader portfolio
looking after a whole bunch of scripting
language driver API is the access
programming interfaces or drivers we
call them interfaces that's the current
terminology that you're familiar with is
it drivers do you like drivers api's or
everybody I think knows there's
different things but anyway that you
know a little scripting languages like
Python PHP basically all of the C C
based languages in fact this is what the
wider group looks after you can see in
the red and the blue that there's a
number of very key technologies they're
obviously Java and.net I tend to focus
on the sea bass things and the open
source particularly which are the blue
blue languages and blue drivers they're
all solely a is where the open-source
maintainer z' that party maintained as
we look after the other interesting
languages Ruby Ruby associate the rails
maintainer and the Ruby maintained I
recently got granted Oracle Asus status
which is really pleasing cuz I put so
much effort into those
into that ecosystem and then we have a
whole bunch of other languages their go
is upcoming who's using go anybody using
go heard of go rust node okay so
basically we do a lot of stuff but if
you have questions on any of this come
and ask me and I can kind of hook you up
with what's going on so who's using
nodejs who's actually using it now who
wants to use it and who's trying to
avoid using it and just wants to know
the arguments that they can defeat it
with okay that's cool there's anybody
actually using it with Oracle database
kind of using it fantastic you don't
work for Oracle right you're not okay
fantastic
it's always nice to see users out there
I'm sorry I do have a few interest
slides obviously the few people need to
perhaps be brought up to speed it is a
very efficient JavaScript based I guess
we would call it a mid tier system
server server gets overused you mean
database server application server
whatever so identical at the mid tier if
you're not familiar with it you don't
really need a web listener it just
listens to network events and things
like that it kind of runs on this
threaded model event loop model so it
has a cue callback as you can see down
at the bottom it's gonna be putting work
into that cue Med threads gonna pick up
that next unit of work and schedule it
on one of those worker threads there in
that bottom middle middle block and that
worker threads going to do this to work
put the results back into the queue and
something else is going to pick it up so
it's kind of really trying to be fast
and threaded and get work done pretty
efficiently it's got a kind of callback
model which I'll go into a little bit
later has pros and cons but they're at
they're ways around it if you don't like
the cons so node Oracle DB is a database
what did we decide driver API drivers
driver okay database driver it sits on
top of the Oracle client stack and that
has pros and cons the pros being that
that has a lot of highly advanced
efficient supported technology including
things such as network encryption so
there's no sort of command in node
Oracle db2 in turn on encryption that's
turned on at a lower level with a
configuration file so can support that
it does
high availability event management
buffering all sorts of caching and
things like that yes you have to install
it separately but then you get all that
functionality for free and we can go and
do some work and add value on top of
that no js' obviously you start with
there's a package install command to
install the node Oracle TV package there
are binaries available and I don't have
users on that slide but their user
sitting off on the side somewhere and
your scripts are being read and
obligatory who was in the Python talk
earlier ok so you would have seen some
of these jokes before but the obligatory
cloud slide there you can connect to any
database where this in the cloud or not
but you can also run note e in the cloud
if you want to as well so quick overview
first release to github it seems just
like yesterday I guess it was a little
bit longer open source we maintain it's
a open source has pros and cons it means
you're accessing the developers directly
if you have issues you stick those on
github with an issues but they call it
an issue questions whatever developers
are looking at it you don't have to go
through the support Channel and log
tickets and things like that
the cons obviously you know no
guaranteed response times and things
like that but in general I think you'll
find it's probably better as good as as
the other way and we do appreciate all
the contributions that have been made
into the driver you can contribute if
you like c-level there's also a
JavaScript level on top which I haven't
really talked about but if you want to
extend the JavaScript level some
functionality there you can contribute
back into the community and obviously
then we would take over maintenance and
you wouldn't have to worry about that I
don't want to read through this whole
slide it's the kind of high level
overview that we try to keep the the
class structure relatively simple in
fact at the very bottom one lobs we
don't even really use much anymore
unless you're doing streaming for meteor
or something like that effectively you
have a connection and you can either
have a standard connection or a pooled
connection and then you can either have
results or if you want a bigger result
set you know hundreds of rows thousands
of rows whatever you can fetch those in
batches using this result set
methodology this streaming is also
available
we have pre-built binary so people who
worked in the v1 days note Oracle dbv
one had to compile it themselves
we now have binaries whether available
so npm install' really just works for
pretty much everybody Linux 64-bit
windows 64-bit and Mac OS everything you
installed no matter what language you
know everybody talks about open source
product X being really easy to install
everything has like four or five tricks
you have to know you have to open a port
here do you have to change the
configuration file there yeah we have
one or two quirks about you know setting
the environment like a path will LD
library path to to make things run
that's our quirk yeah get over with it
works I'm not going to read that we
don't have time to talk about all these
features but you can see that it has
quite a array of features so what are
some things that I might not get down to
n2 in tracing peps today I'm going to
pick on end-to-end tracings a way that
you can set an attribute there's a
static route flag in in the code and the
fact of various cuz there are three or
four different kinds of attributes what
kind of hierarchy of attributes you can
set very useful to set in the code it's
arbitrary string you just set a string
saying that this is my module for
getting the sales data so you just might
put the word sales data in that
attribute that attribute text would be
sent across to the database and it would
appear then in trace files log files you
can query it so you can correlate a
query or administrators can go into
Enterprise Manager and say oh sales data
was taking this much percent of time
because I can group by that particular
attribute so yeah use some of those
features high-end features okay so what
do we have people haven't seen it before
sample query some of you haven't used
node you need to do require this module
standard every pretty much all of that
languages you tend to have to have to
load modules in at some stage rather
look my you've got the results there
already as my animation hasn't quite
worked um you get a connection you have
to connect to that I donated by somehow
so this is the non pooled connection
username password there is the
capability for external authentication
so you don't have to have passwords
hard-coded or passed in by environment
variable so you can use Oracle
external authentication LDAP servers and
things like that here the connect string
I'm just saying this is this is the
modern Oracle connection string under
saying the hug that's computer name
where the database is running and the
service name that that database is
configured to listen on you can use TNS
connect strings you can use TNS
connection files if you want to this is
obviously going to be easier then you do
execute so you can kind of see people
haven't seen that this kind of gets
confusing so focus just on the read you
get a kind of callback and so that when
that connection is has worked then you
get a callback with that connection
object there on line five and then you
can use that connection and we do the
execute there on line seven going to
select the multi-line back ticks
multi-line strings with backticks
otherwise you have to do single strings
and string concatenation gets a little
ugly and bind variables I'll talk a
little bit about bond variables a couple
of output formats it's very fast to
construct an array of output as you can
see there on the right you know as
standard Oracle isms but you can also
say I want an object if you want to JSON
object back JavaScript object back and
you know we don't do that by default
because you have to construct that
object you have to set the attribute
names and things like that but it's
obviously there if you want to use it
anyway so that's basic behavior oh and
then there's a callback from the execute
itself and the callback actually just
dumps all of the role it burns out as
you've been seeing
ever since the stop
the default number of rows fetch is
unlimited v1 we did have a limit which
was a little bit of painful but in
version 2 which has been out for a while
number of rows comes back as unlimited
and there's some tuning parameters for
that and then obviously error messages
at the end any questions and I put this
in we also have promised support for
people using promised support and this
is in your async await which came out
with note 7.6 so technically we sort of
say eight which is the first sort of
stable release long term release of that
stream and so this has as you can see
kind of a weight interface down there at
level line five you just basically wait
for the connection before you continue
so there's no kind of callback you don't
get this nested hierarchy just like wait
for that connection and I don't do
anything until that's completed resolve
is when you do get the results back
there on line 11
no surprises for people who've using
this are using async await what versions
have no to you using no date yeah this
is this stuff starting to look good you
should probably consider using this okay
so this is really the agenda for today
that was the introduction all over 12
media interaction sorry about that
starts and ends with the connections I'm
going to speak about obviously data out
data in there was the topic and sorry I
just go to that's fine connection we
talked about the kind of key thing there
is a little picture on the bottom right
so when you create this connection you
are establishing the pipe across to the
database and you're starting a process
on the database which is that blue dot
there on the database side and that
process takes some time to start up it's
going to have memory allocator to et
cetera et cetera that's okay if you
that's what you want but then it does
take time if you're going to be doing a
number of times so we do recommend using
a pool in most cases so you create a
pool and then from the pool you can get
a connection and you can
not a bunch of parameters you can see
there on the screen about sizing and
what-have-you timeouts when Paul is idle
so that the pool can shrink back down
you're not using resources and you're
holding those connections open all all
the time that though the pool if the
pools was a size to minimum size to you
would always just see those two server
processes and blue on the right-hand
side so one other thing I didn't mention
here of course is you want to release
those connections because we don't
release them using a release that I
think I showed on the previous slide I
didn't show if you don't release it then
nobody else can pull it out of the pool
again so make sure your aren't releasing
connections but you can also have
something called database resident
connection pooling who don't talk about
that in this talk no time so you can
have a pool on the database side which
is great because if you have multiple
node processes each of those as you
might see in this particular example
would have you know two processes to the
database that would be two by n so
they'd have to be multiple processes on
the database side the RCP lets you even
pool on the database side so you can
share processes that side so just
technology you can use if you're running
into constraints in terms of memory on
the database side so what's the scenario
and this is by the way tree of
stand-alone connections you want allow
ten concurrent users your pool max ten
what's going to happen so you've got
four worker threads that's the default
in in node you can configure it up
limits 128 so in use connections are
going to wait for database responses
it's just the way it is if you've
started a you know sequel statement
across the database it's gonna have to
wait for a response to come back yeah
obviously selects kind of wait but even
in inserts gonna have to wait for
response to come back from the database
so that worker threads going to be held
not able to be used by anybody else and
you've only got four of those so you
gonna run into a kind of limit there so
slow response and you kind of expect
yeah things are kind of working but you
could get deadlocks if one of those
threads is trying to you know update
something which something else is
already locked so a solution just to
increase that thread pool size you need
to do this before node starts a thread
pool so you can you can increase it in
application code but if you don't
increase it before the thread pulls even
initialized you will still only get four
threads so you think you've set it to
ten but actually it says you set it to
late and so they got four and there's
actually no easy way to find out what
has been set to so here I'm using you
know Unix Linux command export you can
set it or you can actually run a env and
set it at the environment level inside
no but just be careful you do it like
really at the start you may want more
threats than you actually do it then you
have connections because you may be
doing non database work so maybe one
even increase that thread pull a little
bit bigger and remember you can only go
up 228 so scenario two so you've got
four threads you know it doesn't really
matter but that's the scenario here you
open one connection so this is not not
pooled I'm just trying to show you
another kind of thing you should be
aware of the open just one connection
and you try a new this promised at all
what does promised at all do it tries to
just fire everything one select three
inserts but you've only got one
connection so the Select is going to get
a thread from the worker pool queries
going to block all those inserts you're
gonna get you're gonna get threads gonna
get all four threads but only one of
them can actually do anything there's
you only got one connection so
connections can only do one thing at a
time so the kind of moral of the story
is keep controlled on JavaScript layer
watch out for things like promise all
you know use async series of using as
using either series or async series
instead of async parallel if you're
using the async library so that's the
old-school icing not the not the async
await programming style but the async
NPM package so you know try and keep
control and make sure you're not
starving yourself with threads and or
connections scenario three this kind of
came up and we've actually got better
solutions for this now but a couple of
weeks ago maybe a month ago somebody was
trying to load a lot of data into the
database so they wanted open a lot of
connections and just fire off a lot of
stuff all at once so kind of did promise
all and got a lot of connections and
then tries to insert a lot of data
can your database even handle all those
connections open and if you've got
multiple users trying to do the same
thing on different note tears or
whatever can can you know that
multiplies that the load you've got
multiple connections so you can't really
do transactional consistency so you kind
of got this no problem is it where we
got the solution now which I will go
into a lot detail later later slides
okay so here's a protip so i've got some
pro tips in here yes so this comes from
our real-world performance tuning team
so these are the people who really know
what's going on they say don't have
dynamic pool sizes and there's a whole
lot of reasons if you look in the note
oracle DB documentation there's a link
to their documentation to the WP doc you
can read all about it there the real
thing is of course when you suddenly
need all these connections as people log
in is when the database is pretty much
under most load and you start get
anything is really unstable you really
want to size your systems for the
maximum load that they have these are
tools you can use them don't have to use
it but this is their tip to keep most
stable systems make sure your pool sizes
are fixed no minimum no maximum and they
also have some recommendations about how
big these pools can be depending on the
server side load on the database side
they've got some calculations which may
or may not be accurate they know where
as of when that doc was published a year
or so ago about numbers of CPUs and
sockets and things like that and load
that the database site can actually
handle so that's their big Pro tip we've
also got a lot of settings Oracle net
the sequel net layer as people knows
used to be called sequel net a whole
bunch of parameters I've got one or two
mentioned later but you know there a
couple of other things there that you
could actually add as well to stop the
load slamming the database at a
particular time if you've got a lot of
people trying to log into the database
okay
ending connections when I didn't have
that before so you definitely required
for pull connections you've got to
release it back to the pool so somebody
else can pull it out and it's also best
practice for standalone you were just
one of released resources as early as
possible so that somebody else can use
those resources
anybody used version 2.0 and 2.1 of node
oracle DB have used okay we had we were
trying to we've been working our way
towards managing all this stuff a lot
better so in 2.2 we laid you kind of
release things when you want even if
they're still in use behind the scenes
for whatever operations you know and
know tends to do things when you at
least expecting it but if you're trying
to stream from a lab for example you
still gonna need that pipe across to the
database but you can physically now
actually in your code close a connection
you'll think it's closed no it's
actually using it and at appropriate
point it will close it off behind the
scenes for you so we've done a lot of
work there in the current 2.2 release
and obviously try not to release them
too early in some cases you might see
some some kinds of errors you know it's
it's a little bit variable so just watch
yourself streaming for lab instance oh
yeah I think we've just fixed that
recently okay um
another pro tip basic high availability
so everybody wants free high
availability now Oracle has a rack and
it has all these great high-end
solutions which really have very
beneficial practical applications but
everybody wants to kind of do it in the
code well developers DevOps you want to
do it all yourself so we do have some of
that available to you and hopefully
we'll expose more of this through the
actual node API with node attributes and
things like that but for the moment you
can look at things like the sequel net
parameter configuration file and there's
there's steps on how to set this up in
the oracle node our quality be
documentation things like these connect
timeouts and receive timeouts you know
if I'm waiting for some response from
the database how long can I wait before
I'm gonna say give up return an arrow to
use or retry on a different connection
different connection to a different
database instance you know with rack and
things like that
firewalls we know this is a problem
sometimes we've what we call out-of-band
breaks people trying to send signals and
things across firewalls article by
default in the current versions has a
single atom and breaks on sometimes
firewalls block that so you might need
to turn that off you want to make sure
that your connection not break
works also recommend updating to the
12.2 client libraries so Oracle has
client-server cross version
interoperability so you can have 11.2
client libraries and they can be talking
to you know 9.2 database 12.2 database
18-0 database I think there's a there's
a supported matrix and there's a it
works matrix which has a wider range so
certainly if you're using you even 11:2
database there's no reason why you can't
upgrade your client libraries to 12.2
you get some better features there in
terms of the session pool the connection
pool that I spoke about earlier whether
we detect whether the network has
dropped out if that sessions been just
sitting there idle so trust me use it ok
getting data out of the database
where we're going back little tiny keys
okay so we know direct fetches a few
methods I've got I think 4 points or
something like that you basically get
all those rows returned as I mentioned
you can get a result sets I spoke about
that result set object in the class
diagram you just set the result set
attribute true and then you get a
callback object to results that object
and then you can call methods on that
dead row or get rows if you want to get
more than one row at a time streaming
anybody using streaming streaming it's
kind of kind of useful the Vantage of
things like get row in and the query
stream is you get it's a one data row
per event in case of query stream get a
date or event so just get one one record
back so you can be listening at Freddy
listen this isn't in yet listening for
it anywhere in your code really having
to really worry about the type coupling
between methods in your code get rows
similar you just call get row when you
want we do a lot of buffering internally
and I'll cover some of that a little bit
later so sorry did you want me to go
back for your photograph ok
have a blog post on this up by the way
as well okay trick factors easy to use
we've got this fetch array size
parameter so I spoke about that a little
bit that's an internal parameter doesn't
affect what you see in the application
at all it's just a buffering size
internally I I know that this has
various names in various places some
people call it like prefetching some
people call it a writing house
implemented you don't need to know about
we're trying to cover and hide all that
stuff from you it's just a tuning thing
but you do need to go and tune it for
your system in your queries you know we
can't really tell you what's best for
your row width and number of rows you
want to return and also network
distances and things like that it has
one drawback you need one big array to
hold all your results and it may not
know how be that arrays up front you've
got to do this concatenation of batches
of Records it was internally it happens
so we do that for you but we've got to
allocate memory we're going to re move
things around relocate things can get a
little bit of fragmentation you know
it's certainly something which I would
recommend using but using it with a
provider that you know how many rows are
going to get back and don't be caught
out by these outlier cases there's also
fetches I showed you the direct fetches
way way back so this is a result set
fetch array size true and that's again
this buffering for the get row because
get rows I think in return run one row
at a time to the application but
internally I need to do some kind of
buffering so we let you do that for the
plural get rows you pass in the argument
of how many rows you want and that's
that's a tuning parameter you know you
don't need the fetch array size there
one little thing here is always close
the result set you know we need to know
when you're finished with it when to
cliff free up resources and things like
that and if you want to look at the
final code you obviously have to call
back into that nested routine let's look
a little messy but the key thing is a
look at the read text and it'll all make
sense
fetching lobs so Arkell likes to stream
lobs i case it was kind of designed in
the multimedia days and everybody
thought everybody was going to be
serving video and things like that turns
out of course nowadays people's text is
kind of grown towards a lob space people
want megabyte of text three megabytes of
text something like that not quite long
lobs but you know more than the 32 K
that I wrote holes in its far chart two
columns nowadays and some of you may be
using 11 to database which could only
hold 4k so so you end up using lobs a
lot so you can do that streaming but
it's going to cause what we call
round-trip to the database you've got to
get the locator for that the pointer to
the data and then you've got to go and
get the data and that's a bit slow so by
fetching and binding as strings or
buffers you can avoid that second round
trip to the database and here we've got
this fetch info there's also a top-level
attribute you can set called fetches
string and you can fetch a fetch a
string of fetches buffer you can just
say hey for this particular column which
I've kir called Michael just returned a
trip to me directly
I don't want to mess around with
locators I don't want to mess around
with callbacks and things like that just
give me the day that give it to me now
for inserting you can also do sim
something similar just passing in the
string or the buffer buffer for blobs I
need to mess around with locators so do
definitely recommend that that's a lot
faster than than trying to do streaming
if you if you don't need to do streaming
size data fetches some more Pro tips
anybody use the offset next fetch stuff
and MySQL would be would have been
called limit I guess we somehow I ended
up with your answer syntax I don't know
why that is so instead of having to do
the nested query syntax which you used
to have to do with row counts and things
like that you know version 12 of
database you get this this feature
obviously use ax you don't want to send
more data back from the database to know
then you're going to process and there's
just a waste of resources to even sort
it out on disk to do the auto buys to
block reads and things like that don't
need to do that don't need to do the
transfer across to node of unnecessary
data so
limited in the database side if you're
using direct fetches and you know you
only getting get one row and people tell
me and I don't know where this is true
of no Jesus but people tell me that a
lot of queries against the database just
return one row because they're doing
frameworks which just expect one row one
object something like that so if you
know you're only going to get one row
back and obviously you know if you know
you're gonna get to then do too but if
you just get one row back then set the
fetch array size to one then you don't
have to allocate memory node or in the
network or the database side data
obviously as I mentioned before if you
don't know how much daddy you're gonna
get back
fetch using a result set
and this is another little trick in fact
I have a vague feeling I'm allowed to
mention the word bug in this territory
never if you think this is an optimized
articles sequel optimize a feature quite
unquote and I know they're doing some
work there and I know that because I was
in the elevator with somebody I was
asking him and he said yes and then I
had to fly out to Boston so that was the
end of the conversation but the
optimizer when you're kind of casting a
fixed value here I'm binding in your
bind variables TS or whatever that is in
my node is going to be a fixed value to
a time style column some reason the
optimizer doesn't handle that very well
in some cases and you end up with full
table scans instead of indexes being
used so until you get this sort of
patched versions of Oracle database if
they ever come up with patches and
whatever they decide
remember the disclaimer earlier you
might find that you're going to get
better wear performance if you query
performance if you do this cast to do
okay so you're learning a lot of pro
tips I hope this has been valuable you
know so far but the big pro tip
obviously is this round-trip thing
there's round-trip between the database
from server client to the server and
back to the client again and that's it
your summary you know I've mentioned it
before don't fetch data you don't need
to use don't over commit or rollback
because that may cause a round-trip that
may also actually cause extra database
load but that's it's not a round-trip
issue we have a feature which I don't go
into in this presentation called client
result caching so if you're doing
queries from zip code tables things like
that those results can be stored by
Oracle you don't need to know anything
about it in the node process space and
we don't need to go back to the database
to fetch it again if there's another
query executed with the same same
signature Oracle handles cache
invalidation all that sort of stuff for
you so that's kind of pretty neat so got
got a whole bunch of things they'll have
a look at some of these things in the
manual ping we've got a connection ping
functionality so you can ping across to
the database to check whether the
connection is live so you don't have to
do a select one from jewel this is a new
feature which came out at the start of
the month the connection dot ping yeah
but that's a round-trip so any of those
round trips whether it's not whether or
not it's your own select one
from jewel law or a ping you know don't
do it unless you really need to know
what's happening it's better to try a
statement to try to connect see whether
it fails try and execute see whether it
fails and you know do the processing in
the error handling rather than say oh is
it gonna work when I do it well let me
do it you know that's double the amount
of work and you still have to do error
handling so don't don't overdo
round-trips so putting data into the
database so single row DML you know DML
you know insert update delete merge
anybody use merge and past it okay merge
commands DML see where is pretty easy
yeah we've seen connection execute order
commit I just spoke about round trips so
that this order commit can save you a
round trip if I have to do connection
commit later
that's a explicit round trip auto commit
is piggybacked onto the execute across
to the database so it saves that round
trip improves scalability more
performance and then ultimately
scalability obviously don't over commit
you know don't want to commit every
single row you're gonna lose
transactional consistency but it's a
tool there which is really useful to be
DDL i up don't commit unnecessarily
detail of course in oracle land you know
a create table or something is going to
do a commit whether you like it or not
long history okay so that was kind of
okay a little slow if you trying to do
lots of data inserts you had to call
execute a few times round trips we then
introduced a third party contribution
binding for index by binary integer
tables so you can in PL sequel are kind
of get to that the no code later you can
for example have a procedure which using
the bulk inserts before all command PL
sequel just since it's all over the
array passed into it in one statement
effectively so that the node code below
the line there so it's low we would then
pass in the array of values right on the
bottom line there
Val so we called my in proc : Val BV is
a bind variable placeholder and then
below that is the bind variable block
were passing from node with actual data
values we're trying to pass in the ID
one two three four and then we're saying
hey we're passing an array of numbers
the direction is binding and here are
the numbers so we had that that's good
it works in a number of places
thank you very much DITA who contributed
that just on a year ago so new in 2.2 so
2.2 came out on what 3rd of April
something like that execute many so it's
we debated about the name should we
follow the Pythons name or the JDBC name
we chose the Python name because that's
our group so it's not executing multiple
statements it's executing one statement
with many data values so here's the
example so I've got two data values that
I wanted to do two sets of data I wonder
insert I want to insert an apple and I
want to send a banana I got my statement
at the top so you can see I have a key
and a key and a fruit for each data
value but just one statement we have
some new syntax so we have an options
parameter order commit you've seen
before so that kind of behaves the way
you saw it before we had this bind s
which is that technically not needed for
if you're doing in binds because we can
kind of look at the data but because the
way it works we need to know maximum
sizes so you you know we keep having to
look at data and reallocate buffers if
we don't know what it is beforehand so I
recommend you users bind F's certainly
for input data and you definitely did it
for output data where we don't have
anything to to tell us what to allocate
how much memory to allocate and then we
do this new execute many call with those
sequel the data and the options
parameters and we get a result back
which is how many rows so I did insert a
two rows so I get a rows affected value
yeah okay that's not kind of exciting
but not that exciting so what about
noisy data so would have seen this in
the Python talk earlier what happens you
know the key is supposed to be non null
but here I've got some null values for
the key it's not going to be so good you
know all sorts of other things could
happen there so we've got this new extra
what good it's all new
we've got an additional flag batch
errors so if I set that as true mind s
we saw before execute many we saw before
so the only real difference is you have
some fake invalid data and this match
areas value we get back in the results
callback we get the ROI is affected so
we actually add 2 rows inserted I can
flip back to show you the data if you
want and we had some rows which couldn't
be inserted and the offset is the array
position of the noisy data with a
zero-based offset so this didn't trigger
an error this is not the error callback
there's no error at this stage it's a
result callback what happened is that
you get
some of the rows inserted and the so
transaction is started in the database
and you can choose to commit or rollback
if you want or fix up this bad data
insert that and can you know then commit
the transaction so we're giving you that
choice so there's no sort of error in
this case it's all coming back through
the result callback and we get the array
of areas one for each problematic record
zero-based as i mentioned too many
animations I was having fun so the
interesting thing about auto commit in
this case is that it's gonna be ignored
if there are DML errors
it'll be respected for no errors but if
there are those DML errors that like we
saw here the commits not gonna happen
even though you said I want to commit so
you have to explicitly commit again you
know we don't want to as Oracle just
arbitrarily assume that your data was
okay and you're fine with invalid data
being inserted or a partial data being
inserted so we leave that up to you okay
so demo row count it's a little easier
to see with a delete but sometimes you
want to work out how many rows are
affected so yeah how many how many
apples were deleted how many bananas
were deleted we can do that so we got
yet another attribute gave our row
counts line desks we saw before this
time we have a bad slide because that's
not a number but anyway sorry about that
I think you understand it's supposed to
be a string there and the size and
there's a problem with these slides you
know trying to simplify it so you can
understand what's going on and make the
fonts big enough so you can see
everything is this balancing art so I
had to take out some things later but at
the bottom if you can see in red sir on
the right
it's a DMR okay so we still see two rows
were affected but we now get the row
counts there were five values with five
rows with but a Polly today rows with
bananas were deleted
good to do this is the first time I'm
giving this talk by the way so let me
know how it goes at the end diem our
returning sodium returning is kind of
useful particularly if you want things
like the row ID as I show here one of
their what the row IDs of those keys as
the new data values we like then it can
do undo some updates or something else
on them passing some values notice is
array of arrays and result outlines as
an array of an array of arrays just the
way it is because you can actually have
multiple values particularly if you go
back to those delete examples you can
have multiple values come back it's kind
of easier seen in practice than then
described I'm not sure do I have a
another example yeah I don't I don't
have an example but I may have mentioned
and I think it's worth reinforcing you
can call PL sequel with execute menu as
well so it's equivalent like calling
with PL sequel say procedure
multiple times which is which is really
nice too okay so this is like the key or
all came for this I'm sure how fast is a
how good is it we have a little
benchmark I just create some data so you
can see it's very short data is just a
number followed by a short string gonna
set that into the table and I'm gonna
compare the multiple inserts there so
I'm just have a loop using that new
await syntax but you can obviously do
this with all the versions I just insert
have to commit at the very end a
piggyback the commit on the very very
last one versus the single insert with
execute many and again auto commit so
this is my results so the red line at
the very bottom very very close to 0 is
the execute many and that's how fast it
took to insert a particular number of
Records and then the blue line spiraling
up spirals perhaps not the right word
taking off up is the really slow loop
method and it's a little easier actually
if you have a look at the data your
mileage may vary as I say there every
benchmark is different network sizes as
their performances are very different I
had a kind of local database it was
desktop sitting next to mine
laptop short strings but you can see how
many milliseconds that was 361 this is 2
to 7 as it were
it was just again for slide purposes
it's Auto committeth on the very last
iteration of the execute it's by the
magic of ternary operator anyway so even
even for small values there you can see
number of ten rows if you look at the
data set ten rows it's like 38 versus
nine you know that's pretty significant
your mileage may vary as I said and
another data set so I had a bigger data
sets yeah so I had three kind of long
strings three 1k strings a remote
database from my home to my office and
you know performance wasn't as good I
know our performance tester stress
tester was testing something like 11
columns with 32 characters 32,000
characters in each column and she was
only seeing a sort of two times
difference between execute versus
executed many but it was you know that's
still two times different so you're
gonna need to go back and do some
testing on that but I'm kind of happy to
see this and we we certainly do have
people in various medical industries and
things like that trying to load large
volumes of data so this is pretty
significant performance boost okay some
cases you may still need to call execute
many couple of times curves there are
limits on network sizes PACA sizes
database buffer sizes and things like
that obviously I do the same thing that
you saw me would do with the execute
case you don't commit on the first loops
of what of execute many but you would
commit on the last loops
and here's another pro tip don't use
node Oracle DB yeah we have
sequel loader we have data pump these
things are in instant client 12.2 so you
can download them and use them for free
don't need to have a database footprint
and they're going to be faster for some
things particular got data already on
disk than trying to load it into node
and then load it into the database yeah
so I recommend using the right tool for
the right job
PL sequel okay I did mention that a
little earlier but you can also get out
binds from PL sequel which is kind of
cute don't have examples so just kind of
to wrap up just kind of want to get you
out of here so you don't have to stay
any later than you already are staying
connection management think back when
it's the basis for all good scalability
make sure you don't starve yourself of
threads or connections make sure your
closing connections when you don't no
longer need them so they're available
for reuse use the best query method
obviously direct factory is going to be
good for many many situations but use
result sets or query stream if you don't
know how many rows are going to come
back make sure you're tuning whichever
one you do use and then obviously
execute many for insert update delete
merge use lobs as strings I mentioned
and don't forget to contunue sequel net
because there's a lot of stuff you can
do in sequel that I haven't shown you
about tuning sizes and things like that
so two final slides just FYI slides so
we have this thing called office hours
it's asked me anything kind of sessions
we do have a little bit of a theme I
don't have a theme set for this one yet
we just say exploring but often will
have a fusiform up slides for people
just to sort of get them the mood to ask
a few questions and then we're just hang
around for an hour to wait for you to
sort of say hey how do I do this why
don't you do that that kind of stuff so
this is a another way for you to contact
us it's available sort of webcast or
audio or you can just come and chat and
things like that so it's pretty easy to
access and here's my final slide here's
some references I colleague Dan McGann
he's very active in the space he's only
our JavaScript expert in there in the
group and he has a nice series of blog
posts at the moment about using web
services with with nodejs
and I've got my contact details his
contact details and Anthony - and Inga
down there is doing a lot of work in the
back end of terms of coding no Oracle DB
you may recognize that name because he's
the creator and maintainer of the Python
CX Oracle database interface he's now
doing a lot of work on all of these
interfaces for us so I do appreciate
your time I'm here for questions I've
got business cards if you want to take
take my email details I don't have them
and want to send me something afterwards
privately do you recommend you get
involved with us because we do need to
know what you prioritize it's no point
complaining we don't have things if you
haven't told us that you need need
things and also you do have kind of
direct access to us and three
development which does make life a lot
easier when you have some kind of
complex problems really complex problems
we'll still go through support because
they can upload trace files and all that
sort of stuff but yeah we do what we can
best effort so thank you very much for
coming along I hope the conference has
been good maybe see you next year if
we're in Boston or somewhere nearby I
don't know which they tend to swap
cities a little bit but I've enjoyed my
time here
Boston's really been nice to me it's
good to be back here I think this is my
third time here so this is this is nice
thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>