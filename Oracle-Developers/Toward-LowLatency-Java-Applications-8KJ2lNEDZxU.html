<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Toward Low-Latency Java Applications | Coder Coacher - Coaching Coders</title><meta content="Toward Low-Latency Java Applications - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Toward Low-Latency Java Applications</b></h2><h5 class="post__date">2015-06-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8KJ2lNEDZxU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so a little bit of background behind
this talk is it's one that well I'll
credit mostly John as to putting
together because he's done an awful lot
of background work for this but it's a
mostly like a little bit of a case study
so you know really what we want to talk
about is you know things that that we
think are important in terms of getting
applications running to to meet lower
latency requirements and you know the
you know the one of the questions I like
to ask here is like how many people here
genuinely have low latency requirements
anybody yeah
ok so let's define it you wanna try okay
tonight ask you to shout out some of the
sort of industries to where you're
working
yeah show them out zebras
zebras awesome I didn't know that was an
indigo I thought they made themselves
okay
yeah low latency zebra production okay
okay and that's going it's going this
it's okay let a question already though
yeah okay yes sorry this is gonna last
an hour okay yes oh that's a good
question yes yeah why do you have
startup latency so the question is then
why are you shutting down
it's five it says my only concern oh
that's not little late and see that how
count-outs yeah exactly they work yeah
if you put a hypervisor in there then
all the sudden your thread scheduler sir
just like you know forget it okay right
yeah so we see this increasing employee
what do you want to read the slide there
it's like well this this was really we
sort of strung this together Kirk Kirk
and I have known each other for a good
part of a decade or more I think and
we've worked in a lot of different
projects together and this one was a
problem that we had well he had I mean
we okay sorry we my side of the way not
yes us that we decided to engage Kirk
Eden because he's the right person to do
this sort of thing and we thought it'd
be interesting to share some of the
process we went through with you my
backgrounds financed so low latency for
me we're going to qualify some of that
sort of milliseconds realms and things
so we'll get into that a little bit more
detail whoa
who did what let's you sorry what
happened no that wasn't me
what you put water in it it's good for
performance yeah just don't move okay so
we're going to I mean we're going to
talk about all the little bits and
pieces around latency that's going to go
into quite a lot of detail about the why
we see latency inside the JVM and we're
gonna try and go through some of the
process that we went through in right we
say speeding up this application though
so I noticed that you had a nice slide
out first like he said it was that Java
was the hardware vendors wet dream or
something like that so I can use another
analogy like that I think here and
saying that you know when when when I
first came into you know John trying to
solve this problem there you know
basically all their developers I'll you
know they had a hammer it was an
execution profiler
and you know and they suffered from what
I call CPU Envy right
so they have an execution profiler
focusing primarily on trying to get X
you know highest amount of execution
speed they can get out of this you know
really interesting framework and really
you know in in in my consulting right
now what I'm seeing is that the biggest
problem facing most applications is
memory pressure and sure enough when we
start looking at the memory pressure
issues of the applicant of this
particular framework we found that there
was some tremendous gains to be had by
just addressing that particular problem
you know and that's that's one of the
things that you know as we want to show
three to have this this wasn't due to
bad programming naturally this no no
absolutely not no no no there was no bad
program no that's okay
access just blame it all in Java Sun Sun
did it all you know so well what is
latency well you know there's your
standard pandemic dictionary definition
of right so it's just a measure time
given to respond to a stimulus and you
know most latency in a computer system
isn't it's a mix of your works very well
with zebras stupidest gotcha
okay well right getting back to you know
latency we're talking about active time
and dead time right so there's times
when you're actually making forward
progress solving your problem and
there's time that you're just sitting
around waiting for a resource and what
we find in most latency problems is that
you spend a vast majority of your time
just waiting for resources so the the
largest component of response time is
actually going to be dead time and you
know what we've been yeah as an injuries
industry of being getting better at is
just trying to fill that dead time yet
to make things go faster where's the
dead time how can we fill again right so
in this case you know what is low
latency well you know these are response
times that are not really noticeable by
human so that's generally around 50
milliseconds or in the case of video you
know we do like to hit the vsync if you
don't hit the vsync regularly enough
then you know people looking at the
monitors will end up with eye strain and
things like
low latency for trading systems is
faster than everyone else right I mean
you're in a race condition with
everybody else and you want to win right
and that and that's you know when we
asked about the startup time do you have
startup latency that's a good question
because startup latency at the beginning
of the market day is an extremely
important thing to consider yeah and
generally what you see is the JVM will
be warmed up and go fast go fast go fast
go fast you open the gates start trading
and you get what's known as a
deoptimization storm and all sudden the
JVM just blah and then it starts getting
fast again over time right as it
recovers from the deoptimization storm
and you know so that's if you want that
problem solved then generally we say you
know our friends of is at Azul have come
up with some solutions for that
particular problem but other than that
there's only a few other tricks you can
do to deal with it but anyways you know
the point is is that people are really
primed they're trying to get their
systems to go faster than anybody else
yeah I think this slides sort of quite
important not the fact that's got horses
on it but what really matters in the
industry that I work in financial
services it's not so much the latency
has just been first if you go back 100
years before we had computers latency
was measured by the time it took someone
to run down into the trading floor and
drop the trade to buy or to sell all
that matters has been faster than
someone else what's happened now however
is where we're right up there with the
sort of battling against the speed of
light that's not going to change this
gonna be there forever
and but again it's not low latency it's
just been first and that's really what
matters because there's no second place
if you're trying to get the deal in you
read the first or you've lost right and
it's not just for trading actually if
you look at sales sites so you know
again we've we've had some people do
studies where they've had boxes was
slightly less jitter slightly less
latency then
machines in their cluster and that
actually represent two more sales coming
off of that machine so there's companies
that actually do that direct measurement
and they can see a strong correlation
between latency and the you know the
rate at which they turn customers no I
think over a longer period of time any
any industries is as makers doing a
business is competing with out with this
competitors and if you could be they're
slightly quicker than someone else
you're you're going to be getting an
edge so I don't say again I'll almost
argue that it's again it's back down to
just being first being better being been
faster be more efficient even if we go
to telcos to e-commerce etc it's just
doing whatever you're doing more
efficiently and so it comes back down to
the programmers to to get that done
efficiently there's an interesting slide
up here which sort of talked about some
of the I saw a few hands go up from the
financial services side the sort of
numbers we're talking about in financial
services we're getting things like the
CMA hundreds of thousands of messages
per seconds these are being transmitted
out across to hundreds and thousands of
traders hedge funds and if something
interesting comes up you've got to know
whether you can buy or sell whether you
want to buy or sell so there's a whole
lot of calculations that need to be done
and ready it's not a question of just
buying something because it's come up
for sale you might find out that you in
fact don't want to buy it you've been
trying to sell them so you've got to
know what's where you're ready in what
you actually want to do and if it is
something attractive you've got to be
the first there because it's it's it's
like an eBay you're buying something on
eBay the person who gets it gets it the
everyone else there's no second place oh
yeah are you pressing the buttons
automatically yeah I got it right here
no other man okay
low-latency there was a study a number
of years ago I've noticed not so much a
study but a sort of a review and article
came out somebody quoted that one
milliseconds on the trading floors is
worth 100 million dollars they
quantified that as is quite easy to
justify a cable network recently built a
new network between New York and Chicago
and it just costs hundreds of millions
of dollars and it just shaved off about
two milliseconds off the speed of light
time between Chicago and New York by
cutting through a different part of the
Appalachian Mountains it is that
importance again if you can get those
trades they can make that money back in
a few a few weeks or months what I can
lose it again as they often do but it is
that important that necks yes I should
probably mention the bottom bit there
the the see and this this is so C C++
this is a realm where you you tend to
find less Java you'll see traders
trading and using.net obviously on the
front but when it comes down to the
algorithmic engines these things are
written again it's absolutely vital we
can't afford to have jitters and things
it's these are written in December the
things have put onto FPGA cards a lot of
the stuff doesn't even hit the network
into the CPU it's actually acting
knowledge from the network card so it's
it's it's really absolutely critical
speed of light times you will know that
I assume 300,000 kilometres per seconds
in the earth we're in the US different
186,000 292 thing there you go never got
it okay roughly yeah great I mean we
don't work in an infinite compute
environments environments though because
we don't work in compute infinite
compute environments we have to share
and that means we have to use schedulers
and we have to use a whole bunch of
other things and these these things
actually introduced can introduce a lot
of latency and
systems and and there's actually
techniques you can do to make sure that
you know they don't interfere with you
as much and that gets into tuning your
operating systems right there's another
activity that that John Carson had a
wonderful talk about this morning which
was JVM safe pointing and you know in my
consultancy this is a problem that I see
that's sort of lurking under the surface
it's becoming more prevalent in some of
the teeny engagements that I'm involved
with in in in one case when we're doing
one of his benchmarks we could drive the
CPU to 74 percent until I went and
turned off biased locking no bias
locking as a source of frequent safe
pointing so I just shut it off all
sudden we're running the we had like
less pressure on the schedulers because
of it and we were able to run 99% CPU
system times dropped so they're only
consuming about one percent of CPU so
we're getting really really good
saturation which meant that we're
getting about 50 percent more
transactions through that system per
second than if we had left bias locking
on so the you know so these are just the
things that I think the JVM is actually
going to have to get better at and
there's also some problems in the
application we'll just look at some
things in a minute just what you can do
in application to to get them to respond
faster another thing is that in this
sort of getting into the mechanical
sympathy thing is I don't know if you
guys follow Martin Thompson but he
coined this term mechanical sympathy and
and it's something that we've been doing
for like years another one we're working
on Cray supercomputers and stuff like
that we just recognize things like to
work right now right multirow know right
exactly yeah things like to work you
know things have different block sizes
or chunk sizes and that if you work
within those chunk sizes or block sizes
then things work very well if you're the
smaller flooding then then you tend to
have a lot of problems so you know in
this case then in this in in the low
latency world and as I say size matters
you really need to have the right size
in order to have things flow very nicely
and there's all kinds of you know so
there's an exam
things like Network and MT use of disk
sectors cache lines these these things
all matter you know so I talked a little
bit about safe pointing you know it
wouldn't so what what is a safe point
well the JVM has to do a lot of
maintenance internal maintenance and
when it does that it needs exclusive
access to these data structure so at
least the current implementations do and
so what it needs to do is take all your
application threads and say stop I'm
gonna do some stuff and then I'm gonna
let your threads go again so if you
think about this you know what's going
on here so I could stop all your threads
it means I got it in you know interrupt
them get them out of the CPU somehow
then the schedulers have to fire in they
got to get the maintenance threads in
the CPU they got to get them doing
whatever work it is that they have to do
when they're finished they're out and
then we need to reschedule all these
threads in and that's that's a very
disruptive thing the other thing about
safe pointing is that's a it's a
cooperative behavior in other words the
threads have to come to some point where
they're gonna say okay now I can save
point so the first guy there can be
blocked for a long time the last guy
that arise there's the one that's going
to trigger or you know the maintenance
step to start right and so trying to
save point can be a really a significant
factor in the jitter that you see from
the JVM and also getting threads started
again is not that easy right and then
there's a whole bunch of things that
actually cause you know call for safe
pointing the most common one that we are
all familiar with is garbage collection
and and surprisingly enough that's often
not the one that you need to do that you
need to worry about okay so you know
it's Thursday you know you've all been I
don't know what you're doing at the
Aerosmith concert last night I imagine
I'm not gonna ask we're not in the right
state for that but you know I'm going to
basically set this puzzler up at you
right so here's a bit of code can you
see all that I'll see that I try to make
it big enough font and everything okay
which one's faster
this gets into the application question
you're like how you code your
application your coding style really
matters here which one's faster how many
okay hands will do it this way since no
one wants to be wrong like I said we
can't be wrong with this many people
here I look silly right okay I got it
wrong first time yeah yeah exactly it
was a binary choices one or the other I
could have really got it right and then
you go wow you knew yeah yeah you
guessed you had 50% probability that's
like you know okay
so hands up on who thinks it's left the
the code on the yeah the left 1 2 3 4 5
6 in ya hands up code on the right we
have a third choice all right they're
both the same how many people think
they're the same that's the safe choice
isn't it that's the safe choice yeah
exactly rule how many people have not
voted I know there's a whole bunch in
here who hasn't voted there's no sure
who's not sure okay why are you not sure
you've never decompiled all this stuff
now okay so the answer is the answer is
like I remember we had you write them
this morning the answer is it doesn't
matter right now okay the answer is
actually the the block on the left is
faster and it's relatively significantly
faster and does anyone want yeah I was
like nine 13s there's some strange
number like that anyone want to know why
you're synchronizing on the object
versus the call that's exactly it
now if you look at the byte code
generated by these two methods you can
see that the left one generates a lot
more byte code so you know if you think
the critical section yeah which means
that you know I need to make the
critical section small in order to be
able to get good throughput on this
particular method you know you know the
that it looks like you know the right
should win but when you look at what the
entire mechanism and then the coding
style on the
on the sorry it looks like the yeah it
looks like the right should win but you
look at the mechanism then you see that
the synchronizing on the method call
actually makes the number of
instructions that I have to execute just
greater and that's really what matters
at the end of the day so I mean this is
just a small trivial example but there's
all kinds of other examples where you
can say that question question function
yeah no that's not right because the the
method the method lookup and all of that
mechanism where the where the
synchronization actually occurs for this
one for this piece over here actually
doesn't get optimized by hotspot III
would have almost agreed that in fact it
should be optimized out this shouldn't
shouldn't really make a difference
because it's not exactly complicated
well we're not talking about what should
yeah we're talking about what is okay
anybody here a C C++ programmer in the
past so which is quicker plus plus I or
I plus plus oh you had to get me on this
one this morning a c-plus plus I it's
quickly correct yeah but the semantics
are different really yep okay anyway and
and and and why it's because while they
don't have to do a fetch write one a
you're missing a fetch anyways okay so a
little bit about hardware that I mean it
to make our applications run faster
unfortunately we have to look at what at
some hardware bits and you know ideally
we should also be looking at some
compiler stuff to see how all that
stuff's works so that we actually look
at our code we can have them form a
mental models like what's it gonna look
like when it actually gets down to there
yeah now that's kind of ugly to look at
so let's try this little block diagram
over here that certainly like helps you
to understand what's going on right you
know and in this case you know really
what I'm going to focus on is is is
memory pressures like what does the
memory pressure look like in it from
this particular diagram right well you
can see that basically we got some green
bits up there and those are like some
like some load store buffers and stuff
like that so these are the things that
are feeding the CP is the
the actual course right and they're
being fed by the l1 l2 and you know
eventually by the l3 cache and
eventually by my main memory and as you
can imagine you know the time it takes
to get data to the core increases the
farther you get away from it right and
if the core has to wait for data then
what's it going to do it's gonna spin do
nothing yeah that's where you get that's
where you get just wasted cycles right
now the point is you know but we've all
heard this Moore's Law thing right in
2005 or clocks leveled off and all
sudden our CPUs weren't going to get any
faster because the cores weren't getting
any the clocks weren't going any faster
and you know we have you know herb
Sutter saying you know the free lunch is
over now we have to work to get more
performance by going parallel and all of
these wonderful things right now Martin
Thompson did this really wonderful thing
he did he took the Alice in Wonderland
book and he did some text parsing over
and you said okay is this really true
and if you look at operations per second
this operation is retired on the CPU per
seconds they keep going up and up and up
and up right so in terms of our ability
to increase the number of instructions
that we are able to retire on a per
second basis Moore's Law still seems to
be working and it has been working for
some quite some time so okay
the clocks haven't been going faster so
that means that we don't have any more
instructions per second per se so what's
going on here what's happening well
what's happening is that if you go back
to this particular diagram here you see
that okay we need to be pulling all this
data up and we need to be pulling
instructions up and if we get more
clever at prefetching all right and we
get more clever at just trying to sort
out what the what the application is
going to do next so that we can prefetch
the instructions you know before we
actually need them then all of a sudden
we can take the idle time where we you
know we used to spend all this time just
waiting for data and waiting for
instructions and being stalled all right
now all we can do is we can take this
idle time and we can fill it in right so
the cpus are actually idling less than
they used to
now if you're looking at your CPU
counter and everything like that you're
not gonna see this you're not gonna see
this at all right because as long as
your thread is in the CPU and the
scheduler says yep
you know my thread to calculate I don't
know why PI right Raspberry Pi or
something
is in the CPU you know he's got the
ticks he's going out 100% right there's
there's all kinds of conditions where
you looking at say yep CPU is running a
hundred percent but things are slow and
why they slow don't know why and then
Olson will go on to the diagnostic
registers on the chip and we'll say okay
what do the operations per second look
like and then we start asking questions
like what are the l2 what did the l3
cache hit miss ratios look like right
what's your T lab walk times looked like
all these like internal things inside
the CPU they're telling you how you know
how well is the hardware coping with the
code that you've given it to execute all
right so really what you want to do and
when you're going into little low
latency world is make sure that you can
get some to hold of some tools that can
give you this level of information and
you can correlate that back to what your
application is doing so that you can say
okay I seem to have some problem here
where you know I'm not not getting good
data ordering or something like that and
and I need to do something to fix this
particular problem another wonderful
tool I can mention right now is is Chris
Nilan some JIT watch tool right so we
can dump the output from hotspot and we
into a log and we can use his tool to
actually give us a lot of information
about how the JIT is treating our code
first off you know is it compiling it or
we're running byte code right is it able
to inline it
what optimizations that are able to
apply to it right there's other things
that that you get going on inside the
CPU right you know how do we get better
at
how do we get better at eating up those
idle cycles well we get better at branch
prediction all right
so now you know we can we can look at
the at the JIT optimization that
basically says okay that's the winning
branch most of the time so we can
speculatively execute it that's going to
fill in some idle cycles for us because
we're gonna win most of the time right
however the JIT will say whew I couldn't
apply like a reordering of your if
statement to this thing because half the
time it went if half the time or one
else so half the time I'm losing which
means I'm losing all of those cycles
they're just going idle okay so we can
find that information from the JIT
compiler and say is there anything we
can do our you to our code to make this
more predictive we can fill in the idols
that way you know when we go out to the
hardware in the bigger picture you can
see that you know we got a whole host of
other devices that we have to live with
and to get to them we got to use buses
now this one's slightly out of date
because if we have a North Side and
south side bus and on this particular
thing but you know it gives you the idea
you know if I got data on the bus you
don't we gotta fight for it that means
we gotta set signals and stuff like that
hardware to get everything up and up and
running and you know and anyway so that
you know the point is is that you know
this stuff gets it consists
significantly slowed you down also in
fact here's a nice chart thing going
around right if we define a CPU cycle as
being three no 0.3 nanoseconds and say
that Kay equates to one second you know
just start looking at the times how they
explode out right by the time you're
basically doing oh you who wanted to
start a time fast speed there yeah it's
32 millennium worth of cycles that you
just this slide is one that's I mean
it's on the internet just just google it
yeah sure it's it's something you should
really learn because you look at those
things you're heading you're putting
something on the network or even if
you're getting something out of RAM that
the difference between something of your
your main ram which think of as fast
compared with the net
compared with the disk is already slower
than the cash so if you can get stuff
that's sitting in your caches 1 2 &amp;amp; 3
caches it's it's dramatically faster
than the ram the ram is dramatically
faster than network this you should be
designed everything you design
everything when you're looking at
latency this this is absolutely critical
and look at those orders of magnitude
that it goes down from single
nanoseconds very quickly into
milliseconds into seconds
I think they the column on the right is
probably for people who don't understand
milliseconds microseconds and but that
middle column is is absolutely vital you
should really learn those sort of things
right and such a difference and and and
it's it's worse than that
yep right our CPUs are still getting
faster and there's getting faster and
they're putting more pressure on memory
actually the the gap is 8% per year
right that's a speed differential
between CPUs and memory right so that
those numbers are just getting bigger
over time because the CPUs are still
getting faster and everything else isn't
when you have all kinds of other issues
too and at 32-bit bus 64 bit you know I
mean the caches and the CPUs are getting
bigger and bigger we got to use them
yeah exactly
well they're going with l4 cache next
right that'll bring the diagram out
right so really this is what I'm saying
you know what is the main problem facing
our applications to stay most of our
applications are safe are suffering from
the sync called
memory pressure right the rate at which
our application is going to churn
through memory and this can happen in a
couple of different ways it happens like
if you're making you know well there's
our size frequency chart you can see as
we go to red things get really bad right
making big things frequently is probably
the worst thing that you can do now you
know in the JVM there's no free lunch
because we have this thing called
garbage collection or we have this thing
called object lifecycle on and and
unfortunately objects like you know this
chart says who well let's not churn
maybe we can cache all this stuff right
but on the on the on the flip side you
know all the objects have
an object lifecycle cost and that cost
accumulates over time right so sometimes
it's just not worth cashing right you
know can we tune the JVM to deal with
this problem the answer is no this is
something that your application is doing
to the hardware and not only that is the
JVM is getting out of out of the way
like eight is better than seven seven
seven six six hundred five so as we've
been improving and the JVM is being
getting out of the way of your
application your application is going
faster it's actually putting more
pressure on just because it's it's
allowed to actually win you know one of
the first demos that we put up here is
looking up the numbers anything sick
he's like a 500 megabytes per second
Ellison it jumped to 1.6 gigabytes per
second right and I said okay what was
pushing on this application that didn't
allow it to maintain the 1.6 gigs per
second there are some other interference
in there that you know that was causing
a you know that particular problem right
you know something was getting in the
way because if you don't get in the way
things will just naturally do that
they'll just go fast right you know so
what does it look like well I tend to do
these calculations from the garbage
collection logs and you see I've got a
black line down there that's my
threshold everybody is quite surprised
at that particular threshold but once I
find once I start getting below the
black line in terms of allocation rates
or memory churn then I can't do any
better for you your application is
probably about as fast as it's going to
get from a you know I can't tune it to
be any more memory efficient and and see
any better response times you'll see
minor ones but nothing worth going after
right however if I'm up in that area
where it got circled over there then we
can you know we can talk we can do
something and sometimes the the you know
the difference is in performance from up
there to below the line are quite
significant yeah
very significant surprising the signal
orders of magnitude yeah so we did one
was like it was what if we went from or
we could ship it slides okay we'll show
that later I want
that's that's yeah we won't give her the
spoiler right that means I have to stay
here till the end
excellent okay I'm sorry okay okay so
you know what can we do to do things
well you know the CPUs have these things
called pre fetchers and if you have
everything laid out in memory in a very
predictable way then life is good right
you can find things by what Gil likes
called dead reckoning which means I can
just do a calculation and get it and
then I can stride doesn't matter if its
forward backward whatever as long as
it's regular I'm good
Yeah right now unfortunately Java
objects and Java heap and stuff like
that form this thing which we call an
undisciplined graph now if things happen
to be together you got lucky
right if things happen to be in the
lineman as a matter of fact you don't
even get a cache alignment right you
only have a 12.5% probability of doing
that that's it so things are actually
stacked against you to be able to do
this right and as you go through more
garbage collections things become more
undisciplined which means that now we're
just like jumping randomly through
memory and that's absolutely the worst
thing you can do because you basically
shut down all the pre fetchers when you
shut down all the pre fetchers you shut
down all the vectorization you shut down
all the vectorization you go to scaler
performance and that's just a place
where you don't want to go especially if
you have low latency concerns so you
know the the real the real problem here
you know that that we're facing is that
the way that JVM manages memory is
starting to become a problem and it's a
problem that we're looking to solve in
in in Java 10 when in fact the proofs on
we might put that into context if you're
reading in messages coming off a bus off
the network you're reading stuff off the
disk you put in these into memory you're
working with them as you read those in
there basically being another one you
want that one that one okay
as you read them in they're being
dispersed all over your memory you've
got you know maybe eight sixteen
thirty-two
100 or even a terabyte of RAM in there
it's been randomly or not quite randomly
but it eventually gets relatively
randomly distributed and everything
we've talked about with the cache you go
off to read what you think of as one
object which in fact has got a hundred
other objects inside it because the
throw aggregation etc it's got to go off
and find those from all over the memory
and none of those are going to hit the
cache at all so if you can get your
objects into one small space this is
where we're sort of going with this we
can start to see quite unbelievably
informants so there you can get the idea
of a difference between a cache hit and
a cache miss in terms of timing so your
application is gonna run like a hundred
times slower for no particular reason
you're not gonna see it but just because
it's in RAM you've got stuff in memory
again that graph should be ingrained
it's the difference between hitting the
cache and the memories is phenomenal and
sort of as almost amusing here somebody
talked about cloud with low latency yeah
cloud for me just doesn't it's look at
the time it takes you to get something
hang on to the network and then add the
time it takes to get it off to wherever
the cloud is well here's he I mean
here's another point right do you know
that accessing memory in the lower
quadrant is much cheaper than accessing
memory love in the upper quadrants if I
said yes I'd look really intelligent
with my yeah you better did you know
right okay and and you know this that
just comes down to addressing schemes
and you know even that makes it makes
different system right so so what do we
have what are we doing right so one
answer is structure array which is being
worked on by some people from Azul and
and Martin Thompson the source code is
out in github look at it actually go
with like everyone to basically start
beating it up right now he would like to
get into it like to get it into Java 9
we're more realistically if is if it's
going to get in at all it will probably
be Java 10 all right but he's looking
for as many people to try to use this
stuff it will work in Java 6 Java 7 it's
just not going to go fast
it won't go fast until we get intrinsic
support for this particular
for this particular thing okay so what
is the problem John's facing right well
he has to process a lot of these ugly or
they call them for ugly XML F PML
documents and yeah they're ugly
right there's seven K documents you put
them into memory they're 25 k what is
your quality that's like 40 machines to
every single Bank on the planet that's
working with derivative trades or only
anything which is the boss one in fact
every single investment bank it's got
these literally hundreds of millions of
these large trades each of them roughly
seven or eight K in size in XML you do
Java bind these they go into about 25k
in size we've got about four or five 600
objects inside each one of those and
just what we talked about before those
four or 500 objects get distributed
right across your memory space and
you've got a hundred million of these
sitting in memory that's just goes
slowly and that that was our issue it
sort of seems to run fast compared with
reading an off disk and the the big
arguments years ago used to be between
do I stick it into a relational database
or do I stick it into a called NoSQL
database you know Redis or or a MongoDB
for example and that was that was the
sort of initial thing and then as we get
the volumes increased more and more we
start to load them into memory that we
start to put them into coherence for
example but now is we've reached another
barrier because even when we got them in
memory we're not getting the performance
we need out of these we need to be able
to process these in an incredible speed
and it's just getting more and more and
more of these and so the banks are
mandated to to work with these things
and they're all competing little by
definition banks have a lot of money and
I like to extract as much as I can out
of them and this this was our problem
and the quicker we can make that work
obviously the as soon as they find
something that's has better performance
they buy it because they can they can
compete against the the other banks yeah
so what we were looking at we you know
we were looking to get around a million
a second out of this
I won't go into the details of what it
what it is in terms of trakula
transactional but we're looking to be
able to process read and search through
about a million of these a second we
could get about two hundred thousand a
second out of them you could put it on a
multi-core box you could get a million
but it's that the processors running red
hot and you put any more in there and
it's it's starting to fragment up so
yeah there's no Headroom for for
anything happening so yeah and so we
built a binary version of this where we
basically parse these XML documents and
we squash them into basically just a
byte array and we thought it was pretty
good so we've got the performance up
we'd gone up from originally about forty
thousand a second to two hundred
thousand a second and that's what we
called Kirk totally buy him severe if he
came to London and he was there one
morning here he was there the next day
yeah and well fungus is good beer yeah
right
yeah so what do we do we did some stuff
we eliminated all the object raishin I
would they were zero no object creation
it took took quite a long time there's a
lot of work on that but alright like
we were writing these slides this
morning in my very feeling silly and I'm
so totally sober I might add yeah the
one after those quarters got like I
don't believe you I just couldn't resist
them they were so funny are we gonna put
more in them let's just do a whole pot
of slides with Castleman okay I don't
know what I don't even like cats but you
know oops I shouldn't have said I like
offense okay there we go so we're
getting to the interesting part now
right yep okay quick quick quick
yak we're so quick okay yes it goes very
quickly so basically this apology is
funny but it was in the the last talk
that I did a bit of repeating it but
basically if you put a string into
memory it's a ski it's three characters
for if you put an old Terminator on the
end if it's a Unicode so Brazil a bit
bigger stick it into Java it's 48 bytes
even for three characters like that you
put a country code so currency code in
there US dollar USD it's 48 bytes
instantly that's pricing that's serious
bloating you put a date okay
365 days a year you know that multiplied
by one hundred thirty-six thousand days
covers 100 years right stick it into a
into a job ad 8 is 48 bytes it's just
too big we don't need that much 36,000
you can stick into 8 bits sorry 8 - 2
bytes 16 bits so moving on to the next
one what we what we're trying to do is
basically squash this down so the code
on the left is sort of typical getters
and setters if you're getting
information you basically just return it
so it has an API if you use the API you
can reuse it across lots of different
codes you'd use it internally but if we
rewrite that and instead of creating
Java objects remembering that sure what
you're trying to avoid now is creating a
completely dispersing objects right
across your entire memory space so the
one on the right which I'll zoom up in a
second is exactly the same api which
means we don't need to change the code
that we're working with at all but we've
implemented it differently you just
enlarge that so you can see it a little
bit better now this is just a suggestion
it's not the way to do it
but often we just need the dates we
don't need the microseconds than the
seconds the minutes and the hours we
just want the date but there is there's
no class that just does the date and
were still even if there was a class
that just did a date it would probably
still be 48 bytes what this does I just
get rid of the milliseconds divide it
down and it basically creates 16 bits on
all I use of those 16 bits and I check
them into two bytes again it's just an
example there's lots of different ways
of implementing this you could even go
down to our resolution you could say
that in fact I don't care about I just
want to know the day of the year for
example I don't care about the year I
just want to know which day of the year
it is so you could reduce it further
down to just what would it be 7 bits
down to three hundred sixty five out of
five hundred and twelve different bits
so there's we can come continually
compress this down but what's
interesting is this comes out of a byte
array and says where you see data there
is data 8 &amp;amp; 9 right at the bottom left
that's one byte array now imagine now
we've read in our complex and horrible
message that's coming off the off the
network cards coming off the disk that
is one byte array that could potentially
find its way right the way up into the
cache and hit the sweet spot of the CPU
so it's not just the the simplicity of
being able to deal with the the it's
vastly smaller I mean we've gone down
from 25k down to actually less than 400
bytes so that's less than an MTU that a
network socket size so it's less in fact
size we get several messages and it's
like seven cache Linus yep on a normal
CPU today so not only have we got a
fraction of the size vastly more
efficient doesn't need to be split up
when it goes across the network that
also what we read it in we can actually
hit the right the way down into the
cache and it's computationally useful
which means it's not like a compression
technique where you decompress
afterwards yep and so the data that
comes out of this as you see from the
getter is immediately useful we can do
something with it we're not even
deposits as Coke said we can read it in
as a packet and we can immediately use
it so the class that sits on top of this
is more like a template it's just
template that gets the data out of the
the bytearray okay like a flyweight yeah
yep so you've really important out of
this it's the same API what we're doing
is reengineering this is what we were
trying to to tune is to do the stuff
that these all these banks are doing is
not unique to any particular country or
any particular bank that these
derivative trades go across different
commodities different currencies
different exchanges also there could be
houses there could be oil there could be
currencies it doesn't even have to be
financial transactions nearly financial
transactions but this is just straight
box standard boring old XML but we can
get it into a byte array and then the
efficiency out of those incredible so we
go to that pair yep I've carefully typed
out the text there so for those of you a
heart of sight and read the bottom bit
okay and talk josÃ¨ this because you can
see the dead cat floating in the water
okay so let's let's go make grumpy cat
happy by showing us does this really
work or is this like the perpetual
motion machine things so bear with me a
second while we every bunch ever see
tear cuz you're gonna sit down yeah okay
you can sit
enjoy the balls
oh yeah
okay that's good listening going right
so I think 720p little little day let's
give it a go okay so I've got a little
bit of code here so what I'm gonna do
let me walk you through this so this
isn't the one I wanted to show this is
the one I want you to show so let me let
me show you what we got here
it took a second one I by the way these
are benchmarks and benchmarks are
probably the most boring demo seeking
hassle you know start asking questions
now if here yeah you guys gonna
disappear off like a rocket because he's
got a fly to meet in unison Zack I
scream questions so they'll they'll be
closed they're supposed to be closing
the door shortly after five o'clock so
so let me just show you the XML so you
understand the what we got here this is
the XML what we're gonna do is read it
in I'm gonna ask some of you for a
number or one of you for a number so we
can prove that it's actually a demo and
it's not rigged yes okay pick the number
six six was the last time yeah this I'll
just scroll down we don't really need to
understand it this is a standard example
derivative trade it's basically
exchanging some money this is an amount
what I'm gonna do is read this in I'm
going to change it a little bit I gotta
create a million of these things am I
gonna squash them down compact them and
I'm gonna put them into this byte array
that I mentioned before now there's one
little headache on this stuff is that
most of the api's that are generated
automatically from this generates
objects so when we get a date and it's
find a date in here here we go here's a
date an unadjusted date that comes back
as a Java date because that's just what
people want to use they don't want to
bytes which they don't got to mess
around what they want to job a date so
when we do some a lot of manipulation on
this regardless of how efficient it is
we still have to come back with a Java
date so I've taken one variable in here
which is cleverly hidden down here this
one here the roll convention they'll ask
me what it means it's just a little
thing that I chose in the standard and
I've changed this to an int a lowercase
int
that means that when we search for this
we can actually do some manipulation on
it we're going to search through it and
this is the demonstration where we're
going to look at the memory pressure
that we get as we work our way through
this so I'm going to ask someone for a
random integer ideally for anything
though not sex
sorry what was that
I like 29 versus is it 29 or 30 no pee
no and that'll do the other ones too big
yeah it breaks if you go too big yeah
doesn't our girls are beyond the handle
so again what's happening I'm going to
run this while I want to click it off
and run it so I'll go through the code
with you because it takes about two
minutes to run we've got about ten
minutes to go here so we'll just kick it
off and I'll just study what it's do so
all I'm doing is basically reading this
in so let's kick that off in the
background I'm basically going to read
that in so inside my setup what that
does is read the trade in just does Java
binding as usual it's then going to
compact it's it's going to change a few
variables sitting in the little loop
it's going to put a few random numbers
in there it's going to create a million
of those and it's going to create an
array for me this array here is my sto
for a simple data object list of
confirmation document roots which is
again it's been generated this generated
code that's come out of this and it's
going to sit off and you can see at the
bottom here on the bottom left it's busy
calculating creating those we don't
particularly care about the performances
is doing at all it's doing is generating
the test data then I'm going to do this
little size test what that does is just
going to have a look at the size of this
every it's gonna print it out oh you
don't care well we'll see what the
result of so it's gonna just print out
the size of it and it's gonna just show
you the the ASCII a little bit more
interesting stuff is a little sort of
Java 8 demo and let me show you what
that does so we're gonna sit in a little
loop so it go through our list of 1
million trades we're going to filter it
we're going to get everything from July
so we're the trade dates
is equal to
July where they going to do we're gonna
get an integer value an initial value
which returns a big decimal and then
we're going to do a reduce on that so
we're gonna get the sum of all of those
numbers remember we saw the 50,000 those
are the ones that are randomized and so
it's gonna get the sum of those so it's
basically a little tiny MapReduce across
1 million of these we're going to do
that 20 times and we're gonna pick up
the last one of these and record the
time that that takes to do it we're not
doing this for any other reason other
than 1 we look at the the memory afters
this is almost like it's sort of just
putting a little you know stamp to say
this is where we reached so that when we
look at the GC logs we can actually see
which bit is from from which part of the
process so we just count the GCS into
that then as things run a little bit
faster any reason I moved that down from
20 was because it took a little bit too
long to run them I going to do a
parallel stream we're do exactly the
same but we do it in parallel this is a
fork or hyper-threaded so eight core
machine then we're going to whiz through
this and we're gonna get this integer
value remember I said I changed this to
integer when we put number 29 in there
and we're going to take the sum of all
of those but you'll see what I've done
here is little for loop this is because
when Coke and I wrote this that's 230
this morning
we couldn't remember how to do the
reduce at that time in the morning we
hadn't had enough beer or cocktails and
some brains weren't functioning properly
and I thought I'd leave it in so in in
the light of the morning having had a
just a few hours sleep I suddenly
remembered that in fact you can do a map
to int and then a sum so we're going to
do that it's exactly the same as the one
above we should see the same results but
because we're doing a stream we can now
do a parallel stream as well and we're
gonna look at the results and that's
pretty much it and we just print out the
results of the bottom so let's let's see
what we got get around it so gonna run
it's run its run already was running
there we go running already just
finished yeah see if I run it you'd sit
there love you
it takes as you can see about two
minutes to run so here is the contents
of our entire XML that you saw earlier
it's basically 370 bytes sitting in a
byte array okay
that is the entire XML in that 1 by 2
right I can reconstitute every single
part of that XML every single value will
absolutely queires that document when
you started the XML document is 7 point
4 K the in memory size bounds into Java
is 25k % that amazing said yes it's a
very good question so what happens to
all the strings the URLs and things like
that that's a very good question
rather like zipping what we can actually
do is go through and look for common
strings so we bought a lot a lot of
utilities that basically you can say
certain parts of this document have
common strings those are for example
URLs we mentioned the currencies there
are only one hundred and sixty three
currencies defined so rather than
actually have three characters we can
actually say it's one of these hundred
and sixty-three so we can build a lookup
table now the bits that you do see in
here these are actually the trade
reference IDs and so those are going to
change every time so we don't put those
in a lookup table are we done yeah it's
finished okay the result so let's so
when we now we're retrieving we're
iterating through a million of these
retrieving a big decimal we get a pretty
much 1.5 million a second out of that
now remember we're trawling through an
XML documents looking for these things
and that's retrieving the data at over a
million a second we run it in parallel
or up to four and a half million a
second it's not bad for a laptop running
through data which is more data than I
could fit on most servers now let's come
down to here when we run through our
little for loop this is a single thread
on a for loop bringing back an integer
it's running at five million seconds
it's quite impressive what I thought was
actually interesting and I just
discovered this this morning so when I
run through the same thing you remember
there was just one line
and spines at the top here it's the yeah
so here's the histor for loop and here's
the next one down while we just do a map
to ends in a some that thing there is
running through a six million a second
twenty percent difference which is
interesting because it's just compares
just one line so it's actually the
lambdas are actually running faster than
the for loop which is this getting
nothing in it which is sort of I was
impressed however we do the parallel
stream what we get out of this 21
million a second now that if thank you
that's like 21 million a second so
grumpy cats not so grumpy anymore yeah
dead floating at the top of the that
wasn't like the grumpy ones there's some
other things we can do so basically I
didn't show the code on this because
we're running up five more minutes I'm
writing these out to disk I can write
the entire one million and I've just
chosen this because the size varies
because we're using run length encoding
we can have variable size of arrays so
I've just chosen something slightly
bigger than the remember they were 370
bytes I just thought I spun them in 512
for safety cuz they'll vary in size so I
put all 1 million of them into 512
megabytes it took 3.8 seconds to write
them but look how quick it was to read
them back 880 milliseconds to read 1
million complex derivative trades back
now if you add that less than 1 second
plus the time it takes to search through
them I can actually search through a
million derivative trades of any part of
the trade in less than a second off disk
that's pretty amazing other is there is
a bank who shall remain nameless that
has 17 terabytes of in memory data
distributed across 400 physical machines
I can get the entire content of that on
this laptop using this mechanism and
because we've compressed it or compacted
it down throating myself it is vastly
faster as you can see so this is like
part of our back to binary movement that
we've been so that I've been sort of
talking about it's like getting how do
you get
more information per bit in your data
streams right XML is a very very sparse
format but here we taking this very
sparse format and said hey you know we
can come up with a more format that's
more compatible with the computer it has
much higher information density in it
which means that we need less data to
get you know the information around and
that just fits very very nicely with it
with the internals of how the computer
works you know so you know instead of
getting like multiple packets per trace
we're now getting multiple trades per
packet already you can change the
networking equate you know the the neck
networking characteristics your system
and enormous lis same is like this stuff
is just gonna be cached right instead of
a document flushing your cache we're
gonna have documents in the cache right
so we don't do other alignment things
and we can get prefetching going on you
can just get incredible there's even
more there's room here to get even more
speed out of this and in other words yes
so we have two minutes left and I'm
assuming again they were applauded in
their own time rather than now time so
we got two minutes this this is this is
one of the tools that we use which Kirk
wrote this is a GC log analyzer and Kirk
you're the best man to show me sure well
okay really what we want to just see
what the allocation rates right remember
this remember this line that's there was
drawn across about this sort of level
here and he and Kirk said anything above
that is is problematic now this was like
this is about it's about here this is
this is an allocation rate of about one
one gigabyte per second when we actually
run that test on the int this little bit
down the bottom here is where we're
doing it now we can zoom in as much as
we like there are zero objects created
in there there are no objects and that
entire search and we're not caching
there's no caching so that's straight
into the straighten of the processor
that gives us exactly one minute what
closed
mmm questions yeah
yes hit me again what sorry how can you
tell
there's nothing in the code to cache
we're there's we're just using
primitives right so these just they're
just allocated on the stack and that's
it that's where that's where the
calculation is happening any other
questions yeah of course you know the
frontier yeah okay I think you get the
source code not for that particular one
but I can give you the source code for a
comma delimited file which is exactly
the same so just just contact his office
against anybody needs to once the slides
just tweeters or emailers Alwar ended up
with using source code as well there was
another question at the back yeah you
liked you
I'm not sure I'm just heard this so I do
have none that you're talking about the
optimization I just missed on what part
sorry it's really hard to hear oh this
this works for absolute sorry so as
whether it's just on different versions
of XML or different I'll give you this
this works on any accepted Lea any XML
just my domain is financial services the
actual one of the production instances
of this is for a very large telco in in
Europe and Australia and it's it's it is
any XML basically it's Java its Java
binding so it takes the schema builds
the binary model out of that builds a
codec from the binary so any XML any
jason middle it'll work right across the
board so that's another question down
here and we'll then we'll have to call
her today sorry again spray
oh it's yes so the questions about for
for web services etc these objects will
they're the implemented in binary but it
also means they can actually export
themselves as XML as JSON as binary it's
just a different implementation of
serialization so yes to answer your
question yes you can go from directly
from JSON into binary from binary into
XML from XML into JSON or any other
format that you care to mention it's
it's after the it's out of the box you
literally just say to Jason it's we're
gonna get chucked out I'm afraid so
dude to come and ask those questions
afters we're going to get email address</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>