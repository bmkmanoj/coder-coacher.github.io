<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Leveraging a W3C Linked Data Platform and OSLC for Loosely Coupled Integrations | Coder Coacher - Coaching Coders</title><meta content="Leveraging a W3C Linked Data Platform and OSLC for Loosely Coupled Integrations - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Leveraging a W3C Linked Data Platform and OSLC for Loosely Coupled Integrations</b></h2><h5 class="post__date">2015-06-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/pVgIxehB-ew" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good morning everyone my name is Steve
spyker I work with IBM work within the
software group within the rational brand
so the people that not just bring
irrational rose when everyone i tell
them i say i work for rational it's like
a rational rose that was that was a long
time ago but anyway our work within 2
integration and i want to talk to you
today about leveraging some of the
linked data concepts or and some work
going on at w3c and how we're applying
it to loosely coupled integrations soon
as a small group feel free to to ask
questions i have a number of slides and
concept to get through so hopefully
it'll make sense we get to the end if
not you can either you know follow up
afterwards so so we're going to cover
why we're using it what motivate us what
applies beyond what I just do for a
day-to-day job within rational going to
you know what what is it really from a
high-level concepts to a bit more detail
and then talk about what's going on with
the standardization I'll sort of history
the state of the art and current specs
and and timeline for those and a little
bit on what open source is available so
I've been it IBM 15 years now I started
off with in internal tools within IBM so
sadly maybe I've been working on to
integration my entire career it should
be further along but if you look back in
some of these years I'm kind of
guesstimate about what some of the
history around tool usage and the way we
looked at the problem is we didn't
really look at how they work together
necessarily week we kind of wanted them
to work together but really focus on you
know is is my editor enabling me to do
certain things as my defect tracking
tool providing the right reports for me
query capability added fields and am I
getting the the best function out of my
individual point products you know as my
SCM tool providing cross component based
lighting so a lot of things that were
really focused on the individual tool
what it did
so you needed and work together some of
them would work together especially when
you look at you know from a single
vendor like rational clearcase to
clearquest they would be some type of
integration that existed between those
if you looked at some modeling tools
maybe you could export from one format
import to another but still there is
always this lossy bit and you couldn't
work seamlessly between the different
tools so around 2005 we looked at ALM
sweets if you will where we kind of put
this glue in the middle that helps
bridge and try to use the product epi to
connect to other things often require a
lot of specialized software installed on
your machine the right permissions the
right dll's you know everything to get
it just right but the problem with that
approach was developers the tool users
were limited by choice because they were
now limited by what could actually
integrate not what they wanted to use so
it was a bit limited there as well as
once you have this point to point API to
API specific binding from tool to tool
and you do that again on this other end
now of a sudden you're locked in to this
upgrade nightmare I can't get everyone
to roll out well in order to roll out
new version of one tool you have to roll
out everything which which is quite hard
so we found that was very very hard to
build and develop some of the compliance
reports some metrics around this
information all the information was in
different kinds of formats described in
different ways you know different values
for what a priority is what the project
is what what people are so this
motivated us to look at a better way to
do this so we looked to the web for
inspiration and we use as concept and
I'll get into a little bit more of of
linked data and specifically around a
commute community and community
specification I call OSLC air we call us
I'll see which stands for open services
for lifecycle collaboration
or if you google it it means our savior
lutheran church and Oregon social
learning center and a number of other
things I think there is a new one that
Oracle made up that's OSLC I for you
what it stands for but the the key
message here is we know the web seems to
scale pretty well you can navigate
between web pages quite well you can you
can get this you know upgrade one web
server without anyone really noticing so
what can we learn from this and how we
can we apply it to our individual tools
and not just from a point of view of you
know HTML pages that you go from you
know 18 a tag href link to one page to
another but providing some type of
scalable integration across a large set
of tools I look through another case
around cloud application management and
the things I'm telling you about here
actually some of the cases were we're
implementing and using it for today so
it's not a futuristic view so around the
cloud application management pain point
so again it's quite complex because
there are so many different tools
available to manage cloud applications
these tools all have their own API the
the resource models are somewhat similar
overlapping oftentimes you have to
repeat information between them as
opposed to just add on information again
this is sort of the same problem pain
points that we saw within our ALM sweet
so we want to simplify the way that
cloud applications are managed so we can
do this by you know looking at different
aspects of it one is around the way
these application templates work so
things like I want to build up a certain
instance with say SugarCRM certain
application server whatever so there's a
templates to build up what vmg want or
what what application profiles to run
there's a lot of work involved with you
know making sure you have the right one
parameterizing it starting it up with
the rate parameters you want and then
managing the instance ongoing
and then also within healthcare and life
sciences you see a problem in spades
with dealing with so many different
places that you need to report
information or consume information from
so not only you have your own your own
systems within your own practice or
office either are dealing with
regulatory agencies where you need to
have their own limitations and data
formats you need to submit you have the
the pharma companies that also want to
receive information about the drug trial
different regulatory not just from the
from the FDA that you might want the CDC
to to collect information as well
because of adverse event problems with
medication so all these require off in
their own custom protocol format schema
and quite a mess in and and not to
mention even just attempting to share
information across different middlemen
university studies that occur to
leverage this information to provide you
know better care for the individual so
he kind of gleamed a little bit about
what it is from the uses and background
and there's a lot more uses in baccarat
lovely uses that you could you can see
that would apply here but wanted to get
into it and if you don't know this guy
he's a Tim berners-lee he'd inventor of
the web and so he describes the you know
there's original idea of the web around
what came to be HTTP and HTML was about
providing information to for humans and
then links to other information and
providing it in an open specification so
you know everyone was free to share
information across the internet but he
came up this other concept called link
data that builds on the concepts of the
web following some four basic rules one
of the rules is use your eyes to name
things so sounds pretty simple but if
you're if you're going to scale to
you know the web you need to make sure
you have a naming system that will scale
to that so that's what your eyes were
built for it of course an important fact
is if I have this your I need to it
would be nice if I could traverse that
link get some information so make sure
those are HTTP you are eyes and I can
look things up and then when I look
something up that'd be great but now
they get back a proprietary format a PDF
something it might be hard to crack that
open and know what it is and what it
means so use some standard format so i
can now do something interest more
interesting with that structured data
and different applications and then when
i provide that structured information
make sure that it itself has links to
other things so now you can repeat this
pattern you can now look up the things
that are associated with it so it's
quite simple but quite powerful concept
and i'll talk about a little bit more as
sort of the challenges i would say that
the link data community this was sort of
the specification for link day there
really wasn't anything more to that went
on to define what this really means and
as interested enough he concluded after
this is like you know it's quite simple
this is his own words the you know in
some ways you know the first web that he
invented was for humans and then to say
this is maybe a bit more for better
human interaction or more form machine a
machine processing or to enable Skynet
i'm not sure which so i'll use some
simple scenario from the world of
software development where I come from
so you look at the talk about
information on the web it comes from a
human can read this on a web page and
say test gate 14 is blocked by issue 973
so we generally know what that means
pretty straightforward it might be a
little harder for a computer to figure
that out it could probably do some text
matching and figured it out in some way
but you know there's some problems with
it in a sense that issue 93
is our 973 is you know especially within
IBM I who knows how many bug tracking
systems we have across their 450,000
employees but there's probably a hundred
issue 9 73 s but from the human we know
we know what this looks like we can we
know some other facts to we might learn
these things from you know talking to a
friend at the water coolers you know it
might be Joe and my oh wow it's cool you
work on Apache as a committer you don't
know some project but this information
is you know easily consumed by humans if
we break it up in a syntax or format
that helps the computer understand this
and help structure it we can see it's
broken down into three things you know
that the subject of a statement the
predicate and the object and so now
we're following Tim's rules we give you
our eyes to these things so we named
them so they're now you know clear that
this is what it is we also make them
HTTP or I so we can look them up I
haven't looked anything up yet i just
broke these down so following his rule
of well let's look it up so let's say we
fetch it to you 973 and we see if with
973 we can learn additional things we
might learn that it really is an issue
because you know i actually looking at
this week on its own here just opaque
URL you don't know what it really is an
issue it could have all kinds of weird
characters in it but anyway you look it
up maybe we learned it's an issue we now
learn all this is valuable we learn it
depends on another bug 318 and I think I
picked one out of the some jdk system
but anyway and then you can also learn
what actually the issue 973 is is
actually owned by joe and this this is
you know the valuable will provide links
to other things so you can so now this
and now correspond to this link you can
learn that it is really that same person
you know you provide a link to another
item you can now look up to see what the
status of that bug so it is a very very
simple pattern you can just you know
sort of the grant normal graph pattern
just repeat over and over so draw on the
table form you know it's confusing after
a bit but then you can start doing
interesting things to show a simple
graph of this information so that's a
that's just really a attempt at a simple
view of what linked data is and what Tim
meant by linked data applied to software
development so it can scale quite large
like it was built to scale to the to
internet and web scale so this image is
taken from a project is called the
linked open data cloud so it's LOD cloud
net that's weak down at the bottom so
this is this is like a subset of a
subset of a subset of the information
that's available this is what people
provide to this information to this
website it's public only public
information and this is only showing the
different areas and domains that this
data comes from so just the vocabularies
that is so not the actual data
relationships so they're showing how
well this information can scale and you
can start building some really
interesting relationships and graphs
about just you know where is this data
coming from then pulling down into it
you know what what interesting
relationships exist between you know
different domains you know that you
might see you know information like this
is being provided through scraping
Wikipedia and building up dbpedia and
providing information that way IMDb
information is exposed this way as well
data gov has various information dump
like this day UK gov etc it's a little
bit about how we are applying linked
data to this software development world
and specifically through this this OSLC
effort so as you can see we
have relationship between different
domains different tools so in the green
we have requirements requirements linked
to change management and a brown which
then could link over to you know a
ticket because the defect may have come
in through you know operations and we
can then link all the way through to say
you know the this code came from this
requirement but someone's having this
problem with us this area of the code
etc so we can start building some
interesting relationships across mr.
multiple domains so slc build off this
this common you know four rules of that
tim has in and pulls together a
community to define some let's agree a
little bit on what a defect looks like
let's agree a little bit on what a
requirement looks like we're not going
to do the uber make a metamodel and that
cover all cases that you know because
all the tools we did a survey now a
requirement must have 273 fields in it
so if you look at what we have for
requirement it's it's like three
required fields and a few optional ones
so you know in the top here we have a
change request it says it implements
some requirement and that requirement is
you know it's a link down here and down
here in the green that's you know it
clearly says it's a requirement it's got
some title and again it's got a link out
to something else that's validated by
some test case so resource definitions
are important well beyond that for tool
integration we find it quite useful to
not force because when you're looking to
another tool and you provide some of its
information that Tori knows how to
present its information to the end user
so we also provide some some useful HTML
snippets or how to get some information
from it so this concept of a tool tip or
a link preview so instead of getting the
full web page we just get a simple
preview of the page so you see this in
some webpages where you you know
sometimes you mouse over and you see a
little tip that comes up so this is a
model that we've defined how you can you
can do that and it's quite common in
especially desktop IDEs when you're
working with into old and then the web
is quite useful as you're you know
navigating query results or links from
from one record to another to not have
to you know switch gears if you will
clink click the link move over into this
other tool and the complexity of
presenting all this information is like
I said it was it's over in the other
tool that knows how to deal with it I
mean even trying to get everyone to
agree on what a priority is for a given
record and then trying to get to some
agreement or some description language
that can say what that should be
presented like is it is it integer is it
a string is there an icon associated
with it you know Saul these things we
just say you don't care here's here's
some HTML fragment snippet just show it
so that same thought pattern is when
when I want to you know giving
information it's all linked together
it's nice but I want to care and feed
for my links I want to make sure I can
I'm a requirement stool I want to find
some bug i'm going to associate with it
but i don't want to have to you know as
i want to write write this integration
which typically would have to do is
you'd have to rebuild the query
interface from this other tool you'd
have to learn about the schema of it you
know does it simple full text search
does it have a query builder that you
can use as I'm predefined queries all
that would say forget about we just say
here we define a protocol simply to say
give me a URL of a thing called a
resource picker and we define a real
simple html5 post message protocol to
say you just listen for the they pick
something and here it is and that's the
end of it so everything that's presented
over in this dialogue here is you know
to that tool you're integrating tool to
have to deal with so it makes
integrations real simple and easy to do
because most tools already have those
dialogues you just have to rapper them
with with some of our rules and your
integrate so you know we've seen some
tools be able to integrate with others
using just the Stella da dialogues with
in like half a day so that's a little
bit about what it is and we're going to
link data OSLC there's there's a
a lot more to it I'll get a little bit
more into the link data standardization
and what it is within this section but
just looking at link data in the state
of the art as I mentioned there's a lot
of these data dumps that exist out there
but those all have some limitations
because it say limitations for a true
read/write linked data platform because
they're really all about consuming data
and so really focused on you know
getting it out of certain systems
pushing it to end users who can then do
all kinds of interesting queries report
analysis on it as mentioned there's
there's really no support for updater
create so if you want to update this
data you often have to traverse back to
the system that produced it and Traverse
back to some rules that tell you how to
update it in the backend system in order
to receive updates it's often you have
to throw away what you have and get the
new dump maybe if you're smart you have
some delta patch format where you can
learn about just the new things and then
try to to resolve it and all these
things are often based on the different
deployments out their best practices so
when you try to use them across a number
of clients or you then take your client
want to work with a different server you
often have to do some tweaks because
they're they're quite different between
how those work in there because are
based off best practices often because
of that domain and that data but as
we've seen it's provided some some
really some great value for a number of
use cases
so sort of the evolution of the
standardization of linked data are we
talked about the design principles were
looking to standardize that we could see
it would really help to accelerate some
of this industry adoption we started
this this effort as far as deeding to
standardize based on some of the
alerting we have done within the oslc
community the so we we held a workshop
back in december two thousand eleven we
had a number of participants including
those from from oracle emc nokia etc
that the outcome that workshop and way
w3c does these things is to decide is
there enough interest in the in the
standards world to do some work here no
start a working group give the work to
another working group whatever it might
be so we decided to we should try to
solve this problem create a working
group so we took carved out some of what
we learned in the OSLC core into a
linked data basic profile and we
submitted that back in a March April
last year with a number of cosa meters
there and that was the start of you know
the the work if you will instead of
starting with the blanks back we we use
that as our slate so now if you
fast-forward to today I'm I'm in the
link data platform working group I'm
actually the editor of the specification
too but we're up to 49 participants from
30 different organizations so it's
gotten some good interest as far as
those clicking the I want to participate
button we see participants from the IBM
Oracle fujitsu EMC BBC etc and I wasn't
picking on any one particular by the way
so the whole point of this was to as I
said standardized what linked data is so
it's and if you look with the Charter
actually States and in the way standards
typically work as you're very you know
you're held to what you're charged
says so we're looking for leveraging
what we've learned here and using the
HTTP based restful application
integration patterns for a read/write so
that right is an important aspect link
data a lot of the use cases before or
we're really focused on the the reading
aspect and there's a query query
standard for for rdf based data which is
sort of the data model underneath linked
data that Tim had pointed out in his
third bullet which the link data
platform doesn't require you to have a
full sparkle query engine we're just if
you do have one it you know will align
with it and will often express some of
the the constraints based on spiral
sparkle itself and as we've done within
rational we you know there's a fraction
of our products that support it that do
support sparkle and others a larger
percentage that don't and we're looking
to produce a specification ready for
implementation maybe by the end of year
but likely beginning of next year and
implementation is occurring today so
yeah there's a speck out there it's gone
through last call we're getting ready to
go through another last call for
comments based on feedback specifically
from tim berners-lee so it's good to get
his attention but he's created work for
us so sending my caution here as I was
even trying to update this presentation
and I'm trying to edit the spec they're
both a bit fluid so I'll just go talk
about some of the core concepts and
realize that it probably will change the
whole point of the specification is to
answer some questions about you know
dealing with some of the problems we've
seen around resources on the web so you
know simple what format should I use
their you know especially talking about
RDF formats unfortunately they've over
achieved in formats I believe so rdf XML
was their standard add long-lived and so
it seemed like it was the right one but
we've learned that that has its problems
so they're simple text based one JSON
etc you know what types to use RDF
doesn't mandate any some best practices
around using vocabulary but again
there's choices out there so the one of
the motivating things was to eliminate
choices so that it helps for
interoperability not that choices are
bad but we if you look across HTTP spec
the different vocabularies that are
available you learn that there's a lot
of options that you could support so we
tried to narrow those down for the use
cases and add some clarity when needed
so we look at a more detailed example
and hopefully yeah it looks like it's
coming across okay if you look at the
HTTP request it's just a real simple
normal HTTP stuff nothing special I'm
getting some member I'm accepting some
format called turtle it's a real simple
format for just two defining the sets of
three or defining statements so in the
response to find some prefixes if you're
used to XML nothing too new is this is
the subject position so we're talking
about that member we found over on our
server we learned that it is a this is
shorthand for a type of cash asset has
some so those at the dublin core
vocabulary some title savings account
and then a value of 45 so someone's got
a low balance there in a savings account
so that's that's real simple so so what
is our spec do that helps with this well
not too much as sort of standard HTTP
GET we just say that to minimize choice
is that a server must support text
turtle so clients can expect that and
ask for it and that's what they'll get
so on top of these resources is this
concept of container so it's if you're
familiar with atom publishing protocol
it's similar to that around blog
syndication but it's really narrowed
down to this this concept of you often
have some type of resource you want to
create so it was it mean when you want
to create it where D where do you post
it and once I post it there can I find
it again where should I find it again so
the basic idea is there's a container I
post something to that container it adds
it to the container I do a get on that
container I get the list of things that
are in that container so very very
simple pattern and then you expand on
that saying well I care about order
because I'm giving you everything from
my account you know based on the balance
or my assets based on their their
overall value or something along those
lines or based on a certain date that
they occurred then as I'm getting you
know information about this container
and all its members how do I get some
additional information about these
members but I happen to do you know and
plus one gets across everything because
one of the patterns mentioned earlier
that worked well for this data set dumps
is to do just one get of all the
information not have to do multiple
fetches and then pages and paging is
important and of course update of the
information so showing the sort of the
same example of a resource get but this
is now a container so it follows the
same model a container is just an
extension of a resource itself so again
it's just container one
you poor turtle is going to return back
the subject this is a container saying
hey it's a container very simple
container using some existing vocabulary
from Dublin core and rdf schema the
members number one number two number
three so that's just using relative your
eyes based on this if you were to like
we did in the previous example was day
21 / remember once so now it's real
simple brew v8 format that's how you get
it of course things become a bit more
complicated as the model gets more
complicated so let's look at an example
of create so HTTP I p post typically
means create but if you've done any type
of a rest style programming or even soap
it means transform its execute it means
whatever you want it to mean so that's
the nice thing about post but a lot of
people use it for creation of resources
and so we define when you post to a
container one of the things that it does
is is create resources so I want to
create a great asset so I'm just saying
I'm going to say it's a stock it's got a
name it's got a value i'm posting it to
the container life is turtle so it
looked like normal resource that I just
got except for I'm not giving it a name
yet because the server is going to pick
the name for me and as you can see
standard 201 created member for now
exists
so hopefully it seems pretty simple and
straightforward pattern so yeah just
showing you do it get on the container
and big surprise member force now there
so let's look at a more advanced example
here in my data model have that net
worth that net worth has an owner which
is a person as you would expect your net
worth has a number of assets which can
be different things stock cash bond so
this is a the example I'm going to work
from based on pictures now looking at it
in the text turtle form say a slightly
more complicated example this can anyone
to read that so yeah so it's net worth
one an asset listing out bass line here
i'm using the absolute URLs for easy and
in fact if you typically would see a
network resource it would really look
like that but we're adding some special
link data platform statements at the end
to say well in addition to this net
worth resource there's container
associated with it so give some
description and this is just some extra
data to say well this containers really
linked for this resource and it's
managing the membership through this
property so a little more complicated
but when you're looking at this is what
you know a sort of a end user
application would normally have to look
at the stuff down here is those are
leaked data platform aware and know now
where to look to post assets to to do
creation etc sure
ouch
so the question is whether you know
who's the actual client is it a browser
that's consuming this directly or is it
some application so I would say a
browser of course would have to have
some some application code to handle it
directly so so it would be completely
realistic and some applications are this
way with AD just have a small JavaScript
library that they handle these things
and then consume and produce it so yeah
it would require some application code
on the on some client you know whether
it's within the browser or not the
browser itself doesn't know much about
this format so if you were to just load
it in the browser sort of nice features
you can just look at it because that's
what your browser typically does or tell
you how to look at it so yeah when you
there's browser add-ons that allow you
to manipulate this directly so if you
just use any browser add-on for like
rest debugging so like there's Firefox
rest client chrome has an extension
which is geeky you can do exactly this
and mimic the post command that the
browser application would have done and
mimic the get request one of the
problems you typically have from just a
browser itself if the server does
content negotiation because the the
accept header as we see so here on the
tightly controlling that I want text
turtle but a browser usually just says
star slash star so it says doesn't give
me whatever let the server pick and
oftentimes a server will give you HTML
instead of turtle so
swear was at okay past that
so as I said this is a same example as
before I've got a couple assets but the
problem is I don't want to have to force
my client to go file those each link so
it is possible to include within this
one response additional information so I
know the type of things that these
things are and I know what their values
are so if you will a sneak peek into
those resources without having to
include additional round trips and then
four cases of large containers or
containers with a large number of
members for example all the attendees of
oracle openworld which would be about
60,000 I heard you may just want to get
the information about the container
itself sort of the metadata so just a
showing away where we have to find a
query parameter that you can add on to
the URL the container and then we'll
just give you the metadata about it and
this is one of these you know caution
slides is you know this has changed a
little bit me and the current spec or
will be changing but the same model
applies so as I mentioned the case of
large containers sometimes you may not
want to send back all the information on
one big chunk so we have to find a
mechanism i can page through this
container so it's a real simple define
just like in the normal web cents a page
resource so the page resource has its
own URL saying hey I'm a page I'm
actually a page of this container and if
you want to look find the next page
here's the URL to next page
so simple simple paging in in the sense
of the way these URLs are there they're
opaque the fact that it says p equals
two doesn't matter just what the server
gave me so i just know that for the next
page whatever this is between these
angle brackets is is my next page URL
and just saying well one of the problems
you omitted next page you don't know if
it's so the server just omitted it so we
just have one case a this is the last
page or there are no more pages some
cases orders important so you can then
order your pages so the ordering of
course has to be consistent from page to
page you can't get the second page in
the nola set as a different ordering
criteria so we have some specification
language to to say you should not do
that so we do that by just indicating
that a this page is sorted based on
these values in the model so they didn't
make this point before about what why
this is interesting or valuable is this
model that the the server has and the
clients interacting with about assets
net worth all this stuff and the values
doesn't have to change so we're just
layering on top of it these semantics
for the link data platform you know
there's been discussions win the
workgroup well we should just add some
ordering predicates there to say you
know order and then you just put one
order to and then you know we force you
to write it we've taken as approach that
will you know we don't want to if you
will mess with your model because then
that starts to add additional statements
into your data that don't necessarily
make sense because now is it is that
really a part of the when clients don't
know a whole lot that then left to
decide is this did you really intend
this to be there as part of the model is
at just some additional metadata
so I want to talk a little about some
code various open source projects if you
look across especially Apache and
Eclipse just want to highlight some
number folks are probably more familiar
with Oracle's reference implementation
for jax-rs but anyways I'm familiar more
with linked as IBM did it but that's
that's quite useful for building this
restful style interaction and I'll show
some examples what we're doing there
Apache general project it has a has a
number of aspects to help with dealing
with different formats so it's a parser
for rdf XML json-ld the text turtle I
showed you and also provides a sparkle
query endpoint in in some storage
clarins Istanbul they are you know
various other ones that leverage some of
the similar concepts constructs and some
of those team members are active looking
at our specification or part of the
specification development and looking to
add support there and the last is a
project eclipse leo as we pronounce it
and that's a project that I'm part of
which is really around enabling tools to
integration using OSLC concepts so it
doesn't depend on the Eclipse IDE at all
it's really just a set of toolkits
frameworks reference implementation test
Suites in fact we don't even have a
plugin for Eclipse it's just you know
we're using the Eclipse Leo project
hosting and governance infrastructure to
provide these assets and manage them so
just look in a few code examples if you
know jax-rs already are Jack's be you're
probably quite familiar with some of
this but way we've you know way you
typically interact using Eclipse Leo a
handle various linked data or slc
requests
just saying you know this is getting a
change request or a defect or bug just
saying supporting the get method the
path is providing me with the change
request ID and it produces these formats
and that's the thing that the our
framework provides is a nice hook into
the both a custom serializer for JSON
but also support directly into Apache
Jenna to take care of all this for you
likewise back up second in order to
create resources so you post saying
weakens can consume these will produce
these own results of course you put your
creation magic in the code there some
additional thing that is we call our
library OSLC for Jay it seems like the
way you game java projects i guess and
the there's a concept in a description
document that all SL OSLC has that
describes the capabilities of an
endpoint so it's real nice by just
annotating that's the framework just
takes care of generating that document
for you so you don't even need to worry
about you just have to insert a few
annotations and that they takes care of
it for you so Lily how this maps into
the data so if you really promote Jack's
be this would be similar we try to use
the jacks be annotations to map to the
RDF data model but that fell down in a
couple spots so we decided to provide
our own so you can see you know simply
it's you know there's a status draining
well we're gonna we're going to map to
this property so this is our back to our
rule one which is a URI to name things
so we're naming the status from the
change management specification and then
also from double core we're using this
as a title property so that's the full
URL and then getting into some
capabilities that we have like I
mentioned there's this endpoint
description document there's also a
resource description document so what
does a change request look like it's
called also see resource shapes it's a
nice really nice thing about this
framework is
it's like XML schema but it's you know
handles more than XML so as animals they
are DF and OSLC data model so it just
says it's describing it what a change
request is so that when it produces that
shape document for you automatically you
get that information
so what all of I covered so I think you
see that the you know link data spaces
in some ways mature but in some ways
still active and evolving I think some
of the early days it hit some snags with
being pushed maybe too far and some
interesting research and academic areas
which caused some slow adoption and
commercial side taking this implying
some of these usages and and the restful
architectural principles we can now see
you know how we can leverage us to do
some some interesting loosely coupled
application integrations we've it's been
quite interesting amazing to see you
know some of our customers have maybe
been the one customer described as their
effort to integrate these two tools
together using this approach they you
know the normal plan as they wanted to
go build this plan of what was going to
do testing rollout whatever and he said
it was you know he just put in the URL
you authenticated and it was done he's
like I felt like it should be harder
than this and likewise and when they
rolled upgraded a server from you know
version 2 to version 3 usually normal
major upgrade problems didn't have to do
anything across any other tools the
integration still worked again this is
all still software and there's bugs in
it so I'm sure there were some problems
so as I said we're continue to work hard
on standardization even though we we
have this linkedin a platform 10 there's
a lot of things that we want to do that
we pushed off to our backlog one of the
things that we want to do is define a a
patch model so a couple years ago there
was an extension HTTP to find called
patch so we were work to define a patch
model in a bad patch format because the
normally should be way of getting
something and then you have to change
one little thing and then put back the
entire resource is a bit overhead so it
would be nice just to say change just
one thing or add this one thing or
delete this one thing or three so we're
actively working on on that
stay tuned for various things like data
platform related I do have cards up here
if you're interested in learning more
later you know if you go home and think
about an interesting project or
something you want to kick around feel
free to contact me knows as long as I
don't get abused I'll be happy to help
out any way I can help learn more the
clips leo project we have a reference
implementation for the link data
platform so too early start to it we're
looking to evolve that the one the main
concepts of that is really learning so
providing very small amount of code to
do it so just leveraging either apache
CX f or are patchy wink either jax-rs
framework any would work and apache
jenna we can write a small amount of
java code to be to make you know an ldp
compliance server so so that so that's
sort of the point is it it isn't hard I
mean and it evolves and you can expand
on what's there today in your rest api
to to provide some consistent data
format and some consistent rules around
creation update etc so that's the end of
my material so there's any questions
feel free to ask now yes
here you
so a question is why is there a sparkle
it was completely different from SQL
sequel so the the whole idea is because
the the data you're querying over and
sparkle is actually all graph data so
it's not relational data so in order to
filter the graphs and it's you know it's
a multi Direction network of nodes so
Sparkle language is built specifically
with sequel in mind but in order to
satisfy the different data model as they
or so the general rules don't
necessarily apply I guess in the way you
have to filter out the statements
because if you looked at the pattern of
the it's everything is in the you know a
set of statements so it's subject
predicate object and that's what the
entire database is that sparkles
operating over and so it's all built
around you know where you know matching
the wear on those items selecting the
different topics and then you know
filtering out the results
people
which area
so you're thinking of adopting it in
what it areas you need to to be aware of
so if you go to so if you looked at the
data I didn't go too far in the semantic
web stack and so there's all kinds of
standards as there's RDF which is just a
Dana model and some formats there's
there's beyond that you go down the the
semantic web stack and and that's where
some people have some really hard time
with adoption so if you're it depends on
your scenario and what you're looking to
do so if you go to rdf schema or the web
ontology language those get into a lot
of people have a misconception what
those are used for they look at them for
this is how I describe what my resource
should look like so normal database
schema or XML schema but even though it
says RDF schema that's not what they're
really used for they're used for a world
for for inferring new information so you
define statements such as you know I
have a I have a spouse or wife and then
you can infer from a rule you define a
now that you know my i'm the husband of
my wife but that's different than
describing the shape of the data so
that's area that i would say if you're
going into just may be aware of what
what you're looking into there's a
number of commercial implementations of
sparkle itself Oracle has support for
IBM db2 as a support for it now so there
are in it's quite common i would say
that you can get your hands on
commercial solutions there
opens so open source Sparkle
implementations and others yeah so
Apache Jenna's wimp I the most popular
one there's a another one called sesame
there's virtuoso you probably best go to
the sparkle implementation page off of
w3c they have a number of open source
and commercial products there so number
there's a number of small companies that
actually provide open source and then
commercial varying of their offering i'm
sure i'm missing too many
no other questions like I said if you
want my information feel free to to
contact me I work actively in the OSL
Sikh community within the w3c linked
data community and also we're at the
point in oslc where we're moving from a
sort of a self-defined governance
community model to an
industry-recognized one at oasis and so
we have an oasis member section for OSLC
today and we're looking at starting the
core technical community Technical
Committee there so those who are alesis
members or one may want to become Oasis
members and want to participate can do
that you can also just join the oslc
community participate in forums user
groups etc without joining Oasis any
other questions well thank you very much
for being the select few that came out
this morning in the early morning and
appreciate your time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>