<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Building a Continuous Delivery Pipeline with Gradle and Jenkins | Coder Coacher - Coaching Coders</title><meta content="Building a Continuous Delivery Pipeline with Gradle and Jenkins - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Building a Continuous Delivery Pipeline with Gradle and Jenkins</b></h2><h5 class="post__date">2015-06-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/gG4k3I7jJvc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so it's 5:30 I think we should probably
get started I'm actually going to step
down up the podium I'm kind of afraid
that I'm falling down here but what I
want to talk about today is how to build
a continuous delivery pipeline with
cradle and Jenkins my name is Ben Musco
I work as a engineer on the cradle wear
engineering team Gradle where is the
company behind cradle the build tool and
we employ all the engineers actually
work on the open source project Gradle
but we also provide professional
services to clients so we go directly to
their office help them out with
migrations from other build tools to
cradle oftentimes this also involves any
kind of like project automation approach
like continuous integration continuous
delivery typical DevOps topics like how
to set up a VM so we certainly cater
towards the full space here so don't be
afraid if you don't know too much about
Gradle or Jenkins I will touch on both
of these tools at some point of time in
the slides as well as on continues
delivery so if if you don't know
anything about continuous delivery
that's fine we'll actually talk about
continuous delivery in the first couple
of slides just to give you a broad
overview of what that actually is so
before we actually get into continuous
delivery let's actually have a look at
how releasing software into production
looks for probably many of you guys and
so it looks for me in the past so
usually I used to work on a feature
either as part of a team or as a single
developer used to work on something I
know what needs to be implemented as a
functional requirement would commit my
code and at some point of time I would
say hey that looks great functional
requirements are met so please hand it
over to the QA team we would deploy it
to some sort of environment some testing
environment and they would go ahead and
would say hey this looks great or they
would say functionality is broken so go
back and forth between development and
to a
and at some part of time you get to the
point where you say hey now we're ready
to actually release that to production
so that's kind of like the endgame
that's what you want to do you want to
deliver the feature to the end customer
to the business you want to get it out
there at least for me in the past it
went like that
I used to document each of the steps
that are necessary to actually deploy my
product into production sometimes you
need to update the database as part of
the deployment process or you want to
keep a record of your log files on
production these kinds of things are
usually involved but I never actually
exercised the deployment process so that
was totally manual so when it came to
the time that we actually said hey let's
deploy this to production we actually
reserved a weekend usually because we
don't want to have any downtime and we
already told our families it might
actually something might actually go
wrong so don't expect us back for dinner
from bad experiences you know what this
happened so sometimes we have to
actually rollback releases because we
didn't actually exercise that code that
deploys to production and we ended up
eating a lot of pizza in the office not
with our families and drink a lot of
coke and coffee but there's actually a
better way of deploying yourself into
production there's an automated way to
do this kind of thing and you optimally
want to do that with a push of a button
so you want to actually enable your
business to say at some point of time
hey we delivered this feature so it's up
to you when you actually want to release
this to the end customer so when we talk
about these push button releases
oftentimes continuous delivery is
mentioned and continuous delivery became
really popular with the book continuous
delivery by Jess humble I think it was
published in 2010 and then all the
sudden everybody was talking about
continuous delivery everybody wanted to
do it but what actually is continuous
delivery so continuous delivery is a
software development practice that
focuses on automating and improving the
process of delivering your software and
in short this means you want to deliver
your software fast
and frequently to the end customer to
your business that's pretty much the end
goal that's what you want to achieve and
should be painless I should do it with a
push of a button so how do we actually
implement continuous delivery so in
continuous delivery if you read that
book you will see there's the notion of
a so-called built pipeline and the built
pipeline basically defines each of these
steps that are required to get your
software compiled get it packaged get it
deployed and of course tested at some
point of time and of course delivered to
the end client and you can see in this
build pipeline there are individual
steps so each of these steps basically
represent one of these steps and a built
pipeline is responsible for
orchestrating each of these steps so
there's a certain order that we want to
enforce and think of the build pipeline
as a manifestation of the whole delivery
process getting the software from the
developers machine into production and
to end so now one of the principles in
continuous delivery is that every commit
can become a potential release so
whenever you commit a code to your
version control system that basically
kicks off the first step in your build
pipeline and in the end that could
actually mean it goes into production
now if you talk to your manager you
might say that sounds actually pretty
scary we don't trust these developers
something always goes wrong that's why
you actually have two highly rely on a
good suite of tests but only just unit
tests but also functional and
integration tests there should be run
automated and on top on top of that you
actually also want to have manual
testing as you used to by the QA team so
certain quality has to be built into
your build pipeline and that also means
that if certain metrics are not met and
that could for example be well my code
doesn't compile or my code coverage
doesn't cover at least 30% of my code or
the functional requirements are not met
then please don't move on in the next
step in that built pipeline so you want
to establish so-called quality
the gates in between each of these steps
to make sure your software has a
specific quality that you actually
expect and before that you actually
don't want to push it out to production
because it doesn't make any sense to
deliver software that does not have the
necessary quality
so let's actually have a look at an
example pipeline here and this pipeline
just is think of it as a java
application some web-based application
could be anything else
so the first prerequisite and I
mentioned this before is that you
actually have your source code in
version control now it could be anything
from git subversion CVS doesn't really
matter the technology doesn't matter
so whatever tool you use that's good
whenever you change the code and check
it back into version control that kicks
off the first step in such a build
pipeline and in this pipeline the first
step is compiling your code running your
unit tests so in job application that's
a common thing to do compiling the code
of course you need to create the class
files before you can actually create
some sort of War file you file these
kinds of things and you also want to
have a first feeling about whether
testing your code looks good so unit
tests should run pretty quickly they run
in isolation not as integration tests so
the feedback cycle here should be just a
couple of minutes optimally to three
minutes or so so you know whenever you
check code into version control
something's wrong or it looks good
something's wrong you don't even move on
in that bill pipeline you don't move on
to the next step
that's usually the responsibility of the
whole team that's kind of like a
principle of continuous delivery the
whole team is responsible for making
sure that the build pipeline looks good
quality gates are met and if something's
wrong the whole team team is responsible
for fixing it so it's not just one
person where you say but he broke the
build it's actually the whole team so
it's kind of a change in philosophy so
once that's good you can actually move
on to the next step in the build
pipeline in this case it's integration
testing
they usually integrate some other
external system some other components or
they're usually
longer running tests and the unit tests
so you want to make sure that even on
the integration level that looks good
and then you can move on to some sort of
code analysis tools so I would expect
that most of you guys are Java
developers so you might be familiar with
tools like PMD check style
fine box these kinds of things so that's
optimally integrated here and you can of
course then and for certain rules again
so you can say well if find bugs finds
at least 30 errors then move up don't
move on in our build pipeline because
that's not the quality we want the
metrics that you actually pick for your
project is up to you um these are your
own quality metrics and over time you
might find the perfect match what you
want to get to once you meet that we're
kind of ready to package our artifact
and web applications are Java that might
be a war file and you file these kind of
things and then we're kind of ready to
push it to some sort of testing or
Canaria environment so we want to know
that once it's packaged we can actually
also deploy it to a specific environment
and when we deploy it there's also no
error after we deploy it of course you
also want to run some automated
functional tests so that should be part
of your test suite and if you're
thinking about java web applications
that's oftentimes something like
selenium or if you're using groovy it
might be jeb these kinds of things but
anything you can think of to test the
functionality from the end users
perspective would be met here and that's
all happening before the QA team even
comes in so you want to make sure that
you take the load of your QA team they
shouldn't even test any kind of like
functionality before these quality gates
are met so once that's good we can
actually deploy it to an environment
that QA has access to user acceptance
test environment so you can say with a
push of a button I want to make sure
that we deliver the package to the QA
team so they can run their manual tests
and one thing that's actually important
here is that the artifact that you
deploy here is
same artifact that you deployed to
testing environments so you don't
necessarily want to recreate that
artifact
whenever you already created it you
actually want to reuse the existing one
and why is that because you don't want
to introduce additional problems in your
build pipeline so you want to build it
once you might ask yourself well I have
a specific configuration that I need in
my environment let's say a database
connection string these kinds of things
so we usually recommend to externalize
that don't package it as part of your
archive actually use infrastructure
tools like popular chef to automate
these kinds of things they are
responsible for syncing that with that
specific environment and of course this
configuration can be version as well
once we have it in you achieve QA comes
in at some point of time they say it's
good and then we can actually push it
out to production the logic that we run
to deploy this code also should be the
same so the only thing that should
change is again the configuration so you
say if I want to play if I want to
deploy to production that uses a
different server URL different
credentials then for the UAT environment
for example but the logic should be the
same and you can see when you do this
over and over again you exercise the
same deployment logic as you do when you
deploy to production so certainly
helpful that if you get to deploy into
production you did that like probably
500 times before so you know that logic
is good so you can actually rely on
tests at an exercise code here so well I
told you about the book continuous
delivery you read the book because I
said it's good and everybody talks about
it and then you might ask yourself but
what about the technical aspect of
continuous delivery how do I actually
implement such a build pipeline what
kind of technology is what kind of tools
can I use to have to use a build tool
can this be maven Gradle and or what
kind of continuous integration server do
I use so today we'll actually talk about
Gradle and Jenkins this combination
cradle has certain benefits and I will
talk about them on the next slide but
you can basically implement such a build
pipeline with the help of other build
tools and from my perspective if you
just run into different challenges and
if you're used to maven you might know
what that means so if it comes to really
complex custom logic especially when it
comes to deployment and your deployment
process might look very complex then
it's kind of like hard to implement that
in maven you actually have to write a
plug-in it's cumbersome and so on and
it's much easier in Gradle and Jenkins I
picked because it's an open source
continuous integration server everybody
can get started with it and it's easy to
show you guys how to model such a build
pipeline with the right plugins so why
did I pick Gradle well of course I
picked Gradle because I work for Gradle
where that's only one aspect but if you
never touch Gradle before I can give you
just some some criterias here so Gradle
is fairly flexible when it comes to
implementing custom logic so that can be
done in a very nice declarative way you
can write your own rules and again you
can write plugins as if it comes to
custom logic but this logic is far
easier to be bundled as a plugin to be
implemented in fact in Gradle you just
have to implement a single interface and
then you have access to the whole model
of cradle to be exposed over an API so
you can query for certain information
that you need at runtime or at build
time in this case another benefit of
using Gradle is that you can build
different languages with it so in maven
you can only build Java applications as
far as I know but if it comes to also
building a polyglot kind of project you
want to have a one-stop approach so you
want to be able to build JavaScript you
might want to integrate with some sort
of legacy application that's built in C
C++ and Gradle gives you the option to
build all these kinds of things
another nice side effect of Gradle is
that performance is pretty good so
certain features like incremental build
functionality parallel build kicks in
here so that's certainly helpful when
you build such a build pipeline because
oftentimes it comes down
two custom logic so let's have a look at
each of these stages here and I will
talk about each of these stages in my
example build pipeline so let's get
started with compiling the code and
running unit tests and we'll talk about
how to implement that
what's Gradle so if you have a java
project it's fairly simple the only
thing you have to do is to apply the
Java plugin the java plug-in gives you
all the capabilities out-of-the-box to
compile your code it can point it to a
specific directory or multiple that
hosts your code your source code it
allows you to create the artifact that
you need might be a war file jar file
these kinds of things you can
automatically run unit tests so that's a
built-in capability and you can create
Java Docs that's something that the Java
plugin gives you out of the box so
certainly helpful this is not different
from the maven built in functionality we
have some goodies in there that will
drill into a little bit later but this
first step should be a fairly simple
fast feedback and this kind of aligns
with agile methodologies as well so you
want to know how as soon as possible or
something's wrong so you can implement
that with a first step I mentioned
before this happens whenever you when a
developer checks in code so this first
step is kicked off and of course
priority should be you fix that broken
build that's kind of like the notion in
continuous delivery that you as a team
make sure that build pipeline is green
so fairly simple to implement this
optimally if you stick to the
conventions to be exposed for the Java
plug-in you don't actually have to add a
lot of code you really just have to say
apply the plug-in Java and that gives
you all the capabilities that I already
talked about if you want to adapt to
other project layouts is fairly simple
so the Java plugin gives you the
capability to actually configure these
it doesn't box you in as maven are so
very flexible here so if your project
doesn't look like our conventions that
we think are sensible defaults no
problem you simply reconfigure them
when we look at integration testing it
oftentimes gets a little bit more
complex because you don't use a mock
framework oftentimes or you might
actually make database calls so you call
off to external systems and these tests
usually run a bit longer than the unit
tests just querying a database and
testing the database access layer these
kinds of things can take a longer time
they might actually also require a
certain environment set up so if you
occur the database you need to make sure
that there is a database for them and of
course it has to have the right data all
of these things can be integrated into
the build process so you could for
example bring up a embedded
implementation of a web server which
hosts your application as part of this
build process you can run your test
against it and shut it down afterwards
so that's actually fairly simple to
implement in Gradle the same thing with
databases and setting up the correct
data that you need for testing you can
make this all part of the build process
one more thing to keep in mind
integration integration tests are
usually harder to maintain a little bit
more error-prone I would say because
they don't run in isolation so if your
database is down for whatever reason
well that will fail the build
so let's actually have a look at a
sample project structure that we might
have in Gradle and this might look
fairly similar to if you if you would
set it up in maven so we have three
different sub projects here let's say
these are called model repository and
web and each of them have a source
directory and a test directory which
hosts your either your production Java
source code which needs to be compiled
and bundled into a war file as well as
the unit test Java sources which sit
under source tests Java all these
directories come with a Java plugin they
are conventions but again you can
reconfigure them so no problem here one
thing I like to do personally is if I
have a different type of test in my
project like integration tests I
actually want to keep them separately I
want to separate them physically into a
different subdirectory
and you can model this with gradle
pretty easily I'm not sure if you can do
this in maven easily I'm not sure if you
have any kind of like maven experts I
might be wrong but it could become
pretty hard to do so let's say we put
our integration test Java sources under
source intact test Java
that's where Gradle should look for
these specific test files that should be
of course compiled again they need to
have the right class path to run your
tests so if you use some sort of
embedded implementation for web server
of course you need to add that as some
sort of dependency into the class path
so all of this kind of stuff can be
modeled in Gradle so let's have a look
at an example build script here that are
implemented in Gradle the Java plugin
gives you a certain concept out of the
box and that concept is called a source
set and a source set lets you define
different directories for specific tests
for example in this case I called it
integration test so that's the name of
my source set so when I write
integration tests here that's actually
the name that I give my source set which
are then later on can use to query
certain information so let's say let's
assume we set this up and we pointed to
the specific directory here so we see up
there source in tech test Java and
source in tech tests resources host some
property files XML files that I might
need for for integration testing sounds
fairly familiar for the maven guys so
that's pretty similar and we also want
to set up the compile and run time class
path so we can simply say we should rely
on the compiled classes so the output of
the main source set which are the
classes and of what when I add certain
test capabilities as well so if I for
example usage a unit for my unit tests I
also want to use this framework for my
integration tests of course this is not
a given necessarily you can configure
different things but that's how I set it
up in my example here so once I do that
what Gradle does out-of-the-box it
creates
a compiled task and a task is a as a
unit of work so it lets you do something
if you compare that to and maven an end
we have a target that would be kind of
like the same similar thing or it may
even it would be a a goal so here we
simply say we define a source said
declaratively we didn't actually write
much code there's actually declarative
code here but what the Java plug-in does
out of the box it gives you the compiled
task with which derives all the required
information from the source set so you
don't actually have to write that as you
would have to do that in ant fairly
simple because usually you tend to
copy-paste this kind of code we want you
not to do these kind of things another
thing that we do here is that we set up
a new task and a task is called
integration test and again the task is a
unit of work so in this case the task
has a specific type and the type is
called test here test is a class from
the Gradle API which lets you define how
to execute certain types of tests the
same type of test is used for j-unit and
it's already pre-configured for you by
the Java plugin but because we want to
add a new task that we can run
individually we want to set up our own
task here so we simply say here it
should point to a specific class
directory so test classes here points to
the output of the source set that we had
created before so we can actually use
these source sets as we would use
specific class instances and then use
methods here to query certain
information and refer to it and we can
say if you run integration tests we need
to set the class path which also comes
from the source said these kinds of
things and we can point it to specific
output directories so this is kind of
like how you want this to look like in
your own project you'll necessarily have
to do all this kind of stuff only what
you of course need but this is one
example how to do these kinds of things
what you need to do then to kick off
integration tests you simply say Gradle
W in this case think of it as the Gradle
runtime
that comes with a great distribution
similar to when you run ant or maven so
simply we say here Gradle and run the
task with the name integration test
that's exactly what we defined here and
that will basically run our integration
tests independent from the unit tests so
what happens under the covers it will
analyze the source set because we tap
depend on it in the integration test
task it will kick off the compilation
task which compiles the code for
integration tests and then it adds the
right things to the classpath so fairly
simple in the context of running tests
you're always oftentimes think about
code coverage maybe you think about it
yourself or your manager told you well
you have to meet a certain number of
code coverage I'm certainly not
dictating here any kind of like a number
because from my perspective tests are
only as good as you write your code so
if you make the wrong assertions in your
code you can still have 100% test
coverage but it doesn't actually mean
much you ran through the code but you
didn't make the right decisions so if
the return type or the value that
returned and that is returned from a
from a method doesn't meet your
assertions then you actually didn't test
anything so but I want to give you one
example how you can generate code
coverage with the help of Gradle and one
of these tools here is to cocoa you
might know some other tools like clover
is a commercial tool or cobertura they
use different approaches to actually
generate the code coverage
some of them instrument the source code
some of them instrument the bytecode but
from my perspective it's a bad approach
similarly simply because we don't
actually want to change the already
compiled classes in any kind of form
because we actually want to deliver that
to production so we don't want to
introduce necessarily problems here and
also changing the source code sounds
kind of weird
so what jacoco does here it does on the
fly bytecode instrumentation and how
does it do it it adds a JVM agent when
you run your integration tests
these kinds of things and will simply do
this on the fly at runtime so it doesn't
actually change any source code or any
byte code so a pretty capable tool here
it generates code coverage either as
binary files or as HTML files that you
can later on use to report on so let's
have a look at how to implement that in
Gradle and of course I guess I'm
repeating myself here
of course there's a plugin for it it's
called each a cocoa plugin it's part of
the Gradle distribution so if you
download Gradle and install it you
already have the plug-in out of the box
there's nothing extra to be installed
the only thing you have to say in my
plug-in applied this plug-in so I want
to use jacoco and what that does is
again it uses certain conventions so it
sees that if you apply the Java plug-in
it already knows there's a suite of unit
tests so please add certain tasks
out-of-the-box which lets you generate
these code coverage with the help of
jacoco so you don't actually have to
know how to add a JVM agent to the
process and these kinds of things when
you run unit tests these are things that
the plugin implements for you in our
case here in the example we actually had
some custom logic so we say well we want
to have integration tests as well
separate from unit tests and for that we
actually have to set up something else
some more custom logic to tell Gradle
that we want to separate the metrics for
unit tests as well as integration tests
and for that we simply create a new task
again and that task has a specific type
that is provided by the plugin so again
that's an API class that is exposed and
you can look these things up in the
Gradle documentation for this plug-in of
course that will tell you all about it
and then we simply have to say we
pointed to the specific source set that
we want to test on and there would be
again your production source code you
actually want to see which branches and
which classes actually are covered and
then we say please use the execution
data that comes from the tasks
integration test and determine the write
dependencies that are required to
generate these code coverage metrics so
fairly simple
and then again you just have to say I
believe jacoco report in this case is
the name of the default task that you
need to run for your unit tests and keep
in mind they're executed as part of your
test run so if you run the task for the
unit tests which is called the default
test it will also generate the jacoco
coverage metrics again you can yep okay
yep okay
it improves over time what I would
recommend doing here I would apply the
plugin to all sub projects and your
multi project build this is probably
something you did so you basically
generate code coverage for each of your
sub projects but what you want to have
in the end is one big report right so
you want to merge it potentially
everything together you want to have
like one view on it and you can do this
with the help of another API class that
comes with the jacoco plugin it's called
jacoco merge so you can put it all
together if you want to if you have
specific issues I would say feel free to
come by the booth we have a booth down
there in the exhibition hall so we can
talk about your code maybe you have it
with you and I can have a quick look so
we would be happy to help you
so also let's talk about code analysis
here I already mentioned the tools check
style find box PMD of course for all of
these tools there's a corresponding
Gradle plugin again comes with the
Gradle distribution so nothing to be
actually installed or to be done we
simply have to tell our build script we
want to use the plug-in for PMD for
example and again we can fail here if
the quality is not met that we actually
expect here that's part of the build
pipeline that we might want to build in
and something that is helpful usually
especially for management that whenever
record the changes over time so they
want to know how did the metrics
actually change from March up to now did
we improve on code coverage
the prefix more bugs that find bugs
finds yes please
yes okay so I will first talk about the
so the question is can I talk about how
to run these tools individually in the
build pipeline as well as using sonar to
aggregate these these metrics so we'll
talk about Sona and one of the following
slides and I will tell you how to
configure that in Gradle as well
and I will talk about individual tools
and what needs to be done to actually
run them and I will also talk about the
pros and cons so that's probably helpful
if you want to determine which tool
should I actually use should I just use
Sona sort of just use individual tools
and when would I run something from the
continuous delivery pipeline so I'll
talk about all of these yeah yep
I think I would start with a very low
bar and then work your way up I mean you
have to start somewhere
same thing if you don't have any unit
tests you're potentially might not be
set up correctly for continuous delivery
because it highly depends on automated
testing but you want to of course build
up so you want to not necessarily fail
the build right away because you know
there's no code coverage so well you
don't want to block your build pipeline
because well you know there's no code
coverage so I would say like set
yourself self some goals as a team and
then you would you could potentially say
well until March we want to have at
least 10% code coverage and then you
work and work your way up and from there
on you can basically change these
metrics you can change these thresholds
as you go so let's talk about individual
tools first so I talked about all these
tools here PMP fine box check style this
is kind of how it looks like in the
correct in the Gradle build script so
you can simply apply the correct plugin
for example the PMD plugin were the JD
Pen plugin is just another one of these
code metrics tools and then of course
you can reconfigure them to meet your
needs as well
so you can say if there are any failures
that PMD spits out then please don't
fail the build we know this is happening
we want to deal with it later we
actually want to have a look at the
report instead and we can generate
specific report types so we can say hey
we want to report as as HTML or XML so
we can actually use these XML reports to
later on be consumed by Jenkins for
example Jenkins has all these nice
plugins to show a graph a trend graph
how things change over time as well and
you can use these XML reports generated
by the plugins in Gradle to actually
render in this graph yep
so in Gradle keep in mind you're
actually using a programming language so
you write your build script in groovy so
anything that you can think of in terms
of conditional logic can be implemented
in your script so you can basically wrap
this into a if/else block or of course
make this as part of a plugin because
the more complex certain logic becomes
you want to don't necessarily want to
put it all into one built up Gradle file
actually want to externalize that
potentially tested these kinds of things
so that's kind of like how it would
approach it have one specific plug-in
that might be specific to your project
which has this kind of logic the only
thing you need to do to actually run all
verification tasks that are added by
these plugins is to run the task check
so that's a default task that is added
by the Java plugin that you have at your
hand and all these plugins once you
apply them register with that specific
task here so you can run all your
verification with the help of check or
individual ones so I mentioned so now
before you mentioned somehow before so
of course I will talk about it as well
so for anyone that doesn't know what
sonar is so Solara is a web-based
application that lets you render your
metrics over time so pretty convenient
it's not a commercial product I'm not
sure whether they made it commercialized
at some point of time you can use it as
open source tool at the moment I believe
but what it does instead of just
creating reports and which you can throw
away or store over time it actually has
a dedicated database and that database
is being used to write these metrics so
you can say I ran my built on Wednesday
and we had these kinds of code coverage
metrics that I can push into this
database and we can render these as
graphs in the web-based UI that comes
with the SONA so pretty convenient
especially when you can want to compare
if something changed over time from the
Gradle side you just have to apply a
specific plugin it's called the asana
runner plugin
which basically analyzes your Gradle
project each of your sub projects and of
course there's a configurable again if
you don't want cup any kind of like
analysis for a single sub module you can
define this you can configure the plugin
and then when it is done it actually
pushes these metrics into the Sona
database as well so this is kind of like
the central place where you keep all
your metrics over time how do you do
that in in Gradle I mentioned there's a
plugin it's called the zone or run a
plugin hopefully you can read the slide
that the green doesn't really come out
here there are a couple of things that
you need need to configure which the
plugin can actually know about we have
to tell it about what database to use so
Sona is completely configurable so you
can use Oracle my sequel what-have-you
h2 that's computable but you have to
tell Sona and the build process of
course which database target and of
course part of this is to define the
username and the password
so the credentials keep in mind view
should they actually not keep the
credentials within your build script and
check it into version control this
should be externalized you can put it in
some sort of property file locally you
can encrypt it so you have all
flexibility here that you want it's kind
of like up to you yes
when you say dependency do you mean
external libraries okay so the Sona
plugin the Sona plugin already defines
the required dependencies on sonar
internally in the plugin so if you need
anything for Sona it would be pulled in
automatically by the plugin but if you
need any kind of other dependency let's
say hibernate spring that's kind of
independent of this specific plugin
that's a building capability of cradle
okay yeah come over to me once the talk
is done we can go into details how to
configure it yep there's more you can
configure so you can also say for
certain sub project through these things
or other things so it's fully
configurable here and then the only
thing you need to do is to run some on
our runner that's a task that needs to
be run to generate these code metrics
the same task can be run from Jenkins by
the way so if you at some point of time
model that specific job that represents
your Sona tasks you would basically just
call off to that specific Radle tasks
that you have in your build it will push
out to the Sona database which you then
can create centrally so pretty
convenient
hopefully that explained all your
questions you had before about sonar PMD
and so on good awesome
so once you have all these metrics is in
place you you have basic code coverage
you ran your vs as long as you have any
Java library that would provide that for
you or any kind of external tool that
you can call could even be on the shell
on the command line it doesn't matter
you can model that as a task in Gradle
and can call off to your specific tool
that creates these metrics for you and
make it part of the check task basically
if you wanted to exactly yep and then
you can basically use these metrics to
generate some HTML reports maybe it
already comes for the tool but you can
render these kinds of reports in Jenkins
as well or whatever continuous
integration server you're using so
that's kind of like independent so we
created all these tasks actually we
didn't have to create many tests that's
all done by plugins mostly to run unit
tests integration tests but we are kind
of ready now to create our war file or a
file depending on what it is there are
plugins for that again if you need to
create a war file there's a war plugin
if you need to create an ear it is an
ear plugin it already knows how to
assemble these kinds of things based on
declarative
rules so you don't actually have to
write much yourself again this is
customizable if you want to have
something extra in your war file and it
kind of properly filed that you want to
add at Build time you can customize that
again as well something I personally
like to do is also add built information
so for example if you talk about the
version that you're building currently
were who build it what OS was used to
build it what's the current timestamp
these kinds of things can be included in
the war file or the artifact in general
that you can generate I will show you
one example pretty soon as well and of
course once you want to create an
artifact you have to make sure that you
give it a specific version because you
want to identify later on when this was
built with what specific version and how
it changed over time usually these
artifacts are stored in binary
repositories that's the best way to
store them of course you could put them
into some share file share drive if you
wanted to but these products usually
like artifactory or Sona type Nexus
they're targeted towards organizing
these artifacts for you keeping them
over time adding security on top of that
currying better data these kinds of
things are added by these kinds of
products and I would highly recommend
using them so when we talk about
versioning and the versioning strategy
you might have to ask yourself what
version does my project even half and
how do i define it so if you're coming
out of the maven world you might be
familiar with the snapshot version
identifier that's something that maven
it's basically maven concept that
basically identifies that your current
code is under development still it's not
ready for release and what they do is
they basically add this identifier to
the version that you have that you
produce with your build and whenever you
create your artifact let's say AB or
file that basically adds the full
version here to your artifact name as
well when we think about it again
in the continuous delivery kind of a
side of things we want to think about it
slightly differently because as I
mentioned before every commit can become
a potential release and that could mean
that whenever you commit your code it's
not under development anymore in that
sense it's actually ready to be released
to production potentially if you say hey
business this looks ready please push it
out doesn't mean that you actually have
to release it but you want to enable
yourself to release it and therefore the
notion of a snapshot is not necessarily
needed you can actually have a full
version like a final version for your
artifact and how do you put something
like that together if you don't need a
snapshot identifier here you can say I
have a minor version a version a major
version here and I have something that
clearly identifies my current build and
that could be it the Jenkins build
number for example it could be the
current timestamp it could be the commit
hash from your version control system
keep in mind if you need this kind of
information it's easy to be done you
just need to integrate with some library
that that you can use to query this
information from your version control
system and then you can basically add
that to your version but how do you put
together that specific version here that
specific version can be put together by
representing it as a specific class so
let's say we represent the version as a
as a plain potro here so we see we say
class project version and it has three
different fields here major minor and
some sort of built number looks fairly
simple if we create the string
representation of it it puts together
the specific semantic versioning that we
need here so we simply say major dot
minor and then we add a build number it
was provided to to us and then again we
can say whenever the project is run yes
yeah
it would have the same version number if
so if you have three commits that you
push out together let's say as a commit
come get commit commit push then it
would have the same version number for
all of these three commits
does that answer your question or okay
they would have individual version
numbers for each commit yeah so you have
to make sure that when you generate this
version the project version it has a
unique identifier it has to be unique so
that's something that you have to ensure
and it kind of depends on how you want
your version to look like and again this
could be the time stamp for example
which is fairly unique and what we do
here to actually assign the value of our
project version is we simply say version
equals the instance of the project
version which has certain values here so
major version is one minor version is
zero and we take the environment
variable that is provided to us by
Jenkins
that's a built in functionality by
Jenkins but again you could provide
anything that you can think of the time
stamp what-have-you but that's a fairly
easy way to define a custom versioning
strategy in Gradle if you wanted to do
this in maven I don't even know how you
would probably do it I guess you would
have to write some plug-in and an ant
you would probably have to write some
sort of macro which defines these kinds
of things as well one more thing
the version property there is a built-in
property in a Gradle model for software
every project you can define a version
and this version also defines the
identifier that is added when you create
the artifact so if your artifact is
called my my web application it adds
another identifier which is your project
version in this case I mentioned before
that you can customize how you build
your artifact and this is built-in
capability here again so in this case we
apply the wall plugin and then we say
hey we want to have additional built
information about the time when we built
this project about the version
build-it these kinds of things and we
simply say we put it into this property
file so the property file basically has
key value pairs which has all this kind
of information so fairly simple we
create a task to generate this file and
then later on we actually include that
into our war file so not only can you
use the files that are bundled
automatically for us by the what plugin
we can actually add other things as well
and I think that's fairly declarative
here so we simply say from the built
directory which has our built file build
properties file simply include this
build info properties and put it into a
specific directory so pretty declarative
even if you never touch Gradle before
the only thing you have to do then in
the end you have to run the task
assemble again that's part of the
default build lifecycle of the Java
plugin and that takes care of actually
assembling the specific artifact the
nature of the project if this is a bore
project which is identified by the bore
plugin if you apply the plugin it will
create a war file if you apply the gear
plugin it creates in your file these
kind of things yep
yep
yep mm-hmm
you don't necessarily have to do this in
continuous delivery because everything
began can become a potential release
that's kind of like the basic idea keep
in mind that radio doesn't really care
what you sign here so you can still use
snapchat if you want to
I mean Gradle doesn't box you in any
kind of form you can still use snapshot
many people do many organizations
actually still use the notion of a in
development version of your artifact so
you can simply assign tops dot snapshot
in this case you can do this no big deal
mm-hm
yep
yeah keep in mind you can add that you
can also add additional information for
example in the artifact itself if you
want to or make it part of the whole
versioning the naming strategy that you
have so you can say that's - dev like
you add some sort of classifier that you
might know from maven so you have like
all options here if you really need to
find out if you want to still use that
model that maven has you can so nothing
really prevents you from doing that once
we created the artifact we also want to
publish it so if you come from the maven
world that might sound very familiar so
you either want to publish it to a local
repository or something that is remote
so it's hosted somewhere where you keep
all the versions that you published over
time and again if you have a snapshot
version
you couldn't potentially deploy that or
publish that to a snapshot repository so
you can have even separate repositories
if you need to that's also one way to
distinguish whether there was a final
release or snapshot release so that's
another way putting it into a different
repo you can do the same thing in Gradle
as well and keep in mind that you only
want to publish it once and then reuse
it when you deploy it later on so you
can publish it to let's say artifactory
and then pull it later on when you
deploy your code so that's usually how
it looks like so you have some sort of
server which hosts your binaries I'm
just putting here artifactory as an
example if you want to learn more about
the tools have a look at the j4r guys
what they do also sono type nexus is one
of these products won't actually give
you a recommendation I have a personal
preference but I mean don't we have to
say it here but keep in mind you could
have multiple versions of your own
artifact here they are published at some
point of time maybe last week but you
can still get back to them pretty
helpful if you later on want to rollback
a specific version these kinds of things
you have them over time organized in a
specific kind of way this is how it
looks like in Gradle with the help of
the
maven published plugin so you can say
when I publish my artifact it should be
the component that I created with this
project which I could reference by a
specific domain model here so we say
it's a maven publication so I actually
want to push it out to a maven repo and
it should be a web application artifact
so in this case a war file so that's
kind of declarative here web app is just
a name did I give it so you can actually
have multiple publications from one
project that's also one main difference
between maven and cradle you can
actually have multiple publications in
artifacts that you can create in a
single project and then we have to say
where do I actually want to publish it
to so you have to define the
repositories that I want to publish to
so URL credentials and you're golden
then the only thing you have to do is to
say our Gradle W publish and that brings
it up transfers to file or D files
correctly to your repo yep
yep
yep
you could use the same conventions if
you wanted to so keep in mind that
Gradle is fully compatible with maven
repository so it knows how to resolve
the dependencies from a maven repo as
well as publishing to them so that's one
to one compatibility there so that's
that's a given you can publish with
maven let's say you have like some some
module which is built what maven it
publishes it to to a maven repo and then
you can consume it in Gradle so it knows
the same kind of things how to declare a
specific identifier for a dependency so
it has a group ID it has a name some
artefact ID in a version of course so
the same concepts apply here if you want
to refer to a snapshot version that
simply the identifier you use to resolve
it so you say the version is 1.0 -
snapshot in Gradle if it's built by
maven and published that's fine if it's
a final version of course the snapshot
identifier would not be there anymore it
will pull it down from the maven repo as
well it also creates the palm on flight
that actually is something that this
plugin does so whenever you publish of
an artifact to a maven repository it
also creates the metadata required by
that repository so in this case is if
it's a maven repository
it creates the pom.xml on the fly based
on the dependencies that we have in our
project so if you declare a dependency
on comments Lang it will be added as a
dependency block to the published pom
dot XML file
it should work the same kind of way we
can actually drill into your project if
you want to come by the booth might be
very specific to your use case and then
we can have a look at it but the same
stuff should apply yep yep so the so the
may be published plugin is a what we
call incubating feature or plugin that
means it's not fully done that
potentially could mean that we changed
the DSL so whatever you see here we
might rename publishing to something
else that means you're aware of it it's
an integrating feature if it changes you
have to change a project as well so we
still reserve the right to change that
specific DSL or maybe slightly how it
works under the covers the other plug-in
actually calls off to end tasks that
publish to maven that's in the final
version so it will not change anymore
at some point of time when the features
are the same between the maven publish
plug-in and the one you're already using
then we basically make this not
incubating anymore than it will be final
so just keep in mind if you want to be
prepared for the future you can already
use this but it's not up to par with the
same feature set so if you're ok with
what it provides at the moment you can
use this so let's talk about deployment
and how to get the artifact of course we
know we have it in the binary repo let's
say it's an artifact right now and now
we actually want to deploy it to a
specific environment so we have to make
sure that we actually get that artifact
from there when we think about
deployment it could actually happen on a
different machine so you don't
necessarily have the artifact anymore
that was previously built by Gradle or
maven or something else so you have to
always ensure that you can actually
retrieve the artifact with the specific
version for the specific group and
artifact ID so usually these binary
repos they expose HTTP URLs so you can
always create unload it with the help of
code
fairly simple to be done so let's
actually talk about the deployment
process process itself you want to make
it a reliable process so one it to be a
repeatable it shouldn't break and you
want to exercise it over time already
you want to use the same logic for
deployment to test QA and other
environments that you might have when we
talk about deployment people oftentimes
ask me what plug-in do I use to deploy
and I ask them well how does your
deployment process actually look like
because oftentimes is fairly custom
again I cannot necessarily recommend
just using the cargo plug-in or
something else that you might know from
the Maven world because it has its own
problems and if you want to add
something on top of that any kind of
like custom logic may it be shutting
down the server beforehand or bringing
it up or backing up log files that you
had there before or the old artifact
even anything that you can think of is
basically out of the box of these
plugins that are out there and it comes
down to custom logic again again Gradle
gives you all the flexibility to
implement that kind of stuff so let's
actually talk about the different
deployment models that are out there so
one of them is push deployment where we
simply say we want to deploy a version
1.0 34 so we basically pull it down with
Gradle so we have all the functionality
that's built into Gradle so you can pull
it down automatically as a dependency
and then we actually push it up to a
specific server this could be test your
at heap production so that's one model
of oftentimes people use that but
nowadays you can actually see a drive
towards a different kind of model which
is called pull deployment so people just
trigger the deployment process on a
specific machine and that process will
actually make sure that it knows how to
get that specific artifact for example
this guy could be coming from a rpm
repository it knows how to install
itself or we could basically create a VM
here or a docker image and it knows how
to pull that specific artifact out of
the repository so these are certainly
models that you can think of and it kind
of
on your own deployment process and again
it comes down to custom logic so I
cannot really recommend what they use
but just to give you an impression of
what you can use oftentimes people
actually go for SSH so they might define
each of these steps that are required as
remote commands on that machine that you
want to deploy to for example we want a
shutdown server we want to clear out
specific directories and then we're
going to deploy deploy certain things
there's a plugin called the SSH plugin
it's a Gradle plugin in this case it's
actually an open source plugin and we
can use that to run specific SSH
commands so fairly simple this is just
to give you an idea how to define these
things you can you find specific hosts
that you want to deploy to so this is
just one of the options here again you
can deploy it to different environments
and you use the same logic for it
you can drive where you want to deploy
to by the help of command-line
parameters so you can simply say eh I
want to use the logic for deploy into
production and the only identifiers that
make sure that we deploy to production
is by providing a specific property on
the command line this is something what
we call project properties and you can
do this with - capital P and define it
and you can access it in your
credibility I will actually step over
functional tests you can define your own
logic here again set up source sets and
tasks so do these kinds of things
integrate selenium and so on I'm kind of
running out of time that's why I'm kind
of skipping over it but let's also talk
about the continuous integration part of
it so here we show the full picture with
Jenkins and Gradle so we can see here
Jenkins usually triggers a specific task
or run off cradle in their job
definition we pull the source code from
Jenkins and then we can run all these
tasks that we already talked about like
raising the artifact publishing it
deploying it to specific environments so
all of this can be driven of a job and
in this specifically we implement all
that kind of stuff in Gradle which gives
us all the flexibility you actually need
there are a couple of plugins that are
interesting in the world of Jenkins to
model these
of things you would get started with
having single jobs which represent each
of your tasks in your build pipeline so
for example for publishing you would
have one for pushing something to test
environments you would have one these
kinds of things and then you would put
it all together this is just to give you
an impression of that initial job
so the initial job here uses two jacoco
plugin it uses specific identifiers here
to create code coverage friends again
you can use Sona here as well that's
kind of independent you can define with
specific plugins which the downstream
project should be so in this case we
simply say once the compilation and
integral unit testing is done move on to
the next step which is as you remember
integration tests and that's just
another job in our build pipeline
there's a Gradle plugin for running the
tasks and you can have a visualization
of the build pipeline itself so I can
actually show how this plays all
together and at real time you see it
changes colors and so on there are other
plugins that are helpful I will not run
into them I'm running out of time so if
you have more questions just come to me
ask me questions or outside or visit
visit us at the booth tomorrow it's
going to be open I think by 8-3 so feel
free to swing by with your own build
problems your own integration test
problems and we can simply talk about it
thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>