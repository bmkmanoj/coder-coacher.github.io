<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Is SQL the Best Language for Statistics and Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Is SQL the Best Language for Statistics and Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Is SQL the Best Language for Statistics and Machine Learning</b></h2><h5 class="post__date">2017-12-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/2l1pRjOcxYE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello and welcome to my presentation
today titled is Oracle sequel the best
language for statistics on machine
learning my name is Brendan Tierney just
to give you some background details
about myself I've been kind of working
with the Oracle database for 25 years or
26 years and I kind of do a lot of work
which kind of analytics been analyzing
data with Granite Oracle database being
the core across that I'm from Maryland
and I do a lot of worker kind of a not
just in Ireland but across the UK across
Europe and the Nordics and also into the
u.s. I've been working in the area of
really of analytics know from kind of 93
onwards and with with machine learning
data mining working with a large kind of
telcos and and insurance and banking
type industries and across Europe and
and and the u.s. for the last kind of 20
years I'm an Oracle ace director and
I've written a few books so the first
two books here hard to do with the
Oracle advanced analytics option so the
first one covers the Hinn database
machine learning and analytical
capabilities using the sequel language
as well as the GUI tool the workflow
based tool that comes with as part of
Oracle sequel developer and also another
book on Oracle or Enterprise so kind of
taken the oral language and how it can
actually work as part of the database
and using the capabilities within Oracle
database I've written another book on
kind of sequel and PL sequel which for
our ace directors rock came out last
year and then in kind of February March
2018 I have a new book coming out on the
essentials of data science and that's
what MIT press you can google me and
when you do you may come across my
website called oral ethics and but you
may come across details of you know a
writer or a painter or an artist or a
cage fighter that's also called Brendan
Tierney so if you kind of put in Brendan
Tierney
Oracle or data mining you will fire your
farm a blog and just lots and lots of
information on das to do it no use and
Oracle database and our language which
in combination with our Oracle database
in order to do various if non electrical
techniques so they're all kind of
technical type posters very little hair
on there kind of us as a kind of
futuristic kind of stuff just you're
looking at what is what ifs and ands and
I kind of deal with kind of this is what
we can do kind of scenario statistics
statistics is there on everything that
we do across our entire organization and
even across all the press and you know
all they're known since that's coming
from all good even kind of IT companies
and we're getting we're getting to see
it in lots of different areas and lots
of different terms being used like data
mining machine learning and the current
flavor de Montes artificial intelligence
you know whether it's in visualizations
data process data recognition deep deep
learning and so forth so you know a lot
of us unfortunately you know from from
my experience we come from and you're
our backgrounds and they aren't
statistics is is pretty much limited and
you know we do have a slightly blinkers
obscured of you maybe from our early
learning experiences with with
statistics but in the kind of the modern
era we do need to kind of embrace it and
love and love statistics a little bit
more and particularly when we look at
the air after data scientists and know
what they're doing you know it's have
quite a broad generic term that could
mean lots of different things to lots of
different people but even you're using
all of those kind of broad terms the
common thing that we have in common is
that we need to be processing data we
need to understand the data we need to
be able to see any patterns or insights
and the data even before we get to using
our into machine learning algorithms
that we have and the main interface that
we're gonna have for doing an awful lot
of this is true the sequel language
whether it's the sequel language in on
the oracle database or across on the big
data sources or whether you're into some
of the streaming type sources that we
all know are you using a sequel language
to be able to access
so you know sequel is really the core
fundamental language of data and you
know with data scientists yes you know
they will spend puppy 80 90 percent of
the time effectively you're running
sequel samples and examining the data
but does being produced on this and when
we look at various different surveys
that are produced on what are the most
popular languages for analytics you know
every survey we see is slightly
different and in a way they're all
slightly biased based on certain kind of
Pacific industry norms or particularly
industry sectors that are folks and all
that but one of the underlying themes
that we have regarding statistics and
all of these languages and tools is that
wealth sequel is always there is always
in the top three or the top five or four
what we need to be able to do you know
and so it is kind of a core fundamental
language you know from statistic machine
learning AI into Big Data you know it is
the language that we want to be able to
to know and analyze to process our data
but unfortunately when we start talking
about statistics and machine learning
it's enough for me
and it's just reminds me of my classes
from from our studies a few years ago or
many years ago that's just yeah this is
kind of seems quite boring because we're
working with abstract data sets with
abstract problems that we don't
necessarily know or understand but
things are a bit different now so you
know I'm part of that difference is that
we don't necessarily have to go to all
that pain of those statistics class is
that we can use the various different
statistical functions that are available
within the Oracle database so here we
have an example of using the song
function you know in a couple of
different combinations but you know when
I kind of go into different companies or
have kind of present about this as I
asked the audiences can you list out all
the different statistical analytical
functions that you can that you use on a
daily basis so just take a moment try
and count them out and kind of use your
fingers to connect them out and what I
typically find is that you know I very
rarely
kind of get more than five fingers
included in my account on
I managed to get to ten using all 10
fingers you know in one scenario so that
was kind of quite great rare and in a
way we do need to kind of simplify all
of this and to kind of go well if I want
to do anything beyond these I have to go
and you're from having as simple kind of
like what we call access mystery kind of
sequel very use and very simple into
nightmare scenarios where you know large
complex queries which are not really
usable so hence we need to use other
tools to do our various different kind
of analytic stack that we want to be
able to perform and that's not
necessarily the case
but when we start talking about
analytics is that the usual thing that
we want to or we get exposed to is that
we need to know and understand these
formulas and our way you know this is a
bit of the wrong approach to take and
you when we see this sometimes we go
oh please know that you know this has
reminded me of the those stats classes
that I I didn't really like and enjoy
and I didn't understand because if it
was always a little bit confusing about
what it really meant and because we
weren't working you know when in in in
those classes we weren't working with
real data we didn't quite understand it
you know we got to see you know silly
examples of employee satisfaction has
doubled or profit as doubled but it's
just so small that it doesn't really
make any difference or a yesterday I had
0 wives today I I have one wife and in
you know four months time are the most
time I'll have lots lots more right so
you know it just leads us into the just
common phrase that we kind of see
associated with statistics you know
third statistics can be used to confuses
to bamboozle us you know and you know
what is being said to me several times
this talk is too complex you wouldn't
understand so you know we have this
phrase of lies damned lies and
statistics so depending on who you
believed kind of came up with a phrase
first but if we go back to these
particular statistical formulas is we
have
something like this so what I'm gonna do
is I'm gonna go through each of those
formulas and kind of honest you to kind
of think about do you know what this
means
do you know what this represents because
this is what we would have seen in our
south class and this is what we would
have had to apply to it and in our
coding environments we use API so we use
api's to hidden to hide the complexity
of what there behind the scenes so this
is the equivalent of our sum function
and what we do is we know and understand
what so does but we may not know and
understand the formula that's associated
with it so the developers of the Oracle
database and every other language is the
factory is taken that kind of complexity
and simplified it into a simple kind of
function API function that we want to be
able to use and this one our function is
this this is our average function again
you know we know and understand what
average does but that formula may be
very very complex to do and note on here
this is our standard deviation and our
last one this is our correlation
analysis so like I said we can easily
understand this but again sometimes when
we're talking about analytics and
predictive analytics and machine
learning people sometimes like to put up
large complex formulas when in reality
does no particularly need for it what we
need to be able to do is know and
understand the problem that we have know
and understand what techniques are
available to us and then being able to
match up those techniques or those
formulas and those api's those functions
to use them to address our business
problem if we take that approach we can
achieve so much more but you know when
we start looking at all of this is again
we can have some difficulty or
challenges and trying to understand what
it all means and this has really changed
you know particularly over the last
years and we have far more for sister
gated graphing and and visualization
techniques and tools available to us so
that we can understand what's going on
we can create the different charts who
can use different colors we can use
different
sizing we can use different
representations to represent different
aspects or for our business how it
changes her growth and behind all of
these or behind most of these is we are
using a suite of statistical techniques
so that when we go and investigate and
see oh we're using these techniques now
we have problems and data that we know
and understand because this is our day
to day job with our business with the
data we use day in day out and we can
see and understand what these techniques
and statistical techniques are really
doing so we kind of get it now we we get
and understand a better picture of what
this data does and in reality what we're
doing is we're trying to create stories
about our data by combining a number a
different type of visualization
techniques with the statistical machine
learning and and and other advanced
methods in order to see if there's
certain patterns certain events that are
going on certain occurrence that are
happening you know in certain orders and
and dependencies and being able to take
all of that information and being able
to relate it back to the business so
that they can know and understand it
rather than trying to be bamboozled by
lots of different formulas but when we
start hearing about analytics is we
start hearing about how we should be
doing analytics okay and in a way kind
of this presentation is something
similar but it's not slightly
alternative approach to what we
typically hear so you know everything
that we hear optically a few months ago
has been you need to use our elimination
now Python is kind of the flavor of the
month but you know with the oral
language is you know is that for
everyone is that a suitable tool that we
need to be able to use and you know if
you kind of read you know with all the
reading that we hear about it is you
kind of believe that's what everybody is
off doing but in reality there's another
tool that is more widely used for doing
analytics and that's Excel I Excel is
the most properly popular product being
used for analytics statistics and
and other kind of modeling type
scenarios around the world because it's
a very simple tool that people across
many different functional areas within
our organization can easily use can easy
understand and can easy produce stuff
that we want to be able to do but when
we're working with data behind the
scenes is that the language that off
data is sequel in both of these
scenarios what we have to do is we have
to extract data out of the database and
put it into these various different
tools and we know all the kind of the
security and an Associated kind of
issues relating that tall order but if
we were able to use the language that we
know and understand then you know maybe
we can achieve a lot more but does this
language the sequel language have the
capabilities that I'm looking for so
just expand another little bit further
is that kind of the typical approach to
to analytics is that we have our
database which is then used to extract
data out maybe into a CSV file or text
files on forum and then that's loaded up
into various different tools there for
various different different analytical
reasons out of which your will may come
up with different models or different
kind of representations of the data
which then can be may be fed into
various different reporting and
analytical tools or into other scenarios
and eventually you know dockets
you know results of that may get and
loaded back into the database kind of
for future use have future and analysis
and inclusions in in other aspects or
within our production systems but do we
really need to use other tools and
languages well the answer to every
question nighti its is it depends you
know it depends on what we need to do it
depends on our experiences it depends on
what we're used to using and you know
again if we kind of read the literature
is like we have to be off using some of
these tools and sometimes you know
actually kind of you know do I need to
go and learn a new language you know you
know do I need to use a new tool do I
need to keep on learning
you know asana key yes we always need to
be doing that
you know do I need to learn a new
language in you tool every six months or
12 months maybe maybe not so what if we
could use the language and skills that
we already have so the sequal language
that we use on a day-to-day basis to you
know help us develop our applications to
interface what our database to work with
our data being able to analyze our data
and then being able to expand that to
include in our applications so if we
take the scenario out kind of show the
traditional kind of database approach
and what we want to be able to do is can
we actually eliminate all of us order
steps all of that other software and
just use the capabilities of the
database so did you know that the
database comes with over 350 different
statistical functions within the
database right a lot of what we hear
about is these ones that are highlighted
in the red boxes which is our kind of
everyday type ones with a few extensions
on it but the database in your eyes I
said has over 350 different sets core
functions and you know this is a bit of
an odd word to say with the database but
if you have the Oracle database all of
these functions come as standard in it
okay and there's a few extra kind of
capabilities onto some of the different
kind of license options offered so there
is a huge amount of capabilities there
just to illustrate a few of these right
and the first I'm going to look at is
like when we see people given examples
of using like the oral language it's so
incredible we can run this function on
our date and we get all of this insights
into what is what what our data is
describing this is like to do with your
profile in Dayton or and you know we can
do all these order functions and it's
kind of like really do I really need to
use this order language so I have to
extract the data all out of my database
I need to load it into or and then start
running all of this and this is a bit of
a kind of a simple example of someone
didn't read the manuals all right
someone didn't read the documentation in
order to be able to know none son what
is possible and the exact same function
and most of the order functions and
analytical
functions that's part of the core
language are actually available as
functions in the database and the reason
why we can say this is that when you
have other products and other vendors
that say hey we can do in database
analytics and what they are doing is
taken our product or tool their language
and mapping their statistical functions
to the equivalent statistical function
in the database so when you're using our
tool is you know you're doing a certain
analytics the converters in the secret
code they're running in the database to
get the results I'm pulling it back
that's exactly the same thing that
effect happens at this so you know with
our language so if I'm using protocol
Oracle or Enterprise
you know des will map that function into
into this particular sequel function run
that in the database and then bring the
results back so here's a simple example
being able to use it on the same data
that that was used in in the or and
language and y'all have some blog posts
about how you can go about using us so
every time I start off on a new online
list project I kind of have some scripts
that I have open those blogs that kind
of goes true about how I can gather this
and being able to show how the data
evolves should may be different time
periods or different kind of as sub
setting off the data so it's really
quick really efficient way or be able to
access the data now what I'm gonna do is
over the next kind of few slides is kind
of show you some of my favorite
functions within the database sorry some
does their analytical functions now
these are my particular for everyone's
order people will have other favorite
ones there are only a tiny subset of
what is capable and like I said just
over 350 different statistical functions
within the database the first studies
are the second of these is to do kind of
creation histograms so instead of you
know using tools to be able to bring
data out and being able to kind of do
the analysis and maybe produce some
charity and I can write you know some
sequel statements using the wit bucket
kind of statement to create equal wish
or or variable with type a book at
ranges and being able to do kind of
frequency Council within that and then
if I'm using sequel developer as my kind
of
laughing tool of choice I couldn't easy
kind of pulled that same sequel
statement into creating some very simple
charts so within kind of the one
language the one tool I can start
getting lots of kind of interesting
patterns and see different kind of maybe
the distributions of the data across our
our application and when this is
combined with like the pivot function
which I'll cover in in a few moments you
know just gives us quite a powerful tool
then for like the data scientists and
the machine learning people when they go
about creating their datasets for input
to to the machine learning algorithms
common tasks we always have to do is a
bit of sampling and so what we are
working with small data you know it
something doesn't really kind of matter
in that particular case or for working
even with large data with large data
could be you know still many millions of
records but when we kind of get you know
sniffing really large and then that we
might want to sample the data so there's
a couple of different options available
within the sequel language to be able to
do that so we can do kind of you know
here's us take a sample of 20% of the
records out it take a sample of 20
records within each block for that kind
of table and where the data is stored
but we do need to be able to have
consistency would use in the sample
function is we need to be able to avoid
a standardized seed value because when
we use it open these particular examples
it'll have a randomized seed and every
time you run it you will get back a
slightly different number of records in
some cases that's fine
in other cases that's not what you want
you want to have some standardization on
the number of Records to get back so an
alternative to do and item and probably
slightly more efficient at doing this is
using the Ora hash function
so this is where I can bring back hey
bring me back twenty percent of the offs
off the record zone and I'll do a random
kind of a selection of records across
your data set and if we look at you know
behind the scenes of some of the Oracle
applications on some of the Oracle
products this is the function that
they're actually using within the
database to do some sampling and unsub
setting off the data on some recent
projects I've been looking at doing some
fraud detection and I've been building
you know probably 50 to 60 different and
the pattern recognition systems using
the match recognized command and to look
for kind of unusual or potentially
fraudulent type behavior so here's an
example of one where you know if there
was a ten consecutive lodgements on the
same day followed by withdrawal within
two days then that's something I need to
be aware of so we were able to kind of
build these in to do almost real-time
monitoring and after data as the
transactions are out mean we can either
go flag it to to the retailer or to the
financial institution that the person
kind of serve and the customer and in
real time or in scenarios where we can't
do that
you know we can put a stop on a
destitute to later on we can have an
investigation team kind of on premise
and within like an hour you know to
investigate what is actually being
happening and put blocks on the accounts
and stuff like that so they're really
really powerful tool and I'm sure you
know you will see plenty of
presentations about this particular
function in in other conferences and so
which I forgotten being able to learn
more when we do analytics using
different tools and reporting tools one
of the things that we keep on having to
do is maybe apply different functions to
subsets of the data or to do different
calculations on on the data like so
different subtotals and things like that
so typically you know we'd have to use
those auto report in analytic tools
being able to allow us to do that but
not anymore we can take that complex
processing the heavy duty processing
away from the reporting tool and push it
back into the database and let the
database do what is really really good
at so instead of pulling all of this
data across you know we now just have to
kind of pull across the data that's only
necessary so by using the petition
clause we can now do additional
calculations like show me the average
salary and by person and we can extend
by department and we can extend out up
put in some additional ordering on it
and depending on on various different
things all right so we can you know we
can analyze the data in lots of
different ways we can do our correlation
analysis partitioned by job ID so we
can't just say you know dude on one
particular variable we can do it across
many many different variables to see you
know is is there a correspondence
between like say people on salary basis
their job title versus to experience you
know is that everyone roughly wouldn't
kind of did the same kind of allowed
ranges or not the pivot function I
mentioned this before we went combined
with the with the whip poker to look at
we can get some powerful kind of
reformation of the data so when we kind
of run our queries they bring back the
quantity sold per product item is we get
our typical results coming back in in
row format but when it comes to
providing data in analytical format we
might want to change that from being in
column or format to being in in column
or format instead of row format so you
know how do we go about that we can easy
use the pivot function to be able to do
this so this is really useful when
you're going up into like into the
machine learning data mining ai is being
able to prepare the data for input into
those algorithm is is the data needs to
be in column or format and with one
record per ton per particular customer
so this is a way we can take all of that
data and reformat it into a more useful
format for our analytics again our
reporting tools we can do lots of kind
of sub totaling is well we have this
roll-up command an acute command within
the sequel language that will
automatically do this for us again you
know a lot of time you know reporting
tools would be spent on trying to
calculate all of these put in the logic
and processing within the tools to do
this and sometimes the tools are
calculated automatically for us and you
know the more data we have the slower
those reports are be because all of that
processing is being done in or reporting
to so by using the roll-up and the cue
command we
shoulder that analytical calculations
back to the very base that's what the
database is good at doing is processing
data being able to analyze data you
doing the calculations on the data back
in the in the database because it's
gonna be more powerful than any sort of
kind of reporting platform and we can
get those results out in addition to
that we can do calculations on you know
what happened before that record and
after that record so using the lead and
lag command again your typical we would
use reporting tools to be able to do
this but but now we we can now push that
back into the database and also by
pushing it back into the database this
also allows us to do some additional
calculations and by doing those
additional calculations in the database
you know again we're saving processing
time and being able to present the
results to our user so with that with
the lag is weak I can do a calculation
based on you know my current record
versus the previous record and what the
lead is I can do a calculation on the
current record compared to the next
record and in the scenario so again you
know we're saving an awful lot of
processing time by moving this back into
the database in a lot of companies are
going to they they want to know about
you know maybe some of these kind of
unusual analytical and statistical
functions for financial analysis and
actuarial analysis and most of the time
is those functions exist within the
database they really really do is gonna
you know we need to get out the
documentation and kind of go through and
read it so we can do all sorts of
different forecasting and different
complex modeling of data within the
database so these capabilities are our
dare within the database and I could go
on about this all day long and probably
each at the different statistical
functions really do deserve almost like
a 40-minute 15 minute presentation all
onto themselves in order to get the
fuller insights into it but you know
that could you know go back into being
one of our kind of stats class scenaries
of years ago
stolen asleep but I did encourage you to
kind of go off and read the
documentation just like what I did one
night a few years ago and I kind of went
through the secret language reference
and picked out all the different kind of
statistical functions that exists and
this is most of them you know does a few
more in in in the latest versions of the
database you know that could be added to
this know this list might incur respond
to tree hundred and fifty but when we
look at some of these functions is
behind them they would have maybe five
to ten additional functions embedded and
so maybe it's a parameter setting you
have to say do this particular test so
when we kind of add all of those in yes
there's a huge number of statistical
functions within the database but in a
way so what who cares in in relation to
all of this you know why is all it is
important when I can have lots of tools
and languages to go and use and what we
need to think about is well by using all
these other tools all of these other
languages we are creating silos of data
we are creating silos of information we
are losing the security of our data data
is no longer secure but keeping
everything in the database we ensure
that no everything a scalable our
analytics little functions will move a
lot quicker there's no data movement and
we can easily incorporate all of these
analytics within our applications
because it's just using sequel the
sequel language the interface into this
is yes we can easily include lots of
statistical and machine learning
capabilities via the sequel language all
in real time and we can also kind of
greatly expand accessibility of all our
tests using various different kind of
and restful services give an example of
the type of time savings and how
efficient it is here is an example of
doing some linear regression modeling is
the first column here gives an example
of if I was to use their oral language
on five hundred thousand records is the
great bit delicious the amount of time
it takes to pull the data out of the
database and loaded into the air or
environment and the red bit corresponds
to
how long it takes to generate the linear
regression model the second column here
just delicious
if I was to do it in the database I
don't have to pull the data out I don't
have to load it into another two I don't
have to do anything with it
I just couldn't run the linear
regression model significantly quicker
and order columns then correspond to you
know running it on 1 million or 1.5
million and you can see how your runners
indoor environment you know the amount
of time it takes kind of grows quite
significantly for if I just leave it in
the database it doesn't grow to the same
extent okay so that's kind of shown some
of the capabilities within there or
within the database on how we can
actually do that and this is all kind of
really really super cool stuff really
efficient you know huge capabilities
really quick really efficient but what
happens when we want to get into the
more experimental the machine learning
kind of capabilities or a capabilities
is yes we can do this within the
database so kind of machine learning you
know AI existed long before big data and
cloud so if you can write kind of sequel
statement yes you can do machine
learning and and do out here's you know
you know the size of your data isn't
isn't really important your data isn't
as big as you as you might think
so we've two ways of doing this within
the database the first one is using the
in database sequel features which is
known as Oracle data mining which is
part of the article advanced analytics
option so so this where you can use pure
sequel to do the advanced analytics on
machine learning so comes with a suite
or different algorithms and puppy you
know 90% or 95% of all the machine
learning you want to do within your
organization will be covered by
biologies and know each version of the
database concept we are seeing new
additions to it we have two ways to
access this the first one is using a
workflow GUI - which is available and as
part of a sequel developer color Oracle
data miner and this perfectly run sequel
commands behind the scenes so each node
here Rowlands commands behind it in so
whether you're off using similar
products for motor vendors everything
kind of works on in the exact same way
so the sequel behind it is we have a
whole suite of sequel
functions down here on the right hand
side but we also have some PLC code
packages you're the first one here today
the DBMS data mining is the main one
that has all the kind of the machine
learning capabilities in it
so let's have a quick look about how we
can actually create a machine learning
your model and use it and to create a
machine learning model it's a factory a
two-step process the first bit is we
need to create a table and define some
of the parameters settings each of the
machine learning algorithms comes with
you know maybe 15 20 or 5 different
parameters and we can set that so here's
an example of that we're just going to
set to default parameters these are the
minimum that's needed all the other
parameters I'm just gonna take the
defaults and are gonna let the Oracle
database work out what is the most
efficient way of doing it then to create
the machine learning model is you know
did in this case it's it's a decision
tree is I just give the name I tell it
the type I tell it the data source I
give an identifier here's the target
attribute that's needed for the machine
learning because of the classification
problem and this is where all the
different settings that I've set so
which is the what's back up on top and
that's all that is necessary to create a
machine learning model over here on the
right hand side
here's we have an example of being able
to use it so we just pass in the machine
learning model going here's my data do a
prediction for this particular condition
that's it nice simple process no
complexity involved on that because it
doesn't have to be complexity involved
you don't have to you know get in at the
the internals of the algorithm be neal
or have to rewrite or write algorithms
on scratch you know do is just like our
some function our average function or a
correlation function is that you know
oracle has gone and created the the the
musts and under stats and the machine
learning capabilities of it within the
database and provided a sequel interface
for us to go and use so you know just
plenty there to go off and explore like
you know there are lots of examples of
that on my blog but a new feature within
the 12c database is this idea of
predictive queries okay weird predictive
queries is these are kind of dynamic so
you know des des will all
actually your build test and apply the
data it'll do automatically tuning for
you it'll automatically scale for you
you just put in the minimum of
information and watch the effect you do
is allow an Oracle to go off and work
out all the complexity for you and do
all the work for you so there's kind of
a phrase doing the rounds of you know
companies are creating products that
bridge the gap between the data
scientists and end-user so what that
means is if you have a problem the
business problem you know you need to
use some machine learning on it you know
the type of machine learning that needs
to be applied but you're not interested
in the algorithm old settings are only
the hard work goes on behind the scenes
what I want to say is I want someone
else to deal with - and in this instance
this is the Oracle database you can say
Oracle database here's the data here's
what you need to do is go do all the
work on it there is a slight problem
with this when you start looking in the
documentation you often come across the
phrase on the fly models dynamic queries
and I'm exploring their predictive
queries you know that from what I've
been told predictive queries is the name
you know that should be used for this
and the functions available to you to do
this are like the data mining functions
that are available as part of your if
you were to data scientist you're going
off creating the models yourself and
what we can do this
all automatic so let's go through a few
examples of this so in this particular
case what I want to say is here's all my
my data is I'm gonna feed that in so
using the using star and in the two char
here's the the character field that I
want you to do the classification on so
it'll take the entire data set you know
look I'm making a prediction so here we
can say is you know here's a group of
customers who have and haven't taken out
affinity carrots here's a prediction on
that we've asked the database Co who
should have an affinity carrot and we
can say is well you know if you take the
first tree customers is well
particularly the first two is here is
people who don't have an affinity card
but what they did it the machine
learning model has said is based on the
characteristics of all the data that is
in this table and the mining data build
view
their data set and taken all the
attributes that's in it by using the
using star it has worked out that these
people have this exact same pattern as
those that have an affinity carat so
this helps us to know maybe focusing on
what date is important what customers
are important rather than treating
everyone in exactly the same way and we
can extend this also by in bringing in a
probability score so if we want to say
is your what customer should we target
first is with the probability score
it'll kind of gives a a percentage score
of how sure the prediction is from kind
of being a hundred percent kind of
correct down to maybe being ninety
percent seventy eight percent as 55
percent 64 percent type of kind of
correction and we can use that to
ordered our customers and who we should
be targeting forced in addition to this
and this is probably part of the the
real beauty of predictive queries is
that it'll automatically expand so the
two examples I kind of gave just now
look that we're going to use the entire
data set but in a lot of instances that
won't be necessarily appropriate what we
want to do is you want to partition the
data into different machine learning
models may be based on in this data set
based on each country but this could be
like if you're in the United States it's
like on each of the different states we
want to create a separate different
machine there in the model for each one
because maybe the behavior of our
customers were all different so we can
expand that out by using repetition
clause so we in this case your world
owners on country so petition by
countries and this will automatically
based on our data set create nineteen
different machine learning models I have
one for each of the different countries
and will then apply that to our data so
we'll get a lot more refined a lot more
accurate results coming out on on our
data so when we can use this we know
we're getting a lot more accuracy in
addition to doing to the typical kind of
your classification we can also do
regression like can we calculate you
know what revenue someone will produce
or the lifetime value over a particular
time and we can do lots of other
you know comparisons and calculations
based on this data in order to be able
to pull out additional information so
that we can use and process a lot more
efficiently you know and we can also as
part a petition we don't have
necessarily have one particular country
we can have our one particular variable
we can have multiple variables in this
now this is something that wouldn't be
traditional for your data scientists
because they would probably have to go
off and create separate machine learning
models for each of the different
combinations that you want we're here
DISA will be done automatically for you
so in this particular case which we had
in nineteen different states by two four
values for sex we suddenly get thirty
eight different machine learning models
being automatically created for you if
we moved into a xx state or xx country
just no changes to our code the code
will automatically pick this up and now
we will get you know 28 different states
by two values for sex which will give us
40 different predictive models that
would be juice without change in our
line of code we're on the alternative is
for your data scientists who will go off
and do lots of a additional coding lots
additional work in order to be able to
work across this appropriate models so
this is a kind of a quick way of doing
it so anyone writing sequel
can now effectively call themselves a
data scientists another little feature
that is really important and
particularly over over the next year's
to do with the EU general data
protection regulations which is going to
affect every country around the world so
if you're guarding a processing data
from anyone in in the EU you're going to
have to be able to explain why a
customer had a particular protection so
Brendan got identified that he's likely
to churn I want to go well why I'm
entitled to know why you picked on me
for that special offer and what we can
see is within the database within the
sheet machine learning capabilities of
the databases he can give an explanation
of what particular attributes for
Brandon and what values within those
attributes actually contributed towards
that particular decision-making so
there's a something that that's quite
unique to to Oracle at the moment
a lot of kind of commercial vendors are
trying to kind of catch up on all of
this but there's no getting away from
their oral language this is you know
everywhere we go everything we read it
is it's there but we do have certain
challenges being faced with using our
language from the scalability
performance but puppey typically is that
you know once we move out of the lab
environment if we want to production
Eliezer you know it's very hard for us
to take that code I'm embedded within
our our production systems and what we
typically happiness's we have to recode
it into Java or or whatever other
language of choice that we want but
there is an alternative all right so
what Oracle did many many years ago and
they were probably the first or one of
the first to actually do is they talked
to our language and embedded it within
the database so this now means that any
one who can write sequel can now access
not just the data in the database using
C code but they can also run or within
the database so this makes things a lot
more efficient a lot more analytical we
can use the computing capability of the
database so whether you are still an or
user already you are a java user or a
sequel user we can all run or true the
sequel language so you know we can
develop significantly richer
applications by doing so and this new
overcomes many of the different kind of
limitations of the Canada or language
particularly the the production
deployment aspects offer because we can
take the machine learning models and all
that or code that the data scientists
have produced and being able to store
that and embed that in the database and
write some sequel that can call that so
the data scientist can be working purely
on or and us as developers you're
accessing that data are accessing that
functionality we can do that using the
sequel language but how often do we get
a new version of the database well we're
going to be getting a little bit more
kind of frequently done we use ooh
butters you know how often do we get a
new kind of function
within the database No maybe we don't
get as much as we'd like no but we do
get a new a feature every now and then
but you know if a new analytical or
machine learning or a kind of technique
became available and it was available as
a normal package we can take that our
package installers on door achill a
database server and we can start your
using that capability instantly and
really efficiently so let's go through a
couple of examples of showing how we can
take their oral language and being able
to use it within sequel so in this
example in the top we have a bit of
speed sequel that is going to create a
function within the database and this
function is a piece of our code that
runs a machine learning model that's
already stored in the database and
applies that machine learning model to
the data and reformats the data and
returns there a subset of that format
the sequel at the bottom there's a
little bit probably more complex and it
needs to you know it it can be quite
quite simple as you know this is the
data were feeding into it does have many
records I want to come back here's the
formatter what what I want to come back
and what we do is we end up by getting
data coming back in the typical kind of
column row format that we are used to in
all our kind of sequel statements so
what this is this sequel statement is
done it's gone off and being able to run
this our code so like I said is if the
data scientist comes up with a new piece
of our code that needs to be run is we
don't have to rewrite hunting we can put
that into the database wrapped it in the
function and use a select statement be
able to clear your honor but it's not
just the machine learning and the
statistical techniques of their own
language we can do we can utilize any
the features within their or language so
here's an example of being able to use
the basic kind of plotting features
which you know it's it's fine up to a
certain point but it's not necessarily
very kind of colorful or a useful but
when we kind of go through the all the
different packages and features that are
available for the oral language you know
we can start incorporating lots of order
kind of graphing and visualizations
within our data so we've seen
in order to lure you know they're using
our language graphing capability yeah in
order to create richer visualization
similarly us now within our applications
we can use these rich visualizations
whether were using ggplot2 like it's on
the left or if you want to do some text
mining like you know what's on the right
we can easily do this and integrate it
into our applications so here's example
stone up in in Apex but you know you
know we're just using sequel commands to
run our code in in the database it's all
really good so far but what about you
know when we do really need to work with
big data and when we're working with big
data we have a few different options
open to us and we have the Oracle kind
of Big Data sequel which allows us the
query not just data in our database but
across like no sequel and and had
various different data storages on hit
on Hadoop and and we can effectively
write one query that those various
different analytics and machine learning
across all this data you know and once
we can do that we can create no
different api's and restful services to
integrate into all of this all of this
kind of still really surprised me but
the functionality that's available
within Oracle database and how much we
can really analyze and and and achieve
by just using you know the the the
functions that are available within the
sequel language so this kind of brings
us back to the original question is the
is Oracle sequel the best language for
statistics on machine learning yeah I
believe it it is you know we can achieve
this huge amount really quickly really
efficiently making our applications so
much better and and achieving greater
insights into our data so the sequel
language really is king I mean in all of
this so whatever statistical function
you want probability that it already
exists in the database is there so you
know we do need to go and read the
Oracle
documentation not just kind of rely on
Google in order to give us answers the
information is there at our fingertips
we just need to go look at and yes the
the Oracle languages is king and as our
data grows you know whether you're
working with small to mid-size data big
data
you know big data it's a really bad turn
like all of us we just work with data no
matter what argument is wherever it's
located and we're looking for the most
efficient scalable and secure way in
order to be able to store and access
that data and being able to analyze it
so your stats and analytics if we
embedded within the database it will
grow with the capabilities of the
database and we can have greater kind of
functionality there and being able to
achieve the goals that we want to be
able to achieve by using the databases
are like a compute engine and then being
able to expose all of this not just to
the secret language but truth very
different rest services that are
available to us so yes we can kind of
teach an old dog new tricks yes we can
learn new things using the languages at
the language that we know and love and
that we are used to on a regular basis
that has that pure interface incident
into the data so we can get our
capabilities the main way we can go back
doing this is by reading the
documentation by exploring what is
capable and what is there so that maybe
next year you know or know couple years
time instead of seeing the secret
language being like the deterred
language of choice and in the surveys we
will see that kind of flipping over to
being the language of choice but you
know we do need to to love and embrace
this language and the capabilities of it
more that brings us to the end of the
presentation so thank you very much for
for listening and watching the
presentation if you got any questions
here are some of the details to get in
touch with me
there's some of the details the books
and if you're gonna need to Google me
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>