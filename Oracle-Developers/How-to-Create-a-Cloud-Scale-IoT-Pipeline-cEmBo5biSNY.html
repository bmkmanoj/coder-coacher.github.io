<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How to Create a Cloud Scale IoT Pipeline | Coder Coacher - Coaching Coders</title><meta content="How to Create a Cloud Scale IoT Pipeline - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How to Create a Cloud Scale IoT Pipeline</b></h2><h5 class="post__date">2018-04-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/cEmBo5biSNY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay let's get started so my name is
Jared I work at Oracle development I
have two roles I'm a consulting member
of technical staff which means I write C
code for the Oracle database kernel and
I'm also a product manager so that means
half the time I'm creating bugs the
other half of the time I'm complaining
about people fixing bugs so but about so
what we're gonna be talking about is at
a high level what is IOT and why you
need pipelines now if you ask 10
different experts on IOT you're going to
get 10 different answers what I'm doing
is I'm looking at four different use
cases which are completely different and
seeing how that works and then stepping
back and say hold on they're actually
pretty similar so based on real-world
things I'm deriving what I believe the
important characteristics are and then
I'm plying it back to useful technology
I'm gonna be talking about a thing
called an IOT gateway it's a it's a it's
a term that people use a lot and then
I'm going to show you how you use it and
then apply it back to once again
existing technologies okay so without
going into an explanation of what IT is
because it's kind of dry I want to show
it real-world examples of how the stuff
is used to try and give that some
context so these use cases are based on
three out of four of them are projects
that I've worked on were it's real stuff
so in the first one you've got a cloud
and that cloud has got a lot of systems
and those systems create a lot of data
good things happen bad things happen
security bad things might happen so one
of the things that might happen in the
cloud because you have hackers who are
trying to do bad things to the system
you want to figure out if something bad
is happening quickly efficiently and
cheaply so that you can do something
about that
now as the cloud gets big that gets to
become more and more for challenge so
it's just a really high-level example so
and your cloud if you go you know a huge
number of compute nodes be a learning
experience
those containers holla'd Oracle software
a lot of stuff you got to capture all
that information somehow magic occurs
and you need to alert people you need to
do stuff about it so the question really
is about what does that magic how does
it work so this the system needs to be
highly available
it needs to be persistent a things break
you got to be able to retry it you need
to be able to have an audit trail of how
things are going wrong so hands up can
we use one big computer to do there no
20 years ago that's what people try to
do you can only scale up one machine so
far so it's just scaling out so nothing
I should be saying here is particularly
you know if shattering it should be all
stuff you you're familiar with but it's
just showing you how those existing
techniques you can chain them together
to do big stuff so let's have instead of
one big computer let's had lots of small
cheap ones do everything parallel
processing but more importantly do
publish and subscribe processing that
way you only need to deal with the
things you care about you can specialize
different components and when you chain
more things together change what they're
doing it becomes simpler even though you
may have thousands or millions of these
things so a simple concept that works on
a couple can scale up both performance
wise and complexity wise by using some
of these techniques so a lot of a
technology that a lot of people are
using is called Kefka which is a
persistent published and subscribe
message bus so 20 years ago I was using
misses buses worth you know tuxedo
typical rendezvous you know later on
there was you know Jamie's for Java
things like rabbitmq all of those
technologies are pretty much the same
thing if you do a box and what they do
is pretty similar at the moment kefka's
the kind of cool one it's open-source is
free and it scales very well so if you
use Twitter Twitter's just Kefka people
tweet which puts put pushes stuff in
that publishers
the Revlon people subscribe to that so
they can have tens of thousands of nodes
and it scales so this is very proven
technology it's not necessarily the
fastest you're not measuring it in you
know a very small number of milliseconds
but it doesn't need to be you're
inherently reading and writing files
through the Linux kernel buffer cache
that's all it is it's highly available
it all works out of the box so the point
is a lot of people using Kefka to tie
together these pipelines different stuff
pushes it in is it coming from you know
a cell phone is it coming from little
hardware devices are coming from
computer doesn't matter you push it into
kefka and other people subscribe out the
relevant stuff so that's one important
piece of a pipeline so I've been talking
about it you've got to capture stuff
locally if you capture stuff locally it
will scale if you try and have one
centralized thing to capture everything
on a mini non-points it doesn't really
work management wise because you don't
know where all those points are and just
performance wise doing it all from one
point doesn't scale so if everybody does
processing locally it's simple it's fast
at scales more nodes do stuff locally so
they capture stuff they filter it and
they push it to a nine point so that can
be a Kefka and your L is basically a web
address once Kathak has got that stuff
it commits it which is a fancy way of
saying it writes it to a file in its
file system happens to be a distributed
file system but that's a detail once
you've got that information you can then
filter their information so the act of
publishing putting stuff into Kefka is
independent of people taking data out of
it so stuffs getting pushed in all the
time and stuffs getting taken out all
the time so you're publishing on an on
topic to kefka people of who were all
systems who are understood subscribe or
filter out the information that they
care about it's not going to be all of
its gonna be a subset so you felt hung
out you got this data and it's going
through this magical gateway thing
and from that gateway you're gonna get
alerts and actions so a lot of my
stalkers
what is this gateway you've got these
huge amounts millions billions of
messages and you're filtering them to
figure out did something bad happen what
should I do about it
so we're sort of taking a big problem
breaking it down doing everything in
parallel highly available so what I've
talked about is a pipeline instead of
having one big problem
you're breaking the the problem down
into smaller problems and you're
chaining them together that's what the
pipeline is so it doesn't matter what
the technology is who the vendor is who
does it everyone uses pipelines what it
comes down to is how long or how short
the pipeline is what the different
components in the pipeline and how easy
it is to monitor and manage okay so that
was one example of IOT we had a lot of
data coming in we were processing it I'm
gonna go into more detail what their
processing is later and we're coming out
with an output there's no point
consuming millions of billions of bits
of data unless you do something useful
with it so you normally get data which
itself tends to be not that interesting
you tend to aggregate it you filter it
do something interesting with it to come
out with useful outcomes so that's one
use case that a high-level I'm gonna go
into more detail all that another use
case is this is a made-up one from the
transaction processing Council so back
in the day TPCC T PCH those guys
invented generic benchmarks they've now
come out with a benchmark for IOT what
they did is they modeled a net electric
utility you got lots of sensors you kept
your information you aggregate it you do
something with it so there's really
useful monetary things that real
utilities do for instance I come from
California we have a lot of green energy
sources as it were a lot of solar
wind power that's cheap they want to use
it but guess what when it's not blowing
when the sun's not shining doesn't
create a lot of electricity if the
demand is greater than that they need to
fire up a coal plant or war to satisfy
the demand so you got all these sensors
saying what's happening what's the
supply you're comparing that to demand
to make a very expensive decision do I
fire up a coal plant they cost a lot of
money they only want to fire them up
when they need to okay and another
example this is another project I worked
on it's another electric utility I can't
tell you where it is but what they're
doing is they're capturing the supply
they're capturing the demand and the
doing something called machine learning
which is then matching the supply to the
map demand such that it maximizes their
profit sounds easy but you've got all
these huge streams of all this data is
just numbers you've got these numbers
coming in and you've got this utility
has I can't go into too much details but
they've got stuff all over the US you
can sell electricity at a loss at a
profit about neutral if you can figure
out how to do it right you can make
summarize the profit so there's
algorithms to do that so they're getting
all the data coming in they're looking
at those algorithms they're using
machine learning algorithms I'd love to
go into great detail it turns out it's
not let's talk about that because of
political property issues the point
being they're using Python and using
existing machine learning Python
algorithms which is just libraries
okay another example and hands up who's
heard of Cambridge analytics recently in
the news okay so what they did was some
clever computer science stuff but it was
legalities a little bit great this is
another example of a different customer
doing something very similar where it
was legal these customers did opt in but
the point is you're getting data from a
lot of customers on the web you're
looking click streams you're looking at
behaviors and based on all that
information people basically figure out
how to tag it ads for you so the
interesting thing is they're capturing
about 10 billion rows per day they
capture that from various different
sources they need to pump it through an
engine to figure out how to target those
ads for you so they want to do very
quickly and efficiently so they also
need to do this highly available they're
doing with synchronous replications
across data centers and we do it we're
dealing with about 3 million inserts per
second so 3 million in spurts seconds
not particularly big number but when you
do it synchronously persistently across
data centers that starts to get hard so
that 3 million is with persistence with
synchronous replication so I can't talk
too much about that women we have
analytics you people click on stuff
that's captured for various mechanism
its processed so they can figure out how
to take it ads that's sort of the
business the point being we dump it in a
table there's it lots of tables but
about 90% of the data ends up and the
table looks like that those data types
aren't very interesting but they've
compact or encoded or and pre compressed
data into those they unpack them and do
clever stuff with them
so we've talked about generic two
different electricity utilities both
dealing with demand and supply we've
talked about clique analytics and the
other one I can't remember the point
being
all of them are gathering a lot of data
all of them are processing it to
aggregate it to process it to get some
interesting feedback out I'm gonna look
a little bit more detail about now what
TPC did to come up with an IOT benchmark
because like there's a bunch of
different IOT vendors out there with
demos as a bunch of talks about it to be
very cool thing but everyone's talking
about slightly different things so IOT I
mean the TPC see they're independent
this is their view of the world
you've got different inputs you know as
the data coming from planes or computers
or peoples or cars or whatever it
doesn't matter there are these things
that are connected to the Internet
they're talking TCP they're gathering
data most of it numerical and they're
putting it somewhere that information
gets pushed to some system to be
aggregated so the TPC talks about this
thing called an an IOT gateway CEP it's
a fancy way of saying complex has been
processing they're actually using the
Kefka icon there so you get the data and
then you need to process it so you've
got all this data you need to process it
and they talk very specifically about
things like spark and analytics and no
sequel so how many guys familiar with
spark pitch is back so there's just a
fancy way of doing streaming processing
using sequel over lots of machines so
TPC who's independent give you of the
world is you've got lots of them puts
you go through this magical gateway
thing and then you process it to do
analytics basically or
no sequel type things so that's the view
of the world and they defined a
benchmark with it testing this thing
could an IOT gateway so it so happens
that this is a relatively new benchmark
and only one vendor so far has published
a number for that so that is HBase who
works with Cisco to come up with a
number and the important thing is they
were doing one hundred and forty two
thousand transactions per second through
this workload I'm gonna go into a bit
more detail about what that is so I
don't work for cloud era I don't work
for Cisco so I thought that's a cool
benchmark why don't I use some of the
technology that I use to do the same
thing and see what we come up with
before we go into that so just
generalizing what we're looking at there
attributes you're ingesting a lot of
data which is another way of saying
you're doing concurrent inserts you're
persisting the data which is another way
of saying we're doing durable writes we
can't lose any data which is kind of
another way of saying we need high
availability you need to be able to
retry or reach replay operations which
are kind of like transactions you're
going to be able to efficiently process
it cloud scale which is kind of another
way of saying let's use parallel
processing over lots of nodes you need
to aggregate analyze filter which is
kind of another way of saying let's do
streaming analytics you should be able
to advanced statistics and machine
learning so I've sort of derived what
the TPC said about that looked at other
vendors so I'm saying if you've got
concurrent inserts durable writes high
availability transactions parallel
processing streaming analytics and
Python and our that to me kind of sounds
like a database so I work for Oracle no
surprises here so the point is you need
a fast simple scalable and highly
available database so my point is why
not use the fastest ltp database
probably not the one you think it's not
the Oracle database
that's your achill times 10 Scarlett
database which is a new thing which is
going to be released next month very
similar to the Oracle database it's a
scale out shared-nothing database so if
you're familiar with Oracle RAC or
Exadata that shared disk this has shared
nothing so it's very compatible with the
Oracle database uses the same sequel
same pill sequel the same Oracle api's
it's just a different database and we're
focusing on LTP and IOT so how does it
compare
so Apache HBase from cloud era they did
a benchmark and so they used four
machines each worth two CPUs 2.4
gigahertz and they were getting just
over a hundred forty two thousand
transactions per second using the terms
ten scale out we just on one and a half
million so the point is we're doing the
same work but it's faster it's a lot
faster that's the point it's about going
faster if you look at the head where
it's not really a fair comparison the
Cisco had we were slightly better
slightly faster CPU 2.4 versus 2.3
gigahertz they were using 40 gig
Ethernet we were only using 10 gig
Ethernet so it's not really fair they
had better hardware but we were roughly
10 times faster the other thing was if
you guys have heard of speaked it or
meltdown which are basically bugs in the
way until did stuff with CPUs the
benchmark that was published by Cisco
was last year in November when those
spectrum meltdowns that applied which
allow you to go fast our stuff we needed
to patch it for security reasons that
slows you down a lot we're still 10
times faster so the point being doing
IOT workloads we're roughly 10 times
faster than this
Hadoop thing from cloud era and the
other thing is you got to get data and
fast so that's you concurrent insert
rates
once you got the data in you want to do
your analytics your machine learning
type stuff so here's just an example of
doing analytic stuff so you guys who's
familiar with the TP CH workload okay
it's been around for about thirty years
it's an analytic workload the point
being Oracle took that workload took the
queries and made them harder just just
to make things more challenging the
point being if you run it on one machine
it will go slow because they're really
nasty queries if you look at them their
joins and lots of tables worse nested
sub selects it's just really evil stuff
so it takes a long time so the this is
measured in seconds so running on one
machine was taking about 1,600 seconds
which is a long time it's many minutes
by adding more machines that took less
and less time so using 64 machines moved
down to a couple of seconds so the point
is we're doing parallel processing so if
you think of it scare together I've got
a query you send the query at every node
in the cluster they will process it
quickly and send you the answers back so
parallel processing it's not perfectly
linear because it takes time to send the
query out get the answers back so M
Dells lore applies but going from 1600
seconds down to a coupla seconds as a
vast improvement or speed-up
so that's one example of analytics
queries for the actual TP CH the Thomson
scholar
it's an old TP database it just happens
to do analytics we're not the fastest
analytics database we think we're the
third fastest so it's faster than sequel
server side based db2 another thing
who's who's heard of Amazon redshift
okay so that is a analytics database
which is columnar which is designed to
do analytics and nothing else
so they took
an existing big data benchmark
that all Hadoop vendors use that
converted tables and sequels and they
published a benchmark and redshift was
in fact faster than all the Hadoop
systems which is cool I said thank you
Amazon they said here are our tables
here are sequels so I just took those
tables and sequels and ran it on x 10
scale out which is not an analytic
database and it turns out we're a bunch
of faster than redshift
so the amount faster varies you know a
lot faster six times faster a little bit
faster a lot faster so it really depends
on the workload so I've been talking
about analytics times 10 is really an
old TP database what we're doing here is
an 80% read workload 20% write workload
this is a telco specific workload Tom
steam basically owns the taco business
the point being
we've got scalia scaler linear
scalability up to 64 nodes so we're
doing just over 144 million transactions
per second for a read 80% read 20% right
so we analyzed with the bottle nickers
and it turns out the bottleneck was on
the writes because we do asset
transactions and do redo you know and
we're persisting that stuff most of the
work was in the writes so we took out
the writes and just had the reads so
it's 100 with 100 percent read workload
we're doing 1.2 billion reads per second
so the reads are a primary key lookup
based on a hash index so it's not doing
one query getting a billion rows as
doing 1.2 billion different queries per
second over 64 machines so the point
being the what the way most people do
IOT pipelines and the way the TPC
described it as they have a pipeline
they have a really long pipeline
you got your data on the left you put it
through your gateway system and then you
literally copy it so you gate where
system is inherently a cluster
and then you literally copy the data and
that cluster to another cluster to
process that data so one cluster gathers
and persists the data and another
cluster gives you analytics or machine
learning so that works but you've got
two clusters and it takes time to copy
the data between the cluster so my
approaches why don't use one cluster if
that one cluster can do the persistence
can do the high availability and can do
the fast ingests and it can do the
analytics and it can do the machine
learning you've saved a bunch of money
and by the way it goes a lot faster so
the point is that still a pipeline is
just a shorter pipeline less steps each
machine does stuff a bit faster so one
could argue you know clusters of open
source stuff is free the software may be
free but if you got clusters of hundreds
of thousands of machine those machines
aren't free if you own the machines you
got to pay for them you get a pay to
power them up and you got to pay to cool
them if you don't own them you get a pay
per second or per minute or per hour
it's not free okay so it was sort of the
high level of look at the detail now
with the tpc iot workload pretty simple
table pretty simple datatypes pretty
narrow table so I didn't define this
this is what the TPC came up with so
you're pumping data into a table you've
got an index and the trick is you need
to persist the data with a minimum of
two-way replication if you put it into
one system you can get pretty fast
the minute you add synchronous
replication between systems the
determinant is really the round-trip
time of your network so you've got to
have a fast local persistence stuff and
fast Network so we're doing this with 10
gig Ethernet roughly ten times faster
than what the the Cisco had
system was doing it and at the same time
you're doing some queries not
particularly complicated
that's the workload we were about 10
times faster another bit of detail um I
started off with the cloud alerts you
had all those people connecting to those
systems connecting to vm's connecting
the business we connect into web logic
or whatever if you do login attempts and
those logins fail those are logged you
want to look at know this system that
they're that they're working on has
hundreds of thousands of different roles
an example of a ruler's you're
interested in logon failures followed by
login success so there's just one rule
there's many rules related to logins but
a whole lot of failures followed by
success has been defined by these people
who are interested as it's probably a
hacker who was trying to hack the system
and finally succeeded so you're looking
at a pattern over time so if you've got
hundreds of billions of so you've got
all these records those logs because
you've thrown them into this gateway
which is really just a database with
rows what you're really looking at is
hundreds of millions of billions of rows
and you're looking at patterns because
you've got thousands or millions of
these machines you're looking at
concurrent you know repeated logins from
one machine from one user because the
fact that 100 different people typed in
the wrong password doesn't mean anything
that's when it's for the same computer
for the same user and the pattern of
failures followed by exersice that they
get interested so you're really
aggravating that data and you're looking
at patterns of so once you've aggregated
the data failures bourret followed by
successes so there's different ways of
doing this the way I did it as I used
analytics equal and the windowing
function so if I break this down a bit
you're doing a select get some rows and
you're doing it over some values so if
you use the over thing this your
so how many of you guys know analytics
equal okay a little bit just bear with
me it's like a group by but it's more
complicated the point being the syntax
and the analytics equals for a windowing
function we can look at not a not just a
row but a set of rows in the earlier
rows and the later oh so the preceding
or I'm following rows some I'm looking
at those rows a mapping the results to a
bitmap to zeros or ones and I'm summing
them the point being if they sum to one
based on this logic we know we got it
something interesting the point being if
you've got hundreds of millions of rows
doing this sort of analytic sequel
because you do get in parallel you can
process it sub second so the point is
even they got thousands or millions of
machines pumping data run and you've got
thousands and millions of rows you can
process look for these needle in a
haystack type query sub second so
there's just one contrived example of a
rule it's a real rule but this is just
an example of how you can do it okay so
the point being with this cloud all
these different system pump data in take
care care and then you use x teen scale
out as the i to gateway to look at it so
each oracle x7 - two node which is fancy
one way of saying a pizza box just you
know a one new machine so it's about
that high do CPUs each one of those is
handling about 1.6 million inserts per
second and it can be doing the parallel
querying at the same time so the the
current system needs to handle about
fifty thousand and suits per second one
nodes
handling 1.6 million and you can scale
that out because you're using
publish/subscribe you can point to one
cluster or a cluster of clusters so the
point is if you went to billions of
inserts per second you just use more
machines or at worse you use multiple
clusters
so the point is you can process an awful
lot of information aggregate it to full
read up that information get the answers
sub second very cheaply with very few
machines so they went from the existing
production system had a cluster of
machines and it was doing 30,000 of
these things per second I replaced it
with one machine as doing 1.6 million
per second so the point is we're doing
the same things but by using more
efficient technology you can get better
results still have your high
availability and do it cheaper
yep
what a brilliant question it's sort of
segues into the rest of my talk so if
you just hold on a second
so from your question you understand the
problem so the point is they're not
independent machines will they are
independent machines but you got a
distributed database that can look
across the machines that's the trick and
I'll explain that in detail so if you
guys familiar with Oracle I'm sorry are
it's a programming language statistics
so Oracle supports our via our Oracle
which is a fancy way of saying they've
got a DBA driver which talks our you
know some people use the command line
some people use vine our studio I love
as a tool
it's just product of thing the point
being if you use the our language with
you know on its own or with your
favorite database our as our as our it's
all the same thing our Oracle is just
got a drivers to do that the Oracle
database has got an hour driver times 10
scalar uses the Oracle database driver
it's exactly the same driver we will use
all the same api's because why reinvent
them so the drivers just OCI so the
point being the same our Oracle code
that you use for the Oracle database
when we're con times 10 unchanged it
uses OC under the covers
since the live in 204 of the Oracle
database the Oracle client talks to both
Oracle and times 10 so the only
difference is he changed the connect
string so you either changed Tina's
names or easy connect so here's just an
example of using our was times 10 so as
a really powerful statistical language
it's got a whole bunch of built-in stuff
and it's also got a whole bunch of
packages for doing machine learning it
can do all sorts of things this is a
really trouble example it's a heat map
it's not really machine learning but
it's something useful point is you look
at the brightest color that's probably
your problem us so the point is it's an
output using air you've got a data
structure with some data you tell it to
draw a heat map
stuff we just happen to use the times
teen database as a source for that data
just look at my time so when you're
using our the only difference as the
connect string the database name we
point into the connect client at the
time Steam client when you connect it's
giving you an object and are you do a DB
query which is a fancy way if you say
you give it sequel which gives you
result set that result set goes to and
turn our our object you can't read that
the point being you get the query the
sequel just happens to be complicated
sequel using unions and stuff written
select so that one page created that
heat map you probably can't read it but
I can upload the slides so you can look
at the details
likewise with Python and the Oracle
database it uses the open source CX
library of doing that time Steen
scale-out uses exactly the same library
the point being you use tennis names to
connect to it so if you use Python with
an Oracle database use exactly the same
code with Python against x 10 scale out
to change your connect string so you had
a question earlier I'm just about to
answer there so so how do we do all this
magic we're using x 10 scale out which
is what a product we're about to release
it was based on x 10 and memory database
which has been around for 20 years so
it's a relational database user sequel
uses PL sequel it does asset
transactions persist things to desks and
it's highly available so they it's been
around for a long time what we did is we
did scale out on it this was just a
thing from Forrester and recently
Forrester
claims that Oracle is basically the top
and memory database we passed s AP henna
so there's a lot of other people you
probably heard of but according to the
third party Forrester
auricle biet oracle times teen oracle
memory according to them is kind of the
best at the moment so times 10 has been
around for 20 years you use times 10
every day you just don't know it if you
use your cell phone your smartphone your
flip phone and you press talk you just
use times 10 about three times in the
stack it's not running on your phone
it's running the backends so pick your
favorite handset pick your favorite
provider beard in the US speed in China
basically anywhere in the world so a lot
of people take times 10 and they embed
it an OEM it so like earlier versions of
Cisco routers had the times tina-marie
database embedded in that it just worked
you don't see Cisco DBAs or Oracle DBA
is tuning the Cisco routers it just
works so they're embedded and memory
technology from 20 years ago about 10
years ago we win multi terabytes on big
machines now we're going out you know
hundred terabytes scaling horizontally
so the point being we took we used an
in-memory database in memory so with a
disk based database like Oracle all the
data is on desk and a subsets in memory
so the data blocks are either on disk or
a memory if the data blocks on a memory
you have to get them from disk that
movement is called buffer cache
management with the x tender memory
database you don't have buffer cache
management which means it can be really
fast means we don't wait for disk
we don't wait for network the bottleneck
is your CPU so the faster the CPU the
bigger the l3 cache the faster times
team can go so I'm quite modest Hardware
2.2 gigahertz we're doing a sequel
select and under 2 microseconds not
milliseconds microseconds so this is why
I recall acquired the x 10 technology
back in 2006 because it could do things
that the Oracle database couldn't I
specifically low latency updates take
longer because you got undo and redo and
you need to persist it to disk
so to answer your earlier question you
take this in memory database which is
sequel acid transactions and you scale
it out horizontally you have a single
system database image so if you create a
table the table exists everywhere
if you describe the table or do a select
star everybody sees the same data across
all of them so they're all processed in
parallel so that concepts easy to
understand but it's really hard to
execute efficiently so the naive way of
doing it is you use the network and you
seen messages and it works but sending
lots of messages all the time is really
inefficient so what we do to optimize
that as we try and minimize the number
of messages or if we have to send
messages minimize the number of bytes so
I got a whole different talk on how to
tune that but the point being you use
you don't Dean or malaises your tables
use the same schema same primary keys
same foreign keys your sequences you
sequel you PL sequel you don't need to
change that because a lot of no sequel
systems say you need to denormalize you
need to rewrite everything we think we
think sequels quite good we think
relational models quite good so you keep
your relational model you use your cost
base optimizer and you leave the
cost-based optimizer do the hard bits so
you do sequel you prepare your
statements you bind your statements you
execute your statements just the same as
an Oracle or db2 or sequel server on my
sequel the point being if you do sequel
it will go across the cluster and figure
out what the answer is if getting one
row it just has to find out where that
rowers normally hash it to figure out
where it is the example with the
aggregation doing aggregations as
powerful and parallel is really good you
seen the sequel at once to everybody
everybody processes it in parallel and
sends it back so the more machines you
have the lists each machine needs to do
so you can process it in parallel so did
that kind of answer your question
yeah yeah well I mean it completely
depends on the seat call but the simple
simple the simplest case is you send the
sequel out everyone gets its local
results it sends it back and you process
the end of it you know if you if your
sequels this long it's more complicated
than that the point being you don't
change your sequel at processors in
parallel if you do it right it's a lot
faster so just stepping back from this
we've talked about IOT gateways we've
talked about pipelines we've talked
about analytics
we've talked about machine learning and
hopefully nothing I said was
particularly surprising we're using
techniques which are well known well
used we're just putting them together in
a slightly novel way with some new
technology so instead of using a Hadoop
based system which is what most people
use for their IOT gateways we're using a
relational database that relational
database just happens to be a scale out
she had nothing database that database
just happens to go really fast because
it's a memory even though it's a memory
is still highly available because we
make copies of the data everywhere and
we persist at the disk
so it's just what I'm saying is the IOT
gateway
I'm just we just using a faster
mousetrap it's doing the same thing as a
dupe it's just going a whole lot faster
the example of comparing it with the
Hadoop cloud era HBase we were roughly
10 times faster okay so the point being
we're using relational database if
you're familiar with an Oracle database
same api's same tables same syntax it
just goes a whole lot faster so I'm
saying IOT is all about massive in data
ingest rates which is a fancy way of
saying hi insert rates with current
analytics which is a fancy way of seeing
doing nasty queries most of them are a
aggregations and you might also be doing
machine learning which is you're
throwing basically sequel statements to
get the data converting the sequel
statements into an object be it in
Python or R so that our or Python can
then use a machine learning learning
algorithm to process that data and quite
a few each at it we get a pipeline which
is chaining these different systems
together so we've got a pipeline where
each component tends to be multiple
components running in parallel each one
of them doing simple things so I think
that kind of summarizes what I was doing
any more questions
ok hands up who understood what I was
talking about cool when I talk a lot of
people don't understand it's more of a
challenge ok any any more questions if
not we're done yep
so can you repeat the question
yeah yeah so I'll just qualify pythons
inherently an interpreted language so
the same source code same source code
you change the connect string which with
the connect string you're either using
tennis names or easy connect so exactly
the same exactly the same
so both CX Oracle which is the Python
driver in our Oracle heaven interface
Oracle said there's the interface and
they implemented an OC I times 10
support so CI the Oracle client in the
living Torah for or later supports both
10 and Oracle so it's literally got a
switch statement it's got an if
statement in there that says if you want
to talk to Oracle go that way if you
want to talk to x didn't go that way so
teeniest name is has special syntax to
figure out which one to talk to
so the point is because we support OCI
we for free support all api's or drivers
that auricle has or open-source users
that sit on top of OC i in python and
our oracle are just two examples off
there so that means we get things like
pro c and pro cobol for free then do
anything just works
okay any other questions thank you for
your time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>