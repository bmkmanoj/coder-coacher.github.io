<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Divide, Distribute, and Conquer: Stream vs Batch | Coder Coacher - Coaching Coders</title><meta content="Divide, Distribute, and Conquer: Stream vs Batch - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Divide, Distribute, and Conquer: Stream vs Batch</b></h2><h5 class="post__date">2017-10-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/u_EaagcCN4k" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi hopefully you had a nice lunch and
you happy so it just will just consume
another useful piece of information
today and today was going to be talking
about stream and patches at scale
distributed the style and as you might
notice that it's always when you're
talking about something versus something
it's always impossible to not use this
picture right so and actually this fits
very well because what are we talking
about stream in batch processing stream
usually consider something fast and
badge is considered something bulky slow
moving and in not very pleasant to deal
with so today I'm trying to work around
this and show you some tools that you
can use in your life to switch this
batch versus stream paradigm thinking
about processing of data alright
Who am I so just real quick I work as a
senior Solutions Architect with a
company called confluent confluence is a
kafka company so basically guys who
started Kafka back in the LinkedIn they
start the company to provide enterprise
grade a platform for streaming streaming
data in your organization so being as a
solution architect I worked with
customers and I love to solve their
problems so I have experience with
dealing not only with PowerPoint but
also with real world systems also I
consider myself as a developer advocate
meaning that I talking to developers and
want to bring the feedback if you have
something in mind don't hesitate to stop
by and chat about some of the things
that I can help and how I can bring the
feedback to the rest of the team I'm in
Internet's if you guys have a twitter
how many of us have twitter here so if
you see some entering slide don't forget
to eat this right this is how I would
understand that you like this talk or
you don't you can also follow me if you
have a Twitter you should alright very
important slide
even though because we know conferences
and I'm going to build very
scalable very highly available hello
world application
kudos to Kenny from the Pew hotel for
bringing this slides that basically
explains many people who doing this kind
of presentation all right so I will
start with a batch processing you will
talk a little bit about history and
after that we move on some of the
aspects of the stream process now let me
think let me talk about about the data
itself and what how the data is
originated and how we think about the
data and how think about data processing
so from from origins of the batch
processing terms I like even though I
know that that was a batches of punch
cards that the people were bringing into
the computers all days I like I like to
use this picture to explain batch
processing because in this picture
you'll see how the people literal bring
batches of used tickets in the London
metro back in the day and they
processing to get some analytical auras
are some reports how many people use
user Metro to have a ride things like
that so it is a batch processing
important point here is that as you can
see here this operation that they that
they were doing here is happen by the
end of every day so they have a full set
of data for the past day so they can do
processing and considering this notion
or thinking about the data so data
oh it's Microsoft PowerPoint wants to
upgrade it's just a perfect timing
because why when when when when else
right so
absolutely well it's even worse than the
Adobe Adobe Adobe Reader it also updates
all the time anyway now so data data is
inherently tame time base so everything
that we capture as data already happened
so we capturing events that happen in
the past and we collecting them right so
given the fact that something changed
over the time for example we're talking
about the social social media update I'm
changing the place where I live I used
to live in Russia now and live in States
given the fact that I would even say
right now it doesn't change the fact
that I lived in Russia balkanization in
this case it's a two distinct event and
from this I can say data also
essentially immutable right so events
that happened over the time they not
change the facts that happened in the
past so the if we're thinking about if
you're thinking about this data is a two
things is a time-based events and they
are not changing facts happened they not
changing it's very important to kind of
understand and embrace in in this type
of scenario so what about our typical
thinking like crud crud applications
create read update delete so in this
thinking when you think about the data
as number of events that happened one
after another
there is a create there's a creation of
event and there is a process of raining
data essentially update not exist
anymore because it's another creation of
event delete also it's not that
important because it creates it's just a
technicality so if we don't need some
data we can technically delete it but in
general we don't if we have enough
capacity for processing so processing of
the data is the function that will apply
to our data to get some sort of result
and query usually process it's a query
on the full data set and in theory if
you if you will be thinking about
dealing with queries and processing your
data if we have assumption that we have
unlimited bandwidth and unlimited
computation power you can apply this
equity to full data set projections to
the day the projections will give you
subsets off of your data or aggregations
that give you a result the based on the
certain event that happened in
aggregated form count in etc some joints
of this data data data streams or joint
of these data batches typical query as
we like to talk about the data so in
this case we see there is a projection
of the data we have our data set that
represents in the some table
I use this sequel because in many cases
even though it's Java conference people
from enterprise world have a skills and
knowledge about sequel because we all
brain washed during the years of
building the crud applications right and
for example here I have example of
aggregate now so we do data processing
and we're gaining more and more data
into the process the day that we still
will bring in more complex and complex
systems to do this more efficiently
remember when I said under assumption
that we have unlimited bandwidth and we
have unlimited reputation power but this
is assumption in the real world
there is no such thing as an unlimited
bandwidth and unlimited computation
power so this why dealing with this with
the with the daily scale we start
bringing more and more sophisticated
tools we not or we're not settled with
databases that can also support running
as a single node because not enough data
to fit in one single node you need to
have more data processing system needs
to support same vendor with dealing of
data now
when we're talking about distributed
systems and we're talking about this
tree which is world usually many many
compromises or many challenges of
building this tree but yet the data
processing system came from the field
called CP theorem CP theorem means that
for the system when we design the system
system can provide guarantees of
consistency data consistency system
availability and tolerance for network
partitioning but during design of the
systems we can have only two options so
it can be either consistent data or
always available system in case of
network partitioning so from perspective
of dealing with this kind of problem
there was a many ideas because CP
theorem is a real-life theorem and the
son of the things we still cannot beat
like speed of time etc and the network
fail is still happening we need to come
up with another another trade-off so
basically one of the trade-offs that
made Marg his author of critical book
art called big data and he also water
offer open source framework called storm
from his perspective combining different
open source solutions or maybe not only
open source solutions but combining in
into the is so called lambda
architecture will help build full
tolerant system for data processing plus
human air proof system so in this it is
actually very interesting block was
there's link to this and he his his he's
talking about design of real world
system that can solve the purpose of
being in consistency of the data or in
availability of the system right there's
a very interesting world of games right
so we know the asset is a term came from
the
the database world and the base the term
came from Nasik old world and do you
guys know what the asset versus base any
chemical engineers great interesting
fact though a totally random fact but
related to the champ chemistry so the
Dolph Lundgren the actor who was playing
rocky in rocky 4 he was playing Russian
guy I'm Russian guy he actually has a
master in chemical engineering so
totally random unrelated fact but I
thought the chemical joke will will play
the here well anyway so basically in
acid world which is a thomassie
consistency isolation and durability
this is things that properties of
typical databases typical sequel
databases they provide a thomassie of
operations consistency of the data and
isolation between operations and
durability of the data with the world of
distributed and no sequel databases we
need to ease this requirement so we're
going into the base which is basic level
ability and eventual consistency so data
eventually will be consistent because
there are multiple sources data coming
through the network and many things
might happen during this one and rise of
these systems like Hadoop that provides
the way how we can process big data
stuff essentially Hadoop became a core
of this lambda architecture however
there is a data point so lambda
architecture talks about two layers of
data processing your data my arrive
during the time through the multiple
sources and it might arrive into the
HDFS so when you think advantage you
first think about this picture from the
London subway when you saw a batches of
data so data growth into the HDFS were
and will be stored and after that by the
end of the day multiple Hadoop jobs will
execute pre computational tasks
that we sort result in a way that it
will be easy to queer a query in on the
servant rail serving layer is basically
the your UI or some sort of database
that can be easy query from the UI thing
but here's another thing speed layer
speed layer provides result faster you
don't need to wait for all data will be
available so in this picture to to mine
points one point is that the bench layer
will give us more accurate data but it
gives us takes us longer to get this
result streaming on the other side back
in the day when we talk about a lambda
architecture we're talking about maybe
2015 when when we talk about stream
processing data so this can give you
more faster result but less accurate so
this is why these two layers they have
this ability to merge either based on
the key and after that provide fairly
good results to to display a result can
be stored after that in the form that in
the with applied schema in stored inside
the database can be is the query now
here's a problem that will help so today
in the modern world when you're trying
to deal with this this picture looks
very nice right because many components
and you kind of understand you have a
Hadoop you have if you have a craft car
here the stream processing system but
you go into the modern landscape in
getting more and more different
frameworks and very get lost you might
get lost in all this like technologies
that that happens so we drill down a
little bit into details of of the lambda
architecture right now now basically
from all data we can create a view over
the time and it would be fully
generalized form right it's not it's not
sequel
to query but it would be query by key in
this case he might be URL if we're
talking about click click on URL in time
so in this case yet we need to get
information about how many clicks are
happening on the page and after that
there is MapReduce job that will just
create aggregated result over the time
so in this case the serving layer which
is a simple key value store that
available to to service data now so far
so good right however data is getting
bigger and modern the modern
participants in the business they want
to get the result much faster by the way
all this lambda architecture came up
with from web scale organizations
particularly storm was developed in in
Twitter back in the day so they taught
traditional enterprises that big data
not necessarily needs to be you know
slow data so this why streaming
streaming approach is getting even more
more and more interest now moving to the
stream processing so from perspective
for the stream processing right now
there are ways how you can process data
in the in the new real-time fashion and
get a very accurate result there are
couple architectural points that I will
talk a little bit later but you will you
understand why I'm talking about them
now and right now we want to stream from
many from many sources streaming can
work with changing capturing changes
from databases so our deep traditional
relational databases maybe turn out
inside out so instead of having updates
on particular table we can go level
lower and capture actual changes and if
you remember my couple first slides when
I was talking about the way how we need
to think about the data is rather than
not the set of updates but as a
it's immutable events that happened on
top of the data so streaming platform
should be able to capture and including
many many other many other aspects like
dealing with the big data tools so like
a reading from there like streaming to
it says and I think I think right now
it's no secret to anyone that Patrick
Raph could become in a a de facto
standard for being this nervous system
of organizations that allows to multiple
organizations to share the data to you
know exchange the data and this is this
is how I think everything plays out very
nicely now a little bit of going to even
more theory in the graph theory how many
of you heard about thing called that any
spark users that so dag is actually a
very powerful concept that it's actually
the explains or lays down the basis of
many processing tools so basically dag
defines the way how the data will flow
through to the processing task right so
in this case in the vertices we have
actual computation that happens and the
edges of the graph they define the data
flows so direct it means that it goes
into one direction a cyclic meaning it
doesn't have directed basically it means
that it doesn't have any any cycles so
data flows from one point to another
all right so but before I'm going some
technicalities and about if I'm going to
some of the theoretical aspects I want
to show you some demo and I want to talk
about what count how many of you guys
wrote the word count in your life I mean
if you guys wrote distributed word count
how many of you guys wrote like a
MapReduce job and work out the good
thing is that because the word count is
a hello world application
of big data so if you know how to write
a word count in the hadou you can put
the Hadoop on your resume and your big
data specialist now good thing about
this demo is okay let's start with bad
thing bad thing is that word count is
not distributed tasks how many of you
know this utility CC WC in the UNIX it's
working utility it doesn't need to be
you know Hadoop cluster to count words
however the word count to count words in
a distributed fashion is is a task that
not algorithmic is complex because
essentially what you need to do you need
to go through the stream of the data
when you see the word you need to meet
one you see another word you meet
another one and in this case you will
have this generalized form where we have
a key as a word and this value is a
appearance and after that you run the
reduce and the part of this reduce task
you just summing values for same for the
same key and sometimes the word count
used as a benchmark in all these big
data tools and stream processing tools
also because word count is not
algorithmic task it shows only
bare-metal where strength of the of the
framework or or a tool etc so there's
nothing like a complicated or there is
no algorithm to to cover so let's let's
talk about some demo so mmm so as I'm
talking I will be doing some of the some
of the code review and in this
particular case I will do outgoing some
presentational now before before I will
go into the stream processing thing I
will go with with interesting pieces of
my data now I need to start the
streaming platform I need to start a I
need to start a thing called Apache
Kafka to deal with this
shrim processing now the best way to
deal with we're starting Apache Kafka
we've dependency zookeeper remember this
guy who was how many of you guys see in
the the total sign in Philadelphia so
the guy was Charlie from gender now you
can deal with in the build this deck
like locally we need to bring zookeeper
we need to bring a craft we need to
bring the rest of the stuff but it's not
necessarily needs to be that way it's
uncertain needs to be that complicated
okay okay I'll do this bigger so you can
see so there's a there is a command that
allowed me to start all the things
together in this comment understands
that it required all requirements so I
will do have come up so the beauty of
this command this comment is allowed me
to to to run the craft way and Don's
about all this depends in this case
something is going on so I can do it
something is running on on my computer
always stop and I will do a destroy
which allows me to start start clean so
in this case I can do start so it will
start zookeeper zookeeper failed to
start zookeepers not running okay
something is going on so we will take a
look a little quick if I will not be
able to fix this in a moment of a couple
minutes so in this case I will carry on
and I will send you a link to the live
demo after it's also very convenient
that this command allows me to yeah
looks like a zookeeper is already
running which is was expected
so let me kill zookeeper somewhere is
you know Kiki did you do okay intellij
okay tell Jay all right so let me finish
the slides if I have a time I will
return to them because I don't want to I
don't want to slow slow these things
down
so basically idea oh no I can't really
can kill show some code even though I
will not be able to run it idea is do I
will use a very popular in Russian who
called war and peace which is really big
and that might mean I can consider this
as a big data application because I'm
processing
war and peace it was like three or four
different books so a lot of words to
count and I push it through the Kafka
topic I will have another job that will
listen this topic and will do processing
so I hope because it's a job on it guys
enjoying writing Java so you will be
happy to see that this stream processing
tool that I wanted to demonstrate here
which is called Kafka streams allows me
to write very Java developer friendly a
code in this case it's very it looks
like you do the extreme stream
operations in this case in this
particular case I connected to the
broker with messages consuming all
messages and I created the concept of
the creating extreme if you remember
when you have a Java collection you can
create a stream from collection so
stream with abstraction does allow you
to build all this data processing
pipelines if you can say in this case it
works absolutely same way so I do have a
stream and I can do similar operations
that I can do with Nigel two streams
right so in this case I do the flat map
values so in this case I want to clean
up of my you know the garbage that might
arrive there so because every so because
the data arrived into the Kafka
we won one sentence like one line at a
time so I need to do split to create a
stream that will be represent actual
words so in this case I will have a
stream which contains now as a key I and
the value as a word I'm passing this
through the the mapping operations the
map and iteration here we'll just simply
change the appearance of this world so I
will do it lower case I will do clean up
clean up is a simple function that will
use irregular to figure out where is all
these like words coming now and after
that it is very important operation here
select key that allows me to transform
stream in from the form null key word
into the form I have a word as my key
and something is a value and after that
I will just do group by key that will
give me actual grouping grouping result
and I will place the counts that
essential result of this operation into
another topic so cool think about this
and how this thing is different from
Hadoop perspective that data might
arrive constantly without end and this
job will continue to run and process it
in updating result as as we as we
receiving so the framework the Khafre
stream framework it operates not on a
micro batch style were you waiting for
you know 10 entries have to do the
processing but it's actually based on
per event so when the new message
arrives
it runs this pipeline immediately and
updating result in result topic and this
is pretty much it that you need to know
about the Kafka stream and stream
processing operations no cluster as you
can see I'm running this application at
least I try to run but failed miserably
beauty of this you don't need to think
where your code is executed because your
code is exactly
exactly on when your application
executing dreaming conquer streams uses
Kafka as abstraction and also uses Kafka
as a store for state so in case of I'm
running two jobs two parallel my
execution one of the job might fail but
state of the of this of this job will be
stored in the Kafka it can be retrieved
next let me quickly run through the rest
of the slides and I will try to run the
demo once again now speaking about fault
tolerance that's exactly what what I
wanted to talk it now I told you that
distributed processing is a very
complicated task it's a complex task and
involves multiple aspects many aspects
might fail so when you're running
cluster of brokers of Kafka you're
running next to the broker cluster of
SPARC jobs for example and in this case
you introduce in more and more moving
parts into the into the failure but when
you have one place to one thing to you
know care about in this particular case
broke the cluster of blockers your
streaming application can fail you can
restart and it continued from where it
was left off because data safely stored
in crafting Kafka known as a is a
durable storage that can it's rock-solid
and battle-tested because data is
important right now when we want to go
from one one one node to another like
from the one note into distributed
processing see say we want to have a
result even faster so we can I would you
real-time calculation of of of some
sales when we do Black Friday
type of event and the data constantly
want to see it on the dashboard we can
bring more more application to scale
this out and each individual application
we have its own slice to to deal with so
after we're done with with processing
when can easily scale it down and data
still will be
available in process now interesting
fact that because there's a term of dog
fooding right but French people like any
French people here so French people not
saying this because they fancy and
declasse they prefer drink their own
champagne much better than eating your
own dog food right
so the kafka streams relies full on the
kafka as a state store in the store of
the overall state of the application so
there is a special topic called
changelog that also maintains the data
in case of failure so each application
don't need to directly talk to each
other so Kafka has this concept of
consumer groups which is built in into
Java Java API Java client which is
foundation for Kafka streams framework
basically Kafka streams built on top of
Java Java library and based on this
concept of consumer groups data would be
in case of failure of one of the
consumer from the consumer group data
will be rebalanced and handled by rest
of the consumers and the consumer groups
in this case data will not be lost now
how to deal with the how to deal with
the infinite data how to deal with the
situation where data arrived and always
constantly arriving and arriving so I
want to I want you to think about the
data when I was talking about set of
events but this event might have
different time model so the frameworks
itself many frameworks usually right now
the only major and the major players
like fling Kafka streams hidden cash jet
and other framework they do support same
like timing model they support like
actually I was initially introduced an
Apache beam which is a Google data flow
open source version or open source
version of this basically have it your
vents in the stream they have a
different time cement if there is an
event when the event actually happened
and sometimes you care about this one
and you care about this one because you
don't hear when the event arrived the
system you need to know when this event
happened so you can also process late
events how to process night events I
have another slide in a moment now a
processing time is easy right so when
when we start crossing time we get this
we get this actual time so the event
time and processing time is different
and knowing event time help to build
help to process late events now how to
differentiate how to differentiate
between the event time and processing
time how many you guys have seen the
Star Wars are you guys excited for new
Star Wars coming this year I found my
mans here alright so we know this there
is official timeline of episode right so
the in the history of Star Wars on the
Indiana we have this timeline that the
first episode that is the earliest one
many things happen after that attack of
clones Revenge of the Sith and then you
hope etcetera etcetera so we know this
the stream of events happened in this in
this particular order how many of you
guys seen my shitty order for for Star
Wars just there's something that
important that you need to do after this
talk you need to download the play
around with Kafka streams and find what
the means now my shitty order for Star
Wars it's amazing
that basically Phantom answer never
never existed
alright now event time processing time
though so we know that the Phantom
Menace was released after couple years
but like like 20 22 years after the
first the new hope was released so in
this case this is processing time this
is how
how we as observer of this stream of
events how we process this one right but
in general historical historically and
the events that happened in this
particular order now how to deal with
convenient data so basically we need to
talk about thing called Windows which is
a representation of infinite data now
windowing is a special operation that
allow you to put sort of boundaries on
the stream of the data I guess in this
in this particular case Windows is
something that business or like the
requirements of the system need to
decide what what the windows needs to be
so there are multiple type of Windows
there's fixed Windows like Windows from
1:00 a.m. from 1:00 a.m. to 2:00 a.m.
you have a sliding windows like when you
have every five minutes something is
happening or their session wins for the
different user sessions in the system
there might be different type of windows
that run that contains this data that we
need to deal with now we have a multiple
events come into our system and based on
the event time event time we can build
order basically or we can restore the
water because they some of the events
might have out of the order even though
Kafka provides ordering repair partition
were basically ordering / / one one
slice of data in this case we can
restore order based on the smart
configuration of the windows now in
terms of windows in dealing with stuff
so there's interesting interesting use
case like when you play in with some
game you have some progress and you
collect in this progress after that you
hope one on the plane and you don't have
a wife either
or at least you don't want to pay for
Wi-Fi to play of the game however
you still continue to play with the game
so you're accumulating these events and
the system that runs and your your phone
is actually collecting this the event
time event information now when you hop
off the plane you already online in the
system sent this process the progress
that you did when you were in line when
you were flying so in this case your
overall online stats will be updated so
in this case we're talking about dealing
with late events and processing out of
order of data so it is a very common
practice and this is something that you
might consider in when you're dealing
with the shrimp shrimp processing
framework there's something that they
need to provide in this but your case
Kafka streams provides this capabilities
again it's this example that we just
discussed with the with the plane now so
quickly a quick result of stream
processing it's possible to do a
real-time computation of infinite stream
of data so it is achieved by applying
different type of windows like you can
get result within five minute window 10
minute window but you need to you need
to think about this it is something you
need to take into consideration because
the size of the window will directly
affect the performance of the system you
will need years more memory to keep this
window open so the new event will arrive
and processing will continue to to to to
process the windowing operation this
this kind of stuff will also involve the
state so for example storing local state
of the jobs done in the kafka streams by
dealing with so-called rocks DB which is
database which is has in memory
component and it has file component can
overflow the data that sort in the
memory into the disk so if job will
failed the application will fail the
next time it will bruise
so this state also replicated it with
Africa tropic in case of you lose this
not completely so yeah this is and you
as a requirement for your application
the as a result of this application you
need to choose like what how long to
wait and how much resources you wanna
you want to share for keeping this
window open now I do have we do have
this set of examples of kafka streams
like I just touch very very very little
I just showed you very small thing that
Java developers can easily understand
because it's it's very simple it's
basically Java Java 8 API that brought
into the streaming streaming fashion but
we do have all also more complex
examples and I really like you guys to
go there and check this out so three
things in to do you need to go and play
around with graphical streams again
examples and what third thing was my
shitty order for Star Wars it's amazing
now I'm sorry for a failed failed demo
but I'll take I'll take some questions
if you have questions but you're not
ready to ask them right now there is my
email on it is my Twitter you can always
send me a tweet and ask about some stuff
related to Kafka stream process against
in yes please
yes so question is that the Kafka I will
rephrase you a repeat your question
so you asking about libraries that
allows to connect Kafka to external
service sources so there is a framework
that is quite a part of Apache crafter
it's called Kafka connect Kafka Connect
allows you to have all the shelf
components that you can you know start
and stop during the day and connect
without coding to any sources you can
you can read the data from database you
can push the data to some some messages
from another system you can read data
from Salesforce and push data into HDFS
so all this Hindle by Khafre connect and
yeah so the this is a part of Apache
Kafka framework so as a part of the
confluent platform we provide like
certified the connectors meaning that we
did the testing either we develop them
or we some of the partners developed and
we tested them and they also the battle
proof code that you can use in your in
your application yes please
so if you have out the water events how
you put them together back so first of
all you need to know that if this event
time that you have a metadata in this
event will fit into the window because
you know you want to make sure that the
data you actually waiting for the data
if not this data would be discarded so
there's a ways how you can preserve
first of all like the timestamp that
allows you to figure out this event time
in terms of ordering it's just only only
one thing that we can use to figure out
we actually need this but in general in
general like if it fits in this window
that you define like this timestamps it
will be processed and usually it it
might not change much in terms of result
if you do like a grouping or if you like
aggregation type of tasks so yeah this
is this is something that answer your
question yeah
yeah so here's yeah so the cop
internally the the the topics consist of
partitions and the Kafka guarantee is
ordering her partition so if you have
you know keys and the keys belong to
particular partition and that data will
arrive on this particular partition and
the data will be in order
yes thank you so much guys again sorry
for failed Emma but I can that with this
have a recorded which is awesome thank
you so much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>