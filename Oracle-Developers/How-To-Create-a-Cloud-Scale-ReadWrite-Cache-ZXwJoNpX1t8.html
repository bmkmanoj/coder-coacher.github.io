<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How To Create a Cloud Scale Read/Write Cache | Coder Coacher - Coaching Coders</title><meta content="How To Create a Cloud Scale Read/Write Cache - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>How To Create a Cloud Scale Read/Write Cache</b></h2><h5 class="post__date">2018-02-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ZXwJoNpX1t8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay we'll get started now so we have a
very elite audience here so I'd really
like you to ask questions if I just talk
and you guys don't understand or you're
missing the point please just ask
questions have benefits all of us so
just a little show of hands how many of
you are developers okay and DBAs okay in
yourself excellent okay so I'm gonna be
talking about how to create a cloud
scale database service so this was
actually based on one of the largest
cloud companies you've all heard of them
you've probably used their products and
services I'm not allowed to say who it
is but it's it's one of the big ones so
I've sort of generalized it because of
there's a lot of intellectual property
that they didn't want me to talk about
but I believe a lot of the techniques
are applicable to other cases what
you're doing may not be cloud scale but
it should work
so my name's doc hurt I wit Oracle in
the middle building I have two roles one
is a consulting member of technical
staff that means I write C code for the
database kernel which is another way of
saying I create bugs and my other job is
a product manager which says create new
features and why don't you fix the bug
so I've sort of got both sides of it so
in my talk I'm gonna start really
high-level it's gonna be baby stuff and
you're like that's really simple
hopefully you come out of the saying I
already know that well that's simple
well that's obvious because that's the
point that I'm trying to show I'm
showing some new technologies and ways
of doing things but hopefully this
should be no surprises the trick should
be
it works at very large scale so very
briefly I'm going to be talking about
the business problem and some of the
develops challenges that we came across
but gonna spend most of my time talking
about different architectural and
technical challenges that the cap
customer currently has and then showing
how we apply it with some different
techniques if we've got time we'll go
through a demo so first of all the
business problem it's really really
simple to define they want to go faster
and they want to spend less money time
is money
customers want a good experience they
don't want an hourglass waiting for
stuff so their wants are faster when
you're a really large company and you've
got customers in virtually every country
and you work 24 by 7 you've got lots of
machines if you can do things more
efficiently and use less machines that's
going to cost less money so whether you
are hosting it and paint some cloud
provider less money or whether you've
got your own data centers less cost less
if you got your own data center they
cost money cost power up to power
machines and air conditioning it's real
money so if you can deal with less
machines that's a good thing so the
business problem is really simple
for the DevOps challenges you know this
company has been around for over ten
years they've been there they've done it
they've had mistakes what they don't
want the surprises they want boring they
want to know how much CP is used at all
time how much disk IO how much memory if
something goes wrong if a network card
goes down their disks blow up if DNS
goes down they want to know what will
happen and how you react so they want it
to be all deterministic because if
everything that just works and they know
what to do there's no surprises so even
if bad things happen customers get a
good experience failures will occur that
you want it to happen in a graceful way
and things to just keep going they want
everything standardized be it running in
containers or vm's because it's just
their infrastructure they're really into
the DevOps mentality everything's
automated and basically no surprises so
hopefully none of what I've seen is a
surprising but when you're doing it at
scale 24/7 and you just never want
things to go down you really got a plan
ahead so some of our architectural
challenges this is unbelievably simple
I'm gonna drill down and make it
incrementally more complicated so this
is where the audience participation
comes up how many users do we guess this
company has you sir give me a kiss
someone give me a guess
okay yeah 20 bit more okay let's go for
about a billion okay
so if we've got a billion users
how many concurrent users can you have
in on Oracle database I've got some DBAs
here do we have any answers
the Oracle experts and when you
concurrent connections to an Oracle
database can you have how many 2,000 but
more than that any more guesses okay
it's more than 2,000 but it's less than
a billion so the point is you can't do
it with one Oracle database so that's a
challenge if you've got a web server or
an application server how many
concurrent HTTP or SSL sessions can you
have anyone want to give a guess Kat's
it's not that meaning you're lucky if
you can do 10,000 so if you've got a
billion users
you only have a subset of that connected
any one time so anyone who I guess yeah
that's a little bit on the high it's
like yet I mean whether you have 10% or
1% or 0.1% a fraction of a billion is
still a very large number so the point
is when you've got a huge number of
users you got to approach the problem
appropriately so the point is having a
web server in a database
ain't gonna cut it there's pretty
obvious people figure this out 20 years
ago we're just going to incrementally
look at some of the issues and how we
can increment improve things to make
things faster to make them more highly
available to have da to have lower
latency and ultimately to minimize the
number of machines so you can't scale up
doesn't matter how big the machine is
there's never enough CPU there's never
enough memory it's going to be a single
point of failure anyway so you gotta
scale out horizontally so ultimately
you've got to have load balancers you
could have routing to point the traffic
where it needs to be so I'm not going to
talk too much about the application tier
routing because that's not owned by the
product that I use all the consulting
that I did and as all other customers
intellectual property but let's just say
they have got very sophisticated load
balancing and routing the point being
requests that can come in can be routed
appropriately to a large number of
application servers whether it's the
Tomcat application server or an
equivalent one is kind of irrelevant the
point is it's an application server
that's serving HTTP over SSL and there's
lots of them so we're scaling out
horizontally likewise with the Oracle
database we know we can't run it with
one so we gotta have meaning so this
customers sufficiently lodged that over
ten years ago they started doing
shouting not the Oracle product shouting
that was before that existed they
doing their own custom charting so that
works but you're going to be very
sophisticated in the mid tier to design
it to make it work so we can scale out
the hot Dora core databases by shouting
but the responsibility is on the mid
tier and the application servers to
route accordingly okay so we don't have
each oracle database as a shard but
that's them a single point of failure so
they need high availability for each of
those shards so they're using an
existing Oracle technology the using
Oracle active Data Guard so it's
something you've heard off something
configured but the point is you've got
mini mini shards each one of those
shards is hardly available but because
this is a worldwide thing what if the
site goes down what if a data center
goes down what if you've you've lost you
you you know yeah you Trank network
connection so they've got multiple
availability zones or availability
domains within a region and they've
spread their oracle databases across
those and the boss has spread it across
regions so you've got intelligent
routing to give the data where it needs
to be so the pointers were scaling
horizontally in the mid tier and in the
data based here and it's clever
application logic and the mid tier
that's doing the routing so this is
really easy to say and it works but it's
not the most cost effective way of doing
it if you do the math it takes tens of
thousands and machines to do this so
we're just going to look at techniques
that you've probably heard of that will
achieve the same goals but will use less
machines less machines less money if
you're a public traded company you know
if you can have your app going a little
bit faster
you might make some more money but if
you can use less machines you'll have
less cost slight increase in revenue
less crossed more profit
pretty simple if you're calm
stated on availability missile aids and
reducing cost this sort of stuff is
really important so you know this is a
rocket science instead of using
dedicated connections to your database
let's have a connection pop so instead
of tying up a database of connection a
sequel net connection each time this
just used them as needed this isn't
rocket science you know people been
doing this for 10 20 years so each
application server has got connection
pulse traditionally a sequiny connection
you'd have a dedicated process on the
Oracle server a shadow' process so let's
literally a operating system process per
connection that works it's fast but it's
expensive because it's the process
that's using app memory so using up CPU
if you configure shared servers or
multi-threaded server you can have one
process with multiple threads in these
sharing sockets so it's less memory less
CPU none of this should be rocket
science by default you're going to be
using shared servers anyway so an
earlier question I had was how many
connection concurrent connections can
you have to an Oracle database server
since the living toy for with shared
servers you can have about 64,000
concurrent connections per machine now
64,000 is a large number but it's not
magic if you've got 16 cores and 64
thousand concurrent connections if all
of those connections are trying to do
work at the same time magic does not
occur by definition if you got more
workers than available dispatches you
wait okay so
the more work you can do per machine the
more cost-effective owners but at a
point you have to wait so the cost is
latency so you can app concurrency or
the cost of latency if you don't care
how long things take it doesn't matter
but if you want it to be fast and have
high throughput and minimize cost that's
a challenge so you got to do some more
tricks so any questions so far this
should all be pretty basic stuff okay if
you've got a huge number of connections
going to your local database like you've
got mini mini shout at our core
databases and each of those databases
has 64,000 or actually more connections
concurrent connections to it he is
stressing the Oracle database network
connections are cpu intensive then you
normally heading desk and then more
network to get it back so it's CPU and I
about if you have very read intensive
work load you can offload work from the
Oracle database but using it cache in
the mid tier not rocket science so the
point is the simplest cases if it's
read-only you load data the relevant
data from the arkward database into the
mid tier and you serve up the reads from
the mid tier if you can serve all the
reads from mid tier you're not stressing
the Oracle database for CPU or disk so
that's good there's a lot of ups there
so the point is if you're doing caching
you're going to be smart about it's a
cache how often is that cache refreshed
if it's a day old
if the data is stale for a day and you
need it to be updated every second it's
kind of useless so it's a balance
between the liveliness of the data and
the stress that you're putting on your
aqua database
if you're getting new data from your
database every millisecond there's
probably more work than if you weren't
caching so it's a balance that's for
read catch that's easy but if you're
doing write caching you could have
pushed things back but you also got to
worry about consistency if my server is
doing a write and another episode is
doing the riders for the same data maybe
I've just created a consistency problem
so the point is it's more complicated
than you think so what this customer is
done is looked at their architecture and
said we need faster reads we need faster
riots it's technically cheaper for them
to have more cheaper mid-tier machines
then back in database server machines
this is just a cost thing and for the
last few years they've been using
various different no sequel technologies
to do that so they're not using one no
sequel technology they're basically
using all of them because each of these
different no sequel technologies have
the different sweet spots some are good
at some workloads or some use cases some
are good at others so because this
customer owns their head and knows what
the workload is and knows what the pain
points are they're specializing mid-tier
sequel caches well data caches based on
what they thinks the most appropriate
technology so they've got a lot of these
different technologies some connections
go straight back to Oracle some of them
go through these other mid tier database
caches so hopefully this is all pretty
simple stuff here's where it gets
interesting so any any questions so far
okay so this customers been around for
about 10 years I've got all these cool
no sequel go fast stuff but they want to
go faster and they want to use less
hardware
so when we talk about going faster we're
talking about two different things
there's latency which is how long an
individual operation takes basically the
round-trip time and you're talking about
three pod you got to do both but what
they're very fixated on is latency
because for this service level agreement
they want things happen really quickly
for business reasons can really go on to
what those are but it needs to be done
really fast so when you're looking at
latency you've got you know statistics
things minimum make some of me they're
kind of irrelevant for service level
agreements what really matters is the
95th we'll the 99th percentile it's you
know virtually any technology can have a
good minimum number even a good average
number the max number does always
outweigh as bad things can happen so
instead of looking the worst case if we
look at the 95th of the 99th percentile
that means 95% of the time the time it
takes to do something took this amount
of time or less or 99% of the time so
this customer I was working with was
very fixated on the 99th percentile
latency so all these different
technologists just just hands up and you
guys using Cassandra okay MongoDB
cool Couchbase cool read us kill you
know model times did you work well TB
and or a spike okay
so fear if you have used these some if
you've used multiple other so hopefully
nothing I'm saying is to out of the line
they do stuff they oh well most of them
are quite simple which is good most
similar ball but most of them need mini
machines to work or to be scalable
it's just a fact they a lot of them are
no sequel basically name value Pierre
type technologies like Volta visa sequel
database when they used as a cache
normally you need to be writing custom
application code to synchronize from
your source database be an Oracle or
sequel server on my cycle whatever and
these technologies so quite often
there's a lot of custom code involved
also like how often is the cache kept up
to date what is the cost of doing that
how often is it refreshed what if you
know the Oracle database has got the the
source of truth and you catch me down
and the cache comes up again let's catch
went down for a second who cares the
cache went down for now and the cache
came back up again are you dealing with
sterile data or is it automatically
being refreshed these are really boring
things that really matter when your data
needs to be correct so the point being
this customer has got all these
different no sequel technologies they
work very closely with the vendor and
they get good performance good average
performance what they don't have is good
99th percentile performance some some
cases it's taking as much as 324
milliseconds which may not sound a lot
but for this customer it's big deal so
the 99th percentile is about a third of
a second the maximum value is huge ten
seconds twenty seconds they they cannot
you know be in business with that time
so they've got built in time as if it's
over a certain threshold give up try
again another route so the point is
performance is okay on average that the
not the worst case the 99th percentile
is far more than I'd like
okay you guys have used these technology
in your net supposing you guys have used
some of this technology as anything I've
seen surprising okay so we just need a
way of comparing these systems for you
know for modern databases in the cloud
there's a beach Mac that a lot of people
refer to the the Yahoo cloud sitting
beach mark so it's a pretty simple
workload it's got a bunch of different
use of cases A through F for this
particular customer to read intensive
application 95% reads 5% writes so the
point is like Cassandra scales really
well it's basically got linear
scalability but each node doesn't go
very fast so these are published numbers
that the vendors have seen so for
Cassandra you know to go just overturn
2000 K per second transactions per
second that works but at X 32 machines
with Mongo they've got slightly better
numbers with less machines so that's
good so if you go down the list they've
got different throughput rates with the
corresponding number of machines so I
mean Redis is dead simple technology
does what it do really well it goes
quite fast you know it's doing about a
million transactions per second on a
machine good but the point is there's
other technologies that go faster
like volt DB or aerospike so what we're
doing here is we want to go really fast
and we want to do with Lisa machines so
this is all the technology that the
customer has today so I work for Oracle
let's see come on how can you do better
and look alike we don't think you can we
know we know everything so see ok we can
do that same benchmark I work 4 times 10
for that same beach backed with its own
configuration we're going a lot faster
2.8 million transactions per second
one little machine so at that point yep
yep
yep
and what you say is absolutely correct
some of these technologies persist some
of the remit and memory I'm just going
on published numbers what I really care
about is the comparison my product down
the bottom is in a memory database but
it also has acid transactions persist
everything to disk so it's really hard
care apples to apples comparison this is
the best we've come up with okay yep yep
there's there's hundreds of machine
hundreds of different technologies I was
just trying to come up with a
representative list but more
specifically the technologies that this
customer is using yeah I mean memcache I
mean there's a lot of technologies in
the last you know seven years everyone's
doing the same thing the point is after
a while they all start looking the same
the point being the technology I use
times 10 it's faster than all those
other ones and uses arguably the least
amount of resource so it's a starting
point as a building block so there's
lots of different ways you can architect
it how I would do it but the customer
said I don't really care about there
we've got an architecture I don't really
want to change my architecture make your
database fit an architecture like
everything else be like everyone else
use their app servers use our routing be
a read/write cache what's it fine so the
pointers their applications some of them
are always going to don't go directly to
Oracle and other ones are going to be
routed so we're just going to be another
mint here database cache thing so with
this yep
yeah
yeah
absolutely that is a brilliant comment
and then as effect so the point is some
of the machines need a cluster to get
the throughput some of the machines go
fast on their own you do not want a
single point of failure
it so happens with this architecture it
doesn't matter because it's scale out
there's multiple copies and to keep
things really simple the customer has a
copy of the entire database the data of
interest and every single copy so if
you've got are making up a number 200
machines have 17 of them go down we've
still got the exact same data unload as
other machines
to two things if you have a data grid
and you spend a lot of time chatting
amongst yourselves that's a way of doing
it and if checking amongst yourself
slows you down it's not ideal what the
customer wanted was a scale out she had
nothing architecture so by definition
there is zero chatting between the notes
because instead of taking the entire
data space and partitioning and OVA
machines which requires coordination or
chatting this saying no the amount of
data that we care about is big enough to
fit in one of the events therefore they
have a complete copy of a mini machines
therefore they do not need any
communication or chatting between the
machines so it's the perfect bailout
scenario so what you say it's true
but it doesn't apply in this case
not in this case no okay great question
yep yep perfect question this next slide
so I'm gonna have answer that so in the
first they have many different use cases
I'm looking at one particular use case
and then I've gone to the writes in a
bird in their first use case it's a
hundred percent reads no right because I
their application because they own it
they can route reads to here and writes
to them if you've got a package through
party application you just can't do that
because they own all the code those 15
years doing that sort of trick anyway so
for this particular customer they are
looking at at a use case at a time and
this first use cases for read-only so
I'm going to get to rights in a bit so
the point being they expect for the use
case to head between 99 and 99 percent
cash at ratio which is a good thing the
bigger the cash up ratio the better wean
you get a cash mess I the data of
interest is not in the cache but
definition you will someone needs to go
to Oracle to get their data back for you
so they expect a very high cash at ratio
they did lots of tests like we've been
working for over year with them like
we've done all the functional tests
we've done all the stress tests we've
done all the performance tests because
they don't want any surprises so they go
back to 97% cash at ratio to force more
things what they needed was a 99th
percentile latency of 1 millisecond so
if we look at that and practice
hi tick 87% of the time the round-trip
latency was 16 microseconds that's point
zero one six milliseconds
ninety-eight percent of the time he
doing it about point one of her
millisecond it's not 99 the 99th
percentile at 60 worse 99.7 percentile I
was taking one millisecond so pretty
good numbers so as a comparison there
existing you know then the Cassandra the
Mongo the 99th percentile latency was
way higher you know hundreds of
milliseconds so they were over the moon
about this they were very happy did
greedy they could have said stop but if
the original is if we turn a little bit
more and we improve the latency even
more they can save more money
it's literally profit sorry
profit for them bonuses to the
individual so we're we're getting more
so the point is as of today our worst
case is about 423 milliseconds for the
absolute worst case 99.9% of the time
the worst case is 8 millisecond so the
next couple slides we're going to show
how we're looking at improving them so
you can never guarantee what the worst
case will be because if you forget all
as caching f the Oracle database is not
broken and everything's going good
it takes about 15 milliseconds roundtrip
but you know the world can be against
you what say your process is not
scheduled on the CPU what's a a disk
flush just occur what's a you know
someone else was downloading a video on
the network a combination of bad things
can line up to make the worst
secur so we're targeting the worst case
to be less than 50 milliseconds if it's
over than that the customer can just say
that's my threshold that's taking too
long give up or try again and another
technique so the point is we're already
significantly better than all those
other no sequel databases 324 versus one
millisecond for the 99th percentile so
what we're doing at the moment for cache
misses is not optimal and that's why we
were seeing the performance if we kept a
cache mess we go whoops the data is not
there let's connect to Oracle get the
data and update the cache the problem is
connecting to an Oracle database takes
time it's not instantaneous you know it
can take milliseconds if you've got
64,000 concurrent connection and that
Oracle databases totally cpu-bound
spawning another process or spawning
another threat can take time and the
time it takes to do that
as more an operating system scheduling
problem than a database problem so
instead of forcing a new connection
which is expensive we're saying let's do
what we've done before let's use a
connection pool so just like they're
using a connection pool to the Oracle
database for their mid tier we want to
add another connection pool not for the
mid tier but specifically for the times
ten cache as a way of doing that and the
technology we're looking at using for
that is Oracle database resident
connection poll so that's been around
since about 11.1 you specify min and Max
connections all sports spawn up those
connections for you so the point is if
you need a connection instead of
spawning a process you're requiring a
latch getting connection out of the pool
getting your rows returning the
connection to the pool
so if there is free connections in the
pool it should be really fast if you've
got 10 connections in the pool and
10,000 people trying to hit that pool
you're going to wait so if your
connection pole is too small you wait if
your connection pulls too big so you
make it 50,000
you're wasting connections given that
they were already using over 64,000
concurrent connections they can't just
add another 50,000 connections to their
pool so they're looking at what's the
smallest pool that's useful that won't
make me wait so it's a tuning thing
which you need to basically empirically
measure make sense certainly none of it
rocket science but the point being
we've gone from a whole lot faster than
all the competition till we want to go
even faster again for the worst case
okay
so I've talked a lot I've made some
outrageous claims that was sort of the
architectural stuff now I'm going to
show you how we did it so first of all
fill your mid tier technology for the
cache use something really fast
we used the Oracle time 210 and memory
database and explain more what that is
in a bit for your cache database make it
as simple as possible don't require the
customer to write tens of thousands of
lineups of code to integrate and
synchronize caches if you can do that
out-of-the-box declaratively it's less
work
also you cache you want to be smart
you're caching data from a relational
database if your database cache
understands relational databases
understands what hierarchies what
primary foreign keys are and can keep
those relationships and makes it easier
for the application so the point is if
you can do out of the box means you
don't have to code it in the mid tier
finally there was a little bit of
hardware and software tuning it's not as
bad as it sounds I'm gonna go into the
detail but disk i/o is something you got
to be careful with we either needed two
disks or a SSD not a big scene and
renée's are a huge storage array two ide
disks or a flash disk but more
importantly in the code we had to do
some pretty basic stuff none of this is
specific to x team whether using oracle
or my sequel or db2 or sequel server
you've got to do the same thing same
sort of things
prepare your statements and use bind
variables update your statistics if
you're doing other queries figure out
what indexes you need we were doing
joins don't do catagen joins you know
hello we're klaus based on the keys
really basic stuff and also you need to
know where your bottlenecks are so we've
got a lot of counters metrics so that we
can see as the bottleneck and the sequel
as a bottleneck on disk i/o there's a
bottleneck on networking so it's going
faster data it's not going fast figure
out where the bone occurs in tune
accordingly so there's a lot of things
that's very generic we ended up using
two disks added a couple of indexes
updated statistics and that was it
so I'm talking about you times 10 it is
the most popular in memory database that
you have never heard of it's used by
thousands of customers but they embed it
so virtually all telecom providers in
the world use times 10 so for instance
just air first and the top left Ericsson
over 2 billion people in and just that
one company alone are using times 10 so
basically if you've got a cellphone if
you go smartphone if you've got a flip
phone if you talk on that you're using
times 10 between one two three times
somewhere in the stack it's not running
in the phone that's running in the back
office so telcos stock exchange banks
lots of different people are using it so
what is that it's a relational database
and use a sequel and Peele sequel it's
got acid transactions and all of the
data's and memory with Oracle on my
sequel all of the data is on desk and
you load a subset of that data into
memory and data blocks move to and from
from memory and desk it's called a
buffer cache times team doesn't have a
buffer cache all the data is on desk but
that's the persistence all the data is
used in memory we don't have a buffer
cache because if you analyze the
microseconds buffer caches just slow you
down if you assume the data will always
fit a memory you can use simpler
algorithms simpler data structures and
go faster
the cost is that's all gonna fit up
memory okay so even though it's all a
memory we still persist everything to
disk
we're still a transaction logs for redo
and undo we're still got cheat point
files for the entire thing so if you
think it's an Oracle database so you
think it's my sequel it works the same
way if you pull the plug plug it back in
and come back up it'll recover to a
transactional inconsistent point just
like an Oracle database
so functionally it's just like Oracle
performance-wise it's a whole lot faster
question
um
yeah I got some pictures to cover that
say okay so when I say it's fast we
major sequel operations and times teen
and microseconds not milliseconds
microseconds so because we're not
waiting for dusk and we're not waiting
for the network what we wait for is the
CPU so what that means is CPUs with
faster gigahertz or bigger l3 caches go
faster so if you look at these stats
this this is from about five year old
technology that's Broadwalk until as
well as newer until skylake is even year
again so basically the faster the CPU
the faster the database goes okay so the
point is we can do for trivial primary
key lookup selects sequel selects we're
doing that and under two microseconds
updates take a bit longer because
there's more work going on but it's
still a very small number of
microseconds so because we can do
individual operations really fast means
we can scale and we can have really low
latency so here's an example where we've
got an Oracle database with and without
times 10 as a database cache so it's
exactly the same schema it's exactly the
same data that's exactly the same
application the same JDBC application
connects two times two nor Oracle you
just pass in a parameter on the command
line going directly to Oracle it's fast
but going via times 10 as a read/write
cache it's a whole lot faster so the
name the name times 10 is a marketing
term with an Oracle database if all of
your data is loaded with the SGA when
you don't reads or writes you're going
to memory you don't have to go to disk
because all of the data is in memory
sure it's going to be flushed but that's
not going to slow you down
so the perfect case for an Oracle
database is win all of your data blocks
on the STA so in the best case for an
Oracle database we claim that times 10
will be about 10 times faster and if you
look at this benchmark this workload you
know it's not going to be 10 times fast
it might be twice as fast it might be 50
times a faster it really depends on the
workload because it's a ratio so the
pointers even the best case for Oracle
database Oracle times team we claim can
go a whole lot faster which is why
Oracle acquired times 10 but over they
quite them in 2006 so about 12 years ago
really cool technology they bought the
technology for that low latency so we
talked before about desk all of the data
is in memory the transaction logs the
video and undo go to a file files in the
file system
a cooked file system as a txt 3 is a txt
4 is it you fell favorite file system
don't care it just needs to be a cooked
file system we also have the entire
image of memory being written to disk so
we periodically write the memory to disk
you specify that via time or by a volume
we just write out the dirty blocks we're
also running out redo and unlock under
those things are both hit me if they
both happen at the same time and you got
one disk you're gonna get disk i/o
contention because disks it doesn't
matter what the disk is has a third and
certain maximum throughput rate there's
a 50 megabytes per second as a two
gigabytes per second if we're writing
more than that at the same time you
gotta wait so normally you want to have
two separate devices or a SSD or an nvme
disk which can handle with robot
so how does it actually work in practice
it's not magic
down the bottom here logically we got
some data model an Oracle that we want
to represent in the mid tier you don't
have to cache all the tables he cashed
the subset of the tables that you care
about you catch the subset of the
columns that you care about and you
catch the subset of the Rossen that you
care about so even though you could have
petabytes of data on Oracle if you only
need a small subset it's less memory
less goes faster so how do we do it
Oracle database there times ten
relational database there we've got two
agents or gateways or demons
one of them has pushing changes the
others writing changes so the pointers
will go to databases and some gateways
to do it the kliebert has worked some
metadata to figure out how to do that
the metadata uses things called cash
groups sir cash groups is an object
which defines relationships we specify
those cash groups as objects and sequel
so an our sequel we can say things like
create cash groups we specify schemas
and tables and columns you and that's an
object you can have multiple objects so
if you've got thousands of tables you
can have thousands of cash groups cash
groups can have more than one table of
them so you specify the columns you can
specify all of them or a subset you can
see here we've got a where clause
so you're filtering the rows so we've
got metadata defined in sequel that
defines the tables of interest
that metadata gets manifested in times
10 as tables plain old tables which
means you can join against them you can
search against them
you can insert update delete or merge
against them so it's just a relational
database with tables so how do you do it
you can do it from the command line via
a script or where a GUI I'm just going
to quickly show you in the GUI you
create a cache group you're specifying
what type does it read only or are you
writing if you're writing are you doing
it synchronously or asynchronously if
you do it synchronously you're going to
wait the round-trip time of sequel need
plus Oracle writing to disk so it's
going to be slow if you'd a
synchronously you can commit your
transactions and microseconds well in
Oracle write it in the background and
its own time you specify how often
you're going to refresh it in this case
it's going to refresh it every five
minutes you can specify it down to
milliseconds you can say all and refresh
it every hundred milliseconds
technically we Poland the more often you
poll the more stress you put on the
database the more often you poll the
more live your data is the less
frequently poll the less stress on the
Oracle database but your older your data
is so what is the right frequency
there's no such thing based on the
business they say our liveliness of data
needs to be X and you configure it
accordingly and that's how often at
pulse we do all this polling within the
different tables and a transaction
consistent state so you don't have one
table that's got new a date of another
whenever we update staff we do it as a
single transaction so that everything is
consistent so you can see we've got
tables and data types we use the same
data types and times tina's Oracle but
we also have some newer data types so I
consider having a number which is an
Oracle object which can be up to 22
bytes if we know it's an integral value
with no decimal points we can just make
it
you know a long word on your operating
system an 8-byte you know long long and
see is a whole lot faster than a very
complex structure called a number the
point is we can twenty two bytes for a
number this is eight bytes for a long
lot less space goes faster so you can
choose to map your data types to make it
faster or use less space
if your hierarchy you know you can add
child papers and the point is if you
like sequel developer you can get
click-click-click to define things
ultimately it generates some text which
is the the sequel the DDL for that cash
group that's if you do it in secret
developer and the command-line kind of
guy here we're creating a dynamic
read-only cache group what that means is
if the data is in the cache you're happy
if the data is not in the cache will
automatically go to Oracle and get the
data for you
so we're updating the cache we're
literally selecting out of Oracle and
sitting into the cache if you keep doing
cache misses you keep doing inserts if
you keep inserting you're eventually
going to run a space so by default we
automatically age out the data you can
age it out by time or more commonly
least recently used LRU so the hot data
stays in the cache automatically we
specify the min and Max values from when
it starts aging or garbage collecting
that stuff out so the point being is a
dynamic we'd only cache group give a
name we order refreshing incrementally
every hundred milliseconds we're
specifying the employees table and the
history table we're specifying some
columns specifying a premium foreign
keys so we've declaratively specified a
hierarchy and master detail that
metadata says this is a relationship
when things and certs updates or deletes
occur on our call those changes will
automatically get pushed every hundred
milliseconds two times ten now mid-tier
so I'm not sure how we're going on time
I haven't done a demo yet if you guys
care I can show your demo if you need to
leave that's fine so the point is we've
got one technology
it's called times ten we've got two
products the products are called Oracle
times 10 and memory database and Oracle
application tier database cache I've
been talking about caching so
technically I've been talking about
application tier database cache which is
a an option of the Oracle Enterprise
Edition so if you want to do this
caching stuff it's an Oracle Enterprise
Edition option if you don't want to do
caching you can just have a standalone
database by the way we're about to
release a new feature which is a scale
out she had nothing database so you had
a question before about nodes talking to
each other
it does that yeah
yeah
a good question the answer's no we poll
you specify the polling interval the the
committed insert update or delete on
Oracle is the event change we poll when
we look at the deltas and send over the
Dodgers so it gives you the effectively
the same thing okay so that was the talk
I'll just look at this so do the summary
so in summary you can do cloud scale
read/write caching with a latency of one
millisecond of the 99th percentile and
we didn't use any rocket science
we declaratively configured the
relationships of the data we
declaratively defined how often will be
polled or refreshed we automatically age
stuff out via LRU we use the technology
times 10 which just happens to be a
bunch faster than all the no sequel
solutions out there and we used
configuration not coding to do all the
Kashyap but I'm missing something what
about the application logic I haven't
talked anything about how you
programmatically use x team I haven't
needed to because you already know how
you do it exactly the same way and times
10 as you would on an Oracle database we
use sequel we use PL sequel we use API
is called the JDBC sounds familiar it's
the same I was talking about for an
example the same source code will work
against ohms 10 Oracle you just got to
change the connect string so JDBC is the
same OCI is the same since I live in 204
the Oracle client has got code 4 times
10 and Oracle based on the connect
string that'll route to the correct
database it's already there you've
already got it you just don't know it
your tea nice names strings just has an
attribute to specify I'll go 2 times 10
so because we've got OCI all API is that
sit on top of OC I automatic work
proceeds program
well place the node go Ruby PHP the
Oracle drivers are what we use instead
of writing the same thing we just use
the Oracle drivers so the way you
program against an Oracle database with
your favorite API is exactly the same
way that you do it against times 10 the
little asterisks we have a subset of the
sequel and the subset of the pure sequel
but we haven't like Oracle has a huge
functionality we're going to subset we
believe it's enough for customers so
that was my talk if you understood I can
show a demo yes sir
great question I'm trying to find a
slide where we can cover there
that Oracle database runs on machine
does it a spike machine as an AIX
machine as an HP machine is it a Windows
machine is a little don't care it's
a machine that the Oracle database runs
in the database tier the x team database
runs in the mid tier preferably on the
same machine is your application it
doesn't have to it can be its own mid
mid tier technically you can run x ten
on the same machine as the Oracle
database but it's not a good idea they
both fight for CPU they both fight for
memory they both fight for i/o so don't
do it
put it on a separate machine times 10 is
very memory intensive so you want to
shape with a lot of memory sufficient
CPU insufficient i/o that shape tends to
be a whole lot cheaper than Oracle
database which needs a huge amount of
i/o less memory CPU so the shape the
ideal shape for an Oracle database tends
to be different than the shape 4 times
10
absolutely so if I just go back to our
pictures that's exactly what we're doing
it's in the mid tier so it's just a
compute V M of your favorite shape as
that lovino is a is that a Sun is that
an HP is of my beam one I mean I Adele I
don't care it's just a machine
it's a as it be middle is it a VM is a
container dunk here
yeah
yep
okay um good concern it turns out not to
be true
and I'm just trying to find the example
the cash group can be an arbitrary now
you don't query the cash group the cash
group has a name that's just
yeah that's just a that's an identifier
it creates a table and the schema HR
called employees so if you're querying
HR dot employees on article the
side-effect of creating the cash group
will create a table called employees and
the HR schema and times ten
therefore you go against the same scheme
of the same table with the same columns
with the same type
yeah yeah good question so if you lucky
pets the sequel's compatible if the PIO
sick was compatible because we're a
subset the goal is you don't have to
write it rewrite anything you have to
change the connect string it's going to
be a different host probably on a
different port the username and password
the username has to be the same the
password doesn't have to be the same
absolutely yeah
yep
yeah yeah so I've been explaining the
simple stuff which has read you had a
question on rights when you're doing
right it just works
you got to be careful though if you do
read caching the data is mastered on the
Oracle database and you've got by
definition a stale copy has its stale by
100 milliseconds because 100
milliseconds later by definition at
stale so you choose house tell the data
is which is the polling interval
yeah I can actually show you that I'm
good at demo to show you that so I'm
beach max and literally depends on the
hardware and what you're doing but if
you give me you know if you wait two
minutes I can show you it doing it okay
so the point is with write caching it's
more complicated by definition if you're
doing it right you've got the latest
version and you're pushing it to Oracle
so technically the the right cache the
tables of the right cache technically
the other system of record so that blows
a lot of people's mind if if you've got
one machine with one right cache and it
all works if you've got multiple
machines or writing you've got to worry
about coordinating so you don't
overwrite things what most of our
customers have been doing for the last
twenty years as they do application
level partitioning such that writes
routed to certain places so they don't
overlap so if you don't do application
level partitioning you might get a lost
update problem which is why we created
the the scale out she had nothing
distributed database called times ten
scale out do you run their problem so if
you'll you know you've been very patient
and there's three more minutes to do a
demo if you're interested do you want to
do a demo okay so if I'm lucky the demo
gods for
work with me so the database is
installed it takes between three to five
minutes to install times ten depending
on how fast your Disqus the database
instances app but the database is empty
so if you've been with me I'm going to
create a brand new database I'm going to
create a bunch of schemas I'm going to
insert a bunch of rows I'm going to
create a bunch of tables do a bunch of
queries I'm going to run some JDBC and
ODBC and show you things like insert
rates and query rates so I've got my
handy-dandy demo script so things are up
so everything I'm doing here I'm
cheating we've got a thing called Quick
Start where you can instead of reading
the manuals which takes too long just do
these things to get you up and running
it was a message
okay okay so what I'm doing here is I'm
blowing away any database and creating
brand new database creating a schema
create a bunch of users and hopefully so
I typed in a bunch of passwords because
all that demos are secure by default so
hopefully I remember the password I just
typed it so created a bunch of stuff so
let's use it so instead of using sequel
plus we use an equivalent thing where
I'm specifying a username a password and
basically a service name okay I'm gonna
I typed in the password wrong so I'm
just going to rerun it because it's
reinsurance I took I thought I'd typed
in the password wrong and I did I don't
know what the password is I'm just going
to recreate the user and this time I'll
remember what the password is
so we're creative database we're
connected so what you just saw in a few
seconds zero as I created a scheme with
all those tables so this is a command
line staff you can do it all through
sequel developer you can use your
favorite IDE I just like doing stuff on
the command line
so I'm describing the table imp just
like you wouldn't sequel plus we can see
it's got columns it's got datatypes and
there's a star by that column which
means it's the primary key so if I do a
query from that table that returned 14
rows and it took 144 microseconds so
that's about point one of a millisecond
can we go faster than that
instead of queering everything why don't
we just query a single row so that took
about 57 microseconds I am doing dynamic
sequel no one's prepared in a thing no
one's bound anything so if you use a
JDBC program and you use the prepare
command with bound variables it's going
to go a whole lot faster okay
let's create a new table
trivial table two columns let's insert
some rows
let's query from it okay
it's just sequel we create tables we do
selects and since updates deletes merges
we can use materialized views we can use
sync sequences we can create indexes we
can create PL sequel packages if you
familiar with the Oracle database it
should be pretty similar sure we're not
using the sequel plus but we're doing
the equivalent things drop table okay
so these are just some demo programs it
just happens these RC programs I'm about
to do some Java programs as well so if I
do make a compiler stuff with link stuff
and we're ready to go
so we ship demo programs which are Beach
Maxo
so if we do it out of the box with no
parameters and we type in the password
so I just inserted 10,000 rows and I did
some selects and it was doing children
and 36,000 transactions per second but
it took so little time it's cheating
because it's kind of in the cache let's
run it again with a bit more data
consider considering 10,000 rows I'm
going to truncate the table and I'm
gonna insert a million rows once there's
a million rows and then going to do a
decent reads 20 percent updates
so this is a single process a single
connection you know so to load the data
and to to load a million rows and to do
a million transactions took about four
seconds so I'm getting about two hundred
fifty two thousand transactions per
second with one connection can you speak
out yeah
absolutely unfortunately literally the
the wider that the way to the table the
longer is going to take it's not magic
more work takes longer um great question
the answer is technically depends the
only accurate way to know it is to
create the table and insert the data
because that's a fact I can say it'll
take three percent longer or forty
percent longer I'm guessing for your
table for your data and your hardware
run it it's I just did it it's that
simple
so I was doing a piece entry 20 percent
updates what if I just did reads but if
I didn't do any rights will there make
it go faster
apparently so 432,000 reads per second
with one connection more connections
gives us more concurrency so if I'm
running on a sixteen core machine I'm
using one core so that was some C code
let's do some Java code
so I'm running JDK 8 a 1.8 but a work
was 7 or 6 or 5 but I really encourage
you to use the latest version of JDK so
compile it Java C startup Java it's not
different it's the same so compiled
everything we've got the equivalent Java
program of that C program we just ran
before did a bunch of work Java's
slightly sucked slower than C slightly
slower there's just more work but it's
still pretty fast so that was doing you
know took so little time it's you know
not really meaningful but they was doing
74,000 transactions per second and if
some of you are programmers this source
code is JDBC that's calling PL sequel
the exact same code runs against times
10 and Oracle so if we look at it we've
got a variable that variable determines
when you're connecting to Oracle times
10
what is it variable doing all right
can't use bye
so based on the value of a variable
we're either going to connect to the x
team JDBC driver instead of me putting
up my screen not sure if you guys can
see this based on the value of the
variable we're either dynamically
loading that JDBC driver or that JDBC
driver all the rest of the source code
is exactly the same yep
so this same program you pass in a
perimeter on the command line
yes okay so if the application is on the
same machine as the database you can
what we call direct link which means
load we can use the same address space
as the database which is really fast if
the clients on a separate machine we
have to use a network hop therefore we
use a different driver the source code
doesn't change you just use a driver for
a network hop or a driver for a memory
attached so the point of this program
was you're specifying whether you're
using the Oracle driver or the times
team driver all of the rest of the
source code is exactly the same so you
can pilot your person on the command
line whether you want to connect to
Oracle or times team so it's just an
example the source code can be exactly
the same and I'm well over time and I
think that was the end of my demo so any
other questions
can you repeat the question in fact can
you come closer I can I don't like some
as again I can't really say anything
yep
okay so I'm gonna give you a maybe cash
groups cash tables if your package
procedures our interfaces to querying or
accessing tables instead of accessing
those tables and Oracle if you recreated
that same pure pure sequel package in
times 10 and that package looked at the
local times ten tables that will
probably work so I want to be careful
here we support a subset of the sequel
and a subset of the POC core if you're
doing the lowest common denominator such
so they'll work with both F then with no
code change he just cut and paste
recompile your packages it'll work if
you're doing something thank you that we
don't support that's not going to work
so you need to change your code yeah
okay so and we've been kicked out I'm
here to talk to you but these people
need to go home so think of your time
yep no problem</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>