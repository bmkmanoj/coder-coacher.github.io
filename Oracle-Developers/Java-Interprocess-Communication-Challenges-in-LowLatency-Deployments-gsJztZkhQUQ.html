<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Java Interprocess Communication Challenges in Low-Latency Deployments | Coder Coacher - Coaching Coders</title><meta content="Java Interprocess Communication Challenges in Low-Latency Deployments - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Java Interprocess Communication Challenges in Low-Latency Deployments</b></h2><h5 class="post__date">2015-06-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/gsJztZkhQUQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Darryl Meyer I work for IBM
I've been working on Java and Java
virtual machines for about 13 years now
I primarily focus was there is just in
time compilers specifically focused on
x86 performance and more recently I've
been working on Java benchmarking my
colleague is Anil Kumar from Intel he's
had over a decade experience as well
working on tuning Java for running
optimally on on Intel hardware so we're
gonna be talking about IPC and Java
applications today so mistifying
quickly what we mean by IPC so inter
process communication so we have a
broader definition of that and what
we're really talking about here is any
sort of intra or inter communication
between entities in a java application
or dev applications so this is either
communication that occurs within a
single process or it could be
communication that occurs in different
processes and those processes don't
necessarily have to be on the same
physical machine that could be actually
separated you know quite diverse lidia
graphically and the reason that this is
you know necessary is because if you
look at a lot of customer code like we
do real applications are partitioned
this way into separate modules and
there's a number of good reasons for
doing that it's a very clean design
paradigm we've logically separated some
of your entities from a testing point of
view it actually makes it a little bit
easier in some cases and it can actually
make it more effective to test
independent modules that way and it
really encourages the reuse of the
components as well either within your
own team for future releases or you know
between between organizations and lastly
you may not actually be dealing with two
different java processes that have to
talk to each other in some cases you're
talking with some other entity that is
not even written in Java it could be for
example some legacy COBOL banking
application that has its own sort of
proprietary communication layer that you
have to talk
so but but this kind of IPC brings with
a number of challenges of course so what
one of the most damaging or one of the
most challenging issues that you have to
deal with is the communication latency
so this is basically when you when you
send a request to some other entity and
you're expecting to get a reply back how
long does it take for that for that
request to get there the work you get
done and you get your reply back so
that's the sort of the overall latency
how much data do you have to send and
how much data are you expecting back and
can your system and can your Hardware
handle that in a reliable way and then
once you start talking about our
transactional workload in a distributed
environment you start getting into
issues with scheduling and and and and
deadlocks and these are usually fairly
complex issues to to solve so to be
aware of those so what we're gonna be
talking about today is to help
developers such as yourself understand
some of these issues in the context of
Java applications larger Java
applications and with sort of the
special twist with a special focus on
applications that really do care about
low response time so by low response
time we mean in the millisecond range
not tens of seconds or even in the
second range where we're talking about
low low response times and to do that
what we're going to be doing is to share
our experiences developing an industry
standard benchmark that was released
earlier this year and describe how that
software was designed and what some of
the design choices we made there how you
take that workload and deploy it on
different hardware configurations and
why and then how to take some of this
information that we've given you and
apply to your own applications so when
we're talking about IPC when you want to
architect a solution for communicating
with another entity you need to figure
out basically what it is that you you
really need to do so one of the I've got
a list here of a few points but it's
really longer than this so I just picked
off a few here so one of the most
important ones is being to figure out
how your application is actually
structured
do you have components or entities that
actually communicate quite loosely with
each other are they on different
machines or they you know different you
know different processes or is
everything running in a very you know
what wonder under a single JVM is your
environment just Java or do you have
other legacy or or even let's say a C
back-end the you that you care about how
are the different entities going to
communicate do you just need to poke
something every so often or do you
expect a more conversational kind of
communication with with the different
entities when you're talking with other
entities do you need a special transport
requirement so do you need to compress
the data do you need to encrypt the data
things like that you care about response
time maybe not and what level of Java
are you really dealing with because that
often affects what some of your
implementation choices actually are
fortunately Java actually does provide a
number of good building blocks it's a
really fairly rich toolset for building
applications that they use communication
like this so Java done is perhaps the
most obvious package that you could use
for the socket communications and that
sort of thing
Java util concurrent is is a very useful
package as well for developing scalable
applications it's got good classes in
there for thread management building
very scalable lock free data structures
so it's it's useful ant regard
new i/o for asynchronous i/o and then
some of the more legacy kind of
technologies like carbon RMI that you
may consider looking at as well so some
of the key issues that we have to deal
with with with inter process
communication is in many cases you want
to make sure that you have a high enough
whatever you design you want to make
sure that your system is running with a
very high throughput and that you're
getting a very low latency so often to
do that you need to understand where all
the bottlenecks in your application are
and and eliminate them so this is within
your application within the JVM within
the
we're within the communication layer and
try to reduce those as much as you can
does your application need to grow if or
how scalable is your is your
communication solution if the workload
that your that your application is is
using is changes so if you increase the
workload is your application able to
handle that increased workload if you
move it to a different piece of hardware
deploy it in a different way can your
communication scale with that different
deployment correctness is always a
concern so you have to make sure that
whatever you come up with is able to
handle and avoid deadlocks and and
resource starvation and oftentimes the
solution that you want to employ for
communication between different parts
you can't actually do very easily or
very cheaply so for example if you think
that there's two different entities in
your application that would best
communicate via some shared memory Java
doesn't make it all that easy for you or
very efficient to do it that way
so the best solution may not be
immediately available to you and I mean
take all these concerns and they're even
they're even harder to solve when you
throw in low response time guarantees so
earlier this year we released a spec gbb
2013 so this is the next generation Java
business logic benchmark
it's from the spec consortium and those
that are familiar with the stench
marking for Java realize that this is
the follow-on benchmark to spec jvb 2005
it's really a replacement and it's
actually a complete overhaul of that
particular workload so that the workload
was completely redesigned to modernize
it and make it much more relevant to
customers these days and make it more
similar to the way that they're
developing their applications now the
business model that we chose there is a
supermarket supply chain so you've got
the concept of headquarters central
headquarters you've got suppliers you've
got supermarkets you've got customers
coming in to those supermarkets making
purchases and there's all the
interaction that's going on there it's a
very scalable
workload and it's what we call self
injecting which means that it is able to
drive all the work that it needs in
order to to to measure performance
you don't need an external driver some
sort of load generator to run this and
it works in multiple different
configurations that are intended to test
different kinds of configure a BM
configurations and hardware
configurations so from single JVM up to
distributed architectures there are two
metrics that are used here there's the
sort of classic through Pittsburgh
through throughput metric which is
basically in operations per second and
for the first time for a respect
benchmark we've got a response time
metric which is really throughput under
the under certain response time
requirement so huh you know how much
through but can you get under that's a
you know like five hundred millisecond
response time that sort of thing and in
terms of modernization we're using a lot
of Java seven technologies so new
security stuff xml j case I mean like
fork/join that sort of thing and to draw
on the the relevance to the customer
application it was really designs that
we move a lot of data around between
these different entities in the workload
this is just a quick shot of the output
that one of they one of the outputs that
is produced from the workload it's a
little bit overwhelming but the key
thing here what it's showing is that as
you increase the workload as you
increase the input work along the x-axis
there the y-axis is really measuring the
response time at that increasing amount
of work and from this you actually we
can actually derive the two different
metrics from it so on the far left from
the far right there you see the max J
ops which is the throughput metric and
that's really the amount of throughput
that this workload can handle sorry the
amount of work that this look that this
workload can handle without considering
response time so at that point where
that red line is is where is the point
where the where the workload basically
just fell apart it couldn't handle any
more work and the yellow line further to
the left there the critical J ops is the
is the throughput under these response
time
constraints and the formula for these
response time constraints is a bit it's
a bit involved but it's basically
considering a number of different
response times and then averaging them
out and coming up with that so ideally
we'd like to see the yellow line as far
right as possible because you know if
the underlying system is performing
really well we're able to achieve high
throughput under low response time so it
looks like in this case we've got a long
ways to go with that yellow line to the
right so looking at the general
architecture of the of this particular
workload
it's basically broken up into you know
sort of three higher level entities that
can be configured in different ways so
we've got the concept of controller
that's really running the show here and
it's responsible for for communicating
with all the other all the other
entities and telling them what to do
we've got transaction injectors which
are there to inject work into the
workload they're not part of the actual
workload that actually is being measured
but they're there to drive work into it
and then we've got backends rule which
is the system under test so this is
where all the business logic resides and
it's basically a configuration of
supermarkets and headquarters and
suppliers and that sort of thing and all
through this there is different kinds of
communication that we have to deal with
so for example there's controller
traffic that's basically telling it what
to do and then we also have the
different entities that that are
involved in the actual workload itself
exchanging business messages and
business logic between each other
so from that we can sort of see what our
communication requirements actually are
so because we have sort of a separation
of components we see that we do have
both intra and interprocedural our
interprocess sorry hammock a pile of gum
saying that communication so we have to
be able to handle both kinds of both
kinds of traffic seamlessly there also
is business logic data that's a green
exchange and is also controlled traffic
so it's not just sort of simple do this
do this do this it's it's more of a
conversational kind of communication
that's there we also have to be able to
deploy this workload in multiple
different configurations so what we
really need to design
a very scalable architecture and
something that is very easy to to to
program for we do need very low response
times that's really one of the
requirements of this workload and the
developer API when we're actually
writing code for this it needs to be
fairly simple and we don't it when these
different entities are writing code you
don't want to have to think about how
the benchmark is being deployed and then
use one set of API is and if it's
deployed a different way you want to use
a different set of api's we just want to
have one standard api that's being used
regardless of how the workload is
configured and different layers of this
comm layer will figure out how best to
to do the communication so drilling down
to the actual communication layer on
this workload what we see is we
basically have one sort of central what
we call an interconnect and there's one
interconnect for JVM and into this inter
inter connect these different clients or
entities can plug into so these are
basically things that want to do some
sort of communication so these can
actually be business entities like like
the supermarkets or they could actually
be control entities like the transaction
injectors and that sort of thing and
when they plug into the interconnect
they can actually specify that they're
basically connected by these different
transport links that's capable of doing
different things to these messages that
are being exchanged they can do
compression they can do encryption if
necessary of this data if the if the
entity that you want to connect to is
actually remote you can actually connect
these interconnects together by the what
we call connectivities and it's all that
is being done transparently to the to
the to the business logic it isn't this
is all part of the the communication
layer
so just looking at that that green bar
the the interconnect what does it
actually look like well like I said
there's one interconnect per JVM into
which these clients can connect and the
one thing that we have to be careful
about here is that it becomes a
scalability bottom line because you
really have one if you have a poor
implementation of how this is going to
go your your but you're you're
throttling your performance at that
point one of the key features of this is
that it actually does its own routing so
it's got a registry that it maintains of
so it knows where every other client is
in the entire in the entire application
in a way that works is that there is a
master registry that's maintained by the
controller and when when a particular
interconnect needs to resolve you know
where another client is somewhere in
this big architecture it'll ask them at
and if it doesn't have it already cached
in its local registry it'll go out and
ask the master registry to resolve it
for tell it where it is and can
communicate directly with it that is one
thing about this kind of design is that
each client can actually talk directly
to another client in some other
interconnect if necessary
so as I mentioned before when the the
link between a client and the
interconnect is is is as it has a
transport layer to it as well and the
way that this was actually designed was
to basically model what you'd see for
business data marshalling and on
marshaling so what we have is sort of
this transport pipeline architecture
that we even that we call it that's
capable of doing all kinds of different
sort of operations that each different
stage and the choice of what kind of
transport to apply really depends on the
kind of message and where it's going and
what the expectation is of that so for
example if you're doing some kind of a
payment the expectation will be that
that date is going to be encrypted so it
would likely follow a transport or an
encryption pipeline so the most common
transport is basically just a plain
transport which doesn't do anything it
doesn't do any foreign calculation or
compression or encryption just sends the
data right through but there are others
that actually will do serialization or
or compression that sort of thing the
one thing you have to be careful about
when you start adding transports like
this though is that they are a real drag
on performance and we've really
discovered that when we started adding
these I mean we added them because it
makes it very realistic this is what
customers are doing but it's a real
killer for performance so you have to
make sure that if you're if you're going
to be doing something like this you want
to make sure that it's that it you've
tuned it as well as you can now
thankfully there are Hardware
technologies now that are sort of
catching up and helping out with some of
these kinds of transport so for example
crypto hardware is now available on
Intel AMD Hardware that's that's able to
do this kind of thing you know and je
viens can take advantage of that and
similarly with with xml processing as
well if there's if you have the
capability of working with an XML
accelerator there could be some help
there for you so there are large
overheads you got to be aware of that
when you need to communicate when to
interconnects need to communicate we
connect those with something called a
connectivity and it's very basically a
very simple client-server kind of
architecture where if you want to talk
to something in another interconnect you
basically have a client that talks to
the server in the other interconnection
and the way that we had architected this
was to actually we took a number of runs
of this but the way that we ended up
architecting this was to make it a very
pluggable architecture and that way we
could actually plug in what we call
different connectivity's in here to
evaluate how well they perform so what
we what we end up supporting here are
some open source open source
connectivity providers like grizzly or
jetty and you can choose which one you
want to use based on you know the system
you have or what are your whatever you
already have on that system that sort of
thing and the way this works is that
like you said you have clients that are
talking with a server but we also found
that you really need to be careful about
managing if you want to get good
performance of this you've got to be
careful about managing and pooling the
resources on the clients so you don't
want to have to go and stablished a
connection for every time you want to
talk to another to another server you
can actually pool those and manage that
pool as well so it's going to definitely
save saves and performance there the way
that I've I've got this in this diagram
here is to have just show these
different tiers in the server I'll talk
about those in the sec so but for the
you can basically just think of the
servers as one sort of entry point at
this point so why do we choose to
actually use a third party connectivity
provider as opposed to writing our own
well connectivities are probably one of
the most complex things that you could
possibly develop for for a workload like
this and there's a lot of interaction
happens and there's a lot of issues you
have to take into account so it's
actually we found it was actually safer
to actually rely on something that
someone else has already developed and
has been well tested
shown to be something that's a very
scalable design and you know that's
that's you know really really saved us a
lot of time here so you want to make
sure that you know if you're designing
something you make it pluggable so you
can try you know things like this
and and see where that takes you the
things that actually plug into the
interconnect so the the entities
interaction can be doing business were
business logic or control entities that
sort of thing
we call them connection clients and they
basically just plug into this
interconnect announce themselves that
they're there and they communicate by an
uplink or a downlink so uplink is
basically data you send to it and
downlink is where data comes from and
it's it's a fairly simple simple
connection the one we would the one
thing as I mentioned before we have a
single API that's being used if you want
to talk from if one client wants to talk
to another regardless of how things are
configured it just goes through just a
single sort of a send request or a
message sort of send message and you
know it the the IC will figure out where
and how it needs to get to the
destination so in terms of the
communication itself we do support
messages and requests so messages are
basically one direction we're not
expecting a reply and they're non
blocking we just send and forget
requests are more conversational we are
expecting your reply and we will once we
send a request route we're gonna sit
there and block and wait for the
response there are some things that you
can do to improve the performance of
this kind of a approach if you're having
you know message passing if you know
that you're going to be sending many
messages that have exactly the same
content to the same destination you can
actually marshal these once get them
into a packet form and then cache them
away somewhere so that you can just
continue to reuse that packet each time
to send that message and this is
actually important for scalability
because you don't have to go through
that overhead of repackaging this
message each time and sort of isn't
along the same that same lines you can
actually batch
multiple messages that are going to the
same destination all together into one
you know one big payload and then send
that at once and that's going to improve
your throughput and it's going to
potentially reduce your bandwidth as
well you do have to be careful in that
world because especially when we were
talking about response times because if
if you if you're expecting to batch
multiple messages together to send to
one destination you have to be aware
that if you're if you're collecting
messages you don't want to wait too long
before flushing your buffer because you
might be defeating any sort of response
time guarantees so you can't just go by
the number of messages and then and then
flush you have to sort of consider the
amount of time that you're taking as
well to collect those messages so you
might need to be more sure of
pre-emptive flushing as well so just a
few FAQ is here so why do we choose a
message passing scheme for this
particular architecture well it fit the
business model very well it really lends
itself well to that but what this also
does is it allows us to flexibly
configure this workload like I said
before we've got multiple different
kinds of configurations here from the
single JVM to multiple entities in the
same JVM or you know multiple JVM s and
this really allows us to be very
flexible with that why didn't we use JMS
well the main thing is one of the
requirements that we had was Java SE and
JMS is a Java EE it says it's part of
the Java EE spec so we didn't use that
but also because this was a benchmark we
weren't using JMS because there already
is a JMS benchmark and that wasn't
really the focus of this particular
workload it was more the business logic
side of things and something that I've
already touched on is you know why did
you reimplementation scheme rather than
reuse it so why didn't we write
everything from scratch right well in
this particular case we wanted to have
very tight control over the performance
and oftentimes when you do pull in
third-party packages and things like
that you may not necessarily know you
may not be you may be getting a lot more
than what you're expecting to get you
may not have as much
roll over the performance of that and
how well on how it behaves and how you
can tune it so this being a benchmark
are really driven by being a benchmark
we wanted to have very tight control
over that and and resolve as many of
those issues as we could however having
said that though if the components that
you're that you're needing to design are
sufficiently complex like a connectivity
you really want to think twice about
developing your own and think about
what's out there and how that really
meets your needs ok so once the message
or a request actually arrives at some
client it was successfully there what do
you do with it so in a transactional
workload where they're potentially lots
of different pieces of information
arriving at a particular entity
typically what happens a lot of
applications as you add it with some
thread pool to be worked on and so what
we have is that for each entity is
really backed by a worker thread pool
and what we the way that we chose to
implement that was using fork/join and
the reason for that was because some of
the messages that are arriving can
actually be processed we can actually
batch processing on them which is great
for that's what exactly what fork/join
is is really good at it's doing good for
doing bad decomposition of some data
working on it and then we synthesizing
the results so we chose to go that
approach there is only a single JVM I'm
sorry there's only a single thread pool
per JVM and the reason that was
necessary was because the way that we
had architected this at one point was to
have separate thread pool per entity
which is you know seems a very logical
thing to do but the problem is when
you're starting to pass messages between
one entity to another what we were
finding is that the contact switches in
the operating system were occurring at a
much higher rate than we could really
handle and it was really impacting the
scalability of this so we had to react
attacked things such that we only had a
single thread pool design where there
wasn't so many artificial handoffs from
from from one thread to another
and because we chose to use fork/join
that also brings in some cut challenges
as well because one thing that fork/join
for Kron is great at doing batch
decomposition the one thing it isn't
great for is any operations that's got
let's say blocking i/o it's not really
good it really defeats a lot of the
work-stealing that's built into
fork/join there are api is that you can
use that that fork/join provides that
allows you to work around that you can
have manage blockers and that sort of
thing but you have to be that those
don't necessarily always work you can
actually end up with more threads and
you know what to do with but blocking
i/o is really a concern here so you have
to you know be careful how that's how
that's playing out for you but the key
message here though is that you really
want to be careful about your your
thread pool design because it's really
going to impact your your scalability as
we found so one of the correctness
challenges that I talked about earlier
was in a distributed transactional
environment our deadlocks and just to
give you a simple example of what that
what that really means is you know if we
have a system here that's running on two
JVMs we've got two different
interconnects and in play here if
there's a thread pool there's some
there's some entity in in in thread pool
one that's decides to make some kind of
a request and it turns out that the data
it needs is on the second system it's
going to issue a request it's gonna be
routed over to the correct server and
it's going to be delivered to the
corresponding entity in the second
system while that work is being done the
that particular transaction decides that
it needs some data as well and it turns
out that that data actually is coming
from the from the first system so it's
going to make a request to the first
door it's gonna make its request it's
going to get sent to the to the first
system but what might end up happening
here is that there may not be any
threads there available
or servicing that request especially if
all the threads in though in the thread
pool are waiting on data to come from
from the second or waiting for data to
come from the second system and all the
threads and the second system are
waiting for data from the first one so
you're gonna have this deadlock
situation that you can't avoid so there
are solutions to this there are very
complex distributed deadlock avoidance
schemes that you can implement you can
look at your application and figure out
what are all the cycles in my in my
transactions and can I actually
eliminate those and you can rework your
your application or you can do something
that we chose to do which is sort of a
simplification of both of those and
that's to introduce these things what we
call communication tiers and what we've
done is we actually what we call tiered
the connectivity servers and the thread
pools and what this really is is just
sort of a replication of the thread
pools and replication of the
connectivity servers and the reason that
we've done this is that each of those
tiers is allowing us to guarantee that
there will always be progress on some
work so for the case of thread pools
there will always be a thread available
for advancing the work and you're never
going to deadlock this isn't this is the
kind of solution that may work well it
worked well for us because we had
complete control over the transactions
and we understood all the different
cycles that there were in the in the
transactions that we had and we could
size the number of tiers based on the
number of cycles that we that we
actually had so that's really one of the
biggest drawbacks here is that it's not
the most dynamic approach it's a
reasonably efficient approach it's a
it's a it's a simple approach but you
really have to understand the
communication patterns that you actually
have in order to size this properly so
the way that this works is that when you
are exchanging messages within the same
interconnect they basically get sent to
the same tier as the requesting thread
but if you're ever sending a request to
something that's in a remote
I see then it's actually going to be
routed to the next highest tier and the
guarantee is that the next highest tier
is always going to have a thread
available for progressing work and then
we have you know some some reserved
tiers for doing some infrastructure work
now the thing about these tiers is that
they're very it's transparent to the
actor to the sender of the message it
doesn't have to think about what tiers
the thing in and you know that's the
thing that's all handled by the
interconnect it knows that when I'm
sending my message all I know is what my
current tier is or I can ask what my
current tier is and and the IC will
figure out that oh and it's remote
request so it needs to get bumped up to
the next year so if you look at the
communication now what's going to end up
happening is that if you get a request
that same requests coming from tier 1
it's going to arrive at the server that
the destination server on tier 1 but
it's going to be put into the tier 2
it's going to be put into the tier 2
thread pool and if a request comes back
from the from that second from the sorry
from Network pool it's going to arrive
at the first JVM and be put into the
third tiered thread pool so the way that
we've architected things is that we
never have more than I think three
levels of cycles so that you know we
know exactly how many tiers do to
actually to actually adhere if you have
a very dynamic system that may not work
for you and you probably will have to
have a more complex deadlock avoidance
scheme
so just some debugging advice when I had
to pass off so when you're talking about
doing communication stuff you really
want to keep the API as simple as you
can if it's getting very complex it's
you're often going to find that that
some developers are going to abuse it
it's also going to make it a little more
difficult to to debug some of the
problems that you that you may be having
one thing that happens a lot when you're
talking about communication is timeouts
and if you start to aggregate all these
different kinds of timeouts into into
into one if you're basically just
reporting I try to communicate with that
entity didn't work it timed out and you
know that's the end of it that doesn't
really give you any input as to what the
problem actually was it doesn't tell you
the underlying cause so what you have to
be able to do is to actually break up
those timeouts into something more
meaningful you need to understand the
root cause of it because just saying
that something tied out timed out after
60 seconds there could be lots of
different reasons for that it could be
the you know garbage collection it could
be you know hardware latency it could be
all kinds of different things that you
need to need to get to the bottom of so
it doesn't help you debug and one common
thing that we've seen people do as well
is to use IO like you know system print
--lens and some things like that to do
debugging but using IO to debug IO would
actually works against you in a lot of
cases especially if you're looking for
time out problems using IO to help debug
those it's just going to make the
problem worse and you may actually see
artificial problems because of that and
the other thing is that it might seem
somewhat obvious is that if you're going
to be testing this on larger systems
you're sorry if you're ever going to be
running this on larger systems you
really want to make sure that you test
the full range of systems that you're
going to be running this on because what
might work really well on a small system
will not work well on a very large
distributed system same thing with
things like timeouts
they they they might work you might have
a very low timeout setting for something
that's running on a small device but if
you're running on a big server you might
need a you might need some adjustments
to that
to scale up to that so Anil is going to
talk a bit about a little bit more about
hardware and the interaction between
hardware and hood and the and the
operating system and and how there's
some factors there that can impact
response time so I think once your
application is ready with respect some
testing you have done
you're already architected thinking all
the components now I'm trying to cover
more you are ready for the primetime for
the production and in that environment
you really wanted to see what kind of
response time you are getting and in
both the scenarios when you are running
under low load and when you are running
under high load so I want to cover
several aspect which you need to keep in
mind because just not your application
you wrote there are many other component
which also impacts so I will try to
cover a wide range so the very first one
is just the deployment decisions you
have to make which will impact the low
latencies when we are talking like one
second or so probably doesn't matter but
these parameter make a difference when
you are talking about one millisecond or
five milliseconds in that range all
these factors make a difference so let
me cover just from the very low side one
is hardware configurations you could
from the network side it is possible for
larger system to socket for sockets you
could put multiple network or in them
and we have seen at a high throughput
how you are routing your traffic and
through the network card could have a
big impact on your latencies and some of
the tricks people do are related to you
can Affinia ties the network traffic
going through the network card going to
particular sockets so that always in
streamline your network traffic and you
can have some latency there another one
is what kind of choices you are making
on the back end of the system like is it
infinity band connector or fiber optics
they always give you much better latency
than the regular Ethernet cable now
moving on to the native OS we have seen
if you are within singular
image then there are companies which
sells you the software which do the
loopback traffic optimization because
even if it is a within same host you
have loopback traffic and there are some
software stack which allows you the
optimization on the loopback traffic and
that's a bit different because in the
loopback traffic it's not a call from
the user level to the kernel level and
switch back because that is where you
save your big opportunity and if you are
going across multiple OS image then you
might be able to plug in your multiple
OS image with a network card so if each
OS image is working through socket on a
particular network card you could
streamline them for the network traffic
now going in new trend of deployment of
virtual OS images there we have seen
particularly some of the virtual vendors
they already automatically pin them the
each virtual machine you start is it get
dedicated couple of course the other one
keep it flexible and what we find when
you're flexible then you could have
inconsistency in your response time
sometimes you will get good sometime not
because they might be sharing the
resources they might be moving them and
it might be trouble conflicting with
your how did you wrote in the beginning
your network card and the OS image get
moved to the other socket so a bit
thinking about how you're going to do
these affinity in them
the last one is there on the virtual OS
image we have needed some of the stack
of the virtual image are very well-known
for the network latencies any type of
i/o between the virtual OS images has
extra latencies and can't handle a very
high load so you may want to test that
part also and now going on the below
device limit you the process level so
let's say within single OS image you're
running four or five Java processes so
there also if you Affinia ties your
process to a couple of sockets or so you
might get more consistent response time
because then they are not going to
collide with each other so these are a
couple of threads when you want the
consistency in the response time
otherwise
would have some time very good response
time and some time not so good and the
some of these examples are the factors
which might play a role that why you
have inconsistency irrespective of the
Java fine now let's try to go a bit more
once you deploy your jaebeum's you go
ahead yes yes so yes we did and that is
where we found a couple of latest
software stat work fine and couple of
older ones so you have the eye of
problems so you may want to look either
the errata out that those virtual images
there are at least even six month ago
there were well known issues in them
about io latencies they were not able to
handle high level of Io
yes so a dual system were more notorious
for the IO that is one of the reason
they can do lot of processing as far as
I know but they were a bit couldn't
handle very high i/o coming in and out
because they were offloading and that
was one of their actually healed so far
for the dual system yeah they have so
that's where you need to know I mean it
there may be nothing wrong in your
application if the problem there itself
on the platform of what and an IO or IO
then you need to be aware of it because
no amount of solving your application
tree architecting going to solve that
one
so same if the virtual OS image has a
problem in terms of how it is passing
the messages no amount of debugging
etcetera in your application are going
to solve that problem you need to look
at what's causing it and is the new
stack of the virtual image is doing
better or contact the vendor that there
is a problem that is still evolving area
and now we have seen good actually are
you among some of the virtual so know
what all virtual react well with respect
to the IO yes in some stack there I yes
that is where when we were running this
application we found that it is not
responding well while it works perfectly
on the native OS images so let's talk
about once you are running the JVM is
running what happens in different
environments and so these are the two
examples on the left side we see
response time increasing pretty linearly
as we expect with a load and by the way
this is the 95 99 percentile and this is
your median so mean and you can see a
pretty consistently increasing on the
other hand we see another kind of GC
type where you see big response time on
some points and we try to code look at
the GC logs you want to use the verbose
and
you will match them separately matching
that these are all related to full GC
pauses etc in your system so when you
have that kind of situation you know
that these things are coming from your
GC and those are like whole all-day
course is about GC tuning but yeah at
that point you you can't do much in your
application you have to go back focus on
the GC if you can tune it out otherwise
you have to look in your application and
what generating objects in which causing
but that is just one way in this
application can easily let you see that
these Peaks are directly related to
garbage collection now a couple of
timeouts with Darrell talked earlier
that we are talking about multiple JVM
communicating and how long we are
talking blocking threads they send some
request and the response come back how
long are you going to wait if the
response doesn't come and that kind of
thing requires a bit tuning too and you
don't want to hard code those timeouts
you want to keep them set through the
command lines or so so on different
systems you can set them accordingly
according to deployment so there could
be some cases where you just need to
wait for short time before you restart
otherwise if you are going to by default
wait for very long time then all your
threads will start blocking because they
are now waiting let's say on a large
system you waiting for like two minutes
or three minute then by the time so many
requests will pile up that with the fork
joint approach or so you might have
almost many thousands of thread being
created on your server and suddenly your
server is like coming to screeching halt
with so many threads so you need to be a
bit tricky about that in the timeouts in
the difference and you also want to see
based on your business logic can you
just drop the requests and retry it or
you have to put some other kind of
message that depends on what kind of
transaction you are handling this is
victor timer but you definitely want
this component to be programmable from
the command line at the start you don't
want to record every time for different
environments
these are about communication
so as we use like grizzly or some other
infrastructure we wanted to make sure
again that we exposed some properties
where we can set let's say my deployment
is very small then I don't need a lot of
connection boots based on if deployment
is large and is getting lot more traffic
so here we have provided the flexibility
of number of sockets because if you have
more communication going on then you
want more sockets one of the reason is
if by default you're going to put a lot
of sockets that means there are a lot
more like Java object and your anytime
the GC garbage collects and it sort of
kicks in you would see long GC pause
it's just because too many of the
connections the other part is once you
have enough connections in the grizzly
and IO side have you given enough number
of thread pool to process the data on
the both end so by default usually goes
like 64 threads or so but if you have
much higher traffic then you do want to
make sure you have enough thread pool
there so you want that component is it
is programmable in the grizzly package
itself that you do expose those
properties so from command line you
could set i want more sockets and here
we are talking the max number and most
of the time you should be able to set it
high enough where if they are not being
used they will not be used you you are
pretty safe but at least you want the
max value to set reasonably high the
other one is about the your application
weight for joint we talked a bit you
there also you need some program ability
in the how many thread pool program
tuning and most of the time you will
realize that you want to set it to much
greater than logical number of logical
processes are available and the reason
is even you send the request your thread
is blocked you would have otherwise that
other third nothing to do and your
system what you will notice if you don't
set it high you will not be able to
saturate the system you trying to give
it more load system will not take more
load because all the thread are waiting
for the response back so you want enough
number of worker thread where some of
them can wait and the others are still
available to do the other work while
they are waiting so that value depends
on your latency how
you are getting the responses back if
the all the systems are responsive you
probably could be much closer to logical
processor and the more time it takes
with the load the more bit so what we
find usually two to three times usually
give at least for this benchmark where
we have one to two millisecond of a
response time but if your response time
is 10 or 100 milliseconds you might need
more thread to set up to larger value
this is because we as Darryl pointed out
we have to set with the type of folks
join we use that was the type where you
need to keep it fixed from the beginning
because there is another managed block
thread pool in the fork/join which let
you increase dynamically the problem on
the dynamic is when you are waiting and
some long GC pause happen on another
agent that is causing so many threat to
be created that we found some of the
time it was like 3040 thousand threads
being created and that server can't
manage it and now it's going to have a
cascading effect you got that many
thread you can't response another
servers you waiting response from you
that one start so the whole system
almost like starts shutting down so that
approach did not work for this
application the kind of latency we were
expecting here in milliseconds so we
have to go fix number of threads so that
we don't go over the maximum limit and
this is another example for one of the
interesting thing we find if you look at
so we were getting very smooth the
response time and then there is a sudden
jump and then the response I was again
is smooth so this one was puzzling how
can my median suddenly jump so much and
we took a couple of weeks to debug it
what we found out that by default on
linux the HP ET not the timer over RT
DSC was is the time are usually at the
boot time system checks that if you're
all CPU can be synchronized for timing
it will go with our TDS you the regular
time timer but if it finds it cannot
synchron synchronize them for some
reason and I found that couple of
customer places that they couldn't for
some strange reason
some bug or something in the in the
Linux or those deployments by default it
goes with the HPD time on HP T the high
precision event timer the interesting
thing about that is yes that's your
single source of time it works fine when
the load is not high but once a load on
your timer function goes above that HP t
can't handle the contention contents and
becomes very high and with that you will
notice in the CPU utilization time for
the kernel with increase and your
response time suddenly jumped beyond a
utilization and you will not be able to
find this unless you really test it for
the increasing load and you can put high
low and the jump is pretty significant
it's almost going from below that in one
millisecond that's 10 that is one
millisecond from one millisecond it
jumps to almost 100 milliseconds almost
an ex jump on the response time on
immediate so you need to be careful for
that just based on the time at kind of
thing you can so this one we were one of
the developers and the architect who did
for the this whole benchmark I just
asked him that what are the top ten
messages would you suggest based on
several scaling thing you found the
architect etcetera and these are the
main takeaway from him we're number one
obviously reduce your communication as
much possible because that is like an
expensive process so don't it's not free
lunch so to try to be careful there
other one is if you have to send the
data if you can send any big chunks
possibly do that the number it would be
like combined messages in batches as
they're always talking that if if
nothing else then at least try to do
them in the batching and that gives you
a lot more consistency but this one you
need to be careful that you don't want
you want to do both timing and the
number of batches when you want to do
the batching number seven would be if
you care for the throughput then if you
can then go with the asynchronous model
where you can send it something and
produce a consumer model you send
something and you will get it as
back if you care for the strict
correctness and the business logic then
use a synchronous model that means you
have to make the thread wait and get the
response that that is the only confirmed
way to get your response back so many
time in transactions with the business
model you probably want to have there's
no choice but to go with the synchronous
model but it is more challenging from
scaling point of view and if you can
reuse the connection channel reuse them
because that's otherwise another big
bottleneck with respect to scaling and
performance number four check errors
because it and as much as you can log it
you try to do because if you have to
debug the IPC communication code it can
go into several month and we are talking
live example where Darryl was working
for four months on some strange bug
going through the responses wouldn't
come back and to track it out where it
is we even found a bug in the griz-lee
response code and we submitted and they
fixed it
the number three is a used framework
when appropriate that we already talked
with like grizzly and IU another
framework plus among the agent also it
gives you the consistency one programmer
goes away next one comes at least you're
the well-documented framework there and
there you can reuse number two is a
profile and tune of course that we'd
several of the tuning we talked about
and number one he told was very
interesting is don't believe when
someone says it will never happen and
the example of that is people say this
error or this exception will never be
thrown or this situation will never
happen he says don't trust I have seen
in this code and the other code many
places someone put very very strong
remark it will never happen it will
never go this he says don't be paranoid
when it comes to inter process
communication because that's nothing
might be you might be wasting many
months that thing is happening and you
had not put a check for believing that
part and it can take a long long time to
debug
I think with that if you have any
questions you can find more information
for Darryl here Darryl any more things
and I think I did add one of the
reference here that if you want the
benchmark
it's the spec JB 2013 benchmark we did
working last three years and we did a
lot of response time and other
components you can find at the spec
website for and once you get that there
are two type of licenses you can get
much easier license for the universities
or so nonprofit much cheaper and a bit
more expert it's like $350 I think for
universities nonprofits students and the
professional is 1,500 but you got a
complete code everything the whole full
package to look at and more results so
if you have questions you can
yes so we have we have implemented so
that is another interesting part in the
benchmark we have done even though the
compliant version allows you only one
thing we have at least like 200 other
implementation either you can set them
with the command line or you can try it
so we have even even in the framework
you can set in the batching also beef at
the command line batch for 5 or
sometimes don't use batching just
individually send it and we kept them so
someone just some researcher wanted to
try what are the impact of them you can
actually enable several cases so in the
batching also these allow the how many
do you want to batch how long do you
want to wait those parameters you can
set from the command line to test the
impact of them and so ultimately we
landed up sending the messages with some
batching and the request goes
individually separately so we did that
was the interesting part was that the
batching were perfectly fine at a high
utilization because it was filling in
fine
and at the iord you know utilize and we
saw suddenly why our response time
increased because we have the whole
graph which we can plot so that's the
good thing in the benchmark is faster so
here like that's the thing the as you
are using you can see here and I can we
can plot it one against other and we saw
like why with the batching our response
time increase at the lower utilization
doesn't make sense but then we say oh
because we are not collecting enough and
then you need to put timer function and
the other part is that we are talking at
the lower end a one millisecond or less
response time and it's very hard when
you want to batch your weight at least
you probably talking at 10 millisecond
so that itself ensures your minimum time
in like at least 10 so that was tricky
part to balance
so for right now we didn't we have we
didn't go that is the messages and
requesters and immediately and the
messenger also son I think immediately
they we have the batching implementation
in them but due to the one millisecond
carotid or so the currently the is it
get individually sent yes
any more questions okay well you have
the email of my own Darryl if you ever
have questions thank you everybody</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>