<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CON5394   Leveraging Java Optimizations to Improve Density in Cloud Environments | Coder Coacher - Coaching Coders</title><meta content="CON5394   Leveraging Java Optimizations to Improve Density in Cloud Environments - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>CON5394   Leveraging Java Optimizations to Improve Density in Cloud Environments</b></h2><h5 class="post__date">2015-12-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/X5N4_dhjCaU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">just to start out a little bit about
myself I'm the senior software developer
with IBM I work in the runtime
technologies team we build ibm's java
virtual machine as well as our no
virtual machine and possibly others in
the future I've had a pretty broad
experience I started out working out in
products where we built encryption
related products I then worked in
companies that provided applications
built on j2ee to provide services and
you know it was actually an early
version of providing services in the
cloud we provided credit card processing
as well as electronic invoicing and
stuff like that and more recently if the
last 11 years have been focused on
developing runtimes and that's what I'm
going to talk to you about today at the
bottom line of the slide here is my
contact information so if you have any
questions or want to follow up there is
my information i will post the slides at
the probably within a couple hours after
the presentation on SlideShare so
they'll be available there as well just
to give you a little bit of an idea what
I'm going to go through I'm going to
start out by going through the
motivation about why we're interested in
you know optimizations in the cloud I'm
going to introduce shared classes which
is one optimization that's provided by
the IBM JVM but a lot of what I talk
about will apply to other optimizations
as well how the you know basically I'm
using it as an example of how the
differences in the cloud environments
can affect your optimizations and and
how you can work around that I'll then
talk a little bit about specific cloud
environments so what what do I mean by
cloud environments that includes things
like darker Cloud Foundry but some
others as well as well go through those
and then finally I'll use shared classes
an example of how we can leverage that
optimization even though the constraints
and environment is a bit different in
those particular configurations so just
to start out why are we interested in
this well footprint we heard in the the
keynote speaker speaker the keynote
speakers talk about modularity and one
of the reasons they talked about that is
willing to slim down our run times for
when we deploy them in the cloud as we
move to cloud up cloud environments and
we're using things like microservices we
tend to deploy smaller applications but
many more of them so the footprint of
individual application matters more and
more so footprint still does matter and
in fact it probably matters more and
because we have lots of small apps the
overhead that's associated with anyone
JVM can have a significant impact so you
know you can see the problem here that
if I've got three micro services running
on three different JVMs and I'm using an
app server to support that i'll end up
having a copy of the classes from the
JVM a copy of the classes from the j2ee
server as well as my application and
particularly as we move to smaller
applications the ratio can be you know
tilt in the favor of the supporting
framework as opposed to your actual
actual application itself and so having
a separate copy of that in every running
vm can end up costing you a lot of extra
footprint in terms of startup that still
affects customers in the cloud
environment in fact I'd argue it affects
the more as opposed to you know existing
deployments where you might start up
your application let it run for a long
time in the cloud we're much more
dynamic so we often have things like
support for scale up over head over
there I have a picture of the bluemix
dashboard and if you look at it
carefully you can see that you can
configure scale out parameters and you
can say well when I hit a certain
latency in terms of my application I
want to scale it scale out additional
instances and then when I come back down
under a certain threshold I'm going to
scale it back and each of those scale
outs ends up being like another startup
so we're going to have many more events
that are a start-up like and restart is
another one again cloud has this you
know tends to have the approach of you
know if we fail fail fast and just start
up a new instance so we end up having a
lot more startup type events and so it's
important that we have quick startup and
can do that quickly and so for example
when we're scaling we can actually start
providing the extra horsepower quickly
as opposed to taking a long time before
that can happen so we've been working on
optimizing the Java Virtual Machine for
20 years pretty much since the beginning
of when it was started I'll talk about
one of them which is the X shared
classes option and it's intended to help
you with startup and footprint which are
the two things that I'll be talking
about today in terms of startup
it you can see this is where I've turned
on that option running tomcat and you
can see that you know if I use no shared
classes in the first bar it takes a
certain amount of time the first time I
run with the share classes option it
takes a little bit longer because we're
doing some extra work they're going to
help us on subsequent runs and then we
can see in the third bar from then on we
actually have a pretty significant
savings in terms of how quickly we can
start up and on the subsequent slides
I'll talk specifically about what we're
doing and how that helps start up
similarly for footprint we can see that
with the extra classes option we end up
sharing artifacts within the JVM so
instead of having a separate copy of
every single class that you're loading
we end up sharing those classes across
the VMS and as you have more instances
you can see that the total memory that
they use ends up you know being less
than if they each had their own copies
so what is shared classes well in our VM
we actually separate out the read/write
portion of the classes from the
read-only portion of the classes and
that means that we can actually have a
shared version of the ones that are read
only because they're actually going to
be the same in every copy the JVM and
you know what are some examples so
things like class file bytes or is a
good example of the actual byte codes
for your methods are going to be the
same every copy of the JVM provided
you're running they're the same things
in addition to that we also have
additional things to save memory like
string deduplication again if you
configure a j2ee server you can often
find that you've got a lot of strings
the strings can start with the same past
where you've installed things and so we
can actually do the duplicate those
strings and save a fair amount of space
as well we can also stick in in there
jit data so if you're familiar with the
vm you know that as as it starts up
methods are run interpreted and as the
JIT decides that a method is going to be
used a lot it compiles it and you know
into a native implementation and in fact
it'll actually recompile it over and
over again is it decides it gets more
and more important instead of doing that
we can actually put jit data directly
into the shared class cash itself so
ahead of time compilation data so that
instead of having to compile it as we're
starting up
can start with something which is
already optimized we can also do
interesting things like separate out
different portions of the read-only data
because what's important to your
footprint is not just the memory that
you use but how you use it because you
can end up having pages which are
actually paged in or paged out so in
this case we take our class debug data
which is only going to be used if you're
debugging particular methods and put
that in a second separate part of the
cash so that it won't be paged in you
know so if you're only using the bites
and you're not doing debug you'll get
those actually being in memory and then
the debug information won't be in memory
and your total overall footprint will
end up being smaller how do we actually
implement the cash well we can use sisti
shared memory on z/os we use POSIX mem
map on Linux and AIX and create file
mapping on Windows and so what we end up
with is a single image of the shared
class cash in memory and multiple
processes would share that single image
meaning that you only have one set of
virtual pages and you know one set of
physical pages back in them and they're
all used by the different processes
so how does that actually result in
benefits share classes we get faster
startup because when you load classes
there's actually a fair amount of work
that we have to do we have to read them
from disk we have to parse them and then
we have to put them into in-memory
representation that the JVM needs to be
able to use it efficiently and that all
takes time the other thing that takes
time is that you know as we start up
there's lots and lots of methods which
are run just to get you to the point
where you're going to start running your
application so thousands and thousands
of methods lots of those are identified
by the jet as being important so they're
going to be compiled and so you end up
every time you start up going through a
process where you load all the same
methods you end up getting them going
through the same cycles as you determine
it they're important you end up throwing
away the intermediate states and so why
do this over and over again we can do
things like putting the head of time
compilation data into the the cash or we
can even put hints that will tell us to
say well this method is going to end up
being really important so don't do the
intermediate steps just go from zero to
the most optimized implementation and
all that ends up meaning that we can
actually start up significantly faster
and I'll show some of that data later on
in terms of lower footprint it's pretty
straightforward you know if we have a
copy multiple copies of that read-only
data we're going to end up with a larger
footprint so we end up putting that data
into the shared class cash we memory map
that in to a single image and because
each of your jvms is sharing that
portion of the data their total
footprint is smaller and as we add up
the multiple copies we end up with a
total footprint which is smaller than
the overall you know then if they each
had their own car so we end up you know
by default getting startup and footprint
benefits this is just one of the
optimizations which I'm using as an
example there's other optimizations so
for example EE servers you can have JSP
precompile so you might want to pre
compile your JSP so you don't do that
every time you start up in the nazarone
presentation I was in earlier this week
they have an option to be able to store
in two files some of the pre compilation
or the compilation artifacts as well so
again when you go to run your
your scripts it's going to you know be
able to do that faster so there's lots
of optimizations that use either you
know shared memory or physical artifacts
on disk so now if we start to look at
the cloud environments and you know
really there's a lot of different cloud
environments all with some pretty
different characteristics they can range
all the way from you know bare metal
servers down to platform-as-a-service
better middle servers you can get in the
cloud so soft layer is an IBM product
that lets you actually go out you can
use open stock you can dynamically
provision machines they're fully
physical machines they can be customized
with GPUs physical hardware so you can
basically get machines that are just
like ones you deploying your local data
center butter out in the cloud through
bluemix another IBM product you can get
you know the other options so you can
get virtual machines we're all familiar
with those but you can get them in the
cloud we can get dr. instances so I can
take my docker images and I can deploy
them up into the cloud as well and i can
get platform as a service instances and
that's where really you just focus on
your application itself so you bundle up
a jar file or a water file and you can
push that to the infrastructure and it
worries about all the other components
that you need to actually run your
application so for example in a java
application it may bundle it with a Java
EE web server and give it an environment
that it can execute in and then
basically set that up and get it running
so we have a you know a good variety of
different environments that you can get
in the cloud and each of them has
slightly different characteristics so
I'm going to look at each of those and
see how they affect potentially affect
the optimizations that we've been
working on over these last number of
years so the first one is bare metal
well you know in this environment
multiple applications are common shared
store we have shared storage we have you
know shared memory this is really just
like the environment that we've been
optimizing for since the beginning of
Java you know we end up with one
environment multiple processes the
little green thing is our shared class
cash we can share it everything's good
we get our faster store startup and our
lower serpent just like we expect
if we move on sort of moving up the
abstraction layer in terms of the
different environments we get in the
cloud the next one is a virtual machine
so you know one characteristic it's
easier to spin up a virtual machine than
it is to provisional whole new physical
machine so we have we see a trend
towards you know smaller deployment unit
so we're going to have smaller
deployment news and probably more of
them the difference here though is that
memory isn't shared across virtual
machines and storage isn't shared across
the virtual machines eat either now even
though you're sharing the same
underlying hardware it each looks like
you have your own storage in your own
memory and so if you look at the picture
i have over there we now have two shared
class couches they each have their own
one that they created dynamically the
good news is is that we have persistent
storage so the next time if I shut down
my virtual machine I start it back up my
shared class cash is still going to be
there and so actually I still get the
benefit of faster startup so in that
case you know I'm still doing okay
unfortunately I'm not going to get my
lower footprint though because I've got
to cop these are the shared costs cash
instead of one and in fact you know
because the shared class cash has a
little bit of overhead we've probably
made things worse as opposed to made it
better because we have two copies of
things which are slightly bigger and
we're not sharing so unfortunately in
this environment we know we've still got
the faster startup but we don't have our
lower footprint moving up you know one
more step docker images so if you're
familiar with dr. you know you build
your application basically as a set of
layers so you can start out with a base
layer that has for example the operating
system you can layer on top of that an
image that would have your middleware
and your jvm and then finally you
probably have a layer that actually has
your application in it in itself and if
you look at the picture over here on the
left hand side you know we can see that
sort of on the disk we end up with the
three physical layers but when we start
the containers that use those layers we
end up having our own state in each of
the containers kind of like we did for
virtual machines except for the fact
that in this case until we write
something to those layers so we end up
in the doctor and you know when we use
these docker images they're their coffee
on right so as long as I don't write to
them or change them I end up sharing
those file systems but the state that
i'm going to create because that's you
know by definition something that i knew
that i'm writing those are not going to
be shared and they're going to be
specific because it containers
themselves so just looking flipping over
for a second to you know look at a
couple of images so i've installed
docker on my in a virtual machine on my
machine and i went ahead i went ahead
here and you know you can list the
images that are installed on your on in
your doctor instance by saying docker
images and i basically create a set of
advantages that match those layers that
i talked about on the previous page you
can see I have the in booted images the
base I just loaded that up from the
docker hub I then created an image
called Liberty java where i've installed
the the g the java ee server along with
the JVM and shared class caches and so
forth and then finally have an image
which is you know my application now if
we actually go and start up you know
using those images i can just say docker
run interactive and say you know I say
in boo to you know in this case I'm
pretty much going to have just my stock
environment where you know I've got a
base in boo too but not too much else
installed and if I look in so for
example here user lib you can see that I
don't have any of the things that are
specific to liberty or anything about so
I'm going to exit that image and now i'm
going to start up instead of using the
basin budu image i'll use the one that i
put those components in so this time if
i go to user lib you can see that i have
a directory here called liberty java
I'll go into that and liberty is just
what I bm's lighter-weight g2e server
oops so when they're I have if you've
ever used the Liberty you'll see the wlp
directory which is where some of the
components are installed you can see
then I have an SDK directory which is
just my java virtual machine and then i
have a directory called caches so this
is just a shared class cash that I
created beforehand and I'm just showing
you this because when I talk a little
bit about later on how we make some of
these optimizations work this will kind
of be interesting the one last thing I
want to do is I just want to start up
the application layer so in this case I
create a layer which you know would be
what you would simulate your application
running and generating state
so in here I don't have anything but I'm
just going to say let's touch a file to
simulate generate you know the
application generate some stayed on its
own so maybe it's going to generate a
cash I can see it is there the
interesting thing though is that you
know if I exit that container and now
I'm going to go and start the exact same
image going to go back to the home
application doing Alice nothing's there
so the key thing there is that in this
environment it's stateless when I go
back I basically started from scratch
and so any of the state that I've
written while my application ran the
first time isn't going to be available
anymore so flip back to the presentation
ok come on where are we
okay so now if we look at the same
pictures we did for the others in terms
of docker so again we're probably going
to expect even smaller deployment so
sharing is more important it's really
easy to start up these doctor instances
they start fast the the amount of
overhead in terms of disk space and all
that's very low so we're probably gonna
have even smaller deployment units and
even more of them again like virtual
machines memory isn't shared by default
across the containers and storage as we
saw isn't persistent so unfortunately
now because we are going to create a
shared class cash is going to be we're
going to have a different one in every
container and like I showed that isn't
even going to persist to the next time I
start the container so I'm not going to
get any of the benefits of startup or
sharing so now I'm in a position where
I've just made things worse I'm not
helping at all moving on a little bit
higher to sort of one layer up again in
the in the abstraction is platform as a
service environments Cloud Foundry is an
example that's used by many companies
it's the basis for our bluemix as well
and it kind of has a two stage or a
multi-stage process of deploying your
applications there's something called
the staging phase and that's where you
give the system your artifacts for your
application to a jar file a war file and
it basically takes that and for example
if you're deploying a java application
says okay i need to bundle in a java ee
server with this it puts that all
together and it creates what it calls a
droplet and the drop really is just a
zip file that has your application
components bundled in with all the other
components that are needed to actually
run that in this the PHA the later phase
where you actually start that
application or if you're going to scale
out to have more instances it takes that
droplet pushes it to the droplet
execution agent and then it actually
extracts it onto the disk and then it
sets of what they call a warden which is
a container something like a Troodon
steroids where it gives you your own
file system your own you know if you do
a PS you see only your processes so it
looks like your own environment and then
it starts to start you know runs your
application
not environment one interesting thing
about that environment is droplet sizes
affect your start and push time so when
you're actually pushing your application
and you're in the development cycle the
more you bundle in there the longer it
takes to push that back and forth
similarly if you have a really big
droplet it's got to take that and push
it on to the droplet execution agent
expand it so that takes more time so you
want to be careful what you have in
there and this is just a quick picture
of you know once you're in the you know
you've pushed your application to to
bluemix cloud foundry in this particular
case and you've got up and started you
can actually log in and you can look at
the file system with the fallen logs
option and you can see that you know it
has some of the same things that we did
with the doctor instance where I you
know I have wlp where Liberty is
installed I have an SDK installed a Java
and the different components for my
application so again in the cloud
environments the in the platform as a
service environments so again it's going
to be even smaller deployment units it's
really easy i don't have to set up you
know a dog for instance or any sort of
environment i really just take my java
application and its components push in
the infrastructure worrying about all
the rest so i can do that really easily
again we really wish we could share
memory because we're going to have more
of them again like docker though the
warden containers basically make it by
default so you don't have shared memory
and you don't have storage persistent
persistent storage so again we're going
to be in the position where we're going
to create a new shared class cash isn't
going to be shared with the other
instances and we're going to create it
from scratch every single time so we're
not going to get the faster startup or
the lower footprint that we're aiming
for so just kind of looking at a summary
if you think about our existing Java
optimizations we have to be careful
because the differences in the
environment may mean that they're not as
effective as they were before and just
to summarize you know in our bare metal
we get bare metal machines in the cloud
well okay we have persistent storage
shared storage and shared memory so
we're probably fine don't have to worry
too much there if we go to virtual
machines we still have persistent
storage so optimizations that depend on
that will still
okay which is good but if we have
anything which is based on shared
storage or shared memory we're going to
be in trouble and then in you know
darker and platform as a service
instances we don't have any of those no
persistent storage no shared storage no
shared memory so our optimizations are
going to have a hard time working so
what can we do well there are a few
things that we can do and that's what
the rest of my talk is going to be about
going through the options and then you
know showing you some of the experiments
that we've done along with the data we
collected having done that so if we have
persist persist install then actually we
don't have a problem in terms of
dynamically generating our artifacts so
we can dynamically generate the shared
class cash and we still at least get the
benefits from startup if we don't have
persistent storage we need to start
looking at things like adding cash
sharing mechanism so we could add shared
storage although often things like NFS
don't have the locking properties or
other properties that we want in terms
in terms of making those optimizations
work effectively and another option is a
cache server so that's another server
where we can push caches to and then
pull them back later on and I'll have a
few examples of how we apply those
different options generally in those
cases you know we're using something
like a prebuilt cash if we can't build
it dynamically and reuse it maybe we can
build one beforehand bundle it in with
our application somehow or provide it
through the cache server and still get
some of the benefits we would generate
that through a warm-up phase so take
your application run some load generate
a cache and then push that on to the
along with something else it does have
some downsides as I mentioned so for
example in Cloud Foundry type pass
environments if you have extra data in
these caches could be like 60 megabytes
that can actually significantly affect
your push time so you don't necessarily
want to just take the cash and stick it
wherever because it can make a
significant difference
in terms of footprint you know one of
the things we can start looking at is
leveraging support that's in the u.s. so
something called transparent page
sharing is a feature that's in many
modern operating systems and what it
does is it it leverages the fact that
you know we have virtual memory virtual
memory can be back by physical memory
but it doesn't necessarily have to be at
all times but they recognize that if I
have two virtual pages and they actually
have the same contents I can point them
to one physical page and save memory and
you know the degenerate case which gives
you the most saving as if your page is
filled with zeros you can basically just
note that and you don't actually have to
store anything so the transparent page
says transparent page showing features
basically go through memory and look for
pages that are the same and if they find
two pages are the same they basically
deduplicate them by pointing the city
you know multiple virtual pages at the
same physical page this of course only
works if you're not modifying the memory
in those pages if you know as soon as
you modify the memory it's a copy on
write and they'll split it back out so
you can get your own car that's why it's
transparent you basically get memory
savings without having to do anything
however if you work a little harder you
can actually maximize the chances and
opportunities for page sharing so if you
bundle a prebuilt cash or use a cash
sharing mechanism to get a cash on to
your system you you basically have an
artifact that you know will have the
same pages in memory and therefore the
transparent page sharing can actually
find them and deduplicate them and what
you know the reason you can't just have
it create the caches dynamically is that
class loading is not necessarily
deterministic I can run in parallel
things happen in different orders and so
even though you end up with the same
functional result you don't end up with
something that's going to have the same
layout in memory and so in other options
you can try and actually guide that a
little bit more so that you actually do
end up even if you dynamically create it
end up with the same artifacts the other
option is you can say well you know we
know these these abstractions virtual
memory sorry virtual machines stalker
has environments they try and make it
look like I'm running in my
environment but for some of them you're
not actually you don't actually have
your own environment so maybe we can
actually still share memory accost
containers and maybe we can figure out
how to do that so I'm going to look at
some we'll look at what we did in each
of the different types of environments
to try and work work with what we had so
in virtual machine so persistent storage
again so with the share class cash we
already get the startup foot the startup
benefit so we don't have to work on
anything for memory which we use the
option of trying to build a cache that
we could then you know have transparent
page sharing leverage to deduplicate and
end up you know providing better
footprint there are a bunch of options
that you can use you can pre create a
cache and just install it in each of the
virtual machines you could use a cache
server where you basically go off and
pull the cash over when you need it you
can even try and offload the cache
itself so when you actually go and try
and get individual elements you'll pull
those those elements back from the cash
the other one is we can try and dry
guide the the creation so you create a
cache which looks the same they all have
their pluses and minuses and there's
actually been some really good work by
one of the IBM research teams and if
you're interested in reading about that
specifically I'd go I'd suggest you go
off and read this this article they went
through a number of these different
options to see how they could actually
end up you know leveraging transparent
page sharing and virtual machines to get
member so it's a good read I didn't work
on that specifically so I'm not going to
talk about that pieces but that covers
off the virtual machine side of it yeah
go ahead
so vote for your virtual machines you
would need to be able to know you need
to know the of multiple instances
running on the same the same instance
with things like Cloud Foundry if you
start up enough instances yeah you're
going to be if you're lucky enough to
run some of those same instances then
you're going to be able to get some of
the benefits well yak you're good we
didn't do any of that in our
experimentation that's certainly
something that might make sense now you
have arguments that you want your
instances to run separately as well
right but yeah so but yes you could
definitely you know if you were you know
say the the cloud foundry provider and
I'll talk about some specific things we
did there where you need to actually
modify the cloud foundry infrastructure
you could take what we've done combined
with that saying okay we're going to try
and move things you know all apps of
this particular or you know business
we're going to move most of them onto
these two machines and to be able to
make that work but that's certainly yeah
certainly a concern in terms of docker
so you can say well let's actually say
let's create a prebuilt cash which is
what we did and use that and you could
get some of the startup benefits by
doing that but you can actually do
better what's really interesting is we
figured out you can share memory across
containers and it's not actually that
hard so if you actually put your shared
class cash in one of the layers the
lower layers and your memory map in one
of those that dot file that shared
across the layers you actually end up
with shared memory that is shared
between the containers and that's
because you know in darker you're
actually sharing the Linux kernel and
when you provide a file from one of
those lower layers and provided you use
an appropriate file system like aufs
you're actually sharing the same file
until you write to it and that actually
translate that actually maps to sharing
memory which backs that file as well so
it's really a great thing that actually
very very easily you can now share
memory we validated that through you
know the Linux tools lsof looking at the
pit maps and we can see we're actually
sharing the same memory as we increase
the number of container
you know we didn't increase the amount
of memory for those shared components so
by doing this we can actually end up
with the picture that i have here where
we have our containers they have their
individual states to the state that
their writing will still be in their own
containers but we end up with this
in-memory cache we have to actually set
up our shared class cash so that it's
read-only and you know pre-built
read-only as opposed to dynamically
generator or even modified after it
starts up because that would break the
sharing and we would no longer have the
shared memory so in this case you know
we end up with shared memory and you
know we built the images in a way where
we have this image where we've got the
jvm liberty and the pre-shared cash the
nice thing is that this doesn't only
apply to the cache file itself but by
putting Liberty in the JVM into the
lower layers we also share the dynamic
libraries that go along with those so
you get a you know extra memory savings
from that as well and just to look at
some of the results that we measured
having done that you can see it's pretty
significant this was for you know a
hello world type application where we
didn't run a lot of application code on
top of the JVM or the Java EE server but
as we as we move towards you know
microservices we get closer and closer
to that use case where we have a little
bit of code and then the the underlying
infrastructure that supports that we ran
three different options one which is
sharing which used our technique of
putting the cash in the right layer no
share where we basically said okay we're
just going to dynamically create the
shared class cash in the you know in the
container where when we run and then we
had no share no cash because you know as
we mentioned before because it's like
the first time every time doing work to
create a shared class cash is probably
not worth it because we're going to redo
it from scratch every time and you can
see having done that that actually you
know going from the default to having
you know put the shared class cash read
only in the right layer we've probably
got the you know the startup is pretty
much half of what it was otherwise and
the memory savings as we get to 10
containers is about half as well so
we've made significant you know got a
significant benefit just by thinking
about
the environment and where we can put
certain artifacts so that they actually
are shared so now we're back to yep we
have our foster startup and our lower
footprint which is great in the cloud
foundry environments we experimented
with two different optimizations so far
one we use the same kind of approach
that we used in doc were to actually
share memory across wardens this does
require a change to the cloud foundry
infrastructure so you know if you're an
end-user customer this isn't something
you can necessarily do on your own but
if you operate a cloud foundry
installation like we do with bluemix you
can modify the backend system so that
you can take advantage of some of this
memories the other approach we took was
to employ a cache server and this I need
to call out and give some credit to both
Cannon and panels who are researchers
that work with us from the university of
new brunswick iBM has a research
relationship with them where we have an
ongoing project with about 30 or so
students and professors who work on you
know looking at the Java Virtual Machine
and seeing how we can make it better you
know with a specific focus on things
like concurrency multi-tenancy those
kinds of things so they did a lot of
this work and so i'll give you know
wanted to call that it so in terms of
the cloud foundry environment we
basically took the the JVM liberty and
shared class cash and we move that to
the droplet execution agent instead of
having being part of the droplet that
was actually deployed and the the
droplet execution agent already had
options to say when you start up a
container I want you to map in these
particular directory so we mapped in
three different directories one with the
JVM one with the shared class caches and
one with the liberty infrastructure and
so what happens is in each of those
wardens you get a read-only copy of
those files it didn't necessarily have
to be read-only but because you can have
different customers applications running
we didn't want to have any interaction
so this a late a lot of security
concerns by saying well we'll make it
read only we're actually going to create
a cache which doesn't have any classes
from your applications it's just the
core JVM and
to EE application server clauses so you
know in the end we ended up with the
same file in all of those contains those
werden containers and again like darker
because we're running on Linux and we're
actually sharing exactly the same file
because it's a file that comes from
outside the container we end up sharing
the same memory and we got the same
behavior and you know again we've got
shared memory across the containers
which gives us the savings that we were
looking for so looking at similar
measurements you know in this case we
have no sharing shared libraries and
sharing libraries and cash this is a
little different from the last one one
of the additional bennett's benefits of
moving the shared class cash and the JVM
to the DEA is that we didn't need to
actually have that in the droplet so the
droplet itself was much smaller so when
we're pushing our application or when
the application was being taken and
deploy down to the EAS there was much
less to exact to unzip less the transfer
between machines and so you get a
start-up benefit just from having
reduced dot and then when you turn on
the shared class cash you get the
additional benefit of having the the
shared class gosh speeding things up as
well and on the memory side you can see
the cheering the libraries helps a
little bit that's because again you're
sharing the shared libraries the you
know the dynamic libraries the esso
files for the JVM and from the liberty
and then you get you know most of the
benefit from sharing the actual shared
class cash itself because that could be
you know 30 40 60 Meg depending on the
classes you forced into it
the other approach we took was to use a
shared class cache server this doesn't
require any changes to the cloud foundry
foundry infrastructure so you can deploy
something like this as an end user if
you wanted to and basically we created a
cache server and we deployed it as a
bluemix app itself so you know using the
infrastructure on its own and the
approach was that you know you start
your application the first time it
doesn't have any sort of shared class
cash you run your application maybe
drive some load and at some point it
says okay I'm going to take a snapshot
takes this here in class cash and pushes
it back to the cache server that's kind
of nice because you don't have to worry
about the dynamics of you know creating
the cache yourself or if I updated my
jvm might now have to create a new cache
the cache creation side of things can be
happened can be done automatically for
you the first thing we tried was just
then okay every time we scaled out or
restarted to pull that cash you know as
the first step we're starting Amplatz
pull the cash let's start to run
unfortunately that doesn't turn out to
be that useful because the time to pull
the cash over from the server outweighs
any benefit you get from starting up so
the next thing we we discovered is that
what we can do though is we can actually
force a repo and during that sorry
enforce a restage and during that
restage you know as i mentioned you know
the cloud foundry has the two-stage in a
sort of staging and then its deployment
we would force a restaging in that in
that stage we would say is there a cache
and if there is we would actually pull
it in it get it bundled into the droplet
itself and so in that way we end up with
a droplet that now has a cache and gets
pushed out automatically when you
restart forced or scale out and this
avoids that the poll time to try and
pull the cash over when we're actually
running and we found that yeah we're
doing that we could actually now get
some reasonable benefits so as I
mentioned way at the you know the very
start one of the important things is
when we go to scale out if we have a
certain amount of load and the system
decides that I need to have additional
instances to support that load one of
the things we're going to want to be
able to do is to start
serving that load quickly well we're
under pressure so if it takes me a
minute before I actually can start my
second instance can start helping out
that's not going to be so good so this
shows the default case where we didn't
use our technique at all no chair NSC
shows what we turned off shared classes
and then finally decals was the dynamic
cache server where we actually said you
know we've gone through the restage and
now we've got the cash there we're going
to when we're going to scale it and you
can see that the green line actually
gets you know up to the point where
we're driving in and supporting more
load more quickly than any of either the
otherwise I think it's will see it on
the next page but it's about three to
four seconds faster which can be
significant when you're trying to scale
out your your instances so in this case
you know the default it took about you
know there's the first instance when you
actually push your overall application
and then there's the the next instances
when you scale or restart so it was
about 11 seconds by default you know no
share classes help that a little bit
because you know as we said it takes a
little bit more work to to prepare the
shared class cash but then you know
we're down about three seconds less on
the second and subsequent implications
with the cache server being pulled over
the next thing we looked at is to said
well okay you know scaling outs in
trusting and that makes it better what
about restart because that's often also
another common instance so this shows
the behavior of restart you know at 120
seconds we asked it to restart and you
can see that the green line there we're
actually back up and serving traffic
much more quickly again it's about three
to four seconds faster than without the
technique in place it just shows that in
a in a picture where we can see that you
know deke as is about four seconds
faster in terms of the restart times
that we measure and this one in
particular we only looked at startup
times versus the actual memory sharing
so in that case you know the earlier
question about you have to get them on
the same machines well in this case the
answer is no we actually you know the
the caches were built for each
application so when you scale out that
multiple
instances it didn't matter whether those
instances actually ran on the same
machine or not in fact running on the
same machine gave us a little bit of
trouble in terms of trying to measure
the benefits because often in these
cloud environments if there's nothing
else running in your particular
environment you get all the resources so
we try to measure the scale out benefits
and it's like well it's the same so we
actually had the cap how much we could
get through in a particular instance of
an application so that when we scaled
out and have the second one running on
the same machine it would show that we
could actually go up because we have the
second instance running there but yeah
so in that case you know you don't have
to worry about getting to the same
machine and you're still getting pretty
good benefits in terms of your startup
so that's what I want to talk to you
about I just you know in summary you
know start up in footprint still matter
and they probably matter more than they
did before in the cloud there are some
key differences in the cloud environment
that affect existing optimizations but
by thinking about them understanding
what's going on taking a little closer
look you can probably work around them
and get back to the performance that you
know you would you benefits it you could
get out of them before so I think I have
time if there's any questions sure
in terms of the cash or well we just
wrote a java application which basically
could say okay I'm going to take your
file I'm going to store it in memory or
actually no I think it was on disk
within the container but it was
ephemeral for the tests and then push it
back to you for winking request
so with bluemix I think by default we've
already turned off the shared class cash
because it doesn't you know because
you're starting from the beginning you
know it doesn't make any sense I you
know the in terms of the optimizations
for the one related to modifying the
infrastructure so that's something we'd
have to do to be able to give those
benefits we charge by memory so in that
fact it would be more benefit to us in
terms of us being able to more
efficiently support the memory that
we've given you right as opposed to you
know saving money for the individual end
users
in terms of no there wasn't any
modifications to the JVM itself that's
really about the infrastructure in this
case in basically setting up the
environment and you know putting the
putting the artifacts in the right place
so like in dr. putting them in the right
layers in the case of Cloud Foundry
putting them on the DEA and is instead
of you know in the container itself and
then mapping them so that you would get
shared memory as opposed to something
that's going to be a separate copy okay
well thanks for coming out i think this
is the last session on the last day so i
thank you for the dedicated people for
sticking it out thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>