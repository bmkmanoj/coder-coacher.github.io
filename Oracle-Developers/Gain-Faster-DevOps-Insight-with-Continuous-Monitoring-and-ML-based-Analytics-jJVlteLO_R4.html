<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Gain Faster DevOps Insight with Continuous Monitoring and ML based Analytics | Coder Coacher - Coaching Coders</title><meta content="Gain Faster DevOps Insight with Continuous Monitoring and ML based Analytics - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Gain Faster DevOps Insight with Continuous Monitoring and ML based Analytics</b></h2><h5 class="post__date">2017-07-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/jJVlteLO_R4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone and welcome to the
session on gaining faster DevOps insight
with continuous monitoring and machine
learning based analytics
my name is Jagan atreya I run the IT
analytics cloud service within Oracle
just a quick show of hands something
awesome out of an obvious question how
many here are developers ok how many of
you are agile developers ok excellent so
what I wanted to talk to you today is
what my talk will primarily focus on the
DevOps methodology and the things that
we need to do to make it faster better
so that you can get more features out
faster and I'll talk about how some of
the improvements you can do in
management that will help you make their
apps easier then I'll switch to a live
demo I'll walk you through some of the
capabilities you can use to get some of
the insights to make the whole ops of
DevOps easy and I'll talk about some of
the machine learning that we've built
that are that I would have shown you in
the demo that you'll see as
illustrations as how that can make again
your life in their apps easier so it's
no surprise today that this is a great
time to be developer there's lots of
great technologies out there out there
lots of great platforms out there to
develop your apps and deploy your apps
there's lots of open source tools I mean
that to basically to build to develop to
deploy and automate across direct from
your development all the way up to your
pipeline that grows from dev to test to
production and given the fact that you
know even as you build your applications
driven towards using microservices be it
makes the job of deployment really easy
using these tools but one of the
challenge that comes about is how do you
manage this environment and make it easy
for you so you can focus on building and
developing features and rolling it out
and it doesn't matter which cloud you
deploy it whether you choose containers
whether you choose you know Java or.net
etc so the the challenge for devil
today if you look at this is the fact is
you've moved from your traditional
waterfall into internet development
which is what agile and some of the
modern development processes give you
and the second aspect of this is really
confused integration so you're able to
start building these features and
rolling it out in continuous basis but
then once you start getting to
deployment and in production management
a lot of development time is also spent
in the management aspect because let's
face it even as you develop these
applications what happens is it goes
from your individual developer into a
particular repository and over into
multiple repositories and the overall
master bill that goes out you will find
that there's a lot of afterthought that
comes in because you'll find issues once
they go into production and this is not
this is not this is not new but the
challenges especially if you're
supporting a cloud-based applications
the need to manage and be able to
resolve these issues faster becomes
critical which means that if you're
going to be spine spending time doing
management and figuring out if you have
the right to lling you're not spending
time in building applications which goes
back to the original goal is to be able
to deploy and in roll out new features
so eventually that resource gap that you
see come about comes Agra comes about
because of time spent in production
management which is better off spent in
improving your development and making it
easier and today the fact is that your
production infrastructure is often
different from it developing
infrastructure so which means that what
you're built is not really built to
scale when you and you take a best guess
at it and you're deployed and you find
the gap coming in once you have deployed
what you build something and you deploy
it in your infrastructure so the goal
again is to have rapid software updates
but do it in a way that you can do it
with increased efficiency and spend less
time out of your development process to
handle production issues so let's
together what's the best way to do this
so one of the principles that we say is
from a DevOps process cycle is to
develop deliver and destroy which means
that you go from development into you
know a computer regression cookie
delivery and then the destroy is really
what we're saying here is is not
destroyed but really replace is in place
replacement of your code which means
even when you talk about doing software
updates when you're talking about doing
schema changes it needs to be a whole
software deployment so that you can make
your changes in one build rather than
doing incremental changes to this and
the reason for this is primarily because
when if you think of the Dallas
methodology the goal is to be able to
take changes as they come in especially
if you're moving towards CI is is to
kick off build whenever something new
has been checked in
so that point you have a whole complete
validated bill that can be deployed
anytime which means you can move towards
faster integration and faster deployment
now even within Oracle in my team which
had moved from the traditional waterfall
into an agile this was a learning
process for us because we learnt to
first start by doing deployment and our
deployment is completely isolated from
the development because we handed off to
a DevOps team and as part of this we
learned that to be able to take better
control of this environment every
developer needs to be our development
build environment need to have a full
replacement so only then you can
guarantee that what is gets deployed in
development is what gets deployed in
test is what gets deployed in production
now it's a different scale all together
but the idea is you have consistency
across environments and I'll also talk
about an examples of how we discover
some of these inconsistencies as well if
I have time so this allows you to get to
continuous delivery but what happens
when when you get to the deployment
aspect is and this is where you run into
the realm of DevOps and which is really
in a lot of places you have ops being
separate or a different part than
development now I in the ideal case you
know the whole development deployment is
really one continuous pipeline but the
reality is that's not the case because
you have different infrastructure
different setups that take place and
that is also one of the issue is the
build scripts the automation that you've
done for
production is different from job and and
because of that what happens is the
tooling that's used to configure your
production environment is often
different for what you use for
development so which means that the
operators that run your production have
their own tools which means you don't
have code level visibility into what's
happening in production secondly there
is lots of data that gets generated but
it's often in different log files
whether it may be at the app tier or the
database tier or the infrastructure here
or even custom application files so
which means even the process of creating
and instrumenting log data when you
create something within your dev
environment when it automatically gets
your product-moment it's just another
file descriptor which means you have to
have ops instructions provided to allow
you to capture the data and bring that
in for analysis so what this means is
first of all you have different tools
you have somebody looking at the OS tier
or somebody look at the you know the the
database here somebody looking at sequel
somebody looks at you know the end user
response and you have all these
different tools out there and yes with
the combination of all these tools you
get a a picture but now you actually
have to consult all these different
tools to get an answers to how your code
is performing in production and secondly
a lot of the fact that because of the
fact that a lot of the data is written
into log files you don't have the
visibility into do your logs you have to
have somebody take the extract from log
and copy it or transfer it through a
file transfer or whatever into your dev
environment and then you can go
troubleshoot it to figure out what's
going on because of the often the
arms-length separation between dev and
prod and so therefore what this means is
from your let's go back to our first
principle which is to be able to make
continuous development and continuous
delivery you're not able to do that
because your time is being spent in all
these non development related activities
when you need to actually find a way to
do that in your DevOps in into your into
your CI CD workflow so and then what
does it mean not having these so which
means that because you don't have
visibility into the behavior of your
code you
are not able to figure out until much
later on what the orchestration behavior
should be for your apps for example if
you wanted to scale out a cloud bursting
you often have to go figure out look at
the behavior and they say you know what
here's when we start to get timers from
your users because you know we ran a
large test I'll give an example most
recently you know we ran a sales
training for the entire you know our
North American organization and we ran
this on our cloud infrastructure and so
because of that some of the issues that
we face about timeouts could only be
found once you start to receive at the
order of you know 300 users accessing
the app at the same time accessing the
same workflow at the same time so these
scale out issues that you run into are
often issues that you'll find in prod so
if you had actually if the developer who
already knows about the code who knows
the behavior at one you reached limits
you can actually use your
instrumentation to configure your
orchestration workflows for scale out by
having integration between your
monitoring and your orchestration
framework so another example is
configuration mismatches and this is
again an issue because separate
processes so this is an example that is
going to get to so even on in our own
environment we find that certain we ran
to certain code behavior where things
were fine we had international users
provide creating dashboards some created
the dashboards in you know in English
and some but it was test and it was also
tested in multiple languages
when device deployed to prod we found
that the some of the non ASCII
characters were not getting rendered
properly and we wrote you run into this
because the fact that it turns out the
build scripts that's used to deploy
production are different from the build
script that were used and it turns out
that one of the build script did not set
the MLS settings properly so because of
these differences the lack of
integration from your build your your
development delivery and deployment is
because of the differences that you see
which means you're spending time doing
troubleshooting and not doing deployment
so therefore what we really need from
best practices perspective is a way by
which you can integrate the management
of your development environment
into your development process so that
you have along with continuous
integration continuous delivery you also
having continuous monitoring not so
which means that your developer and
yourself are when you see the
instrumentation coming in whether
they're metrics where their logs you're
seeing this in your DUT environment so
your your your telemetry across the
board has to be part of heads about in
the process so which means if you're
looking if you're running a load test in
your test environment then the same load
if you see in prod you're able to then
see if your instrumentation is able to
measure and use a response
infrastructure or response log errors in
a unified way so to do that you have to
incorporate and and this is not just the
infrastructure level but also at the end
user level so which means you have to be
able to take the binaries that make up
your end-user response and better the
application so basically an application
agent that you can build as part of your
code and deploy into your app containers
so along with the code that actually
runs your application you also have the
instrumentation that manages the
response times the end users that pages
the the number of requests received the
number of what kind of service requests
that were made what server requests that
were made and what was the
infrastructure response as part of that
that should be part of your code so that
every bill actually contains two interim
tation collection so that that gets
deployed into your test environment so
your test teams can actually go validate
that the dissertations are coming in and
then ultimately gets deployed to
production so your DevOps can use the
same tool that you would use to diagnose
in development also in production as
well if you're using containers to make
sure that your for infrastructure
monitoring then your containers also
have to contain the agents so that they
can collect the instrumentation metrics
when you eventually when you scale out
so that you're not doing discovery post
hoc the discovery is happening even as
your applications are scaling out and
you're collecting the telemetry for logs
and metrics for your infrastructure your
databases your host CPU memory etc along
with your app metrics and user metrics
into you know for your comment
analysis now to do that you need to make
sure that this is easy to set up so we
built technologies our agent
technologies our collection technologies
so that whether you're a Java app or a
dotnet app or Ruby or node.js
any of these we have native agents that
allow you to embed if you're talked
about end user data which have very
simple JavaScript instrumentation so you
have browser level data collection
coming in from the end user and it's
being collected so which means
everything from the end user session
data the page response data the Ajax
calls along with logs is being collected
if you happen to have if you happen to
be in the business of writing synthetic
tests to make sure your customers or you
can have monitoring of your environment
then you can also embed a synthetic test
as well secondly the whole dynamic
application definition is critical
especially if you look at towards
cloudbursting or scaling out your
infrastructure you need to be able to
have your infrastructure monitoring also
scale out in response without having to
do separate discovery because you know
what your containers can get fired up
and computer can get quiet anytime so
which means that you don't have time to
spin off and separate do discovery
separate from these so discovery has to
be automatic and part of your management
system as well and your application
definition which may consist of you know
a set of micro services and set of
containers and running app servers
running databases and host they all have
to be also be dynamic in responses so
that when you want to go troubleshoot
your application definition it has to be
in real time so that you can navigate
across infrastructure find out if you're
in fact having a storage error that may
be causing in a sequel slowdown which
may be causing page timeout to be able
to traverse up your entire dynamic
infrastructure is critical and all this
and given the fact that traditionally
you have metrics being collected
separate from logs it is equally
important that the log data which is
really where any exceptions get reported
which is not track has to be integrated
with this so in context you should be
able to go look at a page error and
drill down to the appropriate time
context and the specific entity
where the logs being reported to see
what are the behavior that caused the
page error and which got actually
captured in your log file and the nice
thing about the analytics is that
eventually when you collect the data
over these performance of your
application performance of your end user
the page response time as well as
infrastructure sequel response time and
your the heap utilization across your
entire infrastructure it is important
that you have that being collected and
stored in a long-term way so you can go
back and do period to period comparison
how did I do week to week how did I do
month to month what were the anomalies
that took place or the long time period
because typically most monitoring
systems will hold data for like a week
maybe 10 days or a month tops but you
need to really be able to have this data
held over long term so you can use this
for you know either capacity planning
when you want to plan your next
application or you want to you know
because that there's a cost aspect to
this so only if you know if you're right
size your infrastructure or you know the
impact of you know cloudbursting so you
can figure out this data looking at your
application infrastructure performance
over the long term and then find trends
in it and then improve your app so then
thereby all these future deployments
will get easier and all this has to be
done automatically so with that what I
thought I'll do is run a demo so what
I'm what I have running here is a an
Oracle management cloud which is a
cloud-based management framework for
design for DevOps and your modern
application that runs on the Oracle
cloud so I'm going to start off by
giving you a dashboard view then we
explore different facets of that I will
show you some of the machine learning
that we've built in that allows you to
collect this information okay
okay so this is the dashboard of Oracle
management cloud and what you see here
are a collections of different types of
dashboards that we built you have an
application performance monitoring
dashboard a thing that looks at
enterprise health so the idea is that
the same data is being collected from
metrics logs end-user data
infrastructure performance data logged
in all is coming in one place and these
different dashboards have been built to
allow you to then construct different
types of views designed for different
stakeholders you have end-user
dashboards you have infrastructure
dashboards you have logs we look at
errors and then if you have security
issues all those can be built as well
but this is all running in one unified
data model so what I'm going to do is
and click on the first one which is my
application dashboard and we'll start
with that so this is a an application
dashboard that have been constructed to
look at everything you want to look
monitor about the application so for
example you'll see different aspects of
this on the top left corner you'll see
open alerts and then you see the number
of entities which is you know
application servers hosts databases that
make up this particular infrastructure
and you see the uptime status as well in
addition to this one of the unique
things about organizing clouds is in
addition to sort of you know what I call
infrastructure and application metrics
you can also ingest business metrics so
one of the things that you know
customers and us developers want to is
be able to look at the effect of the
changes in business metrics and response
on the infrastructure for example if you
ran a marketing campaign you want to
look at what happens when your orders go
up and this effect on your you know keep
utilization or CPU a database CPU etc
and you could then plan these out and
then you can also bring in you know in a
revenue metrics like you can order size
so all this data is either being
collected from metrics which is being
stored in tables or it's log data which
where it's being collected so using
standard searches we extract the data
and represent it in these dashboards
and then you can see page response time
and application logs etcetera so here we
see the top pages are you know the order
shop order lists products and this is
the overall page response time over time
so what I'm going to do is I'm going to
drill down into my application and show
you what is happening inside my
application so I'm going to click on
this and click on APM which is the
built-in application performance
monitoring tool on top on running on the
Oracle mobile cloud unified in a
platform so notice that when I do that
it comes in directly so it has chosen my
order app which is what I see here which
is selected so or command recognize that
you are in context of an application so
you can either go across application or
you can be specific to an applicators
here I'm context of an order app and so
this is what I'm seeing here I see the
total number of any alerts any warnings
and those that have been resolved here I
can see the traffic across the different
geographies at an app level so what I'm
going to do is I'm going to drill down
and look at some of the other aspects of
here
I can see server requests oh and one in
one place I can see end-user response I
can see the server requests in one place
so I'm going to click on my pages and
show you some of the top pages here what
I'm going to do is look at some of the
the Ajax calls here so here notice that
I have the response over time okay so
I'm going to click on checkout and
notice that it shows me a couple of
things so first of all it shows me the
number of alerts that are there the
response time and the nice thing about
the the way the screen works it starts
off with the application view and then
when you pick a context it drills down
to the specific view and because we
store actual end user requests and
actual injures response we're able to
look at drill down into the individual
requests and I'll show you that here as
well so for example you're able to see
that we get the overall response time
over
time and this is looking at the time
period of for a week and you can look at
the overall Ajax call response time as
well now let me point out a couple of
things that we've done which some of the
machine learning you notice something
using anomalous period so this is
something that we have added so that you
can initiate analysis of this data to
see one of the anomalies that you've
seen here so I'm going to click on this
that identify so what what's happening
here and I'll talk about some of the way
this algorithm works is there's a
baseline aspect that analyzes the
behavior of the traffic and within that
are your response time whatever metric
we're picking and within that it looks
at what are the periods where anomalies
were detected that are out of match with
that so for example if I so these are
the periods that were identified if I
want to look at the individual behavior
I can go back and look at these I can
look at specific points in time I can
look at the data here
I can look at some of the response break
down by call processing or the call
response so each each of these can be
enabled or disabled as well so what I'm
going to do next and then show you soon
you notice we started off with the
application view now I'm going to drill
down into the this particular Ajax call
and figure out what's going on so now I
can see that I can look at the page
request by geography so here by default
I see the average response time is Lois
in South America but I can so because of
the fact that we collect multiple
dimensions so not just the geography we
also collect the the total number of
calls that were made I can also look at
it by total number of traffic so by the
traffic if I can go back I can look at
total calls made so now you can see that
the greatest number of calls are
actually coming from Asia so which means
that there's different delays by which I
can analyze the same data to figure out
where the requests come in and whether
what where the highest traffic is and
which of the poorest responses okay now
let's look at server requests
so this is this is the page the HS
called which makes calls to this
particular server request and I see the
overall server requests over time and
right here you can see some of the
breakdown at the tier level response so
because this is embedded as part of the
application we collect the application
response as well as this JDBC response
so we can see how much time you spent
inside the app server and how much being
spent inside the database so you can see
what's making a particular call slow if
I click on my call here at this point
now I'm looking at a particular server
request and I'm drilling down here so
what it shows me is immediately within
this time period what are all the calls
that were made so here the ones in this
light greenish yellow that's the apps or
response and those right below this are
all these sequel responses that were
came so what I'm going to do is rather
than drilling down into each one I'm
going to look at the specific metrics
here so again I can use the same
methodology instead of looking at an
aggregate level I can look at the
anomalies either for a specific time
period or a process let me just show you
a couple of other things here so you
remember we talked about collecting the
actual and user response here so right
here we see that we see all the
responses that are coming in here from
each individual request so this data is
being is being collected for each of the
individual users and the aggregate over
time so you can actually see the actual
end user data for all the users coming
in and so which means that if a single
user is suffering each one of them will
show up here so if we drill down here we
can see that this particular user had
errors so we can explore this and figure
out what is it that's causing this so at
this point you're now looking at the
call tree and to figure out you know
what is it that's causing this error so
I can drill down here and I can see that
it's coming in all my submit posts these
are all the errors if I go back all
these is get order ID here and
is service admit so all these errors are
being reported now at this time no in
traditional applications if you collect
you know APM data you have to then go
back and extract the logs because a lot
of the the reason for this errors is
often an app error that is reported in
lakhs now because we have built in the
integration of the application with the
topology we're able to collect the
application logs in context of the
particular application request call so
now if I just go back here and I click
on view related logs that brings me into
my log analysis for this particular
transaction so now I'm able to see this
and see you know how much time is being
spent with where all the calls that are
being made so earlier we saw the anomaly
detection and now let me show you
another machine learning that we've
built in here now
this is what you're seeing which is the
raw data now this raw data is
interesting but it's not you you can't
process this obviously so you need to
have more analysis or more machine
learning done on this so you can drill
down and figure out the causes so we've
built in a machine learning technique
called clustering which allows you to
drill down and find these problems so
for example here what we do is we liquid
all the large requests that are coming
in we look at what elements get our
variables so for example a hostname
maybe variable a web server name maybe
variable but if the air request is the
same then all those matching the same
pattern get aggregated and clustered so
now you're able to see the single error
that gets clustered by that which is
what you're seeing here now if you want
to explore each one of these then you
can then look at these and look at the
specific error and see that in fact it
was at this is actually a machine fault
which says that a fault was triggered on
purpose so this case obviously the demo
app that you're seeing this but what it
does it allows you to drill down to the
specific cause from an end user down to
a page request down to a server request
down to a log entry to find out the root
cause or what
cause the end users down to the
individual end-user level so let's go
and look at another use case that we
have which is looking at the application
and dashboard okay
I'm going to go back to my same
dashboard and I'm going to show you
another and how you can navigate across
so we should on an example how you can
drill down into specific in context I'm
going to here another technique of how
you can actually use the topology of the
application to navigate across so here
we have stored the application logs we
just could being collected a cluster
entire infrastructure so I'm going to
use this and explore my log Explorer so
now this brings me in context of the all
the logs being collected across the
application so so right now I'm seeing
this is again the pie chart view let me
break this down to the standard you know
log analysis view now
so if you think about your application
it's really complex it has an app server
and user data host all the data is being
collected but now you can use this to
see where the well using the volume of
errors being reported to troubleshoot
the application itself now normally it
would be difficult to navigate it the
application so if you look at the number
of elements that make this up you'll
notice that it consists of all these
different app servers and whose logs are
being collected all this is what's being
reported in this particular file you
notice that some are present but
highlighted some are present and not
highlight and we brought the ability to
bring in the entire topology what you're
seeing here but the highlights indicate
which are the one entries that are
actually generating these errors so
which means I can use it to explore
different files and to see what's
causing this now I can use this again my
time selector to go back in time I look
in one week and look in 30 days so I can
use that to also go back in time and
explore different aspects of the
particular entries so naturally I'll see
different errors based on what I pick so
normally the aspect of picking can be
done through the normal textual way
which is I go back
I pick my entity if I click on this I
see all the entities makeup that's a so
we we look at all the entries and we
build a facet list which is the pic of a
list of entries automatically or if you
want to do it in a visual way we
actually have a much better way to do
this which is we have the topology
Explorer now this brings in the overall
topology of your application so now
you're able to see the entire
application instance from the engines
request all the way down to the app
server database and host all in one view
so at any given time if I want to look
at one particular environment say I want
look at say sequel server here if I
select this then if you notice that
facet is automatically added to my
filter and I'm seeing all sequel server
errors so which means the act of
navigating from the entire application
instance consisting of a dynamic set of
containers applications is now just a
matter of visually navigating the
topology to find the specific error and
if I want to explore other aspects of
this to go from here into the storage
here I can pick a particular ASM
instance and which is an Oracle storage
instance and then drill down and figure
out the ASM errors as well and use the
same techniques like clustering to find
out you know what are the most common
storage errors that may be affecting my
application so in context it creates and
in figures out all these different
errors and I can I can do that here so
so let's go back and look at so I so
then so this is really a great way for
you to start from a broad cross
application view and narrow down to
specific error and find this again
clustering is a technique that we use to
do this ok so let me just close with one
other use case and then I'm going to
show you here ok so we talked about
different dashboards so we've looked at
application errors we've looked at you
know the log entries and now I want to
show you that some of the metrics that
we collect also at at the cross
enterprises level so for example
here what you're seeing is my so I can
either look at my hose that for a
specific application or if you're
looking at multiple applications you can
look at all them in my host resource
analytics remember we said that you know
we collect this data and restore it over
the long term so this data can be stored
over six months equal to 30 days in the
six months last year so by default we
retain this data for 13 months so you
can do year-on-year capacity planning or
performance or analysis over from one
Thanksgiving you to another Thanksgiving
in this interface so here you can see
the host utilization across my entire
instance for CPU and memory the top
right corner represents what is hot in
terms of CPU and memory because those
the one that's going to run out of
resources very quickly and we have
automatically identified those by giving
you a quick pointer to this so you can
quickly pick this one and then navigate
to it right here you see the breakdown
by target version so it could be like
host version a more interesting
breakdown is by the number of cores or
the number of memory size so now you can
look at the some of the small boxes
large boxes and figure out which of the
boxes are going to run out of you know
of resources quickly and you can do
capacity planning either an aggregate
level or pickup for example here I can
pick an individual database which looks
you know hot which is a ride app - and
right below I'll see a trend and
forecast line generator automatically so
this is why even though memory is kind
of flat it's going to run our CPU
quickly or if I want to do it at a whole
group level I can just pick a a cluster
off my 12 core machines
and then drill down here and it'll run
the forecast for all the machines that
make up my 12 core machine so which
means that the act of capacity planning
is not an annual exercise it's what we
like to say in or command cloud every
day is capacity planning this so you can
figure out in any given time what your
utilization is you can plan for it and
you can and you can tweak this data and
you can go back and say I'll pick my
settings figure out which of these
you know machine learning algorithms
that I want to use for my forecast and
use that to project my my utilization
linear exponential or best fit and pick
my forecast period again when I was ten
days thirty days and use this to run my
forecast so that's how you're able to
bring in both capacity data performance
data log error data all in one place
through this interface so so let me at
this point switch to my presentation and
I'm going to do is closes a couple of
DevOps examples of machine learning so
what we saw was the different examples
of machine learning we saw anomaly
detection clustering correlation that's
not a prediction that came in from the
forecasting and all these are built in
with in Orca management cloud and I'll
talk about some of the secret sauce that
we've built in and and all these are
embedded in the different services like
application performance monitoring and
log analytics and as well as application
and and infrastructure analytics as well
so let's talk about some of these
techniques so baseline an anomaly
detection now the normal baseline
algorithm uses something called the
daily and weekly additive Holt winter
modeling this is you'll see in a lot of
the vendors out there and then so it's a
standard way to run our forecasting but
one of the things we've done is find a
way to actually learn from the data so
it starts off by looking at a baseline
of history and uses that to identify
what are the highs and lows so it goes
back and collects about two weeks worth
of data to identify what are the common
patterns for highs and lows for a
particular pattern so this is what it
uses to develop sort of a the baseline
behavior so you will see that when the
chart shows up here you'll find these
highs and lows showing up identified
here okay when you start to once the
daily seasonality has been detected the
pattern will be fairly wise because the
you know the the algorithm has still not
developed the full confidence of how
much to predict project out as and when
more and more data comes in it applies
the learning from that behavior to
actually make the
curve fit tighter and tighter because if
it's not discovered daily seasonality
which means as the pattern when are the
spikes when it doesn't go down including
weekend activity where if your
application runs slower on weekends
there's not much traffic it finds the
learning of that so then it builds up
different and so which means that you'll
find the band become tighter and tighter
over time
now having developed this particular
identification then if any variations
take place those become more significant
which means that after your initial
training period and then the learning
period the now these anomalies that get
detected which would normally in this
time period would get classified under a
wider band now having gone through the
learning it becomes a much tighter bet
so now it's able to then detect the
seasonality and then use that to
identify anomalies and use that to then
report them now the actual
identification of the anomaly may be a
graphic relative issue it may be
underlying infrastructure issue but what
it does it looks at the pattern and over
time this this algorithm becomes better
baselining based early-warning this is
also a new technique we have added in
our in our machine learning because what
we do is we we take the data and we
develop the baseline from this now the
difference between early warning and
forecasting is really about time period
the goal of forecast of early warning is
to be able to tell you when problem is
going to occur in the order of hours so
which means it looks at past behavior
based an hourly patterns and be able to
tell you when a particular trend on the
short-term is approaching anomalous
behavior so this is really used to
figure out if you might run into a
potential problem now given that if you
set learning on this there's a good
chance that you're probably flooded with
alerts we've sort of provided the tools
so you value can make it as conservative
or as restrictive as possible as
concerned or as liberal as possible so
you can then keep it loose in the
initial term while you figure out what
would be representing you know and not
early warning issues so that you're not
flood alerts
that's been built in so that you can
tweak it and figure out which metrics
are symptomatic of your application
problems and then tweak them for the
prizes whether you want to do it and
connect a couple of ours or next couple
of you know next half-day etc sorry and
so that brings us to the forecasting now
forecasting is an interesting use case
because the application itself has
different trends of behaviors you have
you know you have peak periods you have
medium usage and then you have low usage
which means that if you think about your
application work code there's different
levels of periodicity built in so the
traditional algorithms really just take
a particular flow and they take it and
they run the forecast so what you see on
top is just the plain forecast generated
based on behavior so what the
forecasting algorithm does with in with
an orca management cloud is it figures
out different types of periods of
repetition which means that you have
certain patterns that are weekly that
occur on weekdays between 9 to 5 you
have periods of low activity that take
place between you know weekdays at
nights or evenings and then you have
periods of like in absolutely idle time
with like exceptions coming in say on
weekends when you might do backups and
things like that so which is why you
have these different types of patterns
coming in so we actually model multiple
of these periods of repetition and use
that so you'll actually see it's a
complex function that incorporates that
a second unique thing we've added to
this is the ability to embed what we
call regime change a regime change
contractor what you might think is
really a case when the same
infrastructure has had a significant
change coming in for example if you have
an application or running on a
particular database and we've been
running for about six months and now you
decide that you added say a payment
processing system on top via now that's
a nonlinear change that has been added
to that particular environment so we
actually capture that
and we make that a something that we be
identified so we know that going forward
there has been a significant change that
has taken place and we use that to now
model the new start time so now future
behavior will be based on the fact that
the new regime in occurring on that
particular infrastructure and that's
used to then model the behavior rather
than any past behavior so what you're
seeing here for example is that you know
the normal forecast might trend in
upwards based on this particular pattern
and going upward but here there might
have been a machine that have been you
know maybe it is use for testing and
there's going to change so this is the
new learning that has been developed
based on the forecast so that's an
example of how forecasting actually is
depends on the specific workloads that
are running in this environment so so
you saw a demo of Orca management clouds
and some of the machine learning with
built in so what is it final takeaways
in terms of dalip so the first thing we
said was to increase their productivity
so you can deploy faster so first one is
automate automate automate to close the
resource gap which means automate not
just the development not just the
deployment but also the management which
means that code has to be instrumented
at development time so the developer is
able to see the instrumentation and
recognize it and improve it so that they
can resolve this in production faster
build production ratings it's early on
and continuously which means that don't
wait for your deployment to take place
in prod to try and resolve issues and
that is just setting yourself up for
having cycles spent in in
troubleshooting and collecting log data
and instead build it right in
development so that the analysis can be
done in past analysis can be done prod
and the same knowledge base that's being
percolated across the enterprise create
a persistent record and a historical
record of your application behavior
whether it's metrics where this Logs
with configuration changes all these has
to be collected in a unified way so that
you're able to then see and analyze log
metrics and users all in a unified
dashboard so you have then resolved
these issues to see when did my user
timeout occur when did my user traffic
all over the log patterns or the
infrastructure behavior all in a unified
way and use the students start your
resolution process so rather than go
back and say hey let's start by bringing
your log or can you get a dump of
metrics so you can actually see the
dashboard behavior and by having a
historical record of this you can then
have this you know persistent record so
you can go back and prevent yourself
from having it to the same issues again
and again in production and then instead
of having a methodology by which you can
find this issues fix it once and close
those class officios going forward in
your development cycle all the director
production and then finally you know
don't treat DevOps as separate don't so
they don't get into the over the throw
the wall problem make your ops part of
your development so that you're able to
be successful with this that thank you
go ahead
absolutely alerts full alerting
framework both based what traditional
thresholding based as well as anomaly
based any other questions okay thank you
very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>