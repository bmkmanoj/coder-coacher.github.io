<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Transactions Returning to NoSQL | Coder Coacher - Coaching Coders</title><meta content="Transactions Returning to NoSQL - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Transactions Returning to NoSQL</b></h2><h5 class="post__date">2015-06-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/SfdQrp1Mu6s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">um thank you all for coming anybody who
wants to ask any questions during the
talk feel free to shout out if you're
stuck behind a pillar arm you can
unstick yourself because there's quite a
lot nice seats long front here it's not
like school the the front row is ok so
I'm going to talk about transactions in
the context of no sequel or big data or
look around this room and god I hope
more than a handful of people have heard
of dr. strangelove otherwise that really
doesn't make any sense or basically you
know how transactions were the the bane
of all evil for no sequel big data
developers a few years back and now we
seem to be coming back to a realization
that maybe they're not all bad a quick
overview of what we're going to do in
this talk I'm going to talk about you
know where we are today what's brought
us in the industry to you know no sequel
implementations also maybe hopefully
touch on some some research that could
be potentially useful and interesting to
other people in in the audience then I'm
really going to talk about no sequel and
the enterprise which is one of the main
drivers for many of the implementations
and and vendors behind those
implementations starting to look again
at the lack of transactions and as we'll
see i'm not necessarily just talking
about transactions within the context of
a single node i'm also talking about
spanning multiple multiple nodes and
potentially multiple instances of a
particular no sequel instance even on
the same node and then hopefully
there'll be time to touch on the future
and this is where I'll
get into a little bit of code some of
the things that we're doing with in
jboss red hat with one or two of our
projects and also mentioned a couple of
long-term research projects that are you
funded that are interesting in this in
this space as well before I go on hands
up anybody in the room who doesn't know
what a transaction is ok I hope nobody's
lying there will be will be questions
later on in which case you you kind of
are the masses I suppose so you know
transactions has seen mass adoption over
the the last 30 or 40 years you know it
kind of start in the in the 60s but it
really saw mass adoption when
standardized middleware platforms took
off whether that's through D comnet j2ee
Cobra etc transactions are core to many
of the specifications and standards that
we use today in that maybe one or two of
us have used in the past such as you
know JDBC JMS jca OTS is the corba
transaction system jts in there in Java
EE is based on that so they really have
become much more mainstream in the last
20 years it literally was the case say
back in the in the 80s and the 90s that
transactions were the domain of so you
know obviously banks databases and niche
niche areas despite the fact that those
niche areas probably were running a
significant amount of an economy's money
but most developers didn't really have
to have to understand them or even deal
with them that's that's not the case
today I'd say that many many developers
whether they're in the Java world or the
Microsoft world understand transactions
and may not necessarily understand what
goes into making them work but know
conceptually what they are
and are quite comfortable with with
using them and obviously one of the
other driving factors for mass adoption
is being open source you know we got
eventually we ended up having
transactions in my sequel and post gross
we got proper transactions in jboss AS
slash wildfly and then a number of other
standalone transaction engines like like
bit ronix up until the end of the 20th
century acid transactions were pretty
much good enough there was research into
non acid transactions relaxing the acid
semantics so relaxing atomicity maybe
relaxing isolation relaxing durability
but most of that was done in the context
of academic research it was you know
what if if we were scaling in a large
way what if what are the bats one of the
things that acid transactions aren't
really good at and most of those things
tended to be things that were the domain
of long-term research academic
environments not the domain of
real-world environments but turn of the
20th century and the beginning of the
21st century that kind of changed with
obviously the advent of the world of the
World Wide Web becoming much more usable
for eeper e-commerce and then web
services and and rest those environments
are inherently loosely coupled obviously
inherently large scale in terms of the
number of participants they're
distributed around the world not just
talking about two instances of a
database that may be residing in the
same building we could be talking about
multiple instances of database that are
on completely different sides of the of
the globe and a lot of the work that was
going on in to the so-called extended
transactions then start to see its way
into industry standards such as the
extended object transaction service
which was done in korba oasis with web
service of transactions in btp in WS
calf and WS star
and hopefully this will all become
relevant in a minute meanwhile back with
the relational databases these have
evolved really nicely over the over the
decades they have managed until you know
the let's say that the 21st century they
manage to cope with increasing amounts
of data cope with increasing distributed
pneus of that data Jim Gray everybody
heard of Jim Gray nobody have Jim Gray
anybody who's not her to Jim Gray and
said they did no transactions I don't
quite understand how you get that anyway
a man called Jim Gray who is essentially
God for the transactions world he made a
quote back in nineteen ninety nine at a
conference call the high-performance
transaction systems workshop that
despite the fact that two phase commit
which hopefully everybody who does no
transactions knows what that is is
something that all transaction engines
have to have and have to ensure that
they can operate within ninety nine
percent of transactions that were
operating at that point were one phase
so essentially the database was you had
a single database you didn't have
multiple participants that's changed
quite a bit in the last decade and a
half now going back to what I was saying
earlier about mass adoption people are
having transactions that have many more
than one participant in them I don't
know what the latest figures are but
it's certainly significantly more than
one percent of transactions that are
used in industry or one phase well
they're actually more than two phase
sorry but with the start of the 21st
century and obviously to where we are
today amount of data that people want to
store in relational databases has grown
exponentially and despite the fact that
we all have used relational databases
we all take them for granted and we take
transactions within context of a
database for granted I think most people
will understand that a relational
database is an actually fantastic
generalist it's you know the old saying
of jack of all trades master of none it
is it is really good at many many things
but it's excellent at virtually nothing
and one of the things that is X it's not
excellent at is storing large and large
amounts of data so petabytes of data and
this became evident when as i said the
data explosion really really took off
and people start to throw tens of
terabytes hundreds of terabytes at
relational database it doesn't matter
what as Oracle or my sequel or db2 they
struggled to perform and they struggled
to maintain consistency because that's
what they wanted to do in relational
databases two-phase commit strongly
consistent put five or six replicas in
an Oracle RAC they're going to strive
very very hard to ensure that every
instance has got the same data as every
other instance as the amount of data
grew to try and augment databases and
make them perform people start to put
caches in front of them makes perfect
sense you know if you've looked at
hibernate for instance hibernate does
that out of the box let's put a memory
cache in front of the database so we hit
the database very very infrequently
because the disk that backends the
database is typically going to be your
performance bottleneck but there comes a
time when even that's not sufficient and
about about a decade ago let's say
probably less we saw the growth of or
the advent I should say of the no sequel
movement now originally no sequel stood
for no sequel now it stands for not only
sequel because people have seen the
benefit of sequel even in these
large-scale data solutions there are a
range of no sequel implementations that
are out there they essentially fall into
four categories there's a document tuple
column and and graph based if anybody
wants to really
see the evolution of big data or no
sequel over the years recommend that you
go to the website for the high
performance transaction systems workshop
that I mentioned earlier and go back to
2009-2010 I think there may be some
stuff before that it's every two years
so 2007 would be the what probably the
first one I think but there's some
really good good papers and it's nice to
compare and contrast what what we were
as an industry was thinking back there
with where we are today but there's a
lot of implementations and then there
are these four categories and the thing
is oh I'll come on to this in a second
so this is just giving an example of the
data explosion so this is but IDC report
from 2012 it's obviously it's old now
but you know 10 billion machines versus
six point nine billion humans in while
they were predicting in 2012 for I
thinks 2013 7 billion sensors in the
world by n2 2010-2013 most of the data
that sensors are producing has obviously
got geospatial information in it so even
the smallest sent sensor is producing a
significant amount of data continuously
and most most applications want to store
that data and that's as I said earlier
that's kind of what drew what drove the
no sequel and big data space another big
driver of it if you were in the previous
talk obviously was Google wanting to
essentially map the web install the web
so that they could then process it and
give you really good hits on your webs
web searches everybody heard of the cap
theorem and base one person sort of cap
quite a few people had a cab okay so so
in 2001 Eric Brewer published a paper
about his cap theorem which basically
said that distributed system can
tolerate only two of the following three
things you know c.a.p consistency
availability and partition tolerance
that has driven a lot of no sequel
implementations to attempt to not do
distributed transactions because
distributed transactions
again you guys fantastic you all know
about transactions so you'll understand
it understand this but distribute
transactions at their heart essentially
a blocking protocol once you get to
two-phase commit if you get past the
first place and you get a crash of a
participant or worst case you even get a
crash of the coordinator you need to
wait until they recover and that will
impact availability you can't make
forward progress until participants have
recovered or the coordinator has
recovered so you might want to trade off
consistency so that you've always got
availability so even if there's a crash
you still move forward but you maybe
give somebody the warning that a this
guy over here is still dead so when he
comes back the the information I gave
you about your bank account might
actually be wrong he may be holding some
information that either increases or
decreases it but I can't tell you yet
and then there's something called base
which is basically available soft state
eventually which is hopefully the clues
in the name but its eventual consistency
no strong consistency we will get there
eventually and you know in a way
eventual consistency also feeds into one
of the reasons that we're out that we're
also seeing acid transactions come back
there's a there's a quote i can remember
who it is that said this thing was some
possibly somebody in google that
eventually consistent isn't consistent
quickly enough so it's really hard if
anybody has written applications with
relational databases and expect strong
consistency in their applications it's
really hard to move to eventual
consistency if those systems hide the
eventual consistency and if they don't
if they try and expose it to you then
how do you deal with the eventual
consistency in the example I gave before
about the bank account if I tell you
your bank account has got five hundred
dollars in it but I also tell you
somebody else over here who may recover
in the next hour or may not recover and
cook for another year could change that
what do you do as an application
developer how do you cope with that and
then as a user how do I cope with that
if I say if I get information that says
five hundred dollars but it's not
necessarily five hundred dollars and i
know i have an overdraft limit am i
going to risk going 501 dollars out or
five hundred dollars when it might not
be five hundred dollars
$300 so based on cap have formed part
parts of the driving forces again into
into no sequel implementations for for
basically for issuing transactions or
certainly issuing distributed
transactions and acid strong consistency
so as I said a couple of slides ago
there are a lot of no sequel
implementations it's probably more than
this now because I think I prepared this
slide about what a few months back but
there are at least twenty four different
no sequel implementations that are out
there several of them have evolved from
caches through to data grids and now to
no sequel it's you know kind of
following the trend or following the
buzzword or whatever whatever you want
to say but because there are so many
different ones and because they
essentially fall into one of the four
categories i mentioned earlier and each
of these categories is itself good for
certain types of data and not for other
types of data and therefore not for
certain types good for certain types of
use cases and not good for certain other
types of use cases it can become very
very difficult to choose the right tool
for the right job as I said you know
earlier at least with the relational
database you know it's going to be
pretty good for anything you want to
throw at it it's going to have struggles
to scale but you know if you want to
have it as back end to a you know to a
bank it'll work if you don't have his
back end to a photo storage system where
you can let people query you know what
photo was taken on the twenty first of
august 2013 and give you all of your
photo album then it'll work ok there
yeah as the images increase maybe it
might struggle to give you the the
photographs in you know in a timely
manner but it'll still work with tuple
column graph and i can never remember
the other one document yeah thank you
you really need to understand what your
problem is and then what what these are
what each one of these categories is
good at first and then once you manage
to select a category you then also need
to
what implementations are in there
casually sucsess some implementations
even within the graph database category
for instance may not be ideal for your
use case so it becomes that much more
complicated so I say it wouldn't it be
nice if essentially you had this
intelligent system where you could give
it all the data you knew about your use
case and you essentially turned a handle
and out the other end came an answer use
use neo4j or or use use ry'ac or use
whatever that doesn't exist but this is
the hopefully maybe some potentially
interesting information that you might
get out of this one of the things that
we try and do red hat is sponsor some
long-term research in various academic
institutions and one of the PhDs that
we've been sponsoring for the past
couple of years at my home University
Newcastle is in the area of oh no sequel
and the student in doing that work
essentially for her first year and a
half she she worked out what questions
she might like to ask of no sequel
category and then a no sequel
implementation and then went around and
found as many of these implementations
as she possibly could and in some cases
managed to figure out the answers
herself in other cases had to go and
work with the developers to get answers
and the idea is not necessary to provide
some artificial intelligence system that
will do this maybe that's work for
another PhD student but to at least be
able to present to people a knowledge
base so that you could look at it and
you could go that this does not match my
my use case at all so I can you know I
can ignore CouchDB this one does so I'll
try this and there is there's a lot of
information here quite a lot of
information and if if anybody is
interested in getting access to this
I mean it's all going to be publicly
available to PhD she essentially the end
of the day needs to make it publicly
available but it's not at the moment so
if anybody does want access to it see me
afterwards if you give me your email
i'll see about trying to send it out to
you in whatever format you currently has
it in I think it's probably just a an
Excel spreadsheet at the moment so at
the moment we are seeing some no sequel
backlash as well and here's just a few
links the the second one from the bottom
I don't know if anybody can read that
but essentially it says MongoDB stole my
lunch money and rude my startup don't
believe don't believe the link actually
if you read it it's it's a pretty good
article in any really isn't about
MongoDB having stole his lunch money and
ruined his startup and the one at the
bottom the very bottom is a link on info
q I don't know if anybody's heard about
you know one of the Bitcoin groups lost
some bitcoins seems to be happening
quite frequently with bitcoins but one
of the things that they potentially
blamed was the fact that they were using
MongoDB and nobody told them that
MongoDB didn't have transactions across
instances and you know so anyway yeah
take take this all for what it's worth
some of this is objective some of its
not objective the reason I put this up
there is essentially we I think with
with no sequel we're now into this well
we've certainly moved past the I'm
playing with it just to see what it is
it's now also to a degree past the I'm
actually using this in seriousness and
we're going to we're into this trough of
disillusionment if anybody's you know
looked at Gardner you know they they
have this trough of disillusionment I
think we're in this people are really
using these things which is good but
once you start to use something in anger
you tend to find issues with it and then
maybe you do really use it in anger
and this can represent some some of that
so some common reported problems you
know if you if you go back through those
links or maybe do a google search things
like slow disk performance lack of
transactions inconsistency you know
failures of a particular instance might
actually take down the entire system I'm
not going to attribute any of these to
any particular implementations by the
way because red house a few partners in
this space hard to configure an
optimized again go back to relational
databases we have got decades of being
able to optimize the hell out of these
things and if you then move from that to
to a no sequel implementation you can
probably optimize it but the experience
isn't there necessarily or at least it's
not it's not as readily available so you
know it's not that hard to understand
why some people might complain about not
being able to configure an optimized
things poor memory management as in in
some cases no memory management it grows
and grows and grows and then you
essentially have to reboot the system to
get it to continue poor process
management so you can get zombies new
orphans and and problems with
reliability I mean to be honest some of
these are all related to reliability and
the question is you know why why are we
seeing these now as I said I think it's
its maturity well it's where we are in
this in the gardener cycle if you like
and it's also the fact that some
implementations are not as mature as
other implementations it's not really
fair to compare many if any no secret
implementation in terms of code maturity
with with with an Oracle or db2 or
something that's been around for decades
you know the I'm not necessarily
suggesting that you know there are no
sequin tations that are good
implementations but it said anything you
know you you get something that's maybe
at most four or five years old you can
part with something that's had you know
20 30 40 years of effort put into it
it's not a fair comparison from from a
maturity perspective some other reasons
for these problems are a lack of
understanding of the problem space
unrealistic expectations people really
thinking well what I want is a
relational database that scales that's
fast so I can just take a no secret
implementation because I'm sure i read
somewhere that it's a no sequel database
and they kind of focus their here blah
blah blah database blah blah blah so
I'll take that or run with it and then
the other thing which is essentially you
know the rest of this talk realization
that in some cases acid strong
contrasted transactions /strong
consistency is a requirement for their
applications and there is no way that
they can get around that fact and they
want that to be followed through in in
there no sequel implementation so this
is this is really why the last bullet
can it comes into play and it's was
seeing suddenly from from a red hat
perspective and I you know I'd really be
interested if anybody afterwards wants
to come up and and either dispute this
or kind of echo it if anybody else is
seeing this but we because we selling to
enterprise we obviously see a lot of
what's going on in the enterprise so you
know it's it's a focus view I'm not
suggesting that we're seeing everything
that's going on in the and the
distributed middleware environment but
we are now seeing a lot of people who
are using various no sequel
implementations wanting to use these in
enterprise deployments new enterprise
implementations existing enterprise
implementations and we're also seeing
people who have started their their
implementation of a service or an
application that's been using no sequel
and it was fine in that environment
without having transactions without
being able to enlist it within the scope
of a global transaction then wanting to
do precisely that wanting to kind of add
it into a bigger unit of work that's
either because you know what they that
the scope of their applications has
increased or in at least a couple of
occasions you know their company has
been acquired and
they need to be integrated with the rest
of their acquiring companies
infrastructure and that immediately
means in these cases they have
transactions we need to be listed in
transactions and we can't reimplement we
need we shouldn't have to implement
we're quite quite happy with a no sequel
application here we just need it to be
transactional just the the second bullet
here just give you an idea of where
we've been coming out from this so we we
have some no sequel implementations that
are out there and the enterprise
requirements kind of drove their
implementation so one of them which is a
tuple based implementation is called
infini spam anybody heard of in finnish
man think coherence but cheaper and
better cameron's now here is it good so
in finnish band started life as as the
next generation of jboss cash and jboss
cash is used quite a lot in well for
caching but also use it in in hibernate
for caching and then as jboss cash grew
or than all the the need for caching
grew datagrid came on the scene and we
kind of targeted to data grids which
have transactional semantics large-scale
transactional semantics and then no
sequel came on the scene and it in finis
man is still predominantly in the data
grid space but we're starting to see
more no sequel requirements impinge on
it and transactions have been at the
heart of Vincent of in Finnish band from
the start because of the enterprise
needs and then we have a graph of a
document implementation called mode
shape and again that's a trend acid
transactions at its heart for exactly
the same reason but the kind of scenario
is that that we are seeing more and more
and it really is kind of over the last
18 months or so is application
developers who want to combine multiple
related relational databases
by n it's n is essentially any number
typically greater than two may be
multiple no sequel implementations 11 is
probably the you know like the
ninety-five percent case more than that
it's the only other two so far multiple
messaging implementations and
potentially also file transactional file
system updates within the scope of the
same acid transaction or and I'll
explain this a little bit later within
the scope of the same quotes transaction
so I said mention little focus later
later on but when you actually get down
into into the heart of some of these
units of work that people are talking
about they're not they would prefer at
least some of some of the examples that
we that we have they would prefer them
to be acid transactions and all that
that entails but if you explain to them
about extended transactions which aren't
necessarily acid but our automated so
you can have compensators and you know
do and undo that's often good enough
what they don't want to do is have to
manhandle consistency and resolving
Fairley's themselves they want this
to be automated and done in a very very
opaque manner if at all possible yes
yeah yeah no no so it is confusing good
question and I'll update the slides next
time I give us so x n essentially means
more than one typically more than two
different databases which could be
different implementations or could be
different instances of the same so it
could be five Oracle's could be six DB
tues it could be three Oracle's and 2d
be teased or you know sticker my sequel
in there the x two for the for the no
sequel want that kind of
because as I said it says they're
infrequently we don't have we don't have
an awful lot of of sampling for these
kind of scenarios at this stage it's
much much smaller than the number of
people we want to put lots of Delta lots
of databases in the same global
transaction but in this case it is or in
these cases that that we know of it is
21 obviously one it's it could be neo4j
or could be rya'c or whatever but in the
case of two it was two different
implementations of no sequels so from
different vendors if you've only got one
no sequel implementation and everything
else on this list is two-phase aware
then you can still have transactional
semantics acid transactions if your
database supports the last resource
commit optimization hopefully well bass
basically essentially means that it ll
our CEO is something that's been around
for a long long time but it means that
if you have a single participant in a
transaction doesn't matter if you've got
a thousand participants or five
participants if if just one of them is
one phase away as long as it's the last
participant to take part in the
transaction you can still have acid
semantics because that last participant
essentially becomes in a way like a
pseudo coordinator and drive the
transaction to complete or not so you
can still if you've got LRC Oh
supporting you've got a single no sequel
implementation that doesn't support
transactions you can you can get acid
transactions however there are still
relational databases out there there are
still messaging solutions out there that
aren't two-phase aware some and some
that say they're two-phase aware but
really aren't not particularly great
implementations so LRC Oh quite quickly
becomes an option that doesn't really
help certainly in the scenarios that
that we're seeing and maybe in some of
the scenarios you guys are hopefully
seeing so this is just to kind of
diagrammatically give you an idea of
what we're seeing and this this
represents a real world deployment that
we had that we saw you
so ago so you've obviously got your your
application server with your relational
databases here Oracle on my sequel
combination you've got lots of different
clients some of them Java some of them
not and the business logic which is
typically interacting with the databases
is also using so infini span MongoDB
Hadoop and and mochi this is kind of the
extreme this this definitely represents
the extreme but it's a you know it's
it's a deployment that that we've seen
so we're seeing a growing realization
that global transactions are needed at
least in the in the enterprise or Java
EE use case and again caveat we really
like to hear from from from others i
mean i have talked to other people
outside of red hat and they see it but
you know this is a quite a nice sample
of people who know transactions also
acid transactions are often easier to
understand as i mentioned before you
know week consistency great in theory
can actually be hard to deal with us
developer let alone a user but maybe we
don't need an all-or-nothing you know
maybe it's not a case of every every no
secret implementation has to go to order
to acid transactions maybe control
relaxation of acid and be controlled
enforcement of consistency as i
mentioned that's that's something that
we did quite a bit in the 2000s for for
web services with extended transactions
maybe something like that will help in
some of these cases and as I come on to
that in a few slides time now change you
know change takes time you know it has
taken time for people to really
understand and want to embrace no sequel
and now to at least in some cases see
issues with it to push for change when
possibly some of those vendors only see
transactions as a niche case but you
know when hopefully when more and more
people try and push for transactions in
certain cases then maybe vendors will
see and
maybe upstream open source communities
will see that that it makes some sense
to add them and I mean sup we are seeing
some implementations some
implementations have had transactions
for quite a while some implementations
that don't have transactions are now
starting to talk about adding them
obviously you know for instance like
MongoDB has transactions in the local
case they say that you know if you want
to do distributed transactions then you
really need to roll your own and that
they have all the hooks in there for you
to do that but it's not particularly
easy to roll your own transactions yeah
telling developers to implement their
own from scratch even if you say you've
got the hooks for recovery and for
logging at Elba shouldn't have to do
that they've got other things on their
mind you know they actually want to
implement the business logic that's
essentially what they're being paid for
their not being paid to implement a
transaction engine on top of the
database that they're using so you know
there's also we heard about no sequel
there's also a movement our two
movements now new sequel and an old
sequel obviously old sequel was
relational databases new sequel is a
term that was coined a couple of years
ago by yeah I try to remember what
groovy works or 451 group yeah 451 group
to basically represent a group of
implementations that believe that sequel
sequel itself is not problem the problem
is the way in which some of our some of
the relational databases were
architected 30 years or so ago and the
bottlenecks are within the
implementation not necessary and the
architecture but not necessary within
sequel itself so let's reimplement
relational databases knowing what we do
today about you know multi-core large
memory etc and that's what what new
sequel is a number of these can new
sequel implementations have had XA from
from the from the get-go and then there
are some of these new implementations
like Mark
foundation DB they're literally been on
the scene for what mark logics been
honestly for quite a while but
foundation DB i think is three years or
so you know that one of their selling
points if you go to their website is
transactions can in your face it's this
is our differentiator you know you like
transactions we've got transactions oh
and you can also get the scalability and
all the nice things that these know
these other no sequel vendors have been
telling you about for the last four or
five years but which they can't do with
transactions so transactions are restart
to become an important differentiator
and there are also a number of companies
who will remain nameless but if you come
to the jboss party i might at least tell
you about one of them who i know who are
hiring a significant number of people
internally to riorca tect some
employment some relational databases
that are out there one of which is
postgres and make that scale so one of
the one of their aims is to ensure that
postgres can easily cope with the
petabyte of information they believe
that that's a better starting point and
at least this one company I'm thinking
about they started with that they didn't
stop that they are at that point now
because for the last four or five years
they they have had people in house
writing applications using a number of
no sequel solutions and just run into
problems with reliability scalability
etc and they essentially said right
screw this let's go back to basics we
know how relational database works let's
identify the problems let's hire 40 or
50 really good people throw them at the
problem and go from there anybody here
not heard about spanner okay you've
heard about big table okay so spanner is
Google's in replacement for big table
and this is a quote from the spanner
paper so again google google for spanner
there's a obviously there's a google
website all about spanner and there was
at least one good paper but this is a
quote from from that paper and
essentially what they've found
was that the overhead of eventual
consistency in dealing with that was not
worth it in the end and they've kind of
flipped back now span is a really nice
implementation the problem with spanner
is that in order to make span of work
you need satellites and that immediately
limits how many people can copy it it's
not like you know yup bus was yeah maybe
yahoo can't these days but maybe yahoo
could kind of like what they did with
big table and hadoop but it's not going
to be something that your average joe in
the streets or even your average Red Hat
is going to be able to duplicate unless
somebody comes up with a you know maybe
a quick start kickstart project for
putting lots of satellites in
geostationary orbit but the reason I put
this up here is because even Google who
are one of the only original driving
forces a big debt or a no sequel and you
know caps theorem is is really important
and we need to not have transactions
they've kind of come full circle and
decided that yeah transactions the
overhead of not having them isn't worth
it let's add them and let's try to
figure out where the bottlenecks are
when we have them but it's just so it's
a programming model which is very very
easy to use so good enough time for this
yep so I I think you know I my
background is in transactions I should
maybe you've said that stuff I'm trying
to be objective by the way no seriously
I am I think it's unlikely that acid
transactions will be standardized within
no sequel I don't think it's necessary
in some implementations again it comes
back to this use case you know the the
the proverbial using the right tool for
the right job transactions aren't
necessary in every single use case
that's out there in many cases that
transactions are used they're not
actually needed so to kind of push for
transactions to become a quote-unquote
standard in no sequel as they
essentially are relational databases
it's a non-starter II you know I would
I wouldn't back that and even if we did
that I don't think that something like
XA is necessarily even going to be a
standard within those groups that decide
they're going to do transactions but as
we've already seen I think you know full
acid is a goal for some employment
oceans and that that's a good thing I
mentioned foundation DB actually should
have mentioned cockroach I don't have
anybody who's heard about cockroach DB
its goals are essentially to do an open
source version of spanner but without
the satellites and you can do that the
only only if you don't have the
satellites the reasons that try to
condense the spanner paper into like two
sentences the reason google uses
satellites are so they can have a very
very small clock skew around the world
they can say with precise that
definition of what time is wherever you
are on the planet and that really helps
them with ensuring strong consistency
and transactions cockroach DB isn't
going to rely on transactions but then
that means that the clocks you can get
quite can consider you increase beyond
what spanner has but if you haven't
looked at cockroach DB and you're really
interested in what potentially is you
know that the hadoop of the next ten
years maybe you know take a look at it
so yeah I don't think acid transactions
are necessarily going to be standardized
let alone XA but maybe compensating
transactions or something that people
can can kind of get behind and I already
mentioned we did you know as an industry
we have got some some work in this area
over the last 15 years or so and there
was some presentations a Java 1 2009 on
like transactions in rest and web
services transactions and the next few
slides I'm going to try and give you an
idea of something that we've been doing
within within jboss to try and allow the
aim is to try and allow arbitrary no
sequel implementations particularly
those that don't support transactions to
be enlisted in it in a global unit of
work so you can have quotes
transactional semantics
say or something like um so XA is there
are a lot of problems with the way in
which XA worked today and in in a much
more distributed environment than the
way in which XA was originally SPECT
back in the in the 80s so if you look at
so we let the JTA the JIT JTA tries to
make XA be more distributed in a way but
XA was never really intended for true
distributed interactions it has no
concept of interposition it has no
concept of something like nested
transactions which i think are really
important it has no concept of recovery
it it well it has a very very limited
concept of recovery there's a there's a
recover operation that you can call on
me on the resource but how you do
recovery in a distributive environment
where your what you think is the
coordinator may not be the coordinate
may be subordinate and then it's what it
thinks as the coordinator could be
subordinate cetera et cetera top-down
recovery or bottom-up recovery that none
of that expect in XA and there are
optimizations that you can do two phase
commit that are simply not possible in
XA so XA doesn't talk about last resorts
commit optimization for instance but
everybody does it I think that XA XA is
good for relational databases because
they kind of Cape XA span out of the
work that was done in relational
databases but I think that a lot of the
bad press that transactions have today
in terms of two-phase commit essentially
comes back to XA and the way H respect
two-phase commit was defined way before
X in fact you know two-phase commit is
just a consensus protocol if you ignore
XA and you look at what to physically
could potentially give you some of the
bad press that transactions have goes
away and I think if I have to fight one
thing I rather fight no sequel vendors
to add acid and two-phase commit than to
fight them to a deck say I'd like to
make that distinction say okay even I
think XA has got some some warts so
let's not try and do that but I think
two phase commit maybe has some benefits
as an interesting so no in your vajay
we've got XD support and you're getting
rid of it I believe yes not good thing
but anyway because you're not providing
anything else mmm so moving on 10
minutes so we have a transaction engine
it's a it can be standalone it's
embedded in wild fly it was called jboss
transactions it's now called Narayana
but it's essentially the same same
project we just renamed it what I'm
about to present is a very condensed
version of a talk by Paul Robinson who
works on the transaction project and
there are a couple things there where
you can go and get some more information
what what what the team are being
developing is a JTA like API again
trying to keep things as simple and what
people understand so it looks like jTA's
got you know begin transaction end
transaction it doesn't have a set roll
back but it does have a set compensator
only and it's also interoperable with JT
a 1.2 the idea is that you can have
trend acid transactions and then enlist
compensating transactions within their
all the system will do that
automatically for you it's also supposed
to be trans air transport agnostic so
the idea is that you know define a local
you can use this completely locally you
do not have to have distributed
transactions in this if you don't want
to but if you start to do distributed
interactions it doesn't matter whether
your interactions are over so poor rest
over HTTP or rmi over iop or JMS or
carrier pigeon the aim of the project is
to essentially allow you to flow the
transaction information the transaction
context over whatever and whatever makes
sense for you and potentially lots of
different protocols at the same time and
it builds on CD I so it's all annotation
based so here we have an example service
it's the ubiquitous banking service so
we have a an account manager and then we
have an operation called transfer money
I tried the comments of my my way to try
and save some space
code on here that you could at least
look at so it's it's cold / pseudocode
but obviously the transfer money takes
in an account where am I transferring
money from what I can't my transferring
it to and how much you are transferring
and this is marked as being compensated
so the container immediately notices
that this operation is compensated so at
some point whatever transaction it's
being used with and if that decides not
to complete successfully we're going to
have to compensate this and in this
example and in the current
implementation it's all based on on
forward compensation if you say you're
going to be compensated well the
assumption is that you do the work so
you debit the account in this case no
locks maybe locks are held within the
debit account operation but they're
released as soon as that debit account
returns and then you do the next
actually one of those should be credit
account yeah see don't don't get me to
write your bank account software okay so
as sumo are those is credit account
don't look over here look look over here
then you credit the accountant and that
that happens immediately is also no
locks are held even if the even if the
transaction is still running by the time
transfer money completes everything
everything has been done and if we're
going to compensate then we need to camp
undo and that that undo may itself be a
separate unit of work a separate JTA
transaction so we have our account
manager and here we have a compensation
manager which we're going to inject
inject into this account manager and the
there's the credit account operation the
one that didn't appear here but it does
does appear here and you mentioned again
through an annotation what the
compensating classes that will be
triggered by the by the container should
the transaction undo in this case I
can't remember the exact reason but what
Paul basically didn't want any amount
that use that you say which is greater
than 500 I think it's the the overdraft
you're not allowed to have more than 500
and you can't debit more than 500 if you
try and credit more than 500 it's going
to force the transaction to only
compensate
so if you know JT a set compensate only
is essentially the same as set rollback
only so it doesn't matter what happens
after after that goes off even if more
work gets done when the transaction
tries to tries to commit or complete
successfully it will be forced to
compensate assuming we're going to
credit more than 500 we drop down into
here and we do whatever work which could
be start a new JTA transaction talk to
an Oracle back-end database add that
amount of money and then terminate the
transaction and the compensation data
which is again maintained by the
container automatically for you is the
account number we need to remember if
we're going to compensate for the you
know debiting and we want to actually
credit some money but we need to
remember what the account was and how
much so that we can then feed that back
to the to the undue credit class when we
need to compensate and then we have the
compensating handler here and so there's
a as interface called compensation
handler or compensators derive from that
they provide a number of different
operations one of them is you know
cunningly called compensate again the
clues in the name but you can see here
if that gets triggered by the global
transaction essentially rolling back
it's going to try and increment the the
balance in the database and it gets the
account number from the compensation
data that we saw there and it also gets
the amount but we obviously want to
increment a negative amount don't ask me
why did it this way why didn't you just
have a decrement I don't know I will
talk to him when I get back home so
that's why there's a minus one times
there he's obviously a computer
scientist not a mathematician or maybe
vice versa anyway so here you have your
compensating handler and the the idea is
that the container takes care of all of
this for you or the transaction system
takes care of all this week it is it
tries to be as opaque certainly when
it's running and all of this is being
set up by by or some of this has been
implemented by you it's it's opaque
system runs exactly the same as you
might expect with a you know say using
acid transactions in wildfly you start a
transaction do some work commit your
transaction maybe you don't even see
this happening maybe you know maybe it's
essentially you know container managed
transactions and they're all happening
as you're doing distributed interactions
again that that's all hidden underneath
the covers for you and in this case what
you don't necessarily know is that in
some services you're interacting with
something that's not too strictly
two-phase aware but again the system is
is handling all of this for you
literally got five minutes so just kind
of skip through the last two slides and
see there's any more questions I said I
wanted to mention a couple of long-term
research projects that we've been
working on both of these e you funded
one of them is called cloud TM it was
originally so the TM is transactional
memory not a good name for the project
in the end but also got approved we
could really change it cuz it didn't end
up being transactional memory it's
essentially large-scale cash in the
cloud it was based on a number of
different projects and new projects and
some existing ones that we had for
instance in finna span and the the
central thesis of this is that there's
no what you know there's no one size
that fits all there are a number of
different extent of transaction models
or a number of different consistency
models and in almost the same way as how
you know two pools or or graph or column
all good for different kind of use cases
different consistency different
transaction models again a good 4
different use cases and the idea is that
it adapts you you start out with one
kind of data store say on the far left
there and then as the data grows and the
use of the usage patterns grow the
system monitors maybe reconfigures
itself and maybe also reconfigures the
consistency model that's being used to
perhaps be slightly weaker consistency
and and to do this all dynamically and
opaquely this project has ended now it
any of you know about uu projects but
they have a finite life this one was a
three-year project it actually ended
last year and it was marked as being
very very successful if you go to the
cloud TM website which is cloudy
EU you can download those papers and you
can get more information on this and
most of the work that was done here has
fed into InfiniBand so if you're in
Finnish ban users you're actually
probably using this without necessarily
knowing that using it another project
that kicked off last year is called
leads it's similar in some ways to cloud
TM but in this case the idea is a data
data as a service but working in
federated clouds clouds of different
sizes public and private clouds infini
span and hibernate are keys to this and
also different transaction models acid
transactions extended transactions the
website here is leads door EU I think on
if not do a do google search for EU
leads project okay so in conclusion I
think and an enterprise applications are
very important obviously I think that
belonging to Red Hat we have found over
the last 18 months or so and are still
finding a difficulties with integrating
no sequel and big data solutions into
existing enterprise applications and
it's not something that were you know
knocking on the door customers saying
you need to do this because we say it's
more cases than calling to us and say
how do we do this and I said you know
that's a subset of applications that are
out there so I'm not suggesting that
every single use case for no sequel
needs to have transactions absolutely no
many no secret implementations now
support acid transactions in one way or
another one size doesn't fit all the
extent of transaction model maybe is a
way that we can we can get close enough
for for for several applications and
maybe those are the like the 8020 case
maybe they're the important ones but
also I think and this maybe goes back to
that PhD that I mentioned the stop
better education and where to use the
specific technologies I mean that's not
you know not specific to no sequel any
anybody writing anything in you know in
software needs to understand the tools
that they're using and and the third
party libraries or the products that
they're going to buy but you know no
sequel I think it kind of burst onto the
scene has been a being used an adoptive
oh
very quickly with some people not
necessarily understanding what the pros
and cons are okay we literally go up
three minutes anybody got any any
questions yes no no is a perfectly good
answer no some no sequel implementations
are eventually consistent some yeah I
would say that infini span as I
mentioned before ours is it's it's
spanning the divide between the datagrid
and and no sequel and it provides strong
consistency you can cook you can turn
that off if you want but you can have
strong consistency in infini span that
we what we've tested it scales to you
know quite a lot of nodes good thank you
oh yeah yes very
we know that the two project I mentioned
are they just happened to be EU
sponsored so the European Union has what
I call framework projects where industry
and academia can get together and bid
for hundreds of millions of euros to do
some long-term potentially blue sky
research and it just so happens that for
some reason the EU is interested in you
know some of the things we're interested
in if we could get sponsorship from
elsewhere yeah oh yeah it's all yeah so
the EU few years back made a mandate
that everything they sponsor now has to
be open source or at least have a have a
strong open source component to it which
is great on one hammer is bad on the
other hand because people come and ask
Red Hat to get involved because we're
open source and you know limited
resources yes very very different very
different yes so infini span as it is to
pull based yes you know that means like
a hat like a hash table distributed hash
table think of it as a distributed hash
table and in finnish band also has more
transaction models built into it than
Cassandra thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>