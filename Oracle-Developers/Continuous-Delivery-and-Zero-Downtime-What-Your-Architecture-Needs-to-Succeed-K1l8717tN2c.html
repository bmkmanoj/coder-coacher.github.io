<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Continuous Delivery and Zero Downtime: What Your Architecture Needs to Succeed | Coder Coacher - Coaching Coders</title><meta content="Continuous Delivery and Zero Downtime: What Your Architecture Needs to Succeed - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Continuous Delivery and Zero Downtime: What Your Architecture Needs to Succeed</b></h2><h5 class="post__date">2015-06-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/K1l8717tN2c" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is axel Fontaine and today we
are going to talk about continuous
delivery and zero downtime and
specifically what we need to put into
place into our architecture to make this
possible so few um few words about
myself have I've actually been involved
in the continuous delivery space for
quite a while as a consultant previously
and I've been talking about it for a
number of years across most conferences
in Europe I've I've also got an open
source project which some of you might
know which is Flyway who's who's heard
of Flyway okay who's using it good
number good for those who haven't heard
of it it's it's a database migration
project if you use a relational database
later and during this talk I'll show you
why it is actually very important to
have a structured way to deal with
change there and to be able to manage
that so we'll look at the the problems
tools like Flyway are trying to solve
and why you should use one I'm also
involved in my current company here
which is called box shoes which we'll
touch upon a little bit at the very end
of of this talk which is basically a way
to package your application as as an
ultra-compact VM and then deploy it on
whatever virtual in string
infrastructure you may have but we'll
touch upon that at the very end so we do
have a fair amount of material to go
through today so I would like to ask you
if you have any questions please still
hold on to them until the end and then
I'll do my best to answer them I think
the next session is only starting half
an hour after this one so we're going to
try not to over run it but I'll be I'll
be available in the hallway and for the
rest of the day and I'll be around
tomorrow as well should you have any
questions I'll be more than happy to try
to answer them the best I can
it's a bit of a disclaimer before we
start I've I've enjoyed John one very
much and I've enjoyed the the cover
session in the hallway very muchly in
the exhibition hall which had the
unfortunate effect that two days ago I
had absolutely no voice left so I could
barely whisper so it's been it's been a
hard recovery yet okay I took a vow of
silence yesterday which was very
difficult for me and I've been
intensively applying advanced Swiss
technology to try to recover my voice so
bear with me I hope it'll it all stay
with us till the end of this talk so the
goal for the presentation today is to go
on a journey and see how we can achieve
a fully automated deployment of a
complete application so we'll see what
that means as well into production
multiple times a day with zero downtime
but before we start we have to take a
step back and put things into
perspective and asks ourselves the
software world the software business
were in what is it really about or what
are we really doing here and to do that
I've invited on stage here a good friend
of mine we shall call in Montgomery and
he is my business person today so
Montgomery so happens to be has an idea
but it just doesn't have any idea he
actually as a he wants make lots of
money with it so he goes to his favorite
customer which we shall call Ned and
says hey Ned I've got this idea for for
this new feature this new product would
this be something of interest for you
and it says sure that sounds fabulous I
would love to have that
so Montgomery goes back to his company
and goes to Jim Bowie's Java developer
and Anne asked him hey Jimbo could you
develop this and Jimmy said yeah sure so
he goes to eclipse or IntelliJ and
develops the feature and tests it and
that works great so once it's ready he
goes to the operations guy
I hope they are not in the room which
then deploys it into production so that
Ned can use it and then give feedback to
Montgomery whether what he actually
described actually met his expectations
or not whether his wishes were fulfilled
and so if we look at this big feedback
cycle here we can actually map a number
of of techniques on the different
branches we have between the different
participants so if if we look at the
first leg there between Montgomery and
Jimbo that's really our development
process where we've made giant strides
over them the last 15 years and where we
we've moved away from from heavy
waterfall based processes onto much more
lightweight ones being it's crammed more
or these days maybe Kanban so that
that's that's been improved tremendously
the next section of course is our is our
whole continuous integration world where
we ensure that once we've we've
developed changes and we've pushed them
to a central repository that they're
actually not broken that they're still
working for everyone that get our
software in a releasable state and to
actually get the software out there that
some the focus of today that's how it
continues delivery and once we go beyond
that the feedback of the customer to the
business that's lean startup territory
Eric Ries Steve Blank Bob dwarfed that
type of stuff but as I said the focus of
today is continuous delivery so let's
get started
but before we start let's get to know
each other a little bit better so we're
gonna do a quick poll so I'm gonna ask
you to all raise your hands and keep
your hands up
as long as this applies to you so do you
go live at least once a year or hands up
yeah at least every three months still
pretty much everyone excellent every
month Oh hands going down already every
two weeks
got a few more going down every week
it's getting thin but we still have 10%
maybe multiple times a week some hands
going down we've got about 10 left every
day yes we have one left there multiple
times a day no but still congratulations
our winner is over there it turns out
he's not the only one there is actually
quite a few companies the do go live
multiple times a day and you actually
know most of them so the first example
is the world's biggest waste of time
also called Facebook which which is
actually very vocal about it on their
engineering blog they discuss regularly
how they they you live multiple times a
day sending it out they've got an
internal BitTorrent distribution system
they've got one big binary they push out
and and even at that scale it works for
them Flickr is actually um it's actually
very present in that space is also so we
don't shame them they change their
developer page before because up to
about a year ago or maybe a year and a
half ago they used to have a fabulous
statistic on their developer page where
they would describe their API and all
that at the bottom of that page they
were showing how often they'd gone live
over the previous week and the faces of
the developers who had been involved in
the last in the last push and on average
they were going live at about about 80
times per week to give you an idea so
that's about that's a fair fair number
for that scale because they still huge
it's been it's been very vocal about
this as well on their on their blog
about the whole continuous deployment
and everything they're doing there's
today it's a fantastic resource to
follow up as well if you're interested
in that space read their engineering
blog it's really really good and of
course last but not least Stack Overflow
I assume everybody in the room knows
that overflow anybody doesn't win think
so but what is interesting is if you go
to stack overflow and scroll to the
bottom of the page and look at the
bottom left they actually have something
very interesting there which is a
version number but not just any version
number this is a screenshot that took
about an hour ago so let's have a look
at what version is live there now might
even have changed in the meantime but
the version one hour ago was 2014 this
year then October this month one first
today so they've already done live this
morning and this will probably happen a
few more times over the course of this
day so of course to be able to achieve
that we don't start from nothing we need
a solid base we can build upon so we can
take it to the next level
so to do that the first step is a solid
development process I think if we need
months between
establishing requirements and then
drawing them over the wall
to development and drawing them over the
wall to test and into production it
doesn't work and this seems ridiculous
at first but yet there are still many
organizations where there might be
iterative development happening but then
a huge batch of features being bundled
together are being thrown over the wall
at this or then being thrown over the
wall at production so this is still far
more common than we would like to admit
the next step here is of course a solid
automated build process I wish this
would be meant as a joke but this is
actually a tragic example of what I had
a day at a customer a couple of years
ago where they had an application with
70 modules and building the project
meant going to eclipse and going to file
export jar file export jar file export
wha every time so unfortunately still
widespread
version-control something we take for
granted in development yet once we look
over the show and we look at operations
it is still very common to have lots of
configuration files and other other
settings and attributes being changed
without any revision control without
knowing who did what when and a easy
possibility to rollback
so again it is still quite widespread
outside of development that no proper
version control is being used and of
course here last but not least I think
this is probably the most common pattern
of all who's um who's currently working
on a project where there is an army of
testers at typing away and executing the
same test case all over here stole a
number hits um and I think this is that
this is really unfortunate because
certainly the the testers have got this
perverted brain that is that is really
great at coming up with scenarios that
we developers probably wouldn't think of
and and it's a total waste of their time
of making them do monkey work so I think
this is a good place where automation
must must supplement their skills so to
kind of put what we've discussed
together in diagram form so far well we
we've got Jimbo and we've got his
development environment so what does it
do when we develop develops this code in
his IDE and then this carries automated
build tool that will then compile the
code and execute the test some for this
some example here I'm using the
terminology from from the Google testing
blog which basically divides the tests
in three broad categories small medium
large I highly recommend you to have a
look at at that blog post just have a
quick Google search Google testing blog
a small medium large they they basically
came up with that concept because they
they had enough of wishy-washy
definitions what is a component test
what is an integration test how do you
to define it and so they decided to make
an exact categorization so the small
tests for example our tests that run
exclusively in memory no access to disk
no access to the network very fast easy
to parallelize and they provide you with
very quick feedback so that you very
quickly can know if they've run or not
and only when they've run successfully
can you move on to the medium tests
which now have access to the local disk
and local host networking and they can
then once again this is for example
typically used to test some database
queries or other things that you may
have there and so they they will then
run a bit slower but they'll provide
your feedback and if they're successful
you can move on to the large tests where
the sky's the limit so this would
typically be your selenium user
acceptance test type of category and
they run of course a fair bit slower but
they they provide you with very valuable
feedback so once all of these have have
run successfully we're actually ready
for the next step which is version
control we are to the best of our
knowledge it works on our machine so we
check in we of course almost on all
projects now I have this neutral referee
called continuous integration server
which will say AHA I see that something
changed there in version control I'm
going to double check to make sure you
didn't forget to check in a file to make
sure everything start working so I'm
going to run the build again with the
small the medium and the large test and
provide your feedback whether what you
checked in also works centrally or not
okay on to the main part of the song
deployment automation so the first rule
of automated deployment is do not deploy
snapshots snapshots are maven
terminology for some in between state in
between versions and in fact there is
some there's nothing wrong with with a
snapshot you can you can run in it and
and the software will will probably work
and let's all find that but you don't
really gain anything because if the SAP
shot turns out to be successful and it's
actually working perfectly you do not
want some in-between state on your
production server you actually want a
well-defined state a version but if that
means you have to rebuild the software
you're then having a different piece of
software a different artifact than the
one you test it so do you want to give
up your quality principles I don't think
so so you probably have to test that
again so you haven't really won anything
so why are people doing it well the
reason is quite simple the release
process has traditionally been
associated with effort and this is what
we're going to look at today how we can
reduce this effort down to zero so that
the need to circumvent it with deploying
snapshots disappears okay so what does
it look like all our tests have executed
successfully on the continuous
integration server we are now ready for
our release and our release is in fact
nothing more than establishing a link
between version control and the artifact
repository so the artifact repository is
the binary equivalent of version control
whereas we have the input of our build
process in version control we have the
output in the artifact repository so
that we can easily reuse it from there
so we will tag version control are
saying for example in subversion
that revision 217,000 355 is now called
version 3.4 and we will upload our
artifact with version 3.4 to the
artifact repository so our artifact what
are we actually deploying well turns out
deploying a number of things first of
all or code of course we need our
application to run
we need the code to execute you need
that but the code is actually rarely
able to function by itself very often it
needs some extra inset through which
resources it should connect to or how it
should behave within a certain
environment and that's why we have
configuration and most of the
applications of course will need to
persist some kind of state so that's why
we have databases who's using a
relational database here almost all of
you
okay so relational databases we all know
it we have a fixed schema that needs to
be that needs to evolve along with our
business requirements as the application
change so we need some del down there to
kind of move that along okay so let's
dive in code code of course is not just
single classes we deploy somewhere we
don't just throw a class file here and
update another one there we usually have
some kind of package we deploy all
together in the Java world this is
typically a jar file a war file an
earful now how does get this how this
gets deployed that's that's a question
that's unfortunately sometimes answered
like this it's um
ah well we've got this wiki page over
there and the guy that usually does it
isn't in the office today but I believe
we must say if the Pete is over there
and then you copy it here and you
extract there you change the permissions
you start and maybe a lot of work so I'm
all too often this is some this is still
happening so the first step there of
course
to to to put that knowledge into a
deployment script so that you are then
able to let the Machine execute that and
let pretty much anyone start that and
already the first the first synergy
potential appears because if you have a
number of deployment packages that fit
the same the same mold then then you can
reuse the deployment script to to deploy
a different ones and the deployment
script doesn't have to be something
complicated it can be as simple as as
pulling down new versions maybe taking a
backup stopping the existing one and
restarting the new one so how does it
look like we have successfully uploaded
our release to the artifact repository
we now introduce a new player which we
shall call the target system I'm
explicitly calling this target and not
development test or production because
these are really just different roles we
assigned to systems based on their risk
profiles but it is not something that
makes difference from a technical
perspective so in this case we have our
deployment script on our target system
the release process invokes the
deployment script which will then pull
the new version from the artifact
repository stop the old one stop the new
one and off we go we're up and running
now configuration
configuration is is an interesting one
because it's actually a pretty broad
term or a term that some that means
different things and so the first thing
we find under configuration is stuff
that well maybe it could change one day
so I decided to make it configurable and
this of course is very often abused as a
as a backdoor for slow-release processes
where it's too slow to push out a new
release so we have to quickly be able to
log into some machine to to change some
config so that what happens a lot there
and the second category of course are
they are the genuine differences between
environments where of course you do not
want to connect to your development
database in production or vice-versa
so how do we deal with these things well
I'm saying if it could change someday
maybe it could also not so why shall I
carry the extra complexity alone if it
may never change and so therefore I'm
saying if we can reduce the effort of
producing releases it's going back in
the code for continuous delivery
hard-coded is a new configuration for
all these things the general differences
between environments is something we can
unfortunately not eliminate and so we
have to keep them around but there's
some how tightly coupled to our
application we have the same life cycle
because the version of our application
may depend on the number of external
services and the following version may
depend on new ones or may not depend on
the old ones or so some things may
change there so we so we have this tight
link there between configuration and
codes of why separate them and risk that
they don't match at runtime let's keep
them together let's put the
configuration back into the deployment
package so how does this look we have
our deployment package we add the
configuration but of course we have
multiple environments
we'll have multiple sets of
configuration and immediately the first
question arises which one to pick
how do I know where I am that question
is being answered by environment
detection a big word for the simplest
possible technique to find out where you
are this could be scanning IP addresses
IP ranges host names or simply the
contents of a phial / etc' slash
environment and livid if the contents is
tests you are in test if the contents is
production you are in production or you
could simply pass a system property to
your JVM for example that says you are
in QA and then it's Sakura environment
and so once we know that in this case we
found out that we're in the test
environment so our code can then
automatically configure itself correctly
for that environment by pulling up the
right set of configuration for that
environments as we can then access the
correct resources and behave properly
within that environment so we've
deployed on software we've started the
code the code will then perform
environment detection we'll find out for
example that we are in production pull
up the code the configuration for
production and start behaving properly
within that environment now the database
the other that's another funny one
because you can push out 10,000 lines of
new Java code and nobody will care
push out three lines of sequel and this
guy's back ooh that's dangerous and so I
say nothing could be further from the
truth this is based on a widespread in
on a widespread industry mistake we're
making of of somehow thinking that they
are
DDL is dangerous I call this the fear of
DDL because really boo
it's totally unjustified and you wake up
and you have this image where DML is
this this innocent little thing and DDL
is just out there to kill you and it's
um and if we look at it from a risk
perspective it's actually quite a
different story so let's look at an
example I've got here an account table
with two columns the name of the account
holder and the balance of the account
now what is the difference if I make a
mistake in my update statement and a
zero out there the balances Twitter or
whether I drop the column from a
business perspective the risk between
DDL and DML is exactly the same as soon
as you have the potential to to
overwrite or erase data you have the
potential to destroy it so in this case
if you updating you have the potential
to zero out all your accounts or some
other value so that it doesn't work
anymore so from a business perspective
the risk is the same you either have to
have a very good backup or you can't
close shop now our DB Delta once again
the life cycle of the data base Delta is
very tightly coupled to the life cycle
of our code because let's say we're
using JPA or some other technology we
will have a number of classes available
that we are mapped to specific tables
that will expect specific columns to be
present and of course over the different
versions of our application these may
change and so why take the risk of
having them not fit together at runtime
when we can ensure this by putting the
data base Delta back into the Diploma
package so we've got it in there and we
can our database but immediately a
number of questions arise in what state
is my database what has already been
applied in the
environment which scripts have already
been run which statements have been
executed which tables have been created
which views have been dropped how do I
know because I have to make sure I
execute the things properly and the same
is once I have multiple database Delta
scripts
how do I know in which order to apply
them because I cannot rename a column
before I've created the table that
contains it so I have an implicit order
there between these scripts and I need
to be able to manage that and it turns
out that the problem is actually a lot
bigger than then we would think at first
because even on a simple development
team with five developers where each
developer has two databases one for unit
tests and one just for manual testing
that's ten databases already you then
add a CI environment a test environment
and a production environment just in the
simplest case so we're at 13 databases
already where you have to be able to
keep these things in sync and make sure
you have proper answers to these
questions and that is the exact use case
flyways set out to fulfill flyweight is
not alone in this space most of the
tools follow the same principle of being
able to have an an ordered way of
executing scripts and finding out in
which state a database is so it'll it'll
scan its house part for example if you
if you include the migrations in in the
deployment package so the code starts
you you pull in the flyweight API it all
starts scanning the class Spartan and
see that we've kept migrations available
up to version 9 for example and then you
query your database see that your
database is at version 6 it will then
apply version 7 8 and 9 and now your
hibernate session factory for example
it's good to start and off you go
with the rest of your application and so
the advantage of that is that you can
then show this deployment package at any
environment
be able to configure itself
automatically connect to the database
and migrate the database from any
version to the one you require for your
code as I said Flyway is not the only
tool in this space on flyways homepage
flower DB dot org I've included a
comparison matrix of the different tools
available in the jvm space to solve this
problem
I encourage you to have a look at that
and basically pick the simplest tool
that does the job for you and that works
for your project so we have we have
start the application environment
detection auto configuration and we have
connected to our database so that we now
use a tool like Flyway to see that we
are at version 10 in the database we
have migrations up to version 7 in codes
who are greater version 8 9 and 10 and
our hibernate session Factory is good to
start so that from a technical
perspective for net everything is in
place to use our application but we're
not out of the woods just yet
it turns out there's a few problems left
the first one very typical scenario I've
got a net here and is using some service
in this case registration service and
the registration services Conner
triggering a mail service that sends him
a letter hey Ned we're happy to welcome
you as a customer blah blah blah all
nice and well unfortunately somebody
from marketing heard that happiness goes
through the stomach and they would like
to include a little bit of candy in this
welcome letter because apparently it
makes people so much happier so um the
candy doesn't fit in the envelope
anymore we have to start delivering
packages instead so what do we do well
what's the classic answer we start
refactoring or we start changing things
so that eventually we have a second
that is then able to sell parcels sounds
easy except that this transformation
doesn't happen instantly there is a
certain time window and in that time
window urgent bugfixes might be
necessary or other features might be
ready ready to be delivered into
production so how do we deal with that
the classic answer of course is
branching we need branching but
branching is not without disadvantages
the first one of course is that you need
to start duplicating your whole CR
infrastructure because you want the same
level of of quality for all your
branches but but probably the most
important one is some is refactoring
your IDE does not see the references
within the other branches so if they
diverge all the nice beautiful help you
have to track them and update them ends
where your branch ends and that's still
not the big problem the big problem is
when the merge comes and we've all been
through that a big scary merge and it
takes forever and super complicated
eventually we make it true but that's
where it gets dangerous because nobody
wants to go through that again and so
what happened we start refraining from
performing essential refactorings
because we see that the danger is just
too big of running into another one of
these big scary merges so is there
another way well it turns out there is
and it's remarkably simple really
because how does how does an a project
typically run well our release is on on
a typical project there's something like
islands of stability so we are we are
stable now we must push out the release
and then we start refactoring so we
break break break break break things and
we fix fix fix fix fix them and
quick-quick we're stable again its
release its push it out and then we
start again breaking breaking breaking
breaking fixing fixing fixing fixing
things so that's we're stable again and
we
to shout again the next 3ds and so the
the alternative to that as I said
remarkably simple is we don't break
things we leave things running and an
instead we put a a a logical branch in
there which could be as simple as an if
statement or hiding a certain feature in
the UI like a menu entry in the UI
that's that's not visible and the old
code keeps on running we're not
impacting our customers and and at some
point as things start to improve as as
we were ready for that we can then start
letting it out and here we have a number
of possibilities we can enable these
things per environment so let's say in
development we want our new feature to
be visible but we still want the old one
in QA and in production and once we're
satisfied with the state in development
we enable it for QA and once that's
going well we enable it for production
and when that's working well we can then
eliminate our logical branch and we
eliminate the old code we don't carry
the weight around forever the complexity
weight of that and that's um it's really
quite powerful a good way to deal with
it on a typical project if you're if
you're breaking your stories down into
tasks as you as you analyze them is to
start asking yourself the question do we
need a feature toggle maybe you can even
deliver the things gradually so you
don't even need all these mechanics but
if you do need a feature toggle then
immediately also create a task to remove
it so that you have a reminder for later
that you can and don't carry around this
complexity forever but we have other
possibilities as well it just doesn't
have to be by environment you can do it
by group of users so that for example
your internal employees will see the new
feature on the production system whereas
your customers main
so that you can then start exploring new
avenues or you open it up to part of
your customers your preferred customers
or a random group a small percentage and
as that's going well you gradually start
bringing more and more people on board
until you then have it for everyone and
get rid of of their of the future taco
so this is really the key to be able to
release your master branch at any time
if you put this in there the whole
islands of stability saying the whole
thing just vanishes and you can press
the button any moment you're able to
release your software but that doesn't
mean you can do it without downtime so
let's have a look at this typical
architecture we've got our client in the
front we have a load balancer there
which will then redirect traffic to some
compute node which in most cases is
rarely able to answer the requests by
itself so it needs to query the database
to to get the information out of there
which we use it processes it and sends
it back out to the client classic
scenario we have continued to develop
our application and at some point our
new version is ready so what do we do
the first thing we do is to install next
to our existing version so we are not
taking the existing version down we
install it next to the existing version
we all remember what happens once we
install it the software start
environment detection auto configuration
so that we can then connect to the right
database for our environment and then a
migration tool I try way kicks in and
starts migrating the database from
version 7 to 8 to 9 and off we go so
here we see the first interesting thing
when we write our database migrations we
have to make sure
they were remain compatible with all the
versions of the code that are currently
live hopefully it's just 2 but it could
be more we're going to have a look in a
minute at a detailed example of how to
deal with this but that's not the only
thing if we migrate our database
successfully and we are happy with that
at some point we want to start letting
traffic onto our instance but we have
some state there our sessions how do we
deal with that
if the request gets to a new server and
the session is not in memory of that
server we don't know the user how shall
we deal with this so the classic answer
of course to this question is sticky
sessions session affinity so that at the
load balancer we remember which user was
started to which machine and we keep on
serving that that user true that same
machine for the lifetime of its session
until it finally expires and all the new
sessions are redirected to the new
machine so that eventually will drain
out all the sessions and we'll be able
to shut down the old version but you do
not even have to be in a 24/7
environment to realize that this could
take a very long time somebody gets in
the office at 8 o'clock in the morning
start working and this session could
stay alive for 8 9 10 hours even if
they're only based on the west coast of
the United States it doesn't even have
to be global so that's really too slow
from what we want to do so how can we
deal with that well the secret to this
is really we have to push the state out
it has to go either to the front or to
the back this sounds exotic but but it
really isn't I think we we have been a
big exception here in the Java world
because for most of of the other
platforms it is actually very normal to
have the state of the session in a
cookie in in the browser this is an
encrypted cookie that can be signed and
where you
a fair amount of security features so
that you are not running into any
trouble there but but yeah it's just
down there so this is the typical way to
do it in a rails environment in an
asp.net and rounded switch it's always
been done it has some additional
benefits as well first of all it kind of
forces you not to start packing too much
into that session but but that's that's
just as normal a TD the interesting one
is for provisioning the memory of your
service because all of a sudden you do
not need to provision for all the
sessions that are currently active you
only need to provision enough memory for
the ones you're actually processing
actively processing and so that some
that's quite a difference in your
capacity planning there and an immediate
saving and even if for some reason you
are uncomfortable with having your
session data encrypted and sign-on the
client in a cookie you still have the
option to move it to the back the back
could be a database and in-memory cache
it doesn't matter some shared resource
all machines can easily access to pull
the data from so here we have of course
just like um just like with the front we
have a a challenge of course is evolving
the state of that of that session data
and we because you're basically
serializing something in your
deserializing it later so you have to be
able to keep on gissy realizing that and
not run into surprises so the smaller
you keep your session ideally just a
user idea or something very very small
the easier it will be to maintain
compatibility and to be able to have a
high level of stability in in the
deserialization process of that data if
for some reason you have to break it
then you have to make the decision about
the risk involved just having a couple
of sessions that don't work anymore
versus having to provide explicit
migration parts for the old serialized
format into the new one when you when
you rehydrate so that some that's a
question to ask Deborah
highly recommend you to to favor the
cookie option if you can it really
allows you to get rid of all the old
clustering and the old thing that used
to can start farming your your servers
and have a much much much easier option
there is um there's a good filter
available on github just just Google
stateless session filter I on the on the
download of the slides I've included a
number of links azure but stateless
session filter for for Java and you'll
find it's just up a standard filter you
put in and and from from an application
perspective you use the servlet the
session API like before and what it does
is on the way out it'll serialize it and
it'll deserialize it once once the
request comes back in absolutely
transparently for you so this is how you
can do it with zero downtime and of
course once you've pushed your state out
there you can then get rid of all that
get rid of the old version and when the
next one comes you put it up there again
so that's what we call Bluegreen
deployments of having the blue one at
the top and then the green one a you
cannot alternate so it's um this is a
scenario that in its purest form is is
easier to achieve in the cloud because
you have a relatively speaking infinite
number of resources so it doesn't matter
if for one hour you will then double
your entire capacity and then reduce it
again or you'll increase your bill by
about fifty cents that's not another
disaster but if you're on premise and
you don't have unlimited capacity this
this is a model that's not always
feasible to apply it in its purest form
but you can you can do what is called
rolling updates which you can think of
as pretty much the the reverse of a wave
in a in a soccer stadium you know where
the people are kind of going up and then
down again and it's um and it's it's the
reverse of that so all the servers are
up and then you take one down your
update it if it's successful bring it
back up take the next one down
do it like that today you never have all
your servers down at the same time you
of course need to provision enough
capacity so that your remaining servers
that are up at that time are able to
handle the entire load so as promised
let's have a look at our database and
how to deal with that so in this case
we've got a simple example it's a little
bit small but it doesn't really matter
we've got a table with two columns first
name and surname and we are currently
accessing it through the surname column
where we're actually not too happy with
that we would like the column to be
called last name so we're going to try
to evolve that how this work we've got a
new version of our software here and
it's accessing the database true the
last name the old one is still running
yeah we've seen our blue green and no
that's so the old ones still running and
it's still accessing it true surname so
how do we deal with that obviously all
the table remain column breaks down
because then either one works or the
other not really bad so that's not
really an alternative there anymore so
we have to get rid of that so what else
can we do well we can add a new column
next to the existing one and we can then
create a trigger that will then
synchronize our data between these two
columns and will then do about copy of
the data to the other one so that
they're identical so that when the old
one access when the old code accesses
the old column we're fine and the new
one is able to access the new column at
the same time once we then decommission
the old version in the following version
we can then send some cleanup work so we
then do not have anybody accessing the
surname column anymore so we don't need
the compatibility trigger reintroduced
and we do not need the column anymore so
that we are now left with the exact
result we wanted to achieve where we
just have the first name and the last
name column we have gone from A to B by
not going directly but by going from A
to A plus B
to be call this expand and contract
expand a contract of the interface this
is something you will need for all your
external dependencies in this case I've
shown you through the database but it is
applicable to all kinds of external web
services or rest services if you're
accessing that are dealing with that is
generally the way to things so that you
decouple the release process and the
rollout process of your different
applications in your architecture so
that this one can roll out in
dependencies this is the recipe to get
rid of your big Saturday night rollout
weekend horror this this is something
that some it's actually it can seem
tricky at first when you when you solve
the mechanics way we solve therefore the
database it's um outside it's quite um
you have to put a lot of time into it to
make it right
luckily some some people have already
put put thought into that there is a is
somewhat of an older book it's not that
old so about 10 years old it's called a
refactoring databases it's by
addison-wesley the Signature Series from
Martin Fowler and even though the
tooling section is is a bit outdated all
these recipes for evolving your your
database like that like I've shown you
here for the rename in a in a reliable
way
are actually described they're in
details so it's just a matter of
flicking to the right page and kind of
following true and then you quickly get
the hang of that and and so that that
really allows you to to get much more
comfortable with rolling these things
out without having to worry about about
having your way back to reading and and
having to rely on slow backups to try to
restore something when when when really
all hell breaks loose because production
is down so that this really allows you
to to take the time pressure of that by
having all your migrations in a
backwards compatible way now when we
look at our entire stack of that we need
to run our application we've got them
we've got our physical machine or any
other it's I've been driven by I know
it's Colonel and and we've got some
libraries on top of that language
runtime which in our case would be the
JVM and then some app server and our
application what we really thought about
today's is a mouth updating these top
two layers and we we have shown that we
can have one immutable unit or
deployment package that that we then
regenerate after every commit as part of
our CI process and we will then promote
it from environment to environment to
our artifact repository and that's
working really well but then why aren't
we doing the same for for the rest for
the other layers below what's so
different there well it turns out that
there's been a number of issues there in
the past of having it too complicated
and no no easy tooling and physical
hardware and there's been a bunch of
things that that didn't quite work there
with athen and big distributions and
gigabytes in size and and so on so this
is actually the bit we we believe in in
my company that should be addressed as
wrong where we should have the the
entire service as one immutable unit
regenerated after every commit and
promoted from environment to environment
which is really taking what we've seen
today to the next level and moving into
a world of immutable infrastructure
where the whole machine really becomes
your deliverable as part of your build
process so if if you're interested in
that I highly encourage you to check out
our website we're about to launch tomb
so it's a it's a box fuse calm and if
not I think the principles behind it are
certainly something that is I'm that is
for me absolutely the way to go forward
so I either way whether you choose to go
with us or somebody else I absolutely
encourage you to start thinking about
going into that direction as well so
let's summarize
we have looked at our entire process but
we haven't really talked about
technology today we just thought about
principle so how could this work how
what is available how does it run
it's very about that so Jimbo develops
with eclipse or IntelliJ builds with ant
Gradle or maven so we can execute the
small tests with SMG or J unit and
execute the median tests with the V unit
so that finally selenium or webdriver is
then executing the large tests we are
then able to check in and to get
subversion on mercurial so that we then
have Jenkins teamcity or bamboo
executing our ant maven or Gradle build
where we then execute our J unit or
testing G tests or DV unit tests and or
selenium tests so that we can then
release and tag subversion git or
mercurial and upload to artifactory or
nexus we are then ready to deploy to the
target system where we pull our
boundaries from artifactory on nexus and
start the new code which performs
environment detection auto configuration
and migrates the database using a tool
like flyweight and makes it available to
Ned everything we've talked about today
it's been available for a long time it
is just a matter of combining the pieces
of the puzzle you can do it one at a
time pick a low-hanging fruit start
tomorrow but it's all well worth it and
you will get the full puzzle eventually
so once again one deployment package
with code configuration and database
Delta to perform environment detection
and auto configuration in migrating the
database using a tool like flyweight so
that we can then use feature toggles to
release at any time and we have to be
careful whenever there is States
it can be out there to get us but that's
not all if we believe in Margaret the
other layers of the stack also from
environment to environment there is a
tool like box use out there to help you
or you build your own and that's all I
have for today thank you for your time
my name is Axl
we have six minutes for questions yes
so the question is you talked about
putting environment configuration into
the deployment package but typically
operations people will frown upon
putting these things in there so how do
we deal with that well it turns out that
actually a large majority of these
things doesn't interest them
it doesn't really matter whether you
have the URL of your DB or or or the
other settings in there but they're
really worried about these passwords and
a very small subset of these values
where they don't feel comfortable
sharing them with with more people and
so at that at that point you have a
number of options that that open up you
you have the option of simply clearly
segmenting your network where you you
don't make it accessible for your
developers or the option of trust
because that at the end of the day you
trust them to execute arbitrary code on
your on your machine so why wouldn't you
trust them with that but if that's not
an option you do have the possibility of
supplementing what you have delivered as
part of your deployment package with
something that these guys input so that
could be coming from different sources
it could be put directly on this they
could have their own git repository and
as you load your application you just
have two different sources from which
you load your properties and you make
their source win against you all source
so spring for example makes this very
easy define the number of property
sources you merge them and and that's
that's good to go and this could be a
fixed one on this and they kind of
provision that however where they want
or it doesn't have to be on disk of
course you can pull it from some some
server somewhere at runtime as well
where only they manage the the data
behind that so that so but you have a
number of options there yes
okay so the question is does this
process also work for large enterprises
not only for the for the hipsters of
this world like Facebook's and others
but also companies that want stability
well it turns out that what they really
want is stability for the users more
than anything that things keep on
working there I think if a prerequisite
for this is a solid automated testing
strategy that you have confidence in
your in your deliverable because this is
really what testing is about is raising
the level of confidence and what you
hand out and this this actually works
very well because when was the last time
you cared about an update for Google
Chrome for example you don't because it
just kind of works and if you can
achieve a similar type of setting there
they'll of course look very carefully at
it the first time they'll be highly
suspicious of this because it's very
unusual but if you can prove that it
worked once twice ten times a hundred
times gradually they'll warm up to it
and it's um it's something that doesn't
have to be black and white it doesn't
have to put in overnight as I said you
can pick the low-hanging fruit and
introduce this gradually so I'm I find
that typically for example the database
migrations is something that's that's a
low hanging fruit there but it's always
a bit of a struggle of coordinating
between the DBA and begun installing the
application and making there and so a
good way to sell it is to to turn things
around and look at it from their
perspective and tell them well you don't
have to wake up at 5:00 in the morning
anymore because now it's gonna happen
automatically and gradually warm them up
for that but but it's not going to
happen overnight there's no magic but
it's but it there's many enterprises
using
that very successfully but it it takes a
while to get all the parties and all the
stakeholders involved to buy into that
any more questions yes
okay so the sort of question is how do
you deal with with applications sharing
a database and how to go along with that
so my the the thing you have there of
course is that you have given up a lot
of exact encapsulation because the the
the very bottom value your very inner
workings of your application and it's
defined details of its state are exposed
to a number of other applications so you
have to be extra careful of course with
that yeah I highly discourage this
practice so if you if you can find a way
to gradually move away from that it's
it's certainly the way I would pursue
but sometimes it's not even it's not
possible and then you have to apply
extra care for that you cannot start
changing things adding things is easier
of course but start changing things and
potentially breaking this unfortunately
no magic solution there I think
unfortunately we're we're out of time if
you have any more questions I'll be
available outside otherwise thank you
very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>