<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Querying Massive Data Sets in the Cloud with BigQuery and Java | Coder Coacher - Coaching Coders</title><meta content="Querying Massive Data Sets in the Cloud with BigQuery and Java - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Querying Massive Data Sets in the Cloud with BigQuery and Java</b></h2><h5 class="post__date">2015-06-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/uBReoRDCmHY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you for coming along my name is
Consul leonetti's I am going to be
talking about querying massive data sets
in the cloud with Google bigquery and a
Java first a little bit about myself I'm
a software developer from Melbourne
Australia I've worked in a variety of
different domains telco being one of
them which this story starts out from
and I also run the Melbourne Java and
JPM users group back home as well and
that's the Strothers largest meetup
group but I'll go on to the agenda the
what I'm going to be talking about is
where Google bigquery sits in the whole
big data hype phase talking about how
people deal with big data
what bigquery is and talking about how
you use that with the java api is
getting data into it and out of it and
also sprinkling in a bit of java rate
streams in there as well
so to here it has run a report who has
ever had to write a report make sure
everyone's awake yet some of you in the
middle may be chimp a here all right
so if you've has taken longer than a
minute to run yes
about five minutes anyone wrote a report
taking them that 20 minutes yeah in an
hour
you've been very patient yeah that's
pretty good
greater than a day anyone Wow
this guy do you sleep at work
unfortunately there are news gotten sick
of that and then tried to use Hadoop or
something else and no SQL database you
got a few few nods and stuff here
I didn't just quickly have curiosity as
anyone already started using Google
bigquery at all in here no you're in the
right place then for that as well
it's probably worth mentioning that
Google bigquery is in the same vein as
any other big data tool so Hadoop and
MapReduce is probably the most popular
way of when anyone thinks about big data
but I suppose there's a bit of hype in
the whole the whole situation where
people think that you have to have so
many terabytes before you're considered
big data and you read certain blogs but
generally as the hype dies down you
realize that there's a bit of there's
certain techniques in what you doing and
that those techniques can apply
regardless of your data size as long as
you've got something that's significant
and you want it to be done a bit faster
you can use these tools so by that I'm
saying that something as small as yeah
you know a few gigabytes can be queried
really really fast in this stuff where
traditionally with a Oracle or whatever
our DBMS you'd be waiting a whole lot
longer and having correct indexes and
things like that so this I suppose the
reason there's a sheriff budge on this
slide is basically to say that you're
all qualified now if you've all you've
had to deal with provide reports and
large data then you're also ready to use
bigquery so before I go on much further
I'll tell you about how we got started
with Google bigquery and why we picked
it over the other options but I know
it's 5:30 in the afternoon and two
already a long long day for Java one so
some memes to to wake you up
much like our problem we had a whole
bunch of business people sit around the
table like this and not really quite
know what was in their data just like
let's go and see what run some reports
and see what happens and I'm sure that
the solution that the justification for
our other large project that we did six
months earlier is in the data that these
these things are hiding so there's that
aspect the business not knowing what
exactly they want but they want to know
that they can get to it
you've got other concerns in the Big
Data realm so you've got Mark Zuckerberg
up here going and basically doing
psychological experiments on everybody
and using large volumes of data to take
away all your privacy and do some other
nasty things and then you've got to
stand the bottom here big the big data
hype that's going on and just it was
actually they gotten a report from last
year big data was at peak high it was
actually physically a curve and I didn't
actually know that there was a peak hype
others lot of people were taking the
term peak oil and then there's actually
scene that got in the chart and does
anyone know what this year's peak
Gartner thing is yeah do you know what
this is peak oh I sorry yeah you're
talking about the lifecycle yeah sure
sure
so yeah the the other thing at the
moment I think he's cloud as well all
going going now yeah okay so um I'm a
bit more of a serious note the client
that we had runs a number of popular
Australian websites and they know who is
logged in to their customers and they
may visit things like news or sports or
whatever it is some lifestyle sites and
they use third-party analytics service
so say anyone here familiar with
Omniture or Google Analytics those kind
of tools which basically track mean for
where your location is by your IP
address they also look at other things
such as you know they can basically add
in all the profile demographic data
about what they know about you and then
give you reports to the webmasters and
say these are the kind of people looking
at your site these are the referral URLs
where they came in where they left
there's massive amounts of data in there
that people
that people want to know after that they
had access to a whole bunch of CSV files
and the basically you'd want to know
they wanted to break that stuff down so
when we put that because of privacy
reasons you can't just give your third
party analytics provider with all the
data of your customers you have to
encode it and you have to scramble it up
so the data goes up into their site the
third party provider does some some
reports about who saw what when and then
spits back a whole bunch of CSV files
and it was our tasks to go through those
we had 13 billion rows and about 15
terabytes of data for a year of which
yeah the the questions that our client
wanted to know where what groups and
demographics were looking at at the
sites so for example what were males
particularly looking at sports sites
more than females and what patterns
occurred with time of day any other
thing that they could correlate against
those those tasks they wanted to know if
they could adapt their content so they
they trailed out it's based on who you
were coming into the site showing a
different content and seeing if you
stayed along on the site longer and then
as a result of that when you know who
you've got a new can target content or
advertising then you can also offer that
it to two other advertisers so
advertisers will then pay you money to
run advertisements and they'll pay you
more money if you know who the person is
that's looking at your site so you can
charge a premium for those ads finally
the client being in advertising and
marketing domain had big hand wavey
general managers who basically said we
need dashboards and we need lots of
animated graphs to make things this look
cool and again justify our existence
so what we did we started using Google
bigquery we used a budget process to
pull the logs from third party provider
into bigquery we wrote a single page
JavaScript up that allowed the client to
run reports directly from their browser
and then we also built the the
aforementioned dashboard and we had some
cool technology in the background
basically with WebSockets and it's very
thin spring move up to talk to bigquery
connect through and then push the
results back out so we ended up with
something very similar to this where
we've got a map of Australia and as
people are visiting that they get this
big red dot and the larger or the the
deeper color of the red dot that means
there's more impressions in that area
and then they're clicks as people are
clicking on on ads then they can see
that in real time or close to about five
seconds delay that otherwise very close
in time and then just just to show that
it's working a graph that keeps showing
how many impressions throughout the day
and then we can also break down things
here so agenda agenda ratio across
across different income groups and
saying kamut at the given last five
minute ratio this is the breakdown of
our visitors so you can do some some
interesting stuff so we query bigquery
is a data store so you put it on it some
of the internet it's hosted in the cloud
and you use it just like you would any
other database you write SQL into it and
then you load in either CSV or JSON
files and it's able to read those
without any major effort and then it's
also blazingly fast you may ask how fast
is fast
so the for our standard run-of-the-mill
month we had about 700 gigabytes worth
of web blogs and on a busy month that
would be about 1.7 billion rows of data
bigquery can tackle that in about 10
seconds and this is using some
aggregation so you're summing and you're
counting of of all those rows and you
get your result back really really
quickly if we wanted to do something a
little bit more complex say we were
decoding all those encoded data that I
was mentioning before and we had a
single terabyte of data yeah 25 seconds
so not much jump at all and it's
interesting to note that the size of the
the if you have only say 500 gigabytes
or 200 gigabytes it would also they
would be only slightly smaller it's not
you know not it doesn't change until you
go into terabytes that are taking a lot
much much longer than that but most of
the time of the queries that we were
performing we were getting our results
back in in within half a minute we'd
have a an aggregation done yeah and as I
said I had early in the earlier slide to
do a query on 30 billion rows or feet
and 15 terabytes of data was a mere 330
seconds so five and a half minutes and
we had analyzed the hobby is worth and
were able to aggregate demographic data
and pull whole lot of stuff in so rather
than take my word for it
I'm just got some videos that I've
prepared earlier with in here this is of
the bigquery web UI that you get when
you first start experimenting and down
here we've got a list of playground data
sets of which I'll be talking through
Wikipedia down here so the Wikipedia
table is a list of all the Wikipedia
edits from 2001 to 2010 and within those
there's a few different things we start
off with the title of the article
got things like he see this is moving
we've got things like yeah the language
the contributor IP address or their
their username and that's this guy
ketchup yeah we've also got the
timestamp when this occurred was it a
robot and let's see yes in session IDs
stuff we've also got the the size of the
table so this one's 35 gigabytes but I
thought for the purposes of this talk it
would be large enough and something over
three hundred and forty-eight million
rows of data you can see down there
there's a preview of the table just to
give you an idea of how how the data is
formed in the table towards the end
there's the got the number of characters
as well and so what we'll do is we'll
use a write a query to show you using
this and summarizing over this data so
yeah this is just a simple query of that
table it does have a limit of 1,000
characters but that it's basically we're
just querying the whole table straight
out that took four point three seconds
and went over the whole 36 gigabytes of
data in just that amount of time we can
then just do an order by we haven't had
to put any indexes on this this is just
the data as it stands and so yeah we can
run an order by yeah
yeah so just under nine seconds and
we've basically ordered the whole that
whole thirty six cubic gigabytes again
so the next step is to do some
aggregation I'm just doing this forward
all right so we're going to count how
many articles have been edited so the
ones that are have been edited the most
and so far you can see that the SQL that
we've used is basically identical normal
SQL ninety-two the query is valid
there's a validator just to double-check
syntax before you run it however if you
do make a mistake you don't get charged
now the so that's me just racking up the
query just to show you that the
validation it can stuff up that the
order by was wrong so I took out that
field there so now yeah let's order it
by title
all right and so the error that we're
getting here Google bigquery there is
one difference in what what's going on
behind the scenes is that it's managing
a MapReduce and managing these resources
behind the scenes they've got a slow
group by called group each by and what
the error message is basically saying is
that the size of the table is too big
for grouping by and basically using
group each which is a slower means of
doing its algorithm behind the scene so
as soon as I added the group each it
took it still wasn't slow it was seven
seconds and it summarized the whole the
whole table there just to show that it
doesn't slow down dramatically now we're
just all so summary summing the number
of characters for all those edits and
they're 8.4 seconds again to do that
whole that whole thing
so just a bit more about bigquery any
questions so far before I move on
okay so Google bigquery started off as a
internal project at Google called Dremel
there was a few engineers who wrote a
white paper which led to the creation of
Dremel Dremel basically focused on on
two things basically a tree architecture
and using a common datastore so that two
two means of making the database access
efficient internet got spread throughout
Google and was used everywhere so we had
things like when they crawl web
documents they use it for analyzing
Android install data spam analysis crash
reporting looking at their builds and
reporting on their fields and their
resources also and basically when you
get inquiries the public implementation
of that so it's just feature on feature
parts of a few steps behind but other
than that that's what you're using it's
the paid the paid version of trim or
basically it's the same technology so
that when you do run a query you're
going off a whole bunch of nodes as well
yeah the other thing you would have
noticed is that you don't actually have
to specify the compute so you don't have
to say how many servers you want or
anything around you know fast or slow or
anything like that data types is just
all the normal stuff you'd expect in a
in a database
the last one is just rest of it because
we can store Jason you can store a
record a nested record and there's some
syntax in SQL to flatten those records
out if you need to
right so to get access to bigquery
you've seen the web UI
there's the command-line JDBC connectors
so with the JDBC some reporting tools we
have been able to basically add bigquery
support by getting a open source JDBC
driver so this one here's star schema
SQL we tried we had an existing
reporting product that we tried tried it
with and we also evaluated a few
different reporting tools as well so
that's still not quite up to scratch in
performance wise so that was still
slower then actually just writing
queries yourself but however with that
said some of their reporting vendors
have actually wrote their own bigquery
support from scratch and one of those is
tableau and what tableau is is basically
a drag-and-drop it's like basically
gives cross tabs to whatever database
table that you throw at it and it
supports MongoDB and Hadoop and of
course bigquery as well so basically the
business users can drag and drop fields
and do their own reporting so that just
means that they're they're free they
don't need to rely on developers to
understand their data and a product
manager whose job it is to say find
who's which demographics are the most
popular at 3:00 p.m. on a Monday
afternoon can easily drill down and find
what correlates with those fields and
finally there's the the API so there's
api's in all the different languages
JavaScript Python and of course Java and
basically anything that Google supports
their client libraries on that they
support as well
I used to present this it's not
presented this toilet talk a few times
and I used to use this slide because
bigquery used to be a little bit more
pricier than it was
so we've big query you just pay as you
go there's no setup costs but you do pay
everything for the amount of data that
you keep and you do pay for every single
query that you run so till of course to
load the data in is free but then you're
paying basically $26 per terabyte per
month for the amount of data that's in
there which isn't too bad
it used to be $80 per terabyte so you
have to think a little bit more about
how much you were keeping because it's
very easy just to write a query in the
web UI and then save that data into a
like a temp table while you're analyzing
and drilling down on that data and
finally they've got this other thing
which is streaming data so it's meant to
be 1 1 cent per 100,000 inserts however
that's currently free until the 1st of
January however its was currently free
until the 1st of June of this year and
it was free until December 3rd 31st last
year and so every time the date comes
across that it gets spread out again
part of the part of the reason that we
suspect is because when we using the
streaming a streaming API it's not quite
reliable and sometimes you get the odd
500 error for no good reason so you have
to put a retry loop in but that's that's
why we we think it is but that's it
keeps pain free until maybe they just
want to make the impression that one day
you might have to pay for it but
otherwise you can also just stream data
in as well as if instead of loading a
whole file in at once the next thing is
paying for queries so queries are at
five dollars per terabyte and what
you're charged on is the amount of data
in the columns used so even if that we
can
table was a whole 36 gigabytes and
you're querying that whole table each
time if you're only looking at the title
the number of characters and yet some
other the timestamp or whatever it is
you're only gonna be charged for the
what is in those three columns and so
the one irregular thing I suppose is
that when you start doing this limiting
the rows doesn't do nothing to reduce
cross so in those examples they had a
limit 1000 but there was still the same
if I were to pull him it out it would
still be limit month out to be the same
amount of data processed because it's
but go by the column and stop by the
month you've been lonely meaning the
rows it doesn't it doesn't mean anything
there is a feature though called table
decorators which I'll touch on a little
bit later and what they do is basically
say I want the last five minutes of data
so if I wanted if I wanted to try and
restrict my data that way if I had a
streaming source of data and I knew that
it was only gonna be so much data within
the last say 10 minutes I could put a a
table decorator on that query to say to
reduce the data and I didn't get charged
accordingly any questions so far okay
all right so query quotas you can do
20,000 per day which sounds quite
generous and there's two kinds of
queries so you've got interactive which
what is what we were just doing there
you've also got a batch mode and the
difference is is that the batch mode
usually waits as an indeterminate amount
of time before it actually starts it can
be half a minute it can be up to five
minutes before it starts is based on the
load that everyone else is generating in
Google's compute farms all the other big
bigquery customers that said you've got
a limit on the number of concurrent
interactive queries so what we found is
that when we had a our reporting
dashboard constantly hammer for polling
updates appear sometimes people in the
business who were also using the product
at the same time could cause some
contention because we're healing our
limits so it's worth keeping in mind the
stuff that you can put aside to match
obviously batch jobs and anything else
where people are happy to wait and then
yeah keep them on batch just to keep
that at that pipeline quota free
yes it's something that they have
applied directly so if you query takes
about the same time and from what what
I've seen is that the query is running
but it doesn't it sort of changes state
so a batch query will be in this sort of
idle running mode and then switch over
and actually start doing its work and
you can usually gather the difference by
running the same query in interactive
mode and then in batch mode and clearing
any caches and whatever else and then
just timing how long the difference is
that basically that's how long I waited
before it got out of the batch mode
State just to speak okay sure so the
batch mode doesn't count to those that's
right yeah that's part of the product
yeah yeah I think it's 20/20 concurrent
queries and on terabyte of data being
queried so when when I'm using the
validator I can see how much the query
is going to take and that's how we go
together how much I'm putting into my
pipeline okay all right so the secret
sauce how does Google actually manage to
achieve this kind of performance the
first thing is the column that data
store so a traditional database loads in
each row and and all the columns get
read regardless of whether your query
uses them or not
so here in this example I'm just after
the ID and an email and then I have to
go through and load the name and the
gender fields regardless I might have a
really smart database if I want and I
put an index on there then the index you
know look at the index first but
otherwise I'm still reading all that
data I still have to do I owe to load
that data into the system to deal with
it
and so yeah it's a little bit redundant
what Google do is that they store all
the columns separately it seems a bit
weird but it's they everything is kept
aligned and went in the cases where
you've got repeated fields there's an
encoding and how they store things so
they keep the rows aligned internally
but though
what this means is that when I'm going
through looking at each column I'm only
looking I'm looking at less data I'm
loading in less data right and the other
thing is that because similar kinds of
data stay in the fit in the fields I can
achieve much higher compression rates on
that so I can compress that those
columns a lot better which means that
then there's less IO to load and
therefore that Korea's faster again the
second thing that Google does you may
have heard that Google has got a lot of
hardware and they're able to distribute
their computing across lots of servers
and the thing being without that is that
when you do this in the Dremel white
paper for example they talk about 3000
nodes just going your things but it's
really up that query and for those of
you who've looked at fork/join and any
of the new stream stuff in Java right
the one of the common things is don't
use parallel streams for everything
because there's this performance here
under a certain number of items in your
source collection yeah you have to pull
that stuff back together and you lose
time so the whole point about tree
architecture it's something that Google
have built basically not just for
bigquery but a whole bunch of their
MapReduce stuff they've they've had to
engineer a way to send their work out to
multiple servers and then quickly pull
the data together so tree architectures
that the other half of this is why why
this works so quickly and they were able
to pull say those three thousand nodes
back within seconds it's interesting I
don't know if anyone was at the big data
yesterday afternoon everyone go to that
the big question answer yeah and this
question around using Hadoop with I
think it's pink or hive the basically
the SQL wrapper so you could do some
very similar thing with Hadoop but the
one of the sticking blocks is the timer
takes two it knows how to distribute the
query but then pulling that query back
together again is a trickier to our task
and eats up a lot of time so this is
where Google at the moment of got a bit
of a step ahead so yeah we basically
looked at Oracle terms of having hosting
the volume of data getting sis admins to
run it look after it having to write
indexes before we actually could use the
the data there was way too long and we
could basically bypass this and go
straight to go straight with bigquery
Amazon has a product called redshift
which is very similar however the
difference with redshift is that you've
got to specify the compute so you've got
to basically uh know about how how much
computing power you want to throw at
your queries but also then if you don't
use that power then you'll know you're
wasted it of course there's a dupe and
MapReduce so yeah what I said before
basically if your it works quite well
for certain kinds of things potentially
more batch operations and with larger
bits of data and there's nothing wrong
with that and Google will actually go
through and there's a white paper about
a MapReduce versus Google bigquery and
they're actually quite quite nice about
where you'd want to use MapReduce over
bigquery however that said you so the
interactive stuff the more quicker
response times this is where bigquery
starts shining and finally the Dremel
white paper that was published by the
Google engineers that started this whole
thing someone took that and then made a
concerns project called Apache drill
which basically runs on top of Hadoop
and it also in a whole bunch of other
different data sources as well and it's
basically the open source implementation
of Google bigquery in this instance
you'll have to use to any do any of this
so do your own hosting and you would it
might work out better if say for example
you you had for regulatory reasons keep
that data within within your own
premises that might be something to look
at the project's still really new so it
was still an incubating Pro project and
it will you know will take a little
while to get to maturity but in the last
few months like every few months or
revisit the website and it just comes
leaps and bounds so I think it's now
starting to get a bit more polished and
potentially that this talk maybe next
year will be that drill and not not
bigquery
so I think I mentioned all the points
here so bigquery the other thing that it
does do is we keep up import data from
Google Cloud Storage which was we didn't
basically have to write our own now run
up our own interim nodes just to be say
tumbleweed hosts that just keep CSV
files that can pull from so it can
directly read from s3 and from Google
Cloud Storage to save that extra step it
is yeah it said there's no setup fees
involved the account is set up it's
basically putting your credit card
details in and have a Google account and
then you're ready to go and as Java
developers we already know SQL unless
you're really into hibernate and JPA
then you might be a bit sketchy but
Westers most of us are okay so we're all
ready to go we've got all this access to
this compute power straight up that we
can start doing serious stuff with and
just this is a comparison it's a little
bit old but this is from the Google
technical white paper so here where you
comparing MapReduce in its default mode
we've just used doing reading full
records then what they've done to try
and be a little bit fairer is that
they've they've tweaked it so that
you're only looking at the columns that
you're after and still Dremel wins out
now this is a little bit biased but yeah
this is Dremel is still significantly
faster then then my produce yeah there's
something that was mentioned yesterday
about apache spark which is something
like the replacement hadoop engine and
that might might be a different story
again with that say that presents it's
like pulling back at all its data from
its nodes after that
so now I'm going to talk about the Java
API and how we went about it and without
connecting to it so the first thing to
get started with is actually getting
some authentication I'll also cover
talking about reading the schema loading
in tables querying the results and then
using some table decorators at the end
we've got with the API uses OAuth and
there's different there's different
kinds of OAuth for different application
types so there's a server to server mode
there's a web server a web server
account so if your hosting or web app
the servlet will redirect to the Google
authentication page where you'd use your
normal Google credentials
there's also client also say if you've
got a JavaScript web app there's a
JavaScript library that you put in your
web page and you get different
credentials again to use for that and
the other mode is desktop mode where if
you've got a mobile phone app or just a
desktop client app you'll actually be
given a URL to paste into your browser
which then gives you a code to paste
back into the app to then give you your
your secret access keys and tokens the
documentation was recently put around to
how you create these it can be a little
bit tricky to find the right one and if
I know which value to copy and paste
from another console into into your
properties file or whatever but
generally it works okay authentication
I'm really talked too much on this as
these are all dem examples from the docs
that come with it
more so everything that you do you call
this new newbie query over here and
there's just a builder pattern to pull
together the details of your credentials
and basically created you be query
request
so the next thing looking at the actual
metadata with the API
so these here what I'm doing is that
this is just wrapping up using Java rate
streams but what we're doing here is
just looking at first the list of
datasets that we get and then from that
looking at each table under each dataset
and so then we're just basically passing
this through and one big stream passes
on to the the next and does the next
query in turn the only notable thing
about this is that just quick show of
hands who's been using Java right and
the streams API so far a couple of you
so probably know that when you get
checked exceptions there's quite a bit
of a pain and so the we're basically
using this lambda or exception wrapper
around around those and so if you do for
that basically out of all that's doing
is it's just another lambda that's
wrapping up that call and then catching
any exceptions and REE throwing them as
runtime exceptions so that it lets you
basically still use the same the same
interface there and not have to write
try catch blocks around it around
everything and so that's the the output
just of looking through getting getting
a project and iterating through and
getting all the tables through there for
loading it's quite simple the Java the
Java API it's it's all chained together
so when we've got over here table
reference the table reference all the
setters will return another table
reference so you can just change the
calls together which is a little bit
nicer thankfully then most of the stuff
you used to with Java so we can pull
together basically our table name and
our columns and their types and fire
that away I'm also going to use some
groovy here so you've not to scare
anyone off but it's almost the same the
only neat feature about groove is that
you've got these constructors which let
you set the
but he's inside them and that makes just
the code slightly slightly bit easier in
this example it's not that much of a
difference I can swap between two right
and they they are almost the same but
the groovy one is you start doing stuff
with more roads and more tables starts
starts becoming apparent it's a little
bit nicer to load in a CSV file what you
do is that you basically just configure
the table ref in the scheme of the same
thing as we did previously and we
basically creating a new job
we'd be query two then loading that data
the only thing that we what happens here
is that we're getting a file reference
earlier on and then streaming that file
in and the bigquery API looks after
managing that in two separate
transactions that will also take in a
few different input streams and there's
different versions of that that same
method to load in things differently
so for querying this is quite easy the
query stuff I apologize for the font
size the the query stuff though is you
basically write your query inline
there's a few different modes such as
dry run where you can just basically
like running the validator in the web in
the web UI gives you back the data but
so it gives you back the statistics of
the query but not the data so if you're
conscious about money or time or
whatever that's a good way to to test
that out you can specify a timeout the
timeout just means how long that initial
call will wait up to before handling
back if the query doesn't finish in that
time it's still running in the
background and you just get back a job
ID which then you can query another part
of the API to get the results later on
there's a query cache and that's on by
default basically just means if I run
the same query again within a 24 hour
period it's going to just get back the
same results and I may get charged any
extra for that query and then you can
also page the results as well so
generally if you're going to have lots
of data but you're not Oprah gating it
for whatever reason you can you can
specify how many paid results per page
and then you get a a page a bit of Jason
to go go through there and then finally
this query response until complete just
basically means I'm just looping through
and checking the job status each time to
see if it has finished or not and
finally using table decorators so the
only thing that changes in this instance
is that our query now has this syntax at
the end so we can specify how many
milliseconds back we want to go so in
here we're specifying the range that
from an hour ago to one milli second ago
we are once they can go sorry though we
we want all the data from that period
and it will only return that that range
of the table so if you've got to if
you're constantly streaming data into a
table and
for in our instance when we had our
dashboard we'd only care about the last
five minutes or we would you know call
for a particular period that's how we
how we did that and therefore that price
of your queries is quite inexpensive in
terms of the options that you get yeah
generally if you when you run a query if
the query result is going to be bigger
than 128 megabytes you have to run this
flag allow large results and save the
results into another table otherwise the
query will fail the other things that
you may want to do that I perhaps
haven't covered is a right disposition
so there's some control if the data if
you're maybe not if you're streaming but
if you're doing another query you can
write your results into a table but if
the table has already got data in it you
can say whether to fail whether to
truncate the to die at first or whether
just to the were penned that data on so
many questions about that
yes we did
yeah and in this sense that we did the
code was a bit more modular than this
that we would literally just look around
to have another thread that was checking
through and checking the job state so
the basically once you got the job ID
you you can do that everything else
offline I suppose if that makes sense so
then you can just query again so if
you've got an event loop and you're
checking that the state is either
pending or one of the little failed and
then everything else is okay then you
can just keep looping around but yeah
it's not it's not ideal it is one of the
annoying things and it seems to be that
it's cost lots of other products not
just the create as well kind of wasn't
any other questions so the question is
do we input that in real time and yes so
we've got another web service that's so
aside from loading in data from the
third-party analytics provider we've
also got a ping back service on one of
their webpages which is also providing
that data to another to a key query
endpoint and it's just using the
streaming API to keep heading through we
can do up to 10,000 10,000 inserts per
second and then if you want more Google
have 50,000 here and a hundred thousand
here I believe so they can turn that up
for you if you want good justification
yeah
yep
so the question is how you import your
existing database to Google there's a
few techniques you could export the
whole thing as a adjacent or a CSV file
or you could use the streaming API to do
that there is also a whole bunch of
examples they generally use Python but
there's a few in a few different
languages where you can literally just
say have to you know write your own app
that basically connects to the old
database of these loading the data the
the data straight in so write your own
write your own app basically to do that
but otherwise there should generally be
a way that you can export your data into
the right format from your existing
database and load and load that in what
we had with our product is what we had
to sometimes talk to our legacy
databases to get some reference tables
and they would be updating every few
days and so what that meant was we've
had a program that connected via JDBC to
the legacy databases called in the data
in batches and would then use the
streams API to update the take the same
tables in Google bigquery and so that's
how we were what we were doing to keep
them in sync that said there's also some
mortals so we haven't investigated them
ourselves but there's a whole bunch of
ATL providers and some other yes tools
that are in that ecosystem that allow
you to do that even more yeah for most
fashion I suppose
you import data they are processed in
the college to make it easier I think so
the question for those who didn't hear
is just do the google do any
pre-processing in the background they do
have their format for their columns so
as you are loading it in the yeah they
do have it's like if your repeated if
it's nested and all the other things
there so they have to do a little bit of
processing to keep the columns in sync
as far as the what they're doing after
that if they're doing other stuff though
apart from the compression that they'd
be applying to those tables I'm not
quite sure what the what it would be
yeah yeah I don't I think that's been
the whole point of it though is this PI
this MapReduce elf algorithm and so
that's where the the weenie is coming
from yep
question is how do you handle schema
changes and the a little bit of
anointing so usually with a you know
NoSQL style theme you've got the
flexibility of you don't have a fixed
document set unfortunately the only
valid solution we've found is to
basically create a new table out of the
old one adding in whatever columns that
we require yeah so given the you're not
sort of paying double though the way the
Google billing works is if you're
created and you create the new table as
basically just a select star from your
old table but like it and load the
results into the new table with the new
columns they're there and then delete
the original table almost within the
time frame you're not going to be
charged double for the month or anything
like that
the timing is it's prorated I suppose
for the life of the table so your
billing won't go out and because it's so
fast anyway it sounds like you know if
you're thinking in traditional a DBMS
Len do you think our whole export of
this amount of data would take days
however you know it's actually fees it's
actually kind of feasible I won't defend
it that it's important it's good you can
create views so I didn't really touch on
that but there is views as well so you
could potentially write a view to return
the results per the the old old table
there any way in terms of inserting into
if you had lots of codes yeah I'd
probably look at just my grading we've
got updated the the schema then I'd
basically update the code as well that
was updating and keep that timeframe
quite short there's a whole bunch of
other things that you've got the concept
of data sets and you could potentially
partition so have the like a staging
data set with the new table when when
you're ready to flip over start pointing
at the new data set yes so that there is
yeah a few support basically there's
nothing there's view is just a query
there's you know as far as far as its
concerned but it's not like a persisted
view like I know the traditional Oracle
wouldn't all that you can do that's
actually using storage to maintain that
view however is a view in bigquery I
don't think it's doing that
so he had a line and sure yeah yeah so
the question is how did the columns
align there is a good video I'll refer
to it at the end of the talk around to
how the alignments under the sky it goes
into much more detail and gives some
good examples it's probably good to
watch watch that video it's also
mentioned in the Google they leave the
white paper as well but that's a bit
more dry so that the video suddenly
short 20 minute watch I'll just move on
nice very nice so the question is is it
only constructed data you can have JSON
data and load that in as well and nested
and repeating things there's a few
different commands so when you clearly
want a table that has repeated fields
you can there's a flattened keyword to
flatten the datasets out so it looks
like a regular table in the traditional
sense and there's a whole bunch of other
keywords in there so you can have a look
at the query reference that Google
provide and they've got some good
examples to make that pretty
straightforward and there's a few other
features that they like special keywords
as to working with those special data
sets as well so you're not you're not
just always having to convert it flatten
it every single time okay I'll talk
about some of the annoyances that we we
faced when we did our project the first
thing was we quickly learned that it
wasn't so great for the traditional
normalized approach where you've got one
big encoded table and lots of little
reference tables and once you started
doing left outer joins passed say five
tables your query performance would
start suffering for really no good
reason so I could do this
they say join on three tables and run
that say I had 16 tables to join on and
I did that same 16 so it did those in
three blocks of three each of them will
take well under five minutes
but then when I went around the other
approach and tried to put them all into
one super query it would take hours or
not even finish it all and then just
throw a very strange error message so it
was just unfortunate that that was the
case but that's that that's the way
that's the way it is
basically just keep your data
denormalized the other benefit of doing
that is that when your clients are using
things like tableau it's much easier for
them the experience is much more richer
because they've got the data right there
and they're not asking their reporting
tool to try and do this the joins for
them as well so they'll don't benefit in
speed anyway the other thing was as you
saw in the demo the group by each and
the join each that's just it sort of
like almost by trial and error that you
because you won't really know how big
your query is until you run it apart
from running the validator all the time
and then that's not always super
accurate so it just feels like a bit of
a ok I've got to run it with it's sort
of like why don't they just do it for me
and and let let me get my results and
just maybe Ward me that it's gone into
the other into the other mode the other
thing being is that occasionally you get
really strange error messages resources
exceeded you can do a few crazy things
such as if you wanted to deduplicate a
table so normally you've got the
distinct keyword in regular SQL a
distinct is just a group by of all the
columns and so when you ask how do you
do a distinct in B career they say just
do a group by of all the columns however
if you've got a very large table and you
try to do that you'll basically get
something really crazy there is a
shuffle error for example so there's a
few number of posts and Stack Overflow
and my contact details are at the end
and if you look at my sock over flow
account you could see the the questions
there where that was that was an issue
and I did mention about the concurrent
rate limit when you've got multiple
people using the same the same ETL
processes also the same whole thing
using the same account at once and you
potentially would mitigate that by
potentially keeping your data in
separate Google accounts or separate
data sets so that you get around around
that query in limit and finally the the
API gives some bad error messages
sometimes that you required in this case
the required parameter is missing reason
that's required that's great but it
doesn't tell me which one it's talking
about so it's not consistently like this
but sometimes especially when you've got
say Anna nested tables stuff that's the
this was from creating a table and it
was just kind of talking a lot a little
bit of time to to find what was going on
there so just with a couple of minutes
left I just wanted to leave you with a
few points basically bigquery allows
everybody to use Big Data
it is also it's quite fast so it is much
faster than anything else so known terms
are also set up in the ramp-up time of
getting developers on board to
understand things and also getting
hardware set up and things like that I
think that basically the amount of money
that we paid to Google over the course
of the last 12 months you know was a
mere pittance it wasn't you know but
anything over say two or three thousand
dollars in queries so you know take that
in to costs of labor and everything
training and everything else then it
kind of speaks sort of speaks volumes in
that regard yeah you've got an API the
API is are quite quite flexible they've
also got people on Stack Overflow Google
engineers and they're quite good about
responding within a day or so which is
quite nice and then you've also got a
paid support option too if you need if
you need that and then yeah as I said
SQL means just quick a wrap up time for
your developers but also means existing
reporting tools can also use use it and
then get stuff with Java right and
looking at streams
I think if I was to give another talk
next year I would be talking about doing
stuff with reactive streams and getting
the data out and doing some more live
understand pulling that data in and
streaming it through as its as it's
getting in and understanding this whole
push-pull concept in reactive yeah
that's that's everything the the
references so to answer your question
before the inside look at Google
bigquery is the is the white paper that
goes on about comparing with MapReduce
and how bigquery works behind the scenes
the video about how the query works
which demonstrates the column the column
format how that's stored they've got
their bigquery java api not the easiest
thing to find surprisingly but i've got
the link there and they bigquery query
reference which you can get to quite
easily but just again for your notes so
you can see what functions you've got
and it's quite they've got a good set of
functions is not like an SQL where you
you have to do a lot by hand okay
question yes the question is do Google
just guarantee not to troll your data I
don't think they do yeah yeah I'm sure
they I'm sure there is some privacy but
if there was some government agency or
regulatory agency then then they would
they would have to give that over so
that is our concern is that you can't
just throw all your data or if you've
got some regulatory requirements which a
lot of companies do then yeah that
you're getting either going to have to
encode it but you're gonna have to just
not use it look at something else
thank you it's any more questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>