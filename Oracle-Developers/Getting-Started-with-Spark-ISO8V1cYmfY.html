<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Getting Started with Spark | Coder Coacher - Coaching Coders</title><meta content="Getting Started with Spark - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Getting Started with Spark</b></h2><h5 class="post__date">2017-10-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ISO8V1cYmfY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you very for coming especially
early in the morning I know I must have
sort of given you a jolt of spark in the
morning and I mean I see all of you here
all of us are passionate developers and
in this big data world spark is it I
won't say it's one of the bleeding edge
of cutting edge it has been around for a
while it was open sourced in 2010 and
now it's sort of getting mainstream
momentum and it's always good to start
with getting started and so today the
way we are going to go about it is first
I'll introduce myself we're going to see
parts of a video and it's a Formula One
pitstop and you will understand why I'm
going to show you that in a bit then go
through a bit of the architecture
because it's always good to understand
why we are doing what we are doing sort
of and then we'll get into life coding
so the session is going to be packed
with material please try to hold off
your questions to the end if you if I'm
not able to take all of them
you can either tweet it to me or you can
tackle me down I'll be at the developer
lounge and we can talk more about it all
right so again thanks a lot for coming
at 8:30 in the morning I know that takes
a lot of commitment so first of all a
big round of applause for all of us so
I'm Nicola an America
I work in Goldman Sachs I'm a vice
president out there in the private
wealth management technology division by
my academic qualifications I have a
master's in mechanical engineering I had
not written a single line of sort of say
Java code before I joined Goldman in
2012 July 3rd 2012 was the first time I
wrote hello world because July 2nd was
my first day so and then from then on I
have now I own eclipse collections which
is an open source collections framework
and recently I was I was inducted into
the Oracle
developers champion program so with that
we are going to move to the the video
I'm going to show a minute of it and you
will see
this was what the pitstop in 1950 looked
like so in Formula 1 you have pit stops
where you know the the teams come in to
change the tires to refuel etc etc now
this is gonna go on for one minute and
you will see why because there's only
one person working on one tire and the
other person is refueling etc etc right
so it's sort of time-consuming and
that's that's the message which I'm
trying to drive out here that in 1958 we
were sort of a single-threaded
application like we that that was the
age where you know we we were trying to
get to we're trying to do a job and we
were trying to do it in a very serial
way and this is pretty much serial I
know there are two people working on and
we can call it parallel but I still
consider it as a serial but then jumping
on to about 2005 or 2013 for that matter
you can see that it's the whole Ferrari
pit crew everything is distributed and
look how in how much amount of time the
actually car came in and it caught out
and that was real time by the way the
the pitstop record for the fastest
pitstop is one point five eight seconds
held in held by the Williams f1 team in
2016 so again I always have to get like
some more the other aspect of mechanical
engineering and all of my talks because
that's like I'm a robotics engineer with
specialized like specialization in
robotics so it's sort of like close to
my heart but anyways the the moral of
the story is back in the day things were
not that fast they were made fast now
they or they're fast now because at the
advent of the century we started
venturing into the fact of distributed
computing like the whole new concept
like there was a D there was an era when
parallel computing was the next big
thing but now we are at that place where
with the big data world and everything
distributed computing is then expecting
it's it's fine this just the agenda
gonna cover through that but it's I've
got like about four or five slides most
of the talking is going to be while I'm
demoing so this is a spark application
architecture and I thank my good friend
Ken site to actually lend me this photo
what strikes you there so you can see
the and I call these as the horizontal
planes or the denominators the the
biggest common denominator is a
distributed file system or the HDFS or
local file system
okay now you will have all of your data
on this local file system but then you
need to process it you need to process
it in a scalable and a distributed
fashion for that comes the next step
which is Apache me sauce or yarn or for
that matter spark stand-alone cluster
computing where they take all of this
data and they figure out and and then
they take all the workloads which are
there they figure out how they are going
to distribute that data okay they they
understand in which clusters they are
going to be pushing the first they'll
understand how much resources are
available which resources they can push
the the work to and then they'll
actually end up running your spark or
execution engine on each one of them and
that doesn't happen on day one right
like there is a bit of an architectural
system going on and the reason why spark
is so famous asked or three the reason
why even spark was found back in 2009 at
Berkeley was because it's sort of an
extension of MapReduce
MapReduce is very slow when it comes to
doing complex computations when when it
comes to sort of streaming the data or
getting the data one after the other or
having complex computations like let's
say you want to process so let let's say
you're an order management system and
you want to process you want to do some
big data analysis on when do we exactly
order the most amount of roses so do you
guys know there are two days in a year
where we order the most amount of
flowers Valentine's Day and Mother's Day
okay and this has been proven by
research but it's not easy to do that
because that's the highest volume days
it's not you know that doesn't mean that
rest of the days there is literally no
volume it's just the highest volume days
so to crunch that much data you need
that much processing power and there is
only so much you can scale on one
machine so in in today's talk I'm gonna
show all the demos on my this you know
Alienware Windows machine but and you
will see how we will go through the
entire spark you know UI and everything
and you will see what I mean by what
each executor is what it's doing and how
when you end up pushing it to the
distributed you can imagine that when it
is pushed to the distributed world how
things should function so on top of this
parkour execution engine you have the
SPARC sequel so these are basically in
integral SPARC components like spark
sequel spark streaming em live which is
for machine learning and graphics so
sort of doing a bigger zoomin of the
spark core engine so at the base of any
spark process is something called as the
RDD rd d stands for resilient
distributed data sets and as the word
suggests they are resilient they're
distributed and their data sets okay the
reason why they are resilient is because
they are immutable if you do any
operation and RDD you end up getting an
RDD back you never mutate that are DD
they're distributed because they can
actually be shared across clusters and
their datasets because you can actually
put in any sort of data in that r DD and
it will hold it you can put in a string
you can put in an iterable you can put
in a long you can you can literally put
in anything that you want in that r DD
and it will hold fine okay so now with
in the recent releases with what sparks
started doing
they figured out that hey you know we we
need to make it easier for users to you
work with the RTD so they came with the
concepts of data frames and data sets
what data frames and data sets
essentially mean is a data set is
nothing but like your database table
okay if you look at it in the old sequel
world or not I wouldn't say old in a
sequel world these are nothing but
tables where you have a specific schema
you have columns you have rows and you
have data pertaining into it and that's
where SPARC sequel comes in where you
can actually do sequel like funk sequel
like operations on those data sets then
bling deviates and approximate SQL it's
an interactive shell where you can just
go on typing out sequel commands and you
can you know you can end up doing
whatever you do with your sequel then
spark streaming what ends up happening
is there many a times you want to sort
of stream your processes you want to be
in like a micro back situation the best
example for that is let's say you're a
Java one we are tweeting right if I want
to do some big data analysis on the
tweets I should not wait for an hour
then gather all the treat gather all the
tweets and then post back the results
right rather I want to do it at real
time I want to do it in micro batches
and for that spark gives you something
called as a spark streaming API okay you
we have to understand this like in this
big data world even spawning of the
clusters it's it is time consuming and
it's resource intensive as well so
that's why when when we want to use
spark streaming we have to be slightly
cognizant of whether we really need it
or not
okay then M Lib machine learning I'm
going to be honest with you I've not
played with it a plated and at all so I
can't talk a lot about the EM Lib aspect
of it
same thing goes for graphics and spark
are so the next this is last of my
slides
and we and this is probably the most
important slide and we'll end up
spending about ten minutes on this one
slide alone so we the whole base of this
any spark program is your driver driver
note your master node and your worker
node and each of these are called
because they serve that particular
purpose though the driver program is
what sets something called as the spark
context you need to set your program you
you need to set your your process to be
okay this is this is my spark she'll I'm
gonna be running this and then you have
to register that driver program to owe
something to something called as the
cluster manager and the cluster manager
is its entire job is to delegate okay
the reason why it's called as a the
cluster master not manager though but
it's the whole purpose of a cluster
master is to actually figure out which
which how many worker nodes it has to
whenever the driver asks for any
resources it's going to pick and give
that resource to the driver program and
then it's going to run on it okay so
generally in the way it goes the way you
actually want to start your processes
you first want to start your cluster
manager or your cluster master so to say
and once you start that you have to
register each and every worker node okay
to that master and then you start your
driver node so again if if you look at
it at a distributed system and if you
seek all of Col deze as each and every
machine you're going to have one worker
machine you're going to have the other
worker machine you're gonna have the
master machine and you're gonna have the
driver machine okay now nothing runs on
the master other than delegation nothing
runs on the driver other than act
triggering the process all your basic
computing ends up happening on the
worker nodes okay do you guys follow
what I'm saying I'm gonna repeat this
because it's a slightly important you
know concept it took me a bit to
actually understand we on what system my
process is running when in when I was
working a distributed system and trying
to pass the logs but spark has also
helped us with that and we'll look at
that UI all right so going back again
you're the the whole role of the cluster
manager is to figure out which resources
it has in terms of the worker nodes the
whole purpose of the worker nodes is to
actually run the process which the
driver has centered with the available
resources the whole purpose of the
driver is to actually set the spark
context when I say spark context is it's
considerate similar to saying that hey
this is what I'm doing with this part
like basic configurations and all of
those things okay and then to trigger
the program if it's a Java if it's a
Java jar or if it's a if it's a scholar
program or if it is of the spark shell
it's what the driver program is going to
do so in a real world what ends up
happening is you bring up the master
then you bring up the workers and you
end up registering the workers to the
master and we will be doing this live
today so I pray to all the demo gods
that it actually works so then you
register each and every worker to the
master and then you after that you start
the driver saying that hey this is my
master before you before you do anything
before you ask for any resources sorry
if you need to ask for any resources go
to the master get that resource and then
push that process on that on that worker
okay I'm gonna actually pause here for a
couple of minutes if anybody has
questions and then continue over here
okay no question either data or test
okay
so the question was are these on the
separate machines or can they be on the
same machine so I'm gonna have it on the
exact same machine and what that
happened in in production or in the real
world you will end up putting them on
separate machines and that's where the
power of yarn or Apache missiles or for
that matter sparks stand-alone cluster
manager comes is where they understand
how to synchronize all of these
processes and that's a nice segue to the
next point okay so now in in the world
of distributed computing what do you
think is the most difficult task to do
persistence okay that's one what's the
other the most difficult task to do is
actually to aggregate your process and
when I say to aggregate your process now
imagine we we are going to do the we are
not going to do the word count you know
example but we I downloaded online
dictionary and we are going to actually
do some group bys and all of that okay
on those words so they're like about how
many our words which are in the English
language we are going to do we are going
to use them and do some queries of it
okay now I will have to work or notes
okay and one master node and one driver
node now when the driver note says that
hey
group by this data actually group buys
the wrong example let me let me go back
and say that hey just select out of this
data or you know transform this data
let's say add an a before each and every
word okay now the power what the driver
does is it says hey I have a it goes to
the master it says I have one process
which I want to run how many resources
you have master says okay take these two
then the now now comes the problem
because the driver doesn't know where
the process has to go ideally it should
go on both levels it should go on both
the clusters okay so it says okay fine
I'll push it on both clusters now once
the process is done then what right how
you gonna get the data back so that
becomes an interesting problem and
that's where the power of you know spark
or Apache my sauce or Yan comes in
because these frameworks inherently the
the entire job of these frameworks is to
manage this distributed computing aspect
where they say okay fine these clusters
are working on XYZ on one process they
are going to have X Y Z results I'm
gonna wait for them to complete their
process and then aggregate it okay so
today while I'm doing the demo you will
see that when I output something to a
text file you or output something as a
JSON it ends up creating multiple files
and those multiple files are actually
out of each and every of those worker
nodes because what they do is now since
I have to already print it out as a file
I'll print it out as multiple files and
you can actually use these files to
persist the data you can push them to a
graph database you can probably go
through one file at a time and then get
it into some Oh a warehouse database
like sybase IQ for that matter or even
mem sequel or you know since these are
just JSON objects you can even push them
to Mongo okay so that's where that's
that's the main critical aspect of
having or using spark where hey you know
I have this process I spawned it off it
is on each and every cluster and now on
this cluster what exactly am I going to
do fine I've done this process now what
do I do it says okay hey driver I
finished this process this stage is over
and and you will see what I mean by each
stage and everything ok so any questions
on the on the manager and the worker
relationship ok so now the next part
well the what are why do you even need
rdd's ok arteries are resilient
distributed datasets now in this in this
distributed world if I need to send data
from
one cluster to the other cluster or from
you know one process to the other one
step to the other step okay when you
make these immutable you are guaranteed
that hey whichever step you pick it up
from it hasn't changed because at the
end of the day the RDD that you have
been working on
that's immutable no one can go and
change it if you do a transform moment
on it you will get a new ID if you do a
select on it you'll get a new ID if you
do a group buy on it basically you do
any operation on it you get a new entity
and another sort of a beautiful thing
about spark is spark will try to keep
everything in memory as much as possible
okay like you
I mean sometimes I'm dealing with these
huge data sets like upwards of a million
rows or something like that and about
hundred columns okay now that does when
when you explode that that is a huge
data set for one machine to handle okay
so it gets distributed and then even
that distributor or like the smaller
chunks of them what spark tries to do is
it will say that hey I am going to keep
as much in the memory as possible
because it's faster for processing and
that's where spark Trump's over
MapReduce algorithms because or for that
matter MapReduce because the fundamental
concept of MapReduce is if you want to
go from step one to step two you need to
possess that step okay once you possess
that step then step two can actually
pull it up and read it from there if you
persist F three step four can read it
and pull it from there
however with spark with the rdd's and
that whole concept it can just go on
pushing out rdd's to you know different
different processes okay if there are no
questions I'm going to move to a live
demo okay and at the end of the slides I
do have all the references which I've
used I mean again I'm not going to say
it was 100 percent you know I knew what
suddenly I took up spark and I
understood all of that but
these references do come a lot in handy
so okay sorry I do you guys need it up
there more okay I know my system is
responding
sorry I apologize okay
to be honest the docks of Apache spark
are not that bad sometimes it just takes
getting used to it but I've gone through
the docks and you when it actually works
you'll be like oh that's what they're
trying to say okay good so the each
worker node will end up returning either
an RDD or a data set or it can you can
make them persist to a file and we'll
see that right now yeah I questioned I'm
actually doing well on time yeah yes
we'll see that so what ends up happening
is and I literally have no idea why this
is happening we had tested this before
once again let me repeat her question
and then so her question was is there a
way to dynamically scale out is there a
way we can dynamically add more workers
to the master and then speed up the
process so when before the start of the
process where as the driver is starting
the program is when it is going to reach
out to the master and get the workers
and then it gets then it distributes the
process however if you want if you end
up adding more workers the next process
which is there that will end up getting
assigned to those workers say I'm saying
but you can definitely add more workers
to the master and we can add them right
now like during the live demo and I can
show it to you how it looks okay there's
one more question at the back
so that's actually a very good question
where so the question was is there a
situation where workers talk to each
other so in my experience have not done
that in in my day-to-day you know work
or my experience with spark so far
however what I can definitely tell is
what you have to do so that you don't
have to have two workers talk to each
other so like operations like group I
where if basically a group by or group
by key are nothing but you know you have
a stream of data and you're trying to
group by something specific okay at that
time what about what spark does
internally is it reshuffles the whole
RDD such that each worker has that same
key so the whole problem of two
different workers talking to each other
completely eliminates you see I'm saying
and then generally if you need two
workers to talk to each other or if you
need worker one to push data to worker
to you you end up persisting that and
then the worker two picks up from that
persistence okay but you try to avoid
that as much as possible because the
power of spark is when you're doing a
lot of in memory computing okay are
there any more questions
so is this visible at the back I know I
asked this before but always like to
confirm okay so installing spark is if
you're using Windows it's a bit of a
daunting task so I'm not gonna lie about
it and that's because it's not natively
supported with Windows so once you
download spark you can see I've
downloaded spark 2.2 it's with Hadoop
2.7 so you don't necessarily need Hadoop
to be running on your system to actually
use spark you spark and bring it bring
up its own you know small sort of
cluster to get you up and running so
don't worry about hey okay so do I need
my own HDFS system or something like
that
it's part because if we go back to the
presentation the base of the whole
system is your distributed file system
right so that's your local file system
or your you know HDS HDFS but you don't
have to essentially worry about it
because Park will sort of take care of
for you while you're getting started and
they're getting started talk so I did
not bother about setting a Hadoop
cluster in one of my advanced talks I do
end up setting up a cluster and do stuff
with it but going back to the point of
installing SPARC you can once you
download spark from from the Apache
website if you're on Linux or the Mac
it's very easy because it's pretty much
you know you download you want R it and
off you go
it will do everything for you if you're
on Windows since there is no need of
support you have to sort of add a
workaround you have to add the win you
toast or exe from another like one of
one of the Hadoop maintainer ziz github
account and that's how once you give all
the necessary folders permissions you
are good to go
and it's very well documented on one of
the references so feel free to take a
look ok
so with that now we are going to work
right now for the first about five to
seven minutes on what exactly it means
by a driver node or in setting a spark
context so because you can always turn
off the driver node turn on the master
and then assign the driver to the master
okay so this sorry
so if you see I've got this woods alpha
dot txt so this is nothing but it does
at a-to-z words out in a text file okay
what I'm essentially going to do in
spark is I'm going to first of all start
the spark shell and spark is mainly
based on Scala so whenever you do a
spark - shell it ends up bringing up its
own shell and you can see like you can
ignore most of the warnings but you can
pretty much look at what's happening
here it hey I can't load the native
Hadoop library so I'm just going to fall
back on the other implementation and I'm
gonna bring up my own how to cluster so
to say okay and if you see here HTTP
this this is the URL pretty much where
you can go on your localhost and this is
a spark UI so I'm actually zoom in okay
so this this is the spark you this comes
native live it's part I have not written
it okay and this is going to help us
because when you go look at the event
timeline so it says executor driver
added because this is the driver note
have not started a master or a cluster
and we'll we'll see how it changes okay
so now here again the this is a scholar
shell so the basic rules of Scala apply
and you're first gonna say you all of us
saw the word file we are going to end up
getting it as an input file so well
input file equal to SC SC stands for
spark context if you end up running it
natively it automatically sets it for
you from /bin but if you want to add
additional configurations like
overriding a few specific configs like
how many processes to attach etc
etcetera you can definitely do that okay
SC dot txt file and then I'm just going
to give the path of my file Spartan
slash
okay now till now what has happened is
the well input file it's nothing but an
RDD and I've been talking about rdd's
but now you actually see an oddity in
real life okay and you can see that it's
it will give you a few pointers and
everything but rdd's like spark by
default is lazy
until and unless I do do a terminating
operation it's not going to do anything
right now it says that oh yeah this is a
this is a text file I'm not going to
something is going to happen with it
eventually kind of thing okay and if you
want to see more we can always do input
dot do so
generally what's going to happen with
you is when you're writing a long
program or like when you're trying to
dabble with it you won't know what
exactly is going on so that time you can
pretty much get it as a debug string
because again you have not got in the
the exact file as yet okay all you're
getting is pretty much the metadata so
that's what it's saying hey this is this
is of type string and it's a map
partitions RDD I'm not going to go in
the interest of time I'm not going to go
into partitioning I'm just going to show
it to you and it's actually a text file
and I'm at console line 24 okay now if
you going back here if you look at the
executors
you will see here there's one RDD block
okay
is is the bottom visible for all of you
at the back okay so you can see the the
status is active that's the IP address
and RTD blocks are one so basically I've
got one one oddity in in memory okay now
I can say that hey you know what take
the input file and cache it and what
caching does is it will keep everything
in the memory it will sort of this is
going to be helpful for you to pass
around those datasets okay then what
what if I want to do now more operations
on it right and I'm I'm going to go
through my sort of a cheat sheet which I
have is I'm going to show two things one
is how to parallelize operations because
now this is an input file which is just
an RTD so if I do a SE dot spa context
or parallelized and then input file it's
gonna fail because it's gonna say hey
this is not a sequence object you're
giving me an RDD and that sequence
object is pretty much I cannot you
cannot sorry it's an oddity it's just
one string it's one big blob of data you
cannot parallel eyes that so instead of
that what I'm going to do right now is
I'm going to take the input file and say
that hey for activate Wow for parallel
equal to
so now you can see I actually got the
whole input file in now this is what
this was a terminating operation this is
when actually spark read the file and it
ended up creating a list of strings now
that list is actually a sequence object
I can definitely paralyze that so I can
just go that Wow
okay
again all you get is an RDD everything
is lazy okay it's not evaluated as yet
all it knows is okay fine I need to do
something with this but for now I'm just
gonna wait on it okay now with this as
parallel I can actually like I said
before let's do a group by let's do some
operations on it so I can say as
parallel and I want to know I want to
basically get an RDD where he which is
the first name sorry according to the
starting letter okay so I can do word
and again this has given me an RDD back
okay so you can see stage one contains a
task of very large size blah blah blah
the maximum recommended size is 100 KP
because I'm only on one node okay so for
that one node it's saying that okay
don't don't exceed it I will still do it
but don't exceed it okay that was one
then let's say now you want to find out
how many letters are in E okay
I'm gonna go back and say input file
here to show that that has still not
changed I did operations on an input
file it is immutable that's never going
to change
okay so now I can do input file dot say
filter because I want to get only the
ones which are starting with letter A
word
okay
nothing has happened I'm gonna do
sorry
okay so there are 25,000 417 words
starting with the letter e yeah I'd done
double quotes instead of single quotes
sorry okay so that was pretty much a
brief overview of what the driver note
is now in the remaining eight minutes
I'm gonna show the master and this and
oh and the worker concept and then how
SPARC ends up you know distributing your
work across it so for that I do have to
exit the driver okay all right now at
reopened all of them all the windows
beforehand but you'll get everything
when you download spark with you and you
can just go again this works much better
with Linux or Mac where because you have
something called as the master class and
the worker class so you can basically
just trigger it but for me on Windows I
have to fall back to command-line
arguments apart
so
you can see in you you can see now it's
saying that hey now I'm the master and
at the end you will see I have been
elected leader new state alive okay
that's the master okay now if you go go
here you're actually not going to see
very many things because your driver has
still not come up okay and driver is
what controls that local host okay but
in this in in in these logs you can see
that where the master is running the
master is running at this host and you
need this host or you need this address
because that's where you're going to
deploy your workers so it's going to be
against part - class
okay so I'm saying that deploy the
worker node and that's the master for it
to register so and you do that you can
see that successfully connected to the
master okay that's pretty much it and
I'm gonna start a worker thread in a bit
because I want to show you how it works
okay it's going to be a normal thing
like spark shell which I've done before
but now this time I have to supply a
separate argument which is - master and
then the the address of the master
oh I find
so I can exact same process which
happened before to bring up you're sort
of the local cluster and everything now
this becomes a driver and you will see
pretty much a similar output as you had
seen before but it ends up saying that
hey spark context is available as I see
and the master as this previously would
not have seen that okay
oh sorry
okay now you can see in the event
timeline that you added executor zero
okay we have still not added executor
one
now let's go add that
okay
now if i refresh this you will see
executor 1 added so you can go on adding
executors as on how you like and in the
last three minutes what I'm gonna do is
I'm gonna end up I've another file in my
in my desk data where I basically had
countries and country codes because I
want to show you
Oh a cool feature with respect to sparks
equal because fine you have our DDS and
all of these things but if you want to
do like you know sequel like features
you have sparks equal and believe it or
not it ends up being useful more often
than you can think so
spark gave a nice API to say that hey if
you want to read a file you can just go
sorry if you want to read a file you can
just do spark dot read okay and then
this they start to build a pattern out
there where they say okay what are the
what are your options the option is I
have a header so header is true then the
other option is what is the what is the
cap set its UTF for utf-8 sorry and then
do you have a delimiter okay yes so
delimiter and that's a pipe and then
what's the CSV that you want to read out
of so CSV
Oh
okay so it ended up reading and you can
see that it is now this is a data frame
so previously it was giving an RDD now
it's a data frame and data frame is a
spark SQL concept and you can end up
saying that hey data dot print schema
and now it understands I this has three
columns one is the country code one is
the region and one is the table well
yeah it's a table name so I just added a
random column out there and with that
you can do data or show what data do you
have okay so since I've got like about
twenty twenty six thousand countries in
there it's only showing the top 20 rows
but you can do all of this now if you
want to do real sequel like operations
you can end up doing data dot group I
and then you say so here's the
interesting thing when you're doing a
group by you are trying to do it by
let's say region so you want to do
region count okay
now that comes that that gets out as a
data frame again spark is lazy until
unless I call or terminate operation
nothing is going to happen
sure so did you guys see it flicker
where it was Stage five stage nine and
everything and now this is a region to
the count and everything now if I go
back to the spark UI and refresh this
you can actually see the stages which
were added and the completed jobs so you
will get a lot of metadata out here with
if you go to stages inside you will
actually get to know what exactly
happened see there was a shuffle read
then it wrote because it wrote when I
did a dot show and if you go deeper in
then you can get more metrics out of it
but this is pretty much what I had for
today's talk if there are any questions
I'll take them offline if that's fine
you I'll be around don't forget to vote
and if you guys think that it like it's
a thumbs down I'm fine you giving that
but just please tell me what I need to
improve
so that it will help me in my next talk
all right</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>