<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning in Action | Coder Coacher - Coaching Coders</title><meta content="Machine Learning in Action - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Machine Learning in Action</b></h2><h5 class="post__date">2018-02-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/dezkUQUHuRU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome everybody looks like my mic is
on so my presentation is going to be
very long and now that it's been cutting
a bit shorter we will have a problem I
will be speaking very fast no seriously
I try to cut it down further the
schedule here so when I left home the
home is in Finland
I had 22 123 slides plus 2 demos and I
was thinking 45 minutes can't do it now
I have only 93 slides so let's start
running you're talking about machine
learning today and we will see a little
bit how machine learning can be used if
I only get my clicker work no what's
wrong with you No okay who cares the
whole computer is stuck not just this
one great
wonderful it's stuck oh this is gonna be
one slide show let's see if it works now
yes Wow luckily I'm technically enough
for days so my name is Helle and as I
said I come from Finland in North it's
pretty cold there now it's 25 minus
Celsius which I can't even imagine how
much it's in Fahrenheit it's very cold I
have graduated from University of
Helsinki I have a master's degree at the
moment from computer science but I'm
currently working on my doctoral studies
and that's one of the reasons I'm
talking about machine learning because
one of my research topic is very very
close related to machine learning and I
find machine learning very exciting you
will notice during my session I am very
excited about the topic anything to do
with databases big data multimodal
databases methods and tools for
utilizing semi structured data for
decision making other topics for my
research I also do some lecturing at the
University anything related to databases
is usually my topic I've been working
with Oracle products since 1993 and I've
been working on IT since 1990 and since
I'm a good parent I have my elder son
here attending the conference he's
studying computer science at the
University yes looks like the younger
one will not follow my steps but at
least the elder one is anything to do
with data and databases if I didn't make
it clear enough it's very close to my
heart I'm also a CEO for a consulting
company in Finland called miracle I'm an
ace director ambassador before amia
usergroups i was listed as the one of
the top influencers in IT in finland
three years in a row I speak and I write
books and two of my books here the first
one about database designing and the
second one about sequel and PLC coli I
co-authored with four other ace
directors
the latter book is something that is
quite interesting for this topic as well
I will refer to that couple of times
during my session I mentioned I'm an ace
director so this is more about the
program itself we have people here that
can give you more guidance on the
program
you are interested or you can come to
talk to me if you're interested about
the ACE program let's go to the topic
what is machine learning well it's an
important part of artificial
intelligence and machine learning
teaches computers to learn from
experience which in the land which is
algorithms so they learn from data and
they make predictions
it's about mathematics it's about
statistics it's a lot it's about a lot
of different things so it's not just one
tiny little thing it's a very broad area
it's a field of study that keeps
computers the ability to learn without
being explicitly programmed said said
mr. Arthur Samuel in 1959 so if you
think machine learning is something
invented last year you couldn't be more
wrong so it was invented a long time
before even I was born so very long
thing it's a systematic study of
algorithms and systems that improve
their knowledge or performance with
experience so two different things but
why are we talking machine learning now
1959 and 2018 it's long gap because of
improved technology we are able to do
things that he was only imagining 1959
because of the new technology the price
of storage solutions is so much much
cheaper than it was in those days and
also because now we live in an
environment that needs machine learning
and we are finally able to use it one of
those reasons that tell us we need to
have machine learning is big data but
what is this big data in the first place
everybody is talking about it but it's
very not easy to understand well it's
not the size I hate this word big data
because the first thing that you think
is that it's a data that is size of this
and then it's big data it's not only
about the size it always depends on our
capabilities the data could be big when
your traditional processing with your
traditional tools is not enough anymore
maybe it's the amount of data
maybe it's the complexity of data maybe
some other reasons you cannot open the
attachment in your
email or you're not able to edit a photo
in the way you usually do you might be
hitting big data usually when we talk
about big data we will give you three
weeks volume the size the scale of the
data the velocity which means the speed
of change the analysis well the data is
streaming and also variety which means
that we have different formats of data
sources different forms of data we have
structured data semi structured
unstructured data so already these three
V's give us a lot of headache but
there's more
there's also Verity which means the
uncertainty of the data the data is
worthless or even harmful and it's maybe
not accurate so in our old relational
world we know our data we know this is
really a customer this is really a true
invoice this customer has not paid the
invoice this customer has paid the
invoice but with the velocity we have
data coming from different sources and
we can't rely it all completely then we
have viability which means that we need
to validate that hypothesis before
taking further actions like you could
say well when Haley arrives to Los
Angeles it starts raining because every
time I have a right to Los Angeles this
is my first time it's raining yesterday
was raining but is that really true the
computer might say yes it's true because
every time she arrives it's raining but
we have to validate it if it's really
true no it could be sunny when she comes
let's hope next time it is also the
value different data has different
values and that value can change
something is very important to my
business today and tomorrow is worthless
or some some data is not important today
cat video it's not important to my
business except next week when I decide
I want to I want to contact all the
customers who love cats
then I need my cat videos I need to see
who's watching cat videos I need to
attach one cat video to make them sure
that I also love cats so buy from me I'm
a cat person
also the variety it refers to date house
meaning is constant
changing like if you have a phrase you
have a word in the middle of the
sentence and you put not in front of it
it's changing it completely or the same
word in different contexts means
something different so depending where
that data lies in which context its
meaning can be different and last but
not least visualization because we need
to be supporting people who are not
technical and we need to be supporting
people who like to see pictures because
picture tells more than 1000 words so
visualization is very very important
part of big data but because of all
these fees we have a lot of challenges
in big data we have more and more data
the volume different data models
different data formats variety loading
in progress while the data exploration
is already going on the velocity not all
data is reliable porosity it can be even
harmful it can be a virus and we do not
know what we are looking for so we have
a lot of data and we try to find some
kind of knowledge from that data and
that's when machine learning hits we
also must support non-technical people
journalists investors politicians and we
need to have visualization for them but
all this must be done efficiently and
fast and because people can't do it
efficiently and fast because it's so
much data we need to have computers we
need to have the machines and we need to
have the machine learning so when you
use machine learning first sentence here
you need to have data
it sounds very stupid but if you don't
have data machine learning has no
business here so you really need to have
data to be able to use machine learning
and you have to remember that part of
the data is used to find the right model
and part to prove that the model works
so you can't use all the data to find
the model you have to save some of the
data to prove that the model really
works you this rules and equations those
are very complex like image recognition
machine learning might be the right
solution oh you have constantly changing
variables like fraud detection like
what's fashionable
today these kind of things are changing
very fast if you try to program it in
the old-fashioned way you can't do it so
we really need machine learning in these
kind of cases so the nature of data
changes and the program must adapt
today's spam might be tomorrow's ham and
the program must understand it like
predicting shopping trends my younger
one is very good with that so I can't
follow it it's too fast for me
what trend is in and what is out so
there are some couple of words I want to
explain before we go to the real thing
the task is the problem to be solved
with machine learning and it's very
important to define the task well what
am I trying to solve what are we trying
to find what I what am i what am I going
to do with machine learning that is the
task machine learning is only a company
is not only a computational subject or
task it's about also human thinking what
what do I want to achieve is something
that we should think and not let not
maybe like the computer do it it's all
about algorithms the experience for the
computer to learn with is about
algorithms it learns with algorithms and
data and these algorithms produce the
model so we learn with experience
computers with algorithms and data it's
not easy to find the right algorithm I
know at least two of you will come after
my session and tell me a problem and ask
me what is the right algorithm for this
problem I tell you already
I don't know so it is a task that will
take trial and error couple of times
before you find the right algorithm I
will demo a little bit about sequel
developer which will help you a little
bit with this problem but of course it's
not solving it completely but finding
the right algorithm is a task for you
which algorithm the selection of the
algorithm depends for instance the size
and type of data the inside you want to
get from that data or whose how those
insights will be used these are just
some examples that will affect but it's
always a trade-off between a lot of
things it's a trade-off
predictive accuracy speed of training
memory usage transparency
interpretability a lot of different
things so depending what is important to
you that will affect the choice of
algorithms the model so the model is the
output of machine learning the task is
addressed by this model there are
different models there are predictive
models descriptive models and
prescriptive models predictive is giving
a forecast of the future descriptive is
telling what happened when Haley arrived
it was raining and predictive
prescriptive models are the most complex
complex complex ones they are a
predictive model plus actionable data
and a feedback system to track the
outcome so that's the most complex one
of those all about machine learning I
already said all these other important
and every time I give you a new word I
say this is the most important features
are the most important thing features
some people call them dimensions an
individual measurable property or
characteristics of phenomena being
observed says crystal bishop in 2006 in
his book deriving features is one of the
most important parts of machine learning
it turns data into information that the
machine learning algorithm can use a
model is only as good as its features if
you have chosen wrong features the wrong
amount of features whatever
your model is not perfect it's also
about interaction between features in
short what I just told you is that you
use the right features with right
algorithm to build the right model that
achieves the right tasks that's what
machine learning is about in very very
short there are two types of methods or
techniques in machine learning you could
say there are plenty others as well but
these are typically the classical two
ones to two choices unsupervised
learning and supervised learning
unsupervised learning is finding hidden
patterns
or intrinsic structures in an input data
Wireless supervised learning is training
a model on unknown on unknown impact
input and output data to predict future
so the unsupervised is about unlabeled
data I have a lot of data which I don't
know what it is and I should find
something from that data and that's
where machine learning is helping me
it's typically used when you don't have
a specific goal you are not sure what
information the data contains or you
want to reduce those features of your
data as pre-processing for supervised
learning this could be a dates are for
unsupervised learning I have no idea
what it is I see there's lax that might
be your Airport or it could mean someone
in Swedish because that's lacks so I
really don't know what it is but my
computer knows even less so from this
data the computer will find something
and such as to me I found these things
when Haley arrives to Los Angeles it's
always raining and then my job is to see
if that's really a valid hypothesis or
not so this is what unsupervised
learning is about data I have but I
don't know what the data is about for
unsupervised learning that the technique
we usually use is clustering it's the
most common method there are two ways of
doing clustering hard clustering and
soft clustering in hard clustering each
data point belongs to only one cluster
Wireless in soft clustering the same
data point can belong to different
clusters more than one cluster hard
clustering has different kind of
algorithms I just put some here if
you're interested you can go and find
more information about them k-means and
km edits are probably very typical
examples and I also put the hierarchical
clustering here because sometimes we
need hierarchies to be able to find the
knowledge from the data but there's
about two billion other algorithms so
don't worry this is not all
the soft clustering only one one or more
clusters for one data point fuzzy see
means go see a mixture model some
examples then we go to the supervised
learning it's learning from known and
labeled data so I already know what I
got but I should teach my computer to
understand it better so we are training
a model with known input and output data
to predict future outputs so we'll go to
the demo is about this so you will know
about it so the same data we had earlier
but now it's supervised because now we
have the labels for the data now we know
the lacs actually means airport so now
the computer could be able to find more
than it was before and I could
understand it better than I did before
in supervised learning the first thing
is training we load the data we
pre-process the data this is very
important it had to be pre-process the
data before you really start using it we
learn using the method and the algorithm
and we create a model and we iterate it
until we are happy with the model then
we do the prediction we use new data we
pre process it we use the model we get
predictions and if we happy with it we
integrate it if not we'll continue
processing it I have a very simple demo
here about chatbots
let's see how my computer reacts now so
I hope you can see some things a little
bit tiny this is the chat bot you know
jackpots you have your phone and you
just order pizza with your facebook
Messenger so that is the jackpot I did
order pizza by the way I get wrong pizza
so what is this chat board was not
perfect so the jackpot is very easy to
build it's very easy to implement to
your phone but because of the risk of
not being able to connect everything I
didn't take my phone here I just demo it
from here so my chat bot now I'll put
this here has funk
it has balance dispute send money Trax
pending transactions so all kind of
functionality is that my chat bot can do
I also have oops I also have entities
that my functionalities can use I have
account types to account from account
address all kind of entities and for
these entities I can define values like
to my my account type could be savings
checking credit card and my transaction
type could be deposit withdrawals and so
on then I have the real program here
which is not actually much because most
of this is done by machine learning the
let's go here let's take the balance as
an example here so for training I can
use just linguistic training or I can
use some of the algorithms but I have
found linguistic being enough for this
so I have given my balance functionality
some examples that whenever somebody is
asking these I'm actually referring to
this piece of code so if we try it let's
say hmm tell me my balance let's test it
oops
so now I'm testing it so it's telling me
I would go to balance this functionality
because I'm hundred percent confident
that that's the right one okay let's see
what you will do if I ask you this again
sorry yeah it doesn't know what is
balance actually I can I can show you
that first why doesn't it know what is
balance
what a simple question but for my
program it's 50/50 should I was he
asking about balances or send money I'm
not quite sure so let's see I will ask
here again what is
oops
I have a new computer and I can tell you
they don't all have the same same place
for the keys okay so I just said what is
my balance it's asking me savings
checking or credit card I said savings
and I get my balance but if I just ask
balance it's telling me I have no idea
what you're asking me so I want to solve
it because this is a very common
question people always ask about balance
let's ask I first saw the intent so
remember it was like this before now I
asked about balance and because of the
new hint I gave to my computer it now
knows that when whenever she's asking
about balance
she's talking about the functionality of
balances so if I now ask about balance
oops it knows what I'm talking about
so so easy to just teach the computer so
check both are very simple example they
have some workshops handle workshops
here and there's a MOOC in Oracle
somewhere I encourage you to go and take
them they are very interested in
exciting and I'm sure you can find a
nice use case for your company to really
use these chat BOTS because they are
very very interesting but let's go back
to the presentation so earlier I
mentioned that supervised learning has
to has different techniques one of them
was predictive models there's two kind
of predictive models classification and
regression classification is where we
train the model to classify the data
based on categories like in this
previous example is it about balance or
is it about putting money from my
account to your account so which of the
functionalities is it about so this kind
of categories it can predict responses
like an email is genuine no spam spam
filter is a very class
example of this kind of machine learning
because they are always different you
can't have a program that knows when
something is spam and when it's not spam
that's why you need to have machine
learning they are changing all the time
or if the tumor is small medium size so
large tumor is cancerous opening person
is great mercy or not so we put things
in categories two three four categories
so for examples like applications
medical image imaging speech recognition
credit scoring this kind of things so
can the data be attacked or categorized
can it be separated into specific groups
or classes if so it might be a
classification problem some algorithms
for that all kind of decision trees k
nearest neighbor naívi base is very
classical example or neural networks
which is very popular nowadays it was
kind of banned for a while because
people were thinking this is crazy
science-fiction but now it is reality
and this is happening a lot another way
of doing it is regression too pretty
continuous responses like changes in
temperature fluctuation in electricity
demand kind of numbers then we are
talking about regression like
forecasting stock prices handwriting
recognition acoustic signal processing
failure prediction in Hardware
electricity load forecasting this kind
of examples is about regression plenty
of algorithms for regression as well
like linear regression or non linear
regression or course the unprocess
regression model but whatever we do with
machine learning it's always
approximation it is not the truth so if
machine learning says that you have a
gene that says that you might have a
cancer
don't start crying immediately because
this is just approximation it's not a
fact then go and see the doctor and make
sure that you are healthy so this is
just machine machines it's not a real
fact
so whatever the machine learning is
given it can be quite true or it can be
complete
wrong sometimes it can be even useful so
depending but what we are doing here we
are searching for patterns and trends we
are trying to be very accurate with our
predictions and there's several models
we choose the best but whatever we
choose it's still approximation no
correct answers for anything so where
can we use this machine learning spam
filters I mentioned earlier lock filters
and lock alarms data analytics image
recognition speech recognition medical
diagnosis robotics fraud protection
online shopping like Amazon you know
Amazon is doing searches recommendations
it recommended me my own book by the way
so they have something of course I
bought it immediately but but still
voice-to-text changes and from that too
smart personal assistance like give me a
recipe for bread find the nearest
grocery this kind of things we have
theory we have Google assistant Alexa
echo Cortana this new coming everyday so
this is quite cool people like to voice
control things I'm still waiting for the
one that understands finish because it's
not easy but we'll see Facebook is one
of my favorite examples and not only
because I'm quite sure many of you are
using Facebook so you know know what I'm
talking about so I took Facebook as an
example I put some references here
because I don't work for Facebook store
everything I tell you is from these
references and I find it very exciting
already as the user I found it in very
interesting but when I went to see what
they've been doing I was thinking I
definitely need to share this with you
because this is exciting
so Facebook's mission is give people the
power to build community and bring the
world closer together that already tells
you we're talking about huge amount of
data and this could definitely be done
without machine learning they connected
more than 2 billion people as of
December 2070 that is a lot of data the
massive amount of data requires machine
learning services
and there's a lot of pressure for their
data centers they have more than ten
dataset and centers around the globe and
this gives a lot of pressure to those
data centers they are using several
techniques to really manage that and
networking optimization is not the
easiest task here I can I'm very sure
you already you know what I'm talking
about because most of us are having
problems with network every now and then
so one of those problems is that huge
amount of data processing that data
efficiently and frequently enough to be
able to really serve the customers and
then handing all the ten plus data
centers hmm by the way what I really
enjoyed was that they mentioned that
disaster recovery planning is very
essential for them because in the new
world people quite often forget this
kind of things like backups and recovery
and disaster recovery and that kind of
things because there's so much data how
do I do it anyway but Facebook mentioned
that specifically they said we really
need to have a disaster recovery
planning because all this work we do to
understand our data must be saved
properly we can't do it again we need to
have it somewhere and they are already
actively evaluating and prototyping new
hardware solutions and still doing some
game-changing algorithm and innovations
at the same time so quite exciting I'm
sure they have a big group working on on
this kind of things so in in Facebook
they used machine learning for they
products these are some of the products
newsfeed ranking ads Search Sigma LUMO
spacer language translation and speech
recognition let's see a bit more closely
what they do the newsfeed the machine
learning is used for ranking and
personalizing newsfeed stories you know
you don't see the same stories as
somebody else so it's somehow
personalized filtering out offensive
content this is something I wasn't
thinking at all but of course it should
be they're highlighting trending topics
yeah I know I get all kind of old to
play these things on my feet which I'm
quite annoyed I should get those 25
things but no no
and ranking those search results so the
search result is found this is what
she's interested in but let's put him in
this order because this is the most
exciting for her and this is something
that she might read if she has enough
time so all this should be done for me
in the newsfeed have you ever thought
about it you just open your Facebook and
you start reading and don't think more
maybe now you will think more they do a
lot so first they have the general model
that is trained to determine determine
various user and environmental factors
and using that they use the same model
for my personal data to find the right
output for me so the general model my
data will give my output so I will get
my newsfeed interesting things in the
right order for me personally think
about this amount of data exciting then
the ads I know we are all annoyed
annoyed about this I really hate those
ads in Facebook but I understand it
because this is how they get money so
but online advertising allows
advertisers to only bid and pay for
measurable user responses such as clicks
on ads that's quite efficient for the
company that is adding advertising here
but as the consequences those click
prediction systems are very central
because you have to get as much money as
possible and it's not very easy to know
where will she click and where will she
not click but they use the same thing
here as they did with news feeds they
have the general model and then they
used my data with the general model to
know what ads would be interesting for
me and hoping that I will really click
them and they will get more money
predicting clicks it has to be robust
and adaptive and capable of learning
from massive volumes of data in facebook
they actually use a model which is a
combination of decision trees and
logistic regression so they have
combined two models to be able to do
this efficiently they say that combining
these two things
they say 3% of their resources so it
will be more efficient than just using
1/3 sounds very small to me but when I
think about all this data
I'm sure 3% is a lot so yeah this is
probably working well based on the
experience the most important thing is
to have the right features those that
are capturing historical information
about the user or ad and the right model
the right model is this a decision tree
with logistic regression but finding the
right features is very important and for
this particular use case they have
defined the measures to be the accuracy
of prediction so those clicks should be
almost 100 percent sure when I put this
out there she will definitely click it
so they want to be sure they don't give
me things that I'm not interested in so
this is how it happens they have the
tree first and each of these three
individual trees will have I don't know
this is not working so I can't show but
this is anyway how it works the
classifiers will will be built for each
of those trees then to search it
launches the series of distinct and
specialized sub searches so it's not
just one search one search that has
several searches under it and all these
searches will go and eject various
verticals like videos photos people
events all kind of things and the
classifier layer is looking after the
whole thing the classifier and the
search verticals consists of offline
stage to train the model an online stage
to run the model and perform
classification and the search itself
then there's a thing called Sigma it's a
class it's a general classification and
animal detection framework that is used
for variety of internal applications
site integrity spam detection payments
registration unauthorized employee
access access and event recommendations
plenty of things where they use this
Sigma platform or framework it includes
hundreds of distinct models running in
production every day
so they must be very well tuned so that
they can let them run in production all
the time then Lumos which is very
interesting they extract high-level
attributes and embeddings from image and
its content this data can be used as
input to another product one of those
that we already talked about or services
as if we have a picture an image it will
be translated to text as an input to
something else quite interesting and the
phaser I'm sure you have already noticed
the Facebook is trying to tell you this
is probably your face this is probably
your husbands or wife's face this is the
functionality so phaser is Facebook face
detection and recognition framework when
you give an image to this product
product phaser it finds all the faces in
the image that's number one step and
then it runs to use a specific facial
recognition algorithm to determine the
like hood that some of these faces
belongs to your top and friends who have
enabled face recognition in facebook so
they can search for somebody who's not
enabled this functionality and they will
suggest to you this is probably your
wife this is probably your husband it's
quite interesting language translation I
don't know if you see that but I do see
it often because my native language is
finished and most of my feeds are in
English so it's trying to translate
those and it's very annoying because it
can't translate them so it's but I'm
sure it will be better so now I have
told the tool not to translate anything
because it's ridiculous sometimes I want
to entertain me and then I just juice
translator and it's very funny
so this is a service that manages
internationalization of Facebook content
it's supposed translation for more than
45 languages so it actually supports
more than 2,000 translation directions
if you think about from Finnish to
English English to and so on so if you
put all those 45 languages and the
combinations together it actually comes
to 2000 translation
it's a lot and it serves four point five
be translated post impressions every day
that is a lot
so each language pair Direction has its
own model currently which is of course
not the best solution for machine
learning and they are working on multi
model multi language models in the
future but currently they are all
separate models speech recognition even
more interesting than just translating
things it converts audio streams into
text and it from a provides automated
captioning of videos mostly streams are
English but other languages will come in
future and I am really waiting for the
Finnish one it will be so funny so
additionally also no language audios are
you using the same thing a little bit
more simple model these are the
algorithms they are using I don't go to
details with them but you can just see
actually not too many it's five they are
managing with five different algorithms
I'm sure they do tests and so on with
others as well but these are the one in
production but how do they do this in
Facebook anyway this is so complex they
have their own platform it's called FP
learner platform it has the data it has
the features training that will build
the model which will give the
predictions the features are handled by
FP learn a feature store which will give
them to the training which is handled by
the FP learner flow when the model is
built it's used with the FP learner
predictor that's how it works so they
have the whole platform for it the
feature store is a catalog for all the
features they have which is very great
because the as I said the model is as
good as its features and finding the
right features is not easy so if you
have a catalog of usable features that
will help a lot the flow is platform for
model training
single pipeline defined with FB learn
flow and the entry point for the machine
learning task so the task comes to the
flow the flow handles it and I don't go
to details here because of the time and
it comes to the prediction the FB
learner predictor is Facebook internal
in inference engine that uses the model
trained in this flow to provide
predictions in real time then they also
have a frameworks for deep learning they
use PI torch which is optimized for
research very fast changing things
trying to find the right model and so on
and then they use cafe to which is their
own product and it's used in production
because it's optimal for production PI
Taj is a framework that is flexible easy
to debug dynamic neural networks so it's
very usable for trying to find the best
way of handling the data and finding the
model Wireless Cafe is not that flexible
but it's very performing cross-platform
form support and it's supporting all
those algorithms that they are using in
facebook but the problem here now is
that if you have two platforms where you
build if you have the one that you train
and you need to move it to the
production you need to code it again and
that's a lot of work and for that they
use this onx open neural network
exchange tool it's an evolving tool it
helps you to automatically change from
one platform to another I don't go to
details because of the time but anyway
it's very interesting and there's a
group of companies working on that to
make it more powerful the SoCs success
factors here are predictable good
quality data using storage network CPUs
they also use GPUs and knowing when you
need to improve something so
understanding what you are measuring to
be able to improve your measure
but how do you know when to tune how
does them how does the model perform on
the data which of the models is the best
what should I do you have to know what
you are measuring what is important to
you to really have the kind of model
that is useful in your environment and
in your use case you can measure
different things plenty of different
things but maybe one or two are very
important to you and rest of them are
not important at all so why would you
improve your models to increase the
occurrence accuracy or productivity to
increase the ability to recognize data
from noise to increase the performance
to improve the measures whatever you
just chose some of the typical
techniques are feature engineering or
hyper parameter tuning in facebook they
said we noticed that the largest
improvements in accuracy often came from
quick experiments feature engineering
and model tuning rather than applying
fundamentally different algorithms which
is a relief because as I said finding
the right algorithm is quite hard so
let's see about sequel develop I will do
it very quickly because of the stupid
time I have so much to tell you and I'm
so I'm using just the HR that we have in
an Oracle database already and there's
this table in sir cost l LTV sample
which has customers here and one of them
one of the columns is buy insurance
which says if this customer has bought
an insurance or not the data in the
table is something like this so some of
the customers has bought in these are no
no insurance no insurance and then
there's somewhere somebody who has
insurance so this is where I start my
demo I already build it here so we don't
have to to worry about it so I have the
data source actually I will show the
slide first so you know what we are
talking about so we have the data source
here which has example input data and
known output data we know who are the
customers who live in
by and we know the customers who did buy
insurance we verify the data and we try
to understand our data better then we
find the right model we train it and we
test it we use that model with new input
data we apply the data and the model
model and we get those customers that we
should contact because they are very
likely to buy insurance from us so in
the in our demo this is the table I
mentioned earlier nothing else just the
table and I have told my my tool to
analyze whatever I have here it has
analyzed to me the age so the red is no
insurance and the yellow is yes
insurance I can see H has some kind of
effect
how about bank funds Wow
Bank fund has quite a lot of effect if I
don't have any money I probably don't
want to have insurance either this is
the one who has less money what about
car thus car ownership effect a little
bit yes
checking amount credit balance so I can
here see how different data from this
particular how different features effect
if this person has an insurance or
doesn't have insurance
so now I know more about my data I'm
ready to build my my my model this tool
is very fun because when you choose I
took classification because I'm
classifying no yes no yes for
classification it already has four
different algorithms it's using I have
chosen here three these are my
algorithms and I have told the tool that
lets use 40% of the rows for testing so
60% you can use for building in the
models and 40% for telling me if it
really works
and let's see compare test results I
have already executed all this so it
won't take time so I have
three different algorithms and it's
showing me what happened with each of
them so I can compare accuracy different
ways I can compare the cost
I can see performance metrics for each
of them I can see the RLC I can see the
lift different kind of lifts and I can
see the profit so I can choose which one
is the one that suits me the best so I
don't have to know these algorithms I
can tell the tool analyze me and tell me
which algorithm is the best after seeing
this very carefully I decided that for
me the best one is this decision tree
because it worked best in my use case so
I told the tool don't use the other ones
only use this decision tree then I took
another set of data here I only took
those that doesn't have insurance yet
and I said I asked the tool using the
model using the knowledge these people
don't have insurance yet who should I
contact and try to sell the insurance
because I don't want to call them all so
tell me the most potential customers and
I can start calling them and trying to
sell my insurance so it analyzes the
data using the model and tells me this
customer is definitely no I tell you 80%
sure no voice calling this customer is
actually 70% sure for you to call this
is the probability 70% sure I would say
call this customer I will do it and
because I wanted to save this data
somewhere I actually created a table
that will store a state so very simple
model and with this tool I only click
and create things here I don't have to
know anything about programming anything
about algorithms I only need to know
about my data so this is one of the
things you might want to do when you
start working with machine learning so
it's about sickle developer what I was
showing you is the sequel dev
it has this Data Miner add-on which you
can install there the problem here is
that it's using advanced analytics which
is licensed products so make sure if
you're using it in production you have
your license is done if you want to
start testing it it's only in Enterprise
Edition and if you want to try the you
know we have these cloud trails here for
free if you want to use that for this
make sure you have either the database
service high performance package or
extreme performance package because
those will include advanced analytics
and here's instructions how you start
with it and the tutorial about it if you
are more interested about it in our book
chapter 10 talks about this and also
Brendon wrote another book about data
miner I think data miner is a very good
starting point if you want to know about
machine learning
another thing is our enterprise which is
also in the Oracle database advanced
analytic option payable option again if
you find our interesting start with that
if it's something familiar to you
also chapter 11 or Brendon's another
book and then predictive queries in an
Oracle database also quite useful they
can be built using this data miner I
just showed you so all the things that
we built here is in a database it can be
used as predictive queries or you can
you can just write them using sequel in
our book chapter 12 but if any of the
other languages is more like you Python
C C++ Java JavaScript all these have
their own libraries for machine learning
so if you want to start with machine
learning I would recommend to start with
the technology you know the best because
this is completely new
so start something you already know and
add machine learning to that so if you
are mastering Python start with Python
and forget what I talked about sequel
developer but if none of these are your
thing data may be data miner would be a
good option but whatever you do the
future is here and now artificial
intelligence
and machine learning are here and they
will be here in the future and there are
so many interesting areas in machine
learning so I really encourage you to
start learning because this is
definitely worth it
so time for machine learning is now
because of big data because it's used
everywhere because Facebook is using it
everywhere because because we need
machine learning but remember it's about
approximation it is not the truth
remember this is just computers remember
this unsupervised learning a supervised
learning predictive models are
classification and regression mostly
improving models is important when you
finally create it you need to improve it
so it will be the best possible for your
use case feature engineering hyper
parameter tuning are usually the ways to
do it you have to know what you are
measuring because otherwise you don't
know if your model is good enough and
you have to understand what your
measures mean but there's so much to
learn about machine learning so start
learning now thank you so much for your
attention</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>