<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Modularity in the Cloud: A Case Study | Coder Coacher - Coaching Coders</title><meta content="Modularity in the Cloud: A Case Study - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Modularity in the Cloud: A Case Study</b></h2><h5 class="post__date">2015-06-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/oJPw_bXz2Ac" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome everybody to this session about
modularity in the cloud a case study
I'll start by briefly introducing myself
my name is Marcelo for months I work at
a company called luminous I'm the
director of luminous technologies which
is sort of the product branch of the
company we're about 120 people big and
I'm also a member at Apache at the
apache software foundation involved in
several Apache projects there my twitter
handle in case you want to contact me
and my co-presenter Paul ample I'm an
architect illuminates technologies and
I'm also architect on the system that
we're going to use as a case study for
today I've been working on that for the
past few years and doing modularity with
oci for that time at all and i'm the
author of book that that was just
published for a for riley that will show
later also about this abouts topic and
just to prove to everybody that we're
not from oracle we have our own safe
harbor slide so read that enjoy and now
we'll start with the presentation all
right so just to give you a little bit
of an ID what kind of systems we are
talking about in this presentation I
just want to give you a little bit of an
overview of the system that we are
talking about we're not going to go re
in depth in this because it's more about
the technology that's very using not so
much about the product itself but just
to give you an idea what it is it's nice
to know so the system if you are working
on the skull pilsen and it's an sister
focused on education so what we're
trying to achieve is changing education
actually and give students a more
personalized learning experience in the
netherlands where we're from it's quite
a big problem that well we are still
teaching students the same ways that you
get 100 years ago and that's actually
all over the world and does not really
effective so we try to do to make that
better and we have some software to help
with that if you look at software this
is the desperate you would see if you
log in as first time these are all kind
of gadgets which are different parts of
tail too
estimates you can go into it is
interesting thing about this is that
everything runs on a different kind of
tablets and well any kind of defy
secretary so it runs all in the browser
and so based on html5 and in the
lighting of course we have the modular
ogi based system for that so let's look
into that a little bit more if you just
look at the technology layers that
you're using and you could say that at
the top to use html5 JavaScript to get
well across the Phi's browser support
there's also kind of the only way at
this moment to get well across the five
systems working because there's not
really any other kind of technology
where that actually works well with our
experience will be doing an html5
JavaScript are actually pretty good it's
it all works including swipes and stuff
like that on tablets I would not go as
far as saying that there's no problems
at all because it's still like like a
difficult world to work with that many
different browsers it's very pretty
happy about it others are clients that
use you use HTML they connect to the
back end using restful webservices not
really something something new a lot of
people are doing that but it's really
nice separation between what we do on
the front end everything happens there
on the device itself and after that our
back-end just have to handle all the
data the rest will say restful web
services themselves and also all the
services around those restful web
services ROG I components so everything
is based on osgi services that means we
have to deal with your dynamics and and
all that kind of stuff and that's what
we are going to talk about today
especially about how you're going to
deploy that for a large large system and
enter everything we use apache felix's
SRO see our container and that means we
don't really use any kind of application
server or something like that it's just
bare bones Felix and we install our shop
windows directly in that and it works
really well actually means that we can
just have those
we actually need without having all the
overhead of an application server
especially on the management side of
that have followed over at of course we
are talking about cloud applications and
we do need things like well have some
some kind of support for creating
restful web services or communicate with
the Mongo database for example or work
with s three and two to do those kind of
things we use done down to framework and
I'm now to framework and it's an easy
project with it's all Apache licensed
and there we have the whole set of our
GI components that offer that kind of
functionality so we have components to
communicate the Mongo we've components
to do remoting with other services we
have support for Jack's arrest all those
kind of more high-level enterprise like
AP ice those are part of a motto the
idea of an that you is that you just
have well components so if you just want
to use the Mongo component that's all
you that's all you need there's no such
thing as a core platform or something
like that but if you're looking for more
cloud like AP ice then allowed to the
place that you should take a look at at
least
if or you're taking out um I'm fine
taking rats yeah she we rehearsed this
and every time we do this person somehow
we always mess up soon so just to give
you some numbers about this pulse on
application it's about 190 components
baked so that's quite a few especially
if you have to install all of them by
hand and out of those components about
120 we really developed ourselves the
restive stuff that we reuse from
existing components so it's it's a
reasonably sized application I mean if
you look for example at eclipse that's a
couple of hundred of bundles so it's
even bigger but it's it's you see the
bundles are fairly fine grained and
that's what you usually end up with when
you start to develop modular
applications so with all these bundles
the next problem is how can we deploy
them on to all these systems because we
have many different schools and not all
of them are exactly the same so it's not
just a matter of taking all these
components wrapping them up into one big
war file or whatever and just shipping
them to a couple of servers we really
need to be able to create different
types of installations for different
schools and the first thing I want to
look into a little bit further is the
general deployment topology that we use
for schools starting at the top we have
a load balancer and we always deploy a
load balancer just to make sure that
there's always some accessible point for
for people to to go to and this is what
all the tablets and other devices
connect to this is where all the rest
endpoints are and beneath the load
balancer we have a couple of notes
compute nodes as Amazon calls them these
are the actual virtual machines running
java and
it's where the actual pearls on software
is deployed that's where all the bundles
are and as you can see for each school
we currently have a different load
balancer and a different set of puzzle
nodes the amount of notes depends a
little bit on the scale of the school
and how much traffic there is but we
deploy all of this in the cloud so
that's also for schools a big difference
they were used to having their own IT
department and their own servers and
we're no longer doing that we're
deploying everything in the cloud so
it's always accessible from everywhere
and below these compute nodes we have a
cluster of mongo databases that's what
we use for most of the persistence we
have a little bit of blob stores as well
but most of the data we actually store
in Mongo and we're pretty happy with
Mongo because we actually store quite a
lot of data about students because we
want to give them a personalized
experience which means we need to track
what they're doing quite well to see how
they're responding to pages to questions
etc so we're generating a lot of profile
information and we can do that in a very
flexible way with with Mongo so every
school might be a little bit different
because they want certain components
that other schools don't have they have
their own ideas about how exactly to do
education configuration wise is always a
little bit different each school might
have their own single sign on server etc
so we end up with lots of different
deployments for different schools there
yep
we'll get in the question was do we
deploy this bundle by bundle or do we
have kind of features we'll get to that
a little bit later in the presentation I
think Paul is taking that there unless
we're switching again but he'll say just
do to expand a little bit on this to
make the application more available we
actually deploy nodes in different
availability zones that's a term that
Amazon uses for this but other cloud
providers have a similar concept so we
make sure that we we deploy notes in
different zones to make it better
available and that's the basic
deployment apology so when you're
looking at this this picture you might
think okay that's interesting so how do
you do stuff like keeping States
replicating sessions etc and the answer
is for our horizontal scalability we
basically have an application that's
completely stateless that means we don't
need to do complicated session
replication etc which makes it easier to
do the scaling but you have to keep
state some wearing applications so
there's basically two locations where we
do keep state at the moment the first is
actually in the user interface itself so
with HTML Javascript you can actually do
a lot of state and keep a lot of state
in the actual browser so for everything
that's user interface related and that
shouldn't necessarily be persisted we
keep the state in the browser so that
saves us a lot of of state and anything
that does need to be persisted goes all
the way down to the database so
basically we're saying in between we're
not keeping State anymore it's either in
the user interface or it's in a database
but no longer in between and for scaling
things that makes things a lot easier
so talking about scaling if you think
about the school they have very typical
usage patterns I mean when school is in
everybody's opening up the tablets and
watching videos and browsing lessons etc
and as soon as school is out people go
and play outside right if you might do
their homework but there's a lot less
usage so what we're actually doing is
during school hours we actually make
sure that we have more notes available
to keep the performance up and when
school goes out we scale that down that
keeps our bill down as well so that's
nice and so looking at at at the cluster
that we have per school like I said we
always have a load balancer and at night
we only deploy one small note to handle
any traffic that might be there outside
of school hours and in the early morning
well it's pretty predictable we actually
scheduled because we know exactly when
the school starts so just before they
start we schedule a couple of extra
nodes so we're ready for the load during
normal school hours and at the end of
the day we scale down again so that that
way we end up with only a small note
again so that's one of our mechanisms
the other is that the load balancer
actually also measures the amount of
traffic and the response times on the
different calls so if that becomes too
much we also add extra notes dynamically
because of load and every now and then
we also see one of the virtual machines
in Amazon failing for some reason so in
in those cases that machine will get
automatically replaced by a new one so
that's that's basically how did the load
balancing works for this application
on to the next question Paul so how does
this actually work and it should answer
your question as well so of course the
auto scaling and elasticity that's one
of their like their core IDs why you
should go to the to the cloud a lot of
people have been talking about this new
in a past few years and elasticity is
always like one of the top features
that's why you should go to the cloud
the question is always then okay so sure
we want to do that but can we do it
completely out of middle out alto magic
magically or what kind of medical
mechanism do we use to actually install
our software on the on the servers so
instead of looking at one of the
existing past providers that are out
there there are not really any goods for
the situations as we are we're in there
too limited in the things that we can do
with it and it don't really support the
golden modular approach that you want to
have instead our approach is based on
apache aids and appreciations
provisioning server so the idea that we
have here at apache ace is that which is
a central component that holds all our
artifacts so let's say we create a
create a set of bundles we have those
two hundred bundles that composers pulse
on and we upload those two Apache ace
that's that's the first step in this
picture it says that we will do this
from the development machine that's one
option but in in real life we do that
completely automated automated script
from the build server will go into that
a little bit later so we just upload all
our artifacts to apache lace and the
bundles just see there the next thing
that should happen is that we have a set
of servers and a set of targets as we
call them that will register to apache
age and say well here i am i'm a new
note for school x please give me so fair
and by that apache age will make a
deployment package send that to the
target and a target will start running
all those bundles that's the basic
mechanism that we that we use and here
it says
those targets are surfers that are
actually cloud notes running on ms one
in our case but it could just as well be
small devices for example even embedded
devices and we actually do use it for
those situations as well the nice case
the nice thing about this setup is that
if you want to add new servers we don't
have to well manually install those
surfers you can just start up a new note
that note value medically register group
at you ace and I automatically get all
the software necessary the other nice
thing is that if you want to do an
update on our show fair so for example
we do a hotfix on one of the bundles in
a pulse on release we just upload a
single bundle to apache ace and a swell
then proficient it to all the surfers
and because it's allowed automated it
doesn't really matter how many servers
we have it doesn't matter if you have a
single server first cool or maybe 50
servers it just all works automatically
so we don't have to have two very
worried about those servers if you look
at Apache ace and you just download and
install it this is probably first thing
you're going to caesars the dep user
interface well it is nice to play around
with a little bit if you're just getting
started with it or nice in the
development environment but besides that
we don't actually use it that much we
prefer to use the rest api and you shell
shell scripts to talk to that and we
will go into more detail there so going
into a little bit more detail about how
the provisioning actually works the
first step is to actually start new
notice because we notice in at eight
o'clock in the morning for example we
want to start new notes and that's four
o'clock in f3 we want to show them down
again we need some kind of mechanism
that actually does that for us well that
the bad point about this is that there's
no like common mechanism for this that
multiple cloud providers are using even
if you're using a cloud API like Jake
clouds there's not really support for
auto scaling in there
because all the cloud providers do this
in a slightly different way most cloud
providers do so duo for something to do
that better still all proprietary so we
run on an emerson at the moment and
emerson s surface called ms on auto
scaling and emerson out of scaling you
can configure a cluster just co-creation
you're doing on the command line for
example now talking to the amazon
services I mean you define a cluster you
can define the size of the cluster so
you can say well I want to have a
cluster with a minimum of two machine to
two machines and a maximum of 10
machines for example and you can define
how kind of rules that say well at eight
o'clock in the morning i want to have
five machines and at four o'clock and
afternoon i want to scale down again the
two machines but if if load gets higher
than then a certain percentage of cpu
time for example i want to add new notes
as well this is all amazon specific it's
very simple to use but it's it's ms on
specific the only thing it does is just
well for example at eight o'clock in the
morning start some new notes with those
notes just run well basically an empty
linux image that's what we use and then
amazon layer called armies so those
linux images don't really contain any
software yet and it can't contain come
containing software because you don't
want to put all our bundles on there
because if it would then do an update of
one of those bundles we would have to
create a completely new image which is a
lot more time consuming than then we
want so we have basically Linux image
and the only thing that runs on there is
Apache Felix and within Apache Felix vfm
management agents in the management
agent but Adam automatically register
itself to Apache ace and the management
agent will then say Here I am please
give me some software and an Apache
hburg and I get all the bundles that
distribution needs and the target can
just install those and start working
when a new note starts it will also
automatically register itself to the
load balancer you could use your own
load balancers for that but to use the
Amazon server as a surface for load
balancers and the load balancer load
balancer will just start pulling the
target so it will ask are you there yet
are you there yet are you there yet oh
there you are and then it will start
serving request to that to that node so
once you set this up it works completely
automatic and that way we can just scale
up and down as often as we like the
other nice thing that we get rid is is
that it also gives us automated failover
because as soon as one of the notes Phil
and that sometimes happens sometimes
because the software failure but
sometimes because of a hardware failure
at some point we still run on Hardware
in amazon cloud of course and we had a
few times suggests well servers Phil
when it happens to load balancer relax
we see well this node is down it's not
healthy any more chance responding and
then have a look at the auto scaling
rules and we'll see well now you're
scaling rules to find that you want to
have minimum five servers at this time
of the day but now we only have four
servers left so let's start a new one at
about a minute later you have a healthy
cluster again because everything is
installed again and again that goes
completely automatic that the case is
not a nice option because instead of
worrying about installing those those
machines ourselves and doing maintenance
on those those machines and doing
maintenance on an application server for
example that you would do in a more
traditional approach we don't actually
care about those machines as soon as a
machine starts starts to be misbehaving
or we just want to update it for example
we just kill it we just shut down the
notes with just a hard terminate
terminate and the load balancer will
start a new machine in seconds anyway so
that way we have a lot less maintenance
than you would have been your bulb
having application server that you need
to maintain for as long as you are
running
to give you a little bit of an idea how
you actually can configure clusters this
is an example of a command line commands
for for amazon so this is ms on specific
again and it's just the way you work
with amazon you execute those commands
on your shell and you can see without
parameters you just configure what kind
of machine image you want to use you can
figure what kind of machine size you
want to have the amount of machine
security groups etc etc that's the only
thing you have to do in to define a new
cluster so if you have a new school that
we need need to start surfing you create
a cluster for it and then it will
connect to to Apache ace and that's
basically it those fishing gears a
little bit how do you actually deal with
software releases this is an interesting
topic because if you are in a modular
world does it still make sense to have
like huge release very say well this is
this is my version or do you want to
have versions for for single bundles for
example um this is of course not easy to
enter and there are different kind of
situations that you might want to deal
with or not want to deal with but the
way we deal with this is say well we do
want to define at least in our source
report story that we have a certain
version of pilsen in our case because
that's what testers are talking about
their testing a specific version and
they don't know about the small bundles
that we use to to actually do the
physical deployment so when we want to
do a new release we actually set a get
tack and that starts a built in the CI
server so we use bamboo for that and on
bamboo we just run a built of older of
all the bundles and when it completes it
actually gives us a directory with all
the bundles in there be during the build
however it does some extra tix it will
do what we call a baseline and
baselining is about a check
if coats or bundles are actually changed
since our less release so we do this
every release and by that we can exactly
see which bundles has what kind of
changes and that way we can exactly know
which set of bundles we actually need to
push to the servers to do an update and
that way we don't have to have huge bar
follow zip file or something like that
containing everything and push that to
the server you can do very small
incremental releases so after the base
lining is completed and it's all
successful because if it's not
successful it means well the ashram
changes but we forgot to update a
version I will go into that a little bit
later in a moment the built actually
fail and it will say well you have to
look at your versions because the
versions are just not correct some
bundle versions should have been bumps
but you didn't if it is successful
however if you publish the updated
bundles to what we call the release obr
an OB are you could say is just a place
where you can store those bundles and it
contains the first information of those
of those bundles if you do a new built
from a new from a new version in get it
will do the baselining against that
that's version so again and then we know
which burn also changed since the last
less release and if that's all
successful we actually have to go get
those bundles so we can install into the
surfers and for that we actually look at
a room configuration if you're familiar
with OC is specifically familiar with
bindi tools that's the idea that we use
you will recognize this and this is
basically a list of bundles sometimes
with a specific version or a version
wrench that that we want to use we use
this during development because you can
immediately start this from being d
tools and then it will just run locally
but we also parse this file to know what
should go into a deployment and they're
all
this is all supported by library so you
don't have to do the actual parsing
yourself this is quite easy to do
actually um the next thing is that we
have well all those bundles and we have
to upload them to Apache ace and apache
eh can put push them to the targets well
they're actually uploading to up at your
age we also don't want to do by hand
because you can make mistakes there so
actually the build server takes
distribution push that to ace using the
rest api and from there it all goes
automatically so we can basically start
a build wait for a while few minutes and
the avenue updated released without ever
taking any surfers down because there's
all hot deployments all the servers just
continue to run and they will they might
have some downtime during the update
process depending on what we're we're
updating but that's a few seconds
maximum
so just now I was talking about
baselining and baselining is all about
knowing what bundles have changed and
what bundles have not changed and even
more than that we want to know what kind
of changes there are are a major changes
which might not be a backwards
compatible or do we just have some hot
fixes on implementation codes we do want
to know that because that can change how
well dangerous deployment is for example
so the way that we know this is by using
semantic versioning and it is a very
important concept if you work with as
small small modules and do módulo
deployments as well so semantic first
thing is described in a white paper
written by the osgi alliance and that
describes basically how your versions or
what what the semantics are of a version
number and usually this is about package
versions if you're familiar with osgi
you know that we don't really care about
the new versions in general but we care
about package versions because that's
what's important by other pieces of code
if you look at if you look at all those
versions in general you have two major
minor micro notation you can use a
qualifier as a fourth part of the
version that is also used and recognized
web page for example but you don't have
to use that so a major version chance
means that it's backwards incompatible
that can for example be an interface
change so on edit or remove the method
from an interface and by that probably
going to break existing codes if that
happens you want to bump the major
version so that users of the of the of
the package or the bundle can actually
see what this is breaking these are
breaking changes try to well figure out
if that if it's still compatible with
existing coded I have if it's a backward
compatible change then you want
update a minor version but that's these
are still large chances so on a compiler
level that might be backwards compatible
but maybe on a more functional level
they are there or not or if you look at
small changes may be so much small UI
changes just from text for example or
some bug fixes we just want to update
the micro version so if you do this
correctly and yeah you exactly know from
the first number what kind of changes
you can expect and of course the
baselining helps here because you don't
actually have to well figure this out
completely yourself I when we do run a
built and a baselining works it will
tell you what's version and you should
use so if you have a backwards
compatible back backwards incompatible
chance and you didn't update the major
version it will actually not completely
built in it will throw an exception
saying what you need to update the major
version because they're backwards
incompatible changes this is all based
on byte code analysis so I've actually
look at your byte codes and it will
compare that byte code with the byte
codes from the from the previous
built-in obr release and that way you
can exactly know what what happens as I
said normally when we talk about
semantic versioning and osgi we're
talking about semantic versioning on
exported packages because those are the
persians that are most relevant when
you're developing and nunu bundles that
use existing code that you export it
when we talk about deployments then well
the export packages are not that
relevant because we are on a different
level there and you actually want to
talk about semantic versioning on
bundles we can apply exactly the same
semantics and what we normally do is
that we actually take the largest chains
on on all the package level and take
that that esta una versión to agree
have a major change on the package level
we use a major version bump on the
bundle bundle level as well
so again this all works automatically
using the base lining in the build you
can even use the baselining directly in
BD tool so if you're using beanie tools
and that way it actually will show up
errors directly you're in your editor
when you make changes on the codes and
didn't upgrade the version numbers yet
questions no it's a general concept the
only thing is and people have been
talking writing about it for a long time
already already it's just part of the
off the whole baselining ID the problem
is that without proper tooling it's
quite difficult to actually do it
because it's very easy to make go chains
and well either change the wrong part of
the version or make no changes at all so
you actually do need some tooling to
actually do that and that's added quite
recently in B&amp;amp;D a lot too but you can
using the built and directly an ID so
now it starts to be a lot more useful at
least and there is some support in the
clip speedy that's being used for
plug-in development they also have
baselining support and I think in Apache
arias they also have a very crude
version of that implemented right now so
tool support and it's getting better and
but I think at the moment B&amp;amp;N B&amp;amp;D tools
has the most advanced system for that
so what I've talked about just now is
when we do like official releases so
we're pushing pushing to either an
exception server or a production server
that's the way we do that but it would
be nice of course if you cannot do this
even more automated every time we build
maybe we want to have continuous
deployments maybe we want to have a test
show for that actually our test team is
looking at and every time we make a
change Jen's and push that to get we
want to have this version installed on
on that test server with the same
approach you can actually quite easily
do that but we need one more component
for that just now on the previous
picture that I showed we had only the
release OPR and release OPRS like
official obr with all the released
versions of our bundles when you're
still in development you don't want to
do official release of your bundles
every time because then you still don't
know the chances compared to production
but you want to create some kind of snap
short version so you upgrade your
version once and then you want to keep
on that version until you do the actual
actual official release to still start
working with Apache ace that uses the
versions to know what kind of bundles to
update on the on the test machine as
well we need this second obr which we
call the snapshot lbr so during a bill
that will first check the baselining
process against the release obr so if
you'll check the first chance compared
to the latest full release that we did
if that's successful we are not going to
push through the release all BR because
the only ones do that for official
releases we are pushing through the
snapshot obr that way apache ace can get
those bundles again and install them to
the continuous deployment server so it's
just a minor chance to the process that
I described before but this way we can
go to condense deployment and it works
quite well actually
and just just to drive the point home
about snapshot versions in maven because
probably most of you are familiar with
with that concept the problem that we
have with snapshot versions in maven is
that we don't really know if something
has changed in a snapshot you might just
declare something as a snapshot version
and it might still be the same bundle as
your last release and in an osgi world
that's a little bit of a problem because
if nothing has changed you really don't
want to bump the version so that's why
we're going through all this trouble
here to only bump the version if
something actually changed so we use a
concept that's a little bit similar and
I'll go into that a little bit more when
I explain into detail how we actually
deploy these these snapshots releases
and deploy onto testing service before I
dive into code here i'm just going to
explain a little bit more about how this
worked with apache ace paul briefly
mentioned before that we have a
scripting interface for ace so what's
actually happening is that on our build
server we have an ace client which talks
to be a server and this age client
execute a script and we do that at the
end of a continuous build so when
everything's working all is green tests
are good at that point we actually
launched this script and it will start
communicating with the ACE server and
through the script we can do lots of
different things the scripting language
that we use is actually the Go Go she'll
go go shell is an implementation of the
standardized scripting language for osgi
so unfortunately another language you
need to learn if you want to do that but
it's it's not that hard to learn
actually so I'll show you a little bit
of code just to get you familiarized
with with that so we run that as a
script and it will just interact with
ace and do everything we want to do in a
no
made it way just to go back before I
dive into the scripting to the question
where there was about updating bundles
and grouping them into features etc we
briefly show that in the user interface
that we can have different artifacts and
group them into features again group
those features into distributions so
that is actually the mechanism that we
use and why you can do that in different
ways it can be fairly straightforward
just say okay this is my feature it
consists of these bundles we can also do
a little bit more complex relations
between features and artifacts and sort
of create dynamic queries and say okay
if an artifact satisfies these
conditions then it's part of this
feature so it can become quite advanced
if you want to I'll show a little bit
about that in the in descriptive as well
so on through some real code and I hope
it's sort of readable for everybody if
not well come and sit closer this is
actually pretty close to the script that
we use to deploy snapshot in a
continuous build innate I left out some
of the details because they're not that
interesting for the story and the script
actually starts with getting some
environment variables and assigning them
to two new variables at the top and the
next thing it does is define a couple of
repositories Paul already set we have
different repositories snapshot release
repository etc and we want to be able to
talk to those reports stories from the
script so we need to define them some
might be existing repositories on some
server and we can also create a
repository just out of a set of bundles
for example the bundles that we just
built so we don't need to first upload
them to a repository to interact with
them so that's what we're actually doing
in the next four lines just making sure
that we have our repository is defined
so we can commute
kate with them and the next step
following the echo command is to
actually start doing the release
deployment of the snapshot deployment
for that we basically have one single
command those commands are prefixed with
a context so it's the repo dot column CD
command which does the deployments it
takes the three different repositories
of Paul mentioned before and basically
figures out all the stuff that we just
talked about so well you will end up
with a list of deployed artifacts and
those are only the artifacts that have
actually changed so anything that's not
changed will not be in this list will
not be in this deployed variable so
that's the first phase so we have our
artifacts they are uploaded to the
correct repository now and now we need
to actually start interacting with
Apache ace and start making sure that we
deploy these new artifacts to all the
targets that we have and for that we
start with creating a workspace to
explain a little bit about that Apache
ace works quite similar to how you work
with version control as a developer you
do a local check out of everything then
you can manipulate that local check out
and once you're satisfied with the
changes you can commit everything back
to the server so you can do everything
in one transaction so to speak so the
first thing we do is to actually create
a workspace that we can do this
manipulation with and that's what's
happening so love in the middle we
assign it to a variable called workspace
and then we can interact with it and the
first thing we want to do because this
server is continuously running is just
delete everything all the old artifacts
that were already there we're not that
interested in keeping a full history of
every build here so we're
just deleting everything that's there
and starting from scratch so that's
that's the next set of commands we list
all the artifacts in the workspace and
assign that to a variable called
artifacts and then there's a for each
loop which is actually called each in
this scripting language and for each
artifact we will actually delete that
artifact and the IT variable is the loop
variable so that's that's what we're
using there so that's how we delete all
the artifacts that are there and our
next step is to start creating all the
new artifacts and we still had them in a
collection called deployed so we're
going to iterate over that collection
again and well what happens in this loop
is a little bit more extensive first I
need to explain we have at least in this
pulse on deployment two types of
artifacts we have bundles and we have
configuration and configuration is
something that goes to the configuration
admin service in osgi and that's the
basic mechanism to configure components
in an osgi environment and if you look
at these configuration files they're
actually XML files and we even added a
feature in a so you can make templates
out of them so if you have configuration
that is the same for most of your
targets but has just a few differences
for each and we'll go into that a little
bit on the next slide you can just
create one template and have that
template filled in for each target that
you deploy it to but because we have two
types of artifacts in the loop we need
to well we first actually get some
information out of the artifact so we
get its its its name URL mime type and
some of those things and then we check
whether it's actually one of these
configuration files or whether it's a
bundle if it's a configuration file we
create a new artifact for it in apache
ace with the information we just extract
from the repository and if it's a bundle
while we use slightly different
information but the mechanism is still
the same we create that stuff and add it
to ace so that way we have all the new
artifacts in place and our next step is
then to actually create the other
metadata that ace needs so we have to
create a feature for that we need to add
that feature to a distribution and
finally make sure that the distribution
ends up on one or more testing targets
and this is a little bit of boring code
so to speak because in this case we
don't create many different features we
just have one set of bundles we group
them all in one feature Adam to one
distribution and that's what goes to the
targets because that's good enough for
this situation so what we do first is
create associations in ace you don't
only have artifacts and features but you
also have associations between you two
and there are also entities that you can
create they have a left hand and the
right hand side and they can either be
simple assignments or they can be
complex queries where you can have many
too many relations between the two so
you can as a programmer get pretty
creative with their with that stuff and
we're here just making sure that if this
association doesn't exist that we
created so we have an if construction
that tests for a condition it actually
tries to query if the association is
already there the query will result in a
list and we take the first item out of
this list and see if it exists so is if
there's at least one association that
has these characteristics then it's okay
then we don't need to create it if not
we simply create it and then move on we
do the same for the feature for the
Association to the distribution for the
distribution itself and then for the
association with with the target and
that one is even a
little bit simpler in the sense that we
say okay we have one distribution and
any target that we find in ace will get
this distribution so if we have lots of
targets they all get the same
distribution yep yeah one bundle can be
part of multiple feature and of multiple
distributions and in the end we just
grew up all of the bundles together and
if there's any duplicates in there we
just filter them out and in this example
you can just put everything into our
empty ace yeah yeah yeah the first time
it will create all the way yeah this
this crypt can be used really to
bootstrap with nothing and get
everything configured and once it's
configured it basically only throws away
all the old artifacts every time
replaces them with new ones and goes
from there yeah yeah so the last step
that we need to do is to actually create
a target to define a target we only have
one in this case again we first query if
it's already there if it's already there
we do nothing and if it's not we create
a new target we assume that this target
is running already somewhere in the
cloud so we're not actually starting up
a new cloud note here we're assuming
that this testing target is continuously
running somewhere either in the cloud or
within your organization and the last
step is we make sure that the target is
actually registered that's a nice thing
if it's not registered it won't get
software updates so we make sure that it
is and the last step is actually
configuring the target with a few target
specific properties and that goes back
to what I said earlier about these
configuration templates so if you have a
configuration template you can do a
variable substitution in in that
template actually uses Apache philosophy
below the cover so you can do a lot more
if you really want to but we use it for
basic property substitution and that
works quite nicely because then you can
usually create one configuration and
fill out different para moisés that way
so you can just set those on the target
and at that point you're completely done
so all that's left is just commit the
workspace back to the server so the
changes become active and we can go from
there so lots of code but hopefully not
to complicate it so that sort of
summarizes the whole story about pulse
on and how we currently deployed at how
we do continuous integration I also
wanted to spend a couple of minutes on
sort of an architectural slide after all
we're both architects so we love
pictures and this is a slide that
explains a little bit more about our
vision of cloud deployments and
automatic scaling in ambato and we're
not totally there yet this is still
something that's in development so we
might change it a little bit over time
but this is sort of what we're working
towards and well just following the
different numbers in the picture if we
want to do a cloud deployment we start
with actually making sure that we have
configured all the artifacts features
and the different deployments that are
possible so that's our first step in
making sure that that's all registered
in apache ace and then the second step
is we need some kind of definition of a
cluster and this definition of a cluster
tells us how many nodes of what type do
we need to run this application and it
might also contain some information some
cues about when to scale up and when
scale down certain nodes so this
information is fed into a cluster
management components and this component
starts to actually materialize the
cluster create the different nodes so
that's where we get with step three it
starts talking to some infrastructure as
a service and at this point it's really
our
to make that as abstract as possible so
we can run on any cloud that we want or
even deploy an application on multiple
clouds at the same time so it will start
some some notes on some clouds and that
will take a little while and it will
also once it gets back the information
about these just started notes feed and
information back in step 4 back to
apache ace because it might need such
information to further configure these
targets for example like we saw in the
previous slide actually get the IP
address of a node and maybe some ports
and other information and feed it into a
so we can use it in configuration
templates to automatically configure the
software on on such a node so that will
happen point 5 the node this action will
actually be we started up it will boot
strap it will get a Java VM it will get
the OSHA framework and it will then
start talking to Apache ace and get the
correct set of software and
configuration and then the whole thing
will be up and running from that point
on we start monitoring the system sort
of like the load balancer that we've
been talking about before and just
monitoring different aspects how busy is
the whole cluster how much data is going
back and forth etc and we're using that
information to actually do the automatic
scaling so if we see cpu load going
through the roof for a continuous amount
of time we will actually go back to the
cluster manager and try to get another
node of that same type to get the
lowdown again and the other way around
if it becomes too quiet we start
shooting down notes to cut costs a bit
so that's the overall picture that we're
working on question
that's it's a good question so what we
have different types of note this
picture shows osgi targets but also a
MongoDB or it might be all kinds of
different data store notes so one of the
OSGi is only one type of note in this
whole cluster we have many different
types of nodes that are not that
dynamically configured as always i but
that we still need to run actual
applications so the cluster manager will
also be able to deploy and start more
traditional notes maybe an amazon image
that contains a pre-configured mongo DB
note and make that part of the cluster
so that's something we're also still
working on a little bit because those
notes also probably need some
configuration data and we're still
figuring out what the best way is to get
that data to these notes but
unfortunately we're not living in a
world where everything is always GI or
maybe that's a fortunate thing but so we
have to deal with different types of
nodes in such a cluster question at the
back
she is the fact that so many schools
have so
that was one thing the other thing is
i've been using ogi for about 11 years
now i just like the concept in general
of building modular systems and having
all shy and forces that at runtime so
make sure that our design really gets
implemented in that way so to be honest
I built most systems in oci nowadays but
yeah those requirements that every
school is a little bit different that
was something that made us choose this
here as well yeah okay another question
back there so okay so when we use B and
D tools it automatically does do
analysis like Paul said the bytecode
analysis it will actually look at the
exported packages and see if you added
or removed any methods or new classes
etc at this point it will not
automatically update your metadata it
will generate warnings inside the IDE or
errors inside the Eid on the actual
version so you know that something's
wrong if you hover with your mouse on
top of that it will also show you a
tooltip explaining you what went wrong
but it will not actually make the change
that's something you need to do first I
think we're still working on or maybe
it's already in quick fixes do you can
quick recovery from what mostly most
errors so but there's still a little bit
of human interaction there so wrapping
is up I mean we've been talking a lot
about pulse ohm and I think this this is
an architecture that fits a lot of
modern web based cloud applications so
maybe you've recognized some of the
components that we've been using and
using them yourselves as well another
node to make is that although we're
using this in the clouds
you can also use it in private clouds or
you can use it outside of a cloud as
well it's just a mechanism to build and
deploy applications and you can use it
basically everywhere and an interesting
remark is at least in our opinion at the
moment if you look at the tooling in the
frameworks that are available I think
they're mature enough right now to build
applications like this we've been
working on this for the last I think two
years and with a reasonably sized team
and what we it's been pretty smooth for
us we've had a few hiccups but well
everybody using an IDE has some yep
question is it highly well not highly
and we have some dependencies at the
moment actually the only thing at the
depend on which is Amazon specific is
the Amazon out of scaling API that they
use the console commands to actually
configure your cluster if you look at
other competitors in a cloud space they
all have something similar like that so
you can get the same kind of model but
just using different API for different
ways for configuring it but I DS remain
remain the same so on the software level
we don't have anything specifically for
Amazon not question yeah
instead of having a set of DM first ever
looking at more of a Content 720 p.m.
maybe somehow tied into the way you call
that kind of trade-off nails
we did actually do that and in um data
we even have some components to do
multi-tenancy it was at the moment it
wasn't worth it for us to do
multi-tenancy because the knows that we
run on in amazon are not that expensive
and since every school has a different
deployment any way that makes
multi-tenancy a little bit harder as
well so at the moment the trade-off was
is it worth it for us to go the extra
mile and make the application
multi-tenant and at the moment that
wasn't interesting enough for us but
this is still an application that's
that's growing and we're getting more
and more schools that are involved so we
do envisioned that probably in a year or
two we will have a multi-tenant version
of this application as well and it the
way we have set it up it's going to be
pretty transparent to do anyway the code
we have can just take normal bundles and
make them into multi-tenant ones so once
the need is there i think we can switch
pretty quickly to a multi-tenant
environment I'm just quickly going to
wrap up the last few closing slides that
I had is we were sort of going into
questions during the talk as well just a
quick reminder of the undoubted website
if you want to get involved try out some
of the components there's a lot of
screencasts on there as well that
explain the various concepts and
components that we have and there's
information on how to join a mailing
list and actively participate I have to
plug Paul's book again because it was
just really just released you just held
the first copyright I just got on a
rally booth and hold a first paper
version I didn't even see it before so
far yeah there's a book signing tomorrow
there's some free copies if you're
really quick so that's something to to
go to tomorrow we have a few more
related sessions tomorrow and the day
after some more about the lessons that
we learn build building modular cloud
applications in Java
and a little bit different a talk on
modular JavaScript we're seeing more and
more JavaScript in our user interfaces
the need for modularity is there as well
so we've been doing some research in
that area and talk on that on Wednesday
just a few links for everybody just to
look up the technologies that we've been
talking about and a big thank you for
being here today</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>