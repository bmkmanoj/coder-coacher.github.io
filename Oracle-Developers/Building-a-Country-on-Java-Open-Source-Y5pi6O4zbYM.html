<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Building a Country on Java Open Source | Coder Coacher - Coaching Coders</title><meta content="Building a Country on Java Open Source - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Building a Country on Java Open Source</b></h2><h5 class="post__date">2015-06-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Y5pi6O4zbYM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to our presentation building a
country on open source my name is Jeff
contender and this is Johann and it's
from what we're here to talk about today
is how we helped introduce open source
and Java scalable architecture to a
country that was hell-bent on staying
with commercialized software and we're
going to talk about some of the history
of what a curb and then we're going to
talk about what we built our
architecture some code snippets that we
put in there and how we made us very
scalable solution this presentation is
all about scalable Java and scalable
open-source i'm going to try to rip
through the beginning here pretty
quickly for the history to kind of give
you an overview of what we've got
because i don't want to dwell on that
you're all here to kind of see the real
technical issues but i want to kind of
give a little bit history of the problem
so you all kind of know what's going on
and what we had to do so first of all
let me just introduce ourselves i'm jeff
contender this is yan it edstrom were
from the Denver area I do a lot of stuff
with Java this is my token marketing
page here where I'm founder of savoir
technologies we do a lot of java based
Silla consulting and in particular with
the Apache stack as you can see here I
do a lot of stuff with Apache but enough
about me that really doesn't matter too
much except that I do have a hobby as
for not being a geek I'm a firefighter
for my for my community up and evergreen
and so that's why we get get a lot of
fun and johan johan loves tattoos he's
got a good tattoo of himself on his
wife's arm this is snapshot of him doing
that come on guys help me out here he
also works for similar technologies he's
also he just offered a book and just to
let you know it's always GI starter a
lot of stuff we do is with osgi we have
several books we're going to give away
for good questions so if you give good
solid questions you might get yourself a
free book it's been
autographed by johan and the other
author Jamie who's sitting in the back
as well he's also a committer on a
couple of Apache projects and one
interesting thing Don geek stuff about
Yan is he's a foodie he is a great chef
he's a severe cook these are these are
pictures of some of the things that he
does follow him on Google+ you will not
believe the creations he makes he he
actually has been offered jobs as a chef
up from his Google+ posts I'm not
kidding you so you know it doesn't pay
well enough yeah that's how it works
okay so let's go from Denver let's fly
down to South America to a country
called Ecuador Ecuador is an awesome
country in South America he's got 16
million people or so they think they got
16 million sometimes they say there's 40
million they don't really know it's
somewhere in between but they're their
unofficial count is 16 million they're
based on the US dollar which makes it
easy when we go down there that's kind
of nice and it's also democratic nation
they have a president they kind of have
legislative body so it's kind of got a
similar setup to the United States
there's two major cities in Ecuador
there's Quito and guayaquil guayaquil is
the biggest one and Quito is the second
business biggest one kiitos up at 10,000
feet and it's just a beautiful place
something about Quito is they have
several many different government
entities that control the the IT there's
a simple registration police business
registration so there's there's 12
entities in in there's more than 12 but
these are the 12 that we worked with in
Quito and they all have different pieces
of information that track individuals
and businesses so what they wanted to do
is create what we call data seguro so
what is data seguro there's anybody here
speak Spanish what is data seguro hey
hey there we go secure data that's right
they wanted to create a system that
allowed access securely to the internal
infrastructure so everything in mex in
Ecuador is very similar through a lot of
nations of what's called a sedge Allah a
sedge Allah is like a social security
number so if you have a says if you want
a driver's license or something along
those lines you need to have your social
security number or in Ecuador sedge
Allah and once you get a sedge Allah you
get one of these you get a really cool
card with your schedule a number on
there this is julian assange Ecuadorian
you know he's still in London he's this
is his card I don't think that's a
scheduling number that was a joke come
on guys but but this this is the same
thing this is the equivalent of a social
security card you need to get you need
to use this to be able to get driver's
license and that sort of stuff so what's
the goal the goal of the government was
to take all of these different entities
which are based upon sedulous everything
it's kind of like a business has a
federal tax ID number and we have social
security numbers at the end of the day
they're all nine digit numbers they
identify either business or a person
said you'll is the same thing so these
entities kind of represent an individual
or a business what they want to do is
make it so they're so their citizens or
ciudadanos I think I see pronounce that
correctly have access to these to these
products so to speak so give you a
little bit of history what's going on
rafael correa he is the he is the
president of ecuador and in 2007 he
produced a message to the people saying
that he wants everything to go on open
source so he put it up on youtube they
broadcasted on the TV and say we want
open source because that's going to be
where we want to go and interesting
enough I think you want you had a
comment about maybe had foresight of
wanting to go to open source because of
well we've had a lot of announcements
around and is a back doors everything
else and throughout South America
there's been a big push to get away from
vendors get into open source solutions
and if you look at places like Brazil
they will leave and push open source in
the university's pushing that out so
that you actually have branches where
people only do open source try to get
into it
to learn code and they're seeing
benefits from major benefits because
they can focus the money on developers
and skilled resources as opposed to
licensing so maybe they knew something
that we didn't know at the time at any
rate let's go back oh I gotta move this
there we go so basically what he said is
he said basically he said that they want
to be the producers of technology and
not simple consumers they want to own
the source code they wanted to go into
open source and that bottom part he
really says there and I'll read it
because this really hits home for a lot
of South America is that for everyone
must use free software the Ecuadorian
government has already established this
is a governmental and state policy this
will be an important step in the
integration of why not say the
liberation of Latin America they saw
what Brazil did anyone hear from Brazil
nobody they saw what Brazil did and they
said we want to go that direction so
what they did is they created the system
called data seguro you can actually go
there on the on the web data seguro that
go bc but the question was did they
trust open source well to be fully
honest nobody got fired for buying
microsoft they went out and they built
this whole thing on microsoft so they
built a version 1 dot 0 and what they
did was they created the initial version
of data seguro which was a web net
application and it was connected to all
these different entities over the net
via soap web services did they have
problems of course they had problems
first we have the elephant in the room
you know they were told to go to open
source but they didn't so they went with
Microsoft and everybody wanted to use
Microsoft but everyone had to be quiet
and say we want to go towards open
source but that didn't really happen so
another problem he had was licensing
licentia problem is that costs big bucks
that cost them a lot of money to license
all that Microsoft product that was out
there sequel server the.net stuff all
that stuff was costing money and then
they had inseguridad or insecurity so to
speak
in the what hit the front page in el
comercio back in 2012 was a citizen was
arrested and this is directly converted
from Spanish a figure most you don't
speak Spanish so it looks a little funky
but basically citizen was arrested for
doing a data breach on on the on this
data seguro system he was able to act as
the president get a schedule and pull
all this information out president
caught wind of it and he finally said
hey let this guy out and put all my data
out there because this is this is how
insecure it is when we're using these
products I want it to be secure the
other problem they had was the
architecture the architecture is
basically a web-based system net and it
was point-to-point to soap web services
on the back end the problem was it was
point-to-point there was no message
guarantees there was no failover kappab
capabilities the infrastructure was
completely unstable it wasn't modular at
all they weren't able to deploy on
demand when they needed it so they ran
into a lot of problems I mean they would
absolutely go to these website and they
tried to get their data and they
couldn't get it nothing would come up so
they needed to escape Stables a scalable
solution and the first thing they
started looking at was open source okay
look we already told the people we want
to open source we need to go for open
source what did that really mean for
Microsoft that meant by by Microsoft
it's going away and they said let's go
to Java they saw Brazil Brazil bit
bought the farm on Java they did well
with it they said there's a lot of great
stuff out there we're going to go with
Java and we're going to go with the
Apache stack we're tired of being held
hostage to a particular vendor we want
to go to something that's open source so
they went with Apache servicemix which
is enterprise service bus totally SB
who's heard of service mix great they
used carafe which is basically the osgi
container that allows for modular
deployments it allows for division of
labor we're able to set up different
carafe servicemix entities and deploy
modules into them and basically put
commonality between them and also allow
them to scale together we used activemq
for guaranteed messaging and
we used it for persistent messaging and
we use it for a vent consumer-based
polling in other words that's what got
rid of the point-to-point a consumer
didn't need to know where anything else
was so we so it was nice to have
consumer-based polled architecture and
it completely removes the point-to-point
this is the main backbone we used apache
camel Patchi camel is data and
communication routing we were able to
transform data from one type to another
and it was a container of all our
endpoints this is was this these were
the main components of our modules that
we would deploy we're going to show you
these in just a moment we used to patch
you see X F to be the web service
container and it would also we also use
to be able to use jax-ws and soap for
external communication and jax-rs for
internal ESB communication kind of
lighten the load so to speak and then we
use go patch II Cassandra patchy
Cassandra allows us to use to put
together fault tolerant data and a
fault-tolerant system and we basically
use Cassandra as a cash for service
queries to be able to backup for when
services are down and it's kind of cool
show you more on that so let's talk
about the solution we came up with we
went down there and we partnered with a
company called latinus which is an IT
consulting company down there American
companies have difficult time doing
business directly with the Ecuadorian
government typically there needs to be a
partner who's there so they reached out
to us and said would you help build it
we know your Apache guys we know your
service oriented architecture guys can
you do that and they introduced us into
the direxion national data heathrow de
datos pÃºblicos or dainard app for for
short they were in charge of putting
together this new data seguro and
putting everything together and making
it so they always go all goes through
this ESB so we built the system called
sinner dap our system and nacional de
datos publicas so we call the sitter
down for short it's much easier and
essentially what we built with something
along these lines we have three tiers
here we have a web tier that we use
liferay we have a routing tier that is
an ESB and we have an adapter tier which
are the
ESB endpoints these are three different
locations that live in three different
data centers all together in multiple
data centers within but throughout the
rest of this talk try to keep in mind
that we have three different areas that
we're going to concentrate on 44 where
we did our development or deployment and
our architecture any questions yes where
did we have the most problems as far as
scale so we really didn't have problems
with scalability per se it's scaled well
and you're going to see what we did in
order to achieve scalability and fault
tolerance but our biggest problem we had
was the that the adapter tier is what
talked to the legacy systems and these
components lived on the adapter tier or
near those and the infrastructure was
such that these systems would go down
and when they would go down our adapters
would would kind of go down and then we
couldn't communicate so we thought how
do we get around that what we're going
to use Cassandra and you'll see what we
did in order to get around that I think
the biggest hassle we had was defining
the contract between legacy
implementation pulling that data out and
converting it and it took a long time
before we got the government side to
side with us to say this is the old will
bring it in convert the data will make
it uniform but there was also one of the
key patterns in making this scale so
let's talk about the web tier first so
the web to this kind of standard stuff
they basically built a life ray portal
for the front end and the whole idea
about the web tier was for your end user
to do registration to sign in to log in
with the said EULA's and get the data
access this is the centralized point
that people would utilize to be able to
access all of their data and the whole
goal here was to bring all this data
together for them to see him one one
place so what it end up looking like was
kind of from a really high level and
what we're going to do here is we're
going to start at the high level we're
going to
drill in so you kind of see what we did
so from a high level here on the website
we basically used at a front end of an
apache httpd server it communicated a JP
to liferay for security it used ldap and
then it would the life right would
communicate via jax-rs using json
restful service calls using the the CX f
clients and that would communicate with
the ESB root layer so it would do
stateless communication with the ESB so
a lot of people say okay well how do i
scale this in the beginning so I kind of
show you the physical layer and how that
works physical layout was that we would
have the the Apache the Apache servers
would have load balancers in front of
them so if anything happened to a data
center a data center would drop a load
balancer would be able to the load
balancers would be able to move you
around to the different data centers
that was like the final backup of what
would go then we have multiple httpd
servers talking to multiple life raised
and it used a JP my JK sticky sessions
the session clustering to be able to
communicate with the liferay and then
liferay would communicate with with the
routes behind the scenes using restful
services interesting to thing to note is
we use the CX f client not only because
that's one of my babies that i get to
work on but also because it has some
really cool capabilities to be able to
failover on jax-rs communications so if
you're doing stateless communication
with a service and it fails you can have
it automatically fail over to a
secondary server or secondary ESB and
try to hunt down a service that's alive
to be able to connect to and you don't
have to do anything else you just set up
the failover on it you don't have to
write any code like you have to do with
some of the other some of the other rest
of all clients that are out there but an
important thing here is that this layer
here we're completely stateless all the
state is kept in the liferay container
there may be a token going but we don't
push state into the routing layer that
way we can scale this
yes the session clustering was across
this but not in between data centers
what so the data centers would have
multiple servers in them and you'd fail
over the only time you would ever have a
situation where you'd lose your session
is if you had to fall over to another
data center it's just too costly and too
much latency to be able to put to push
sessions across datacenters but with in
data centers you were fine what what it
meant for the client was it was very
rare to lose a whole data center so
things work pretty much pretty good but
if anything did happen they always had a
back-up to go to and they might end up
losing their session but that was rare
enough that it wasn't worth the headache
or the latency to try to push session
clustering across I'm sorry manage new
versions uh we haven't gotten to that
point yet I mean it's been running since
March I think and we haven't had to
upgrade the Apaches so I mean we've
upgraded two or three releases but since
this is Bill canoas gie it's it's a
rolling deployment so they have never
taken it down which will show you in a
moment it's it's had almost a hundred
percent of time it's it hasn't gone down
and you'll see when we get into that
we're about to show you that stuff
alright so this is the route steer which
is where that stuff'll it lives on that
question you just asked so the route
steer completely could be a completely
different data center different area
different racks depending upon how they
want to put this stuff together and the
routing tier of the ESB is kind of the
main conduit for the request it's going
to do all the mediation between the ROI
web tier and the and the adapter tier
this is kind of dude this is all of our
routing and the the this is also what
will handle the guaranteed messaging so
we don't lose messages yes question Oh
tomcat
so let's see what this looks like so
what we had here was multiple service
mix and none of these were on the same
machine the whole idea about fault
tolerance is being sure that we have
multiple ESPs out there and what they
what it would do is it would connect to
activemq and would network of brokers
that might even cross data centers
across racks in a network of brokers to
be able to communicate between different
service mixes what we're able to do is
do is be able to deploy similar module
since in different service mixes to be
able to create failover if we lose a
service mix the consumers on another
service makes could easily pick up and
take over or they could share the load
of similar routes what that does is that
makes a much more refined use of the
processes of the resources and it
creates failover there's no single point
of failure as long as we have similar
consumer spread out between different
service mixes this component basically
the service mixes would take inbound
jax-rs calls it would mediate the routes
it would and it would go out and make
calls to the jacks WV a jax-ws out to
the adapters to gather the information
pull that information back and
essentially store the answers in
Cassandra there's a reason we stored the
answers in Cassandra was because if the
adapters were down we would go to
Cassandra and say I'm looking up this
said EULA for this piece of information
do you have it and the and the Cassandra
servers will come back and say yeah here
it is it's all yours yes it's for reads
an right is read and write the hold the
whole system and the whole key to
scaling something like this is that here
you have a synchronous call between your
client and liferay and tomcat everything
else here is asynchronous
to respond synchronously right so what
you're doing is you're simulating think
think of a servlet request you have a
server crest serve the response put that
on a queue you gotta deal with the
timeout but you can supply your response
so the whole idea is that we're taking
one web server putting the request out
there can be thousands of answering
services that will provide you with the
response it in Axum q you can do request
reply using temporary q so I can a
client can go and say I'm sending this
here's the name mike you i'm expecting
response and it's all asynchronous and
the client can wait for a response and
it's still very asynchronous in nature
and we'll we'll show you a little bit
about that right now we're at the
hundred thousand foot view but we're at
the fifty thousand foot view we're going
to dive in deeper on this stuff yes we
did on the service mix side we did not
what we did do is we had several service
well let me let me state that there were
there were VIPs so there were VIPs that
would handle load balancing to a certain
degree for what would come in yet the
answer is yes f5 load balancers and VIPs
so the answer is yes we had to do that
otherwise it made no sense to be able to
cluster that stuff especially the jax-rs
I'm sorry to say that you know so there
were no sessions they're stateless so
you are jax-rs is stateless when you
when they went to login and there were
any credentials it would pass the
credentials along so we wanted to keep
it stateless for for this reason we
wanted to make it that they could fire
up another service mix without any
impact on and basically become a part of
of working without having to worrying
about clustering from that perspective
the way this is to sign it can run on
one machine it can run on 100 machines
and where we put them how we load
balance and so on we get to but
basically we can scale this indefinitely
because there no there's no state
and you'll see that as we drill down
into this further and then the Cassandra
the Cassandra any time we got a response
that came back and went to point towards
it yawn every time we got a response I
came back from jax-ws we would put the
response in Cassandra we'd also do when
we got updates if they updated the
database for some reason new information
we there would be a call a jax-rs call
that would come back and it would update
cassandra directly and we'll show you
these routes in just a moment third tier
is the adapter tier these are the
adapters they live in the locations that
are very close to the legacy systems and
these these ESB endpoints basically they
would translate the requests and proxy
them into the backend legacy systems
like what kind of service it was
communicating with was a database or
what was it a soap service basically did
all the communication and it adapted a
response that that would work within the
ESB and all the way back to the to the
web so these lived very close to the
legacy systems what did these look like
we got the calls that came in from the
jax-ws they would talk to service mix we
had an active mq master and slave for
updates and what that did was if we
could not communicate back with the ESB
because of the infrastructure we would
load that information onto activemq and
it was those messages would sit there
until and it would do a retry what every
15 seconds to try to communicate back to
the ESB and say I have an update for you
it's configurable and what we use this
one for here is guaranteed message
delivery so anything that we requested
or anything that they're pushing up to
us is guaranteed to go through as long
as we have this and this same as this
core solution can be expanded but this
was more or less our set-top box
integrating to whatever agency were
dealing with it could have been Oracle
postgres sybase MySQL
PHP services we don't care we have to
write the adapter layer to get it into
our format and so we would talk to
back-end database systems using jpa and
we'll show you some code snippets in a
bit on how we did that and or we would
use jax-ws to talk to back-end soap
services and when we had so far PC we
had to write very specific low-level
code to be able to communicate with that
they actually had some old Microsoft so
PowerPC stuff that is completely
deprecated that was very difficult to do
so we had to adapt and translate that
into jax-ws pull out the XML to palos
and be able to send that back to the
clients okay so let's talk about service
mix how do we use servicemix basically
in the whole system service makes
utilizes Apache carafe we leverage the
OSGi container pretty heavily because we
wanted modularity we wanted to deploy
things where we how we wanted to without
having to bring down servers and we used
apache camel and see excel as our
bundles with our routes so what a
service mix look like for those of you
who don't know servicemix basically is
Apache carafe for all intensive purposes
it allows shell login it does all the
management and utilizes Apache Felix as
an osgi container and we use the apache
camel on apache see except bundles in
there and we wrote camel routes and web
services this gives us modularity these
are our deployables we can deploy our
camel routes and our web services on any
service mix container that we want and
this allows us to do competing consumers
essentially clustering servicemix
engines servicemix routes this is kind
of the magic of everything so we'll talk
a little bit now about how what happens
with camel routes the camel routes will
talk about first is those within the
route steer the camel routes that live
within the service mix so in the in the
apache camel used for the routing tier
its main use was to basically mediate
the jax-rs calls and be sure that
calls went through and even if the
adapter failed when it had to
communicate with the adapter we needed
to handle that and give them some data
some way or another which is hot where
cassandra came in and here's the route
so y'all go ahead so here's UML markup
for those that like it or dislike it but
it's actually useful in representing
this we would get a HTTP request that's
sitting on liferay we're turning that
into a jax-rs after authentication
everything has been done we look at that
when we get it in we do a data check so
we check this cellular we check the tow
can we check everything then we try and
go out to the adapter and request in an
update on this edge allah if we get the
update we say perfect we wiretap that
response down to Cassandra if we don't
have the data sorry if we don't get a
response we go to Cassandra pick out the
data we then wiretap this put it on an
update q that's the whole asynchronous
part so what we're doing is we're saying
this adapter here is controlled in a
proxy so we can say during that
synchronous call are we allowed to ask
this question are we going to get a
response if we don't get a response go
directly to Cassandra that meant that
they could prep the system they could
pre fill the system we're still not the
data owners in this for a lot of
political reasons we delivered the data
much faster than any of the agencies
because we pick different technologies
if this call went through in a live
system you were looking at maybe 450
milliseconds when it failed we're
looking at 75 milliseconds so I actually
it went down as low as 20 milliseconds
and that's how we knew when the system
got really fast that's how we knew that
there was failure on the adapters
because it ran faster yes this has
nothing to do with technology it has to
do with politics so your anger the data
owners your point is correct we said why
don't you cash everything the reason is
the most up-to-date data is going to be
on the adapters so what they do is they
try to push they're pushing stale data
over to the caches the whole idea here
was to make it so it fixes the
infrastructure if they had to push all
the data over to Cassandra than what
you're saying is absolutely correct and
it's the way to do it but like yon said
it was a political decision that they
wanted the freshest data at any given
moment they wanted to go to the adapter
so that is what we use Cassandra
basically as infrastructures down were
giving you what we know alright sorry go
ahead
right we will show you will show you
some code yeah yes here this was
difficult because this could have been
out sourced it could have been licensed
agency it could have been anything and
on a strict time frame we were basically
so we've been given database access
we've been given views we begin soap
services and so on then what we did was
we forced the government people to help
us define this contract that's why we
picked soap because then at least we
could compile the entities and say
here's the new way yes luckily not I've
been given that in Brazil for credit
card information you can do it but it
sucks but we did have to write a
low-level soap RPC xml raw xml because
we couldn't jax-ws pukes on oh so Barbie
say it's like no way so we did have to
do some for a couple of we had to do
some magic wait do some magic adapting
know when it comes down to the writing
the interfaces doing anything like or am
i or connector because there are if you
take soap it's at least more
standardized do you have more people
understanding it and like any
outsourcing thing half the discussion or
if you take EDI is finally the person
you can talk to you on the other side
who actually knows how his system works
and everything else the second route OOP
sorry
the second route here is yes the update
that's another asynchronous that's
picking up the celula information that
came in from the wiretap puts that down
into Cassandra the we used quorum on
these rights with the anticipation that
if you were requesting something you
were most likely going to be on the same
cellular same servers so eventual
consistency worked out really well for
us and you write blazingly fast with
this and then we have so this is the
route route 1 and 2 or used from the web
client route 3 here is used from the
agency to push information back up into
the complex but the components are
reusables we're putting the same thing
onto the update Q asynchronously
updating the databases off we go and a
large chunk of the scalability and
building this was to find the right
patterns so every agency looks smells is
the same to us we're only changing types
that restoring for a service call here
is where we had to do some integration
work but everything else is reusable and
looks the same okay so real quickly who
here's familiar with camel is yay good
excellent so basically for those who are
this is what we call a camel exchange
it's kind of it looks kind of like JMS
almost that this is what a camel message
looks like you get an exchange it's got
some headers on it then it contains a
message the message has some headers and
then there's a message body this is what
really goes through this entire system
so all the payloads all the requests XML
payloads when we're doing the
translation will convert it to XML or
JSON and those are the payloads that
kind of move through the system so these
routes here if you're interested in what
this looks like this nice little pretty
graphic it's going to look like this
hopefully you can see that we can this
is what a camel what the camel routes
look like this is exactly we pulled
these routes out of our code that
utilizing we removed the password
especially with the policia the Palacio
is funny they actually let us play with
real data so we pull people's guys were
working with us so let's see if you've
got a record but was cools I could pop
in my passport number my american
passport number i can see all the
entries and exits unlike aw man they
really let you play with this stuff this
is cool um yes we use both we used both
there's you know I I so the reason why I
like to use the xml stuff is for unit
testing it's just kind of easier for me
he likes to use the Java so when you
looked at our code you'd see both sides
of it being done you see his routes were
Java and mine were XML we just mixed and
matched a lot on a limb and say I
passionately hate Nick some oh yeah he
hates it so all right so the restful
processors basically well yawn
implemented the coolest thing it was a
service pool that would talk to the
adapters there's a lot of overhead in
creating services and what is what we
see in a lot of software development is
people creating the CX F client services
or metro services over and over and over
and you get just a tremendous amount of
garbage collection a lot of time and
overhead so we create he wrote this
really cool code that creates a service
pool and if the adapter was down it was
referenced in Cassandra so let's take a
look at that look at this code I'll zoom
in so the first thing we do here is that
we basically have a common spooling
service pool where we're hiding our web
services inside of a proxy so we're
calling those ones basically with
reflection on the proxy that way inside
of the proxy I can say I have enough
service clients I haven't timed out I
can mark a service as dirty that way I
get the instant cut off we don't have
timeouts anymore we will have one
timeout the first client who gets a
timeout to an agency we mark that them
agency down we don't allow any traffic
and we start pulling data from Cassandra
sorry even if even if it's a fluke what
we found is yes by marking it when you
have had a timeout we give their end
some time to revive everything if they
came back up they'll go off and sit
there so we might give a five minute we
had a configurable time limit depending
upon the service and say like five
minutes time out so it's marked as down
and pulls everything and that's what
this code is doing in here it checks and
it comes from Cassandra 45 minutes at
the end of the five minutes it marks it
up in the neck client who goes gets it
and everyone who follows because as well
then we have in red here the other
question what we're looking for is if
we're turned out or we get an old
response the proxy will actually return
a null if it's chopping off we pull the
data out of Cassandra if we can find the
data in Cassandra we finally throw a 404
and this was the contract they agreed to
we then stop the route and say that we
can't update anything but this is
equivalent so what we ended up doing is
this which I probably should commit back
to camel our end point that we're
calling is completely controllable so we
can cut off the web service we can time
it out we can market dirty and we can
reuse the same processor code from camel
over and over for every process we has
changed the types think of it as a
generic Deo pattern this is a generic
web service pattern and you notice we
checked out the service in the beginning
and here's the check back end we used we
use Commons pool to be able to pull
these services and it works real well so
we would have all these services ready
to go and it would pull them and it was
wicked fast another scalability trick
hidden inside of here is that everything
we're running we're running on Jack's be
we made a JAX be container that we have
all types in every type conversion is
done from the same osgi service so even
if you have an object graph of the
700,000 to millions of Jack's be objects
if you instantiate ones and pull that
marshal errs and our marshal errs aren't
thread-safe but you can pull them that
means that you can start reuse
those you can speed up XML marshaling
that's another common problem that
you'll see in web services you create
you recreate you create you recreate all
of these contexts instances all they do
is bring up the object graph process
your request then release everything and
you have garbage collection we just keep
everything floating that's one of the
magic of any high load system reuse
anything and everything you can the more
you do that the more performance you'll
get out of your systems under load you
should literally have if you look at j
visual vm you should have like a
straight load graph you shouldn't see
increases if you start doing that you
are leaking somewhere or your building
and rebuilding okay so we took a look at
the route steer let's take a look at the
adapter tier Jeff yes we used normal
regular Jack's p we just took the
interfaces for Jack's be context and he
pulled the context and I pulled the
context and then I put a comma spool
behind it and it's wicked fast and the
other thing that it does is I it gets
rid of the leaks I don't know if you've
ever experienced high load Jack's be
reuse of the context you end up with
lots of leaks as you create destroy your
martial arts by create having the
marshal is already created and pooled we
get no leaks at all it cleans up after
itself really nicely so it's it's a good
pattern to use in any high load system
that does any kind of type of martial so
the adapters let's take a look at the
adapter layer so we use the apache camel
adapters each each company or government
entity basically has its own adapter
service to adapt its legacy system to
what we want to see as I said before it
lives very close in the data center for
where the legacy systems are because
it's an adapter it's adapting their
stuff to the way we want to see it and
then we communicate over the wire to our
own stuff that's living that remotely we
certainly don't want oracle streams
going over the net so we'd rather make
these adapters that work much better and
basically it'll meet
between soap and any of the back end
company or government entity data sinks
so let's take a look and see what it
looks like go ahead yawn fairly similar
where we have jax-ws endpoint this is
our request from the ESB we're calling
with this one we're going out to the
legacy system which ever way we could we
pull in the data this is where the code
that you don't want to pass code review
you're sitting this this is you know
whatever it takes to get it in we're
converting it we're putting it back
giving a soap reply we have a second
route where we have updates coming from
the agency they put an update that it's
the same payload as the satellites just
to put update we put that on a queue we
put that on an activemq update queue
which is the reverse of the ESB layer
then we put that into a prepare update
and retry update so we knew that these
adapters on the agency side we're going
to go down more they could be behind dsl
for updates they could be behind the
firewall or something like that what we
do is we take that the payload put it on
a queue we try and send with a activemq
we basically have a processor that's
doing the produce or templating if we
can send up on the jax-rs update the ESB
once again we use the same technique we
mark that as dirty any updates that we
have we put on a retry queue then we put
a timer in and say let's see if we can
send up now if we can send then we start
draining with a slow basically a thread
sleep so that you don't want to flood
the central system but you don't want to
miss any updates so what we're basically
doing is we're slowing everything down
as much as we humanly possibly can and
then we drain the system that way this
adapter is really only a reverse of the
ESB route the code is the same the types
are the same
the jacks become texts are the same we
can reuse all the classes that leaves us
with this implementation class here okay
and that's our introductory yeah notice
I was just going to say so the the one
of the things we did we had to talk to a
database so we use we use jpa for the
most part that was JP was the way to go
we also need a rapid development which
is why we use jpa and we used generics
for crud in other words we didn't want
to rewrite this stuff so we use these a
really cool pattern for a jpa so this is
what we call our generic ballot I don't
know if you can read it if not i can
zoom in but basically this is a generic
dow interface that uses generics here
and we basically have create read update
and delete on these pretty standard
stuff and then what we did was we would
implement it with a generic jpa Dow so
the generic jpa Dow basically set our
empty manager and allowed us to do our
general create read updates and deletes
so now that we had that we'd be able to
implement this stuff by extending the
generic jpa Dow we declare what are the
types what is the type keys what is what
does the return type supposed to be on
it and if you take a look at this just
by by declaring a dhow in this fashion I
have full create up read update delete
and then I can i can do specific getters
if i want to in there it's that simple
so we were able to do that so for I
don't 410 of entities or eight of the
entities we had to write this stuff and
write pretty intricate stuff this
allowed us to knock that stuff out this
is a fantastic pattern to use for jpa it
gets rid of a lot of replicated code you
write it once and it's this simple to
implement
okay so we kind of looked at we're back
on the routes now we want to take a look
at what we did with Cassandra so let's
have a look at Cassandra so Apache
Cassandra it's basically a no sequel
database that's all java-based who hears
used Cassandra can I see hands a few of
you what did what clients to use to
connect to it anyone anyone so there's a
few libraries to do it a lot of people
will use something like Hector which
works pretty well and the and well I'll
get into that in a moment but we want a
consentir to be there for because it's
fault-tolerant it mends itself it's it's
cross data center capabilities are just
amazing and there's no single point of
failure which is really nice we really
needed that pretty huge and you know
talking about what clients did we use
this well this is kind of what Cassander
looked like so that you can you can make
it so it scales well in Iraq between
racks and between data centers so we're
able to set these up to put priorities
for where it's going to store its data
based upon whether they're in the same
rack different racks or different data
centers what it did was this really gave
us a good solid fault tolerant way of
pushing our data and get our answers
back into Cassandra when we were talking
about the the api's how do we do it
Cassandra a lot of people use either
Hector or the netflix library or a
thrift API I think they all kind of
stink to be honest with you there's
still extremely code intensive to use
all the above thrift just sucks to use
no offense to the thrift guys but it is
ugly code and it's a lot of code and
therefore we need to do like what we did
for jpa so we need an API that was easy
to use for jpa for crud we needed
something similar what I just showed you
so in a nutshell Cassandra stores things
and what we call yes
maybe already knows yeah we actually did
a lot of development in the cloud and it
worked just fine but for the most part
when it went into the government they
want to control control data centers
part of our development was using cloud
and using github and that's our stuff
because we did a lot of development up
here we'd make a couple trips down there
but we were working with them purely
through github which worked out real
nice all the production stuff is in this
day Center but this works for the cloud
this exact architectural woodwork
absolutely finding the cloud as long as
you know act you know where your stuff
lives you don't get those randomized
dynamic ip's this stuff looks fine so
this is what a column family table looks
like inside of Cassandra we notice we
have a row or a key and then we have
columns and they're kind of randomized
one of the things we noticed was if we
took a column family and kind of filled
in all the columns it was nothing but a
javabean we could actually map a
javabean to this stuff easily and even
if we didn't have the columns they would
end up as null values so it made it
really easy so we said hey listen let's
just map the road in the column two
fields in a javabean so we wrote this
really cool API what we did was we want
to just use beans so we would create
entities some keys are called composite
keys where you can have two fields
together so we created an annotation
called a composite component and we
order them so these two for example was
tipu and said eula they would make up
the key and that's called a composite
key so by adding these annotations on
here our annotation processor was able
to mold those together behind the scenes
and use reflection to be able to map
this entity to a cassandra fee to to
cassandra a record so to speak so we
created a generic dao kind of looks
similar you know delete get key safe
fine kind of some crud stuff that's in
there so we're able to do that and then
we wonder what was called a generic
iterating doubt we wanted to be able to
use something like a result set we
didn't want to blow up memory we want to
be able to say give me all the records
that match this criteria that could be a
milli
in records well that would blow a jvm
and under normal conditions so we wrote
an iterator so it has almost no memory
footprint and we can get as many records
as we want by iterating through each one
of the records so we created this
generic iterating Dow now there's a lot
of code we put behind it to handle this
because it was all based on reflection
but in the end it was this easy in order
to create a doubt for Cassandra all you
did was create an interface and an
implementation of the generic iterating
down or the generic dao and that's it
there's no code you have full crud going
in and out of Cassandra we did it take
us to knock out these the dowels to
Cassandra for each of these well I mean
we can take this story further we have
expanded on this library we made sure we
open sourced it to ourselves before
because we realize this is going to be a
home run but in a project this type and
scale we can put people who have no
Cassandra training does not know what it
is they're typesafe we know what's going
on we control the type in fur this is
all the code you need to start storing
things and retrieving things out of
Cassandra if you can think and solve
your problems with asset map and a list
you're home free we have the solution
and I think it took Jeff initially a
couple of weeks there were a couple of
weeks to write the first one of this we
put people who had no other skills than
being literally able to start eclipse
and said copy paste this put the type in
you're storing data to Cassandra the
other beauty of Cassandra say no SQL
solution is that it's fully 100% Java
which means that it's really damn easy
to unit test because you can embed this
you can run unit tests you can run low
testing you can run all of these things
in you read a ide it's not going to
change the deployment if there's a great
product out there called or open source
product called Cassandra unit that we
used so we would run contender in your
unit tests and we could exercise all
this stuff we
drop it in production I think day one we
dropped in production it ran flawlessly
if we were like wow no bugs unbelievable
so it was it well there's no bugs
because we didn't write any code when
you deal that's exactly correct there's
no code and that's the point is and this
is the use if I want to use it I created
Dow it which is what you can see up here
and then this is an example iterator i'm
using my dau to do a find ok in that
middle the middle line there and then I
just go through the iterator I'm able to
pull the comment columns and read the
college it's that easy if I'm just using
a Java mapping being a general generic
dial not even the not even the iterator
look look what's in blue that's how easy
it is I'm pulling an osgi service that i
got from spring or blueprint so i'm
pulling in the dow there look there's my
save their i'm looking if there's a key
i'm doing a finding doing a delete and a
look up that's how easy it is what's the
good news we're going to open source
this library because it is so cool and
it's been so great to the people who
have used it they they were just blown
away out simple as we're going to open
source it probably in the next few weeks
so everyone can have them at in the
masses can utilize it so it's something
we're definitely get out there and y'all
can use it it's just a great library so
that was that was Cassandra our final
component is activemq and how we used it
basically it's JMS we used it for
persistent messaging we did not want to
lose anything anything that went through
the system we wanted to be able to track
it it basically was the backbone that
removed our point to point and it
implemented the consumer polling
strategy what that meant was all of our
modules did not need to know who to
connect to it just need to know where
activemq was and we could plug in any
amount of end-user routes we could scale
service mixes endlessly if we really
needed to and if you think about it we
don't with this we don't need uddi we
don't need a registry we don't need to
know where services are we can just
build in a lego fashion and start adding
things in so activemq is the backbone it
is the bus and how we set this up here
is we had multiple data centers each
data center had what we call a broker
node and
we'll sit up as a master and a slave so
if the master goes down the slave picks
up immediately and if both the master
and the slaves go down we fail over to
the second data center as you can see
the camel component on the left there it
tries the master can't get it goes to
the to the slave can't get it then it
starts going over to the other data
centers that are further away same thing
with the one on the right it goes to the
closest data center and it starts
picking up this is all done for what we
call the failover transport it
automatically connects there's no code
you use the failover transport and
activemq you can tell it where you want
to do it and you can do it in the order
that you want it to connect or you could
have it do it randomly if you wanted to
each one of these data centers what we
call a network of broker node we can
scale massively horizontally by adding
another broker node if we want to scale
up we just add another node it can be in
the same data center it can be in a
different data center we can scale
massively and handle any type of load
the messages the neat thing is the
messages flow freely between the
different brokers so I could stick a
consumer on any other broker and those
messages will go across so we're able to
scale up and everyone can see the
messages as they need to consume them
this also allows for consumer based
event handling it basically means the
endpoints they do not need to know where
the other where others live I can plug
it into activemq anywhere you'll consume
messages that's meant for it to be able
to do things by itself and what that
does is that loosely couples the
producers and consumers it allows us to
competing consumers so you can share the
workload based upon how many service
mixes that have similar routes in them
so now I'm able to really scale by
sharing the workload and passing around
one surface mix may pick up one type of
route and another service mix may pick
up a separate type so that is kind of
how all these components all the
different tiers of the system fit
together so what is the result of what
happened the result is we're able to
handle over 40 million transactions per
day 40 million we can handle up a lot
more than that the when we were load
testing it was amazing what we were
pushing through this thing we handled
full failover I think one of you asked a
little early
layer can you know where were the
difficult points of failure as you can
see we kind of we kind of had we fixed
everything we kind of set it up whether
it really wasn't a single point of
failure for the most part so we didn't
have an issue and that was the whole
goal of what we wanted to do what we did
find was the Ecuadorian government loved
the system they found it was an immense
success and it was a tribute to Java and
leveraging open-source it was probably
one of their biggest success stories for
a massive governmental nationwide
implementation it pretty much that it
got rid of all their doubts and it
really made them get rid of you know
starting to get rid of hate to say it no
offense to the Oracles of the world and
Microsoft getting rid of these licensed
products and it's become such a poster
child that Chile Peru and Colombia have
been looking at that system and said wow
how can we reproduce this this is
fantastic we want to do that too it gets
them out of vendor lock and it saves
lots of money and they know what's going
through their there's no back doors for
our friends the NSA to read I didn't say
that yes
well network of brokers kind of does
that so that'll give you some degree of
high availability it's not a true
cluster because I have to communicate
but you kind of get a hybrid between
that of the master-slave really is
pretty much if your machine drops the
other one's going to pick up and start
going immediately with almost no
downtime whereas a network of brokers
you're evil they have two masters being
able to communicate with each other and
pass messages back and forth that was
our goal was to be able to do that so we
kind of did a little bit of both load
testing load testing I I think we load
tested a single a single service mix mix
was able to get 200 transactions per
second that we didn't measure that but
since it was going over to activemq
clearly minimum of 200 per second per
service mix talking to activemq and that
was go to several reads and writes
remember these were small payloads too
so we will get very very high load
through there we've gotten up to 30,000
per second and network of brokers you
know on activemq with very small
payloads and we've we've really pushed
it if you have to tune it you know
you're not going to run that out of the
box that way and the last point the
system success enabled other Ecuadorian
government further build out
applications built on Java and open
source it was a raging success and from
that this is kind of like the poster
child of a raging success of deploying
something like this in the government
unseating a commercial vendor and
getting comfortable with Java and open
source to say this is some of the best
software we've ever had and seen and it
all had to do with Java and that's
that's what this is all about and I
really hope that some of our techniques
here are some of the things that we
showed was helpful and we do have
questions we're going to start handing
out books two good questions you had
your how long how long was about six
months six months for the first
iteration then I think they
let's put it this way the first adapter
yeah it probably took three weeks and
writing the Cassandra library writing
the proxy code handling all of those
ones the last one I did I did at the
airport in four hours because I knew
exactly what to copy paste and so it's
all about finding patterns you can reuse
can that man a book well I'm going to
take this off because Cassandra's
java-based it also has one of the
highest throughputs I think there was a
really good a really good throughput
view on Cassandra and on on as you add
on the nodes the amount of throughput
you can get on it and I think it just
smeared everybody the decentralized
capabilities of it that it has was
pretty good the fact that was java-based
it made unit testing really really
simple you know we didn't want to have
to sit there run another entity it just
it worked real well and the way it just
merges and the replicates itself is just
fantastic and that's really why we chose
it give this man give this man a book
one one moment he's had his he had his
hand up go ahead
for say that again
we log to everything that went through
we used logging with the log4j and then
they would use they would use splunk to
be able to analyze that data so we know
what IP addresses came through what
worthy what was the the schedule is that
was requested who did it when what
systems so we would we would log all
that stuff which that was some of the
stuff that came out of that security
problem was you know we didn't want that
happening again in fact we wrote caching
ldap adapters was what we did give that
man a book in the white hat the
architecture itself would be reusable
obviously the backend adapters would not
be also they the their security
mechanisms would have to change but for
all intents and purposes this could work
directly in Peru yeah and I think that's
why they're looking at it yeah
absolutely
naturally
let's go over here yep I'm sorry so
there's a camel test that we there's a
camel test framework that we can utilize
that we can put in there different types
of machs to to to work with the data and
and make it look like how it's supposed
to speak so when we would piece them
together obviously we had integration
pattern the integration testing however
we use camel test which allows to inject
mocks into different points or to say
hey did this go through with the camel
test let me know if this exchange how
many messages went through and the end
result message needs to look like this
so we unit tested all of our camel
routes heavily yeah yeah absolutely you
could do that and that's that's why we
use stuff like Cassandra unit and we
would actually use that in line with
camel to say hey are we saving to to
Cassandra and did I get data back so
that's how we would do that we'd build
like short little unit tests give this
woman a book yes
I have no idea we stay out of the
hardware talks because the network side
guys are a different breed of thinking
and human beings it actually wasn't the
initial install isn't very expensive it
was off the shelf I think that equipment
you know 12 gig machine stops the whole
idea is we're scaling horizontally so
hardware's cheap yes pick up your mic
because they're recording I'll give you
my business card but I did a
presentation at uber conf on this the
benefit is if you think of an EE
container it's a monolithic synchronous
call right where you run out of
processes is in how many threads you can
maintain and if creating the response is
more expensive what you can do is you
can pre pool you can pre-compute you can
set up more response builders allowing
you to one field it across more data
centers you could shower databases you
could replicate databases all we're
trying to do is to get the read as
quickly as possible people with PHP used
to do things like creating memory tables
in mysql of things that came in to make
it look like it it's instantaneous and
if you think about it this is very
similar to paypal google whatever you
have a front end farm that's disposable
assets then how we build a response
that's the real workload
underneath yes so the iterator that's a
slice query but what we wanted to
eliminate was this whole by typing fur
converting data now we can based on
reflection say that this is a string
it's a number is this and that yes yes
we didn't really work with the temporary
stuff we just when it was down we let
activemq and ensured that it got written
to cassandra period which is really what
we did so we used activemq is that all
right for the book one more question you
yeah actually so we said on the new
iteration we're going to be going
through a new iteration we did a lot of
copy paste but what we're going to do is
something called a dynamic route factory
which is really what we're going to end
up doing so we have one type of
component that we write we fill in the
blanks there is no copy paste we fill in
a couple of class types and that sort of
thing and boom it'll work the same so
are our next iteration is going to have
this dynamic route builder with a route
factory which is what ultimately we're
going to do and that that that have we
gone down that route at first we would
have really knocked this out but you
know as you go down it's kind of like
you know you're you're trying to figure
it out as you go so give that man a book
you
you know that's a good question I keep
an eye out on the savoir Tech and will
be tweeting about it that's our Twitter
handles at savar tech will what will put
it out there will probably ASL it apache
license it and just check for it on
github and it'll be out there no we
don't have a name for we need to come up
with one so but it's going to be cool so
anyone who does Cassandra work i would
highly recommend it tweak students in
that way i can actually handle dynamic
column so I can handle apps and lists in
this because I had a use case where I
needed to use our library where I wanted
to use our library that we had customers
who want to do you seek you up and CQ
our know if that's a whole different to
date but the customer's always right
definitely I can make this event they
can look at the data types
yes uh tom cat has session it will
handle the clustering inside of it you
can set up the clustering engine and
that does everything so but you need to
set up your magic a to handle sticky
sessions and be sure that you're talking
to the same one otherwise you get too
much latency going in between the
different clusters and it's important to
balance how many time cats you want
together because then you end up in a
star network and then you start getting
more latency because they're all pushing
the same cluster around so any other
questions no well thank you very much we
appreciate it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>