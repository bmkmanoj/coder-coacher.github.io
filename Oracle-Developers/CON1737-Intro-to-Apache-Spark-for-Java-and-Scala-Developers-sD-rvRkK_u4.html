<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CON1737   Intro to Apache Spark for Java and Scala Developers | Coder Coacher - Coaching Coders</title><meta content="CON1737   Intro to Apache Spark for Java and Scala Developers - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>CON1737   Intro to Apache Spark for Java and Scala Developers</b></h2><h5 class="post__date">2015-12-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/sD-rvRkK_u4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello alright so we're gonna do an
introduction of spark today for Java and
Scala developers just to give you some
background on myself my name is Ted
moleska I've been doing Hadoop type work
for about six years now
I am a PSA at Cloudera which means I get
to work at you know it says like north
of 70 companies I don't know how many
companies I've worked for so it's
somewhere around 70 to 100 companies
mostly on the East Coast some on the
west coast and it's been very exciting
to learn from a lot of different people
and to get tons of use cases in terms of
contributing I've contributed to HDFS
MapReduce yarr and HBase spark Avro kite
Pig navigator cloud manager flume Kafka
I can't even do it in one breath and
scoop and accumulo and I'm trying to do
kudu if I get some time how many people
in here contribute to open source all
right that's not too bad how many people
don't know what open source is alright
open source is amazing open source is a
place where people collaborate ideas go
and big companies fear so it's a great
place to grow and learn also to build
your resume there's companies now that
if you if you can contribute to projects
you have a job right I mean big money is
an open source and you know it's good
it's helping everything if we have time
for questions later on I'll talk about
some of the real world problems that are
being solved today because of open
source so other than that I'm a Marvel
fan boy so for all your DC lovers I'm
sorry and I'm a runner any questions on
that alright so the look ya know
Superman would kick Batman's ass I'm
sorry sorry Batman's got no chance in
hell
okay alright so what are we gonna do
today we're gonna go through learning
like what do you do when you learn code
we're going to talk about distributed
programming
we're gonna talk about what spark is
we're gonna talk about parallelism and
we're going to talk about you know some
memory considerations right so I'm gonna
try to start out really basic and then
I'm gonna get very complex you can ask
questions at the end of every slide at
some point I'll get tired of you and I
won't and pick on you anymore right so
uh so be you know be you know conscious
of how many questions you're asking all
right so so learning to code right under
there's I'm listing here barriers to
thinking understanding applications and
the underlining costs of things so I may
be a little different than most I
self-taught myself to code when I was
eight years old I loved coding that's
all I will ever do I will never become a
manager I will never become a Salesman
I love coding I will die with a keyboard
in my hand so but one of the things that
makes coding amazing is this hurdle of
getting over getting getting your mind
around a concept or a problem and it's
fundamental and when you can do that you
can build onto these other things like
understanding how things work and
understanding their fundamental costs
now I try it and this is definitely not
all of the things this is definitely not
academic but I try to think about the
things that I went through in learning
code right you have to learn the
concepts of a variable right I'm trying
to teach my six-year-old an
eight-year-old a code right now and the
concept of a variable is something you
have to get over the fact that X can
change right all of you hopefully know
that X can be assigned a value and can
change so that's that's an easy hurdle
right and then there's the come there's
the control flows right you have to
understand your if statements your for
loops and whatnot and then the big thing
is like you moved to a function Wow do
you remember the day when you wrote
everything in one function right
everybody did it right and who had to
put numbers on the left side of their
code do you remember those days yeah and
then you like you like added 50 on every
single one just so you could put an
extra stuff later on yeah it was a thing
and then when objects come became
did that not blow people's mind like
there was a while like do you understand
object-oriented programming and it's
like now if you don't understand
object-oriented programming he was like
what's wrong with you right but yeah
there was a time when people were like
this object-oriented program thing will
never catch on it's all too slow right
and then then there was the concepts of
threats right as I get through this some
of you may not even deal with this right
if there may be some of you that are
totally fine and never have to deal with
multi-threaded programming right but
that's another layer of thinking right
because now you have to think about I
have many different things going and in
parallel I have to be very careful when
they communicate to each other I have to
be cognizant of my CPU I have to be
cognizant of unit testing and race
conditions so lots of fun things like
that right and then the one that we're
really gonna be talking about today
hopefully hopefully when I get across is
distribution right doing multi-threaded
across an entire cluster right and
that's where really where SPARC is gonna
help us right and that's and you go
we're gonna talk about a lot of things
about SPARC but in the underlining thing
it is helping us write distributed
programming with ease okay how many of
you are distributed programming masters
all right so okay we have a pretty good
is everybody understand the concept of
distributed programming so just okay so
let's say you have a cluster of a
thousand notes and every node has 32
CPUs and you want to get every inch of
those thirty thirty-two thousand cores
burning on all of the disks as fast as
they can
that's distributed programming okay okay
so I'm going to start off with some
basic concepts so spark works like this
right it has these major components
there's more this is very high level
I'm not going to dig too much into it so
take a picture save it for later but
there's this cotton there's this concept
of a driver think about a driver as like
the boss right it's it's my wife right
so there's this driver and the driver
kind of tells people what to do
right and kind of keeps track of what's
going on right then there's these
executors in different systems that we
call different things right and
MapReduce they're called mappers or
reducers sometimes they're you know
they're you'll you'll see the word
container or whatever there or sometimes
what we call workers in the spark
context they're called executors right
and executors can do things right they
have they have so many number of cores
assigned to them and they have so much
memory assigned to them and they are
given work by the driver and then
there's this other thing that really
isn't a thing but I had to put it in
there because of the future slide you'll
see is this process which isn't its own
thing but it's a it's a concept of the
shuffle and I'll show you a diagram what
that means in a minute but it's not
actually a service it's more like a verb
where the other ones are nouns any
questions on this so far yes why do you
have functions because I cut and pasted
that that's why is that is that is that
good enough for you yes that's exactly
why I don't know um all right so ignore
that so the first concept in distributed
programming is this concept called
broadcasting right and the the the the
next couple of slides are really going
to be talking about how do these
services interplay with one another and
what we're going to go into is all this
interplay is extremely expensive right
now when you're doing multi-threaded
miss you have to have synchronized
objects right they're synchronized
values and people are like oh you know
you're gonna get locked up on those
values and it's going to slow you down
and that's true now when you're across
multiple machines it's like a magnitude
right it's even more painful and in some
situations that can get so painful it'll
kill you so broadcast variable is saying
the driver has some type of content that
it wants everyone to have right
sometimes this could be metadata
sometimes this could be something called
map side joint so everybody know what a
map side join us so let's say you have a
really big table and you have a really
little table right there's different
ways to join tables one is to put the
little table in memory and then just
scan over the big table and make it make
the joints with like a hash map right
very efficient the other is to reshuffle
and we'll go to in a minute reshuffle
all that big data so it and partition by
the data that's in the small table
that's not really that efficient so
sometimes a map side join that's what
you would use here you would send the
smaller table out right so so
broadcasting patters when you have
something small and when I say small
small is a terrible word small meaning
probably underneath a gigabyte right
once you start getting over a gigabyte
you're start going to getting into
trouble so something small that you want
all the executors to have one of the
things we may talk about later on an off
question is I got to build the HBase
integration with SPARC and we send out
the HBase configuration through a
broadcast pattern it's very small it's
meaningful and all the executors have to
have it alright the next one is this
concept of a take write it's the exact
opposite now after the workers have done
some work we want to bring the values
back so we can do something with them
right so this is great and all but you
have to be careful right that one driver
is real small you got to be gentle with
it especially when you're working in a
distributed system and you know let's
say you have a thousand nodes and you're
working on a petabyte worth of data
don't return a petabyte worth of data
back to the driver right so you want
again these things to be done sparingly
all right the next one is the idea of an
action and this is when the driver sends
commands to the executors and the
executors actually do work okay it's
very simple do this tell me when you're
done and then the last one is this
concept of a shuffle and I'll draw this
this is this is what happens in a
shuffle
so in a shuffle there's a map side in a
reduced site right and what's going on
here is the map side will partition the
data by some type of partitioning
normally at the hash
it'll partition the data then sort the
data normally sometimes it doesn't and
then the data will be funneled to the
appropriate reducer so an example would
be let's say you want to get the total
number of word count as the perfect
example we're going to show that code
later all the words would be partitioned
into four partitions the reducers would
get one of those partitions and then
they would count all the words in that
partition right and the word the would
only land in a single partition and the
word cat would only land in a single
partition so now the reducer then can
count them all up and aggregate them
does that make sense all right
so even though MapReduce is very much
dead or dying the concept of MapReduce
still exists in spark and it will never
go away it's a fundamental pattern right
it's just which engine is firing up on
it
so some of the takeaways more nodes give
you more i/o and more processing power
exchanging data is always expensive so
as we start talking about these things
we're going to talk about we don't want
to exchange data as much as possible we
want to do as many as we can in their
isolated silos and then only take or
broadcast a small amount as possible
okay oh the next thing is everything has
to happen as a lockstep I didn't really
talk about this but it's very much true
so just because just like in threads
right you need synchronization points
you need synchronization objects or sync
points the same concept goes here with
distributed programming and you will see
this in a later picture of a dag all
right so as we move forward
what is spark right so when I think of
spark I think of four main concepts and
if you understand these
four main concepts you'll pass an
interview okay so hopefully the first
one is this concept of an RDD a
resilient distributed data set this is a
read-only this is just a read-only thing
it's a read-only entity that's
distributed across your cluster it's
very important what to read only we'll
talk about in that in a minute but
essentially just think about it it's a
collection behind the scene the
collection is distributed across all
your notes but it's just a collection
you can't change now you might also hear
this concept of data frames out there
don't worry it's not that scary it's
essentially a collection with a schema
and that's pretty much it so so why why
is it read-only right well I'll show you
in the next slide and that's because of
the dag who knows what a dag is all
right so what's a dag
a direct asynchronous graph now when I
first heard that this was a dag engine
there's a true story I went home and I
told my wife and I told my kids I said I
love you
but there is something called a daggit
and I must learn this and I will spend
all night long learning what this dag
engine is because it must be amazing it
sounds like a jet fighter you know like
jag engine it's gonna change the world
and then I go on wiki and it's a graph
that doesn't link backwards that's it so
I went back down had dinner so it's a
graph where the line doesn't loop back
that's it that's a dag engine anyone
who's ever done ETL in their life has
been doing this for years so nothing new
nothing exciting the exciting part is
it's now built into spark and because
those rdd's were read-only and will go
through in a little bit it allows us to
recover from failure and replay from any
position we need to to reconstruct our
answers right because another thing to
think about in distributed programming
which hopefully you don't have to think
about the developers of these engines do
but you won't have to hopefully
is anything could fail at any time
anywhere anyhow when you have a thousand
machines drives fail like I don't know
does anybody know the frequency of a
drive failure well every day
right I mean the drives are gonna be
failing all the time so it's gonna be
assumed that you're gonna have failure
right so the system that's doing
processing has to recover and you have
to not care or think about it right it's
just part of that game so just putting
in a little bit of vocabulary in here
the circles are rdd's okay the circles
with the black dots are actions okay
and the red lines are transformations
we'll go into what these mean in a
minute and the one with the white dot is
the source okay I like my pretty colors
all right all right so what does that
mean okay an action is something that
like you answer a quiet you ask a
question the answer is what you get back
so remember we talked about a take where
everything comes back to the driver
that's one type of action another one is
a count which is really a take of a
count right you're saying give me the
count
another one is a for each which means do
this and whatever happens in the
execution let it happen
right so these things when you fire them
they happen they actually execute right
then and there
transformations on the other point when
you call them nothing happens except the
dad gets more constructed all right
think about it like when you're asking a
question all the words that are coming
out of your mouth before they pair the
excellent with a little question mark is
transformations right you're still
formulating your question you're still
formulating the structure of your
question but it isn't until the end of
your question that you demand an answer
right so it's kind of lazy constructed
that does it does that open everybody
get that it's not necessarily a big Java
thing this lazy stuff right but it's a
big scholar thing it's a big Python
thing where you don't it doesn't
actually do the work so if I do to join
or I did a group I order to reduce you
doesn't actually do that until I
actually demand an answer it says okay
you want to do this but until you want
an answer I'm not actually gonna do it
because it's a waste of my time
right this is just a pretty graph to
show you if I had a failure right and I
have a beautiful beautiful pink failure
not saying that pink is a failure color
but if you had a pink failure it would
replay these nodes and then eventually
replay the entire output anybody have a
question on dag or RTD because these are
really simple yet kind of not simple
just like object-oriented concepts so
the actions are yeah they're like give
me the results right not not really I'm
trying to think conceptually if you can
go off an action I mean if I took a
result I could put that I could
broadcast that result back out and then
do something with it it's possible I
could have more stuff but it wouldn't be
it wouldn't be the more stuff would come
out off of the nodes before but I still
might need it right no so simple answer
no complicated answer it would make a
really ugly diagram but simple answer no
whatever is still available he's so yes
where would you replay from would you
replay from the closest node and it's
actually if I really drew it out it
would be really ugly it's actually the
closest nodes that survives partition
because in every node there are many
partitions so let's say one of those
circles so this is a great question
thank you if one of those circles is
let's say a group by right that group by
might be on a thousand computers but
only one of them failed right so it's
just going to replay that portion right
so it's going to replay the least that
it can what that is will depend upon
what failed does that make sense
cool
perfect question because of the replay
would not create duplicate answers or
something like that and that's why our
rdd's are immutable so because our rdd's
are immutable if there is a failure we
delete everything we were constructing
and we start again see that's the genius
behind it so if you get more advanced
and you start using spark streaming one
of the big amazing things about spark
streaming is no duplicates right
if whatever came in you can't promise
duplicates before it but duplicates
within it it will not generate a double
count because of that there was another
hand yes will you get slides after this
I don't know but I figure out how to get
it to you we'll figure that out
afterwards I'm sure I'll talk to these
guys I'm sure they know what to do yes
what is the biggest dag I've ever seen I
have seen a single query crate I think
it was 1500 steps and then I went in
there and I said can I have an hour to
fix that and they said well it's perfect
it was generated by a tool alright
that's as soon as somebody tells you is
generated by a tool like red flags
should go way up and then in the end it
turned out to be a single reduce by it
did so but then going back to the main
premise of what we're talking about
right the more you got there the more
pain you have to go through the more the
more steps the more i/o the more network
the more everything serialization
deserialization right it's all bad right
so if you can make your dag smaller you
you're a better man for that or woman
any questions on that one and I think
that's the number one thing that I do to
try to make things faster is simplify
the deck
so the question was where to rdd's fit
with reference data so the RTD so think
of the output of any given circle to be
an RDD right so this circle is kind of
an RDD and a process together
it's not a perfect diagram but it looks
really pretty right all right all right
so that was that I'm gonna go on a
little bit okay
now somebody mentioned about XML being
this is the next XML so so I was at a
conference and I remember the year I
think was like I want to say I don't
know it was probably like six years ago
and it was a it was Google i/o and I was
you know I was young I was building an
enterprise service bus at the time and
we were using J bosses BPM system and I
was I was gung ho for XML I loved XML at
that point XML was amazing yeah we
change opinions and then so I went to
this one talk about this one guy Google
and he was like I hate XML I want to
write distributed programming the same
way that I write normal programming and
he came up with this concept called
flume Java not to be confused with the
project flume and the concept was
essentially to use functional
programming to make an illusion of
distributed programming happen right so
I'm going to demonstrate what that means
because it's almost impossible to really
explain it properly all right so this is
a super simple Scala I'm going to show
you the job in a minute and then when
you see the Java you're gonna hear I'll
just show you the Java right that's why
I'm not showing you the Java right okay
because the job was big all right so
this is how you do word count word count
is essentially I want to count the words
in a page because in case you don't know
like when you're building search indexes
and stuff like
that the more times you see a word the
less important it is to the value of the
the indexing for that word I mean for
that document like do like the word the
has been really no significance and
indexing right stuff like that
so anyways word count now everyone can
kind of see what's going on here I'm
getting my first line I'm getting my
spark config then the second line I'm
give my spark context right I'm that's
kind of like what you need to set up
your driver and then the next line I'm
reading in data and that to means I want
to use two partitions which means what
we won't go too much into it but it's
using two threads across the cluster the
next line is doing a flat map a flat map
just means for any one entity give me
you know I get one or many back right so
if I get a line I want to get all the
individual words that's what it's doing
here and then for every word I'm gonna
say word and one is in a tupple and a
tupple is just two items together right
so it makes it really easy to do tuples
in scala verse Java and then the reduced
by key will essentially and I know this
looks really funky the underscore plus
the underscore it's a skull of thing
where essentially they're adding the
left and the right so if you get the
word more than once it adds the counts
from the left and the right and it
brings them together
I should drew a diagram of how reduced
by key works because it's kind of
fundamental if I had three records right
if I have three records and I have you
know cat 5 cat 3 and cat - well reduced
by key will do is take two of them first
and add them together so 3 and 2 will
equal 5 then it'll take the next two and
add those together and 5 + 5 equals 10
the really cool thing about it is it
only ever needs two at a time so you
could have millions but it's only ever
adding two at a time so that makes sense
okay and because it's additive and
associative we're all happy
doesn't matter which order they get
added in okay so what's so cool about
this see those green lines those green
things those are not executing on the
driver those are executing on another
note and in different threads and by
adjusting that number two or having more
data we can run this on thousands of
threads and I don't change the code at
all now come on that's not sexy right
that's sexy you don't have to think
about failure you don't have to think
about multi parallel you do have to
think about it but you don't have to
code it right any questions on this
because this is huge this is huge
yes so that number 2 doesn't have to be
there so let's say you the number 2 is
just something I put in there
number 2 is optional and and this is
going probably way out of scope for this
talk but let's say you're sucking the
data in from Hadoop right and Hadoop has
this concept of blocks well if you're
reading in the file the partitions will
be the number of blocks right so if your
data is a terabyte right it's I don't
know it's depending upon your block size
it could be thousands of blocks which
would equal thousands of partitions
right so that - yes
but I could I could remove that - and it
would still kind of do it but yes you're
right
but I don't have to change the green and
I don't have to I don't have to change
four or five six or seven or eight those
are all gonna stay exactly the same
is there a way to set a throttle as a
question that too is a way that too is a
way there's lots of ways that you can
tell how many partitions you want I I
could spend a whole thing on it you can
write this concept called an input
format which essentially says how many
partitions how do I make the splits can
I do any collocation can I do anything
like that right but most of those
already written for you so you don't
have to worry about them but if you're
doing something advanced you might
yes
you're perfectly right so you don't have
to worry about it because it's kind of
done for you but line four and five or
what they call map operations right map
operations don't worry about the word
it's meaningless but just map operations
means it's happening where the data is
or where the depending upon your input
format yes anything I ever tell you is
like ninety percent right there's always
a magic condition but most of the time
like if you're running on ass off of
HDFS or HBase the map will be running
right where the data is so there's a
zero network right it's off the data
doing the operation the reduced by key
obviously has to do network right but
it's actually pretty darn cool not only
does it reduce at a global level like
collecting partitions and saying cat
only goes here and dog only goes here it
also does what's called a map side
reduce or or I what's the right word
it's like huh combiner it does a map
side combiner which means that it will
try to get it's a limited amount based
on memory but it'll keep you know
thousands or ten thousand items and do
that operation on the map side but and
then only send out the summarization or
bigger chunks or yes summarizations of
the data to the reduce so that will
reduce the network right so there's a
lot of magic there to try to make as
little go over the network as possible
so does that make sense
no no they'll always end up in the right
places so whenever you do a reduce a
group by and order by a join anything
like that it's going to go over the
network now what I'm saying
reduce I join not Maps I joined it's
going to go over the network but how
much goes over the network is really up
to you right there's a lot of things you
can do to prevent that so reduce by key
is a big one to reduce how much goes
over the network
because I haven't learned them yet I
have to use them
yeah yeah yeah so which version did that
come out in 8 yeah I don't think any of
this works on 8 does it work on 8 yeah
but I mean this Hadoop and stuff is that
all tested for 8 you're testing it yeah
so I'm kind of pract I'm trying to
pragmatic when I deal with customers and
I do coding I I'm on one 7 right now so
that's probably why I didn't do the
lambdas yet so I'm always I'm a lagger I
got a lot of customers that are banks so
yeah I wait a little bit but I will be
there and that would that it'll make it
nicer yeah banks don't like new stuff
they don't like unpredictability one
more question that I got to get going
over more
ah that's a good point
um what are these things on the left
right so you might yeah your right my
left because Scala has this wonderful
thing so if you're new to if you're a
Java program and you see Scala for the
first time you look at this like what is
that because it doesn't declare anything
right it's just like ah this would be a
very bar it's a little bit like
JavaScript in a way but anyways so what
the difference is you have to understand
where it came from so the SC is a spark
context the Lyons is an RDD the words is
an RDD the pair's
isn't as a pair RTD the count that
believeth stole a pair RTD the word
count and then the local values is
actually a local collection right and
then oh yeah I didn't go into that line
seven and eight right seven is when we
take so it all came back to the driver
and line eight is all executing on the
driver right so that's happening on that
single node
their object everything but seven is a
spark object so let's go to the Java
version can everybody see that it's kind
of big so there you have your Java RTD
job already be Java pair RTD and then
you have a list whoa that was a big
difference um does that make sense
alright so that's like the hardest thing
I think I had to get over with looking
at Scala is trying to figure out what
these variables are um yeah it is what
it is so just for your Scala people
there's bars and vowels
right Val's you can't change bars you
can change okay so I want to show you
this one why did I want to show you this
one I wanted to show oh because the
Blues are transformations right nothing
is happening in the Blues like dag is
getting constructed but nothing is
actually happening right the red is our
first action right and the green is
stuff we're doing locally so it's not
spark at all right that's just regular
Java or Scala all right any questions
yeah No all right so this is the java
version without the lambdas I'm sorry
for being old school comes with being
bald you know you're an old man at this
point but if you're not using lambdas
this is what it looks like the greens
are again executing distributed ly I
think it's a little bit easier to kind
of visualize that with this just imagine
those objects get sent over the wire and
the work gets done on the other side and
the same concept here the Blues are
transformations the Reds are as an
action and the green is happening
locally any questions so far
now this is a lot yeah okay all right
before I answer before is this is a lot
now if you leave this room and you're
like this still a little bit confusing
it's totally okay just like a Victorian
programming you could totally read the
object programming book and be like I
don't get it
right this is you walk on it think on it
right try to write a little bit of this
it's easy but not easy at the same time
so I have him and I have him so do this
one first
so Green is what's running distributed
ly right white is what's running on the
driver and then this other one is about
blue our transformations which means
they're just altering the dag green I
mean red is actually when an action has
taken place and work starts to happen
right and then green is all local work
right no spark at all yeah totally the
first green those are all
transformations so yeah totally over
left yeah um so one thing if you really
get bored put like system that out and
in a system current time stamp or
whatever and do the times you know print
the times out the Blues will be like
they might even all be in the same
millisecond right but then when you get
to the red it'll take as long as the
entire dag took to execute okay there
was one over here
you know he's fine now I'm amazing I've
answered it through to the Leppa three
all right what else do we said yes
oh you can't change the rdd's it's just
an immutable object right yeah
Scala just they're nuts about
immutability it's just they're kind of
obsessed about it it's a it's a it's
their thing but Scala developers get
very mad at you when you make things
Barr try doing a commit now get get all
over you
all right are we good on that all right
so go home think about that it's gonna
take a while they totally baked in but
if you get that concepts your spark
master but you can see here just really
quick I didn't have to think about my
cluster I didn't have to think about a
lot of things as a regular Java
developer I can just get started you can
go home and get started today and start
writing spark and it's not gonna feel
that different than regular Java right
the other big thing is because the code
looks like this it can be run locally
for unit testing and distributed right
so and when you run it locally for unit
testing and stuff you can debug through
it through your IDE right it's really
nice really convenient yes this is not a
mate okay so what we're talking about
here is flume Java right flume Java was
brought up six years ago
then there was cascading and there was
crunch Pig kind of to an extent um
Beck stole those ideas from flume Java
and then spark the guy who invented
spark was an intern at Cloudera and he
stole it from crunch so everybody steals
from everybody and I'm sure that Google
guy stole from somebody too and it's
just the way it works right no the
concept of the language is not new
that's not what made spark spark
remember is all four of those put
together right it's it's the rdd's it's
the flume Java it's what I'm going to
talk about the next the long live and
stuff like that it's all of them
yes but you would be like one of two
companies in the planet but no there are
a couple people who use cascading there
are a couple so yeah
the the the advantage of using them
abstraction is you get to use an
abstraction the disadvantage of using
the abstraction is you're using an
abstraction the fact is you can't you
won't be able to get to the some of the
lower level or some of the cooler things
like an abstraction is nice because you
could code in one language and then you
can switch your engines but to you may
not be able to get to the full extent of
ML Lib you may not get to the full
extent of graphics you may not be able
to work with the spark on HBase
connector like there's this opposing
Kahn's right yes
should you be conscious of how you order
the code well you have to do the blue
before you to the red because you have
to build the deck you can't you can't
you can't ask for an answer before you
ask the question so you always have to
ask the question first is this the
absolute perfect way to write this code
no like there's many different ways to
write it but I think it says a story I
like to order it the same way the dag is
being done alright let me get on a
little bit more I'm gonna I'm afraid I
might run out of time
so we've talked about RTD so we talked
about DAGs we've talked about flume job
so the last part about spark that I
think is very interesting is this
concept and to you guys it's gonna be
like duh but to the distributed world it
was a huge event right so when when
distributed programming you know Hadoop
and all that really started in the open
source community
we had MapReduce right and MapReduce did
one shuffle right so it did one join or
did one group buy or did one count it
did one thing right
so normally you had to tie all these
together and you had to tie them
together with something like XML or some
kind of scheduler or a cascading or a
crunch or a pig or a hive so you had to
have something on top that did it for
you
right and then at every single step you
were the yellows are making yarn
containers so you had to go to a
resource manager and get resources and
get them locked down and then start your
JVMs and grab the memory and every time
you hit read you had to write to disk or
you had to send things over the network
and it just got really it you know it is
what it is and then what spark came
along and said well why don't I just
grab that stuff once from yarn and why
don't I just output the answers instead
of outputting every individual stage
right does that everybody understand bad
good
right makes sense right everybody's like
duh right if I have a pipeline why do I
keep giving back my resources and why do
I have to write everything to dis every
single time I do anything right a couple
things changed right Computers got a
crapload more memory between the
evolution of the two systems and we just
learned a lot from the first system in
his spark unique no sparks stole from
about half a dozen other projects to
make this right are there other projects
that do this
yes but spark has probably the lion's
share of the ecosystem at the moment so
the reduce will never be on this well I
mean one of the partitions will be on
the same note but all the other ones
will be in other nodes no maybe I've
made a bad image I did make a bad image
I apologize I'm putting an S snake next
to container so it's containers so all
the containers across the cluster are
requested at once it's not like they
requested them let go then requested
them let go it's just all of them are
done in once and you can have many
containers in fact we're going to talk
about memory in a little bit because
it's Java base and don't tell me that
Java 8 solves it because I've heard it
on Java 6 and I've heard it on Java 7
but may be right that I don't want to go
too much above 16 gigabytes worth of RAM
right on a single container because GC
is we love GC if I could say anything
that's gonna hurt Java long-term is GC I
think G in there it better be amazing
that's all I gotta say I think yeah and
I know I'm at JavaOne but I think that's
a fundamental thing that Java will face
over the next decade is that computers
are just gonna have so much room Ram
we're gonna we're talking about
terabytes with a ram on a single node
Java's got to figure out something
because you can't just have a hundred
processes on to get that memory we have
to have something better and off heap is
a solution but it's not the most op
but anyways that's my ranting in Java
even though I love Java
I love Java so don't quit your jobs it's
a lot of money in Java but I'm just
saying I want Java to fix that so
anyways the other thing is then the
people asked the question well if I can
have the container up for an entire ETL
pipeline why can't I have the container
up all the time and do all my pipelines
and never have to start and stop right
so some implementations of this are hive
on spark or spark streaming or some
people have made it so they keep their
applications up and they put a little
jetty server in the driver and they send
it instructions right so very very
powerful right any questions on this
so it's in another space but yes it is
doing it is doing some of so a BPM can
apply to many solutions right then an
Enterprise Service bus ETL process stuff
like that yes this is doing some of the
components of what a bpm could do but
let's put it in perspective right when
you write a query that has sub queries
you don't need a BPM system to
orchestrate which join goes first and
right same kind of deal there like get
out of my house
my house is here and if I have my if I
control my dad and don't give control of
somebody else there are things that I
can do in terms of efficiency and
recovery that I can't do if I give
control to somebody else so does that
mean a BPM is gone no there's tons of
things outside of a single ETL process
that need to be orchestrated right but
within the scope of a flow SPARC is
saying if you give me control I don't
know what that was if you give me
control I can do better than you can in
a can
but that I mean it it doesn't replace it
just says it's it's broader boundaries
yes and that's in about six more slides
so I'm not gonna talk about it yet all
right so can you scale the application
while it's running we'll get to that in
a minute but let's move a little bit
forward because I'm running out of time
the meat of it is managing parallelism
right so we talked about shuffling we
talked about all these things
I might have to speed up a little bit so
I'm gonna go a little short in the the
questions so if we were to do some of
these jobs with the single thread it
would be really long right and if we did
them with six threads or a thousand
threads
it would get a lot shorter that's the
whole concept that's why we like spark
right and these are all equal distance I
actually measured it so you can check
that so but one of the concepts you have
to understand is whenever you start and
stop any of these things there's cost
right and bad things will happen if you
split it up too far right and there's
not just bad things about in terms of
time there's bad things opponent driver
memory there's bad things and firms on
intermediate temp files stuff like that
so it's you have to find a balance and
it's very difficult because there's no
great answer for you but maybe some of
this will help right
well actually wasn't only three slides
so here you are so you get two so one of
the things is when you when you start
your executors you get so many tasks
yeah so many threads can run in an
executor and that's when you give that
you give an executor how many cores do
you want right but you may have more
partitions that need to be processed in
the cores that you feel allocated right
now today you can do fixed allocation or
dynamic allocation right and because I'm
running out of time I'm not going to go
over these too much but in fixed
allocation you tell it exactly what it's
allowed to have in dynamic allocation
it's kind of like the Java Center never
been anybody use the the thread pool
executors in Java where you say start me
with these mins and increase me and but
don't go above this same deal right
everybody steals from everybody else
same deals right if I need more cores
give me more cores if my cores are idle
go right so what will happen is let's
say you have you know eight cores but
you have all these partitions they'll
all get in line to go through those
cores but with dynamic allocation it
recognizes that you need another
executor and it can move some of that
work over to other executors does that
make sense yeah and you don't have to
think about this and that nice nice it's
good not to have to think about this
stuff all right so that's one right
picking the right number of parts and
it's tricky and you'll have to play with
it and it's one of these curves right
where you'll hit your best performance
and if you go left or right it's not
going to be that much different but it's
way better than one partition and it's
way better than a billion partitions
right there's somewhere in the middle
that works right for you I like to align
my partitions with the number of cores I
have that's a rough place where I start
all right so the next thing and this is
the death of everyone is the evil skew
and this happens so much if I come in
and now I've told you about this if I
come into your company and you pay me
$500 an hour and I tell you your problem
is skew bad on you so skew is when one
partition has all the work so you've
spent a million dollars or whatever it
costs today and you bought a thousand
node cluster right and then it didn't go
fast enough and you bought a 2,000 node
cluster and then you give me a call and
you say I hate your product because I
buy more nodes and it never goes faster
and it's because you're just using one
CPU and it happens so often and this is
where I get to go in and I get a 400
hour job and I change one tiny thing and
then it goes down to 6 minutes and I
look like a hero and I collect my $500
an hour or whatever they could charge
but that's all it is right and the
solution is super simple right so this
is the this is a starting partition
right and we talked about in the HBase
class earlier you just salt it the magic
of hashing does everybody know what a
hashes does everybody know what a salt
is
it's just a it's a repeatable kind of
random it's not random but it's kind of
random value and what you're essentially
saying is yeah so if you were doing
something by the alphabet like a
dictionary right there's a lot more SS
and T's and there are Z's right but if
you put a hot assault in the front of
every word and you said the salt could
be this hash mod value then every word
is going to have something between a
zero or a nine in front of it and it'll
get perfectly distributed between ten
things right so this is very huge so
just by doing a hash mod of two we've
you know we've gotten a lot better if we
do a hash mod of eight it's almost we
have zero skew left right so it's
something you really need to look at
another type of skew is when a single
word is huge and then you'll have to
break it up kind of like if you're ever
doing stock trades I think it's like
Apple in this one spider fun take up 20%
of all the stock trades on the stock
market you'll have to maybe count or do
Apple processing in a couple different
nodes first and then bringing the
answers together something like that I
only have six minutes so I got to go
quick the next one and if you do this
don't do this it's Cartesian joins
Cartesian joins are many to many they
are bad there are ways to not do them
don't do them and I don't have enough
time to talk about it but don't do them
if you can avoid doing them and
essentially even a 2000 note cluster
won't work at some point because
Cartesian joins are exponential and your
cluster is linear and linear is not very
good against exponential actually not
nothing is very good against exponential
but there's different ways to approach
it right
there's nested structures there's
windowing and there's reduced by key
there's lots of different ways to
approach it
alright trying to make a little bit more
headway so I can get more time for
questions the other concept is
repartition
so let's just say you had a scenario
where you were doing like validation or
transformation and the individual action
was actually very expensive per record
let's say you can only do 2,000 for a
single CPU could only do 2,000 records a
second because your validation was so
expensive but
you had a million records and you needed
to do it fast what you can do is you can
redistribute that data to n number of
partitions and you can it'll spread
across your cluster right so here's a
simple graph where I put in a two
millisecond wait to process a record and
I split it up from one core to 32 cores
right makes total sense
it'll get faster because I have more
cores so it's just something to think
about I'm not gonna go into this because
I'm really running out of time but
there's a really great article about
Sandy rise talked about how to get spark
more optimized and stuff like that and
one of the things you have to think
about is memory so you only have so much
memory some of that memory is going to
be going to the shuffle some of it's
going to be going to your rdd's some of
it's going to be going to your own code
and some of it's going to be going to
overhead because you know how Java loves
to stay perfectly into the amount of
memory you give it that was a joke
because it doesn't it it expands or
contracts by like 20% and you just have
to be cognizant of what you're using
because if you're going to start
stepping over your bounds and start
using memory that you told Sparky could
have bad things can happen right so
there's another pet peeve there's a Java
guy watching be really great if we could
estimate memory usage a little bit
better but I know that's not gonna
happen but anyways um all right extra
stuff another really easy way to two
easy ways where you can make your spark
tubs really fast one is use broadcast
correctly broadcast small stuff don't
broadcast big stuff your broadcast big
stuff you're in trouble right because
not only is big stuff has to fit in all
the executors that one driver now has to
send that broadcast to everyone right so
very expensive the other thing is group
by verse reduced by right we talked
about reduced by key it only has to have
two entities in memory at any one time
right group by key so let's say you're
doing the counts of cat if you're doing
reduce by key you only need two of those
at any one time in memory if you're
doing group by every single cat
has to stay in memory and you're not
just an old cat woman now now you're out
of memory with an out of memory
exception and your bosses Matthieu so in
my advice if you can avoid using group I
don't use group I use reduce by key all
right so you should be as lean as you
can be on memory I think I think that's
it yeah I finished with two minutes to
go
any other questions spark there's
there's three major schedulers right now
there's yarn maysa and it has its own in
the end it doesn't really matter the
reason why I use yarn is because I I
work for a Hadoop company and yarn works
best with Kerberos but that doesn't mean
meso s-- might not do it in the future
but it shouldn't impact your performance
because as we talked that your executors
just stay up right
what is the best way to build a lab I
just download IntelliJ and get a laptop
you can do everything in two and local
with unit tests and everything like that
and then when you're ready to have a
cluster start with a three node cluster
and if and then the other one just
whatever your budget is you could but
you won't
I don't use VM anymore because I don't
have enough memory on my laptop right I
have I have 16 gigs of RAM it's not
really enough to do 3 vm's really well
so I would go get some instances from a
cloud provider or something like that
yeah yeah so javis color and Python so
it goes Java's first-class citizen I
mean I'm sorry Scala's first-class
citizen Java second-class citizen Python
is a far-distant
and I am even wondering personally which
will get more love or or Python in the
long-term future so in terms of pandas I
don't know if pandas are in spark yet
but cloud era has a panda's
implementation you can do with Python
that that we have but I don't think it's
in spark core yet
I don't spark dataframe is a little bit
different than a regular data frame you
would have to convert it to a local data
frame what do I think about spark sequel
so I wrote an email to cloud era I think
two years ago that says we must support
spark sequel there are tons of sequel
engines out there and wherever you know
we don't have unlimited resources so we
have to pick which ones we want we've
decided to support I don't think we
officially support it yet but we very
well very very soon sparks equal and
Impala and hi those are our big engines
right spark sequel i from going through
the lifespan of now three sequel engines
and oracle would love to hear this
building a good sequel engine is hard
right getting a sequel engine started
and getting pressed that it works is
easy
I think spark sequel is good I think it
will get better
and that's the best way I can say it
I love ml live oh my god because I used
to have so what do I think about em all
up
I used to have to work with my how
anybody work with my house oh my god
that thing was terrible terrible I think
the problem with me and how it was it
was developed by statisticians not and
not didn't have like an overall
engineering aspect to it and everything
had a different interface everything was
very different with ml lib I went to a
client last it was last week and I
didn't know half the functions I was
having to implement I am lament
everything in one hour it was that easy
like I did Bayes 990 Bayes I did k-means
I did random forests I did like three or
four other things I didn't know what
they were and I just worked and it's
because the interface is so simple there
there you just have yeah so I mean I I
think it's a clear winner right now I
don't see anything really
I'm sure there's still lek there's still
a lot of development in it
I would say if your functions not in
there see if it can get in there from
what I understand what I'm talking to
people the vast majority of stuff is
already in there like the stuff people
use every day but not everything it's
going to take a while and it also
depends upon demand
so MATLAB is a company that has
companies paying it to build functions
where SPARC functions only get built
when people want them to be built now
right so if you want them to be built
like if like if you were with someone
like Cloudera and you would say I need
this you just make that part of your you
got to link it back to money
somebody's got to build it right it's
either if somebody's got to build it for
money or somebody's got to build it out
of passion so unless you can convince
other people to have passion or you have
money it doesn't get done any other
questions all right nope
totally different
oh you mean sparks streaming Oh sparks
streaming first storm so storm I'm gonna
I said this one I said storm is dead um
storm is dying that's more politically
correct okay storm is dying Twitter kind
of gave up on it quit Twitter doesn't
use it anymore
it was a system that was built for
inaccurate counting almost like a decade
ago at some point I don't know the exact
date there have been a lot of
advancements in stream processing
technology since then spark streaming is
definitely one spark streaming you can
build topologies without building a
topology that dag is a topology right if
I wanted to make that I don't have it
with me on this slide but that
word-count I could turn that to spark
streaming with only two lines difference
right
yeah yeah similar thing here the big
thing you want to use is you want to use
Kafka and it can also throttle it with
Kafka and stuff like that spark
streaming has much higher throughput
than storm so that's another big thing
and it can do exactly once operations
I'm not sure the exact state of the
dynamic allocation but the dynamic
allocation with spark streaming is
supposed to be a thing I don't know if
it's out at this moment but it would be
the same thing we talked about it grows
and shrinks I haven't used it yet that
doesn't mean it's not there yet I would
just double-check yes yes so if it's not
there today will be their version
because I know I know they were building
it are we done</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>