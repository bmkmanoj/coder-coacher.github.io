<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Next-Generation Hadoop: It’s Not Just Batch! | Coder Coacher - Coaching Coders</title><meta content="Next-Generation Hadoop: It’s Not Just Batch! - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Next-Generation Hadoop: It’s Not Just Batch!</b></h2><h5 class="post__date">2015-06-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ShUQyVeGCVQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi everyone and welcome to next
generation Hadoop is not just bad so the
goal my talk today is to look at changes
I've been happening in Hadoop over the
last year or two and look at the changes
that I connect helping Hadoop support
not only bat which has been its main
stay up until today but also look at how
we can now start enabling interactive
near real-time and real-time processing
so here's our agenda we're going to
first bendel a few minutes in beginning
just talking about why Hadoop is
relevant we specifically focus on some
of the batch side of Hadoop and then
we're gonna look at some of the
challenges in the hedge it one
architecture who Duke one is a current
GA version of Hadoop and after that
we're going to look at changes our
upcoming and Hadoop to how they're going
to fix some of the challenges that we're
going to em talk in a previous section
and then finally and they're kind of
meat of this talk is really in the
blueprint so they're we're going to look
at how in Hadoop to and the changes are
connect riving innovation and
specifically enabling different types of
technologies that can I be integrated
with Hadoop and provide some of these
real-time mini real-time facilities that
you know using one more of these days
I'm going to be covering a lot of
different technologies in this talk and
don't worry about writing them all down
and making notes because my blog which
i'll update at the end we'll have a
reference to both the talk and for each
technology cover will have a number of
em links that you can go click through
and find out additional data points so
this is me my name is Alex homes I'm a
software engineer I've been working with
disturb your systems now for more years
than I thought I care to recall I've
been working with Hadoop since about two
thousand eight and 2008 I was working on
a distributed and web crawl search and
index and project and Hadoop was kind of
really the only way that we could do
some of the etl type activities that we
had em on that project and does my
Twitter hashtag and I'm blog as well
okay so why do why was about Hadoop that
so compelling so we live in this age now
where were commonly
never throw out your data right and and
the reason for this is because the this
hidden untapped gold and your data that
you don't know about today and if you
throw away the data then you can like
I'm you know and removing the chance of
the future you you know going back
looking at your data and and finding out
these connect untapped resources and
which which can hopefully you know give
you a business some competitive edge
over over your competitors so data
really falls into two categories there's
a kind of obvious kind of data that we
have we need to work with it the data
that you know drives our businesses so
if your google for example and web pages
and being able to know them is can I get
your data because otherwise you can't
research if you're a bank the
transactions that people are doing
online and need to be connects toward
obviously if your social and if you're
in social media people uploading videos
tweets all that kind of stuff needs to
get stored so that's kind of you know
the obvious connect and data that we
need to deal with but then as I can like
more subtle data em so clickstream data
is a great example of this so if you
have a website for example and you have
a search facility or not if you're not
capturing clickstream data you have no
idea how effective your search is right
i mean people do searches the results
get presented and if people are
abandoning results and because they're
not useful or they're scrolling down and
clicking on the second or third page you
don't have that information unless
you're capturing so there's a whole
suite of em data which we may or may not
be thinking about if we're not capturing
that but can like losing out valuable
information so i mean the way i connect
visualize this it's kind of like a data
mountain right we've got a lot of data
and many systems today even the ones
that report you know supposedly big data
system just can't deal with the kind of
volume we're talking about here so in
terms of the kind of things we need to
be able to do that data I mean clearly
the facility restored but then we need
bill to join I mean think about data
that exists in your relational databases
and you know sequel stores and you're on
your web servers and clickstream data
these are all desperate data and
different data formats and being able to
join them and enduring a scalable
fashion is
is important and being able to index
your data I mean increasingly and work
reading a book but we have search
facilities of our data and much like
Google does providing analytics
providing insight into out and you know
data researchers are data scientists
analysts and surfacing a data and easy
ways to help visualize it and in to work
over and being able to aggregate data
the raw data we need to store the raw
data but that's not much used for human
consumption we need aggregations or that
data to help us understand what's going
on systems and then finally a lot of
data it really isn't graph form so being
able to do some kind of graph processing
I have a netgear built is important so
this is really am in a hadoop really
solves all these problems and and and
what if we step back is google that came
up with the technology to begin with so
google created a book of our system as a
way to solve their storage problem and
it's basically a distributed fault
tolerant and project and and and the the
key thing here is its fault tolerant and
automatically so many when you store
data across different nodes and node
goes down and the book of our system
will automatically and rebalance and
blocks over on on existing nodes so it
will automatically make sure that your
data is available at the replication
level a specified and am google also
created something called MapReduce so
MapReduce is a shared nothing scalable
distributed and process and processing
framework it's rooted in FN view of
functional programmers and most
functional languages have map in reduced
concepts that's really where and
Google's MapReduce and came from the
share some properties but and the have
some differences there but but the key
thing here is that a shared nothing
models so I'm each map task works on a
subset of the input data and it makes
some output the shuffle is where the
magic happens and not produce which
determines what I approach to go to what
reduces and reduces do things like
joining data transformations before
producing the final outputs so Google
publish these papers around 2004
frame and duck tolling in my car ever
heard the term about notch which is the
open source web search and crawl system
they saw this they had scalability
problems in the open source project they
saw and the papers at Google published
they implemented them that was
internalized into yahoo at some point
and then they open sourced at not became
I do so they were placed and the GFS
with HDFS which stands for Hadoop
distributed file system and they've got
a cute elephant logo which I quite like
so so the key thing here is am now you
can do a whole bunch of things on top of
MapReduce you can do your pagerank graph
and processing you can do indexing you
can use higher level m dsl like
languages on top of my produce to do an
electric type functions so you know one
one of em things that I find very
compelling about Hadoop is a factor it's
it's kind of like Java I mean the reason
we all use Java today is is not only
because the language is simple but
because of the extensive tooling support
both IDEs and tools as well as the Agni
as well as a rich connect third party
ecosystem around and Hadoop is much to
see in me you have the Hadoop core which
is my produced in HDFS and then on top
of that you have various abstractions on
top of em em Hadoop's you have hive and
pig and like em sequel and and
pigs on dsl you have and cast you have
em closure and scala interfaces well on
top of it so if a very rich and vibrant
connect em layers on top of it and then
and beyond that you have and things like
and data integration adapters you've got
ways of pulling data from em sequel
systems and to Hadoop you've got you've
got ways of doing scheduling scheduling
MapReduce jobs if you go administrative
tools such as a minbari and Hugh and
machine learning and some real time
tools that we're going to look at today
as well so what are the canonical use
cases for hi drew well for me and it's
really etl it is a big one here so with
et al you're typically dealing with huge
data volumes you may be able to connect
move them into your ETL system you need
to work with and data adapters for the
disparate data like we kind of talked
about earlier and doing enemy joins
coordinating all of that I mean this has
really been acting like dirty em task
that's been done before Hydra came along
it's just be nice every company's had to
do this if created their own homegrown
solution it's been complex hard to
maintain and once the group came along
and people started realizing this was
like a useful system to help solve some
of the complexities with the ETA custom
ETL solutions people started using
Hadoop and in a big way and again the
key thing here is a scalability of
Hadoop the fault tolerant and
capabilities it has and the rich
ecosystem the fact that there's already
mechanisms are to move data in an hour
effectively using and your other
external data sources that you likely
have in place I'm data by housing is
another big thing that Facebook have
been doing with Hadoop I mean there were
really the pioneers and using Hadoop as
a data warehouse and that's one of the
reasons that they created hive in a hive
being a sequel on Hadoop and facility so
that it could they could externalize it
and due to the interim and researchers
and system of record is also an
increasingly common use of Hadoop
episode HDFS storage to you I've been
working with Easter presence doesn't age
and I've never yet encountered a problem
we have lost a Don HDFS HDFS is a really
robust scalable system and and as such
people are using it more and more as
their single truth of data just because
of amount of data it can store and under
sly ability um because of the connection
nothing and am nature of MapReduce and
Hadoop is also a great batch ingress
egress an example use case here would be
lets you have a no sequel system like
Cassandra Voldemort and you need to
embattled Atta into my produce is really
a great way of doing that because again
is highly paralyzed and and oftentimes
you'll find you need to tune down the
powerless MapReduce because you're
likely overwhelm whatever system you're
writing to you and the connect tooling
around research business intelligences
is connect only growing in a group
community as well
alright so that's my connect basic and
fro enter into Hadoop and what are some
of the challenges with hooded one who
Duke one again as a current GA version
of Hadoop over one BGA for too much
longer so the first problem is that
Hadoop really only scale to around 4000
nodes now this sounds like a lot right
but I'm folks at Yahoo Facebook and
other you know an ebay and other
organizations if they've had this limit
and you know at some point they need to
have multiple Hadoop clusters because
they just they can't have you up just
doesn't scale and the reason here is
that there's some places in the master
demons and Hadoop to have some coarse
grained synchronization which was kind
of MP limits and pizza ability to kind
of scale beyond this number of nodes and
a second hit thing here is really Hadoop
has got a notion of slobs so slaw as a
very rigid thing and Hadoop so as I do
an architect when I'm kind of setting on
my brand new head of plaster I you
specify for each node how many parallel
map and reduce tasks can run now em but
these settings are static and a
predefined so we often find happening we
out is when you're running a large
MapReduce job over your cluster and you
get in towards the end where you're only
running with juices and there's no map
is all the map tasks all the maps lost
in your system are just sitting there
unused so if you look at the resource
utilization of at your typical em
MapReduce cluster it kind of goes in
waves like that according to what kind
of what stage of my producer running and
so so I mean and long story short you
know the reeses utilization and the way
it's done and Hadoop one isn't the best
and then finally the big you know
elephant in the room if I can you know
be so bold us to use that phrase is
really the fact that MapReduce is bad so
when you're running a MapReduce job it
takes around five to ten seconds for it
even starred and it's not even done any
work at that point it's just you know
it's just getting started so when you
think about em doing interactive
transactions with your system like in a
running sequel queries nice
things maybe not the best tool to be
using and finally this is kind of like
you know a step back from Hadoop and
more just about our data systems in
general and you'll be live in a world
now we have these connects eyelid
verticals right we've got relational
databases the Hadoop stream processing
systems we've got no sequel web
applications they're all running on the
all running on hardware and they've all
got our own data but oftentimes me to be
able to share data across your systems
and mostly systems don't know how to
talk to each other so at least in my
experience um you know working big
organizations you have these big data
move projects you know you're going to
have a whole project around moving data
from your relational database into
Hadoop of them do and some other project
you've got never architects into our
project managers it's just a big fiasco
and then even when the projects you know
kicks off you're actually starting to do
the data move oftentimes you're
saturating some network pipe and you
know causing some outlets to some other
server sir you didn't realize was using
the same network pipe so you know bottom
line here is that we need to be able to
share data and effective and simple
mechanism and doing across a network
maybe isn't the best way of doing that
and oops so what's new and Hadoop too so
here's a good one I can review Hadoop
one as being like an operating system
where you can only run like one type of
process you can only run like said orc
or something right when MapReduce is our
only and processing framework is
available in Hadoop one with Hadoop to
what's happened is that the scheduling
part of MapReduce has been retained but
everything else has been moved out of
Hadoop core so what that means is we now
have a general purpose and and what was
being called a big data colonel now we
have a mechanism by which we can run an
arbitrary applications on Hadoop still
leverage a common storage and fabric
across all these different applications
and what's happening now is that
MapReduce has become an application
along with enough another and a whole
suite of other applique
asians some of them are clinically
attend- real time we're going to we're
going to take a look at some of them in
a second here so looking at how yawn
works so as a client you're going to
submit a yarn application and the first
component here is a resource manager and
the reasons manager it's one and only
job its job is to arbitrary available
cluster resources so it's going to go
and it's going to see ok I'm going to
create a MapReduce application so using
a use case of submitting a MapReduce m
application here i'm going to i'm going
to create a MapReduce application master
an application master is specific to em
each application type so from app you
know so and there's two important things
an application master needs to do the
first thing is it's going to be asking
the Wizards manager for resources in the
in the in the cluster and so as part of
that request is going to ask for you
know the host I want to run it wants to
run these and resources on and also
specifies now how much CPU and memory
and requirements each process is going
to have and then the second thing that
application mass is going to do and so
when the resource manager asynchronously
notarized application master okay here's
here's some resources available for you
application master Dane is going to talk
to the Nord manager so the node manager
is a demon process running on each slave
node and its sole job is to simply
create processes for application masters
monitor them and then let the resource
manager know if anything goes wrong with
them so it's going to go ahead is going
to create a process for us and then once
that happens the application master can
choose to talk to the container but this
is now all I side out band of Hadoop
itself this is all look like
application-specific RPC f you choose to
do this so in the case of MapReduce and
this container that's going to be
creating will be map and reduce
processes so what we really have here
now is one who do cluster with common
share distributed storage but were
distributed scheduler they can just
schedule arbitrary and applications and
arbitrary resource of an application and
we have many types of applications that
can now coexist on the same Hadoop
cluster alright so now that we've looked
at how do too let's look at some use
cases and that would have historically
been may be done on MapReduce but now we
can maybe look at redesigning we
architecting them with some of the new m
systems are starting to appear isn't on
Hadoop yarn so in this section we're
going to cover five key elements here
we're going to look at em no sequel
stream processing braff processing
cyclone Hadoop and then some calm the
data formats at the end alright so for
the first blueprint we're going to look
at no sequel with HBase and and the
reason we're going to look at this is
that because we're going to look at an
example of where you would have done a
high latency indexing using MapReduce
and we can we'll look at how H base can
be used to solve key parts those
challenges we have there but before we
do that I cannot want to give a quick
background on white space is and why
it's a useful technology so a space is a
implementation of Google's big table
it's it's a distributed system it's
available to em thousands of nodes and
can store petabytes of data and it has a
notion of tables and each table can
store billions of rows and each Vokoun
of millions of columns so it's highly
scalable so let's look a little bit
about the architecture and see why you
know why is a compelling technology so
there's a couple of concepts here first
of all each slave node is running a
process called an eight regional server
which is a demon process for ma space
and each each region server has one or
more region and it now a region best way
to think about region is you have a
table in HBase a region is a number of
contiguous rose and in that table and
each region uses HDFS to store files and
it's reason is going to have one or more
files in HTF
is now by choosing to use HTF s they've
gained a couple of things for free first
of all you know the whole fault tolerant
and availability of HDFS which is great
and by the fact that matter is as you're
writing as the excretion servers writing
these and regional files into HDFS
they're automatically being replicated
to other nodes transparently a space
doesn't even know about it and then if
we fatally lose it snowed for whatever
reason we can we can't resuscitator we
can spin up the h region server on
another node and it can pick up from
where this MH regions have a left off
but I think more interesting it or not
even as a fact that you can actually run
my produced jobs over your H based data
and and MapReduce is smart enough to be
able to schedule the map tasks such that
they will read locally so I will say
okay you're running this MapReduce job
over this data and I know where the
regions are and i'm going to schedule is
map tasks on the same map map and
processes on the same slave node and
they're all local reads they've got very
strong em dead locality properties with
with a MapReduce and HDFS here but I
think what's even more compelling and
this is something I all the time is
again coming back to the use case but
being able to load data efficiently into
no sequel stores the fact that you can
run a MapReduce job and the reducers
will also have strong locality semantics
here so reduced processes will again one
in the same slave node they're going to
they're not gonna have to talk directly
to a state space they're going to write
directly into HDFS and then the H region
server is will be able to fold them into
the existing regions so very very very
efficient way of loading data and Tate
in HBase sure oops
what you're saying if I understood
correctly reducers so the reduces em and
so am the register so basically there's
optimization we can talk afterwards
about this optimization away with the
partitioner you can actually partition
records so that reduces will get all the
records a lock that unknown to be local
to this region's residing on the same
node yes yeah okay and then finally
there's a whole new I mean an
interesting develop in the last couple
of months there's a project called hiya
it's oh why a I'm so far em a space on
yarn and what it does is actually em the
H region server is now within a yarn
container and I'm the yarn application
master now can coordinate with with the
young node manager to manage these
containers and the great thing about
this is that typically with HBase you
have a monolithic HBase deployment where
you know multi-tenancy with no sequel is
always a challenge you've got multiple
projects this this fear around security
and data sharing all these kind of
things this guy writing your guys data
the great thing about my as you can
never have multiple HBase clusters in
100 cluster so you've got you know
strong data isolation properties strong
resource isolation properties and and
you've got a very dynamic way of
managing these clusters and if we look
at the a space data model and real quick
you know one of the kind of em unique
properties here when we compared to
other no sequel stores is that throws
are actually stored and lexicographical
order and so if we compare this to
something like Cassandra for example
Cassandra at the row level orders
columns HBase orders rose and then the
second kind of concept here that's worth
em both kind of calling out isn't way
from a column family so Colin family so
each which is a corner store right so
stores connect em data and you know calm
on come to you can order and I column
family is simply a way of thing you know
these these columns are connected and
pickle read use cases I have my
application so they'll be persisted side
by side so it's a very efficient way of
connecting reads into a space and then
final thing here is the final
interesting property by HBase the data
model here is a fact that and you can
actually have multiple versions and five
sales so for free you can you can have a
historical track of what has been done
at a sale level and HBase so let's now
take a step back and look at how we
would have done something like indexing
and MapReduce and then look at how we
can swap parts out for a space so here's
our system we've got a crawler it's
downloading content it's writing into
HDFS we've got a MapReduce job that's
doing a number of things creating
inverted index and pushing out some
external search engine like solar cloud
for example so there's latencies here
right i mean there's latency like we
talked about MapReduce there's also late
in season moving data into HDFS for a
variety of reasons and then the fact is
that our data is an HDFS right and we've
kinda close the door and other and more
real-time systems having access to our
data so how do we replace or solve some
of these challenges here so um the step
one would be to em move em a space of
the pic HDFS are the picture and have
the koalas write directly into a space
and now we can do is we can still run
MapReduce over a space to create a
search indexes but we can do on a much
more efficient function and much more
rapidly and even better you can use
something called core processes and
HBase core processes are kind of like em
triggers an algae and relational
databases whereby you can have code
sitting in in a space itself and that
gets executed is part the right path
into a space so it's part of the right
and you can do some basic ETL and then
write directly to your to solar cloud
for example and now by doing this for
free where you've gained ability is for
other tools and to actually now have
real-time access to your data so
a couple other projects em both thinking
about here if this sounds interesting to
you accumulo who hears hard-working
middle I'm gonna okay couple you guys um
so accumulate this very interesting
story behind this so when Google and
published her back to the big table
paper and the NSA and read it focuses
really interesting and they were ahead
and actually implemented the internal
with an NSA and obviously building in
security features stare level security
was important to them the center at some
point em got wind of this and there's
this big Senate hearing around a key
melot and the Senate was upset because
NSA I hadn't used open source and I
decided to write their own connect no
sakura store so that that's you know one
of the reasons that m accumulo was
eventually an open source i actually
available now under the apache and
projects elephant DB who hears sort of
elephant DB yeah i forgot so it's a
really new project it's only come out
and recently and Nathan Mars he's got a
book coming out on mining called big
data and if you guys haven't heard of
that it's his vision about how you
confuse real-time and batch processing
together and and one of the connect
assertions that he makes and his book is
that no sequel systems I connect hard to
reason around so he solves that by
having this batch here that basically am
you know a few which is obviously high
latency like we talked about a bunch of
it I can basically generate the same
data that you're generating a real-time
layer and then push the editor system
like elephant DB elephant DB is a
read-only key value store where it can
read the only source of data for
elephant DB is out of HDFS so it so
efficient way to connect load data from
HDFS into a real-time MDM data store
definitely worth checking out so some of
the use cases for a space and friends
here and capturing system metrics
facebook investing lee i mean they're
also the ones i am broke cassandra in
the first place and they picked HBase
over Cassandra and for several internal
systems and primarily because some of
the
and stronger and consistency guarantees
that a space has over Cassandra and
friends and content serving something a
big obviously Google em design big table
so they can serve you know gmail and
other other similar applications okay so
who here has doing any stream processing
you again okay miss a few guys okay
great so in this section we're going to
take a look at you know why stream
processing is as important and we'll
look at storm as in the reasonable pick
storm is because it's one of the more
mature solutions out there and then
we'll look at how storm can be
integrated with Hadoop as well so why
stream process and why is this useful
thing to do well there's this data
entering our systems in real time under
certain decisions that we do want to
make in real-time aggregations for
analytics or real-time fraud detection a
normal detection and these are the kind
of things I do you want to do as rapidly
as possible and the port up until now
and most organizations has been some
custom message queue and rocker type
model which is inherently complex it
just requires a lot of engineering and
type projects to to maintain and and
manage your systems so stream processing
is a connect a new set of technologies
I've kind of risen in the last year or
two and they can abstract away all the
dirtiness of distributed computing for
you take care of making sure it's highly
paralyzed and then offer you connect
simple programming models on top of on
top of the streaming so and we're going
to talk about storm today I'm again just
because it's one of the more mature
systems out there um so Twitter hard one
of these kind of custom message queue
and Waka type models and tunneling they
got real tired of just maintaining that
so they actually created storm and it's
now being open sourced and maintained by
Nathan so let's take a look at how we
would use it and so example we're going
to use yours trending search so where
Google and we want to create a service
like the zeitgeist service right which
in
shows you the most you know topical
popular words that are being used and on
in such right now so we've got an
application and within where search
terms are coming in and we're going to
stick it on Kafka which is distributed
and pub/sub solution and then the first
am storm component we're going to talk
about here as an ocean by spout so the
job respiro is to basically create
streams of information and and typically
spouts do that by M connecting to
message queues pulling pulling data off
them and then and then pushing them
toward out pushing them downstream so in
our case we're going to get your search
phrases and we're going to tokenize them
by words we're going to create these
downstream streams of of these words and
then we're going to push them to
something called a bold I bought it for
a small phone you guys are the back for
I can't see that but I'm the thing the
fluffy cloud there is called a bold so a
ball is simply am a unit of rock the way
of doing work and in a boat you'll do
aggregations and you can do joins and
filtering all these kind of things so in
our use case we want to we want to
create I can a topical count or I can
like a time-bound it count for each word
so we're going to create a sliding
window for each word and and maintained
in the current bucket just keep bumping
up incrementing a currently every time
we see a new word so a question you
might have is well how does parallelism
work and storm so what ends up happening
is that we have multiple instances of
these bulbs and making me different
threads on different different horse all
together and when you're when you're
specifying your ball how these tuples
from the spout should be routed to you
you're actually special you can you can
tell it you know em that you want to do
some field field routing what that means
is that it's going to do a hash mod on
the contents of the triple to determine
what what ball is should go to and then
finally we're going to about I can make
an aggregation ball here and was going
to maintain a top ten list of words and
the frequencies which and then can
periodically by the data out to hbase or
whatever other external system you want
to write a true so um
I was interesting here is mu has been
using a storm quite a lot and these for
various things and such as fraud
detection and trending topics like we
just talked about they wanted to
integrate it into yarn Yahoo's been
yawning yawning at least a year no and
pretty much I think all the classes are
young running you on at this point and
they wanted to be able to em both run
storm on yon as well as use em HDFS
within the yarn streaming flows so the
added two features which storm didn't
have a defeated to do auto scaling and
ability to access data and a secure HDFS
Arab okay so and what are the systems
are the outdoor well Samsa and which has
been used internally by Lincoln was open
sourced not long ago I think within last
month and it uses cough cough for an
driving messages and has many the same
concepts a hovering storm more flying to
something that's even more recent this
is coming out of cloud era and this is
you know if we the way to think about
more flames is Samsa and storm are kind
of like solar by the distributed systems
and morph lines is kind of like listen
it's a library but so lively that
enables you to do streaming activities
like etl and the nice thing about the
morph lines is that they have plugins
for flume a space and MapReduce so where
you can actually do is and if you're
using flume for example to move data
from your servers em to HDFS and you can
you can actually have a tea in your data
and you know you can write out your data
into the HDFS then you can use a morph
line and flume to actually do some etl
and then in near real-time populate a
solar cloud server for example so so I
kind of a powerful way to to connect
push data to other m excellent systems
so M use cases we talked a little bit
about trending topics there's also
aggregations and interestingly Google
and recently published their paper on
how they do streaming with again within
the last couple of weeks and called
Melville and one of the use cases they
talked about in our paper
for stitching images together for the
google street view okay so let's talk
about graph processing so um a lot of
our data is in graph form when you think
about you know the internet and web
pages and links between web pages
that'sthat's a graph and when you think
about users in a social network that's
also a graph where where they connect
interconnections are obviously
relationships so what we're going to is
intersections we're going to look at a
particular graph algorithm which is M
page rank and look at how we could
implement that in MapReduce but then
also look at some of the challenges
there and see why you know some other m
frameworks that actually do graph
processing would be a better friend so
here's here's a his m page rank the page
the way page rank works is a way to
measure the relative importance of each
webpage for the purpose of ranking
search results so beta algorithm blue
box is that and each Nord starts off
with with the same value I waited the
page rank is kinda propagated to each
neighboring node and then each
neighboring node and takes all the
inputs and then we can play spanked and
before propagating out again so that's
basically one iteration and in page rank
and the challenge was doing this and
MapReduce is that one iteration and
graph processing is one MapReduce job so
why end up happening is an example of
calculating page rank where you need to
run page rank say 30 times you're
running 30 my purchase jobs and each job
has a huge barrier all the reduced tasks
are writing all the data into HDFS and
then the subsequent job all the mappers
of reading the data back out of HDFS so
you've got 30 you know times where
you're writing your entire data set to
disk and then back out again so it's
really inefficient from a narrow
perspective so and this is why m google
created prego and giraffe is an open
source implementation of prego and it
uses bulk synchronous parallel and
computing which we'll talk about in a
second here and you can also supports my
produced by version 1 and yarn as well
so on the left-hand side we have our
MapReduce floor and on the right hand
side we have how it works in giraffe so
the way it works in giraffe is that it
the map process reason that entire data
set but keeps that and did keep that in
memory and they do the calculations they
communicate between each other so this
kind of outside of em MapReduce paradigm
they communicate directly with each
other and then it's a global barrier
which is M which is super for and where
everyone waits until everyone's done and
then we start over again so the great
thing about the system is that we've
removed disk barriers we're not writing
I data set to disk n times and now have
a much more efficient way of doing our
graph processing and I'm going to show
you some code here I mean the reason I'm
showing the code is books I really like
the the model that draft folks came up
with so and from a program X program
perspective the only thing you need a
reason about is one iteration on one
node so and this is an entire this is
the entirety of calculating page rank in
the beginning if your f is very first
step your you connect setting the the
page rank by fall knows to a constant
value and then if it's you know the
subsequent steps it's collecting all the
inbound scores aggregating them adding
the damping factor for the random
possibility of someone visiting that web
page and then propagating out page rank
the page rank and which is weighted em
to all your neighbors again so it's a
very connect simple and programming
model so the use cases they have with
processing are things like social graph
algorithms like friends or friends page
rank that we talked about and and
networking in transportation and use
cases here as well okay so next we're
going to talk about sequel on Hadoop
sort the way that I sequel in Hadoop has
worked with em Hadoop version one is
that we have picking the hive as I
connect higher level and libraries hive
is really the one that does sequel on
Hadoop they can only use MapReduce so
for any sequel still not your type n is
at least one MapReduce job as having to
run and more
not does this many of them so again
we've got the connect the Layton's here
and latencies here and and as data
researchers and I used to using
enterprise 2m data warehouses or they're
going to be used to connect very
interactive quick response times and
this is not a good experience for them
so M Impala is a tool like Cloudera have
come out with it provides interactive
sequel library excuse me as written in
c++ and and benchmarking is up to 10
times faster than than the hive although
that does that does am very with with
what kind of em SiC laden and the way in
the way you can achieve this is that
there's actually empower demon running
on each slave nodes so it doesn't use
MapReduce it's got its own demon and it
really acts like a distributed database
like Vertica for example so a client can
connect to any M demon node a clear plan
gets created it gets farmed out to all
the slave nodes and in the results of
stream back and back to the client so
it's a very you know very efficient way
of doing em no sequel saw a sequel I'm
another project worth em latina is a
party drill a missile n competing and
apache mrs. this came out of map are
originally and they're one of the
differentiate is between Impala and
drill as that drill is going to give you
the capability of using not only a
Hadoop as a data source but Cassandra
MongoDB and splunk as well so em it's a
lot earlier on in the life cycle but i
think you know if the amount of green
gained traction is going to be an
interesting project to keep track off
and use cases for EM sequel obviously no
data research and exploration and in
finding the untapped and gold and your
data okay so the final blueprint we're
going to look at is call nor data format
so most of the time we were dealing with
data it's stored and roll in raw format
so when we think about CSV files or a
ver or protocol buffers or thrift
they're all connected or primary data
formats
so the advantage you get out of
commentator for max I mean some of them
are more more obvious and it's you know
if you're if you're doing if you didn't
look ups in your data and you only look
up a subset of the oval columns then
it's going to be you know you're going
to get your results and by faster but
it's also important data properties that
there's optimizations you can make and
compression and things like that so for
example and there's two projects out
there right now called pocky and or a/c
file and paki is coming out of cloud
ehren dorsey file is coming out of
hortonworks and they have a lot of
similarity the both support nested data
and bit packing is really interesting so
if you're dealing with numerical data
and your column and the and the data is
basically that the numbers are small you
can actually truncate all the more
significant bits away and in this
example of example you can you can
compress down 24 bytes down to three
pretty efficiently and run anything
coding is kinda interesting as well so
this can work on Easter Tipler works on
string type data where if you have a lot
of repeating values it can it can
compress that down but you're specifying
the cardinality and and what the value
is an index in recording again if you're
dealing with M string data where the
cardinality is around 50,000 or less you
can have a dictionary encoded and where
you're only representing each value as
as a numeric as opposed to the string
literal so the question becomes you know
which of these two is is connect the one
to go form so in terms of the system
support and orsi file only supports hive
right now versus em with perky there's
my produced Impala hyvin pigs I think
the winner in that front is paki and
from a query optimization perspective
and one of the interesting and
properties of our C file is that and as
its serializing out these blocks of
columns actually keeps track in the
footer and some basic stats around the
men Max and the summon account so what
that lets it do is if you're running
like a query for example like select
where and some value is greater
other value it can actually look at for
example the max at for each column blog
and skip over reading that entire block
if if it's lower than whatever your and
your predicate has so this is kind of
really interesting property that or see
has and so that I deem it a winner in
terms of the optimizations but really
and when we look at it and paki is on 10
traitor I've already deployed it I'm
missing some really good savings they're
both in terms of speed and space so I
think on the whole and paki looks to be
you know the kind of burner so in short
MapReduce is still relevant I mean we're
always going to have to do etl tractor
type activities in our organization and
and being able to connect in a batch
move data in and out of Hadoop and with
my producers really a useful property to
have as well and but now with Hadoop to
we have this big Gator Colonel we've got
the shared storage you know now maybe we
can have fewer of these Gator moving
projects and and focus on you know
what's the best way of connect you
today's in our resources and our
clusters and and that enables us to
connect fuse together batch in real time
the connect same shared host that we
have in our class storm so I'm head up
to the tube are really close and living
working hammering away at ghana for
years so it's i think m within next few
weeks expect to see the first GA version
of hadoop too and there's more apps
coming soon so one of the optimizations
m the the Hortonworks folks are doing is
that they don't have empower so cloudera
in horton box our competitors and clara
have impala Hortonworks don't so they're
working in this is this initiative
called tears which is a way of doing
arbitrary processing on a dag so what
they want to do is actually wouldn't be
able to write em be able to rewrite my
produced for example using tears and
remove some of the disk and network bear
is that that you get with MapReduce so
that looks like an interesting things
we're coming out and at some point
mezzos is kind of a competitor to yarn
it's another em cluster resourcing
visas management system out there and
it's it's probably bit more mature at
this point but I think once yarn comes
out is going to be an interesting like
showdown between these two M lambda
lambda architect in summing bird summing
bird is has come out of Twitter and it's
a way of f connect it's an
implementation of Nathan Mars is lambda
architecture if you will escala based
its is definitely an interesting i think
it's i think we're going to see other
lambda architecture type systems coming
out but this is this is a first one has
come out again very recently last couple
of weeks and then m from berkeley you've
been working really interesting self
technologies called spa can spark
streaming so this is berkeley you know
looked at MapReduce said okay this looks
great but you know it's connect slow and
it's got all these discs barriers and so
they spark as an in-memory way of doing
arbitrary and processing so you can do
my produced esque things in spark it's
all in memory but it's also got
streaming and graph processing type
systems as well okay so i think my
that's my presentation any thoughts
questions people may have a lot of stuff
very quickly but any questions buying
the content
it's more but enabling anything else I
think things like tears which is what
the hortonworks guys are working on dice
gun and often optimizations on top of my
bruise I think that's going to get us to
the point where we can run MapReduce
jobs in memory kind of like what's the
Buckley guys doing with spark I think
it's gonna be like like the mid term
goal of some of these projects but
you're on SFO just a general purpose you
know you know em resource management
facility it doesn't really solve any
problems other than you know which is a
management yeah all right well thanks
everyone</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>