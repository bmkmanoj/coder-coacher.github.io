<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Ask TOM Office Hours: PL/SQL 101 May 1 2018 | Coder Coacher - Coaching Coders</title><meta content="Ask TOM Office Hours: PL/SQL 101 May 1 2018 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Ask TOM Office Hours: PL/SQL 101 May 1 2018</b></h2><h5 class="post__date">2018-05-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/fZAIPwTOdoc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so thanks everybody for joining us for
this May edition of ask on office hours
with a focus on peel sequel in this
month what I'm going to do is start off
with a kind of a fast review of
associative arrays how they work at
Oracle and PL sequel in why you might
like to use them and some of the newest
features in 12c and 18 safe so my name
is Steven Feuerstein I'm the Oracle
developer advocate for PL sequel with me
is Chris Saxon Oracle developer advocate
advocate for sequel those lines are
somewhat arbitrarily drawn certainly for
Chris he covers a lot more than sequel
including pretty much anything to do
with Oracle database and he's on the ask
Tom answer team along with Connor
MacDonald and Maria Coggan please do
follow us on twitter write down our
twitter handles contact us by email
check out our blogs and our videos and
so on and so forth
ok so I'm going to spend a few minutes
talking about associative arrays then
we'll open it up for broader
conversation in chat so associative
arrays are the oldest type of collection
in PL sequel interestingly bagging
Oracle 7 they were the first type of
collection and back then they were
called PL sequel tables the venon oracle
8 we added nested tables in the via
raise though we I wasn't with Oracle at
that point but I was with it in spirit
and then PL sequel tables were renamed
to index by tables because of the index
by syntax that you'll soon see if you
haven't seen it already and then an
oracle 9i released 2 we added string
indexed arrays on top of the integer
indexed arrays and we renamed our type
of collection via associative arrays of
course the name doesn't matter that much
what matters is what you can do with
them and the reason I really like
associative arrays is that they're so
flexible and easy to use first of all
you can index by integer or string
you cannot index by string with nested
tables or via arrays you can use
negative index values which can
sometimes come in handy for your
algorithms you can take elements that
are in your positive array range and
just kind of move them temporarily to
your negative space and do work in the
positive side and move them back they
are naturally sparse in other word you
don't have to have every value defined
between the lowest and the highest index
value you don't have to initialize and
extend as you do with nested tables in
the array so the kind of the overhead of
programming with collect associative
are lower than the other two types of
collections but prior to 12 C and 18 C
there were some advantages to nested
tables and V arrays especially nested
tables for example you could use the
table operator and select from a nested
table but as of 12 C you can do that as
well you can use associative arrays
within a sequel context in a number of
ways and then as of 18 C we've
implemented qualified expressions or
really constructors for associative
arrays and for records okay so enough
PowerPoint will come back to PowerPoint
when I'm doing when we're going through
the questions let's take a look at some
code and my favorite way of exploring
code these days is using live sequel so
live sequel Oracle comm is a website you
can access any time of the day or night
to both write sequel and PL sequel play
around with the latest features of these
languages because live sequel always
offers the latest version of the
database we are now running on 18 C if
you don't have access to it elsewhere
you can get it here and what we've also
done is build a library of scripts that
you can access to learn about our
technology so I'm going to search for
associative and we get all these
different scripts and we're gonna look
at a number of these today so let's kind
of go through them quickly give you a
sense of what associative arrays can do
and then again off to questions and
answers so first of all a quick example
of associative arrays so what you see
here is the definition of a index by
collection which means it's an
associative array whenever you see index
by it's an associative array I'm indexed
but indexing by integer I've got a happy
family variable based on this list of
names type and here you see the kind of
flexibility you get with associative
arrays I can pretty much pick whatever
index value I want between a range of
values about 4.3 billion to choose from
and I can use negatives and positives
they can be sparse as as I mentioned
before they can be sparse this just uses
values that are not sequentially defined
by any means I can then use methods to
find out information about the array how
many elements are in it what's the first
or lowest defined index value what's the
highest and I can iterate through them
this is the classic iteration for a
sparse collection I don't want to go
from one to the count with a for loop
because I might get gaps
if there's any possibility of a gap what
you want to do is start with the first
and then while your index values not
know do whatever you're going to do go
to the next one or the prior if you're
going backwards and then take and then
stop when the index value is no and when
I run this code I see my values and
notice it goes from the first lowest to
the highest so that's a very quick
example of an associative array index by
integer let's go back to my search of
all the different associative very
related stuff now let's I'm just gonna
jump her out of it because it's more fun
that way
so in 12.1 one of the big advances that
we made with PL sequel is that you can
now bind user-defined types like arrays
collections
exclusive arrays and records with within
dynamic sequel so here's an example of
binding a boolean and binding
collections so here's my own collection
type defined in a PL sequel package and
I can now where's my binding I can bind
in that array or record in dynamic
sequel but also I can now select from
that array so prior to 12c the only time
you could select from table operator of
a collection was with if it was a nested
table or V array so now that's no longer
a restriction with integer index
collections it's got to be integer index
you can also select from that collection
as if it were a relational table and you
can join a collection therefore to a
table data set you can apply where
Clause filters to the contents of your
collection and it can be collections of
records not just collections of
individual scalar values cool stuff
let's take a look at qualified
expression so what's new in eighteen
city so one of the really nice
advantages of nested tables is that you
could call a constructor function and
create one of those on the fly with
associative arrays you weren't able to
do that so you had to do a line-by-line
assignment of values in the executable
section to populate your associative
arrays no longer the case in a teensy so
now when you I here's my integer index
type and it now creates a qualified
expression Airy function it looks like a
constructor function but they're called
qualified expressions and I can call
that function or that expression I can
pass it the values that I want to put
into my collection and notice I use
named notation to tell it which index
values or not which case it will use
sorry you must use name notation to
provide the index values this will fail
they don't have to be in order and what
that will do is populate the collection
in the declaration section and then you
can work with it
so whether you whether you assign the
value in the declaration section of the
or the executable section the bottom
line is it's much more compact much
nicer to work with and it also works for
string index collections but here's an
example that's really nice I've got a
collection of records indexed by integer
and records also as of 18c allow you to
have a qualified expression so I'm going
to on the fly in my declaration section
use my qualified expression for my
collection type to populate my array and
each element in my array is a record
type so I call my qualified expression
for that one and I construct my records
on the fly pretty cool stuff so this is
a great example in eighteen sia of the
PL sequel team focusing on making PL
sequel easier and easier for us to use
on a day-to-day
Karuma basis write less code construct
things more compactly manage your code
more effectively here's an example using
qualified expressions for string indexed
arrays so again I specify the location
in my array which is a string and the
value that I want to put into it so it
works with integer indexed arrays and
string indexed arrays nice stuff I hope
you'll agree and you can even use
expressions look at this I can use an
expression to say what is the location
in my array what I want to work with
good stuff
so what the question is about is when I
select from table first of all I can
have a column value which is a fixed
name hard-coded name of a column if it's
a collection of scalars and we would
like to be able to do is get the index
value from that Arabian look here's
another one the index value of this
array and use it to join it's a lovely
idea it's definitely on the list to do
by the PL sequel team but currently you
do not have access to the index value
from the table operator you essentially
have to construct it yourself in the
sense that you can construct your
collection so that you have one other
element let's say it was a collection of
records then you would add to your
record and attribute which is the index
value and then you would populate that
index value
I know it's hokey but that's the only
way you can do it right now
so let's take a quick look at string
antic indexing and how it can help you
from my perspective among other things
really simplify the code that you write
so this is a very simple package I wrote
years ago to keep track of strings
basically I needed a key track of a
certain name of a variable as being used
and some code that I was generating so I
needed to be able to mark a string as
being used and then ask if the string is
in use and my first pass at doing that
was to create a list of the strings that
were used so it's a an index it's a list
of variable names and that's defined
with a subtype one of my favorite
features of kiyotsugu as well to market
is used I'm going to pass in that string
and add it to the list so notice what
I'm doing is taking the count the
current number of elements in the array
starting with 0 adding 1 to it and then
simply adding that string to the list so
the list gets longer and longer and
longer then I ask is the string in use
and the way I'm going to find that out
is I'm going to iterate from the first
to the last and look for a match so well
I haven't found a match and I haven't
mix the end of my list it's found if the
value equals the value in my array now
this is not a lot of code it wasn't
didn't take me forever to write but it's
the sort of thing that you do have to
sit down and write then you have to
debug and you have to test and and edge
cases are always a pain in the neck and
it's just not necessary because what I
could do and sorry intrusive not being
necessary the other thing to realize
with this kind of algorithm with this
kind of algorithm is that I'm
essentially potentially scanning through
the entire collection and as the
collection gets bigger and bigger and
bigger the cost of executing this code
depending on where that string matches
found could actually increase so this is
a great example of a an algorithm that
is not data neutral depending on the
state of the data in your application
your session the performance of your
application can degrade something you
want to avoid so let's take a look at
using a string index collection instead
so same API mark a string is being used
is the string in use but now I'm going
to declare my array this the list of use
strings as
a table of what a table of who-cares
indexed by a variable name know what's
going on here so notice I've declared
subtype who cares as a boolean and I've
got this other constant called does not
matter I'll show you that in just a
second so what I'm saying in my code
here I like to write self-documenting
code that tells its own stories I don't
care what the value is turns out I only
care about the index value so Marcos
string is being used I passing that
variable name I don't add it to the list
which is what I did here instead I use
that string as the index value in my
collection and the value and the element
value is who cares it doesn't matter
it's like an indexed index oriented
table all you care about is the value in
the index so market is used and then
check this out is the string in use go
to that location use the exist method to
say does it exist done so now my
collection could have ten rows you can
have a million rows the cost of checking
to see whether or not that string is
being used essentially will be the same
regardless and of course it's much less
code to write I'm using built-in
declarative programming instead of
writing an algorithm so I think this is
a nice simple example of how string
index collections can really help you
so we had a someone was asking how can
collections help improve performance of
a program rather than use global tables
and I think the example you just walk
through is one situation where this
could help tell us more so well if you
were writing this to a temporary table
then you've got to go through the
process of writing it to the table and
then actually querying it back out now
that can be fast enough but with the
index array we know that's going to stay
in memory whereas with temporary table
can be writing to and from disk from
it's going to be temporary tablespace
but we know we're not going to hit that
with this collection that's so they
there isn't like a fixed answer of when
collections are going to be where that
better than global temporary tables
unfortunately as part the reason both
these things exist we discuss this last
time I think as well a little bit than
waste even they're there they say
because you've got constant time lookup
here you know it's always going to be
that and you say - what do we want - is
that going to exist potentially facile
in whatever the sequel equivalent might
be great points and so let me add - in
general that the performance of working
with collections in memory so pls equal
collections nested tables VRA is Colet
associative arrays all consume PG a
process global area memory and in
general accessing pga memory is much
much faster than SGA memory or system
global area memory it's shared across
sessions so the cost of iterating
through collections in other words the
cost of of executing the string and use
that involves a loop it's very fast and
unless you've got a massive number of
rows you're probably not going to see a
big difference in performance between
say this implementation and the string
indexed implementation the main
advantage here is the reduction in code
and the simplicity of the algorithms so
collection manipulation is super fast
and it's almost always going to be tout
a sequel operation because you got the
context switch from PL sequel to sequel
and you've got the overhead of executing
sequel that you just simply don't have
on you're accessing local or session
memory the trade-offs will be things
like
big that collection is so as the
collection grows and consumes more PGA
number one you have the threat of
actually hitting the limit for your
session and number two the cost of
manipulating that collection will go up
and it also if you need to work with
some from data and tables you might want
to move it onto a globe area temporal
temporary table so that you can do it
all in sequel however again with 12c you
can now use the table operator and
actually blend in pga based collections
as a kind of table in SQL with data in
relational tables so it's a nice way to
to make them work together
so the first one is a question on multi
set so multi set except does not appear
to scale very well and for larger sets
100,000 rows and higher it's much more
efficient to use equal with a select - a
select query
have there been improvements in this
area since 12:1 Chris do you want to
speak to any improvements while I bring
up some code to look at so in terms of
I'm not aware of any specific
improvements relating to multi set
except that would make its performance
work any better and this traditional set
operators however it is worth thinking
if you're trying if you've got a query
that is slow and you're trying to get
more performance out of it there are
various other techniques in tulsi that
are available so it may be they actually
need to step back and look at the
business question what you're trying to
achieve
if whoever asked that is able to give us
a bit more background we might be able
to help in a bit more detail but when it
comes to multi set it except itself you
know nothing particularly new there yeah
I agree as far as I know there haven't
been any changes and so let's just step
back a little bit and talk about the
multi set operators in fact I can show
you my code but why would I do that
because I'm pretty sure we got something
and life sequel for you so multi set
except so one of the features and I
believe was added in 10g
is the multi set operator so the multi
set operator allows you to do set
oriented manipulation of nested tables
inside PL sequel multi set you can also
use multi set in sequel and it's similar
to doing the set operators like Union
intersection and - so with multi set you
have accept which is - but they didn't
name it - I'm not sure why you have
multi set intersect you you have multi
set Union and the basic idea is that you
can take separate multi separate nested
tables and you could perform set level
operations on them in PL sequel one of
the other nice things about working with
nested tables this is not directly
Jermaine is member of so just in case
you guys don't know about this
when you're working with nested tables
this is an alternative actually to the
kind of searching that I just showed you
there's also a feature called member of
or not member of that will allow you to
very simply and declaratively ask does
this element exist in the collection
again only for nested tables so multi
set operators give you a lot of power
within PL sequel to perform set level
operations on collections in PG a memory
but if you need to do sequel operations
let's take a look at here's a comparison
of nested table and sequel operations
unless you need to do sorry if you're
doing lots of complex set out oriented
operations you're almost certainly gonna
be better off with sequel because it's
optimized for those kinds of steps ok
cruising along
question-how triggers what is it instead
of trigger Chris I'll give this one to
you yes so an instead of trigger is
something you place on a view and what
it allows you to do is kind of intercept
the insert updates deletes and instead
of that pushing those straight down to
the underlying tables you do something
else entirely for example you could do
something if you really crazy like write
the data to a completely another other
table and of course this brings a
question about why why would you ever
want to do this why would you want to
have this kind of craziness going on and
for the most part you wouldn't however
there wasn't one situation in the past
which I found it very useful so we were
going through refactoring some of our
tables so we're restoring flight
information so what people had booked
you know which flights they purchased
and we'd also stored with that details
about the flight such as its departure
time and quite often what happens
sometime after you booking and actually
flying the schedule changed slightly so
the times of those flights will subtly
adapt and of course if we've then stored
what time you've saved they were time
the flight departed at a point in time
you purchase the flight that note then
no longer matches the actual real time
of flight departure twice and this led
to all sorts of complicated workarounds
we had to do and really the real
solution here is just kind of drop the
flight information out of the booking
tables and just join to the flight so
sounds simple but of course now you've
gone for a situation where you're just
querying one table and you've got to
join to them that this is one of the key
tables in our applications that you know
huge ripple effects throughout the
application what we were able to do
instead of kind of exposing all that was
create a view which mimicked the old
table and did the join and then so we
could query the information back out but
of course when you try and insert some
new someone makes a new booking they try
and insert it that's not going to work
because there's two different tables
that this is where we use the inn
instead of insert trigger it was able to
map the fields to the correct and blind
table so M it can be a good way to help
you kind of refactor your database
schema with minimal impact on your
application ultimately in this situation
you probably want to actually update
your real code these real tables and do
the join but it's a nice way to help you
isolate the schema changes without
having to potentially change every line
and inspect review every line of your
code so thanks Chris so you'll see on
the screen here an example of an instead
of trigger that I loaded up this morning
this is taken from my Oracle PL sequel
programming book written by Darryl
Hurley the code itself and we won't go
through all the details of it you can
check it out on your own time that's the
whole idea behind live sequel but the
basic ideas I've got to join against
multiple tables that's the main data set
that people are using and accessing in
the application and of course you can't
do inserts updates deletes directly on
this view well which day would it be for
so essentially the instead of triggers
says well instead of the normal insert
operation I'm going to divert it to run
this code instead and you can see that
he's got a lot of logic here but
essentially what you can see is that it
crosses into different DML operations on
the different tables depending on the
the state of the data that's being
passed in and here we've got one an
update as well so that's the base guy
between basic idea behind instead of
triggers you can't apply the DML
directly or one of the reasons for using
it is you can't apply DML operations
directly on multi on views with multiple
tables and so you can basically steer it
in a different direction and you write
all the code needed to execute the
appropriate DML
the question on log errors so why
doesn't la garish trap might worse 6502
error so this person wrote to say that
she was building a pipeline table
function to get the records and pass
them back into the SQL statement but
while doing the insert operation that
the pipeline function was being used for
Oracle is throwing it a 6502 and she was
confused because she thought the idea
behind log errors is that if you get an
error in your sequel operation it will
log the error and not raise back an
exception field sequel and allow you to
continue so why was this still failing
and she wondered well maybe it's because
it's not actually an error in the DML
but an error in PS people and yes that
is almost certainly the case so I don't
have all the code to show you and I'll
show you an example of log errors in
just a moment but the basic idea first
of all is that if you ever write to
ascom or to myself or to Chris and say
I'm getting this error what's gone wrong
we usually can't figure it out unless
you show us a simplified version of your
code and shows the the actual running of
that code preferably on live sequel
demonstrating the error let's take a
look at log errors and Chris you just
dive in at any point yeah and sorry
Sven Vanger has also said that there's
another view Vita or some pga stat which
should be helpful with analyzing a
statistic pga consumption okay so I'm
sorry Chris were you about to say
something that's fine it's a guy can you
come so just a quick quick review of log
errors in case you didn't know about it
so log Eris is this really great feature
where if you put the log errors Clause
on the end of your DML statement this is
a part of the sequel injury not Pele
sequel you can also specify the number
of times you'll accept an error then
what will happen is that for if a row
fail so sorry let me step back here's an
update statement against the entire
employee's table and we're giving people
a big raise they deserve it
unfortunately we might not be able to
give to everybody that bigger raise
depending on how large a salary they
were already making the constraints on
the column size now in general without
log errors
DML statements like this are all or
nothing either every single row is
modified successfully or none are and in
this case for example we found at least
one row that we could not update so the
whole thing failed and nothing was
changed and that's usually the way you
want your diiemma operations to proceed
but suppose you're updating a million
rows and if you got 900,000 of them done
and you had a hundred thousand left or
three left that didn't work that's fine
you want you want the rest of them to be
saved
you had the logger as clause you'll
previously have created a log error
table using the DBMS error log package
and then when I run the same code notice
it found that it updated 49 rows out of
my 107 in the employees table and what
it did was write information about each
of the rows that failed out to my log
errors table so here's my log nara's
table
it found 58 rows that failed and then I
can look in information about what
happened so that's the basic idea behind
log errors is that if the DML if the row
level operation fails rather than force
a rollback of the entire statement it
writes information out to this table
undoubtedly using an autonomous
transaction procedure and allows you to
continue but beware if you do use log
errors and it should be the exception
rather than the rule you've got to check
your table afterwards you're sorry your
error table afterwards because the
sequel engine and the PIO sequel engine
will not raise an exception at all so
that's the basic idea there and this
reader or writer was correct in that as
far as we can tell from the limited
information she gave us if you're
getting an aura 6502 or any number of
other PL sequel related errors when
you're running your sequel statement
with a pipeline or regular table
function inside of it almost certainly
and you've got loggers almost certainly
the problem is within your function so
the first thing you do is you take that
function out of your sequel statement
and you run it independently and you
test it and you make sure that it works
and then you put it back into sequel and
see how it works now with a pipeline
table function you can't actually run it
outside of a sequel environment you
might so that so you'll probably have to
still keep it in C
but in general I would say as you're
building at your code test incrementally
make sure that your individual units are
working before you put them in a larger
and more complex context anything to add
on that Chris I think that's just good
general advice you know make small
reusable dates gradually build things up
rather than try to make you know I've
seen people like one giant massive
procedure and as soon as anything goes
wrong you're completely stuck and it's
like why does it do that so the smaller
you can make your functional units and
then build on top of them better yeah
that's as you say that is a great piece
of advice to give generally I just did a
code talk session that's sponsored by OD
tug on unit testing and using you TPL
sequel an open-source framework for
testing one of the biggest problems I
think we have as back-end developers is
that we write code that's not easily
testable so if you've got a procedure
that's 5,000 lines long that updates 25
tables that has six in parameters and 25
out parameters just coming up with a
list of test cases would be horrendous
thousands of test cases and you're
immediately overwhelmed and so you don't
do much of anything in terms of any kind
of proactive testing
so in general when you're writing your
code break it up into smaller modules
make those modules available for testing
you'll have to put them in the package
spec which you can also use use
conditional compilation to only expose
for testing hide in production and by
making your code more manageable and
testable it's also going to be much more
reliable and easier to fix over time and
we have a comment from you death on the
call maybe the problem is that the are
6502 is raised by the select part of the
statement well a log errors logs the
errors for the insert statement if the
Select fails then the answer it is
probably not attempted at all
so you know there are a number of
possibilities with that or a 6502 that
could be causing the problem again start
with the individual pieces like the
pipeline table function of the regular
table function isolate that if that's
working and not cousin error then step
further on and take a look at the
overall statement and here's a question
for Rajesh how about the performance
impact if we use log errors from
millions of Records Chris
so yes I mean there's gonna be some
overhead to doing this and the key thing
here is always just make sure you test
and benchmark and see if it actually
meets your requirements I mean I are you
saying this because you're worried that
one day you'll hit millions of rows or
are you actually processing millions of
rows right now but it's also important
to kind of think about okay this has got
an overhead but what would you have to
do instead to avoid that overhead there
are various other approaches such as you
can use bulk collection with save
exceptions that might be an alternative
or you might have to go through some
more extensive codes to kind of do all
this checking to stop any rows which
would be invalid
and we're trying to insert them into the
table and typically that is going to be
much slower than using the database
functionality for a start any
improvements we make to the sequel
processing engine you're going to get
those for free when if you use log
errors whereas if you eve if you think
of a better algorithm or a better way to
process your own code you've got to go
through and rewrite that yourself so
it's always better to use the inbuilt
functionality unless you proved that
it's too slow for your user requirements
and you can write a better you can write
it faster yourself so I just kind of as
general advice we're trying to use the
functionality of you can and only step
back if you find that it isn't meeting
your needs but if you've got a specific
instance then make sure you benchmark
and test and see it does what you need
it's a great advice Chris thanks yeah I
say in general if you're if you're
looking at a situation where you're
loading millions of rows doing the an
instruct select and all the lanes roses
one thing but if you actually think that
you're ending up with millions of errors
in that process of moving data across
row we're doing a bulk operation chances
are you're not going to want to rely on
loggers certainly you will see a
performance impact it is literally doing
an insert into a table it's a regular
old relational table even if Oracle will
create it for you by the way you can
actually modify as well yourself to add
columns and so forth and I'm pretty sure
that I have to say done
know for certain that it is calling an
autonomous procedure a PL sequel
procedure so it's almost certainly doing
a context switch writing an insert into
the table and committing within that
context so clearly there's a lot of work
right there
I would suggest that if you decide that
you that log errors is a good fit in
other words you want to suppress errors
at the row level again that's a pretty
unusual scenario then your if you're not
looking at a situation where you've got
a You're Expecting relatively few errors
then you probably don't want to use this
approach you can want to look at
alternatives and as Krista's relying on
the the built-in sequel functionality as
much as possible is something to look at
including things like super letter okay
cruising right along
question on for all so I got this in my
inbox of the day with for all it has
always suggested to use the limit clause
to avoid the memory overflow problem but
if millions of rows of data are
transferred from one table to another
via direct select is it possible to have
the same memory problem and if so is it
better to use floral with the limit
Clause now I'd like to make this a
little bit of a quiz first so what do
you think of this question is it better
to use Faro with the limit clause or the
statement with for all it is always
suggested to use the limit clause what's
wrong with those statements anybody know
so you have quick people I can think it
will type it and hey we got the flavor
outlet
Oh nobody wants to type an answer what
you got use the limit Clause never say
always that's always a good one
but there's something factually wrong
with this statement what is it factually
wrong with this statement the limit is
our profession out the for all exactly
Tim thank you so there is no limit
clause in the for all statement there's
a limit clause and bulk collect and
often we use them together we bulk
collect data into an array or set of
arrays make some changes to it and then
push it back with for all but the limit
Clause is a part of the bulk collect
process and you should use the Lu McLaws
at all the times to cut down on the
amount of pga memory that you're
allocating for those collections but if
you do have a very large collection
let's say you might have a collection of
10,000 rows and not that large but you
can't do an update of all 10,000 rows at
a time you might have to fall back on
incremental commits with for all which
is an interesting thing to do and I
think I might have a script on my sequel
let's say please find something there it
is so if you're working with floral and
you have the situation where you've got
you're trying to put in let's say 10,000
20,000 rows and you and you get an error
because you don't have this table space
size to support that or rollback
segments size what you can then do is
switch to incremental commit processing
here's the traditional way row by row
update
if I hit my counter I'm gonna do a
thousand individual row-by-row updates
then commit you keep going then reset
the counter this approach works it's
very slow assuming any number of roads
so you'd want to switch to bulk
collecting for all but anyway you can go
through my example of using for all with
a limit without with a pseudo limit
clause so for all doesn't have limit per
se but you can limit it by the start and
end in your in the range of roads for
your collection let's go back to the
question though to get some clarity here
so we do suggest that if using ball
collect followed by for all that you do
use the limit clause you're fetching ten
rows or a thousand rows or ten thousand
rows at a time but not fetching
everything in the set of tables to them
do the update and then is it possible to
have the same memory problem with sequel
Chris is that is that an issue so when
we the same memory problem no in the
sense that if you've got a sequel
statement
you're not gonna it's not going to run
out of memory in the same way that will
collect but you're not gonna blow out
your pga however you can have a
different problem in the sense that when
one's individual sequel statement is
processing too much data to fit in
memory it will have to write it to the
temporary table state temporary
tablespace so it will stage that data
and kind of go right I'll come back to
that at a later point and of course at
this point you then starting to read and
read and write from disk and as soon as
you start having to read and write from
disk the performance of your query is
gonna drop drastically because disk is
still hugely you know on orders of
magnitude slower than memory so you
don't have the problem in the sense that
your whole programs going to crash but
you might have a problem in the sense
that it could be potentially a lot
slower now exactly when you'll hit this
and how it happens entirely depends on
your sequel statement and what you're
doing but this is something you know is
mentioning before about looking in the
execution plan looking at the tracing it
and seeing
it's really doing this is where you can
get into that analysis of what's going
on seeing whether it's using that
information or using that temporary data
thanks Chris so I think the way to sum
up on on this page on the question about
for all first of all if you can do it in
a hundred percent sequel if you if you
don't need to use collections and floral
and pj and bulk likes that's great you
should do it in straight pure lovely
sequel encapsulated inside say a kale
sequel procedure so that's always the
best if you do need to use for all you
want to avoid row by row processing and
you can't do it in straight sequel or
either because there's a requirement
that makes it impossible or maybe you
can't figure out how to write it in
straight sequel some of us are not
amazing sequel gurus like chris and
connor and kim and tom and so on and so
forth you can always fall back on right
and peel sequel and you could compensate
by using for all and vocollect we had
one other comment from sven the question
makes sense when we consider the limit
clause for bulk collect it's only useful
if we also do a for all otherwise we can
use a simple for loop which also does
bulk collect so it is true that as of
10g a cursive frog for record in query
will automatically bulk collect 100 rows
at a time so you do get the advantage of
avoiding row by row fetching with a with
a cursor for loop that's not always the
optimal approach even for larger volumes
of data you might get better performance
out of doing your own explicit bulk
collect with a fetch limit clause
because then you can say I want to get a
thousand rows at a time where 10,000
rows at a time but in general you want
to look at using bulk collect versus say
a cursor for loop if inside that loop
you're doing DMA operations inserts
updates deletes because then you're
getting into row by row DML operations
and those are very very slow did we have
one of the common in chat yeah so we
think it's for all yeah take that yeah
so I think Stevens kind of covered it a
little bit already but in general you
should probably you should look to do it
in Singh in a single sequel statement
but there may be situations where
between actually querying the data and
writing it you might
to do something complicated maybe you
need to do some complex calculations on
that data before actually writing to
your table to the table and in that case
processing it in PL sequel can be faster
than doing it in the sequel engine so
that's one instance and in my experience
it's I can't I can't think of a
situation where I've needed to do that
so probably think carefully about
whether it really is faster a more
common example were more useful example
is when you're processing large volumes
of data so let's say you're processing a
hundred million rows that is gonna take
a little while it doesn't doesn't matter
what you do doesn't matter how fat what
knobs you twiddle or what approaches
you're gonna take it's gonna take some
time and there's nothing more
frustrating than ever process that's
going to process doing a hundred million
rows it gets to the 999 million whatever
and then breaks for some reason network
error runs out of undo who knows and the
whole thing falls over and so you've
wasted all that time what using bulk
collects and for all can allow you to do
is break up the processing so you're
staging it and doing it bit by bit you
do have to think very carefully about
how you're writing your transactions so
you say how do we know when we save this
that we you know don't reprocess it
again if the whole thing fails but there
are situations like that where something
like this can help Stephen and I just
brought up a script on the screen from
my sequel about save exceptions so the
other reasons you use for all versus a
single sequel statement is that you can
actually trap exceptions on the
statement by statement basis so you can
keep on processing this is a level of
error suppression above log errors which
is row level this is statement level so
it'll help you preserve your changes
okay and I have a one coming you're
coming back to the question on memory
usage of associative arrays you might
also take a look at sequel monitor and
Oracle Enterprise Manager which shows
live sequel and execution and can give
you some additional insights yes
all righty coming up in the end of our
hour we've got one more
this one's for Chris mm-hmm
partitioning yes okay so this is a
interesting question and it's something
that people have been struggling for a
while is how do I part I've got an on
partition table and I want to convert it
to a partitioned one and traditionally
there hasn't really been a good way to
do this because you've essentially got
to rebuild the entire table so you've
got your current table which is non
partitioned then you've got to clone it
make a new table which has the
partitioning properties you want
obviously that's kind of tricky to do
because particularly if you're thinking
about partitioning it the saw starting
table almost certainly is large you know
has millions or billions of rows in it
so they've got a process of copying from
the old to the new so we had a various
technique the very simple technique to
do this was just do a crate a select
where you provided the partitioning
properties but in order to do that I'm
trying to do it correctly you needed to
have an outage for your application
which isn't very good for a lot of
people so whether we introduces this
DBMS redefinition package which allows
you to change physical properties of a
table online so things like it's
partitioning options so it's a
relatively convoluted process and it's
best to kind of look it up and rather
mean try and explain it here essentially
you say it manages the process of
translating those tables for you so it's
the same principle
you've got your adrik old table you've
got your new table you're copying data
from the old to the new but this just
adds some extra steps in there and
allows you to do it in an online way so
you don't need hours about it to do this
so this is techniques we've had in the
past and they say they're all bit clunky
and not too great so in twelve two we
introduced the ability to partition a
table using an alter table statement so
now you can do alter table modified
partition and specify your new
partitioning properties
and so you can change it to hash range
whatever you can even specify sub
partitioning as part of that as well so
this is much simpler much easier to do
you know you don't have to faff around
with DBMS redefinition or whatever and
it's an online operation so you can do
it online there are also some questions
about the managing the indexes there so
if you've got a bunch of indexes on the
existing table then when they switch
over you know what do they do they
become global indexes or do they become
local indexes well you can specify that
as part of the odd table as well so that
is much easier to manage so the other
questions how is the index created on a
partition table well you know just as I
am
same way we create a regular insects you
do a create index if you want it to be a
local index and you specify the keyword
local and when it comes to creating and
local index only on one partition so
this again is something which in a 12-1
actually we introduced this we got local
index you can disable indexes on
particular partitions so you can say I
don't want any indexes on this partition
so disable that feature for you I think
to kind of get more into this local
global index it's a very and there's a
lot of ins and outs to it there is no
one straightforward answer if there was
third only one type of index
Conner's got a great presentation on
partitioning which you can download off
the ask tom site itself so if you go to
the resources tab there and look for his
partitioning he talks a lot about the
ins and outs of that but yeah we're kind
of coming to up to the hour and I I
think it's if you want to know more we
probably better to address that in a
and the separate offs a session great
point Chris and Chris in fact has a
sequel office hour session you can come
back and Hammer a meditation
it's like Connor has put together a
really great resource this is a single
page on partitioning he's got a series
of videos so do check out this page and
dive in deep</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>