<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Ask Tom Office Hours: Oracle DB Upgrade &amp; Migration | March 21, 2018 | Coder Coacher - Coaching Coders</title><meta content="Ask Tom Office Hours: Oracle DB Upgrade &amp; Migration | March 21, 2018 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Ask Tom Office Hours: Oracle DB Upgrade &amp; Migration | March 21, 2018</b></h2><h5 class="post__date">2018-03-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5OV6MnRDWbk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I'd like to thank everybody who's
already in I expect we're going to have
several more participants joining
because I think we had something on the
order of a hundred and twenty one signed
up for these office hours I do like to
start things on time just to be
respectful of folks folks time so we're
going to get started with this for those
who haven't participated in office hours
before this is a way for us to interact
with you for you to ask questions to
either submit them ahead of time or to
talk about then just whatever happens to
come to mind even today about database
upgrades and migrations and last month
it was me and Mike doing the office
hours for upgrade today it's primarily
going to be me and bill will introduce
our phones if you do have any questions
I encourage you given the number of
participants we're going to have please
use the chat window to enter any new
questions because that way we won't have
the usual webcast people trying to
figure out who can talk and when and
that kind of thing so please use the
chat for new questions I have prepared
this very brief presentation to cover
the two questions that got submitted in
advance and we'll start with those but
then we can move on to whatever anybody
else has on your minds so first order
business introductions my name is Royce
longer been working for Oracle for a bit
over 22 years now actually a bit over 23
years and I'm vice president for
database upgrade and utilities and in
this context utility mean utilities
database utilities mean a data pump
sequel loader the metadata API
transportable tablespaces those kind of
things the they were documented in the
utilities guide
so it's very different of course from
the data they from the Oracle utilities
applications out there so I just want to
be clear about that and I grew up in the
Buffalo area in western New York State
it's an area renowned for the amount of
snow we would get every year we would
often get up as much as a meter of snow
in a day and I now live and work in the
New Hampshire work in the Nashua New
Hampshire office we have about 500
employees total and we develop other
technologies there including math
multimedia there's a bunch of Oracle
secure backup a good chunk of our man
data guard and some of our new upcoming
autonomous cloud features as well I
think Oh Billa would be next year so
bill why don't you introduce yourself
hello everyone I'm and worked with Roy
for on and off for a long time and came
over from worked with the digital RTB
group and then that came over to Oracle
was acquired by Oracle I've been working
with Oracle ever since and primarily I
had focused in the spatial and graph
area and now I'm working for Roy and the
utilities in upgrades group
great thanks Phil and we are thrilled to
have Bill with us because seems like we
always have new places to migrate and
move things to and lots to talk about
when it comes to data pump and Bill's
also in the national office here I have
Mike slide here I think Mike is still on
or is he Mikey's gone no so I am on at
the moment
so for the next ten minutes until I
bought the airplane yes so Mike is on
everybody knows Mike when it comes to
database upgrade he's been
with my team for how many years Mike
nine ten something like that but
actually we celebrate our 10th
anniversary on 1st of April ah yes I
prefer the Easter and like the 10th
anniversary upgrade so so that's that's
our product management team here for for
the area so now let's get that get to
the questions that people have asked in
advance and this is the first one and it
was about what's the best approach to
take when upgrading 411 203 to a 12c
pluggable database and they were looking
at what what approach to take especially
going to 12 - and is it better to
upgrade in one step should you upgrade
to 11 204 first and then to 12 - and so
on and that's um this is this is a
common question from people who are in
that non CDB environment who are going
to go into a PDB because excuse me the
upgrade and plug is not really a one
step approach because you can't go
directly what during an upgrade from an
11 203 or 11 200 for non CE to be into a
pluggable database you would either have
to upgrade and then plug or use a
different method to migrate so let me go
over a couple of the options here one
option would be to simply use something
like Oracle data pump to migrate
directly into a pluggable database so in
order to do that you would first have
your container database created you
would then create a PDB now that's a
very fast operation creating the PDB
less than 30 seconds and then you can
simply export from your source database
and import into the PDB you could do
this over a DB link if you wanted to and
that would be a very good way to go into
that 12 to PDV
it's a it has the advantage of leaving
your old database as a nice fall back
just in case something didn't go well
during the migration or in your post
migration test and you find that
something isn't performing the way you
want it to so you could do you could use
data pump like that another option would
be to upgrade your database and then
plug it in so again when you start with
your target container database you have
your 11 200 3 1112 for database which
the first thing you have to do is
upgrade to 12 2 in this case you then
start the database read-only create an
XML description file and this slide is
not displaying properly okay I'm just
going to go this way I don't know why
status playing properly
we'll just go with the full screen
normal view so hopefully that's visible
so you would create this XML description
description file using the DBMS PDB
describe call that XML descriptor file
is basically a manifest of everything
that's in your database along with the
most important characteristics of it
such as character set that's a big thing
that you want to know when you're going
to be plugging into a container database
less important now in 12 - when we
support multiple character sets / PT /
cdb but it was definitely very important
in 12:1 then you'd shut down the
database and then you plug-in using the
create pluggable database statement so
here you we show a flavor of create
pluggable database where we're saying no
copy and that means that we will simply
use the data files in place on disk
where they are so that becomes very fast
and you know if you've got a 2 terabyte
database or something like that you're
not going to necessarily have the space
or maybe the time to copy your data
files so that's when you might use the
no copy of course the downside of doing
the no copy is that you don't have a
fallback to your old database at that
point because after we plug in that pdb
we will then do our sanity operations
which is this last script here non-si
db2 pdb dot sequel non-si db2 pdd-nos
sequel deserve our special attention
this is I'm gonna go back to full screen
mode here we call it the sanity script
that you run when you're plugging in a
non-si dB
well what it does is it basically goes
through your your new pdbs dictionary
and replaces all the entries in the
dictionary that should be references to
objects in the container database root
with what we call metadata pointers so
if you've got say
and source dollar you've got an entry
for the dbms stats package and each of
the sub programs in there well in a
container database environment the pdb
doesn't have its own DBMS stats package
that would live in the container route
so when you plug in a non c DB as a PDB
we have to go through source dollar it's
just one example and take all of those
entries that refer to DBMS stats and say
no these now point down into the
container route so this is an
irreversible operation once we've done
all this work you are now a pluggable
database there is no PDB to non c DB
sequel script it's a one-way trip you
only were on it once at least in 12 - it
is rear unabled so if you have a problem
running it and can fix that problem then
you could rerun the script to finish
that plug operation I'll be honest I
don't have an example of a problem that
you could encounter that would allow you
would be able to be fixed and rerun it
but it is rerun herbal so maybe if
something happened like you had an
instance crash or you're you lost your
VNC session or something like that you
could still rerun that the runtime of
this script as we say it depends it
really depends on more than anything how
long it takes to run UTL RP because
after we do all these replacing have
metadata pointer if in your database or
with metadata pointers we have to run ut
lrp because we will have invalidated all
kinds of things that depend on anything
in this C DV dollar route so you may
have a sense of that for your database
you know if it's an e-business suite
database running UT lrp can take an hour
or more on a simple database it could
take five minutes so overall I would say
that you could depend on non CDE to PDB
about sequel taking anywhere from about
10 minutes on up which means it does
take a significant amount of time in the
scheme of things it could take in
some cases as long as the actual upgrade
took so this is an option for you though
to upgrade and then plug in generally if
I was going to do this I would not use
no copy because I like because this is a
one-way trip I want to have a fallback
mechanism of being able to go back to
mine on CD B if necessary so I want my
files still around but it may depend on
how much space you have available how
quickly you can restore backups if
needed and so on then a third method I
want to highlight about moving from an
eleven 203 non CD B to a 12 - PDB would
be to use full transportable export
import this was a feature that we added
for migration of 12:1 but we actually
added the export side in 211 203
we didn't document it at the time
because you couldn't have used it for
anything until twelve one came out but
we were preparing for this idea of being
able to migrate databases forward and we
were able to do that by putting the
export side into the earlier patch set
so with full transportable export import
what happens is that it uses the data
pump interface very much like doing a
full day to pump export and import but
we're going to move all of your data
files using transportable tablespaces
so we you move them as files we don't
have to export an import all of the data
that means this can be a lot faster than
doing a traditional data pump exporting
important even as fast as data pump can
be it doesn't necessarily compare to
transportable tablespaces so in order to
follow this technique you would create a
target pluggable database to import into
because we are still doing an export
import here we need a target to export
into and then you might want to use a
database link in this case the reason
for that is that we would we can move
all of the metadata
/ that DB link and we don't have to
worry about any of the any of the
limitations on DB links such as long
data types and other things like
database links don't support so in this
case we create a DB link for our p DB
back to the 11:11 g database and then we
set our tablespaces read-only that's
when your downtime would start for this
you could copy the data files to their
new destination if you want to you don't
have to but I consider it good practice
again so that you have that nice
fallback mechanism and then you run a
import over that DB link so this is a
one command migration from 11 to 0-3
non-si DB into a 12 to pluggable
database and this is basically the whole
command here I mean you could add other
options of course like metrics is in
there but you could use lots of other
options here we give it our DB link and
then the key for using full
transportable is we're saying version
equal 12 full equals y and transportable
equal always this full transportable
export import feature is invoked by
combining full equals y and
transportable equal always but in 11 203
you have to specify version equals 12 to
denote the fact that you are indeed
moving forward to a 12c database because
otherwise you will simply get an arrow
saying you can't useful and
transportable people always together
because for 11 2 - 11 - you can't you
couldn't do that once you're going from
a 12c database you don't need version
equal 12 anymore
so this is a nice technique that
basically combines the ease of use of
transportable table spaces with the
speed or the ease of use of data pump
with the speed of transportable table
spaces and it was really designed with
exactly this use case in mind of saying
I want to go from a non CD B into a PDB
one quick step because notice one thing
I don't have to do here I don't have to
run that non CD B 2 P DV dot sequel
bring in all my data files over taking
care of all the metadata for you with
one export and import command which is a
lot simpler than normal transportable
tables places and doing it fairly
quickly so that's the first question the
second question what this was I didn't
change the title on this from the last
one it's not an EBS upgrade scenario
this is about exadata with a teensy
installed they're asking they've got
databases on older exadata machines most
of them multi-tenant and a few 11 203
how they'd like to migrate with data
pump through database links and the
biggest one is 5 terabytes any tips or
hints version comparative
incompatibilities or performance issues
well there are not any version
incompatibilities with 11 203 going
forward actually data pump still
supports all versions since it was
creating 10 ones so we don't have any
versioning compatibilities moving things
with data pump there are a couple of
important notes that I want to reference
for you if you're using data pump let me
go down here one of them is the master
Note 4 data pump and that is here let me
increase the font size on this
so our support team does maintain this
master note for Oracle data pump with
common issues character set
compatibility that's one area you always
want to pay attention to if you're
migrating with data pump we will handle
character set changes for you but you
want to make sure that your database can
also handle character set changes for
example performance issues and so on so
there is a note specifically for that
and there is also a list of data pump
performance bugs that have been fixed
over the years and because you're
talking in this case about potentially
11 203 migrating some of these are ones
that you may want to to look at so this
note this it starts with bug number
descending it's actually easier I think
to go to the second part of this note
which is it tells you what version the
prefer the performance bug was logged
against and here you can see that
there's 11 203 ones fixed in you can see
twelve 212 111 204 and so on and there
may be back ports available already on
1104 11 203 so if I was migrating from
11 203 forward I would look at this note
and say any of these where it says expdp
file for 11 to 11 203 I would look at
those to see if there is maybe a back
port that you might need for specific
fix so that would be my advice there now
beyond that we do have some general
hints and tips for
exporting when you're migrating with
data pump you do need the exp full
database role now that is a very
powerful role so only grant it to people
who really need it
because it includes things like to
become user privilege because we need to
be able to export everybody's objects as
those users but to become user privilege
is super powerful it would allow a user
for example to become sis
so you don't want that a XP full
database to be granted to just any user
we also recommend against logging in as
sis sis has some very powerful
capabilities and allows you to basically
do whatever you want in the database but
it also doesn't do parallel DML and it
has some performance issues with it so
if you want good performance with data
pump you system not systems no reason to
you sis with data pump as long as you
have the right roles you're fine now if
you're exporting flow a running database
then and this might be the case if say
you just want to export a snapshot from
your production system and bring it into
test or you're going to export and data
pump and combine that with gold engage
to minimize downtime then you're going
to want to have a consistent export and
data pump just like original export is
consistent on a per table basis which
means if you have a table with a hundred
partitions in it and you do an export
with say parallel 12 we'll put one
worker in one partition per worker and
we'll get all those partitions done as
of the same SCN but when we go to do the
next table if there's stuff going on in
that database we will start at a new SCN
for the next table so as you can see you
might not get consistently consistency
across
across tables if you don't specify
flashback as the enter flashback time of
course if your database is quiesce
that's not a problem but often you want
to do this on a running system so you
either specify flashback sen or
flashback time or you can even now say
consistent equal y we implemented that
syntax it really just is the same as
saying flashback time equals this
timestamp but be aware that when you do
that you will need to retain undo for
the duration of the export and that
could be a long time we always recommend
that you exclude statistics on export
importing statistics is slow it's gotten
faster over the various versions it was
about twice as fast an 11-2 as it was
intend to but it is still relatively
slow compared to rebuilding the
statistics in a new environment we also
want you to set this parameter metrics
equals yes it'll give you a lot better
information in log files about how long
each object takes to export or import
now for performing sprat purposes of
course whew oh you want to use parallel
with data pump data pumps a lot faster
than the old export import anyway but
with parallelism we can get really
really fast especially when you have
lots of tables or lots of partitions to
move and in 12-2 we added parallel
export and import of metadata in
addition to data so on 11:00 to 12:00
1:00 in earlier when we import met
import metadata it's one thread doing
all of the metadata all of your indexes
all of your packages and so on done with
one thread well with 12:2 we can import
metadata in parallel and we backported
that capability with this patch that you
see listed here two two two seven three
two two nine four eleven two oh four
twelve oh no - we
backported it only for constraints and
indexes because those are the easiest
things to do doing things like types in
parallel takes a lot more work because
of things like type dependencies due to
type inheritance and there are other
things that have those kind of
dependency chains that we couldn't
really back pour it but if you get that
patch then you can import indexes and
constraints in parallel in 12 102 or 11
204 we mentioned excluding statistics on
export and then another thing that you
might want to look at in 12c is
disabling archive logging especially
when you're doing a full import
generally the reason you have archive
logging is to be able to recover from an
error and with data pump you can recover
from a problem with import by either
restarting your data pump job or redoing
the import so you may want to just
disable archive logging for your import
so that you don't have lots of redo logs
switching and filling up now I have
found that this isn't a big performance
issue unless the log writer is your
bottleneck the log writer is very high
performance anyway but it can be that
you just don't want to generate
unnecessary archive logging and you can
turn that off if needed now of course
you can't do this if you have force
logging enabled such as you're in a
standby environment and you're importing
into your primary you wouldn't want to
turn off archive logging in that case
and there may be other cases where you
have force logging on and it won't you
won't be able to turn it off for data
pump but it was a new feature we added
so it was worth mentioning so
some tips about data pump migration in
general one thing that was mentioned in
the question was using network mode
which is a great idea with data pump you
can import directly over a DB link and
this was this was added at the very
beginnings of data pump and we've been
using this for years and years it's nice
to be able to not have to write out a
dump file copy the dump file over and
then import the dump file you can
actually set up a DB link back to your
old system and import over that network
link you saw the how we did that for the
full transportable for example now the
performance here is going to depend on
the network bandwidth and on the targets
especially the target CPUs a little bit
on the source of CPUs in the case here
where this question was from somebody
who's migrating from older exadata to
the new exadata
both the network bandwidth and the CPU
should be plenty good to really saturate
that network if you specify a huge
degree of parallelism we've had
customers achieve over ten terabyte per
hour with with DB links going from one
machine to another that was over a ten
gigabit network now one of the new
features we added in 12-2 was the
ability to move long data over DB links
database links do not support selecting
long or long raw columns over a DB link
they never have so this was always an
issue for data pump because when we go
over DB links usually what we use is
external tables and we're using a create
table as select over the DB link in
order to do this migration well that was
a problem for Long's so in twelve to one
of my engineers figured out how to
basically support direct pass load over
a DB link and that means you can use
that for any tables where you have long
or
rock columns so that's that can be very
useful because often those tables would
long and longer our columns are pretty
big and you don't necessarily want to do
the export and import to files and then
move those around so those were the two
questions we had in advance I will now
get to the ones that have come in in
chat I just want to remind everybody
that the upgrade blog is where you can
find all the information about database
upgrades and migrations including the
data pump side of things it's at Mike
Detrick de dot-com and we have a slide
download Center where we put our slides
also of course you can get the recording
of this along with the slides from the
office hours website but let me get to
these are the questions here let's say
does dua automatically undo
automatically if I check make
tablespaces read-only we have dr so
guaranteed restore points not useful
will count reload dot PL work for a
failed upgrade who a lot of things going
on there okay so the the option that DB
you a has to leave table spaces
read-only is based on technique that
we've used in a manual way for a long
time then it will not automatically
restore from that but the idea behind
that option is before your upgrade if
you make if you shut down the database
and make a copy of all of your system
table spaces and that would mean your
system system since oxygen like CT axis
XD visas and you'd want your temp in
undo and so on you copy those off to the
side
and you have your user table spaces read
only because the upgrade isn't going to
touch your user data then if the upgrade
fails for some reason or even if it
great succeeds but then you want to do
some read-only queries afterwards to see
how's the optimizer treating your
application and maybe you don't like
what you see you can go back to the
previous version by simply copying back
that set of system tablespaces you saved
and then opening the database in the old
Oracle home because we haven't updated
because those tablespaces user table
spaces will read-only they can just be
reopened without any difficulty because
we won't have updated the compatible or
anything like that so that's what that
DB way option does it doesn't it will
not automatically undo if you set the
tables if you say make tables faces
read-only but it if you copy off the
that set of system table spaces it will
automatically set the table spaces
read-only after it runs the free upgrade
check Android is this mic here quick
addition to that don't use this before
12:00 - because it wasn't implemented
correctly in DD way so just implement it
correctly and 12:00 - before that they
forgot something and the restore didn't
really work so just wanted to add this
Thanks
so um will can't reload that PL work for
a failed upgrade that would be downgrade
so will a downgrade work for a failed
upgrade you know that would be I I'm
gonna say that depends I honestly don't
know because I can't make a general
statement there because an upgrade could
fail for a number of reasons and I'm
sure that some of those could also cause
a downgrade to fail so there are going
to be some types of failed upgrades
we're running a downgrade and then cat
reload will recover but they're going to
be others where it won't
let's see I'm not sure what you mean by
a guaranteed restore point not being
useful for deal with a dr environment
though because generally when you
upgrade in a when you have a primary and
a standby what you would do in that case
there if you're not using data guard
rolling upgrade then you would generally
stop your redo apply during the upgrade
so that you can upgrade the primary make
sure everything's okay before you start
shipping redo to the standby so in that
case a guaranteed restore point would be
very useful yeah and if you're using
data guard rolling upgrade then you know
then you're going to have a situation
where yeah you could still do a
guaranteed restore point on your standby
I'm not I'm not sure why a guaranteed
restore point would not be useful in a
dear environment I can't wrap my head
around that one so if you want to
clarify that I'd appreciate it okay I
have a clarification sure the issue is
this we have roughly 150 to 200 terabyte
animÃ©es and our our strategy is to
switch the primary to the dr and and
basically they keep the primary down
while we are upgrading the older dr now
currently new prod okay so we use the
let's say we use the guaranteed restore
point
something happened whatever it is we
have not changed the compatibility and
everything looks everything stinks so if
we flash back to the guaranteed restore
point we have to open it reset locks
that basically means we have to rebuild
the standby and standby is 150 to 200
terabytes and we don't have the space we
don't have any we we won't be able to do
it
that's a problem with the gallon flash
back at the guaranteed restore point
after a failed upgrade or after
successful upgrade but you know initial
testing shows that this is unworkable
something terribly went wrong everything
is frozen that kind of situation we we
flash back to the guarantee restore
point but then we have to rebuild they
are that's not possible
so okay okay I like I'm trying to so I
mean we may I quickly I think that the
issue is that you should do it ever
yamo be around you should use your
production database you should drain
your production load set a TRP and then
upgrade there and while you're upgrading
you take away the lock transport from
the standpoint so if anything goes wrong
now destroying the production database
you would fail Oh what is that boy which
is still untouched and whenever you do a
flashback and the recent locks that
reset locks and the incarnation going up
will be propagated to last and probably
the rear locks as well yet the problem
you're seeing happens basically because
you're opening descent by with a recent
locks and then you pump up the
inclination and if you haven't done a
proper switchover before really
exchanging your owns between the two
databases that incarnation will never
get propagated and then you're in a
terrible situation sorry the proposed
various that you upgrade your production
and not your standby if you want to do
that then the right way is transient
large extent by rolling upgrade where
you start off with a guaranteed restore
point on the production first and then
you make your stand by a logical standby
for a short amount of time
upgrade it and then you switch over that
would be really open but don't upgrade
your standby as the life upgrade that's
not the proposed way and not best
practice yeah we are thinking of doing
the switching first
sync up everything so d'art because prod
so for all practical purposes it's a
prod right just that we want to keep our
fraud down our old brought down just in
case something horrible happen
have you done have you have you done a
proper switchover and exchanging the
roles before and then then this will
never be a problem because then your
stunt buddy comes to production and when
you flush like now you increase the
incarnation of the database but you will
propagate this to your previous
production we had a reader lakhs which
is known your official step by at this
point still I don't get why you exchange
that role before because it's really not
really not necessary in that case you
can just an extra work on you cause an
extra downtime for the switchover
operation which is not bad it doesn't
harm you or but it's not necessary in my
opinion here okay that's fine so what
you're saying is I understood what
you're saying is even if I open the my
production in open reset locks
I can still propagate the same thing
stand by and and I can still read sync
yes I don't I don't have to rebuild my
stand by blocks thank you Mike I'm glad
your plane has not taken off yet not yet
but I'm in the plane already okay I was
looking already bad on me so I'm the
next one saying if I have eleven 200x on
the same machine you don't have to
physically move the data files and can
use impdp to attach the files to a new
CD presto yes that is that is basically
the the fastest way in general to move
what the the time there will be
basically determined by how long it
takes to do a metadata export and import
of your database so if you want to get a
sense of how long that will take then
you could just do a data pump metadata
only export and you know and import into
some dummy database and that will give
you a sense of how long it would take
for the full transportable operation
next one I had mentioned that sis has
some restrictions on there so there
there are some things that sis cannot do
in parallels very strange this is a very
strange user and I have really tried to
find official documentation all I have
is internal emails about this about what
it can in some in some cases can't do
but there are cases where it cannot do
parallel DML which of course data pump
depends on heavily there are other cases
where sis is too powerful because it can
get around locking protocols in the
database so that's why those are the two
reasons that we don't recommend using
sis I wish I could give you more details
on that but basically go with system not
sis when it comes to data pump
let's see uh actually ken rolling
upgrade using transient logical standby
be achieved in a hybrid Wow you mean the
other way around right when you said
your system system system not says yes
ok thank you that's by the way does this
we have a problem with our lobs
okay these are secure file lobs when
regular lobs they're all single threaded
and it's been frustrating those tables
are one terabyte and up we could be
spending 16 hours to do the backup of
one table and so we open an SR and they
said you know you secure file ops but we
unable to go to the secure file ABS yet
so but our we bisque always use the
cysts in order to do the expdp so the
question is if we go to system would
that solve the same this problem also
unfortunately no the problem there is
that old basic file labs do not allow
for parallel DML either into or out of a
table with LOD columns period it's just
built into their design they are single
threaded and that's frankly one of the
two reasons that secure files were
invented the other reason was the
scalability of those lobs the old lobs
they just don't scale the way that
secure file lobs do yeah I will tell you
if you do go to secure file lobs you
will love the performance might you
might look at and we can talk about how
to migrate basically using online
redefinition would be your best way to
get there if you're talking terabytes of
data but it's something that you should
definitely be looking at people have
complained about lob performance with
export import data pump for a long time
and that is out of our control right now
you mentioned expdp to your system
instead of sis but would you just say
the same thing about I am PTP our impdp
doesn't care I would I would say the
same thing for I
P DP as well.you system and have the
right roles and that's the right way to
do it that's awesome response let's see
I have a question here time equals imp
definitions meta any other remarks
regard to this method I'm I'm not
understanding that question from Pete
what does the if you can clarify that
yes okay yes okay got it the migrating
by plugging an old version into a
container is my preferred way of doing
it because it has the simplicity and the
speed and if you do it on the same
storage you don't have to move your data
right so did this is what I would
recommend everyone so and the question
is then are there any other general
remarks using that method are there
restrictions things that you would
recommend we think of for round that'd
be good ended and the aim is to give a
bunch of DBS the simplest and best
understandable methods of going from 11
to container pluggable database thanks
yeah so with full transportable I would
say the the the restrictions that it has
are they've been greatly reduced over
time one is that we don't do metadata in
parallel for transportable jobs yet that
will be coming sometime not in eighteen
but maybe in a release after that so the
metadata performance won't be quite as
fast as if you're doing a regular data
pump import but the other thing is to
think about is just in general
transportable tablespaces does have a
set of restrictions on it
um one nice thing about it is you can go
cross-platform with this you can do an
arm and convert on your tablespace data
files for example so if you were going
from AIX to Linux you can use full
transportable for that that's very handy
compared to trying to move across
platform first do your upgrade and
plugin but depending on the version
encryption and going cross-platform
can be difficult when we get to 12 -
actually data pump will handle that
automatically for you but with 12:1 that
that can be an issue you have to make
sure that you have the right timezone
version in your pluggable database you
have to make sure that it has exactly
the same character set so basically any
of those if we look in the I want to say
it's a database administrator guide let
me I'm gonna try to pull this up on the
fly here so hopefully you can be patient
with my typing here we're gonna look in
the administrator's guide about
transporting data across platforms
because that's where you will get the
restriction
about various types of transport about
transportable tablespaces let's see
resource distributed database management
let's see our Doc's have gotten a little
bit better online but our search is
still not the best uh-huh see I took no
I'm not going to keep pursuing that it's
not gonna get me anything so but you'll
want to look in the administrator's
guide in the chapter of about
transporting data across platforms there
are specific sections for full
transportable which can handle a lot
more than regular transportable for
example it can handle encrypted
tablespaces going across platform
whereas regular transportable can't but
basically other than that it's been a
very successful feature next question
was can rolling upgrade using Trangia
logical standby be achieved and hybrid
cloud dr model as well the answer to
that is yes you can do that in a hybrid
cloud what you're gonna find is though
is with a hybrid cloud model where you
say primary is on-premises standby in
the cloud the the most difficult part to
me is is getting the connectivity from
the cloud back to your on-premises
database that I don't find any decent
documentation on how to do that it
basically says this is an exercise left
to the reader and no and that's because
it's very simple you can do things like
create a DB link from the cloud to your
on-premises database if you can open the
right ports and that is always the
question for example working in Oracle
as I do if I went to our global IT
department and said hey could you Oh
at this port in a global firewall for me
so that I can have a DB link from my
cloud database to on Prem they would if
they were feeling nice they would laugh
at me there's no way they're gonna do
that on the person by person basis so
then you end up having to do things like
VPN tunneling or something like that to
get that two-way communication going so
it's possible to have dr in the cloud
with just one-way communication that's
the easy part
it's doing the backwards communication
from standby to primary which is needed
for a rolling upgrade that's going to be
harder if you can get that set up then
by all means yes go ahead and you can do
that transient logical standby in a
hybrid hybrid cloud environment so those
are the questions that we have the
question is you mentioned somewhere ago
VNC Terminal vanishes in the good DB you
a what situations does it vanish you
mentioned that definitely yeah he
actually had we had a customer who who
talked about this where they're the VNC
session just crashed okay so they they
were remote server and to us yeah and it
was just a VNC era
now the problem there with DB UA of
course is that you're partway through
your upgrade and your terminal session
is gone that means your database is
partially upgraded you can't restart it
with DB UA
because DBA will always open the
database in the old Oracle home first to
try to run the pre upgrade checks and
then it shuts it down moves to the new
home to do the upgrade well if you're
partially upgraded then when it tries to
open it in the old home it fails so if
that happens to you then your your only
option at that point is really to do the
manual upgrade you can rerun the manual
upgrade and you can actually have it
just pick up
where the upgrade left off if you use
the - capital R for a restart in 12 -
then it will figure out what phase you
were on when you when the upgrade was
terminated by in that case the DNC
crashing and it will restart so if you
were on phase 59 it will start on phase
59 and go from there so that's the
general advice if if dua crashes or if
the NC crashes or something like that
you can restart the upgrade with the
command line right okay
yeah I understood but cat CTL darky l
would crash because it's it's east of
that Greek goes away exactly at 64
percent each time what we tried it and
so it looks like something like a sequel
script or something oh well that okay
that's so that's dua crashing not VNC oh
yeah I meant dua crashing sorry okay
that I would I would really ask you to
file an SR about that I mean if it's
stopping at the same time and this is I
assume on one database not on all your
databases yeah one database say that
yeah to me that would be a case where we
need to see the upgrade launch and
there's obviously something going on
there that's causing a problem in the
upgrade I couldn't tell you what 64
percent is because I don't trust the DBA
project product you also just now
mentioned DB you eight to twelve point
two we are actually trying to go to
twelve point one because we know the
vendor tools are twelve point one but
you know is there a more are there more
problems DB you and twelve point one
compared to point or can we just use the
12.2 utility to upgrade to twelve on one
just closet over no 12.2 utility where
you would have to be going to twelve -
okay you can't use DBA for it basically
is very tightly coupled with a version
of the database right but - upper case
our cat CTL - Africa's are will work
with four point one also in twelve one
you would have to you
give it the phase to restart from so if
you look in your so we we implemented
adjust the - are in 12 - however you
could still do the equivalent by looking
at the log files figuring out where it
was when it left off like and you can
look for phase underscore time
underscore start and in your log files
and you'll see what phase it was on when
it failed and then cat CTL you can give
it the - P for phase lowercase P for the
phase and it will start at that phase of
the upgrade okay
the one other thing I have used that we
have a dr situation which makes things a
little complicated so we go to from
nancy debut to CD be using that describe
and create XML and all of that stuff and
we use loca let's say we use no copy and
so that we don't have to move our 150
terabyte 200 terabyte there are base to
a new location we don't have space for
it so now obviously have to sync up
right I mean so I mean then we think
about I mean we obviously move the dr to
the new oracle home etcetera sync up
would happen the same way as before or
any additional little things we need to
do for when we go to on CD b to c DB
with no copy option right so so with the
whole thing about dr when you're dealing
with pluggable databases that has a
whole set of rules and things to
understand about it but for this
particular case first thing that happens
is when you create your new CD b and
plug into that you now have a new
database right so that means you
have to have a new standby as well you
would need to create the stand by for
that cdb first then when you're using
the no copy and doing the plug-in as a
new pluggable database that create
pluggable database command will get
replicated to the stand body so as long
as all the data files in the standby are
in exactly the same location as they are
on the primary then that plug-in will
work and it will be replicated to the
standby as well so that that's how you
would move that database from being a
non C to B to a PDB and retain that dr
yeah it would require it does require
that you have identical directory
structures which you probably you know I
think you have to have that for a
standby anyway but yeah that's you would
want to make sure you have the c DB
standby already set so that we have the
credible database obviously what we use
our managed files and everything so yeah
so everything is you know basically
following the standard right procedure
of OMF so we basically create a new
primary c DB empty database a new
primary standby Sudi be empty database
and basically do the nozzoli b'stilla be
with the xml and with no copy and then
basically sync up you know the whole
thing right okay straightforward but any
rules you said a lot of rules I mean any
oh well that's it's it's what happens
when you want to do things like add
another pluggable database there you
have to then do the same thing where you
have to get the data files for that
pluggable database like if you want to
plug in a database from another c DB
into that one into your primary
then you have to get the files in place
on the standby as well as the primary
before you do that create pluggable
database to plug it in because the files
the Data Guard replication won't move
files it wounds redo so if you're
dealing with files like you are when
you're plugging in a PDB then you have
to have the files in place on both sides
right when you do this thing that when
you create a new standby right
and now you've dislocated moved it right
now is it going to copy the whole nine
yards of this 150 we're going to have
one 3 DB with 150 terabytes or 200 a
verse of that user data user data right
is it going to sync up the whole nine
yards of a drawer it will also move
those standby I mean we
yeah so all it will do in that case is
it will replicate the create pluggable
database statement and then it will
replicate everything that happens during
non CE db2 PDB sequel and that's
enduring 9 c DV 2 p DB sequel what it's
going to do is it's going to hook all
your data files in and it's going to go
through the dictionary and all of that
and replace everything with metadata
pointers it will not replicate the files
ok that's good that's great ok one last
question before we wrap up moving a
multi terabyte table with blob and cloud
columns for 11 204 - 12 - a 1
experimenting with network link i'm PDP
vs. DBMS parallel execute and standard
insert select over a database link are
there any key advantages of one approach
over the other that I'm not considering
there are not
to my knowledge any advantages over of
either approach if you are using basic
file lobs neither one will give you
parallelism
you can say DBMS parallel execute with
standard inserts but it will still what
my alright what I'll say is it will
you'll find that each parallel job will
wait til one till the previous one is
done because you cannot do insert as
select in parallel into a table with
basic file lobs if that's secure file
lobs then either one will give you
parallelism and people do one or the
other for you know kind of whichever is
easier for them because moving a table
isn't that isn't that difficult so I
don't think basically when we do it over
a DV link we are using Create table as
select so or if you've pre created the
table we do any sort of select so we're
doing essentially the same thing so
hopefully that answers your question
well I'd like to thank everybody for
attending this session of office hours
we're right up at one hour now time
flies when you're having fun I'm not
sure what our time slot is going to be
for next month we may have to change it
due to some travel and by next month the
US will be Reis inked with everybody in
Europe on terms of daylight savings time
again so things might shift around a bit
but keep your eyes open for that and we
hope to see you and more at that time so
I'm going to stop recording now and
thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>