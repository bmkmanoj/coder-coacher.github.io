<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>DistributableStream: A Java 8 Stream Computational Model for Big Data Processing | Coder Coacher - Coaching Coders</title><meta content="DistributableStream: A Java 8 Stream Computational Model for Big Data Processing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>DistributableStream: A Java 8 Stream Computational Model for Big Data Processing</b></h2><h5 class="post__date">2015-06-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Wdi0Ci1cc-Y" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so this is joint work that I'm going to
be talking about today on a neural
interface for writing parallel
distributed programs on top of the Java
8 platform this is joint work between
three different actual different groups
the Oracle Judd the Java team with brian
and paul who mid maybe some of you have
been to their talks already on on java 8
today or over the last week brian oliver
who is in the oracle coherence team
which is a middleware product and
asurion on I who are in the database and
big data appliance team which is our
Hadoop product and so we work together
to to try and address this problem we
had with how can we make it easy for
people to write distributed applications
on the Java platform and so there's
first of all there's I want to give a
little bit of background about Big Data
and what it is and why it's got so much
hype basically the idea here is that
people are looking for looking at new
sources of data coming into their
applications and if you sort of think of
the history of data processing from you
know ancient Sumeria where basically the
Kings would keep track of their assets
and they would have those carved into
stones to keep track of how much stuff
they owned because if you're a king you
know it's what you own and and in the
early data processing that you know
keeping track of general Ledger's where
you keep track of your assets then
things moved on to keeping track of your
transactions you know all the movement
of day of assets from one place to the
other and now we're beyond us we're
basically tracking behaviors so and when
you think of the monetary value of each
behavior of what is the value of each
mouse hover or each eye movement as your
browser is looking at you and the answer
is for each individual one of those
entities it's a very small small value
but then taking an aggregate it can
actually have a large large value so
when you have high volume high velocity
because if you think all of the billions
of people and all of especially we now
going to the Internet of Things all
these little devices all you know
phoning home all the time saying you
this is what I've done in the last 14
seconds you've got a lot of data coming
in and when you also think about every
one of these apps every one of these you
know components on a jet engine every
one of these you know your your knit
your nest thermostat your refrigerator
and all these things are phoning home
with all this information and it's all
in very data formats it's not like
there's a universal XML or JSON format
for refrigerators to tell you what is
your running out of in your refrigerator
so that means you've got all these
bunches of varied formats but there's
big value in them our data and that's
what we're trying to take advantage of
in this whole sort of big data space now
when we look at the software what's the
there's also been a progression over the
years in what kind of software people
have been looking at for this and for
big data the first thing with
scalability and if you look at the
initial Hadoop systems that came out the
focus was how many nodes can you get
working and it doesn't matter how usable
or scale or efficient each node is what
matters is how many nodes and what's
been coming after that is the focus on
usability how can you make it so that
mere mortals can program these things
efficient and program them in a
reasonable amount of time because when
you come up with some new way of making
money you want to start it quickly and
before you're a competition can get to a
toe and so having usability be high at
the programmer level is a big emphasis
in the industry now and finally the
thing that hasn't is just sort of
starting to poke its little head up is
efficiency / no deficiency
which is how much work does each of
those thousand or ten thousand nodes get
and and with people are now pushing on
is sort of saying okay you can solve
this problem with 1000 node cluster but
when you rather buy a 500 new cluster
they can do the same thing and so per no
deficiency is starting to become more
important as well and also are very
kinds of hardware if the in the
enterprise space you know people buy you
know these are for 832 processor systems
that all work together and can share
memory and but there's also scale out in
the commodity space and this decision
about oh do i what is the sort of magic
unit of scalability that I'm going to go
for is it the two socket system or is
the four socket system or is it the 32
socket system that's going to change
over time as we look at efficiencies and
the kinds of problems that people are
wanting to rid us so the job echo system
was built out of ideas that came out of
Google but was then reimplemented at
Yahoo on top of Java and Java and the
JVM languages are pivotal to this entire
approach and there's two main things
that I've have that sort of drive job in
the space and why Java is pivotal and
that is the ability to store code and
access the code remotely so that the
code can ship around very easily and
with an can run in a in a sandbox and so
if you look at for example how does a
data catalog work in the derp world do
you know how it stores the format of the
data it doesn't sort of have some JSON
document that describes what the data
looks like it stores a class file and so
you talk about being heavily wedded to
the Java world
it says when you want to interpret
interpret something's in this data
format you open up this class and you
use this class to access your data so
that really ties the the format of the
data and how to access the data to Java
and what is it by it buys this
incredible extensibility is that when
you come up with a new data format you
and you describe it with a class it
doesn't matter even if that data happens
to be local or remote so you can have
data access methods that go out over the
internet and pull the data in from
wherever it is using whatever code it is
that you I want to supply so it
basically allows for incredible
extensibility in the kind of data
formats and servers that can be accessed
the other place where Java is important
is in computation shipping so when I
write a computation and I wanted to be
distributed do I I don't want to
basically have to sort of say oh here's
my C program I'll compile it into a
shared object I'll take that shared
object and I'll ftp it over to 500
places and then I'll load it and hope
that it doesn't screw up my machine java
serialized objects are a much easier way
of going and so from day one when you
have a map map produced program the
actual map or a reducer is an active
object that can be serialized and
shipped around the network and that kind
of capability is a capability you just
don't get in other languages and that's
why your job has been so important in
this world because if I want to run
something on a thousand machines that
something is going to change every five
minutes and I need to go up and take
that that's something that I want to run
and distribute it out so Java 8 Java 8
i'm sure you've been hearing about a lot
over the last week and one of the major
features that everybody's been waiting
for and is are the lambda lambda
expressions and one of the things that
people want lambda expressions for is
for driving this newer generation of
libraries and one of the big libraries
that's been coming out is the job aid
streams how many of you have been to one
of those talks on Java 8 streams through
the week ah nearly everybody terrific
and so um Joe so age the Java 8 stream
what you can do is it basic gives you a
kind of extensibility the way I think of
it is you get to define your own new
control structures because it makes it
so easy to write these lambdas that can
fit in to various places that you don't
need to write a for loop anymore you can
write a for procedure and for each
procedure that does the work for you and
dupe has passed down your code and now
these the implementation of that for
loop is now defined by the library not
by the language and that's what allows
parallelism and streams and if what
allows distributed parallelism in
distributed strengths so the model here
is you've probably seen this have you
seen this exact example with what's
going on here is that we take an input
this procedure takes an input stream or
this method rather takes an input stream
and as its input and basically parses
each line of that input stream and then
then returns a stream of the parsed
elements which are then collected into a
map which counts the number of item
number of words and the the only thing
that sort of semi confusing perhaps as
the is the to map method here which
basically takes a set of lambda
expressions basically of saying okay
where do I get the key with the key for
a word is basically itself what's the
initial number of references for that
word which is one and how do I combine
multiple words multiple entries for
words that are the same which is
basically add them up and so that's
basically your combined method is the
last one and so
in this very succinct notation you can
write these kinds of parallel
applications and notice that this
application doesn't care where the
stream came from it's a give me any old
stream as long as it's a stream of
strings it doesn't matter whether it
came you know from a Java collection or
a or a file or whatever and so the model
of this application is that every stream
is basically a pipeline of intermediate
operations ending with a terminal and
what we've done is we have extended this
model for distributed and each of these
intermediate operations returns a lazy
stream basically a late evaluation next
stream which when it finally gets passed
down when you finally get to a terminal
it actually looks at the data structure
that's been built up and evaluates it
and that that terminal operation is what
triggers the evaluation of the data
structure so it's a little bit a more
like language processing where you sort
of think of yourself when you have a
language that you read in that sort of
tells you what to do like let's say for
example I happen to be in the oracle
database team and what are the things we
do is we read sequel and guess what we
do with far sequel we turn it into a
data structure and what happens when
somebody says run the darn thing is we
take that data structure figure out how
to run it and then run it and that
terminal operation it takes that data
structure that would sits to the left
and whether it came from a parsing a
language or from this collection of
stream operation it's the same thing
it's been unavail it can actually
optimize it and evaluate it in the
proper way and that's exactly what
happens in the streams interface and the
most complicated operation in the
streams interface is probably collect
and I don't know if you guys have bit
seen 13 this kind of a slide before
but basically it has a model that when
you want to combine things and this is
sort of the basic sort of primitive of
how you combine multiple streams
together multiple data items together is
that you take a stream each each the
stitch stream then first calls a
supplier to make a container and then
each new item from the stream get
accumulates into that container and this
can happen in parallel because of course
streams are parallel and each parallel
stream gets its own container which it
accumulates to and then you can finally
combine these multiple containers into a
combiner to get sort of your final
container which you then run through a
finisher to get the final result so this
is the sort of basic structure of how
you write complex operations that take
multiple data items and basically smoosh
them together the the simpler kind of
hyper item operation like map that sort
of says who or take one item produce
another item and flat map which says I
take one item and produce multiple items
and so those are the simple guys this
collect one is the is the hard one but
one of the things that is done inside
the stream library is it sort of set
this is a little tricky so what we do
actually is we use that as the
underlying primitive and on top of that
build various simpler primitives like to
map that you saw in the previous slide
that uses this basic collect framework
collector framework to do something
that's a little simpler like build up
build the map out of your of your
incoming stream but when we saw job
eight streams we're in the distributed
systems a dupe team and we sort of said
hmm this is terrific as long as you're
inside of a single JVM and you know when
you're a distributed system snob you
basically sort of say anything that runs
inside of a single JVM is boring so you
and of course the rest of the world
doesn't always feel that way but you
know we're distributed systems stubs so
basically said okay well how do we make
this thing distributed because we want
it to scale out to multiple things and
part of this world that makes it
complicated is the distributed systems
world is cut as a bunch of different
stuff it and now even if you take the
subset of distributed systems that is a
dupe it's getting more and more stuff in
it so there's this you know panoply of
different products that people are
pushing in and basically multiple data
sources multiple engines multiple
languages and when you put those all
together you've got a pretty complicated
world and what we wanted to do is to
make that world simpler for programmers
so that you don't have to learn every
single tool under the Sun you can just
use a single API and it will look at the
facilities that you have and then figure
out how to take your intentions and map
it to your underlying resources that you
have available and so of course people
might have ted's which was really being
pushed by Hortonworks you've got hive
hadoop you know got oracle 12 these are
all engines they know how to take beta
and manga and which engine is right for
you that's you know that's basically
that's a market decision and what you
don't want to do is as an engineer is
you don't want if your buffs you know
suddenly makes a makes a contract with
somebody else to provide a new engine
for you you don't want to say oh that
means I have to reprogram all my code
and you don't want to do that and so you
want to basically live up above it and
so the idea here is that we want to have
a friendly programming interface a
simple computational model and let
everybody be able to do the two right
you know big data applications and to
allow people who take those big data
applications and be able to run them on
any engine of their choice so when your
when your boss comes and says
uh you know your boss is eager or to get
you to get onto a new platform you can
get that get on that new platform
quickly because you don't have to recoat
everything for it and the and this is
made more complicated by the fact that
you've got lots of different processing
requirements for an individual
computation you might have and for an
individual engine so when you think
about oh where do I want to run this you
have a lot of different sort of
variables you're trying to optimize for
you might be a trying to optimize for
you know the cost of acquiring the some
this product you might be interested in
which engine is closer to what data so
let's say I have some data and I will
now want to clean an address address
cleaning usually involves having
basically a list of every possible
street name in every city and every
range of addresses and stuff like that
to do address cleaning well sometimes
it's easier to ship the the address to
be cleaned to an engine that already has
the database in its memory and so
sometimes you know so some you know so
you've got to basically get two pieces
of data together and so do i check the
address and ship it to the address
database machine or do I take the
address database and ship it to the guy
who wants to process the addresses and
basically we want to sort of say we
don't care we'll give you the facilities
to do things the way you want to do it
and that's so we want to fit so
basically if you have an engine it can
be distinguished by the fact that it has
a GPU by the fact that it has a bunch of
memory by the fact that it has a big
database already loaded and all these
things make these different compute
engines different and you want to use
the right engine for the step of the
task and this is one of the big things
is that it's not just mapping for this
not one engine / job it's an engine /
job step and so that you want to be able
to build a computation that
those through the multiple engines that
you have available using the best one
for that particular step of the drop
because address cleaning is not is not
the entire job is not the work all the
work you're doing you're doing address
cleaning then you're doing matching then
you're doing risk evaluation and then
you're looking at access logs and so as
your computation the nature of your
computation changes as it flows this as
it as you get through these job steps so
does the engine you might want to use so
basically this is the sort of big table
takeaway is we want to do distributed
engines for processing distributed data
sets and end in a federated way so one
question you might ask is you know what
why do this in Java at all I mean this
is Java one so you probably won't be
asking this question but in hey in other
venues people some people say why drop a
wedding I button it program it use this
in sequel or anything else and the idea
is that you know Java is you know been
there is a big move to run sequel on top
of big data and that's a terrific thing
and we support that a big data sequel
operation you know we've like Lex equal
but there are certain things where Java
can make more sense because Java isn't
more of an algorithmic language what
sequel is it's a way of specifying what
you want to do and it's up to the
implementation to get it out for you but
it's not a great great language for
writing a new algorithm so when you're
doing a new machine learning algorithm
that's going to distinguish your company
from all the other ones damage you want
to write your algorithm and you don't
want to be limited to what the language
capabilities are and we didn't also want
to do another data parallel MPP system
guess what we already have a bunch of
those and we you know as a company and
as a world and so what we wanted to do
instead was to do a
api that all these different or a bunch
of these different MPP systems could
plug into and so that you could use the
cyst the engine of your choice you know
running this kind of computation and so
now we get to the actual the meat you
know so now we've now we've gotten
through you know why we're doing it and
now you know what it is that we've done
is we've defined this new abstraction
called the distributable stream now it
takes a little it's a lot of syllables
there uh-huh but the idea is it's the
you know sort of traditional way of
naming your new Java interface and so a
distributive both dream is not
necessarily a distributed stream it can
be a local stream it can be lots of
different kinds of streams but it
follows the stream paradigm in that this
is a abstraction that represents data
that is actively moving from one place
to the other and it comes through the
string as a bit so so a stream is more
like an iterator and less like a
container so stream is basically you
know it's a single use you consume your
stream once and then it's gone and so
this is different from if you think
there's a couple of spark programmers in
here who riches in spark is more
container oriented with its rdds so here
if you remember we looked at the word
count example well here it is completely
rewritten to use distributable stream
and you'll notice that there's no change
well except for the word that we added
the word distributable a couple of
places and this was necessary in or
because one of the big things about a
distributable stream is it has to be
distributable which means that your
computations have to be serializable and
that's one of the big sort of changes we
had to make which is not rocket science
the rocket science comes in the
implementation of distributable stream
not the interface and that's a good
thing because you don't want rocket
science in your interface you want
simple things at your interface so what
we did is we basically said
okay there's a new distributable stream
and instead of taking just regular old
lambdas we take serializable lambdas and
and but then all the operations are all
exactly the strange same as those that
you'd see in a stream in regular stream
we have made a couple of extensions to
basically allow us to have fewer
terminating operations remember in
regular local stream whenever you run a
collect it's a terminating operation
because it combines and returns you
something we wanted you to be able
basically string together more
computations together so we actually
make have a nonterminating version of
collect called collect a stream which
basically takes the result of a collect
and puts it in a stream but other than
that it's basically the same thing we
offer and so it's we're trying to put
more capabilities rather than less and
the idea now is of course that this
distributable stream application can run
on any of these engines whether it's and
we actually have implementations all
working on the local thread pool on
hadoop mapreduce on apache spark and an
oracle coherence and so we've got three
to four different engines and the other
piece of distributable streams is data
sources because remember I said that
there are three big things that
distinguish these kind of systems one is
a language you're right in well that's
obvious we write in Java and we use this
API and what is the the engines we
support what kind of engines it's these
kind and then there's usually a much
wider variety of data sources you know
so one of the things that this engine
for this pin plug into is a very very
wide variety of data sources because
it's it's not very useful to have an
engine with no data and that's the whole
idea is you want to connect your engine
to data and perform a computation on it
so so we're now we're talking about
multi steps in a computation and why
would you want to run a computation on
different engines
it's because each engine has its own
sort of little bit of goodness that it
has that distinguishes it from the other
engines so for example we in the
scenario implementation of of k-means
which is basically an application that
is an iterative application that figures
out how to group bunches of data items
into K different groups so that each
group is similar to each other so you
can think of that as one of the big
applications that most people have used
of k-means is in Netflix um how many
have gotten here's to use netflix and
look at the categories that netflix
makes so they use a k-means kind of
algorithm to basically group the movies
into groups and then they use actually a
really sneaky algorithm to figure out
how to comb the group something that is
better than a group number 14 3 j which
early versions dead so so naming the
groups is also important but let's not
think about that let's just think about
finding what the group should be what
things should be in a group and so when
you're running a k-means algorithm one
of the things you want to do is you want
to do your initial parsing and filtering
so you have some data source that might
be anywhere usually your initial data
source is written living somewhere on
disk or you know SSDs or whatever that
you're trying to you want to get that
you want to pour through it really fast
and get the data that you want your
input data and so that first imposition
parsing and filtering where you reading
from disk what's the best engine for
reading from disk in this world it
happens to be hadoop mapreduce which is
a is very optimized for going through
this data but then you're going to go
through an iteration phase four basic
you're going to take your going to
compute a crappy result and then you're
going to you get that crappy result and
use it to produce a slightly less crappy
result and you're going to repeat and
repeat and repeat until you get a not
completely crowd a no sorry a good
result and and you usually define a good
result in this world by the fact that it
converges and that's actually a good
exam you know
sort of general approach and writing
these kind of algorithms is that you
have a a make better version of this of
the the previous result that then
iterates and then you have an evaluation
phase that sort of takes the result that
was made but by the iterative iteration
and sort of evaluates that decides
should I continue am I done yet and that
one so be it eration you want to run in
memory you don't want to be reading your
input data again and again and again you
want to be reading it out of MapReduce
into a into memory iterating on it there
and if part of each iteration you have
to make this single the point of
decision making of saying do I continue
or do I not so that is not a distributed
decision it's not like every little
machine you know machine decides that am
I done it's there's a centralized point
the ones who decide am I done and so
that makes sense to actually run on a
local engine and it actually works best
you can now do local used local
parallelism and so now you've got this
sort of this this computation that takes
place using three different
implementations of the distributable
stream each optimized for a different
different use case so now you might want
to know well how the heck do we do this
because this is it's a pretty simple
kind of model but one of the things that
is very important is to make this stuff
distributable sorry is make it
extensible because it's one thing to
sort of say oh we handle these two
engines and why would you want them
another one well you know the computer
scientists whether they said you know
they're usually as a in computer science
counting their 0 1 and infinity and
basically once you get beyond one you
sort of say infinity so we've got to be
able to allow any number of engines that
want to plug into this infrastructure
and so there's so there's so one of the
big things is how do we provide
extensibility the other is how do we
actually implement this at a language
level so I'm not sure how many of you
guys are language design mavens and know
what parameter contravariance is but I
looked it up on Wikipedia and and what
did this is basically is is that you
know this is a job it doesn't have it so
so don't worry that you've missed some
feature of drama this Java does not have
parameter contravariance for method
overrides and so one of the options is
that our news that we take our
distributable string which is in some
sense a a more it is a more restricted
form of stream basically a tape because
when you pass at a lambda it insists on
getting a serializable lambda and so
then you might think no the most natural
thing to do is have stream which already
exists implement distributable strong
because stream is now becomes a
restriction on what a distributable
stream is because it sorry the stream
now becomes sorry that's are you opening
up a wider interface than distributable
stream because the stream a local stream
can take either a serializable lambda or
non serializable lambda because it's not
going to ship it around so this is this
is one possibility and this is in some
sense sort of the cleanest option from a
sort of a language design problem but it
has this there's there's an issue in
Java we're not an issue it's just sort
of a let's say you know what it's what
happens is that serializable objects
have extra information on them to allow
them to be serialized Wow and so if you
declare an object to be serializable
it's going to have a it's going to have
more weight even on at new time for it
and so one of the things that the java
team said is don't screw up stream don't
make it slower because you know we want
a lot of people want to use stream for
are iterating through local objects so
don't make that worse in order to make
make it distributable and so we sort of
said okay I understand that and so we
said well I guess this thing doesn't
work the other option which is actually
the first one we implemented was to make
distributable stream a extend stream and
what we did is we sort of what we did is
we checked at runtime whether the lambda
expressions that we're paying passed in
were actually serializable and if they
were not serializable we threw an error
and but this is really super error-prone
because you want to write some
distributable program and you want to
find out at compile time if you screwed
it up and head and had something that
was non distri non serializable so it
was just too error-prone and so we ended
up with this approach which is basically
the cross wrapper approach I'm not sure
if it has sort of a pattern and the in
the pattern book but it's but it's I did
but it's a quite common sort of thing to
do so we have these two interfaces
they're independent but we allow what we
do is we make up our own subclasses that
actually wrap the other class so we can
make a distributable stream on top of a
stream and what that means is we
basically we have a special class of the
special engine which is the non
distributable local engine and that
engine can actually be used to wrap a
regular stream because it's not going to
ship it stuck so what you get is you get
a so if you have an application that is
distributable you can pass it one of
these local distributable streams that
is built on top of a local stream and
it'll work just fine because it does the
local stream doesn't care that it's
stuff is serializable the other option
is that we can make a local stream that
wraps a distributable stream and what
that means is that the distributable
stream has to actually change engines to
the loca from the remote engines to the
local engine and I'm going to shock you
about you how to change engines in
midstream and the next next couple of
slides so in any case the function
shipping all happens via java
serialization and what we doing is that
we allow the distributable stream we
actually build up multiple computational
stages which like in a computational
stage you might think of as a pipeline
in the in the local stream case and so
we take multiple pipelines we build them
up lazily as we can and then when you
finally get actually execute a
terminating operation we say oh no he
actually wants to run this thing oh now
you know because that's the thing about
laziness is you want to be as lazy as
possible until finally you can't be lazy
and you actually have to do work and so
at that point we actually run an
optimization process that goes through
this data structure and sees if it can
refactor and do some some work and it
does that at the data structure level
because it's built up this computation
that this data structure represents the
computation and then once it's finished
stop doing the optimization it then
sends these pipelines to the proper
strength the proper engines and so it
does it sends the serialized pipeline on
the other kit on the compute engine it
then receives that pipeline d serializes
it and runs it as a local stream so
basically what we're doing is we take a
distributable stream which we then build
up a data structure that represents
multiple pipelines we then ship those
pipelines across we then you've just add
water and they turn back into regular
stream pipelines which we then run on
the local machine and so one of the
things that this means is that the local
machines all the engines have to be
running java 8 as well so on the left
side of course you're using java 8
because that's where your that's where
you're running the stuff but
because we're taking serialized
pipelines and so it uses the stream
infrastructure on the right side as well
so the means that all the engines that
we are sending to have to be running on
the Java 8 a JRE it doesn't we don't
have to actually recompile them but we
do have to be running that running the
JRE so we break it up and so we use then
so the the other interesting thing about
it is there the engine interface which
is used for separating the low-level
details of the computational model from
the computation itself so when you want
to run on a particular engine how are
you going to say which engine it is
first of all there's multiple kinds of
engines MapReduce spark coherence Oracle
sequel there's all these different
engines out there and you have to say
specify what class of engine that you
want and you also have to say what
instances you want most people most
customers who is running running Hadoop
nowadays they run Hadoop in an
environment where they actually have
several Hadoop clusters all for
different purposes and so as you are
running your your job you will want to
be using different possibly different
clusters for different purposes and so
so here's the exhibitors an example here
of now taking word count and running it
on a particular engine so we basically
take our are a eunjin agnostic word
count example and now embedded in a
little driver that says oh i'm going to
run this on this engine and note that it
has to if we're going to run in a
MapReduce engine guess what you need to
do you have to configure your map reduce
engine to say which engine you want and
give it all the information that
MapReduce once and so each engine has a
different configuration that's specific
to that engine because each engine is
different it needs ass the different set
of configuration but it can run the same
program but with just different
configurations and so we've done to
actually allow you to program with this
is that we have
you we have an added a new primitive
which is the width engine command and
what width engine does is it says okay I
want to run the you start off with one
engine on the left side and you go along
and you're executing all in that engine
and now with engine now changes your
default engine to be a different engine
and so everything that's to the right of
with engine will run on that new engine
and so that's where we get this kind of
putting together multiple engines and so
0 7 45 i thought it was running behind
that it or not so one of the interesting
things about this is that when you
change engines data has to move and when
you want to move data you want to move
it in the most efficient way and there's
usually two different ways of moving
data from the base we use we call them
the upstream engine to the downstream
engine because I mean these the streams
and it runs down straight and so the
data has to go from one from one engine
to the next one and so one of the big
questions about you know moving data is
do you do push versus pull so should the
downstream engine push data too sorry
should the upstream engine push data to
the downstream engine or should the
downstream engine pull data from the
upstream engine and what we did decide
and we kept coming across things where
it there's not a right answer here and
so we did is we actually came up with an
engine negotiation protocol that allows
the two engines which might not even
know each other because remember these
engines are written by different people
and now they're being introduced at
runtime and say hello engine you know
and so they have to basically figure out
who they are and what the best way of
moving data from one to the other at
runtime is and so so pushes our default
option and what happens is that the
upstream engine writes into the data
store of the down
stream engine and this actually works
quite well especially when the
downstream engine depends if it's a is a
disc based system then you can just
write into that if the downstream engine
is an in-memory system then it has to
have a way of doing that so for example
coherence allows the upstream engine to
write right into its memory and so
that's the most efficient way of moving
data into coherence but spark doesn't
want to do that it does not like getting
push to it says don't push me and so
what was with spark we want to do is we
want to have the downstream engine pull
and so whether these two end you know
who the engines are depends on which is
going to be the the most efficient and
of course now the upstream engine could
have its data in memory or on disk you
know that's another consideration but
now the downstream engine is pulling
data from the upstream and so for
example with and what you can do is you
can if you find a pair of engines that
end up talking together what you can do
is you can special case them for example
to those who are spark lovers you can
make a coherence rdd out of coherence so
that spark can read from currents really
well and that's something that's a
special case connector that you can make
between two engines or what you can do
is when things get really rough in these
two engines you know don't like to talk
to each other then you usually have to
go through a third engine for example
MapReduce or HDFS so you can have one
engine right into HDFS and the other
engine pull because basically in this
world everybody knows how to talk to
HDFS and it can be sort of a default
thing if two guys don't know how to talk
then they can basically talk to a third
guy but that becomes that's an added
cost of doing that and one of the things
is that you can also short circuit that
sometimes you have an engine that
doesn't have to have to pull their push
it can just pull right from data storage
and so we've written a bunch of
different applications on top of this
of this framework and and we presented
this paper at a paper about this at vldb
in China last week when last month and
so did give a quick performance you know
I this is running on our older version
of our big data appliance which is a
Hadoop engine and and we basically
installed a Hadoop and coherence and we
actually brought up all of these
distributed systems which had never seen
Java 8 before and we ran them under the
Java 8 JRE without recompiling or doing
anything and guess what it all worked so
though there is the one of the things
that he is a problem and I suggest that
you all talk to your defenders is to get
java 8 as a supported platform because
even though it works that doesn't mean
you can report a bug so if you have you
if you have Cloudera and you're running
under and you started up under Java 8
you you know Claire it doesn't want to
they want you to reproduce the bug on
Java 7 so and so there's two things we
looked at was 111 was looking at
overhead does this level of abstraction
actually add anything to the cost of
running your job or would you be better
off running it natively on the
underlying engine because you know hey
it's an abstraction of fractions costs
stuff right and so so we ran basically
you know a word count over Wikipedia and
we actually did two implementations now
there's and repeated two implementations
one using to do pry tables so most most
guys of few here do programmers know
that Hadoop Hadoop has its own sort of
type hierarchy of mutable objects and it
does this because new is expensive and
you can sort of see how expensive new is
right from here that you can have a
version of an application that's using
strings or you can have a java string so
you can have a version of the
application that uses Hadoop texts which
are writable objects
thus don't have to get written for every
new one and so so we compared on both
sort of models of computation because
thing is when you start to compare
something that uses immutable types of
something that doesn't you know it you
can't it becomes too difficult so we
compared those two separately and in
fact the native implementation was
actually slower than the application
that was on top of Java 8 and that was
mainly because part of our way of
mapping Java the aid on top of MapReduce
used an extra level of combining which
actually made it faster and we also do
some then that this partially memory
merging before the using the map output
buffer is something that actually
improved performance quite a bit so the
other peaceful is actually gets to
getting some results about Federation
about wide as using multiplies multiple
engines better than than than one engine
and so we did here as we compared any
system that was all running on Hadoop
versus a system that was running on
Hadoop encode and coherence and we'd get
similar results on spark and to make the
sort of fair what we did is we first
read all the data into the the disk the
operating system buffer pool because we
wanted to take disk out of the equation
and so to say so in this in both cases
even though the dupe is for disk-based
we ran it in such a way that the disk
cache was already full and so we didn't
actually have any disk i/o in either
version and this makes up say a fair
comparison and the idea is that once you
avoid the disc the disks the version
that uses in memory is still faster even
though there's no disk i/o in either one
because OS the OS cash oddly enough is
slower than the Java heap and you don't
have to go deserialize the object when
you're reading from a file so in
here we compared coherence versus Hadoop
and you can see that the this but the
speed ups were as and the speed ups got
larger than went you know as you do more
iterations you get bigger bigger savings
by running inside of running and memory
and so by the time you get the 30
iterations you've got a pretty decent
result there the ramones we're working
on more engines more competitors more
compute engines and further out better
job planning and optimization so job
planning is the piece where you take
this distributed this data structure
that represents the distributed
computation and figure out how to map it
right now the width engine call is very
explicit the programmer has to say will
do this here and then this here and then
this here what you'd like to do is
basically have a optimizer that figures
out which engine is the best may be
based on even on current usage
statistics so we can call let's say
you've got four different yarn systems
sorry four different to dupe systems
each running their own a scheduler and
schedule and Hadoop is called yarn and
it can call it and sort of say hey which
one of you guys is free which one of you
has some resources I'll schedule it on
that one and not make the programmers
job but now make it part of a part of
the clients job the client application
jid optimization for low-level JVM
turning basically trying to get the the
speed of streams to basically get it be
as fast as the speed of four loops and
then API extensions for example streams
only handles pipelines currently we
wanted to handle Daggs because a lot of
interesting computations are are our
directed acyclic graphs rather than just
pipelines and so we are starting an
OpenJDK project we have the de started
the approval process on that and what we
are going to do is we're going to split
the project into sort of several pieces
the one may the main piece is the piece
that is engine independent the it's it
sort of says okay here's the framework
where new engines can fit in
but it's not going to depend on anything
that's none that's not part of the JRE
so the idea is you can't just sort of
willy nilly in the jet you know start to
use Hadoop api's or Hughes other Apache
api's because then you make the
distribution of this package dependent
on that particular engine and we don't
want to do that we want to separate the
engine specific code from the generic
layer so there's a generic layer in
distributable streams that then has
multiple engines implementations and
each of those engine implementation is
specific to the engine and we'll use
that engines API so so we're going to
end once it's open in the JDK we're
going to have an initial you know sort
of release but it's going to be
definitely a work in progress and we're
definitely that interested in
everybody's contribution so thank you
very much and three questions
yes I'm sorry once we get through the
approval because there's a there's a
pretty J openjdk isn't as hard as jsr so
it's a pretty lightweight approval
process and will say and then you if you
if you're on those mailing list you'll
you'll see when we're ready yes oh yeah
then the action yeah it wouldn't be very
useful if you got different answers on
different engines so the only difference
should be there to the speed rather than
the rather than your answer yes
and I guess that they using object
extremely
yes the allocation that can read a lot
holding a large object to allocate that
bunker and never leave unless you we
yeah the place for a thing is that the
serialization is not used in the
implement in the running of a stream
it's used in the setup of a string so
that means that it's not like oh there's
a separate serializable object for every
single data item we tend to use right
abels and things like that because those
are way cheaper to transport but when
you actually construct the the
pipeline's the despite playing
descriptors but the pipeline descriptors
tend not to be four gigabytes if you
find yourself I'm serializing a pipeline
descriptor into four gigabyte object
usually made a bug
oh no not the data to describe to
transfer the computation we use
serialization to transport computations
definitions not the data because it's
that's not the right way to do data
transfers data because you know it's
real I the serialization is great for
transferring 25 objects to describe a
computation it's not great for first 25
billion objects and 24 the data yeah so
that's how we sort of split it up yes
interesting this is being able to sort
of like query nodes in a network by
pushing essentially
well basically everybody is trying to
match big data brow big data is that is
you know so if you have a distributed
system one of the first things you're
going to do is make that distributed
system available to on the hadoop way
and so for example MongoDB now has
something called they've implemented a
MongoDB Hadoop input format and to
basically allow Mongo to be accessible
from Hadoop because it's like you know
it's like not doing that is like
chopping off your foot you know it's
like I want my data to be accessible to
these distributed infrastructures
we're sensitive statements so so just so
you can actually build this on phone
Redis I mean he'll be would you would
like them to do is you would like Redis
to do the work of course you know but
but if somebody else could do it too
because it's you know it's like you're
implementing engine and you can just use
the published Redis api's and implement
it yourself registered I assume reddest
would probably do a better job of
implementing it than you know Joe Blow
but but then that becomes for them a
matter of oh you know how many users do
you have and stuff like that</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>