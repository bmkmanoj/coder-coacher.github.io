<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Pragmatic Big Data Architectures in the Cloud: A Developer’s Perspective | Coder Coacher - Coaching Coders</title><meta content="Pragmatic Big Data Architectures in the Cloud: A Developer’s Perspective - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Pragmatic Big Data Architectures in the Cloud: A Developer’s Perspective</b></h2><h5 class="post__date">2015-06-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/WyS0KIdqXLY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">well I'm glad you guys are here for
heating about Big Data our presentation
is Frank mighty big data architectures
in the cloud a developer's perspective
so if you guys are interested in big
data suggest that you stay in this room
for the whole day because after this
talk there's a very good talk about Big
Data and then there's a hadoop focus the
one at 1pm I guess so my name is Fabian
DiNardo and this is my co speaker
Fernando papadopoulos and we are going
to tell you are a few or stories about
our experience with big data so with vb
we've been developing large scale
applications for quite some time in our
careers is not it was never knew to us I
was the architect of a system that was
considered at some point in largest java
application the world i'm not sure if
you to us or not but they said it was oh
I that kind of believe it in like to say
it was and for another was responsible
for many popular websites in Brazil and
he also worked with spam filters and
things that usually deal with lots of
data and it was always large scale for
us we don't like small applications so
and a couple of years ago we met a few
business guys that said that they had
lots and lots of data and they invited
us to join a team to build a data
science company that you would try to
detect patterns profiles and these guys
really had lots and lots of data I mean
like hundreds of websites being method
and collecting information about
millions of people and in a couple of
years we managed to build watches now
the biggest date
science company for advertisement in
Brazil so that's the happy ending of the
story okay that's not what we're going
to talk about what are going to talk
about it's more like this okay so what
happens that when we notice that you had
this much of data things didn't work as
we expected the architecture to make an
application a company like this to work
is a lot more complicated than where use
it when you are doing simple
applications or you know applications
they've that have only a few users or
maybe a couple of hundred user it's it's
it's not it's not big data so what we
want we want to do this in this
presentation is to tell you how we
managed to architect a big data
application and how we did it in the
clouds keeping the costs and it under
control causing the cloud you can you
can you can do amazing things but the
causes can be amazing also if you don't
take care so we are going to shows
several tricks some things may be known
by you some things are not on the books
but work in practice so everything we're
going to show we test it in practice
this is not a hello world presentation
okay so let's go so first of all how big
is big everybody's talking about Big
Data these days but not how big data
applications are really big enough to
use all the technology that's out there
okay in our case we are dealing with
around six billion new records per month
we have 20,000 yeah 20,000 simultaneous
called connections tour system right now
we have in numbers that's yours more
like
a hundred and fifty million users mappa
dinner system so these are the numbers
we are dealing with in our application
of course it's just for Brazil it's it's
a large country but it's not the word a
small piece of the word but Stewart's a
lot of data but something that is that's
important that we are going to talk a
little bit more how you you know that
your application is big data or not I'll
go back to this later so in terms of
data that do you guys work of big data
already or who has good things hell off
you have application with big data those
three four five okay okay a few of you
okay so if you start working big data
usually these are the tools you're going
to you're going to hear about okay the
main to nowadays is Hadoop who knows
Hadoop almost everybody okay good so
Hadoop is kind of decor of this
ecosystem of tools that you can use to
deal with big data of course if you are
starting the field you may see all these
these tools and they may think which one
I should use it depends off on your
problem so Hadoop is the core of
MapReduce implementation and it's
actually an ecosystem there are several
other tools that were built over Hadoop
for example HDFS is a file system a
distributed file system in the Hadoop
ecosystem you have h basic database you
have hive which is more like a little
house I mean a query language over
doc files you have a pig which is a
scripting language for for also doing
queries over Hadoop you have cascading
current choices for me building may pay
those pipelines you have mahou to it's a
very nice machine learning tool that we
use it a lot in our system you have
several no psycho databases like MongoDB
readies you know Cassandra even my sicko
has no no psycho characteristics
nowadays like in memory storage and
things like that so the good news for
java developers is almost all these
tools that are highlighted here they are
either build buting java or they were
partially built in java so as a Java
developer you are in a very good
position right now because you can see
the source code you can understand the
source code and you have the tools to do
what everyone ok so that's the good news
right there are of course a lot other
big data tools out there this doesn't
cover everything it's just the most
popular in our opinion but it's it's not
everything actually there are so many
big data tools that every day you have a
new tube I mean probably by the end of
this talk there will be a new tool that
you don't know about so it's it's it's
getting harder and harder to keep up
with all the tools sometimes we do
presentation like this and someone says
why did you did he use X or Y and who
had never heard of it because there are
new tools coming up every day but anyway
if you know hi boopey you are probably
doing what most of you for doing so ah
but this presentation is not to just
about big dated about big data in the
cloud and the two technologies are are
closely related you a company like ours
for example it would be impossible five
years ago because we are before I start
up and had lots of data and as if i
start up you never know if you're going
to work or not if you're going to
survive the second year but using cloud
computing you can build a company you
can have like hours hundreds of machines
and still process all the data and maybe
you can do some applications that were
impossible a few years ago and you can
do as a person it's not very expensive
to buy lots of machines do some testing
and there's then just turn off the
machines and you you are ok so imagine
the possibility you have now with the
power of the cloud and this large
database and you don't even need to have
a private database big data you can use
several open databases that are out
there like you can use Wikipedia you can
use there are several open basis over
there that you can leverage using a
cloud big data and do something that is
completely new and unexpected so that's
that's the good news I mean there are
lots of jobs in this field there you
have the power of cloud computing you
have lots of good open source java tools
to deal with big data and the problem is
how you put everything together make
something that is not going to drive you
crazy so that's our talk ok so
if you want to become like the Jedi
Master of big data architecture you have
to remember three things that's what
this talk is about okay the first one is
nothing will have a bigger impact in on
your big date application performance
then make your own code faster okay this
is a lot more interesting when my friend
says with his Jedi Knight voice so maybe
you can read the other two you know this
is there's no interesting presentation
of all and if it doesn't have a star
wars reference all they have to do this
hello good morning everyone um you have
unlimited resource in the cloud but the
cost of are unlimited I probably have to
press here okay you know again when
applying Big Data technology make sure
your data is really big okay you again I
can guarantee you that in Portuguese
it's a lot more like a Jedi than this
but anyway ok let's start to prove the
first point here that your code is the
thing that you have to worry most if you
want to have a good performance in big
data I'm saying this because when you
start reading articles about Hadoop and
optimization things like that usually
the tricks they teach you are on the
Hadoop configuration like putting more
nodes or changing these these settings
and things like that but the fact is
that if you change your code you
probably can have a bigger impact on
performance than changing Hadoop
configuration so I'm going to try to
prove this with some code okay this is a
the simplest example you can think of in
terms of big data I mean
Hadoop so this is a it's not really a
real example but it's something that we
do something like this in our system so
you have a log file with user
identification and the web page they
accessed and here's the IP they usually
use it to access that page and what I
want to do is to count how many times
the particular domain was accessed by
other users so here Te'o target.com was
access it to Times and CNN that cone was
accepted one time so this is what I want
to do it's a simple problem the hello
world of Hadoop and big data is the word
count example have you heard of it yeah
so every time you look for a Hadoop
example you're going to find word count
is not word count words is almost like
that so this is what we want to do I
want to count visitors that visit a
particular domain okay and they're going
to do this using MapReduce how many of
you are familiar with MapReduce okay how
many you actually implemented MapReduce
in a system okay so we are familiar with
the theory but very few of you actually
use it that's pretty much how the word
is okay everybody read about it but in
practice where if you have used it so
for those of you that don't know
MapReduce that never heard of it it's a
very it's actually very simple I mean
see it's a simple framework you have
your data here and then you have a map
function that is going to get this data
and transform is something else usually
it can be i don't know maybe separate
parts of the log file or I don't do some
processing and then this data is going
to reduce a function that's going to
transform this dating another thing
since the name is readers you may think
that it's going to shrink the data like
we are going to shrink the data in our
example but that's not true you can have
with those functions that actually
produce more data than the map the
original map but what's cool about
MapReduce is not disappoint me could
have done this with several other
frameworks what's cool is that MapReduce
was created to do parallel processing
okay so when you're dealing with big
data it's impossible when you have
really big data it's impossible to do
the processing in just one machine so
you have if you want to scale you have
to do parallel processing and as you can
imagine this is not a easy thing to
implement you have to think how you
split the data in several pieces what
with pc you are going to send it to
which machine huh and how you get
everything together in the end so this
is what Hadoop sauce for you it's how to
do parallel processing right okay and
this is a simple example of how you can
implement that particular problem since
you guys heard of Hadoop before I'm not
going to get into too much detail but
anyway you have a mapper function here
and I hit the sir here and what you
actually implemented these two functions
here map and with us in the map you are
going to get each string at each each
line of that log file and you are going
to split just to get the different parts
get the host of the the page and then
when you do contacts right use you are
actually sending data out of the map
function and going to the to the Hadoop
framework and then the hadoo sir is
going to receive the data that you map
it from several machines okay and it's
going to count and then it's going to
send these these response out of the
widow's function they're all right okay
okay so if I execute this right I'm
going to exit usually I like to do live
coding but with big data it's cool it's
complicated because each processing
takes a long time to run so I'm going to
run this code in a file with four
gigabytes log file which is not really
it's not huge but it's a good size to
test it and when I run this test what I
have is a processing that took seven
minutes around seven and 58 seconds
almost eight minutes to run it was a
four gigabytes a file and you can see
that Hadoop chose to splitting 64 tasks
so it was 64 pieces of files that were
processing ok so if I change just one
line of that code I can have from almost
eight minutes it's going to take almost
seven min so i can take one minute of
that code we've just changed just one
line of code ok
so to understand what I changes I'm
going to show what I what I change it
what first you have to understand how
Hadoop you works behind the scenes oh so
everything starts in the distributed
file system HDFS so this is a file
system written Java it's pretty cool and
it's completely distributed so we are
going to put when you start a job your
files are in this file system
distributed over all your notes and the
first step Hadoop is going to split the
file in several checks in our case it
was 50 60 for chunks okay each chunk is
going to be processed by one machine by
one of your notes and inside the map you
do this really record executive actual
function code the part of the TU program
is just this part here okay the map
function and then there's a optional
combined face I'm going to talk about
this later and then the result is going
to be stored in a local storage to the
same with the other machine and then
send everything to the readers face
that's going to some everything you're
with us function is actually here okay
and there's a sort face because all that
the keys that you you producing in your
map you are going to be together but I'm
not going to get into too much detail
here the importance to learn how this
actually works the theory behind
MapReduce so the line that I change it
was actually this one what I said is I
said to Hadoop to use a combiner what's
the combiner going back here
you you know that we are counting
domains right and in the end dear
readers is going to some other domains
so if i send all the data from this face
to this face here i'm going to send lots
and lots of lines with just one right so
CNN one tell your target want CNN wat k
31 and then i'm going to reduce all the
data in this phase but there's a there's
data going across the network between
these two phases right so what the
combiner does is to do a mini reduce in
the local machine before sending to the
head to the hairdresser that's why here
i'm saying that the combiner should use
the same class as the with us when i do
this what Hadoop is going to do is to
execute I with this face at the end of
the mapper some everything that each
mapper has and then send the data semi
reduce it to the next phase so if you
look at the numbers so each Hadoop job
has what we call counters so the
counters give you statistics of how the
process went out how many memories you
use it how many files or produces and
things like that so if you compare the
numbers of the two implementations with
the combiner and without a combiner you
have some very interesting numbers the
first one is this so file by treating is
the number of files that were written by
the process so if you look at the
numbers here and compare if here you can
see that a lot more bites or written to
the disk in the first implementation
then in the second implementation and
you can see that the map output records
I mean the number of hackers that were
produced by the map function are the
him but in without a combined it has no
combined input but with the combiner
receive data from the combiner and the
combiner is send it to the Hindus phase
only 74,000 hackers right and so where
is the same here they deduce input
records in the first implementation is
33 million records in the second one is
only 21 thousand records so if you
imagine that they're going to send this
data across the network this has a huge
impact right so we took one minute with
just one line of code is this news for
you or not okay so at least something
new you learn today okay another very
important thing when you're dealing with
big data is your code is going to be
repeated millions or billions of time so
everything you can do to take some
milliseconds from your code it's
important okay see there's this example
here so it may be reduced you have to
when you do this context right when you
write the result the result of this
function you have to use special classes
that are what Hadoop calls right books
so one of the right well you can use
this text like this one so if you look
at the previous implementation we were
creating a new text every time you had
to to write to the mapper but actually
every time you call this method you are
ready you are writing and it's done so
you can actually reuse this object
several times if you reuse this object
you're not going to create new objects
and then it's going to be faster so we
do the same here and here with the right
hole okay
and another thing we optimize it here is
instead of using a split in Java we use
stringtokenizer professor okay split use
behind the scenes regular expression so
we use it tends to be is lower than
using stringtokenizer if you do this and
you measure the difference in time you
can see there's a there's a small
differences not as as as visible as
adding the combiner but if you think
that you're going to run these millions
and millions of time this is going to
make a huge difference later right okay
but this is not the thing that is going
to have the bigger impact the bigger
impact you can do if you use the
knowledge you have about your data to
change your algorithm and this is what
I'm going to show you here it's
everything okay till now everybody you
guys started tweeting already or just
everybody here okay it's hard to do
presentations this days because after 15
minutes everybody goes Twitter or really
may isn't but anyway stay here so what
I'm going to do is you know I'm a county
or domains and probably don't remember
the data but in the end of that
processing we counted three hundred
about 300 domains so 300 domains is not
big I can probably put everything in
memory right so I know my data and I
know that the result of my functions is
probably going to fit in memory so why
if instead of running the combiner I
create a hash map here and in my map
function instead of doing what I was
doing before I put my data in this hash
map and then in the end of the map
function I have the data that was
already summoned up here and then i
right
the result without going to the combiner
in Hadoop you have this cleanup function
that Hadoop cause a bite in the end of
the map function so this is a trick I
can use this cleanup function to
actually write what I have to write here
instead of writing the map right so if i
do this i don't have to use a combiner
because i'm already sending by the end
of the map function everything combined
it right and then it's going to drive
the same effect by the time but yeah but
as i said in my data and i know that my
date is going to fit in memory this you
can use this technique only if you know
your data I'm counting domains it's not
going to have you know a million domains
it's going to be a few hundred to
guarantee do the same instance of the
class where we call to map is that it
makes yeah I mean yes yes for this
particular chunk I'm going to run in the
same in semester instance of the class
so it's going to work so if i do this do
you remember what was the original time
before the combine it was almost eight
minutes after the combiners around seven
minutes and with this technique i can do
the same in five minutes okay and if you
look at the numbers you asked it is
going to be the same result you see I
had 343 records in the end and have the
same here so it's the same thing exactly
the same result but if you look at the
numbers the time and everything is
everything is better okay so this is a
few tricks that you can use to make your
code faster cheat a little bit the
framework and you can have better
results but you have to know your data
as you said if you have it depends on
your data sometimes you're going to run
out of memory so
can do that okay but the biggest lesson
here is this the bottleneck of your
application usually is caused by the
amount of data going across the network
right so you have to think that you are
working in a distributed system so
you're going to pass data from one site
on the other side of disease what
usually have the biggest costs okay so
another thing you have to remember when
dealing with Hadoop and distributed
system map he does is you have to try to
make your application for tolerant what
happens is that Hadoop has a something
that is really good which it did if your
job failures it tries again until
usually four times before giving up but
the problems that the data you receive
it's not always well behaved it you can
receive data that you are not expecting
and then you have to decide if you do a
try catch and ignore the exception or if
you just throw the exception if you just
throw the exception the job is going to
fail you and Hadoop is going to try
again and if these phrases are all the
time you are never get any work done so
sometimes it's better just to swallow
the exception sometimes because the
pains of on your application what is the
exception but sometimes it's better to
just swallow the exception log it and
look at it later instead of making the
job failure right so
there's something else that you have to
worry about and it's the size of the
file you are going to process so I run
the same process in another file not a
four gig you know four gigabytes we
process it in five minutes and this
processing took 34 seconds any guess how
big was the file so five minutes 4
gigabytes 34 seconds what was the size
of the file any guests it was a file no
log file with just one line and it took
34 seconds okay why because Hadoop a in
HDFS they do not do not work well with
small files they are not built to deal
with small files actually HDFS stores
all the metadata about the files in
memory so if you have lots of files you
can crash your system really fast okay
so lots of small files is no good we
crashed our system in the first month
because we decide to start no 5k is more
files like thousands of them in HDFS it
was terrible a small files are evil
don't do that okay you're talking big
data not smelled it
ok so these examples I show to you this
is Jesus almost hello world this is not
real life you never have a problem as
simple as just counting domains ok so
what I show you I showed to you is this
it's just a map a simple MapReduce real
life is not this hill life is you get
the output of the first MapReduce and
you have to use as the input of the next
May pedals and maybe the same input of
the first MapReduce is going to be the
input I mean the output of the first map
it was going to be the input of the
third MapReduce and this was is what we
call a MapReduce pipeline this is real
life usually the problems are not as
simple as the first one I showed you ok
have you guys work with pipelines of my
pedals before anyone just one ok ok so
let me show you a real problem this is
one of our problems but I simplify it a
little bit to make it even easier to
understand but it's a it's a real
problem so imagine that I have the same
log as before but what I want to do is
have this output here so for each user I
want to know in what subject these users
is interested so for example easy user
the first horse is interested in
technology the second in news the third
and technology again so the way I do
this is to process this file get which
page was exit by by itchy user then I
get all the pages i have the process and
for each page i use mehat or any other
machine learning algorithm to detect
what this page was talking about and
then I combined the his oath of this
with the result of this and then i have
this indent right so this is our real
problem now in our system so to do this
I have to do several phases right
process a little bit
of data then process the other bit of
data and combine everything fortunately
there are some tools that you can use to
implement a pipeline these three are the
main ones so Pigza is a script language
that you can you can write several
MapReduce jobs in a higher-level
language and chain them together
cascading is a is maura library to
program pipelines and crunch is Anna
patch a patch blob rarey also an open
source library that you can use to
program your pipelines we are using
crunch in our system and what you can do
the same with the other tools I'm going
to show how crunch works so you can see
how interesting it is so imagine that
you have a pipeline with two functions
and you have encouraged you have these
special collections p collection p table
and there's a big group at table as well
and they are not real data they are more
like pointers to the real data and so
what happens is that you build your
pipeline say the input what each fusion
is going to have an input and an output
and the output can be the input for the
next function and then you co a method
called parallel do which means just do
the pipeline and then crunch is going to
create a MapReduce pipeline send pieces
of data to every note like this and then
each function in crunch has to be
serialized so they are going to send the
function to each node which is something
that you have to keep in your mind
because when you put these in production
if your function is not serializable
it's going to be a mess I mean
every file that you will try to open
your every library you need you have to
make sure that they are serializable so
they can be using in the note in the
remote node where it is this function
our sins so the function is sent to
every node and then each function is
going to run produce data and send to
the next phase and in the end I'm going
to have my target data produce it for me
so what's cool about crunch is it
optimizes your job so if the input of
these the output of this function is the
input of this function current can
decide because it's faster to put both
functions in just one right a MapReduce
job so it's going to be faster usually
you want to have as less jobs as
possible usually because if you have too
many jobs maybe some of these jobs are
going to deal with small data right
because the output of that that first
function may be too small so it's better
to put everything together and what
crunchy does is take care of the two
intermediate data like this so if you
are implementing this by yourself you
would have to maybe deleted easy day and
and make sure how you're going to maybe
storing now in a know in a cache or
something so Hadoop Crouch is going to
do everything for you and it's it can
make your pipeline a lot faster then if
you are implementing yourself right so
when you're making pipelines the key is
to try to find the better granularity
for your pipeline depending on how you
architect it it can be more or less
efficient so for example this pipeline d
if you are think just like java
programmers that
you have mobilization in light we may
decide to do this way we are going to
first process this file and then produce
an output with only the the pages d
where else and another output 50 users
and the URLs and then for the URLs we
are going to classify each of them then
we are going to marriage and in the end
you're going to have the result this
would be DD de nieve y way of
implementing this a better way in this
case what to do would would be doing
something like this you get the file and
you put all the data you transform the
data you need but you put everything
just one step instead of doing two steps
to do in one step and then you do the
classification and in the end you have
the result so instead of doing six steps
like this you have everything in four
steps so usually this is a lot more
efficient than try to modularize too
much your ear pipeline okay but it
depends on your problem actually because
if the process of categorization here is
to is low you may want to split the
pipeline in two pipelines so you don't
have you don't have your job waiting too
much for something to happen ok so the
lasso here is if you reach data from
disk do as much as you can affect
because there's a lot of your overhead
when you read data from this right you
have to read the data put a memory do a
Hadoop has lots of things to prove that
and so if you have the data you you
probably want to do as much as a kind of
it okay so going to the second part of
this talk and I'm going to let my go
speaker here take over but we talked
about big data so far and we are going
to talk about how to integrate this with
the cloud now and the most important
thing here is there
with big data divorce is really really
important are you guys familiar did with
the divorce concept yes right so having
the infrastructure team and the
development in work working together
makes a lot of difference for us it's
changed everything and going to let
Fernando tell the story now okay before
I begin and who here works in the
company with unlimited money then nobody
yeah I do so let's save some money today
as everybody here knows some years ago
we have some some other approach to do a
development and as a developer we needed
to to tell the the devops guys or the
operation guys which machine we want to
run our code on and that was a difficult
difficult thing to do because we are
working an internet and we cannot we
cannot as see how many users we are
going to have in a peak of time and
during the day so the traditional
approach we usually buy lots and lots of
machines to to respond to these Peaks
yeah but nowadays we have the cloud and
the cloud makes it easy to have a
servers running and and to deploy
servers on the cloud but as soon as we
start deploying service on the cloud the
costs are erasing too because we are
painting paying for on-demand machines
and the tricky to save money and if you
want to do some magic if you have a
flexible service that works for you we
used to have to have working too with
some some cloud-based systems and to
some
how can I say to some companies that
that do the cloud we use Amazon in
Brazil we use also google and as you
from Microsoft and for us the the
company that fits better to to our job
was amazon but just because of the the
API is the support and because amazon
has a data center in brazil but you guys
can use any system and any provider you
want and so here are familiar with
amazon amazon instance and amazon
service okay almost everyone but for for
you did that you don't know amazon has
three type of instance has on the man
instance that you pay as you go so you
just pay for the service that you are
running and it's paid by hour you have
reserverd instance that you pay an
upfront for one year or three years and
then the hour let you use is going to be
cheaper and they also have a spot
instance and that you can buy and spot
instance our servers that are not use it
so our servers that amazon have and
they're going to sell some time but as
the servers are I don't know you can
beat and use the server for cheaper
price so in this example if you use for
a sever one year you know about 8,000
hours eight thousand seven hundred hours
if you spot instance it's gonna you are
going to pay 300 and something dollars
and if use the same instance on on
demand system you're going to pay about
two hundred eight thousand dollars so if
you can use the spotty instance you can
save some money but it's not a it's not
as easy as just do just using these
party instance
your system needs to be aware that you
can lose the the machine at any time
because you are bidding for an idle
machine and as soon as Amazon need this
machine they can amazon can take this
machine again and you're going to lose
your server for example this is a real
example one of our system is running
with spotty instance and on-demand
instance we have 85 servers running per
day and 660 of those servers are running
full time for about forty two of those
with spotty instance so when we have pic
pic access of our systems we just buy
more servers and with this approach we
save about sixty two percent in one year
but how to choose a correct spot
instance to save money is the price of
those the spot instance may vary based
on the type of the server issues and in
this day of abilities on the let you
choose also so you need to monitor all
those numbers to choose a correct you on
that you save you money because you need
to beat to use the servers okay and how
to use those spotty instance as you can
lose your server at any time because the
price can can go high you may choose
spot instance in different zones at the
same time or for example amazon has a
viability done a b c d and e hope if you
put some spot instance on different of
our builds owns you can expect to lose
one's own or two but it's difficult to
lose everyone in the same time you
canes but it does not happen always so
she frequently let this never happen to
us right yeah you can also mix spot
instance with on-demand instance if you
net if you need to have in high SLA all
the men in distance you never you never
lose them and you also prefer share
nothing architectures because as you can
lose your machine it's not good to to
have things that you you cannot lose
inside those machines for example if
have a database you don't want to lose
your database at any time and how to
upscale all those these features without
get crazy so the best thing is be lazy
what do I mean would be lazy ultimate
eyes everything so you are going to need
to do your code ones so if you you get
just a bit of time just a bit a little
bit more and you do your code ultimate
ice ball you can use scripts that do
things for you and you don't need to be
inside of the computer if you lose your
machine for example
what we did in your case as I'm a
developer and also a sysadmin we build a
system that monitor our entire network
and all of the systems that we are
running and we take actions based on our
own data for example this system is
monitoring right now all the requests
that we are having other responses every
single server memory and CPU internet
traffic and almost everything so this
graphic for example shows the requests
per second that we are having and
there's also a trend line if the system
detects that we are going to have a big
of traffic in the near future what it do
it automatically connected to Amazon via
API it detects the beads it getting new
machine we bid for a new spot instance
and as soon as we get the server it
connects the server to the load balance
we stall every single piece of software
and we have another server running show
a little intervention yeah without
anyone need to press any button so it
can be here speak to you and the system
is running and the company is almost
safe and what we did was we put some
intelligence in there so we don't need
we don't need the people to be
monitoring and take actions
what we also do is anyone familiar with
to see BICS quite fill zabbix is a
monetary system you can have zabbix
running in your infrastructure and
confuse a box to monitor any kind of of
metric you want zabbix has lots and lots
of plugins but even if there is no
plugin for what you need you can write a
simple plug-in and put your metrics in
size lyrics this graphic for example we
are monitoring how the other price of
Amazon how they are going over the time
so the system i show you connects to the
server get all these beads that other
the history of beads and they can side
what's the beach for the feature another
tip to save some seasoning time is to
use this mark buege instance templates
so instead of get an amazon template
without anything in it just an
operational system you can have you can
do your own template so if you know you
your system needs java and an
application server you can put all those
piece of software inside the template
and when you need another machine you
just use this template to build the
server so it makes it easy to travel
servers on your out scaling system
another thing is let your scrip to
decide which private IP you want the
server's through one with because if you
don't decide it the Amazon will just
give you an random IP and with random IP
you need to maintain a table of IP so
you can plug your systems plug your
servers together and if you if you
decide which IP you want your server you
can just turn on your server and for
example if I have a Hadoop cluster and
you know which IP a new job tracker will
have you can pre configure your Hadoop
master and allow a hinge of high piece
to join the closer so you just turn on
the server and the note automatically
join the cluster and to heaven an IP is
as easy as one line of code in the
Amazon API and talk about how to pulido
spotting instance are good for job
trackers as Fabian said the Hadoop out
to hit try if a job fail so if you lose
a job tracker and it's not a huge
problem because Hadoop you try to do the
job again so this pc is good to you to
use this party instance and as i said
there is a file it's called slaves where
you can allow every single IP you want
to join the cluster so this eyepiece are
used for out scale if you'll have a
range of ips pre-configure in this in
this file you can just turn on the
server and the server will join the
cluster
there's are three things we would like
to know when we started but we didn't
know so we make some mistakes and using
the Amazon API it's possible to buy
instance we Freddie find IP it was not
possible in the console a few months ago
I'm not sure if they changed the console
yet but as we are only using the API and
Vicki I make it's possible you can use
also provision it I oh PS in this that
need more performance so you are not you
don't need to use just the the disk that
Amazon give to you you can get better
disks using using IOPS and use an Amazon
API you can do some things that it's not
possible using the web interface um the
last thing we we want to tell you is
that when applying Big Data technology
make sure your data is real big okay so
how do you know if your data is really
big the first tip that your data is
really big that if you can't fit all
your date in a single machine I mean if
you can't use a single machine to
process everything if you need parallel
processing probably have big data the
second tip is that if you are talking
more about terabytes 10 gigabytes if
you're talking about gigabytes probably
your data is not big enough to use
everything if we're talking more about
terabytes probably you're having big
data and the last one is that you see a
trend off of your data is increasing
consistently and it's probably going to
double next year so if your data is
increasing
you probably need to have a better
architecture and everything for
everything else keep it simple keep it
simple probably you can do if just a
Java called a simple Java code you don't
need Hadoop or anything because there's
a lot of overhead when I started using
Hadoop and all these things we show to
you and the truth the truth is most of
the problems we have today we can still
solve without Hadoop you first single
java program so keep it simple if you
don't have big data because you're going
to save time and money probably thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>