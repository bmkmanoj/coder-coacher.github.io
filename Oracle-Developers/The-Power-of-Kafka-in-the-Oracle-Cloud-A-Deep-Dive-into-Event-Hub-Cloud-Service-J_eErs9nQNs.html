<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Power of Kafka in the Oracle Cloud: A Deep Dive into Event Hub Cloud Service | Coder Coacher - Coaching Coders</title><meta content="The Power of Kafka in the Oracle Cloud: A Deep Dive into Event Hub Cloud Service - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Power of Kafka in the Oracle Cloud: A Deep Dive into Event Hub Cloud Service</b></h2><h5 class="post__date">2018-04-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/J_eErs9nQNs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome thank you for for coming
hope you had a great great lunch and
that you're having great conference so
far this session it's going to be the
power of Kafka in the Oracle cloud and
we're going to take a deep dive into
event hub cloud service which is
actually managed Calf kind in the Oracle
cloud so I will just briefly introduce
myself my name is Arturo Olivares I am I
come from Mexico what I actually work
and live in Norway and it's my first
time in in India actually so it very
happy to be presenting here I also
presented in either about a week ago has
been nice and yeah there are a couple of
links my Twitter if you want to follow
me usually twit about this this stuff
I've written a couple of things as well
this about my the company for which I
work which is important for me to
mention because they also support me in
this travel and this activity activities
the name is Cisco it's a Norwegian
company Khan consulting company were
basically focused on integration and all
kinds of integration data integration
service integration enterprise
architecture we have a multinational
team I actually have a lot of colleagues
from from India as well so yeah that
that Cisco and just quickly the
commercial about the the Oracle ace
program this is this is a very nice
program where many people have gotten
involved to support and help grow this
this community or a called experts
basically or Yaba experts we have some
asus and developer champions and the
directors presenting here at bangalore
ian yeah if you can if you spot them
around just talk to them ask them a lot
of questions if you want to become an
Ace or nominate an ace do so
Lori loser which which is one of the of
the members of this committee is here
in the conference so if you are
interested in becoming part of the
problem just look for her and ask her
what are the requisites her to become a
nice so that's the ACE program and this
is our agenda okay we're gonna I'm gonna
give a very brief introduction to Kafka
just just some details because if I
start talking about just Kafka we could
be here like all day it's it's a very
big very big topic then I will try to
provide some justification and some
examples about why why do we want Kafka
in the cloud or what is the fit you know
because we will be talking about even
top cloud service so why would we use a
service like this for building
applications for cloud architectures
then we will go into the into the
product and we will see an overview and
a little bit of a deep dive into even
hopped-up service let's see how much how
much time we got to see all of the
features and in the end I will mention
some interesting use cases some
architectural considerations for the use
of this product and then let's see if we
have also some five minutes or something
for questions and answers but even if we
run out of time you can find me around
and ask me whatever you want about this
about this session so yeah let's let's
let's begin okay so what is what is
Kafka did does any of you have heard
about casca we are working with Kafka
how many people here are familiar with
the with a concept of Kafka at least
heard it like buzz word or heard around
yeah so okay Apache Kafka it is a
distributed streaming platform okay it
handles data streams I will tell you
more about why it's a distributed
streaming platform it's it's it fits
certain types of architectures are and
certain needs of applications in
nowadays it's not for like everything
it's it's not substitute for the
database you know but but it's a very
it's a very powerful powerful tool and
we will see why it has many capabilities
but among them
most important once our this it can
publish and subscribe to stream source
records big streams of data you know
similar in a similar fashion to what
Message Queuing those or Enterprise
traditional enterprise messaging systems
you know like traditional JMS or
something like that but it's much more
powerful and in a larger scale Kafka is
for handling very large amounts of data
even Big Data scenarios it stores these
streams as well in in a fault tolerant
and durable way specifically durable
maybe some of you have heard this this
term about immutability that the catalog
is immutable
it cannot be deleted or can hardly be 3d
doesn't change it records everything
that has been happening and that will
happen and that will keep happening in
in a very smart and manageable way and
it lets us process these streams of
records as they are happening so this is
one of the most important features about
Kafka that together with with some other
components that I will also show you
it lets us handle this this huge amount
of amount of data in real time we have
like like the warm hot data as it
happens no it's not it's a change of
paradigm as opposed to going and pulling
everything from database for example you
know and and then you have like cold
data a little bit outdated even if it's
minutes or seconds but but it's not
what's happening in the moment so calf
ecology you know what's happening in the
moment with your systems with your
customers with your applications with
all your data basically this is what I
just said you know it has processing
capabilities many of those and many
other components have built on top of
these you know like there's a SQL
there's Kafka streams there's extremes
it provides the messaging and
integration capabilities also it has
api's to produce to worst Koshka consume
from Casca connect to Kafka from
different providers and it can work as
it works also as a data store know so we
have topics partitions replicas
materialized views and some other things
compaction retention which we can
configure because our Kafka or event
lock it it's also a datastore and it
keeps the information there you know I I
borrowed some a couple of slides from my
colleague Jorge which works also in
Cisco he's you can you will hardly find
the biggest Kafka expert in in the world
I think so if you're interested in this
topic
make sure to to follow that guide he's
he's really good in and he's even a
committer contributor to the Kafka
project itself so that's why I borrowed
a couple of slides from him he's a very
good friend of mine some Kafka fact just
just to get context you know this this
product this tool was developed by the
Lindy I guess everybody has heard about
LinkedIn about you know like seven years
ago eight years ago but it quickly grew
on its potential and it was open sourced
in 2011 by Apache Incubator it graduated
as an Apache project very quickly 2012
then a company named confluent was
created in 2014 and confluent builds on
top of catkin confluent has created a
lot of tools to connect to work to
process with kafka my company Cisco is
also partner with with confluent you can
see the number of messages processed
each day by LinkedIn for example and
over 35 of the biggest companies in the
world are already using Kafka for one
thing one another for some scenarios
some bigger some smaller but many many
important companies companies that kind
of big big amounts of data or using
Kafka in production with success
so that's Kafka you know some of these
companies are very famous like Netflix
is one of the most famous just-just
cases Airbnb for example interest PayPal
and just by looking at these companies
you can you can see or you can imagine
the the the volume of data they usually
handle like eBay for example it handles
tons of data tons of events and we're
gonna talk a little bit more about
events and I'm gonna give you some
examples of my own experience but yeah
this these are good example
of the companies that use Kafka
worldwide this is how a typical Kafka
cluster looks like we are not going very
deep into this because it's it's bird it
would take us the whole session to
explain but but yeah
Kafka we said that it's a distributed
streaming platform so distributed it
normally is deployed on a cluster no
distributed way okay
it you said this component typically
names too keeper to manage the cluster
so its component that usually comes
together with Kafka there are producers
which are putting messages in there
there are consumers which are reading
from the streams you know and inside we
have topics we have partitions basically
those are like like the two more basic
concepts of a Kafka
architecture if you are familiar with
with the MS for example this will ring a
bell like topic name is topic or JMS
queue here we have topics as well okay
and yeah that's that's the basic
architecture just keep it in mind okay
and why do we want to have Kafka in the
cloud okay basically that there could be
many reasons and with Kafka we could go
like many different ways but in this
session we are talking about event hub
cloud service which is I think a good
name for that for the product because
it's focused on supporting event-driven
architecture event-driven architecture
which is a fairly new design paradigm in
which tower suffer execute in response
or reaction response of events it is
receiving or the events that are
happening you know all over the place
so this this even doing an architecture
it what it does it lets our application
server systems resemble a little bit
more or get closer to what the end user
wants it resembles like real life you
know you're getting information in real
time you're you're processing what's
happening in in the moment okay why
should we care why does that matter okay
because events drive a lot of things or
they have a lot of potential
they let us move fast
they forced her loose coupling
scalability and here maybe good good
analogy I consider for example the way
the way we we work as human beings our
perception you know we we are usually
receiving stimuli and and we are
registering events from everywhere in
real time and we react in real time to
this event so for example yesterday I
was thinking okay for example to cross a
street here in Bangalore it's quite a
challenge you know with all the traffic
and and there and that took talks and
everything and but you know you are
receiving and processing all of this
information you know there's a tractors
had to cook there's a bicycle there's a
motorcycle and I need to do to be quick
and run and pick the right moment you
know but if I wasn't being able to
process this information in real time
with my brain then it would be
impossible to cross the street you know
if I if I was getting like if my brain
had like a three-second delay or
five-second delay and I try to cross the
street I would certainly be run over by
something by a boss or something so
that's the importance of real time
information real time events if I didn't
have that possibility I would have to
find that work around you know like
building a bridge on top of the street
or a tunnel or some someplace I could go
easily without worrying of whatever is
happening out there but that is costly
you know and that takes time and it's
not the ideal situation so that's why we
want to handle events appropriately we
say that events allow for time travel
also which is I want to recall events
that already happened and I want to do
it quickly and I want to have the
complete information so data streams
lets us let us do this okay it's not the
same as going and into the database and
trying to get scattered information out
of there it's the same important
information if someone comes and asks me
what is your name I have it here after
oh I wouldn't say like oh I don't know
let me go in to my birth certificate and
and then I can tell you my name or my
birth date you know so that's that's
like I mean it's very simple example but
that's real-time event ribbon as opposed
to traditional
traditional integration traditional data
data integration so that's why we want
to handle events another consideration
stream processing which we're talking
about
versus traditional integral integration
traditional ETL now when we want to we
are talking about huge huge amounts of
data here so what do we traditionally do
when we want to move
huge amounts of data from one system to
another or from one database to under we
do ETL a lot like batch processing
that's very scalable that lets us move
millions billions of records with with
good performance and everything but it's
not real time we get the information the
next day or in a few hours so it's it's
like called information we have the
traditional enterprise integration like
web services and so on service bosses
which let us do some real time stuff but
they are not very scalable if if I have
worked for example in companies that try
to move big big amounts of data through
a service boss for example Oracle
service boss and that's not very good
idea doesn't scale so well when you're
talking about this big big amount of
information and I will I can think of an
fun example I in Mexico back when I was
working back in Mexico I used to work
for my customer was a pawn pawn shop
where you go and and pawn your stuff and
they give you money and but it's very
big in in Mexico so they have stores all
over the all over the country no one and
events are happening all the time all
the time so they wanted to use this
event to for many things
and feed many systems with them and they
were using like traditional enterprise
architecture with service boss and all
of this stuff and what happens we have
too many too many messages coming in
maybe the boss can take them because
it's it's it's using a lot of processing
power then it just spits this
information out to the many systems but
the systems cannot handle correctly all
the load you know and we will put a
little bit more about this in in a few
minutes but then we start losing
information some systems get the
messages some systems don't get the
messages then we have inconsistency and
then we have slow performance you know
and then they
they said okay let's not do this let's
use ETL and we will have that
information when they after but at least
it will be reliable and but an
alternative is streaming platform Koshka
you know to handle this kind of this
kind of situation another advantage of
using Kafka an event log and streaming
platform is that it empowers or enables
us to implement this very not even and
very impressive
design patterns like CQRS you know like
even sourcing materialized views back
pressure and and some more you know CQRS
for example that was the couple
completely the the reading and writing
from from databases or systems of
records no but pressure I think I have
and yes this is my example of just to
talk a little bit more about one of
these patterns back pressure and I have
a demo of this back pressure is when
when a system is being overloaded by all
the number of messages coming you know
like like in this figure this the
producer is very fast but that consumer
consumer is not as fast so this is
producing a lot a lot of messages that
one has a certain capacity and when when
the which is it its capacity maybe it
has a buffer or something but that can
also be filled and that what happens the
messages we start losing them they start
being dropped no it it doesn't have it
cannot put them anywhere and that's what
happens for example that emerges from
Alibaba I think when they have too many
packages you know and what do they do
with them they cannot handle it anymore
and of course something is gonna get
lost in there and some some packages are
not gonna reach their destination and
some customers are gonna be very angry
because because of this because of this
mess so that's back pressure is the
pattern we used to handle this and and I
will I will show you in when we reached
the demo okay but in just to finish the
introduction in summary you know when we
want to handle events big amounts of
data and we do it correctly with the
vendor in architecture and assuming
platforms it's like like this is the
difference you know like like going
circles or going straight up to what we
want at like speed of light so that's
what cartels lets us do you know and
it's as I said it's not for every case
it's not for every scenario but when we
want to handle huge amounts of data and
even driven architecture then Kafka is a
very good option and even hot blood
service from Oracle is one product that
leverages and implements this this kind
of of technology so yeah let's let's go
and take a look at the product
okay so if we just take a look at page
four web page for even top cut service
as I said it's managed
Casca in the cloud this is platform as a
service so what is the I mean we talked
about the advantages of using Kafka and
streaming platforms but what is the
advantage of having managed CAF kinda
cloud that we don't as with any pass
that we don't need to worry about
infrastructure or configuration or
maintenance or patching no Oracle takes
care of that we just create our our CAC
love clustered we give the
characteristics we want and Oracle will
provision it for us and they will take
care of this service you know and we'll
keep it running
we can scale up or down if we want we
will be in like we can choose different
payment models but then we will have a
Kafka implementation available to let
our data flow through it you know so
that's basically the die imagine in the
right hand one other advantage within
the Oracle cloud I mean we could
provision this service in the Oracle
cloud and use it with any other cloud
providers or connected to whatever we
want but if we're also working with some
additional services within the Oracle
cloud then there there is some
out-of-the-box integration with these
products you know at there we can see
like for example the Oracle database and
IOT you know and the application
container cloud and the API cloud
service so just with a couple of clicks
and a couple of some configurations we
can interact so that's that's the like
the value which is being provided by
this service now let's let's go into it
yes okay so basically I just provision
this this instance for the Oracle code
my I actually provisioned it like half
an hour ago because I was having some
some trouble on how to recreate it but
this this is a dedicated Kafka Manish
Kafka instance okay it's like the basic
configuration if I go into this into
this summary page I
can see that apology how is it created
what kind of resources is it using you
know like for example it's created into
notes it's using three three CPUs some
memory some storage
you know I created some tag to identify
it easily but there is some important
information here and it's this I think
it's one of the services within the
Oracle cloud which is very easy to get
easy to get started with you know it's
it's very quick to create you can't
create it in like 5-10 minutes I in the
end I will provide some slide with some
links to resources where you can have
the step-by-step guide on how to
provision this service so those are the
I think you can already download my
presentation from the Oracle code
webpage but for example if I want just
to write some code and start connecting
and sending messages to this to this
well I need to create the topic which I
will show but once equity that topic if
I want to start pushing some information
or reading some information from it I
just need to use this disconnection
script or you know this is the IP
address and the port this here if you
see Sookie police also running to manage
the cluster we talked about that
zookeeper is always part of the Kafka
architecture I configure this to use
Kafka connect you know and kapha Connect
is an API on top of Kafka that lets us
connect very easily to several sources
and several sinks several destinations
you know what else can I see yeah that's
basically it
from here I can scale up or down my
service I can stop it if I want restart
it I can do
he'll check ok so monitoring but this is
just this is just my cash cab backbone
so to speak ok this is the Kafka cloth
cluster that has been provisioned for me
in able to start using it or to start
pushing or pulling something from me
information from it I need to create
topics partitions you know the like the
logical the logical segments that are
going to be handling these this
information so I can come here
these are my topics I create the two of
them demo topic and the motivic - they
are created on top of different of
different Manoj Kafka instances you know
this is in the dedicated Kafka service
this is like in the generic one that's
why it has this little icon here okay
creating the topic it takes like one
minute I will create one right now let's
see if if the Wi-Fi helps us instance
name for example I'm gonna give a name
to this topic so I can say like mp3
description it's okay I provide an email
where I can get notifications about this
topic so when this topic is created or
or if it's down or something I will get
notification in this email address it
will be hosted on my dedicated instance
which is sort of called called demo I
can tag it code if I have a lot of stuff
going on in my cloud account then these
tags will be useful to find my my
component two partitions that's the
default 2k retention period 24 hours
lock cleanup policy delete that okay so
that's all I need okay confirm and
create okay and it will it will start
the Oracle will will start working
automatically to create this topic and
in a couple of minutes it should be
ready I think it's ready now it was
pretty fast actually so if I look at
this topic I can see the information you
know there are several ways in which I
can connect to this topic to produce or
to consume now I can use as we saw in
the let me show you one more one more
slide or go back to one of the slides
as we saw here we have several several
different capabilities you know so for
example to to do messaging and
integration I have direct api's which
which I can use to produce and connect
for example from some Java code but I
also have on top stream capabilities I
have a SQL I have another stateless app
API stateful API so I can use if I'm
using for example akka streams or
carcass dreams or the confluent platform
or some other Oracle service or some
other technology I can there are also
different API so that I can use to
connect to this topic
I can use just plain Java code and
exploit this API switch I guess I will
show you okay I have another slide yeah
this this is normally the way we can do
it you know if we see we have Kafka in
the middle okay and we have the
messaging API we have the connect API we
have the streams API so we can have apps
there apps here data sources or data
destinations there syncs okay and we can
use all of these api's to interact with
Kafka you know that the choice is the
choice is ours the the different api's
have different capabilities and
different strengths no so it depends on
what we want to do particularly we at
Cisco we are using a lot of we're just
we are partners of confluence so we use
confident that form we use capture
connect and we use also kafka streams
and okka streams depending on the on the
use case for example back pressure what
I showed you akka streams has a very
good implementation of the back pressure
pattern so whenever we want to use back
pressure we use akka streams we have if
the data is too big then we use Kafka
streams which is a little bit more
scalable and more resilient you know so
let's let's go back to the to the
service yes
so these are the topics okay I have the
information there there are there's also
a very good metrics page here for this
this topics if I'm if I'm sending or
receiving messages I can see the metrics
and I can see what's going in on what's
going on in there but I think something
some maintenance is going on with the
service right now because if I if I try
to see the metrics just giving me some
strange error routine or it doesn't load
right but but it usually does okay yeah
so I think there was some maintenance
going on like half an hour ago but
fortunately we can see now that's nice I
didn't think I was being it was gonna be
able to show you this so this is the
this is the matrix yes sorry Kafka is
the backbone Kafka is your your
messaging infrastructure you know and
kapha Connect is an API to access this
okay yeah within within the Oracle cloud
you can provision you have the option if
you want if you want it to come already
with Kafka Connect if you are gonna use
Kafka connect to connect to it then you
provision Kafka Connect as well and it
will create a separate compute node or
after Cornett leaves
yeah yeah you can you can decide which
size do you want it to be or or you can
decide not to have Kafka connect and
then you will save some money if you for
example are connecting from some other
type of code or with some other
technology then you don't have cut the
connect and and your sizing is narrower
okay so here we can see we can see
statistics you know maybe it's still in
maintenance book because something is
not loading but this here you can see
whatever is happening inside your inside
your infrastructure okay and I will show
you some some actual code for example
it's it was very quick for me to do this
yep
this is a demo producer I just took some
code actually from the from the Oracle
documentation which I which I wanna
share with you and I just I just had to
set these these properties you know the
name of my the name of my topic which I
can see in the in the webpage the IP
address that I showed you in the port
and then this this properties I just let
them by default I can change them if I
if I want okay and this when I run it it
lets me just produce messages to cut cut
from a command line just very simple and
then I have a consumer also Java pure
Java code okay I took it from the same
place I need the same topic name the
server information and then I have here
a while loop you know infinite while
could you start consuming this this is
like a very not a very good way to work
with Kafka but just for demo purposes
I'm just want to get all the messages
that are flowing through you know so I
want to know the the offset and and the
value of the value of the message you
know that the content of the message so
if I let me show you that okay so for
example I've been producing some
messages demo Oracle
you know simple messages and my consumer
which is running as well it's it just
receives the same messages you know so
it in this case I'm just using this like
kind of a messaging queue very
simplistic views but just to see that it
works so if you go ahead and provision
your Kafka instance I think everybody
gets like these free credits for the
Oracle cloud if you came to the Oracle
code so if you want to go in there and
provision your even hope that service
instance and just look at the
documentation copy the code or clone it
from the git repository then you can do
this right out of the gate it's very
quick it's very easy to do and you can
really see that it's working you know
that the messages are going through and
you are being able to get the messages
even from your own laptop which is nice
it's it's not
here I don't I didn't have any problem
like security problem connecting to the
Oracle cloud I mean you need probably a
public key but that you can download it
when you create the service okay so as
long as you keep that you are yeah
you're off to the races you can start
working with this very easily you know
you can connect another application
whatever you want in in Hyderabad I had
a different talk about micro services
and I was using actually even hop cloud
service as my backbone as my data
streaming platform from the micro
services and my micro services were
deployed in many different places and
they were publishing and consuming from
even cop code service and I didn't have
any problem with that it was the easiest
part actually then I will show you
something else that was just pure Java
code as I said it's not the best way to
to consume Kafka but this is an
application created with akka streams
akka streams is a scholar based
technology from a company named light
wind it's a reactive used to create
reactive applications so in this extreme
Zemin what I'm using is the backpressure
capability so if you see here what this
app does it starts producing messages to
Kafka really fast a lot of messages
messages using a factorial function and
then it has to consumers but a consumers
one of them is very slow and one of them
is normal no so if I just let it run as
it is then the first consumer will get
the messages it will suffer a little bit
but it will get all the messages but the
second consumer which is a slow one it
will fill its buffer and it will start
dropping messages it will start losing
information you know so we'll have
inconsistency there because one consumer
gets everything and one customer gets
just part of it
so ARCA streams has a built-in
capability for back pressure all I need
to do is in my code tell it okay my
overflow strategy when the buffer
overflows I'm gonna use that pressure
okay that's it it's the rest is just
Scala code which I don't have enough
time to explain right now but I can show
you how it is
run actually yes okay so I will just run
it let's see if it rose
I think it's too big
so it's it's just producing an amazing
quantity of information towards the
Kafka
the Kafka platform you know and if you
see this these two consumers begin
running at the same speed and begin
receiving all the messages so what
backpressure does it's it the the
applications or one of the application
senses that this that this information
is coming too fast for him you know so
it signals the other application and it
signals the Kafka platform that they
need to slow down so the producer starts
slowing down starts coping with this
with this problem you know so then I'm
getting same information in both in both
consumers it's it's very simple very
easy in this case I just have to
consumers but I usually have many many
consumers you know so if one of them is
suffering I need the rest of the system
to I need to be able to orchestrate the
rest of the system to cope accordingly
you know otherwise I will just be losing
information and then I can keep running
and running and running and there's no
problem even if the messages are big and
and I have a lot of speed going on here
so that's that's back pressure that's I
just wanted to show you because it's a
different way to to interact with Kafka
you know and and this is just connecting
to to even hope that service using
properties and stuff okay so yes okay
let's go how much time we have I think
we have like five minutes left
so again you have you been hot service
very powerful data streaming backbone
very easy to provision no but you have a
lot of options to interact with that to
connect with that ok you can in the
Oracle cloud you can you can choose you
know to have Kafka connect you can
choose to have the REST API that's part
of the of the provisioning and then you
can use those tools to connect if you
don't then you can use whatever else
like I did Java API you can use a cast
remit you can use Kafka streams some
some other products if I go into IOT
cloud service for example or I go into
Big Data cloud service they are already
prepared to send or receive information
from even hot cloud service so that
those are just like kind of adapters
that you configure and you start sending
and receiving you know so that's that's
very easy to do as well that's that's
the basic architecture you know in Kafka
terminology we call with you sources
sources of data and sinks you know which
is where the data is gonna end up ok
that's basic terminology some
interesting use cases within the Oracle
cloud for example you know I've been
saying that several of the of the stuff
in there is prepared already to work
with even cop cop service for example
golden gate integration if someone here
Jesus Golden Gate Golden Gate integrates
seamlessly now with event service in the
in the links I'm providing in the next
slide yes there is even the the 18 blog
which they created where this
integration with between Golden Gate and
even Cod services is totally detailed
step by step it works I tested it big
data stream analytics obviously we're
talking about huge amounts of data so
that's the better fit for a platform
like Kafka even cop cloud service
several less functions for example the
the we have the FN project within Oracle
so that that's a very common use case
now in the different cloud providers you
know so for example I know that
Microsoft Azure for example they they
have a lot of use cases because they
have their own - complementation
event hope something and
and they are using for example actual
functions a lot to take messages and put
especially to consume messages from the
from the data streams you know so in
Oracle we have the FM project and that
can also let us create some service
functions that interact with Kafka with
very little very low code no
infrastructure so that's that's nice
for smaller scenarios micro services of
course had a very good fit together with
toshka micro services can be built with
akka with wiki for example which is an
Oracle open source project that that's
what I talked about in in Hyderabad how
to use wiki micro services with Kafka we
can use application container cloud we
can use the container native application
development platform IOT of course that
that's another very good use case for
using and even top cap service because
with the IOT we are expecting to receive
like huge huge loads of information you
know from from devices and and from a
lot of from a lot of sources you know
I'm there we can use for example the
rest proxy we can we can provision the
rest proxy because that will either will
let us interact directly with IOT cloud
service or even with integration cloud
service
maybe integration cloud service is not
the best fit together with with Kafka
but but IOT is no because this is like
kind of like so a service boss stuff so
it's not not so compatible but what it
can it can work together you know so I
have put together some some resources
for you if you are interested in this
topic and in this service ok if I see
many people taking photo but you can you
can also download the presentation just
from the Oracle code agenda it's already
there and we have GoldenGate integration
we have that connected care example
which is very nice from Oracle that it's
it tells the story pretty well about how
to use this kind of platform we have a
sizing if anyone is interested in sizing
then you can you can see a nice resource
there of course the administration guide
you know how to connect ICS and and even
hope that service through the rest proxy
that's interesting as well I think this
example uses to eat
or something that with our adapter with
the ICS to receive a lot of a lot of
information and send it to Kafka so I
think these resources are interesting if
you go into the as well into the into
the bankrupted service page then you
even hop dedicated achieve that if the
Wi-Fi
okay now that's going to
I think it's too slow but if you go into
the into your cloud dashboard
you know if you create your account go
into your club dashboard go into the for
the first time into event help service
then you will immediately have a page
with access to tutorials to be those and
to several resources that let let you
provision this very quickly you know
that first time I mean I have worked
with a little bit before so I have some
experience but still I use that tutorial
just to sort out couple of things if I
don't know what a specific field means
that tutorial will tell me so that's no
that's no no problem you know even how
dedicated
the open service console
quick start yeah for example we have a
quick starter you can create some basic
configurations right without configuring
too much no so yeah that that's
basically it I think I think we are out
of time but if there are some questions
that I can answer right down
inconsistency in partition yeah probably
you can you can use some event or
consistency pattern or something in your
in your application code you know in
your implementation take care from it
but first you would have to see if if
that's a problem of of the applications
producing and consuming to Kafka or of
the CAF complementation itself now if
the problem is in implementation then
you need to tell Oracle to fix it yes
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>