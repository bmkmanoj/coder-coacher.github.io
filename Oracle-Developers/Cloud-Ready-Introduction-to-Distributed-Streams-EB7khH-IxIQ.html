<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Cloud Ready: Introduction to Distributed Streams | Coder Coacher - Coaching Coders</title><meta content="Cloud Ready: Introduction to Distributed Streams - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Cloud Ready: Introduction to Distributed Streams</b></h2><h5 class="post__date">2017-08-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/EB7khH-IxIQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thanks for going to the well thanks for
surviving the day thanks for coming
along to the conference this is the last
last talked before beverages I seem to
have a some sort of knack of getting a
last talk so I'd like to get through it
quickly these are my standby beverages
so my name is Brian Oliver I work as an
architect of Oracle I worked as a recent
known work would be now the J cache
specification the world's longest
running specification which is finally
finished and I work in I work really in
that oracle coherence team which is
primarily focused on distributed
computing it's primarily Java focused
though we do do a lot of C++ and C and
other language as well but primarily
Java focused and that's part of the
Oracle cloud group everything is part of
the cloud so we're in the cloud group so
today we're going to talk about
distributed streams and so really the
talk is mainly about Java and we're
going to migrate from talking about Java
in to have them do stuff in a
distributed manner obviously in the
cloud so get started my obligatory
nothing I say this slide if I say it
exists it may not exist all you know you
get that either the point
okay so agenda obviously if we're going
to talk about distributed streams then
we really need to talk about the non
distributor streams first and so that
that's where we're going to start and
then we'll go into the distributed stuff
and I'm going to do demonstrations and
we're going to do lots of code there's
going to be quite a bit of code I'm
going to go through lots of code so any
of you don't write code in Java you want
to escape now
and that's fine or if you keep your
snoring to a minimum and then if we get
time we'll talk about optimizations of
this stuff now I gave this talk giving
this talk a few times so I apologize for
that but the last time I go is talking
in Austin for Oracle code it was
interesting to see the dynamic of who
uses Java 8 because we're starting I'm
talking about Java right
so my bligg Ettore every speaker asks a
question raise your hands if you use
Java 8 so very few so for the rest of
you you don't use Java 8 you're
obviously using Java 9 10
you give there's a leak for it so you're
on 7 I hope or if you're on if they're
on 6 you're there for a reason and and
you're and you're crossing your fingers
wishing you weren't on 6 and you could
move forward and you'll probably jump
from 6 to 8 is is generally what we see
happening in our experience but by the
time you do that you'll probably want to
jump to 9 anyway so don't worry so it's
important because we're going to be
talking about lambda functions I'm going
to talk about streams and the codes I'm
going to show you has lambdas in them
and we're going to talk about streams so
if you've never used them you will say
this is not Java if you've never seen
them you'll say this is weird so I'm
going to briefly give an overview of
that to get started and then we'll get
on to the distributed stuff so if you
have questions about this stuff about
streams I'm going to try and help you
along the way but there's a really good
thing that helps you that's called
Google and you can ask your questions
and you can I don't know don't ask me
questions but we could spend an hour
talking about streams and not about the
distributed cool stuff which I really
want to talk about ok so briefly
destruction a brief introduction to
streams so one of these things called
streams so the definition of a stream is
a pipeline of 0 more intermediate
operations followed by a terminal
operation sounds like something you
could get out of specification and if
you told someone in the street hey I'm
doing Java streams and that means I'm
doing 0 more
or intermediate operations followed by a
a terminal operation that go unique
probably need to be in a room and so
basically it's a concept that stolen
from functional programming I mean
borrowed from functional programming and
brought into the job language that helps
you deal with data it's about processing
data and different data structures
that's all it is it's typically used for
queries and aggregation so basically
finding stuff summing stuff grouping
stuff getting some answer out of a data
structure where that whether it be a
list or a said or a map or some
something some collection so we're
talking about the collections API but
equally it can be applied to all sorts
of other stuff and you see this thing
this style of programming this
functional programming language style
appearing all over the java landscape if
you're uncomfortable with the code I'm
about to show you sorry get used to it
it's happened there's going to be a lot
more of it but I'm going to I'm going to
coaches through reading it basically it
replaces the for loop in a lot of cases
you replace the for loop if anyone uses
IntelliJ IntelliJ has got this really
new nagging feature it's actually quite
cool it says hey you're using it notice
to using for loop I can convert that
into a stream boy if you like and you
could and it may actually run slower so
we're not going to go into why but
there's all sorts of articles about
whether streams are better than for
loops and we won't go into that but for
now we're just saying streams awesome
we're going to use them so they're
declarative in nature and this is the
first sort of example of a stream and so
who hasn't seen streams okay so there's
one person we'll go through go through
this but all right we'll go through it a
little step at a time here so how do you
read this like when you see this in a
piece of code you have to be able to
read it so so anyone can adjust hazard
what this does how's the guess
so so in in laymen's speak we're getting
the you're right you're getting we're
getting the products and a customer
resorted well the number of products
basically that of a particular product
ID so one there are two ways to read
streams one you can start at the top and
you can say all I'm I'm going to start
I've got a hash map of borders and it's
a let's say the long is the it's the
order ID and this is the order so I have
a map of order ID in order so you can
Majan that's my current state in a
shopping cart or it's the current state
of an entire system of orders on Amazon
that's holding the clarifying but in
this case it's just a hash map and then
what I'm going to do I want to actually
go through all the values so I say order
values values is a a new way of getting
things and I want to stream of values
and so now I'm saying I want to pipeline
and so what you can do is you can work
work through from the top so I can just
like work through as you as you have
done but actually sometimes it's easier
to read it from the other way so the
result of this if I said you know what
is the result of this well I know it's a
set of Long's and I can start at the
bottom saying I have a collection and
I'm getting a set back and what is what
is going to be in the set it's going to
be customer ID
I've got custom ID where is the customer
ID coming from it's coming from the
order where as the orders coming from
well what are the orders well they're
filtered out of all the orders are it's
the order if all the customers who
ordered this product ID so you don't
really see this when you start to read
obviously it's very hard to write them
in in Reverse when you see streams we
first see the first time but it's
actually easier often to read them the
other way around is like what is the
resulting time I've got a set of
something okay what is what is it a set
off and then I walk what worked
backwards so you can imagine if I had to
create this with a for loop
I would iterate over the orders and then
what would I do I'd say I'd go through
them and then there's an order item so
then I'd say for over the orders and
then each order I'd say four of the
items if the item is this product ID
stick it in a sack and then empty out so
it's a loop within a loop now that's a
sort of a load way of doing it but one
of the nice things about streams is I
can say I want to do this in parallel
because of because streams are a
declarative way of saying you want to do
some processing across a collection it's
not saying here that I have to iterate
over individual items it's saying here
is the rule here is the thing here's the
processing I want to do and so a stream
you can say actually I want you to do
this in parallel so by just saying one
flag I've just done in parallel or you
can say one of them in sequence you can
take advantage of the parallelism cool
everyone's multi call everyone to do
things fast and that is way easier if
you think writing parallel traversal of
a data structure and you managing the
threads yourself and what I want to do
are create some threads how many threads
migrate I don't know I have to clean
them up once I've created them or maybe
I'll use an executor service
okay I'll create an executor service
well how many threads so I'll put it how
will I do
and so you write all of this code
whereas streams like you say things
naturally sort of me okay you have to
read them upside down sometimes but it
gives you a much simpler clearer way of
writing things now of course they can
become quite complicated
because screams you can agree at them so
it's a bit like sequel so I want to do
like a grouping so in this case I'm
getting a grouping back so I have the
sum of the order item totals okay so the
sum of the totals are grouped by product
idea so I've got some of the totals by
product 30 and where are they coming
from our flat map lets me take an order
which contains other things and flatten
those out so I can have a stream within
a string so this lets me get a map of
the number of each type of products that
were bought it across a collection and
again imagine doing that in parallel
that would be really cool so that
streams this is just standard Java 8
basically every collection API has some
string capability so a little bit more
about streams they're really we talked
about intermediate operations there's a
whole bunch of them you can just chain
them together which is really nice there
are really two types there are long sort
of stateless which as data passes
through them because we call it a
pipeliner
data comes through through the pipeline
through each through each of the
segments through its operations and it
gets in the end a terminal as it goes
through that pipeline some of those
intermediate can hold on to state some
may not so filter for example just says
yes continue on as the entry comes in
continue on or don't continue on in the
pipeline
whereas limit they say I only want to
select a certain number of entries
through the pipeline before nothing else
comes good so the combination of these
things are really quite important so
stateful and stateless and you'll see
these and then the terminal operations
are the ones on the end and so there's a
whole bunch of them built into Java
thanks Java team for putting all these
in so let's say I have a collection and
I want to iterate over it instead of
saying for and then like credit for loop
I can now just say order to stock values
for each specified function or lambda I
can reduce I can pass in different
reducing operations I can convert it to
an array so I'll filter some stuff and
then put it in a ray to give it to some
other function and you can just like to
array I can count I can match there's a
whole bunch of things built in and the
end I can collect so if none of these
none of these are great you don't like
them you can use your own collectors so
collectors are a special type of
terminal that basically has four bits
what's called a supply which produces a
container because at the end of our
pipeline we're going to throw stuff into
this container so data comes through
goes in the container cool and then we
have a function which lets you
accumulate stuff to put in the container
stuff that lets us combine two
containers together and stuff that says
okay I've finished with all the data now
I've got a container give me the final
result and so of course you can create
your based on this collector interface
you can then implement these four
functions and then create your own
collectors and there's a whole bunch of
really cool ones so for example group by
you can do nested groupings summing
counts order by and so on and as I set
much better than nested loops there's a
whole bunch of them out of the box
so hopefully you never have to think
about writing and collecting yourself
basically I think most of the things you
would see you do in sequel or link from
some Microsoft net you would you could
you could use with these out of the box
so so we talked about streams now let's
imagine we want to distribute them why
honest one earth we're going to
distribute this stuff well imagine this
you're a Java developer and you've
written an algorithm but you can easily
go into the web in your final tutorial
and streams and you go okay I know how
to I want to cut cut my data up and I've
got it in a map and I've built something
cool and I can live a little unit test
and I've written this function that
processes some data query some data and
then suddenly I've got a huge amount of
data
many many terabytes of data and I won't
fit in the JVM anymore
what I have to do I just throw throw out
that code because basically streams only
work in process Java streams are
designed to work inside the JVM so it's
sort of weird for me that we have this
really cool language feature that lets
you manipulate Java it's built in the
language everyone knows how to do it we
teaches it teachers everywhere kids are
learning it and yet when you actually
want to do with any sort of data that
won't fit inside of JVM it's pretty much
useless so what we've been doing working
the Java team and Oracle is like come
out with a way of saying well imagine if
we could use those same concepts with
like virtually no changes but across
huge datasets and they're stored
somewhere which is call it a big fluffy
thing and call the cloud and what would
that allow us to do so we have the same
API but somewhere rocks processing the
data in parallel across hundreds or
thousands of machines instead of just a
couple of couple of threads because if
you think if I can process use the same
stream definition to process some data
on a couple of threads it's actually no
different from processing this huge
volume of data on lots of computers
because multi-threaded programming and
distributed computing is very very
similar very very similar in terms of
concurrency latency is a little bit
different serialization gets in your way
but it's very similar conceptually so
it's a big problem like
Java API Zin collection api's are
specifically designed to work in process
there's no way that you can run this
stuff outside without a lot of work so
for example it assumes today there's
always local it assumes that all the
functions are non serializable and and
so on so but surely we can make it
better and in fact things like the
collectors return stuff that's not
serializable but I have to deal with
serialization because if I'm sending
requests across the wire ether into some
farm of computers in some cloud and I
have to collect all that data and group
it up and bring it back to you there's
stuff going across the wire so I have to
serialize functions for a request the
actual stream the pipeline and all the
pieces in the pipeline any state that's
part of the pipeline and as a request to
go over do the processing and then into
intermediate results I have to be able
to serialize that collect that bring it
back and then compose the final result
but it's not only that it actually
becomes a little bit more challenging I
can sort of see why the Java seem punted
on this because it's not really easy
when you look at a pipeline you have to
ask yourself where does where does this
function execute so if I forgive you
this stream definition we went through
and so well collect does collect execute
locally or as it executes in the service
in the cloud so does this function which
I'm running in my unit tests for example
pull all the data from the cloud just
pull it all in flatten it all locally
filter it all locally and then give you
a set locally or does it push the stream
out and all the pieces have parts of the
cloud do the bits that I can combine the
results combine the results and then
bring it back to you and then give you
the set or is it some combination of
both of those things and how do you
decide and more importantly should you
decide
do you want to decide with the other way
of saying do you really care I just want
this function please execute it across
all my data so so we had to go through
this and go well where would this
execute where does it make sense and
then now the engineering team had
numerous arguments for a long period of
time and we tried some stuff that went
well that doesn't make sense and then we
had it like so we're backwards sports
this implement takes the implementation
of this took us about two years just
streams so we had to go through all of
these and where do these execute
cloud or quiet cloud client both one
other so we let's just see for each
that's the terminal at the end does for
each execute that function the land a
bit in the cloud or should it execute in
the requester some a vote who wants in
the cloud to three have a guess that's
three in the quiet so there's like maybe
10% will guess so that 20% gassing and
the other 80% is either napping or
really doesn't care like who is this guy
or please and thank you so I can get a
free beverage the the answer is like we
didn't know either
that's completely okay every any route
any choice you made apart from you want
to leave now
actually that's okay too is a completely
valid repellent idea right so we start
to think well one of these use cases
most people whose use for each are
probably going to display something or
using it to put it in something local
so for each for us or is it implement is
executed low what we call locally in the
requester but reduce that can probably
happen in the cloud min max account we
can well catch a tricky one where do we
count how so to account in all of the
servers in the cloud and then we can
bring it back and we can count locally
or they could be arranged in some sort
of tree and they could count there and
then they could aggregate it counts and
ring it back so counters but
so what we did is we went through each
of these functions and and said okay
this one will be best optimal count in
place aggregate locally bring it back
give you a final result so we can so we
actually use the pipeline in in multiple
places and and so on so we go through
all these and then it when it comes to
collective collectives has four
functions it has a supplier which
supplies a container that we can use
locally in the cloud in these machines
it has an accumulator that we can use
locally to accumulate things it has a
combiner which lets us combine the
containers great so I can combine
containers from different cloud machines
and put them together and it has a
finisher which lets me finished finished
the final result so collectors
themselves Java team yay you design
something that we could use in a
distributed with virtually no changes oh
except you to make it serializable so
what does that mean so today I'm going
to use coherence I work in the coherence
team and we're just going to call it the
cloud you can download carrots and play
around with it I'm not going to go into
like a huge amount of stuff about
carriers because I want to show the
concepts and we'll demonstrate we're
just going to play around Java so it's
an in-memory data grid and cloud run in
the cloud runner if you like shared data
scale out scale back massive heaps or
all good in it implements standard Java
ap is like so we have this thing called
a name cache which is basically like a
map it is a map it implements a Java API
map with exactly the same semantics the
difference being the streams connect can
execute remotely in your cloud so if you
deploy currents in a cloud then guess
what
that's when you use streams it's
naturally going to work as I've
described a few subtle things we know in
coherence when we when we get a request
we know whether the collector is has to
work locally or remotely so we re
implemented all of the collectors so the
set remote remote collector for set
actually do partial collection and then
collect locally whereas a regular
collector will have to bring more data
to your client and collect so a couple
of optimizations there's a few other
optimizations around entries in maps so
one of the challenges with the Java API
is you can iterate over the keys or the
values and not really the entries so we
said well really end up wanting to Train
over both so so we made a we made a new
optimization there and you know
basically it's the same so let's not
worry about it
so let's look at code code let's see how
this works probably wants me to end the
slide sure yeah IntelliJ so I'm running
some I'm running three mysterious cloud
service here so basically down the box
coherence and and so that's basically it
the original I'm running three servers
basically that's my cloud I want to be
putting stuff into the cloud so into
these three servers originally when I
wrote this demo two years ago and we
presented some of the stuff at JavaOne
it was actually about the u.s. election
and I had all US election candidates
probably I decided yeah probably not not
the best example so I went and got all
of the Java rockstars and I put create a
list of all those there's a lot more
data and I think probably have a lot
more integrity than a lot of politicians
so I can't really speak on the stream I
cannot vote anyway so so I have all of
these rock stars and what we're going to
do is a bunch of them 283 we're going to
load them into our into our cloud so
basically this is demo gods are with me
all this does is load
read through that note text file and
tuck them into my cloud and our codes
codes pretty pretty straightforward
as you'd expect it
takes in a named cash and we give each
of the rock star a UID we read through
them all and tuck them in the cash we're
just doing the point oh I'm in cash I
actually meant map it's just a map all
I'm doing is read stuff out of a text
file making up a UUID is the key and
throwing whatever the text is on each
lining into the map it just so happens
that this map is really a front for a
cloud so math is partitioned and hanging
out the cloud okay so that's cool so
let's let's uh let's run my my first
test so here is my first question we
have the ability to invoke a lambda
function against all of the entries so
in in Java so in coherence I can say hey
I get a session against the against the
cloud I say hey sexually giving my cash
on my map in this case the rock stars 1
notice my loader it started uploaded all
the data in the cloud and your
application went away so all my data is
in the cloud and now I'm saying well
let's use get a session to the cloud
give me that map and then I'm saying
well I want to invoke all and I'm saying
this land I'll invoke this against all
of the entries in the map so those
you've not seen lambdas this is a an
entry which is an entry of the type of
map entry of the type you are Dida
string in Java right it has type
inference so I could leave out the types
or I don't have to write anonymous inner
functions inner cost functions so this
is a function which takes an entry a map
entry and on the right hand side execute
this calculates this in terms of a so
what this is doing is I'm going to
invoke I'm gonna send this and this sort
of call entry process is lambda into the
cloud
and I'm going to execute this so when I
run it where is the output going to be
in my test is the output of all of the
names of all of lock stars going to be
in my test or they get it or is it going
to be in the cloud Klout correct so we
are going to ship this lambda across the
wire into the into our cloud and then
we're going to execute that lambda there
and then the return now on the end
because this function can actually do
some stuff to those entries and then
mutate them and return some value and
that we're not doing that what we're
going to do and say basically print out
where all of the rock stars are in the
cloud so let's let's have a lot I join
and nothing is really output apart from
a bunch of coherence logs but if I go to
each of the cloud servers I can see that
a bunch of people here there's a whole
bunch of people in here and it's a
little bunch of people on here so so one
of the cool things about coherence is
that it actually automatically
partitions and balances the data across
the servers and it makes sure that
there's backups and the backups are on
different servers automatically and the
partitions are evenly spread across the
servers and if you lose the server the
backups are done and so don't worry
about all magic all good
don't lose data persisted to desk
federated to other data centers like you
just all you have to do is use the map
API ok so that was pretty cool what else
can we do I have over scratch of ideas
here well I really like to find the
count of all the distinct people so
let's do that I think it's pretty
reasonable to be able to say we want to
do this anymore
it's just write some code let's this one
out so we're going to go through now
what are we going to do
distinct so rock star string yeah
stream of rock stars you can imagine
just coming in
pipelining in beautifully or very well
presented then I'm going to map II their
entries so I'm going to map them and the
first thing I want to do is I'm going to
get the value remember this is a map of
UUID strings so E is a map entry so you
can say map don't get key or mapped or
get value so I can get the value which I
know is a string so it's the name of the
rock star and then I'm going to split
the string because it's the rock stars
have first name and last name so I'm
splitting it by the space and then I'm
saying give me the last one or give me
the second one because this stream is an
array 0 is the first name ones gathered
last name and then I want to I want to
distinct all of those so this gives me
all the distinct last names account them
so let's do a reverse I want to count of
this distinct of the last names of the
rock stars where's this going to run in
the cloud that's right it's all going on
in the cloud so and of course that I
want to output well how many up rock
stars are there and it's pretty
reasonable to assume that there are some
duplicates and the distinct surname
should be less than the actual number
so Dunn says here there are two hundred
and fifty seven distinct last names of
the current 283 rock stars now this this
is like a very small data set but you
imagine lots of compute as I add more
and more memory and the data is
petitioned across all of that state and
you have lots of cause lots of CPUs data
spread out this is done in parallel by
default in fact if you want it done
sequentially you have to say I want you
to do it sequentially so we just think
if you go do stuff in the cloud you're
going to do it in parallel by default so
as you add more CPUs as you add more
instances that you imagine you go into
your cloud API management see more more
service please yep I don't care how much
it costs put on a credit card and then
as you do that now you hit the request
you double the number of CPUs you have
you have the time it takes to go
response for at least for this type of
linear query okay but you do that at the
site while this is running you don't
redeploy your application actually that
reminds me anyone find anything strange
about this
not that it's pretty poor poor way to
write it I could actually we could just
write as a function I guess I guess I
could could have just said this we go
back to this what did I do
what it actually do here hmm cloud this
running or clot what's the classpath of
my cloud servers well it's what what it
was when I started this system right
it's like I haven't really defined any
pojos or anything but where these cloud
servers running and then I was changing
my test and when I was creating a lambda
function what does that actually do
creates a brand new class my tech I'm
like creating new classes I'd like
change functions I rewrite this but at
the same time I just run my test and it
still works so one of the really hard
parts of engineering coherence for the
cloud is being able to let developers
work like that I don't want to have to
if I change a lambda function I don't
have to redeploy my entire application
and restart all of my servers if I
terabytes of hundreds of terabytes of
data that's come from other systems and
they've all funneled into your data
cloud with your cloud maps they're from
all over the place all over the world
and they go ah redefine a lambda
function well we're gonna have to just
restart all those gbm's because the
class part is different because that's
what you do normally fighting to find a
new function a new class I implement a
new class and I implement in your run
avala implementing your callable and
then suddenly want to use that against a
remote system that remote system needs
to have that in its class mark and it's
exactly the same as what what happens
when you write a lambda lambda
effectively makes a synthetic class so
what we did was we worked out a way of
capturing your lambda messing with the
byte code in the serialization and we
ship your lambda bike code and all of
the bits around it to the servers with
your request so you imagine if I have a
you know I'm working on this cloud and
Shaun's working on the cloud down the
back we're both writing independent
tests but working at the same same data
we could write the same class with the
same class name with lambdas in the same
location and there'll be different
lambdas and they won't run into each
other they'll be completely isolated and
that's what you have to start to do we
found when we were working with
distributed SEMS otherwise it's sort of
sort of a bit gimmicky because what you
end up doing is every single time you
make a change and by let me let me mean
let me explain this you find move the
location of a function in a class that
even if I don't change the definition of
the lambda the class has changed it's
different it won't exist the call site
won't exist in the cloud it has to be
moved so imagine if I'm using the IDE
and I go I'll commit and as part of my
commit it reformats my code suddenly by
lambdas will be different the ones I
deployed from the ones that are
reminding it and now nothing will work
you will just get class not count
exceptions so that's why only say we
spent two years it is pretty hard we
worked with a Java team a lot to solve
this problem to allow us to do this does
anyone play the Java 9 yet so I'll get
to your one second
well it's really hard to say the
question was what was a performance
improvement if I if I have like ten
elements and I'm I can run it locally
well that's going to be faster than
going across to the other side just well
I have to make a net were requests after
serialize the request I could ship it
across the wire even if even if all the
server's like say this is a micro
service this is sitting in the cloud
next to the cloud deployment even if
it's in the same data sand it's still
like serialized scan across the wire
even if in the same machine I start to
serialize it and get it to the other
process so it depends on the size of the
data set so small data sets you can do
locally and that's the really cool thing
is you can start to optimize the same
stream we can say well we actually have
most of the data local locally and with
coherence you can have views of data and
so you execute against the local beauty
versus really large data sets like if I
have like a 20 terabyte data set I can't
fit it in the JVM it's not even possible
to compare because you can't do that
work in the JVM at all so it very much
depends on the use case it also depends
on the function imagine like this is a
trivial like just put a string up it's a
function how to do some complicated like
price evaluation and I want to do that
in parallel even if the data set was
smaller maybe in a thousand elements it
may be better to ship that to a thousand
computers and have them all work in
parallel because creating a thousand
threads may not work so the idea here is
that you can use the same API and tweak
the configuration if you insurance
talked early talked about the
configurations in external and you can
start to see what we can do now with
things like this because you can say
well why don't I apply some machine
learning to this like we're running in
this runtime we can collect some data
about the types of requests you're
making we know the size of cloud your
heading
for example coherence can automatically
index data now so we don't have to
deserialize things all the time
then you can start to do these really
crazy optimizations and you think like
the JVM does amazing optimizations right
when it compiles we start to apply data
science to streaming api's then you get
like it's very hard to compare but for
small datasets fit it in the JVM and you
and you're okay with any GC time that
that causes then I'll just use regular
regular Java streams but in a cloud case
you would use bigger you'd use a cloud
stream the important thing in here is
that we want the API be the same you
don't have to learn an entirely new
programming language or an entirely new
product to deal with a larger data set
and just dealing with something that you
already know how to do in a small data
set like doesn't make sense to me that
you'd have to do that okay there are
there are special use cases there are
tools for processing large data sets but
so many use cases fit into this category
yeah we actually do we do that stuff for
you so it's interesting though yeah this
is that this is this is the code so we
have a bunch of optimizations we may not
really like get into this but it's
interesting looking this field is like
well if I'm going to do some distributed
computing the arms don't see well I need
to create a directly directed acyclic
graph of the routine I want and I need
to set up four layers of clustering of
infrastructures and some zookeeper and
like some basis and like kubernetes and
then run so good all that work out then
I'll allocate the tasks out and I'll get
a coordinator and a schedule and all
allocating that oh geez I better put
Tasker in because I've got all these
things streaming messages back to me and
now I've built this thing and now well I
have to put this in containers it's
going to be a nightmare to manage they
were like well reality is a lot of this
stuff is like you have a bunch of data I
want to do you think well act alone
MapReduce my streams basically gives you
MapReduce and it's just standard Java
like how
can we take that concept to the cloud
for Java developers and I person
anything is quite away and we've we've
actually demonstrated that we can do it
on pretty pretty big datasets I mean I
hope this is useful yeah well like I
have to learn I have to learn Python I
have to like yeah it's I life I thought
I've been doing it for many years so I
don't take no offense but yeah it's this
there's so many things like different
like you're forced to become polygons
because of all of this Monisha around
you when it's pretty clear you can do it
here so so we did a bunch of
optimizations like so we know when for
example we can filter and we can index
parts of pojos that are in the cloud
because we don't if you're like doing
filtering on things in the cloud we
don't know like deserialize them and
let's say they're all on disk and we
have to bring them out from a managing
like huge volumes then we can make very
efficient indexes like a database could
do so while normal stuff works we have
optimized versions on the coherent side
sort of like don't do it like this -
like this and so on they all they all
work anyway coming to the end so we hope
this stuff is coming soon we hope this
stuff is very useful in terms of you
know we're working and worked quite a
lot with the Java team and we're
continuing to work with the Java team on
a huge number of other other things that
are coming along so we have things like
stream we have things like flow in Java
9 which applies very well enters in the
distributed space we already have
existing compute concepts like executor
service and things like that which are
equally applicable under the cloud there
they're a little harder because executor
service doesn't really have the concept
of Fallot value and failure fell over
built in so we have to like math so the
semantics a little bit but we've done
all the work on in app team to help the
Java team to do all of the magic and
really is a huge amount of magic into
like taking and identifying lambdas when
they don't have any identification
because a completely anonymous basically
the biker level and cleaning them up and
shipping them across the wire
efficiently and making sure multiple
users of a genre cloud if you like you
know
cloud map cloud data structures don't
run into each other because it's not
like in this case where I just have one
application I have lots of applications
at different version levels on different
JVM Zahl accessing the same data and so
you need to be able to do that you need
to gather you know evolve your
architecture and have different types of
clients and you know we haven't talked
about JavaScript and all the other ways
you want to plug into this stuff as well
so I hope you found it interesting you
can see how not only can we do streams
we can execute stuff like lambdas in
place which is really efficient and if
you want to play around coherence for
you to download it and thanks for coming
so I'm around for some questions if
you'd like yeah I don't I didn't say
that
right it's big enough I just well
there's a certain amount of there's a
certain amount of overhead in doing for
each in setting up the stream like the
stream is a set of objects which are
tied together and where as a for each
just says give me this I'm going to
iterate over the data structure so easy
right yep right yep
yes yeah so we we made the decision with
this with the standard Java for each do
it locally but the in-vehicle method
that I had will do that for you and you
can also there are other there are other
extensions we did so I didn't I didn't
want this to be a hey here is a
introduction to coherence talk I wanted
to talk about Java streams and then and
then you know there was a whole range of
other things that we added on the end so
for example I briefly went through their
optimizations on indexing so we can
extract pieces out so I think what
you're after you can do is normal with
with normal coherence out-of-the-box
that would probably the only change with
4-inch I wish we should go back to
engineering and just say yeah add a flag
remote for each
while it does not yeah if you've not
been trained in functional programming
this is quite hard to wrap your head
around it takes quite a bit of time can
give you a headache but yeah and as I
sort of said like it the my example of
like this crowd of skilled engineers
trying to decide which should be done
where we'll never reach consensus so so
we we all sort of went for each is
probably going to get done locally and
yeah we could all think of cases that we
were wrong and you use well we had to
make a decision so but we know we know
sort of on the coherence engineering
side that there are other alternatives
so the the regular developer run into
the hey I did a for each but my stuff's
not iterating locally or as you like hey
in order to Forex but it's over there so
you could you could use a custom
collected custom collector would be the
way to do it and not return anything yep
yes yeah so and then I'm the carrier
inside we like work well we have ways to
help you serialize things as well but
yeah so it's the big it's the big trap
of like oh yeah it's it'll just
magically work as long as your things
are serializable and and there are some
really I hope it'll probably come out at
JavaOne this year there's some really
amazing research going into making
civilization go away well go away in
terms of the effort that developers have
to do to implement it and at the
language level it's very very NOx
beautiful elegant and it will make this
even easier
cool
thanks for sticking around</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>