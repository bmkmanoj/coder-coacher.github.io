<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Use Apache Kafka to Add Streaming Analytics to Your Application | Coder Coacher - Coaching Coders</title><meta content="Use Apache Kafka to Add Streaming Analytics to Your Application - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Use Apache Kafka to Add Streaming Analytics to Your Application</b></h2><h5 class="post__date">2018-03-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/DaSEDcVLUzg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right I think we can start thingy is
becoming and welcome I always get the
slot after lunch oh it hasn't been lunch
yet or you just waiting for lunch great
you still still all awake that's awesome
all right let's see this talk is about
Kafka and streaming ETL if that's not
what you want to hear about then you
might be not not be in the right room
let's get started
my name is Bjorn I'm originally from
Germany town called Hamburg which is by
the water just like Chicago I've
recently moved from Germany to to Canada
so I live in Ottawa now and here's your
pictures of Ottawa that's all over in
the summer and then there's a picture of
Ottawa right now on the left side of the
screen it is it is very snowy and then
my question I get a lot is how do you
pronounce this name and it's really
really weird and I can't even pronounce
it properly myself when I speak English
but it's pronounced be Huron ooh when I
go to Starbucks I usually call myself
Brian it's just easier it can be spelled
like that and that is even more
confusing and it's actually not a German
name is a Swedish name and there was
this famous Swedish tennis player called
Bjorn Borg
he was very famous in the 70s 80s and
then the singer of a bar was also named
Bjorn and it's it's a word that has a
meaning it means it means bears or that
would be me or this one is I guess more
more proper fat and lazy I happen to be
an Oracle a director and an Oracle
developer champion and just a quick into
the Oracle ace program and develop a
champion of programs that Oracle have
have put together for people that are
not Oracle employees but that do give
back to the community by blogging
writing articles presenting at
conferences like this and it's a nice
recognition that they do support us and
some of these travel things so I don't
work for Oracle I work for a company
called Pythian they're headquartered in
Ottawa but we have people all over the
globe so we we do is we actually enable
people to deliver work remotely and we
hire good people wherever they are which
might be in North America it might be in
South America in Asia I've got
colleagues in India and in Europe and
it's quite fascinating and what we do is
we we help clients with data problems
and we
started out doing managed services for
Oracle and sequel server and other
relational databases and we have
discovered that big data is big in this
year to stay so we throw the big data
practice we have consulting and services
around machine learning and AI and so
there's lots of things that come
together and I work in a consulting
group that actually covers module of
these technologies and this is kind of
where this talk is coming from it's
coming from me with the traditional
Oracle database background but looking
to how to interface Oracle with Big Data
solutions like Apache Kafka and this
talk I want to start with a bit of a
motivation I want to show you a demo
that I've built and I don't know if we
have the time to go into the demo by the
end but I'll just show you what what
this demo does and then we get into a
quick intro of Kafka and then we go to
the different steps that I that I built
in this pipeline this demo that I'm that
I'm showing that's just not so this is
this is a big picture of what we're
building so we want to take a relational
database and we want to do some magic to
it to get to a real time analytical
dashboard and I can show you how that
looks like in the end hopefully larger
well that's also not the wrong that not
the right window you need to see this
window so this is something that's
already running that there's a my sequel
database somewhere and and it's it's
generating orders for wine salt and
olives the reason why that is I did the
stemware in Israel the first time and
those were for us I came to mind and we
can see that the sales just coming in in
real time and updating every 15 seconds
so the three four dots that you see to
the right that are not quite high yet
that's because we're still receiving
orders for these for these time buckets
and this to screen thing is a bit weird
and big enough I don't even know how to
make this bigger that's not how we make
it bigger oh yeah I don't know how to do
this but I just I just I mean this is a
simple command I think we can assume I
sequel database look there and type here
that's gonna be weird my sequel code for
any of the database
that's my sequel and I need a command
that I have copied here somewhere it's
just a simple insert the monitor thing
that's weird actually practice this so
this stick amount it's a simple insert
and it can make this bigger by the way I
think
so it's an insert into this orders table
and I'm inserting a new product called
lumpy will get what others in a bit
applies to 500 the user is 42 and I'm
ordering this this now and just so let
me copy that into this window doing this
and I'm doing another insert let's say
this is not 5 fun of this is just 200
and we can see we can see this here
right so this orange line is this new
order for for this lumpy product and
it's at 704 all of these time pockets if
we do another insert do another insert
now should see that only some of these
orange things update there we go so
basic that's that's what I'm showing
that's the end that's the end goal it's
doing this relational database and then
doing some real time dashboard thing it
it doesn't look like much magic here
because it's really just a small simple
use case and you could probably just as
well clear the database directly and get
the numbers in to some some dashboarding
solution this is the whole pipeline that
we're going to build so actually I have
two data sources I have from my secret
database and I have a generated click
stream or a generated stream of data we
ingest the data into Kafka using in this
case a product called Museum then we do
some transformations so these
aggregations the data is comes in it's
just one order at time and the dashboard
shows you the orders per product per
minute so we do some transformation some
aggregations there we store the data and
elastic just because the tool we use for
displaying it can't create Kafka
directly and then we have the nice
little dashboard so that's that's the
big picture and we'll just look at the
components now quick intro why why does
Kafka matter and where does cough cough
it cough cough it's really build to the
picture of big data and there's I'm not
the guy who came up with this the three
V's of big data but it's about volume
and variety which are two they
is that a lot of big ADA solutions
covers so Hadoop covers us very well you
can store large amounts of data and you
can use different data sources a dog
that doesn't all have to be relational
or even doesn't even know I love their
schema but velocity is the third aspect
of Big Data where you want to get
insights in your data exactly we want to
process your data quickly and this is
Kafka's definition of what they what
they say they are so Kafka is an open
source product that they just say they
is used for building real time data
pipelines and streaming apps it scales
is fault-tolerant it's wicked fast and
it runs and production and thousand
companies so that doesn't really tell
you what Kafka does or how it looks like
so let's see if we can if you get a bit
of a better picture of what what is
Kafka I stole this slide from from the
confluent website and this is this is
how your your enterprise might look like
today without Kafka where whenever you
have to have two applications or two
databases or two different sources talk
to each other we built these one-to-one
connections of some sort it could be
just an extract to a file and import or
it could be streams it could be Golden
Gate it could be other message queues
but we build a lot of these and we
usually build them one one producer one
consumer and the the promised land of
not doing the spaghetti stuff is this
where you take something like Kafka you
have that as a central streaming
platform where all of your events
publish messages into and then any
number of subscribers or consumers can
read messages from from Kafka and these
can all be very different systems right
so we see some database and there you
see some applications talking to it
directly you see monitoring solutions
consuming data you see Hadoop being a
consumer or producer so that's really
what Kafka aims to be the aims to be
that glue that holds your your
enterprise systems together and makes
them talk to each other or easily
connect with each other to me Kafka does
four things and the first thing is the
clustered message bus and it's certainly
not the first message bus right so
there's there's what's the Java thing
it's JMS and then there's some rabbitmq
and there's tons of different message
systems so that's not something new and
it's a clustered version so that fault
tolerant that's one thing they do the
other thing that I think makes Kafka
unique because other typical pops up
subscriber things do
do this is capable by default retain
your data it will store your data for
however long you want to keep it on
these cluster notes and replicated so
that if one node fails you still have
access to your data and that allows you
to subscribe to topics or to messages
after you've produced them so I could
just start producing whatever I want to
get from my relational database into
Kafka now and I might not even know who
wants to consume it and have to set that
up now I can in a week or a month from
now somebody might decide they want to
look at the stream of events and they
might just consume it another amazing
thing about kefka's they've the
community's done an amazing job of
building connectors that are all open
source
to connect without most the open source
like speak to many different systems so
there's connectors for Oracle for other
relational databases for Hadoop for
Twitter for things like that and you
only have to configure these connectors
to implement connections to new systems
and then the the part I find is really
cool it's rather new it's actually
processing these events in real time as
they flow in and out of Kafka and doing
these transformations right there and
impact us so that's the four things I
think Kafka does and at least the data
retention and the stream processing in
the same technology stack or the same
cluster that handles your messages
tribution is it's pretty unique to Kafka
and I also don't know any other system
that has as many connectors that are
also all over mostly open all right so
what I use cases for Kafka event hubs
one use case I see is linking together
the system so just exchanging messages
about about a system so you might have
an old 20 year old Oracle application
that you are quite happy with but you
would like to add something like a micro
service like every time a user signs up
at that user to the email marketing
campaign and an send an email and that
would typically be a new development
request and you would ask somebody who
developed this old application 20 years
ago to change them on with the code to
add this connector or to add this the
service to it but you'd have to do it
that way you can put these events into
Kafka and just say whatever other
service wants to listen to me
registering in your user I can I can
take action on that think about IOT
where you have streams of data
big and so fast that you can't even
where you might not be able to easily
stored in a relational database you
might just want to stream it into into
Kafka and then getting insights about
this so again think about IOT think
about traffic sensors for example would
you really want to wait a day to analyze
your traffic patterns to tell you that
yesterday there was a traffic jam
somewhere on the highway now you would
probably want to get inside very quickly
and see if there's what 1050
measurements of somebody going less than
five miles per hour
you want to raise an alert and say this
is probably a traffic jam we want to
report this and/or it might even be an
accident that we haven't discovered and
then this use case today is really for
real-time dashboards so think about a
real-time dashboard in this case it's
sales data and you want to just in real
time see your sales and maybe even in
which that with some data which would be
the the ultimate goal to my format for
my demo getting started Kafka Kafka's
not only used by thousands of customers
Oracle also has built a product around
this so there's an Oracle cloud service
called the Oracle event hub cloud
service which you can use with either
trial or or a paid Oracle account
obviously and you can easily spin up a
Kafka class Oracle provisions and
manages to some degree for you so you
have to worry about the whole setup of a
cluster and all of that there's a few
slides with a lot of text which we
probably want to skip over I kind of
want to make the distinction between
state and events so when I say streaming
and Kafka I talk about events and in
event really it's just one thing that
happens and it's unbounded we talk about
unbounded data sets we're just data
happens to flow in whereas anything we
do in databases really is giving me the
state of something databases are really
good at storing state or reporting the
state give me the current money that I
have in my bank account that's a
question for relational database but the
tell me when I made a transaction that's
really more of an event and I think that
distinction is is important to make and
again the IOT thing is the same thing
you have an IOT sensor it reports
something that is an event the state of
how many cars have gone through this
highway traffic sensor thing yesterday
that's sort of a a state thing right so
might might demo use it this simple
table it's called orders as these four
five columns and it's really simple it's
just a product
price the user ID and and some some
order timestamp and I've got a simple
shell script running that just randomly
insert some inserts orders into this
table right so we have data in a table
we don't want to touch the database it's
if we don't want to create the database
necessarily I'm directly from one from
an analytics tool so how do we get data
from a database or from a system into
into Kafka and the first place to look
at what we have got connect and connect
to say is an open source framework that
is built on top of Kafka where Kafka
itself doesn't really care about the
message format you could just make it
any format you want as long as it's
serializable you can store it and calf
can consume it on the other end
Kafka adds some support for Avro and
based on data types if you use Avro it
has a schema registry where the producer
would write the schema information the
registry and anybody who consumes the
information would but know the schema if
the data it goes in and then this
Catholic Connect framework has tons of
pre-written connectors there so I can
from a dupe here there's sequel
databases there's files so it would make
it very easy to just write a
configuration for a connector to for
example consume messages from Twitter
and then send them out our no produced
messages on Twitter right into Kafka and
and have a different connector that
writes them to s3 for example my
examples use use this thing this is this
is actually lumpy so when you saw a
lumpy in my demo first this is a kind of
mascot of the ice program that I used to
carry around to conferences but it's a
real pain to travel with and answer
questions to TSA
so I didn't take him and he's quite
pissed about that he's got his entire e
where he keeps us for Cyrus so he said
he hates the snow and Canada and these
soldiers he didn't go to all these warm
places and so that's a styrene if you
take a thing of this diary table that's
that's really easy that translates very
very easily into events because every
new entry in the diary
is an insert it's a new insert and the
ID increases and fetching that into
Kafka is it's very simple with a Kafka
connector that's called Kafka Connect
JDBC which will just carry this table to
do a select star from diary and then it
will remember the highest ID as an
offset and the next time it Claire is
that it will just say give me all the
IDS that have an ID higher than 18 so I
do this year instead anew and you row in
this table
and now it would fetch this thing and
this is all you have to write to get
data from this diary table into Kafka
you write a config file where you give
it a name and and in this class and then
you just connect the define the
connection string you say which table
you want to fetch data for and you say
the the ID column e you start from a
offset and then gets written to a topic
topic is basically the table or the name
of the stream and Kafka which will be my
sequel - and then in this case theory
because tyree is the table me and and
that works rather well and this will
just wake up every five seconds and
carry the database and but we're still
kind of calling the database and this is
also limited if you if you want to if
you want to look at more complicated
tables like this this is Lumpy's
inventory where he has certain things
like he's got friends and selfies and
nametags and he loves to take selfies so
the selfies row might just get updated
but you might update that to to more
than this so we update the inventory now
let's get more friends this is something
that our default country would not pick
up because the ID didn't change and
we'll just say select star from
inventory where addy greater than 5 it
wouldn't pick up this change so how can
we how can make this simple connector a
simple JDBC connector work with what
these updates so one thing you could do
is if you already have this it's awesome
if you already have something like a
last modified time stamp you can easily
use that if you don't you probably don't
want to add it to your table just for
just for this requirement and what I
certainly don't like is this vary from
valid to way then have to have a lot of
logic in a sequel to determine what's
actually a valid role right now
one way to do it is with the method I
called performance change data capture
where if you are an Oracle it has a
feature called flashback query where you
can specify select from a table versions
between yesterday and today and it will
give you all the versions including
updates inserts and deletes and Oracle
does that using undo data or even
flashback data archives and yes it got
another talk where I go more into
details of this but just just like this
is actually the inventory table with
this versions operation so I say to
select from inventory versions between
min value and Mac
Valley bitches means give me all the
versions that you that you have and and
then you see it actually has two so
friends started with 42 and it was an
insert operation and then friends got
updated that's you 242 which was to stop
doing the same thing and then there's 50
more friends and there's more friends
since all these updates so we fetch all
of these things that are basically
events and this weekend fetches into
Kafka turned them into events and that's
the way how you can use this Kafka
connect JDBC driver or module connector
to get changes into into Kafka from
Oracle that's just a config for that but
but really what you want to be using
probably if you want to get data from
relational databases into Kafka is you
we already have event streams in
databases like every database that I
know has has some concept of we log
files or pin locks or transaction locks
and and they are basically streams of
events we store every insert update
delete in some way or another in these
in these in these locks so that's there
an Oracle some video my view costs and
pin locks puskÃ¡s calls them right ahead
locks and then secret server code some
transaction locks and what we can do
with this and this is just one example
from from one product is they take these
video logs and they mind these redox
write them into a intermediate format
called P logs in this case and then
these P Docs can be mined by a casket
connector stored into Kafka and once you
have them in Kafka you can do whatever
Kafka connects to with this with these
messages and this is the example of PB
visit which is a product for Oracle
sources there's a bunch of other
products out there that do it for Oracle
sequel server passwords my sequel etc
let me skip over this the connector I
use for this demo is a product called DB
z 'm and DB z 'm is an open source
project that is sponsored by Red Hat and
they have connectors for Red Hat
Postgres and Mongo and they are working
on a free connector for for Oracle as
well they use the capo Connect framework
they store Avro and it's quite simple to
setup again you just write a config file
and you and you submit it and
this might be too small but what you see
here that's that's the whole message
that we generate so whenever I insert
into my orders table in this case it's
an order of a wine for eight whatever
the currency is that's ten years we have
this and then we have a bunch of
metadata like the source database and
the time we made this this this change
is made and all things that I don't
really need in Kafka so I'm looking for
a way to transform such just filter out
only the things that I need because what
I need is only the pull stuff here it's
the ID of the order and the product and
the user ID and all that so to do the
way to do that in an Kafka connect is to
use what's called SMT source single
message transforms and those are just
simple commands that you configure in
the connector config files so when I
configure this connector I added this
these parameters transform equals filter
filter type is replaced field value and
then I say just whitelist everything
that's in this in this nested type
called after or nested object so I do
this and then the output will be just
this nested piece with this thing in it
so this is awesome and almost what I
want I don't need this to be nested I
kind of want this flattened and luckily
there's an SMT connector that will
flatten it for me so I add a second
transform to it called flatten and I say
make the limiter be this this underscore
and now my output is this after
underscore ID after product etc etc it's
flat it's the it's all the information
about this order it sits in there that's
pretty cool I still want to get rid of
the after underscore because there's no
need for that so that's a third
transform it's called rename but I just
rename everything with after underscore
two to nothing so so now I've got my my
database order whenever I put an order
into my my sequel database the B's iam
will read it and between the B's iam and
connect it gets transformed and
flattened into to this so now I have an
avro object that has all of the data
from from each 20 transaction it so
that's cool that's the way how I get
data from a database into into Kafka and
nicely in an a probe with the schema so
you see that data types are correct
that's an integer
there's a string and then they also in
in my sequel that's actually a date/time
column Avro doesn't have daytime they
only have long for this so they start as
a long data type which is actually quite
useful down the line alright so now I've
set up my database I have stream data
into into Kafka and now the question is
how do i how do i process that what do I
even want to do with this and
traditionally when we when you data
process and we talked about ETL for
warehouses where we take data from
transactional systems or from several
sources we ingest them typically once
twice three times a day and we enrich
them by joining its tables together we
aggregate them and we might put them
into something like a star schema to
make them easily variable by by
analytics tools and that's how we eat
relation to do with ETL right but the
problem is it might not be it might not
be fast enough and so hot topics and
analytics today are in memory like all
requests and every feature sequel server
has this Tara data does this we're but
you do stuff in memory so that you can
in theory run OLTP and report in the
same database so that's a hot trend
really with the goal of providing
something like real-time or near
real-time analytics machine learning and
AI RPC heart and then new data sources
are coming up so this IOT example I
talked about or clickstream or other big
data sources that we might want to just
model together with our with our data
warehouse kind of data that's new and I
wouldn't even know how to handle some of
these cases in a traditional data
warehouse like what I actually want to
import the whole clickstream behalf of
my website in HDFS or in an object store
into into something like an Oracle
database probably not similar with IOT
maybe we can but do I really need to
import all this data into relational
database I know so so that's where
stream processing and actually the idea
of processing data asset happens in a
stream as' has started and that's also
not new and there's a bunch of solutions
that do this with the most popular
probably being storm fleeing spark and
then the true that I really like is
Apache beam or cloud Google Cloud
dataflow
which allow it to do exactly the stream
process
on unbound datasets the the problem that
that many people happen that I also had
essayist in as an entrance barrier to
this world of these tools is that you
actually have to write code for these
things so all of these tools typically
if you if you want to write Java code
they all do Java
some of them do Python some of them do R
or Scala but really you're writing code
you're not writing just just sequel
anymore so so this is this is how I fell
for a long time I want sequel give me
sequel and I and I really wanted to get
into stream processing and this is you
know so this is this is what the core of
this talks about is about tool called
case equal and case eager was develop as
an add-on for for Kafka by the company
that runs Kafka so there's a commercial
offering you don't have to buy their
product cafe case equalists is still
open source you can use it and the idea
of case equals to enable you to run
stream processing directly on your Kafka
class so that you already have using
sequel as the language to express your
transformations which means you don't
have to write Java code you don't have
to write any other code to just write
sequel with a little bit of a syntax
add-on for for something you need for it
and it's an exciting time this has been
out for for beta for love like what half
a year and my demo is actually built on
the latest beta release but come Flynn
has announced that it will be GA at one
point they announced it would be GA in
March 2018 which would be now and then
they said April 2018 but it's gonna be
any weakness so look out for this right
now this is still beta but it's it's
coming into production so so what's the
big deal about stream processing why
stream processing so different or so
complicated and why is it not the same
as regular processing and when we do
regular data processing or regular ETL
we usually work on bounded datasets the
data we have in the database is usually
it's probably haven't start we have an
end and we know that all the data inside
the database is to be D consistent with
respect to each other which makes it
possible to do everything we do when we
talk about streams its unbounded we
might not even have the data from
whenever we started consuming the stream
and we certainly know that there's more
messages coming in
all the time so it's it's kind of
infinite certainly unbounded and Ness
the the messages in a stream don't
necessarily have to be consistent to it
with respect to each other also you
don't really get it guarantee for Tannis
of messages it might just be that your
messages come in later because the
system that produces the messages are
just late and sending them or because of
some networking problem so might be that
you receive events out of order so you
also have to take care of that and the
kind of transforms you want to do
there's to me there's three kinds of
transforms the first one is easy I've
already showed it is the single element
transfer if you just need to take a
single element and change its its shape
like you need to flatten it or you need
to filter out certain columns or you
need to apply a function to one to one
column or to 120 data element that's
kind of easy because you don't have to
look at other elements in the stream and
you can do that with the single message
transform that I just showed in Casey we
can do it too and Kafka's streams can do
it there's many solutions that do this
this is simple this is easy and again
that's just my examples of that I showed
earlier the the harder solution is how
do it we aggregate an unbounded data set
like how do I count all of the orders
for salt or for wine or for anything
like that that is obviously easy and
abound data set we just do a group by in
a database and sequel that's easy but
how do we do this on an unbounded data
set of what does it even mean if I say
select the sum of all orders group by
product what does it even mean and how
do we do this because it's unbounded we
don't have a start we might not have an
end and the solution is this it's
windows not the windows that thus lowers
for but it's these kinds of windows so
you take your unbounded stream your your
having no end no beginning kind of
stream and you break the stream down in
Windows and this is an example of what's
called wow these are tumbling windows
they don't overlap they have a fixed
size so in this case each window is 5
minutes in size and now we can take all
of our stream events and put them in one
of the windows and we can easily group
aggregate and do stuff on on each window
this sounds way too simple it's not
quite as simple as that because what
might happen as I said there's no
guarantee for events arriving in order
and
you processing events at the time when
they actually happened so there might be
a bit of a delay so take this example
we're at 8:17 there's a new event that
shouldn't be cut off by the way there's
a new event that at the time some of 802
so what do we do with this does this
does this event go into this bucket
because that's when we process it or
should it go in this bucket and logic
says if it's the event times them and we
window by the event time so because
that's usually what we want to do you
want this sales for the window of 8 a.m.
to 8:05 then we need to make sure that
we put this data piece into this bucket
and KC will in any other streaming
solution will actually do this but then
the question also arise how long do we
do this for like how long do we keep
this window open for do we keep this
open forever or for an hour or for a day
and what do we do with late arriving
data that comes after that that's an
example of I said tumbling windows so
they a fixed size they don't have any
gaps for the easiest window type to
understand the second type of window
that streaming solutions typically
support is called a hopping window where
the length of each window is fixed in
this case it's fixed to five minutes but
we open a new window every minute and
I'm actually using this in my demo if
you remember back the demo it had four
or five data points that we're still
kind of growing in size and that's
because I had actually four windows that
were open for a minute each but it
started every 15 seconds and it just saw
that the graph is a bit more a bit more
dynamic and then I got lazy with the
slide and it didn't copy and paste all
the all the pieces together and the
third type of windows is is called a
session window where the size is
variable in size so you might say I want
to create a window every time a yellow
event happens and I want to keep this
window open until no event has been
happening for three four or five minutes
and this is obviously useful if you want
to do clickstream analysis and you have
a web session and you want to say I want
to keep these events together in one
window and as long as there's some
concept of a session still still being
active right so now that we know what
the window is it's really easy to write
sequel so this is this is actually a que
sequel statement where we create a table
on an order
so we have a stream called orders and we
create a table on this select the
product the sum of the the amount of the
order from this stream or and starts a
stream that's an unbounded dataset and
we created as a window hopping 60
seconds each it advances every 15
seconds so for every minute that would
be for open window so for active windows
and we do a group by product and then I
do this little thing where I just
enhance it with the timestamp associated
with each of them and that'll just be
useful for displaying them actual window
buckets and if we have time we can go
into the demo and actually look at these
the extremes this is the stream for for
this is the I just created the orders
per minute or the oil per minute time
stem suggesting Microsoft Update and you
can see this is the window start time
and then this is the products we can see
that there's these four active windows
for olives and we can see that this is
the demos this probably the newest
window that's only been open for the
last 15 seconds so there's less order
mode in there and then as we keep going
with a demo it'll just it will just
increase this these numbers so we get
for each window we get actually get more
than one update like every time and you
have a new event it will actually update
all these four windows for us there's
just something I copied this are the the
functions that you have available now so
it's and you notice that the it does
poison doesn't have an average so
there's no streaming function for
average which is kind of hard because to
have the average you have to have all
the previous data if you want to build
some like an average you have to do it
yourself you have to count them and then
divide by the no stand them up and
divide by the count to to build your own
average so that's how you do a grits and
streaming and this is already pretty
powerful and this is something that that
you couldn't do otherwise write your
first we could fund Clarisse like this
directly against the source database but
doing in stream processing
piece-by-piece you just count and the
last challenge really is what happens if
you need to join together two data sets
or two data streams and my I'm doing
this in this demo to where I take I've
got it set
it's dream called users and the users
have have levels there is the gold
silver platinum and my orders only have
a user ID so what if I want to group by
the order amount by user level for that
I would have to join together the user
information like the give me the level
for a user for user ID and join that
together with the with the orders table
there only after the user ID and that's
something you can also do simply in
encase equal it's just a simple joint
command just like any other pretty much
like any other join you you you run a
secret right now is limited to joining
tables two streams and a table in case
equal has to have the concept of key so
you have to define a key and each key
can only have one element so it's a bit
different from a stream where you are
free to stream anything I hope that will
lift this eventually this other syntax
what looked like so I'm creating a table
and inside of case equal called users
from a Kafka topic or local health code
users type is a probe which means I
don't even have to specify which columns
this table has because it'll fetch this
from this f4 schema and then this is my
joint so I create a stream called orous
underscore users as select order stuff
and user city and user level from Oris
and i left join the web users should be
called web users up here on the user ID
so do that and i get my I can't carry
this and I can get my order volume per
um or user level alright and at this
time I'm pretty much done with with the
talk and we've got a few more slides
that I would probably go through and
then if you have any questions feel free
to ask questions otherwise so I probably
just dive into the into the demo and
just show you some of the commands as
they are yes
okay so yeah the question is when I do a
create table like like this and K sequel
what does it create and what's the other
thing that you can create so you can
create two different kinds of objects in
case equal there are either tables or
streams the difference is that you can
carry a table and it will return only
the latest event per key so think of
well think of think of the the users the
user has a user ID and if I just stream
data in there I will have like initially
able to do that one stream and if I do a
select star from this table I will get
what I just put in there but now assume
I modify user so I have a new event for
the same user key the same user ID if I
select from the table I will only get
the representation of that last state I
will only not get two rows back for
these two events they create and the
update I will only get the last row back
and the reason for that so why this is
important is because Kafka's immutable
you can never update data and kapha can
only truncate data that's all or you can
add new data how is this persisted both
streams and tables and K sequel are
persisted as topics again so as you do
your transforms they will actually all
become topics and K and Kafka which is
both good and bad the good thing is that
you can actually use these topics and
subscribe to them and look at them the
bad thing is if you do a multiple step
transform you get multiple topics which
could mean that for complicated
pipelines you have one order coming in
and then this one order has three or
four or five transforms on top of it and
then it might produce three or four
different topics which will just easily
inflate an amount of data you have to
process in Kafka good question yes
what changed to admit making database so
that depends on how you fare to data
from the database the the first method I
showed was using polling well use the
Catholic connect JDBC driver to pull
data bases to just always create a
database and say give me the new data so
for that one you wouldn't have to make
any changes as soon as you have as long
as you have an ID column a primary key
kind of thing you're fine if you do the
log based mining you have to make sure
your logs contain all the information
and in my sequel you have to make a
small change which is I can remove what
it is in Oracle you have to enable
what's called supplemental logging just
means that they have to adds a little
bit more information to the video stream
for the video stream to be possible and
to make sense other than that you don't
really have to make changes which is
also why I think that Kafka is a really
really cool tool because like I said
I've got many clients that have these
old systems that they don't want to
touch they don't want to make code
changes maybe they can't make code
change because the third-party
application they do not really want to
modify the database so in this case you
can either pull the data no changes made
or necessary or as long as you have
forced logging and supplemental logging
enabled and in this case in the Oracle
database you can mine the video stream
and you can just put it there and you
don't even have to close that stuff
inside the database so that's that's the
other really cool thing where you can
just attach things or do analytics on
your database without having to a query
it right so if you if you interested in
stream processing there's other other
things so I really like Apache beam and
data flow a lot and they've got some
work constructs where they have a lot
more in terms of how do you handle late
arriving data or when do you close a
window how long do you keep a window
open and they have something that KC
will doesn't have which is triggering
events what I said earlier every time a
new event comes in it will update this
this window every time which means that
if I have a thousand events per second
those means I update my window a
thousand times per second and write that
too in this case elasticsearch and then
in an Apache beam you can say only do
this once per minute and only why did
all processor two to this go to whatever
you write
- and then for late arriving data only
do it whenever it arrives so there's
that what do I like about Kafka and K
sequel it it combines storage and
processing in one platform so you only
have to have your one Kafka class sign
it's how you like to use Kafka anyway if
you need to ingest streaming data
sources it has the support for so many
connectors like I also love to work with
Google pops up for example but they they
have a very limited amount of connectors
and every time you want to put data into
it you have to write custom code and
then if you want to get started with
screen processing and see what's
possible and see what what you can do k
sequel is the simplest thing you can use
to explore especially if you're coming
from the DBA world you've probably
already you should if you come up with
DBA what you should already know sequel
and writing k sequel if you know sequel
is really simple
whereas writing spark or data flow or
Apache beam code a few of your DBA and
you they are not that great with Java
it's a bit of a task the cons are a it's
it's not really released yet it's still
beta and there's some some kinks in it
the joins are somewhat limited to two
simple things like what do you do when
you want to join a large table what do
you do if you need to enrich your data
like like this user example it's simple
if you have four hundred or a thousand
users but what if you have five million
users
I can probably not hold five million
users a memory but you kind of needed a
memory because you need to look this up
for every event that comes in and then
it has some limited math for ETL so
there's no case no no MDL no other
things you can always use Kafka streams
which again is Java Scala or a coat or
to enhance it and if you're running
stuff on the cloud you still run VMs if
you want Kafka most of the time today
there's a bunch of links I put the
slides later or I give the slides your
organized so you can get them there some
some things to get you started specially
with the case sequel and getting data
from relational databases into Kafka and
I think this this is my summary which is
databases can talk stream so if your
company is talking about streams and
putting together a streaming data hub
then you can put your hand up if your
DBA and say yep I kind of have an idea
for P how we can do databases to
streaming
stream to
sesang is I would say it's still very
young and not too many people are doing
it but I see a lot more requests for it
every every week customers are asking I
want to get analytics faster I want to
get intraday numbers for for sales and
for other things
IOT you don't want to wait a day to
process your data you want to have
insight into it very very quickly and
I'm really excited about K sequel
because it's just so simple and cool and
I would have three more minutes I could
shuttle around and try to get the demo
but I think that's what we answer
questions oh sure any other questions
I had a hard time hearing the question
please
speak up yeah I am miss the last bite
when you have multiple decide when you
have math with clusters of Kafka how do
you handle that well so the first thing
is what I said is when you do the
transforms you get merged with topics so
topic is just basically an
organizational structure to put messages
together so that's just topics you can
also set up multiple clusters so you can
also create multiple kafka clusters and
then the clusters can talk to each other
with the tool called mirror maker which
is basically one of these kafka
connectors connectors but you specify a
here's my source kapha cluster there's
my target kafka cluster and now please
replicate the messages across start
answer the question cool or just find me
afterwards I just had a hard time
hearing you yeah what's
yep fair enough so questions what are
the purposes of these clusters wise calf
cluster and that think that's for two
reasons it's for high availability and
for scalability so one thing is you want
to have multiple nodes that if one node
fails you can still consume and like
replicate messages the other reason is
it has its own local data store so Kafka
will store and persist its messages
locally on the cluster nodes so if you
want more storage what you'd have to do
is you'd have to add more nodes to your
craft cluster just to get more local
storage just like I would say in the
early days of Hadoop where you had to
store everything on HDFS in order to
grow HDFS you had to add nodes to your
cluster so that's the that's the purpose
of white Kafka runs in the class so my
demo obviously just runs in assuming a
docker image and it's just one single
single thing that you build these
clusters mostly for AJ and and
scalability and then also but the that's
the other neat thing is when Kafka was
first released it was really just a
message happen the distribution center
and you would really only use it for
that but all the code that you run also
runs inside of a casket cluster so the
simple case the my JDBC connector the
thing that connects your database and
reads and pulls data that will also run
in my Kafka cluster and they will make
sure that to run this this connector on
one or more nodes as needed and also
relocated if a node goes down and then
as soon as you get in the case sequel
you actually what doesn't need it will
actually create Kafka stream jobs and
these jobs also have to run somewhere
and they also run on the same Kafka
cluster which is I think kind of nice
because you combine the the storage and
the message distribution with the
processing whereas if you use something
like spark or data flow these are two
separate things you have a message
distributor which could be Kafka like
spark and would put work with Kafka you
would read messages from Kafka but then
you have to have your Kafka cluster for
storing messages and distributing them
and you have your spark cluster for
processing messages all right think yeah
one more
Oh
how do you measure the message
sequencing Kenny what yep yep
so the question is how do you deal with
message being out of sync and
replicators so one thing is Kafka is
probably not the best tool if
replication and replicating
state-to-state as your goal like if you
want to take an Oracle database and
replicate that to a sequel server
database there's probably other tools
that do this better than Kafka that's
probably the answer to that and then if
you do your aggregations they take care
of being in the right place if you just
add the transactions and what you keep
on the other end it's just a it's not a
table that has state but it's just a
table that has each event in it then you
also don't worry really much about all
that so one one way you could do is you
could take your source system store
event for event and row by row in the
destination table which would then kind
of make it upon the data set and then
once a day for example you could say
well now I'm pretty sure that all my
events have a life and then you can do
whatever you need to do with it to do it
in order I think it was one more
question and I think I got a kick out of
here
or that yeah fantastic I hope this was
useful should appreciate you guys coming
out I think now's lunch thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>