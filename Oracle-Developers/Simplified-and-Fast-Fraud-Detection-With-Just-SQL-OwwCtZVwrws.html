<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Simplified and Fast Fraud Detection With Just SQL | Coder Coacher - Coaching Coders</title><meta content="Simplified and Fast Fraud Detection With Just SQL - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Simplified and Fast Fraud Detection With Just SQL</b></h2><h5 class="post__date">2017-08-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/OwwCtZVwrws" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">we want to talk about how to do
something as complex as a fraud
detection in a set of transactional
records with just using sequel and said
that this is me rather new picture with
a little shoulder here um at Oracle but
this is probably where you will find me
and when you look at the internet so I'm
a soccer fanatic I'm also developer
dealing mostly with the database code on
that side and I'm happily into friends
and family and get together whenever I
have a chance and Cheryl is smiling here
because we were out last night um
finding patterns in your data when you
look into all the languages that are
actually providing something like
pattern recognition this there's quite a
number of languages out there and when I
was looking it up as a preparation for
this session I was actually disappointed
that I didn't see sequel in this list
because we have to fix it
sequel has indeed pattern recognition
technologies are raw petal technologies
which is there I think since the version
9 which is actually the capability to
find a petal within a string where you
would find for example whether someone
has written my name with one N or two
ends or whether Isis spelled my name
before no to ours these kind of petal
recognitions within a string what is
kind of new in terms of the capabilities
in the database and you see I put new in
quotes because new in a sense means
since 2013
if the capability literally to detect
pattern within a string but it's a
technology that detects patterns in a
stream of events where the stream of
events is actually a stream of rows or
transactional data mostly transactional
data when I don't get the customers that
we working with that are using this
technology the time is playing very
often a role here and when I met this -
the pattern recognition in this stream
of events
it is analysis of patterns of attributes
of states across a set of rows so it's
not about finding okay it's the name
home and written with one or two ends
within a string but it is about finding
a pattern how did the state of specific
things change over time or over the
order of my stream of events it's an
internal pattern recognition which is
from a technology perspective very
exciting and helps us solving very
complex problems so when we in
development approach this and we're
looking into building this we basically
had a simple goal we want to provide
native capabilities within sequel there
that here and takes over some of the
technologies and language constructs
that were known before and provide a
framework to really make the usage of
this new functionality very intuitive
and simple and as you will see later
right if I can do it everybody of you
can do it there's a reason why I ended
up in product management I didn't stay
on the hard core development side so
what we did is we introduced a new
sequel language construct called met
recognized which is since two weeks
three weeks traveled roughly two or
three weeks now officially part of the
brand-new NZ sequel standard the NZ
sequel 2016 it only took us about six
years to get it in so it's standard and
when I look at the intuitive process we
are looking into how do I approach to
find such a pattern and there's actually
three simple things right first of all I
have to define somehow what is my stream
of events how do i distinguish all this
this this data that is coming from to
identify what belongs into one stream
bucket or not is it is it that the
stream of events as we will see when we
get into the fraud detection that I just
want to see the transactions that have
on a specific account on a specific user
ID or is it is it a specific product
line a specific region where I want to
track how the state is changing or a
specific machine where the temperature
we have this is one of our manufacturing
customers in the in the island business
where the temperature is changing on a
per machine base so the stream of events
in that case is defined as change of
temperature of a machine then it's
obviously you have to define the pattern
what am I looking for
what is the pattern that I'm seeing is
the color changing from red to green to
yellow and as it happened five times in
a row before it identifies the pattern
that I'm looking after and last but not
least well if I found a pattern what
information do I want to collect why do
I want to know every detail state that
happened within the Paris or do I just
want to have a summary when did the
pattern start when did it end like a
stock price right you want to track when
a stock price shows a V shape or a W
shape am i then interested in window
shapes started and when it ended or do I
want to have each and every individual
data point within the pattern so these
are basically the the logical intuitive
concepts that we have to think about
before we actually can translate this
into what I always call baby English
also known as sequel come on now we have
solved the hardware problems now we're
going over to software problems come on
next slide please my machine has decided
to take a time out um but we're now
looking into exactly that with a simple
fraud case and then it's actually coming
out of reality I will show you the code
running life later on using our life
sequel framework which gives you the
capability to actually write sequel
against an Oracle to f2 database I've
also backup I haven't installed it in my
docker file here on the system
but for you we will go through that in
life sequel and everybody of you can
actually start playing with it later on
and we simplified this this is a real
use case we simplified it a little bit
to make it easier to follow the concepts
and the paradigms of how the pattern
matching works by saying okay suspicious
money transfer money laundering in our
simple example is defined by finding
three small transactions that a small
means less than $2,000 within 30 days
it's kind of to probe whether this new
account you know raises some flags
somewhere in the bank or the security or
the audit and when this is fine there's
a big transaction one big transaction
following within ten days after the last
small transaction and then run forest
run so this is the pattern that we are
looking for and what we want to see we
pull it from our analysis is we want to
see when did the pattern start when did
the pattern end and what was the amount
of the large transfer and we're now
going to step through a simple example
using ten records or so the data in that
case comes from a transactional log in a
JSON format which is a format that the
database natively speaks by its built-in
schema on read capability so what we
going to do is this we start off with
this Jason log that we have in the
database and then we just basically put
the structure on it with with sequel
constructs and then we work against it
as if it was a table so this is the
sample data that data said that we're
having and if I give you enough time you
will actually spot that pattern in that
case and wouldn't even eat simple but
it's a very good thing you know to
actually demonstrate it so I also
limited to only one account you see my
friend John here there's three small
transactions between the second of
January and the 20th of January and then
he does the big transfer of a million I
wish I would have
my bank account transfers it on the 27th
which is within the ten days of the last
small transfer okay I said at the
beginning we have a brand new sequel
language construct which is the match
recognized which obviously needs some
input to to define what we are looking
for and we're now looking at these three
simple concepts that I that I said
before the first thing is okay how do i
define the stream of records that
represents the events that I'm focusing
on and in that case is pretty simple for
everybody who has account an account in
this transactional log I want to order
all the transactions that have happened
by time just Express with the partition
by user ID order by time ID and you see
partition bar is a clause that is in
analytical sequel since since version 8i
in the analytical function so this
conceptually we try to reuse and and
adopt technologies that are using
already familiar with so now after we
have defined our stream of events the
next thing is we have to define our
pattern right and pedalo express using
posix-compliant reg apps like eps
regular expressions easier than
short-arm where you just give the
pattern a variable name in that case I
picked X I'm a lazy guy I could have
also called it small transactions but
you will see when you start typing small
transactions becomes a very long low
world at some point in time so X is a
little easier um we said also X with the
syntax 3 comma open-ended which is the
regular expression for at least three
occurrences or as many as they are
within our pattern we said we want to
have at least three of the small account
small transfers plus one big transfer
which in that case became you know you
could have probably guessed it the
variable name
why that's the panel that we're looking
for very simple very straightforward
the next step is obviously and can you
see it this is just a sample syntax
refer to what you have available here
you know you can find exactly one event
you can find one or many events zero or
more matches with the star and and so
forth we were using the bracket open
bracket ten comma and bracket four n or
more matches in a case the next step is
how do i define these variables these
events within the data set that I'm
having and what you're seeing here is is
pretty straightforward sequel where you
say okay the variable X that means the
small transfer is defined by an amount
less than $2,000 and within the series
of the events of X the fact that the
last of this event is less than 30 days
away from the first event that's all
you're doing right so if you had the
three transactions that we saw before
and the third transaction would be a
month after the second it wouldn't
qualify for that pattern and in the same
style y is simply defined as the amount
larger or equal than 1 million and and
you see here we have a cross variable
reference that the time when this occurs
is no further away than 10 days from the
last occurrence from X because we can
have other transactions within the panel
right so our friend could do other
things in the meantime so this is how I
define the pattern and how I find the
events within the pattern that I'm
looking looking after and now coming to
what do I want to see reported this is a
new class within the sequel statement
which is called measures where I said in
that case I want to see the first of the
X events the small transfer and I want
to see the last of the Y events the last
transfer the information within the
measures could be specific data points
like you have seen it here or it could
be aggregations
or cumulative aggregations fine
aggregations and so forth so those are
the three measures and what you're
seeing here is was implicitly already
set is that I'm not by the nature of the
measures that I'm not interested in
every single data point here right so
I'm just interested in basically the
summary of the pattern of what has
happened so when I get a hundred little
transactions and one big one I will get
back one row per match if I have three
the minimum criteria and whatnot I will
get one row back so this is what I'm
saying here in industry tax and this is
the whole statement that you need to do
in a fraud detection just to give you an
idea and we will actually do some of
these other things um you can also ask
Oracle to give you back all the detailed
information about the pattern which
would be almost kamesh or something
where you want to see not only the data
records that have matched but also the
data records that have not matched to
see for example how many transactions
did he between two events of this
suspicious pattern right so there's
information in there you also see
normally people starting off with thee
with unmet row just from a debugging
perspective and we will briefly look
into that as well arm because it petals
can become very complicated and it's
very easy to to oversee specific edge
cases or corner cases so having the
capability of of doing some kind of
debugging if the functionality is very
handy
what we will see when we run Elife we
also have built-in measures that help
you to analyze the system or to get even
more information about it which are
namely identify which of the events a
record falls into is it a small transfer
or a large transfer which is probably in
that cyclic a is easy to see that
there's more complicated parents and you
know how many matches did I have in my
data set it's also powerful
sense that when you found a pattern you
define where does the next analysis of
the next pattern starts you can think of
you if I'm looking for a W shape in in
let's say a stock price change right if
I have three of the V's next to each
other so now it's my D
my decision is this one W and one V or
is it two overlapping w's right so you
have to have the capability to say where
do I want to skip to if a pattern is
satisfied and where do I want to start
the next investigation okay and finally
are you define what is the output what
we're seeing here is we use the
aggregated aggregated version the one
row permit meaning that at that point in
time I can only get basically the
measure of spec and the information that
I have in the partition by clause
because that's a unique value right so
in with the one row per match we are
losing the detail information and you
would not see that the initial data
points for each individual record within
the pattern okay you probably have
recognized me right away by my nice
glasses let's now actually look on how
they looks in the real world so I'm here
on life sequel and I was just looking
for match recognized and we actually set
it up specifically for the Oracle code
event and that's a framework which gives
you the capability to actually run live
sequel so a select star from input give
me a 942 table of you does not exist and
if I were to go after the take into
indexes and what you see here on the
right hand side is basically a written
version of what I'm just telling you
probably in a better English let's not
just create a table which basically only
has one column transaction document
which is a silicon can you see is that
big enough in the back perfect oh we
just create that and copy it over we
create a table and insert some of the
dummy data okay if I'm now going and
just select from the table
Jason transaction okay you will see it's
just a whole bunch of JSON documents
it's from a database perspective at that
point in time it's a lot right you can
put in whatever strings and whatever you
want um normally in that case actually
we we arm we restricted you a little bit
because we have a check constraint where
we say okay when data is inserted we
check that it is a valid JSON document
but apart conceptually a lot normally
can get whatever you want right so if
I'm now using the built-in capabilities
for achill to look at the data in a
structured form basically the schema and
wheat which is defined by okay you start
with the alias of the table the table
and then at the column and then
basically the chase and value within the
change in document you will see all of a
sudden that the data is coming back in a
structured form so now I'm blaming the
internet or the ball one of them
okay we could okay very my okay dunker
stats up just to have a fallback you're
coming back oh yeah I'm I'm violet
that's fine thank you
okay but we can start start it here as
well okay let's do it okay we inserted
the data as you can see I have a
fallback we saw the change in
transaction and this is how it would
look like when you use the building
chasing capability now looks and smells
and behaves like a table returning just
the trustee attributes in the image at
the document that you that you wanted to
look at okay you also saw what I did it
the next thing is I'm just filtering the
data so you saw here in the initial file
we have transfers withdrawals episodes
and so forth
the moment you transfer it into
basically relational format that syntax
I can filter on it and from a fault
detection we already interested in a
transfer so by adding the filter
predicate on event ID only being
transfers you see the data set that we
had on on the PowerPoint and when I ran
the sequel that is the statement that we
showed you before you see we have a
partitioning by the user ID or by time
IDE we have the one row per match we
have our x and y defined as if you saw
it on the slide with the small
transaction and the large transaction
where needed at least three of them and
as a return you get back my friend John
had a suspicious money activity starting
on January 2nd and the pattern ended on
January 27th with a transfer of a
million to
now I know no account at that point in
time right because we didn't get the
data within the the chase and
information it was provided to us for
analysis
um this is the same statement where
Eddie modified it a little bit starting
with I want to have all rows with
unmatched rows meaning I'm getting out a
detailed information across my whole
data set and what I also added is what I
what I briefly mentioned on I added the
match number and the classifier to give
me an understanding of where am I
within my pattern and you see here there
was a transaction somewhere in September
yeah then my friend John didn't do
anything until he got active again in
January second with a small transfer and
then it's it's basically an internal
state machine what we're building here
and stepping through and you can see on
the training F we reached the critical
point of three little transactions at
that point in time we said who X and
satisfied we look at the next variable
general we are having greedy variable so
we would try to find more XS but if we
don't find an X any more and we have
satisfied the first part of the pattern
you say oh it's time to look for a Y and
if you can see on the train is where is
the 27th we then have the large
transaction and what happens later on
smaller transactions nothing we stopped
that was probably the time you run or
the time it was put in jail you also see
this is the detailed information of the
record and from a measures perspective
that is the information that we got back
in the first go-round the moment a
pattern is found the first time is set
right the first time is fixed it is the
second of January and the moment you
find the last transaction the other two
measures are our satisfied so these are
the measures that we are looking at
right
so the first X when I started my pattern
the last why baby when did the last when
did the big transfer happen and what was
the amount of the white with the only
happens once so you see the information
plus the match number and the classifier
which part of the pattern does the data
record belong to you see that all you
have the whole information of how the
data was analyzed by Oracle and how the
pattern was found okay so you see me
monkeying around with some code um now
as is always in real life and disposable
security there's always new requirements
coming up right so we have the same case
here with our petal matching where we
all of a sudden you know the system that
actually feeds us the information got
improved it also has the information
about the target account in the
information and we're refining our fraud
detection analysis example to a point
where we say okay
history evidence shows that it's
fraudulent if the little transactions
are just scattered around different
accounts and the total sum of the small
transfers is less than 20k so we get in
which data set and we have a changed
business requirement here for for the
data and when we look at the new data
set with the additional information of
the transfers and a new requirement that
is less than 20k you will see that we
find the pattern again we have the three
transfers from John to three different
accounts and the big transfer is still
within the ten days so the pattern that
we had before is satisfying our new
requirement so how does it look like in
sequel it's pretty simple it's the same
pattern that we looked before with
additional conditions so all we have to
do is basically to it to address this
new requirement is
there's a new requirement to define a
data record as X as the smaller
transaction now within the context of
the X itself where we say okay so every
time I'm doing the small transfer it
does not go to the same account as it
went before right it's very easy to
identify by self referencing within the
X to the previous account that the money
was transferred for and the other
requirement was that whenever a large
transaction happens I'm
cross-referencing the other variable
thing okay the sum of all the X events
is less than 20k so this is this plus
all that if necessary to to get there
done and let's now see there life let's
see who it came back okay okay do I
still let me just quickly see whether it
works just do the same where it was
three okay we had that you had the all
matches per record okay the changing
requirements it's now insert truncate
the table that we use for the example
before and insert the new additional
change in information okay
now the interesting thing is if I were
to go back so this is different data now
right if I go back and run the original
one that's not the original analysis
guess what it will still work because it
will still work because we don't care
about the additional information to
change the look we don't make it a part
of our schema in the statement that we
had here before so we have the full
flexibility here by by using this
new kind of data processing okay we look
at the new data as you can guess we have
pretty much it looks almost like the
thing as before just with the transfer
information now and when we look at the
enhanced business rule you can see here
we added the additional previous test
and that the sum is not there you will
see that coming
let's see that the information is found
so from a parent perspective we have
quickly adjusted and adopted a new
environment um
we could do the extraction of additional
information and what what what we doing
here is it's a little bit of an edge
case what I'm doing here you can
actually refer to values within a
pattern as well so things we know in
that case that we're getting three three
data records back you see that we are
kind of flattening the data with amount
one and transfer one amount to transfer
two and a transfer three we get more
information and that is the point where
you actually really can go high wide let
me just Foley I could change this to all
rows per match should simply work you
see now you have the information about
that is the that is the aggregate
information like you say the first time
you see the transfer one transfer to and
you see how it fills up um you could do
a just remove all of that this let's do
a sum of the I wanna see the total
okay I'm sure I'm making some mistakes
in the syntax but your friendly sequel
will tell you in a second so this is a
life running database in the background
where you can go and enjoy yourself
amount what do I have the cumulative
amount let's take the transfer idea that
is that interesting let's see how I got
it right
oops I got it right you see now I added
the cumulative information and
interestingly you see I did a cumulative
sum for X only that means it states the
cumulative sum amount of X also for the
last record to Y I were to say okay I
want to see all the money that was moved
it's as simple as removing two
characters and you see that pops up to
the 1 million 3700 and this is when you
look at it what we can do in sequel
using Oracle as a data processing engine
at that point we don't even care about
the data management because that is
something that we're you know looking
the way of how the database or the
framework evolves the database is only
one part of the data store it can be
accessed and closest by the database so
I could have actually run against this
Jason lock being stored in an external
table being stored in a no sequel system
and just accessing it and using the
sequel data processing framework for the
analysis that I have shown you so it's
just the tip of the iceberg because if
you combine it with other capabilities
that are within this framework whether
this is your to your spatial information
that we're having in there it might be
interested in knowing ok where are the
accounts where the money is going to
which may or may not be information that
is readily available as an attribute
within the Chase and information may be
you oh yes Q spatial code and you have
to analyze which country is it in and so
forth you get a lot of Oh
power of what you can do with with
sequel as a development language for
detecting these kind of problems or you
don't want to know what we're doing with
some of our customers that are close to
our current location where it's not
necessarily the movement of money but
the movement of people are that there's
a lot of analysis that actually happens
in data that is collected in in these
kind of systems we also in a notice that
is actually very old thing we have Java
code available that does exactly the
same in MapReduce what we're doing it's
equal on a larger data set arm and when
you compare the lines of code that we
had pretty much what you've seen in this
example versus what we have in Java and
the runtime you're getting oh it's kind
of handy using sequel as a pre-built
data processing anguish a engine with
the building capabilities compared to
actually write everything by yourself so
the summary I think all the gray
checkmarks forget it
I think the big summary here is I would
all encourage all of you you know to go
back use Life sequel and just start
playing with it to see how you can
actually leverage this functionality to
your advantage and let me tell you out
of personal experience right it's it
comes in very handy the last problem
that I saw of this I wanted to
understand how our extensive in the data
base co-located physically co-located
and which extends our bed basically
adjacent to each other what is the
average size and if you try to do that
with a normal sequel it it becomes
complicated with pattern matching it's
big you say oh yeah n block and row ID
address is that why the address of the
next record sorted by row ID done so it
very simply comes very handy for a lot
of use cases that you probably cannot
even think of today and I just encourage
you to start playing with it we've also
as you seen on the slide before a lot of
tutorials and examples
or four specifically this this powerful
capability on life sequel and before I
get into the QA if you want to get more
information besides playing with life
sequel there's a lot of information on
the on the home page we have an
analytics block where we talk
specifically out about that part of the
functionality quite often or just drop
me a line and at that point there's no
more life demo unless you really want to
oh absolutely we will upload this like
kick back forth back forth back forth we
will also upload the slides and it will
be all available for you guys as well
that's a question for you Joe you will
see the presentation with the monkeys
okay everybody has taken a picture I
would have expected you guys to be more
interested in seeing the monkey than
that but okay I have probably a couple
of minutes left for Q&amp;amp;A no more Handy's
up in the air okay so I know it doesn't
proceed any questions from your side so
okay million dollars average yes because
like specific amount
there you can do it and I had it
actually and implicitly when when we
refined the requirement where we were
going after the some of the previous
event right so it's and or when I
quickly typed it in I use the cumulative
amount so you have both your final
amount and you have cumulative
information that you can use for for the
processing as well it depends on where
in the state of the analysis you are
right so you cannot ever the final sum
of X before X is final point or you
cannot be referent
a variable that hasn't happened yet so
you could not say in the in the X refer
to something that happens in the Y you
would have to have a different event
that happens before did you actually get
that state within your pattern but the
short answer is yes lasers uh-oh
thank you since we are to a serious one
yes it's also in release two yeah we
returned to not throw things away with
new releases yeah yeah absolutely
I just wanted to sneak in chase and
capabilities and dis presentation since
we didn't get a specific slot for it so
i was sneaky here but no it did you
don't need it right at the moment when i
use that place and syntax it looks and
smells and behaves like a green a tional
structure so joins essentially they join
all that X in that X expression where
you might join to address or you might
join the code you would have to you
would have to basically flatten the
event record as the input value where
you saw when I had the approach yes yes
you turn up front you basically in that
block you define you define what is my
streams of record yes yes yes yes it's
also no from up from a usability
perspective it's easier to write and to
understand what you're doing it works a
lock or two as well but you have the
limit of 32,000 characters beginning
with 12 for any further questions and if
you remember any for others a half
question yes yes well the geographic
information that I had on the slide was
you know you can have geographic
information and its flattened in a
relational structure like you said an
egg an address but it's also very often
you know that you want to go for
distances or so how far did someone
travel for example and so forth there is
let's see that's a use case where the
spatial capabilities in the database
come in handy which can be expressed
with functions and basically translate
it into something like distances or
locations as well this is what I was
referring to right obviously whatever
you have in a relational structure is is
there but when I look at it not so
relational structures and geospatial
information being one of them since
Oracle can transparently deal with this
kind of information in sequel you can
transparently leverage it within this
framework as well blazingly fast no it's
actually that's a good question and
actually I could talk probably half an
hour about that um it depends it's hot
it's you know it's that is one of the
things how far the relationship are
apart and then it depends on on whether
you end up from from a pattern
definition whether you have a finite
state machine that you get to or whether
you have to traverse back 120 thousand
times in the state machine because that
adds you know if you want to go for so
it's fast it's faster than what about
what what capable Java developers could
do with MapReduce how about that and if
you find something that is not so fast
please let us know and we tell you any
other we can make it faster or that we
are in the state where we had to
traverse back in the state machine which
is and isn't pretty yes and then it
depends on how far you have to traverse
a
any further questions if not then I
thank you for your attention for coming
here and if you have anything else drop
you note thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>