<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Looking into the JVM Crystal Ball | Coder Coacher - Coaching Coders</title><meta content="Looking into the JVM Crystal Ball - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Looking into the JVM Crystal Ball</b></h2><h5 class="post__date">2015-06-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/GYh173TzIug" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so welcome to Java one and welcome to
this session
I'm Michael vid stead I work as JVM
architect at Oracle my background is in
pretty much JVMs when I graduated from
the Royal Institute in Stockholm 13
years ago ich where I wanted to graduate
I was looking for work
and I was looking for a master's thesis
and one of the coolest things that was
going on in Stockholm at the time was
this small startup thing called appeal
of virtual machines they were competing
with IBM son others on the JVM side on
and specifically on the server side so
did Jay rocket JVM or the de rocket JDK
was where the action was I was set up at
the time so basically what they wanted
me to do was to port you know I was in
my master's thesis work I was supposed
to compare register allocation
algorithms in our JIT compiler on two
different platforms x86 and SPARC and
j-rock II didn't exist on spark at the
time so the first thing I had to do was
to port
GI rocky to spark and that took like two
years but then finally oddly at least I
graduated and Here I am many years later
I've been through five or six years of
platform porting efforts on J rocket so
every you know made your operating
system every single or CPU architecture
there is I guess and a number of years
working on a small product called J
rocket virtual edition was which was
essentially extending the j-rok JVM with
just enough functionality to run
directly on a hypervisor without a
general-purpose OS in it and then three
years ago now almost I took on the role
I have now which is JVM architect and
I'm trying to coordinate the technical
vision on the JVM side and that's what
I'm here to talk about today and this is
a very long way of saying everything I'm
about to say is a lie and here's the
agenda for today I'm gonna say something
about what wiII be enough to lately on
the DVM side and I'm gonna cover some of
the more future stuff we're looking at
how you know looking at this as in
working on or we're looking at it in
terms of vision and where we want
two head and then there will be
questions at the end obviously so again
a few years ago not only did I take on
the role I have now but we were also in
the face of acquiring Sun and trying to
figure out what to do with all the JVM
so we ended up with I think when we when
the acquisition happened we counted the
number of JB m/s we ended up with and I
think we came to something in the order
of seven and obviously it's you know
it's good to have a big variety of stuff
but you know seven JVMs felt like it was
at least two or three too many so very
early on we decided that you know we had
Jay rocket coming from the DEA
acquisition Oracle did a few years
earlier and we had the hotspot JVM that
came in with the Sun acquisition and
what we realized was that a very very
large part of those jaebeum's were
effectively the same functionality the
same kind of basic no value adds just
scaffolding to run java and so having
two implementations of that didn't
really feel like a good idea so we
started combining them and the way we
did that was to look at what the pros
and cons or strengthen strengths or
and/or weaknesses were of the different
JVMs we came up with the list you know
there are already good parts about a
Rocketeer or the bad part good parts
about a hotspot and when we compared
lists we realized that it's actually a
wash like there are equally many things
that are good with a rocket as there are
with hot spot and ultimately we decided
that what we're going to do is to use
the hot spot codebase as the basis for
our new future JVM and then we'll
cherry-pick features from the a rocket
and port them over to hotspot and we
started doing that pretty much
immediately the key functionality I'm
gonna say if we just like bucket things
up that we took from day rocket were
serviceability type features and I'll
mention a few of them in a few later
slides here but one of the key ones was
java flight recorder so just like as a
preview to what I'm gonna say later it's
a an event-based
profiling and tracing framework but but
in essence it was
serviceability and/or manageability
observability type features from the Jay
rocket side but that was not the only
convergence we did at the time the other
type of JVM we had we actually had two
other types or implementations of JVMs
one was the hotspot embedded version
which essentially was the same codes but
a hotspot or have started as that and
then slowly drifted into something else
and we also had a CDC implementation for
ME and what we decided was to take both
of these and again merge them into the
same hotspot code base and have hotspot
scale all the way down from these
relatively small devices with tens of
megabytes of memory all the way up to
server and enterprise with gigabytes of
memory so from the embedded side we took
a lot of scalability type functionality
so making sure that we reduce the
footprint that we again scaled down both
in terms of static footprint on this but
also dynamic footprint in terms of
memory usage and again we want the same
JVM to span approximately tens of
megabytes or ten megabytes for the JDK
all the way up to hundreds and you know
hundreds of gigabytes I guess when we
look at memory so that's that's been I'm
gonna say the theme we have been working
on over the last few years and the next
few slides will cover some of that so
last year when I had pretty much this
presentation my mental thinking was and
this is a fake quote so I didn't need to
get any approvals for having it in my
presentation my mental thinking was that
you know the functionality is there
we're about to release this product or
you know this functionality so it's kind
of a dumb deal convergence is over the
key features again like the fright
flight recorder and Mission Control
functionality from J rocket as well as
the permian removal for example that
also kind of fade in there they're all
done you know yes we're stabilizing them
but we'll be releasing that version a
few months later and everything is cool
in foody right well turns out that
reality isn't always on your side so
what we have spent the last year working
on and I'm sure most of you here are
aware of is security
security is a key priority for us is the
highest priority will you know move
other things will push releases to work
on security and that's one of the
biggest things we have been working on
for the last year or so you know there
are some taking a few snapshots from web
pages on the internet here but
essentially this all led up to the fact
that we now have a separate security
track here at JA one and we'll be
continuing to work on security is always
important for us so that's kind of what
you saw in the news and all that and I'm
gonna do a small small detour here just
to get a feeling give you a feeling for
what it is what it's been inside of
Oracle or working with the products and
the releases so I was in the release
team for our next seven update release
so in August last year we released 7
update 6 and the idea again my mental
thinking last year was that the 7 update
8 which was supposed to include all this
functionality was basically December
last year or so 7u8 was the the name or
the number on that release and another
thing to know here if for those who
don't know it is that we used 7 is
obviously reflecting the major version
of Java the 8 in this case is the minor
version and we use even numbers for
effectively we feature Alisa's and odd
numbers for security releases the
problem we had obviously at some point
was that somebody needed to release a
CPU a security release in this time
frame and obviously they couldn't use
7u8
so they wanted to release 7 u9 ok no big
deal like we planned it for it to be 7u8
but it's 10 that's fine
guess what happened then somebody wanted
to you know release so yes not 10 we'll
call it 12 instead and at some point
this gets starts getting confusing if
you work with people let's say inside of
Oracle you have fixed a bug in what you
said to them was
new 12 and then somebody wants to do a
CPU and we push that functionality into
what was now 7u 14 they expect that to
be in the 7u 13 CPU obviously like you
know 12 13 comes after 12 and we always
make sure that we have feature
compatibility and all that it's not like
we were addressing on that side so if
you told them that it was in 7u 12 and
then secretly changed in version number
to 14
they expected again to be in 7 to 13 in
the middle there okay
and obviously 7u 14 worse was not where
it stopped we finally came to the
conclusion that we can't use a specific
number for now let's call it X and
meanwhile we have for a long time that
planning to make this version schemes
like more flexible so what somebody did
you know the problem is we only have two
digits to play around with or two
numbers to play around with that we want
to change but for now as many of you
have probably seen what we have done is
to come up with kind of a a band-aid for
now and the way that works is that it's
now called 7u 40 in essence what we did
was to provide some spacing in between
these releases so that we don't have to
renumber them all the time and there's
I'm sure a webpage where you can find
exactly the information on how we choose
which number would to have them on this
specific release but that was a big and
it's likely a kind of a detour on what
we have been up to the last year or so
and I'm happy to say that the 10th of
September like two weeks ago we finally
released what was originally called I
guess seven or eight with a lot of these
convergence features in it so a lot of
work has gone into this and the next few
slides will again cover some of the key
functionality pieces on the JVM side so
I'm gonna say that the key again feature
from the J rocket side was flight
recorder and flight recorder was
something we started working on many
many years ago the the way it kind of
works is that you sit close your
sustainability people and they tell you
that the customer ran into some weird
issue somewhere
and you want to help them but you don't
have enough information to do that and
what we decided at some point was that
you know let's start keeping track of
all kinds of information inside of the
VM we already are to some extent so in
order to figure out what to you know how
to tune the garbage collector for the
next cycle or which methods to compile
we already keep a lot of this
information internally so why not expose
that so that we can use it ourselves and
very quickly that led to the conclusion
that if we want it chances are others
will want that same information as well
so what flight recorder is is again an
event-based tracing and profiling
framework it works by you know we have
instrumented the JVM with a lot of
probes where we collect the information
this information is fed into a an event
stream and this goes both for VM JVM
internal events but also for events and
operations that are performed on the jdk
level so io events for example and
internally this is you can think of it
as stored in a cyclic buffer it slightly
more complex than that but in essence
it's a cyclic buffer so you know over
time we keep these events and that's the
old ones get too old and we need to
reuse that that memory we do so and but
you always have kind of a sliding window
of what happened over than the last X
seconds or whatever it is and that's
configurable and the other thing you can
do is to store this data to disk so you
can keep a log of you know this over a
very long time if you want to so the way
is designed is so that it can be on and
you actually this is how we had it in a
rocket it's by default on it's using
some more memory but not an awful lot in
the default settings and again this the
very design of JJ rocket you're sorry
it's actually Java flight recorder now
used to be J rocket flight recorder but
same abbreviation I guess JFR it's
designed to have very very low overhead
so most of the of you who are turning
this on won't notice the performance
overhead it's in the order of represents
or numbers tells tell me or people tell
me that it's 2 to 3% but that's the
design behind it so we
make sure that we don't collect too many
events to make it a problem so this is
one part of flight recorder the other
important part of it is obviously you
know you end up with this event stream
but what do I do with that
so j-rock admission control is a
graphical user interface that does among
other things visualization of the flight
recorded data but it does more than that
as I pointed out so the way this started
out was in the very same way we wanted
internally to know what the JVM was up
to we needed that information to tune
the garbage collector the compiler all
of those things and what Mission Control
does is actually it uses all the
information that I said we're keeping
internally to make use of ourselves
Mission Control goes into the JVM and
extracts that and just visualizes it so
again it's a very lightweight way of
getting very very detailed information
because it cooperates with the JVM and
this is again something that is meant to
be used in production so if you have a
production JVM running somewhere you the
very design of Mission Control is to be
able to connect to that JVM and don't
worry about any overhead it will inflict
on the process it just works still and
the the picture on the right here is a
an example of what it looks like to look
at JFR data I'm not gonna go into
details but the kind of the horizontal
bars are threads and all the the stripes
on it are events so you can go into that
two men get very detailed information
and so on in 70/40 Mission Control is
now included
so normally you you would have to go to
a special place and download Mission
Control but if you take the JDK that's
1740 JDK you will get Mission Control as
part of that and it will be the launcher
is called JMC and it's in your bin
directory so I encourage you to go go
play around with Mission Control right
so those were convergence features from
the J rocket side in addition to that we
have worked
on a number of other features in the JVM
one of the key ones one of the biggest
projects was to I'm gonna say pretty
much completely rewrite the
invokedynamic implementation so for
those of you who don't know
invokedynamic is the first byte code
that has been introduced in the java for
it into JVM spec or Java since its
creation and it was introduced in Java 7
and our implementation the one we had in
the JDK seven major release or the GA
release was relatively hard to both
understand port maintained all of that
because a lot of it was implemented in
pretty low level assembly and as
powerful as that is in many cases it's
not exactly how we want it to be so it's
been rewritten and now actually the vast
majority of the implementation is in
Java instead and what we believe with
that is first of all there's obviously
much easier to understand the reason
about because Java normally is easier to
read and reason about but the other
thing is that we actually believe that
it's much it's a much better basis for
our future so if we want to improve on
this if we want to add functionality to
it if we want to improve on the
performance this is much better
implementation to work with and speaking
of performance one of the things that we
know is that we need to work on
performance in this implementation
there's yeah if forward did anybody here
attempt the JVM language summit one
person that ok good charlie
we there were lots of discussions on
this it's something we know again this
this implementation is relatively new it
needs a lot of tuning and improvements
but so we acknowledge that no no secret
and then another thing we did was to
continue our work on improving our g1
garbage collectors so g1 is short for
garbage first
it's our future let's say garbage
collection framework and I'll cover more
details in a few slides but as you know
being in you garbage collector we
realize that there's a lot of work
left until we have something that works
as well as all the other collectors but
one of the things and I'll come back to
this later as well is that we have too
much garbage collection code too many
versions of garbage collection right now
and we need to look at cutting down on
that and he won is is going to be the
framework for that going forward a
couple of things first of all the first
one here bug is essentially just you
know retuning some default values
internally so based on feedback we got
from customers from users inside of
Oracle we realized that a few of the
settings were in top tamal and we just
you know retune them and improve the
performance quite a lot and the other
one is you know essentially a heuristic
problem we you know the check was
supposed to happen early on in a garbage
collection cycle but it happened late
and therefore we skipped one cycle and
they're actually just both examples of
relatively easy things we found that
made a huge difference and I expect that
we will be finding a few more of those
going forward but a lot of good work has
happened on the g1 side in 1740 as well
so another thing that I'm both happy to
say and somewhat sad at the same time is
we have bumped up the size of our string
in turntable or it's likely called we
have more than just in turn strings
there's let's say but it's the string
table in in hotspot and you can think of
it as the hash table where we keep the
string dot in turn strings so if you
call string in turn we need to keep one
and only one copy of that string and
make sure that every single time you
call it with foo you get the same string
object again this is a hash table
internally and it was implemented as a
relatively small hash table so it had by
default a 1000 a 9 entry bucket array as
a data point I can say that for some of
the bigger fusion applications we have
at Oracle it's not at all uncommon to
see two or three million strings and
what that means is that your 1009 m trie
bucket hash array table turns
- basically a linked list so as a
short-term workaround for this we bumped
that number up in this case to sixty
thousand thirteen because that was a
nice round number I guess but the
long-term story is obviously different
we need to make sure that this is
dynamically growing table on the
embedded side for example even one
thousand nine is wasting space if you
don't have that many strings on the
server and a price side with two million
strings I gonna bet this number is
slightly too small so that's something
we'll be looking at going forward as
well and then just to finish off seven
you forty from a JVM perspective we
fixed in the order of 300 plus bugs and
so again it's been a relatively long
development cycle but 300 is still a
relatively big number I'm gonna say so
um there's been a lot of fixes going
into to hot spot and that's hospital
loan remember that and that kind of sums
up seven new 40 and now we're moving
into what we've done for JDK 8 so in JDK
7 still we had have the concept of a
permanent generation in hotspot and you
can you know basically again I guess the
picture is trying to depict the fact
that we have split effectively the
memory into three different buckets one
is the Java heap where all your normal
objects and up one is the permanent
generation where class metadata is
stored this would be anything that we
you know a class object or a method
object things that we need internally to
kind of represent the the types and the
Java metadata needed to run your job
application and then we have native
memory where I'm gonna say like the rest
of the things are stored anything from
generated code to something we allocate
internally in the JVM
problem with the permanent generation is
that you can set some parameters and you
know configure it at startup but it's
actually relatively hard and we found
this with especially with relatively big
applications it's extremely hard I'm
gonna say to tune this correctly
and once you have tuned that there's no
retuning it so to say during runtime so
if you set it to be too small and then
you load an awful lot of classes you'll
get a job out of memory exception or
error so what we did and this isn't
again on the convergence theme is to
just simply remove the permanent
generation so permanent the permian is
no more I'm happy to to say in Java 8
and the data we had in permanent the
permanent generation have has moved the
idea to the Java heap or to the native
memory side of things and at least now
we only have one thing you need to tune
and that's the size of the Java heap
there are a couple of options that were
related to this the xx perm size and the
xx max perm size parameters they they
are no more if you use them on JDK 8 you
will get a warning and in JDK 9 they
will be gone so start removing them now
I guess and all of this is transparent
to the user excellent and I use the same
thing last year or is it you know we've
moved things and we know that people
tune their applications to get the best
performance and all that hopefully this
just works magically should work
magically there's nothing saying it
shouldn't but please you know help us
out there's a you know a chance risk
whatever you want to call it that you
will need some modifications to your
settings whatever we'd like to know
about that and the sooner the better so
we are publishing builds of of actually
all our releases on a weekly basis and
JDK is no exception to that we are you
know every single week there's a new
build and we even put a somewhat
synthetic stamp on one of the builds a
couple of weeks ago saying it's a
Developer Preview it actually kind of
means that we ran more tests on it and
that we believe that it's not going to
break immediately at least okay it's
better than that but um so I encourage
you to pick that up and start helping us
you know we we can only do so much
testing ourselves the world is a big
place and
we have infinite input in terms of byte
codes and job applications and behavior
so please please pick it up help us run
it make sure that it works for you
and we'd prefer obviously the feedback
now as opposed to after GA and then we
have two more things for dedicate one of
them is that we have now turned by
default on tiered compilation so for all
so you don't know that in j-rok sorry in
hotspot we have three kind of different
ways to execute Java bytecode one is the
interpreter so you know for a quick
start up the interpreter is unbeatable
if you just want to start executing your
method what the interpreter does is it
looks at the bytecode it does whatever
the bytecode is supposed to do and you
know moves on to the next bytecode and
you know eats them one at a time but
that's not the best performance
obviously so what we normally do is we
realize after a while that this method
is being executed quite a lot so it's
better to compile it so we have a
compiler called the client compiler
which is the one you get typically on
your Windows 32 system in the JDK or JRE
and what that compiler does is it turns
your bytecode into some more like native
native code assembly whatever and we get
better performance that way but
obviously there's an overhead to
compiling it so you know you want to
make sure that that method actually does
get called a lot before you compile it
and on the server side we and this the
client compiler by the way it goes is
also called c1 for short and then we
have a server compiler which is the one
you typically get if you run on a server
type system 64-bit Linux would be an
example and the server compiler does
pretty much the same thing only it's
more targeted or more focused on
performance while the overhead of
compiling the method is slightly bigger
so that's if you want to get the top
performance then the server compiler is
what you're looking for so one of the
things though is as I mentioned is we
want to make sure that the the things we
do Kampai
are the right things to compile we don't
want to waste all this cpu time
compiling things if it's not gonna you
know help you in the end and that goes
obviously for the methods that are being
executed but it also goes for a lot of
the the data you know we want to make
sure that if there's an if statement and
only one and one part of that if
statement is ever executed the other
part of it we couldn't care less about
let's handle that as an exception not a
job exception but you know let's let's
do something special if that ever is
executed and in order to get all this
information we need to run for a while
and traditionally this is something we
have been collecting as part of
interpreting the bytecode so it's part
of the the interpreter what we have done
now with jdk 8 is to make sure that that
profiling information is something we
collect using c1 compiled code so
logically what happens here is you you
interpret things for a very short while
then you compile those methods and add
instrumentation code to collect all the
profiling information and you run some
more and now you're getting this profile
built up and you're getting that data
much more quickly than you would if you
executed it in the interpreter and then
once you have that enough data to to
make use of it you do the compilation
using c2 so it's a you know three step
or even more step process on the way
there and what this leads to among other
thing is interestingly enough is fastest
startup time because we again get the
profiling information much more quickly
we know which methods to compile and how
to compile them much more quickly so
this is a relatively big change as well
again please pick it up help us out and
the last thing I wanted to highlight
from JDK 8 is a series of embedded
related optimizations we have been
working on and so big disclaimer at the
start this is not gonna like
revolutionize the IT industry
necessarily but I think it's actually a
pretty cool set of optimizations the
insight here is that there are a number
of structures internally in the JVM we
have that are very very frequent very
very common typically this would be
class straw
Jers or methods and it's not you know
uncommon to see applications that have
in this example I have here it's ten
thousand classes with a hundred thousand
methods in total that's a relatively
small application if you look at a
server an enterprise where it's more
probably in the order of fifty thousand
or a hundred thousand classes with a
million methods in them or something
like that and what we have done here is
to look at those structures and make
sure that every single bit is used
there's no slack there's no padding
there you know if there are mutually
exclusive fields that are only used in
one case or another let's merge them and
make sure that we really pack all the
data densely and in the example here
again it's like if you have that
relatively small application which is
still I'm gonna argue a big application
on the embedded side you save someone
wearing the order of three and a half
Meg's per process which doesn't sound a
lot I guess but it's wasted memory it's
it's not used for anything it's really
just waste and even on the server side
if you imagine hundreds of JVMs or
hundreds of processes or thousands of
processes it does add up over time so
that's just an example of something with
it was kind of straightforward and easy
but it's actually relatively powerful
I'll cover quickly just say that there
are a number of sessions on Mission
Control and flight recorder which I
encourage you to attend there's also a
session on the permanent generation
removal on Wednesday again please attend
that if you want to know more about the
permanent generation and and the fact
that it's called and with that said
we're moving into future so now it's
it's all mystic and not committed to in
any way and all that so keep that in
mind
security I'm not going to go into
details but this remains our key focus
this is something we will continue
working on knob set so a few trends or a
few themes that are will we'll continue
working on the first one is cloud and
it's obviously like a wide concept you
can come up with all kinds of neat
different things that fit into that
bucket but again on kind of a high level
about thousands of JVMs executing
exactly the same or at least almost the
same application that's something we
want to optimize because it's getting
more and more common the we're moving
into a world where we are executing a
lot of JVMs and for different purposes
or reasons these JVMs are in some way
split up
they run separate from each other
because of isolation requirements or you
know just because it's easier to deploy
them that way today and that we need to
change because currently there there's a
big waste in resources running all these
processes so what comes with this is you
know a few years ago ten years ago let's
say we pretty much were running one
single JVM per machine and machines have
become very potent ins and people are
now running tens of or hundreds of JVMs
per machine and you can solve that in
two ways I think we need to look at
attacking that from both angles but at
least it will always be the case I think
from now on that it's not gonna be one
JVM per machine it's gonna be more than
that which means that we don't no longer
have the luxury of just using all the
resources and you know bathing in CPU
time and memory we need to share that
very carefully with other JVM is running
on the same machine or in the same data
center so memory CPU all these resources
can vary a lot because of how workloads
vary over time or the fact that
different jaebeum's kick in at different
times and virtualization is is adding to
the problem in the sense that we we have
another layer of abstraction and more
resources being used for that so long
slide or I set a lot of words basically
saying we need to make sure that we
manage our resources more carefully and
the thing the way I think we do that is
by enlightening the Java platform about
this no surprise there I guess we need
to be able to adapt to changes in this
environment if you know suddenly more
memory becomes available and we need it
in one day VM processes as opposed to
the other we need to make sure that that
scales up
and again the key challenge here is
doing all of this while maintaining at
the high availability and isolation
requirements so you know you can run all
applications on one single JVM but
chances are that you you know if that
JVM goes down for some reason or you
know if if you really want isolation
because your customers are it's very
sensitive data then you still need to
maintain that part of the isolation in
the requirements so that's you know
they're at odds with each other but it's
something we need to work on we'll
continue our work on the manageability
and observability of the Java stack and
the JVM specifically so flight recorder
would be an example of this and one of
the key aspects there that I like to to
try to push as much as possible is
ergonomic sand out-of-the-box behavior I
hate when I see a long JVM command line
with 10 J you know garbage collection
options and you're setting this and that
and this and that realistically if you
want to squeeze out the last few percent
of your performance you'll have to add
some options but the defaults should be
good enough for the majority the vast
majority of the workloads so reducing
the the need for tuning or JVM and
failing that at least you should be able
to look into the JVM and see that here's
why it's not performing well or here's
why I don't know get that exception that
I'm getting so an important area being
able to look into the JVM and understand
what's going on even more important
because of the fact that there are now
more JVM is running on the same machine
another theme we have been working on
for many years now is getting the JVM to
a point where it can not only run Java
Java code or Java source code whatever
but also run other languages and the key
inside here somewhere is that we've
spent or we invested decades or you know
it's actually centuries of man man-hours
man work on the the performance the
manageability the serviceability all of
this in the JVM so by exposing a few new
primitives it's actually relatively easy
to do run other things than Java on the
JVM infrastructure and the way again
this the first thing we did kind of to
enable this is the invokedynamic
instruction in java 7 and we're
continuing our work on that so again as
I mentioned it's performance and
performance and performance mostly for
now acknowledged and but the nice part
here is that we actually have two very
important consumers of this
functionality internally in the JDK now
as well so the first one is lambdas
lambda actually does use invoke dynamic
internally so it's in our best interest
to make sure that the performance of 292
and invoke dynamic actually does scale
up and work well because we want lambdas
to be the way of implementing your your
data problems data focus problems and
the other thing we have done in JDK 8 is
as you may have heard in the keynote
yesterday is we've updated our
JavaScript engine from Rhyno to Naz horn
and Naz horn discrete JavaScript onion
uses invokedynamic heavily as well so
again we have a vested interest even
inside of the jdk itself to make sure
that this functionality that we continue
working on it and improve on it and then
mark in the keynote yesterday called
this J&amp;amp;I 2.0 in innocence we need to
just realize the fact that there is
we'll always be native data a native
code we need to interact with and for
also you have well had the questionable
privileged or privilege of working with
da and I I'm gonna say that there's room
for improvement in that area so that's
an area that we're also interested in
it's not necessarily multi-language but
I fit it into the same slide I guess and
then scalability this is definitely not
something new there are a few different
aspects of scalability one is the
continued trend of ever more cores in
your machine and what we want to do here
is to make sure that we can leverage all
those course and turn many of the mostly
control flow problems or at least that's
how people express the problems they in
and turn those into data parallel
problem instead of problems instead them
here's again where lambda comes in
together with for coin we now have a
very powerful easy way to express your
problems as data parallel instead or
data focused and then leave the
computation the actual infrastructure
and scaffolding over to the libraries
and they can in turn paralyze this and
use them in the course and tightly
related to this I'm gonna say is locks
as you scale up if you have a lock
somewhere and you're pounding on that
that's gonna be a big problem obviously
in the bottleneck so our work on
improving synchronization java
synchronized or locks in in general will
continue as machines grow up you know
grow up but it become bigger and they're
also getting more memory and big data or
big as john from IBM put it yesterday
big asterisk dot asterisk it's gonna
happen to us as well obviously we want
the JVM to scale up not to just a few
gigabytes but to tens of gigabytes or
hundreds of gigabytes over time so this
this is again where the garbage
collection the g1 garbage Direction
framework comes in
no no bottlenecks in the Jaypee in the
garbage direction to start with and and
then other things as we go
and one of the things I'd also like to
put my finger on is the fact that the
Numa trend seems to be continuing it's
gonna the way I envision it it's gonna
be worse before it gets better hopefully
this is again not something you as a
Java developer should have to care about
that much because we hopefully can solve
a lot of this in the JVM and the JDK so
that you don't have to but that's that's
definitely an area where you know I see
that we need to improve over time as
well and then footprint and embedded as
a scalability thing as well our again
convergence means that we need to make
sure that the same JVM the same java
platform now scales from very small
devices all the way up to enterprise
servers so we want to streamline Java
for small devices and here's obviously
where jigsaw kind of comes in as well we
have a compact profile the compact
profile in JDK 8 but going forward
jigsaw is is all about trying to make
sure that we can split up the JDK into
smaller components getting a smaller
footprint both on disk and also in
memory so that's where some of the
themes and now I'm going to mention a
few of the things that are going on in
the respective JVM components and when I
say Jamie M components the way we have
split this up internally or the way we
work on it
is that we have one runtime component
actually maybe I should start with we
have the compilers those are kind of
obvious they implement the JIT compilers
that take byte code and turn it into
something negative high-performance
thing we have garbage collection which
is also pretty obvious it's implementing
garbage collection and then we have two
teams which are slightly harder to
define but still relatively
straightforward it's the serviceability
team which is working on serve you know
everything that has to do with
manageability and observability and
control and all of that and then the
runtime team and I'm gonna say that's
where the rest of the things end up but
on some high level you can think of it
as classes class loading synchronization
the goo and the scaffolding that holes
holds all of this together so starting
with the compiler I'm not again gonna
mention a whole lot of details here I
encourage you to look at your schedule
and follow up on this there's a buff on
project Sumatra tomorrow so product
Sumatra is all about trying to get to
the point where you can offload your
java computation on GPUs so if you know
we're getting used to the fact that we
have 4 10 20 scores in our CPUs but it
turns out that GPUs are their weird
beasts they have many cores it's
hundreds or thousands or whatever the
interesting thing is they can only
execute kind of one thing at a time
all of them together so it's not as
flexible as a CPU but they're very very
powerful if you have a problem where
you're executing the same thing on many
many different data points so what we
want to do is again make sure that it's
almost like transparent it's you're not
even supposed to know that it's being
offloaded on on GPUs it's just supposed
to happen
that will obviously take time to get to
that point but as a start we're making
sure that we at least can take your Java
code and compile it to or run it on a
GPU with all the data transfer that
needs to happen as well one of the key
overheads in this case is transferring
the data so you need to you have it
somewhere for on your Java heap and you
have to move it over to the GPU memory
before you can execute or do things on
it and then you need to transfer the
results back and we want first of all we
want that to just work but then the next
step obviously optimize for it and then
we have a few more detailed things that
are going on in the JVM so one of the
things that kind of comes with Sumatra
but also because of tiered and just the
fact that again we need to be more
careful about memory consumption is our
code cache as it called internally but
you can think of it like the area where
we store a compiled code so one thing we
need to do there is to make sure that we
can have both code that is compiled for
the CPU but also code that is
for the GPU in the same place or at
least we have we support that let's say
and in the next step obviously you can
you can waste so you can have as much
memory as you want to represent code but
at some point you you know you know this
therefore performance and if we steal
memory for code that is better used in
the garbage collect collector for
something or on the Java heap then then
we have a problem so we need to make
sure that we use again the memory for
code wisely and so a lot of work is
currently going on in tuning and making
sure that the code cache the code memory
management system scales well we're
continuing our work on the manageability
and observability of what's going on in
the compiler so especially as we're now
picking up other languages and running
that on top of the JVM we need to make
sure that it's easy to understand what
the compiler does like why did it make
this decision to inline this method or
you know only look at this part of the
if course all those things you want
control over or at least visibility into
so that's another thing we're working on
and the final thing here is and this
will always kind of be important is the
time it takes to start up and compile
and warm up it's they're the kind of
same aspects or multiple aspects of the
same thing we want it to be obviously
very fast to get your application up and
running and up and running in the most
performant way so currently it's like
you run your application once and then
we throw away all the information we had
about what was hot and useful and
whatever and when you start the next
time you need to get all that profiling
data again and so on and so I mentioned
alt we request your mark after it here
one of its kind of a loose definition of
it but one of the thing alt is short for
ahead of time which is kind of loosely
its you know store code that you have
somewhere ahead of time so that you
don't have to compile it on the fly when
you run your application but it's if you
widen that a bit it's again making sure
that we have the information needed to
get to that point
your application is up and running and
performant quicker so that's something
we need to look at on the garbage
collection side as I mentioned we have
the garbage first or d1 framework we're
continuing to invest in that and make
sure that it works better and better
over time the way g1 works and why we
need that framework going forward is
that it in traditionally you know if you
look at the other garbage collectors we
have they're essentially dividing the
heap up to you know a couple of
different areas we normally call them
like the Jong generation where objects
are born so if you allocate an object
that's where it is and then over time as
we notice that yeah it's actually going
to survive and it seems like a
long-lived object we move it into the
old generation and then you can garbage
collect the young generation separately
from the old generation and everything
is fine problem is obviously as that
scales up as you're starting to talk
about tens or gigabytes of memory that's
not you know having only two areas is
actually too limited so what G one does
is to divide this up into hundreds or
thousands or again it's configurable but
you know think of it like thousands of
small regions and in essence they can be
individually garbage collected so you
get to you know continuously we're
picking regions that are the most
profitable to garbage collects the ones
that have the fewest live objects in
them if that helps and we can also know
in advance like this is prediction but
we can know how long it will take to
garbage collect because we know that the
region isn't bigger than a couple of
Meg's and the worst case is that it will
take 20 milliseconds or whatever it is
so the primary focus of G one so far has
been on huge heaps or big leaps 40
gigabytes and low inconsistent pots post
times but over time again this is our
framework that we will use for for all
the different types of garbage
collection requirements and that's
something we'll continue working on
again the primary focus on big it
without all those ten twenty hundred
tuning options so
good out-of-the-box behavior work
continues to identify and eliminate the
rough edges and especially on the
ergonomic side as I mentioned there
there's still you know we have
parameters we have default values
internally we need to make sure that
those are well tuned for the the vast
majority of use cases and also eliminate
as we find ball necks in different
places so that work continues are
clearly stated goal here is as a first
step to deprecate the CMS the concurrent
mark-and-sweep collector so I'll say it
again will be deprecating CMS and remove
it but if the sub-bullet says obviously
we need to get to the point where with
g1 where it can replace the vast
majority of use cases for CMS as well if
you have feedback on either our you know
our vision on this or for that sake on
g1 specifically as you run into problems
or you have you know if you know even
better if you want to say that this is
actually working really well for me
there's a an email list here and I
encourage you to send feedback there so
it's hot spot - DC - use at or OpenJDK
dot java.net and there's a at least four
sessions on g1 and garbage collection
again I'm not going to go into details
but it's everything from tuning to get
your hands dirty labs and things like
that the last you know the
second-to-last the third component is
the runtime system so again that's look
at the bucket where we pour everything
that isn't a compiler GC or service bill
T the first key bullet here is
modularization and jigsaw so of all the
things in the JVM that will be that will
need the most support for jigsaw it's
the runtime system clearly with classes
class loading class paths all of that so
that will be a large work item for us
earlier I mentioned that we bumped up
this
static size of the hash table and
clearly on that was the short-term
solution so making that dynamic instead
is something we will be working on
we have functionality called class data
sharing in hotspot in essence this is
running your application once so that
you get all the data memory storing the
metadata to disk in an image file and
then for every subsequent JVM you start
up we just mapped that file into memory
and reuse the information instead of
having to recompute it when you start up
the application and traditionally this
only worked on the client JVM because it
only worked with the serial garbage
collector which is one of our nine
garbage collectors internally but with
the permanent generation removal this
actually kind of just fell out naturally
so now it actually works on all the
different collectors and it's still
limited to system classes so you can
think of it like the the JDK classes but
what we're looking at now is if we can
expand on that to include other classes
as well but in general it's at least
making sure that we can share more of
the the data between JVMs one of the
things you get with this is that instead
of having a separate copy of this data
in every single JVM you share it and
it's transparent it's process or
operating system based so there's no you
know the isolation comes for free and
and that's a obviously good thing if you
require the isolation and then just as a
finishing bullet here we are again
continuing our work on improving
synchronization and locks and very
specifically the contended locks so if
you have many many threads competing for
the same lock we want that to be
scalable and that obviously goes in line
with the lambdas and the multi-core
story so the last component is the
serviceability component so JFR or Java
flight recorder was now introduced in
70/40 but that's obviously our first
version of that functionality and we
will continue working on
some of the things we have queued up our
additional events which is kind of a
no-brainer I guess like as we find
things that we would like to have in
that event stream we'll add them and one
of the limitations right now actually
our flight recorder is that you have to
specify on the command line the fact
that you may later want to use flight
recorder and it's kind of obvious that
that's not how you want it to work you
will you want to be able to say oh I
have JVM running over here and yeah I
don't I know I didn't do anything when I
started it up but now I want the flight
recorder data so you know no no brainer
it's something you know there was only
time limiting us from from doing that in
the first version will improve on our
event sampling and make that more
flexible and see that you can tune which
events to collect currently you it's
basically latency driven or duration
driven I should say so if if an event
takes more than X we will store it
otherwise we'll throw it away and one of
the coolest things this is one of the
things I've been wanting to do we're
wanting to see from I don't know five
ten years is automated analysis of this
data so in Mission Control on the
Mission Control side we're working on
taking all this these events and kind of
drawing high-level conclusions from
what's in there so that you don't have
to zoom in and try to see how it is that
event and it correlates to this thing
and therefore this is what's wrong or
this is something here's what I can do
about it with all these details in this
detailed information we can now look at
it do the analysis for you and throw up
something saying looks like you're I
don't know young generation is too small
and you should do this so that I think
is pretty cool we introduced in seven
update for which I think was April last
year a new Java launcher a new service
built e command in the JDK called J CMD
and the goal with J CMD is to replace or
implement all the functionality or at
least the relevant functionality from
the other J commands we have J stack J
info J map not Java
isn't Java launcher is not included in
this it's not jstor and just it's a few
well-chosen serviceability commands so
implement all of those in the J CMD
command and then over time again
deprecated those other commands two more
things we're working on here is JM axe
and looking at what we can do going
forward with DMX there are a few
suggestions on features here one is
instead of having to maybe name your
class mbean to make it you know turn
into a bean why not have an annotation
saying that instead or parameters for
the bean so on in general at least you
know when DMX was implemented
annotations did not exist so just
bringing JMX and the the well bringing
JMX up to speed with what language
supports today rest is obviously a
protocol that is very widely used so one
suggestion is to make sure that we have
a rest implementation as well so that
you can for example interact with JMX
and beans from other script languages
python etc and then batch operations
today it's very much like you know you
need to use you do an operation you wait
for the result you do another operation
you wait for the result and if you
depend on those values being kind of
atomic or at least you know having it
somewhat coherent there's a clear risk
that over time they'll drift apart and
so on so having some kind of way of
saying I want to do all of this at the
same time and report back to me when
you're done so that's also something
we're looking into
and then we have more deprecation and
removal one of the things is j console
so if you look at the jdk today we have
three different visualization frameworks
now at least the first one were you know
first but there's j console there's
visual vm and there's Mission Control
and much like we I don't think we need
seven JVM so I don't think we need three
visualization frameworks in the JDK and
of all of these I'm going to
a consult today you know there's this
functionality in there that may be
interesting but again the chances are we
should try to have that be in visual VM
and or Mission Control is that I'm
having a separate framework for that so
that's something we're looking into and
the last thing here is the H profit how
many people in here are using the H prof
agent okay so there's a couple or well
handful let's say there so what we want
to do here is to look at what parts of
the edge profit people are using and
then see if we can replace that with
functionality elsewhere so if you look
at for example the agecroft amp itself
that's something we we have in the H
profit but we also have it in the JVM
there's another command and another way
of getting that that data so this is
also something we're again looking at if
we if it's time to deprecate and remove
overtime and have but again have the
functionality elsewhere instead and then
there's a few final bullets on on
miscellaneous less glamorous stuff so
what we want to do and this is a key
thing will be working on going forward
is just improve on the testability
of the JVM so make sure that it's easy
to write targeted tests you don't have
to run you know write a system test and
hope that you're stressing the corner
case in the garbage collector so you
need testing in some wide it's some wide
use of the the terminology and try kind
of coupled with that is there's we're
getting to the point now where we have a
lot of we supported a lot of different
operating systems and CPUs and a lot of
that code both the CPP code and the make
files is just copy-paste things it's you
know somebody ported it by copying the
make file making the small modern
necessary changes and then that's what
we have and the obvious risk with that
is when you fix the bugs in one make
file you don't do it in the for other
ones or you do it in three and not the
fourth so that is something we need to
clean up as well
a final callout encouraging you to part
Speight again we can only do so much at
Oracle or in open JDK the people that
are actively working on this we need
your feedback there's an infinite amount
the bytecode combinations we can't test
them all and you know how to use two JVM
we know how to build one so give us
feedback on this there is a developer
preview available please pick it up and
run it now is a good time and if you
want to to work on the JVM it is open
source there's a mercury mercury
repository all of the JDK is help us
participate come work for Oracle and
we're out of time I guess but here's
where the questions would be
so the question is there's a special
option needed to enable fight recorder
unlock commercial features is the name
of the flag yes house that's going to
play into okay so I actually don't know
exactly you know the I'm not a lawyer
and I'm not in sales and all that but
basically I think you know don't take
this as a read up on it but effectively
Mission Control flight recorder is
available for development use without a
license you need to again don't take my
word for granted but that's
approximately arias and then if in
production you want to use these you
need the license the reason why we have
the option is that we don't want people
to accidentally use this functionality
without knowing that in some cases you
need a license so as much of a hassle
this believe me to us but also to you it
is something we're hoping will help you
not fall into the trap of you know
accidentally again using functionality
without the license
so the question is on one of the slides
it mentioned 40 jigs and beyond is that
a limitation in dedicate and it's not
I think somebody tests the g1 with a 2
terabyte heap if I remember correctly so
I think the the upper limit is somewhere
in the order of 2 terabytes don't I
actually don't know the exact number but
it's relatively big at least let's say
and there's nothing like that's just
because that's the machine we had
available at the time there's nothing I
think preventing us from moving even
beyond that so I had two of those slides
the first one was the JDK 8 meanwhile
I'll take the next question
right so on the contender locking so
what improvements have we made the
contender locks so there's a number of
relatives we done so one is for example
here's the fact that if if you say you
want to go in you are releasing a lock
the thread is releasing a lock and there
are no waiters there's no other thread
you need to notify and say you know the
lock is now available we actually bypass
that on the Java level now we are also
using there's an especially instruction
on the SPARC processor we're using now
for spinning I think but in general its
you know it's a series of relatively
small things that together may contain
the locks more scalable and I think it's
a final question
right so the question is has there been
any work on I'm gonna say having the day
the Java Mission Control and or visual
VM functionality available on the
command line is supposed to in a
graphical user interface because of
problems with connecting with DBMS in an
environment so I think that's actually a
really interesting question I think in
general there's there's some
functionality that is currently inside
of Mission Control that actually in
theory at least should be in the the
target process in the data Kay itself
what I recommend is that you reach out
to you know the Mission Control guy is
his name is Marcus hurt he's the
architect for Mission Control in one of
its sessions I think the first one here
which starts in half an hour is
potentially yes or at least the tutorial
find him talk to him or send him an
email asking about that and I think
we're violently out of time so I thank
you very much for showing up</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>