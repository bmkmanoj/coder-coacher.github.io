<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Building Systems with Asynchronous Microservices | Coder Coacher - Coaching Coders</title><meta content="Building Systems with Asynchronous Microservices - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Building Systems with Asynchronous Microservices</b></h2><h5 class="post__date">2015-06-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/TiY4BhHM7y0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so just a little bit about me so my
name's tim ward and I'm a senior
consulting engineer at peramis and I've
been working with osgi for a really long
time so I first started when I was
working with IBM are using wires and
then developing wires and building the
osgi applications feature pack on top of
that I do a lot of work with the osgi
alliance so JP a weaving hooks and
asynchronous services which we'll be
talking about today I'm active in Apache
Aries and I've spoken here at javaone
before I also speak at a variety of
other conferences about topics including
osgi and other stuff and i also have
written a book about this which as
anyone who's written a book will tell
you doesn't mean that you've got any
real credentials but people think it
does anyway so what we're going to talk
about today is a little bit about why
why things are becoming more
asynchronous because there are plenty of
talks about it here I'm sure people have
noticed a little bit about some async
programming primitives something about
then how we can take a synchronous call
and or a synchronous service and try to
make it more asynchronous and then how
we can actually optimize those calls to
try and get a little bit more out of our
system than maybe we could before and
then I'm going to be doing a demo in the
cloud now thankfully unlike many
conferences I've got wide network here
which means I can't blame the Wi-Fi but
if it does go wrong then I'll be blaming
amazon instead so just a little
something I say I do a lot of work with
osgi so the stuff I'm talking about is
actually still not quite final it's
pretty close but it won't it's not going
to be released until Q for the core
release 6 was done back in June and
there is a there are early drafts of
this stuff so I what I'm talking about
is public so it's not you're not getting
much of a sneak preview but it is pretty
much the the first stuff that's actually
implementing it and using it that you're
likely to see
so as I said why is the world going
asynchronous well in order to talk about
that we really need to look at the
origins of asynchronous programming so
how do people think a sink programming
started what about eight years ago with
Ajax people think that it's a recent
thing but actually the terms promising
future were first used in the 70s in
research papers so people people think
Oh asynchronous new great but it's not
it's been around for a really long time
and it was basically there to cope with
how do we do large-scale parallel
execution because you know computers
they're much faster than people and we
need to be able to cope with this stuff
so as I say really long time and you've
got this thing where your user interface
eventhandler can't block the UI thread
that's been around for a long time to
you know you know what happens if you do
block the UI thread in your in your
application you'll get a nice little
spinner and nothing will happen so async
really important and has been for a long
time it's not new so the reason that
people want it is because typically
asynchronous systems do offer a better
performance but of course you're
assuming there isn't a single bottleneck
somewhere in the system so if you're
already at a hundred percent disk access
with asynchronous program or one hundred
percent CPU then you're not likely to
get a whole lot more out of your system
because you're already maxed out it's
not going to help you what it might do
is it might allow you to interleave some
tasks that do something else and use a
different part of the system so you
might be able to improve throughput but
it's not going to be magical so
effectively what you're really looking
at is trying to get something that will
scale better so you don't spend time
sitting and waiting for a high latency
call when the answer comes back then you
do something and you also get slightly
more reasonable failure characteristics
and a lot of scenarios you know you've
got the option to handle a failure in a
way that makes sense
and parallelism is easier to exploit
with asynchronous stuff because you're
already doing something and not waiting
for it to come back anything that you do
next is implicitly going to be a
parallel operation so parallelism
becomes just a kind of de facto way of
operating so network infrastructure has
been a synchronous basically since the
first bits were sent down the first wire
so if any of you have background in
network engineering then you're going to
be saying well yeah cool this is all
really obvious you know is having this
the way it's always been so some
problems well human brains whilst they
actually do work like a synchronous
computers internally the logic they
can't really cope with it so you tend to
think i'm going to do a and then I'm
going to do B and then afterwards I will
have C because I did those two things
and that's the way brains work it's why
the basic procedural programming is
actually something that people find
quite easy to follow even if they're not
that good at programming they can kind
of look and go one two three four and
kind of work through it and go on a pad
of paper it's like yeah I get what it's
doing so asynchronous programming is
really only going to be useful to us if
we can find a way to get rid of that
cognitive overhead so you know I can
cope with I do a and then I do B and
then I start doing F and then C has come
back and then D happens which I didn't
even know about happening because
somebody else and that we can actually
cope with that better if we've got the
right tools but it needs to be worth it
something else is that actually
synchronous calls offer you're currently
yourself as a kind of natural break so
if you've got a big stream of
asynchronous events happening then
actually if you start making if you
start turning those into remote calls
that you're farming off to lots of
machines you can tip you can quite
easily overwhelm a machine by sending
lots of requests that are cheap for you
to generate and expensive for them to
process and if you've got our pc then
you've got this kind of natural break
because they're going to be pushing
stuff to you
the rate at which they can push is being
limited by how quickly you're returning
the answer so you've got to be careful
there you need to you need to cope with
that in a system that really is going to
generate event storms and also you can
still deadlock and it can be very
difficult to diagnose what's happening
because you don't really quite know
what's happening so basically you know
asynchronous there are there are good
things about it but what what has
changed why is asynchronous suddenly a
big thing and that actually seems to be
pervading everything we do well
multi-core systems are now more or less
than norm there aren't very many things
that are single Cornell I mean even
mobile phones are the quad core now if
you buy a recent a reasonably modern
phone so multi-core you can't always max
out that Hardware really easily without
going async also our languages are more
functional now than maybe they used to
be so Java and C++ now have lambdas I
mean Java held out for quite a long time
but they're there now you've got this
more functional style Java that you can
write really easily and then JavaScript
has been a major driver you know you've
got a single threaded environment you
pretty much have to go a think if you
don't go a sink in JavaScript then you
just sit there and you make zero
progress because you sit waiting for
something for ten minutes and you can't
do anything else at all so you've got
this huge variety of JavaScript
frameworks that are popped up with kind
of different ways of handling and
processing all this async work and
that's something that really has driven
actually a lot of a lot of more modern
handling of a sink concepts because it's
just necessary
so I've talking about these async
programming primitives well so we're
going to need something if we're doing a
sink programming we can't you know if we
if we're working with standard code we
know we've got you know various numeric
types going up from kind of bite too
long and we've got strings that we work
with and you we've got all these things
and you put them together and you build
a program but if you're working async
then you kind of need some extra tools
because it's not enough to just have
these kind of primitive type things you
need some other primitives to help you
work with them so basically the promise
is the primitive of a single programming
some people will say future I'll come on
to that a little bit more in a moment
but effectively a promise represents a
delayed value that is going to be
resolved at some point in the future so
I when I say resolved that means setting
the value or setting a failure value
either way you are resolving the promise
it now has a value so osgi has promises
or will have in q4 officially but you
can see in the draft osgi has promises
and they're based on the promises that
were existing JavaScript but they're
also quite heavily influenced by the
stuff that's in Scala so for those of
you who use JavaScript or scarless this
is going to look quite eerily familiar
and strictly I said I'd come onto this
strictly osgi and JavaScript promises
our future types if you were reading an
academic paper they be described as
futures because the future is the thing
has the method to resolve it and the
promise doesn't but one of the things
that we were looking at was we tend to
have code where people are using Java
util concurrent future and osgi promises
at the same time and as one of the
people who is responsible for specking
it I didn't want to be chased by
pitchforks when people were trying to
use two different kinds of future in the
same code and saying why do I always
have to package prefix one of them so
that's why
in both cases so both javascript and
osgi there is a deferred type that can
create and resolve a default promised
implementation so you can kind of get it
and you can set your value and give the
the promise off to someone else and it
it means that you don't have to
implement this interface yourself every
time because it's actually non-trivial I
mean it's not hugely difficult but it's
not free to implement so this is the
contract of the osgi promise so it is
resolved or failed with a single value
at most once now it is usually
considered quite rude to not resolve a
promise ever because you'll just have
something where people start hanging
stuff off it and sooner or later they're
expecting an answer it doesn't have to
happen immediately but they kind of are
expecting it but if it is resolved it
will be resolved once and that's it
listener callbacks that are registered
are then called at most once so when it
is resolved they will be called maybe
not immediately and maybe not on the
same thread but they will be called and
then though they will be called with
that resolution value want and that's it
promises remember their state which
means that actually if you have a
promise that has been resolved and you
later attach a listener call back to it
it will get the value that it was
resolved or failed with so that's
important it doesn't matter when the
promise is actually resolved so this is
promises behave the same way regardless
of whether they're going to be resolved
or have already been and that's really
important so it means you can actually
take a promise and use it as a shareable
value type because it's effectively
immutable yes its value can it come into
existence but what you do is you where
you register callbacks to receive it and
when the value is there you'll get them
and it's thread safe and you can't it
won't change after that so you don't
actually need to unwrap it and start
passing the value round you can just
pass the promise always and that's
actually quite a JavaScript kind of
thing
to do you don't pass the value you pass
the promise because you can just use the
promise to get at the value really
easily so as I said osgi does provoke
provide a default promise implementation
when I say default promise
implementation people are free to
provide their own so i have written one
which hooks into a thread pool in our
product but I also know that some of the
guys who work with acha have actually
built an implementation of SGI promises
on top of the echo primitives and
apparently it works really well so this
is a default implementation but it's
certainly not the only one you're ever
likely to come across and it's called
the deferred and the person who wants to
give out a promise creates this deferred
and uses it to kind of get the value in
there and give them an object that then
they can go off and use as the promise
so this is kind of how you would do it
you have a deferred that you create and
then you go off and do some work in some
thread and that may take a long time but
then when you're done you resolve the
deferred or you fail the deferred and
then you can return this promise thing
that you get from the deferred so really
easy to use I know this isn't the most
elegant code but it was kind of the
simplest example I could come up with
actually did something meaningful so you
can do asynchronous type call with a
promise a lot like you do with the
future so you've got is done getvalue
get failure so those those do block the
get methods is done obviously doesn't
block it will just return false but I
would just suggest that you don't use
those because the moment you start using
those you're not really a sink anymore
it's much better to use a promise with a
call black you shouldn't ever block so
you've got the then methods so you've
got your promise and then you can do
some
on success or you can register to call
backs want to be called when it you're
successful want to be called if the if
you fail and they're both Sam types so
we're worked very nicely with lambdas so
you can see here we can call a success
function or a failure function there
when this thing eventually resolves so
call back should be fast but if you've
got a success callback that needs to go
off and do some other processing then
you can return a promise from your
success method and that will actually
then use be used to create a chain to
promise that does yet more work so you
can actually changes together so the way
changing works is then and that will
return a new promise then that promise
resolves when either the success
callback returns null meaning I didn't
have any additional work I needed to do
or I've finished the very short work
that I was going to do or it throws an
error at which point you say oh okay
this new promise is created is going to
resolve with a failure or it returns a
promise and that return promise resolves
this is one of the bits that people find
hardest to deal with and it's because
realistically what we've got at the
stage are three different promises we're
talking about you have the original
promise that you started with you have
the promise returned by the success
callback and then you have the promise
being returned by the then function call
and it's once you grok it it's not that
bad but basically what's happening is
the original promise that you're calling
then on is resolving which is causing
your success callback to run which is
returning a promise when that return
promise resolves the promise that was
returned by then will be resolved with
that value and that's the triangle
completing so it is ok i'm seeing a
couple of people we are with kind of
slightly screwed up faces i realize this
is just after lunch so we're probably
all a bit sleepy but you know you'd go
away pen and paper you know
may it suddenly you'll have this aha
moment and everything will just fall
into place so chaining promise allows
you to compose a sequence of events
together and that's because a promise is
a monad and yes I know after lunch
everybody's sleepy using the M word bit
of a risk the main thing is that you can
create complex behaviors from this
without if-else branching and without
doing doing things like you no try catch
blocks and error handling you can just
compose these chains of things in a
really elegant way so for people who've
been going to say some of the streams
talks you know it's that kind of thing
it's that functional flow where you can
control your whole program without
having to have complex logic so more
complex functional chains there are
native support for so you can build
everything with well actually you can
build everything with the single on
resolve method which just takes a
runnable however it's a bit of a pain
because you have to do a lot of stuff
yourself so success and failure are
there because you know they're quite
convenient and then from those you can
build lots of other things but filtering
and mapping values and recovering from
failure are all things that are natively
supported by these promises so these
common there's also a promises type
which is going to give you kind of
utility stuff so you can use it to
create a promise that is already
resolved or already failed may seem odd
but sometimes you do actually already
have the value but you still want to
give someone a promise because you want
them to be able to use it to maybe
change stuff or maybe what you want what
you want to do is you want to return a
value from your success function and you
don't want to have to go through all the
work of using the third so you can just
wrap a value in a promise you've also
got promises all which is kind of a we
call it a latch so it's a single promise
that only resolves when the entirety of
a load of other promises have resolved
so it's really good for final cleanup
that that works really well or you can
notice
by a user that you know some aggregate
task is finished because all of the
component bits of it are now done so
these things work even if a promise is
already resolved so you don't need to
worry about trying to order your logic
so that this thing is done and then this
thing is done and then this thing is
done you can always take a promise and
then change something else off it that's
really really powerful so can go through
a simple example here so if we want to
do something like download an XML file
but use a mirror URL if we've got one
and then pass that using Jack's be and
it might be quite big so what you can
see here is that we can get hold of the
mirror for this given URL it's going to
return us a promise when that look up
has finished but if that you know fails
for some reason because say it throws
exception it might just say oh you know
some networking error or you know
something's gone wrong then we can fall
back to actually just using the library
URL directly so this is where promises
resolve can come in handy and you've got
fall back to this value then when we've
worked out whether we're using the
mirror or this this original URL we can
download the file and that might take
quite a long time so when the files
downloaded only then do we map that
downloaded file into the jacks p object
so what you can see here is that we've
got this nice chain of work that's done
I mean it's not crazy complex you know I
don't think anyone here would say or you
can write that in a single method but
it's still it's a lot easier to follow
the flow because you don't have the
noise of all of this flow control around
it you can just say get the mirror fall
back to this if it's not available then
download the file and then map it to
some other value we've got a question at
the back
sorry I'm struggling to hear so the
question is whether you can have
multiple fallbacks so what you can
actually do is just keep chaining so if
you want to have a fallback for your
fallback then it would be get mirror
then fall back maybe to an alternate
mirror server and then fall back maybe
to another alternate mirror server and
then finally fall back to the real
library URL but you just keep chaining
does that answer your question great
another one so so Ben asked if there's a
conceptual difference to complete a so
there are a couple of things that are
different firstly the promises API is
quite a bit smaller I don't know if
you've looked at completion stage but
there are a lot of methods there that
can be pretty difficult to understand at
times also completa belong creat tight
so you don't actually have the ability
to provide different implementations and
then the other critical thing is that
obviously complete all future is only in
Java rate and whilst Java rate is
brilliant and I want to use it for
everything a lot of people can't use it
and particularly the osgi framework as
yet has never required a feature that
means you couldn't run it on jsr
compatible 1.4 now that's not to say it
never will but actually there's no
there's no need to go to Java 8 in order
to get promises to work you can
implement them really effectively using
Java going all the way back to like 1.2
the main thing that actually makes them
better is the use of lambdas but you
don't need lambdas to make them work so
that there are a couple of reasons but
that's kind of at least part of it
so I've been talking a bit about a sink
primitives and asynchronous programming
and I hope you know everybody's now
we're ready to maybe wake up a bit more
for just going to start talking about
services so I've name of the talk here
lots of buzz words but building systems
with asynchronous which we kind of
talked about and micro services which we
can talk about now so we have the OSGi
service model now who here has used osgi
okay quite a few maybe half so those
people will be aware of the OSGi service
registry other people maybe aren't so
the OSGi service registry is basically
just a broker it sits there and people
can register objects with it with some
properties to say what the object is and
then other people can ask the registry
can I have one of those please and
they'll either get given back one of the
objects or they'll be told no there
isn't anything and the other thing they
can do is they can register a listener
with the registry to say when something
comes along that looks like this tell me
about it and that's basically what the
OSGi service registry does really
powerful but fundamentally it's that
kind of find and bind broker based model
that people are familiar with from web
services as well as the OSGi service
registry so a service object is obtained
from the service registry and then you
can invoke it and obviously the call
semantics are going to completely depend
on how your API is written but most API
is a synchronous in operation because
most Java is synchronous in operation
it's the way that people have written
Java programs since 1995 you know stuff
happens in this order because you know
when I write my for loop I write it as
this kind of for loop osgi services can
they're not always in fact they're
typically not but they can be
transparent local representations of
remote resources so you can make a call
on a service locally and that will
happen off in some other osgi framework
calling a service so it in that way
they're quite a lot like micro services
web services you can call on a local
object
and stuff will happen on a remote server
and then you'll get an answer back so
you know people saying I thought this
was about microservices well osgi has
has used the term microservice for quite
a long time and it was it chose the word
microservice to differentiate itself
from web services which everyone was
just calling services so you know osgi
thought right let's you know they're
they're definitely smaller and lighter
weight you know nobody could argue that
Ajax was services lighter weights than
just an object in the registry so we'll
call it a micro service and then there
won't be so much confusion but okay so
but as I was saying micro web services
follow a very similar pattern so find
and bind using a registry or some well
known location if you've got a config
file where you just want to put it and
say it's over there and then you make an
invocation locally typically on some
object that gets mapped to actions on
this remote service so a micro web
service is just a much bigger and less
dynamic osgi service as far as I'm
concerned you know other than that
they're pretty much the same so
everything that I've been talking about
will apply to both kinds of services
because I knew that at least some people
would probably come here expecting me to
be talking about you know rest and stuff
so the asynchronous bits that we're
applying definitely do work with both so
I have a question for everybody as a
service client how am I going to make an
asynchronous call to a service
okay well I'm going to make some
assertions that I can't or don't want to
rewrite the service to make it
intrinsically asynchronous because maybe
I don't have the source code or maybe I
just don't want that level of effort I'd
prefer not to start managing my own
threads which is normally where most
people go it's like oh yeah I'll just
kick off some threads I don't really
want to have to do that myself because
you know then I'm suddenly managing this
whole set of stuff that's just for me
and could really be shared maybe should
be shared and I'd like my code to stay
clean which is another reason for not
wanting to start threads because you
know suddenly my code is really busy
with all of this thread management when
really all I wanted to do was just call
this thing so an ad-hoc solution is
messy and has been pretty much your only
choice so far so this led us to look
into osgi and say we need something to
let us do a sink calls better so allow
an arbitrary osgi service to get called
asynchronously and work so it has to be
transparent one of the things he said
was we can't rewrite that service so you
it's not an opt-in thing you can't say
oh yeah just change the service in these
five ways and suddenly there you go it
must work with things that return a
value as well as void because async is a
lot easier if everything's void but it's
not necessarily that helpful we
shouldn't be relying on generated source
code client local interfaces or
reflection you know clients doing
reflection because basically that's
horrible or the source will get out of
sync I don't know how many people here
have worked with generated code but it's
typically a pain because you make a
change the model and then you forget to
make the necessary updates or you know
suddenly everything's out of sync and
it's just a mess so yeah we've got these
requirements so what do we do about it
well actually there was this other
requirement that we suddenly realized
which is some things are already
asynchronous and a very happily
working that way so they should probably
be able to hook in to this model now
obviously this does require some opt-in
because we can't know in advance whether
you're going to be asynchronous or not
so at that point you can hook in and you
can you know leverage the fact that you
are already a synchronous and provide
this this value to people so I'm just
now going to have a quick aside and talk
about unit test mocking because
everybody's interested in that right so
you've got this arbitrary object that
you need to call and you need to
remember those calls it's an important
part of mocking because you've got to be
able to test assertions afterwards so
mocking must be transparent you don't
you shouldn't have to implement a
particular interface in your object in
order to be able to be mocked thats you
know really important you've got to be
able to return specific values from your
methods in from your mocs you can't rely
on generated source or client local
interfaces because actually if you start
start generating loads of source then
it's got to go through another
compilation step and it's all big mess
and the test code really does need to be
using the real interfaces and real
objects because that's what you're
supposed to be testing so do we notice
any similarities here because this was
something we were kind of throwing
around and suddenly we realized this has
a quite a lot of the same features as
what we need from our a sync framework
so using the async service we create
something called an async mediator and
it's pretty easy you basically just say
mediate and then you give it object to
mediate and you tell it what type you
want it to be and you can mediate for
either from an osgi service reference or
a service object the service reference
is better if you are in osgi because it
can track the availability of the
underlying thing and stop working when
that service goes away but if what
you've got is a service object because
you've created some configured thing for
talking to a rest service then you can
mediate that too that's fine and then
you just call that mediated object just
normally and it records the call just
like a testing mock would
and that that was this kind of really
simple realization that we could
leverage this model that people
understand really well because pretty
much everyone uses mocking in at least
some parts of their testing and actually
we suddenly managed to record this call
you then can begin the asynchronous work
by giving the async service a call so
you can use the the nice type safety
here so here we've got this mediated
object and we're saying okay how many
how many pianos are there in New York
that's going to take a while to count so
we should probably do that
asynchronously and so you just do a sink
cool and then you wrap around this call
to the mediator and that way you know
it's going to return a promise of type
long and that's the value that you're
going to get back eventually so under
the covers the async service is going to
go off and do whatever it needs to do to
make sure that that call happens
asynchronously so you're not going to
block but you are going to get back a
promise that you can use to find out the
value and it's really easy now I can do
the whole thing on one reasonably widely
spaced slide void methods vary slightly
trickier and that's because obviously
the pattern that I've just shown doesn't
work for void methods so there's a no
args version of call which returns a
promise of ? and the reason it returns a
promise of ? is because a void method
can't actually return anything so what
will happen is that will resolve with
null when the method completes because
there isn't a value to give you but it
can still be useful to know that
processing this long-running void method
has finished so people say oh why do you
need the promise it's because when it
resolves it tells you that the thing is
done and that it can be very useful
information but sometimes you don't even
need to know when it's done at which
point you should consider
fire-and-forget course so fire and
forget async is usually a bit easier and
it can be quite a lot more efficient and
we'll talk about how it can be more
efficient in a bit something important
to say is that the async service
thread safe but the mediator objects
aren't required to be so don't share
them around always get your own and if
you do start sharing them around then
you may find that it works because the
object may be thread-safe but if you use
may start giving people access to some
kind of object that they really
shouldn't have been able to access in
terms of security so don't do it just
let people make their own so we've got
an example here we can perform too long
running calls in parallel and then
aggregate this result so here you can
see we're just getting hold of an async
and then we've got my service as well
we're mediating it we're doing a call
and within doing a nother call and we're
flat mapping the two values together and
you can see effectively what we've got
here is adding together the two numbers
that come out so flat map is like map so
map converts a value from one type to
another but flat map does it
asynchronously so you return a promise
that's doing the work does that make
sense to everybody
so it did say earlier that you can kind
of optimize these calls a bit so
sometimes as I said you've got more
sewer service than meets the eye if it's
asynchronous under the covers then using
up an async service thread is kind of a
waste because this thing is already
asynchronous so instead we should allow
these people to provide a promise back
to the async service just do it that way
so this is where the async delegate
comes in if you implement a sink
delegate then you've got an opportunity
to say I can optimize this call here's
the promise I'll resolve it when I'm
done if it can't optimize a particular
call for some reason it can return null
at which point the processing will
continue normally so the as far as the
clients concerned it doesn't need to
know or care whether this interface is
implemented whether this thing's an
async delegate it will just work and
this isn't this a bit niche so we've got
this thing but is it really worth it it
feels kind of like we've micro optimize
this but actually you know how many
services is going to apply to but
actually we're talked earlier about
remote services and how you can
transparently remote an osgi service or
you can transparently remote a micro
service and the details of the
distribution mechanism are completely
hidden so if you support asynchronous
communication then you can optimize
those calls so you can return the
promise immediately even before the
answers come back and then when the
answer does come back you can resolve it
and everything's happy but you can do
that optimization and not steal this
thread so in distributed systems it's
pretty good and we also talked about
fire-and-forget calls so if you're
ignoring the return value and you don't
need a callback on completion then you
can optimize this even more because what
you can do is the request can go in and
you return immediately and then you
never bother sending an answer back
because nobody cares so you've saved a
whole extra hop of network traffic
serializing objects you know
you can optimize this a lot and get a
lot more throughput from your overall
system because you knew you could make
this optimization so if you want to
implement your own async delegate there
are only two methods you've got the
async which returns a promise and you've
got execute which doesn't it's only
worth implementing if you can optimize
some of the behaviors of your object so
don't just say oh we should always
implement this it really is only in the
case where you're likely to be called
asynchronously and you can do something
better than the async service would be
doing for you if you return null or
false then you're saying I can't
optimize this call and that's absolutely
fine if you throw an exception what you
are saying is this thing should be
aborted now someone's giving you
complete garbage you know they've given
you and I they've given you an argument
that's of the wrong type or you know
they've given you something that's way
out of bounds so those are the only
times you should throw an exception and
exception does not mean oh it's fine but
I can't do it right now so been talking
a lot so what we're going to look at now
is how we can actually build a system
from this so we've seen we can have a
standard synchronous API and you don't
need to return promise or future from
any of your interfaces because there's
no duplicate interface needed either
you're just writing a standard object
it's a good idea to make sure your
arguments in your return types are both
thread safe and immutable they should be
good value types for communicating
around but you were all doing that
anyway right that's that's the way
everybody works and most remote api's
fit this really well because obviously
if you're returning something over a
remote connection it kind of does need
to be reasonably immutable not have
behavioral state so existing
microservices osgi microservices all
kind of web microservices
are going to work you know that's that's
the good thing about this and you can
also use a mixture of synchronous and
asynchronous we're within the same model
and have it work so a latch promise is a
good way to wait for multiple results i
talked about those earlier and then the
main thing is afterwards hold on to that
promise because the callbacks you can
register after the fact really easily
and when possible split your big calls
into lots of small ones and then you can
aggregate them later using the call box
so you can load balance across multiple
remote machines using lots of parallel
async work and then when everything
starts coming in you can start reducing
those back into the value that you want
to return and it's really easy to do
that because retry and failure logic
isn't kind of try-catch crazy handling
you've got the recover with you've got
fall back to you know you have options
for how you can process these things so
you know do a quick demo just a
description of what it is it's a jax-rs
app and it's running in an osgi
framework it's got some angularjs and
html5 canvas if you're interested but
I'm not really going to be showing the
UI other than to use it it had rest
service which is generating some fractal
images on the server and then sending
the thing back as a stream of events to
provide chunks of this image and we've
got some osgi services to provide a
scalable set of workers that we can call
the backend and all of the source code
is on github so if you do want to take a
look at the full source code in more
detail than we are now then absolutely
go and look there and I'll wait for the
camera phones and iPads to drop
okay three two one so the other thing
we've got is we need a run time to
deploy into so this is the reason I'm
using it is because I develop it so it's
what I use most of every day but the
it's effectively just a cloud runtime
that can deploy osgi stuff it's quite
dynamic very modular you can
transparently scale and remote stuff
which we will be using because we kind
of need to do that with our remote
services and you can also dynamically
deploy these osgi things or docker stuff
so that's what we're going to be using
so you'll see some more of the pictures
like this as we use the UI but what I
need to do now is start showing you some
code so what we've got is we've got a
set of actually just really simple rest
services so let's see this is the this
is the simplest ones show it really is
sorry
chair
how's that
i'm not using IntelliJ but so what we
what we have here is just a really basic
jax-rs service so you can see it's
annotated up it's producing JSON and
it's pretty simple in terms of how we've
got the thing set up so it's not
interesting really to see the classes
themselves but you can see we've just
got a resource config that we're setting
up and registering these things and then
just checking them out as a servlet so
it's really pretty simple the main work
is happening in this renderer so the
renderer we have two methods I'll start
with the synchronous one so you can see
here we are asking to render a fractal
and we get hold of some equations and
color mapping information we validate
that the parameters we be given make
sense and then we just stream a response
which is in this case asynchronous
render or in the async case we have an
almost identical method where we do the
same thing but with an asynchronous
render I've separated these out just
because it makes it easier to
demonstrate I understand that you could
probably just collapse those into the
same thing so the question is what are
the synchronous and asynchronous renders
doing well effectively we've got this
abstract render output which is just
doing some setup of the stream but
fundamentally we're getting successive
calls to render which is going to do a
little chunk of rendering and in the
synchronous case we're just calling the
equation asking it to execute and then
we're writing out the chunk and then at
the end we get an awake completion which
basically says is everything done and
there's nothing to do here because
everything is being done synchronously
you know that we're already done because
actually we were asked to do it and we
did it at the time the asynchronous
render you can see here we have a really
simple so in this case the equation is
mediated and we've got an a sinkhole
which is giving us a promise of all of
these color values and then we're
adding to the list called pending work
and we're writing out the chunk so what
we've added is actually a chained
promise that is the result of writing
out the data and the reason for doing
that is that the promises that go into
pending work will only resolve when
we've actually finished writing out all
the data rather than just receiving the
data and then we in a weight completion
we use a latch promise and we get hold
of all the pending work and then we
actually do asynchronous blocking get
here so there's a load of stuff
happening in the background but right
now we're saying get me the value and
that will hold on hold the application
from returning and closing the servlet
connection until all of this stream of
events is finished being sent so not not
very difficult
so the question is what does it look
like so I mentioned that we'd be using a
product so what we can do here is we can
deploy the application so this is just a
really simple application it's got a
couple of pieces one of which is the
front end which you can see is here and
then we've got this one which is
currently red and has brought up a
little warning thing and that's
basically saying there aren't any of
these and the reason that there aren't
any is that I have told it that I don't
actually want any this thing has size
zero at the moment so we don't have any
background workers we've just got the
main in line worker in the same
framework so what we can do now is we
can go and look at the application and
you can see here that we've got a very
slow rendering of in this case a Julia
set and when I say very slow I mean
obviously we've artificially slowed this
slightly because computers aren't that
slow anymore but this is this is an
example of somewhere where we're making
a lot of micro service calls and we're
having to pay the latency hit and the
expense every single time but you saw we
can just call the async one and do stuff
asynchronously instead at which point
all of these things start happening in
parallel so if I check that box and just
change the color scheme so it's more
obvious what's happening and hit display
you can see some of those I hope you saw
actually filled in out of order just to
prove it really is doing it
asynchronously but it was a lot lot
quicker in fact it drops from about 10
seconds to about for that no question so
a lot lot quicker but actually what you
can do is you can say okay well we can
load balance across a bunch of services
so we're spreading the work out even
further and what you should see is that
it goes even faster but actually in this
case takes the same length of time and
why is that it's because we've got no
remote workers but if what we do down
here is we say actually
put three of them in and then we wait
and we should see that it deploys out 33
notes so we've now got three remote
workers deployed so if we asked to do
that again and you know just for giggles
I'll zoom in this time and you see that
that fills in much more quickly and much
more asynchronously it's already all
over the place because the workers at
different speeds they're different
actually sized instances in ec two so
the quick ones obviously return faster
but that's all possible because we've
got these optimizations in the remote
procedure called stack and we've made it
able to be really really fast and to
leverage threading both locally and
remotely and we did that with well you
you saw the difference in the amount of
code that we wrote so the synchronous
one was like two or three lines of call
the asynchronous one had a little bit of
flow management around it so it was five
and we've gone from taking 10 seconds to
on a reasonably fast network connection
finishing in about one so I would say a
factor of 10 speed boost is probably
worth it anyway so that's the the demo
that I have fear so I think at this
point what i will do is I'll just wrap
up and just say that if you want to know
more about osgi then obviously the osgi
Alliance has website and you could
always read my book you can get forty
four percent off all Manning books not
just mine although you should definitely
buy mine using the Java 1c FTW code so
that that will operate for at least the
next couple of weeks but if you're
interested in getting any books from
Manning go for it and if you want more
information about the demo including
source code then there's a short URL
there and you've got if you want to see
more about service fabric because you
thought what I was showing you there
look cool then you can do that too
do we have any questions okay back there
so the question was how do you control
the thread pool so the thread pool in
this case is the implementation of a
sink services is being provided by the
service fabric and you just configure
the thread pool for the async executor
you can tell it to grow as big or as
small as you want you can constrain it
if you get really really long running
tasks then our thread pool kind of where
pushes that off into another thread and
spawns a new one to make sure that you
don't actually just exhaust your thread
pool and sit there forever but you can
configure how often it does that as well
so question over there
so the question was how's the service
discovery done yes the service discovery
is done by the service fabric it's in
the remote services admin specification
it's not a proprietary thing it's
there's a something called a discovery
service or discovery manager in the
remote service admin spec and that
person is responsible for basically
saying someone overhears got an endpoint
tell everybody about it so that's that's
just a built-in part of the product so
if you register a remote service then
anyone who's interested on one of the
other fibers in the fabric will find out
about it question upfront so the
question is can you specific an you
require container manage threads for the
thread pool so that's up to the async
service implementation so the one that
is built into the fabric is using
container manage threads follow-up
question at the back
so the question is whether you can
deploy things that are not osgi or you
know maybe even Java into this kind of
container and the answer is yes you can
you can deploy these things you can
configure them up typically what you'll
do is you'll represent them with some
OSGi service data so that you can push
configuration around using the discovery
layer but you can absolutely deploy
deploy non Java artifacts we've got
demos through things like engine X any
other questions
this gully her back
so the question is whether this is
something that's going to keep going
more or less forever or whether at some
point we'll go back to completa bellucci
so I think that what is likely to happen
is that there will be probably even
standardized some transforms between the
two because the they clearly you can
take one and turn it into the other it's
not a hugely complicated operation but I
don't think that it's likely to
disappear ever because it's actually
becoming part of the the OSGi API all
over the place because actually there
are lots of parts of the OSGi
specification where things were either
very long running or actually defined as
asynchronous but you had to listen for a
framework event when they were done and
it was very complicated and actually
promises make it really trivial question
instead of using the service
so the question is whether you can
instead of using a service tracker and
have a promise so I can see that you
might you might want to adapt the
existing service tracker API so that you
can get hold of promises because at the
moment you've really only got the option
of of wait for service if you're calling
in line trying to get hold of something
but actually you can use the service
tracker customizer to already handle the
service events so I think they're there
may be something that could be done
there but that would really be a topic
to bring up and discuss with the osgi
core team anything else Oh another one
there
so question is would you suggest using
promises rather than so asynchronous
events I'm assuming you mean event admin
so event admin is quite old and
cumbersome so the thing that event
happening gives you is it gives you a
way of handling events of multiple
cardinality whereas promises are really
only a unary thing so you may find that
there are some use cases where actually
you were waiting for a single event and
actually a promise is a really good fit
if what you're trying to do is consume a
stream of events then a promise isn't
really designed for that it's a it's a
I've made a request and sat at some
point someone's going to return
something so I think we're pretty much
bang on finishing time so I should
probably wrap up thank you all for
coming and I hope you enjoyed yourselves</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>