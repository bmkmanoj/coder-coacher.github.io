<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Taming Hadoop on a Low-Power Cluster | Coder Coacher - Coaching Coders</title><meta content="Taming Hadoop on a Low-Power Cluster - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Taming Hadoop on a Low-Power Cluster</b></h2><h5 class="post__date">2015-06-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/WBzB4KU3k9g" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi everyone I'm Monica Beckwith and i'd
like to welcome you all to this session
they're still there were about 35
pre-registered for the session but I
have a lot of slides and I want to start
get started so I guess I'll apologize to
them when they come in all right all
right this is something about myself I'm
working as a performance architect at a
company called server G and in the past
I've worked with many other companies as
well so basically I'm a software
engineer that knows how to not fall
asleep during those hardware meetings
and I know a like quite a lot of
hardware buzzwords as well all right
something about my company we we do
technology solutions and our main
focuses data security so there are some
of the solutions that we have and big
data for IOT is one of the solutions as
well and I'm going to cover Hadoop as a
part of that a few members of the
servitude team these guys are the minds
behind server g-d high-rise the vp of
engineering and jack smith is a vp of
technology we have helped me a lot with
with the understanding the fabric needs
and the systems and kind of like the
hardware aspect of it and they're not
here today okay so they're not really
helping me here so as a payback I mean I
mean thank you I when I immortalize them
here forever all right I have like I
said I have a lot of slides because the
way I wanted to talk about this as
anybody who's getting started with her
do would would get an understanding of
it as well as people who know about it
already would would know would learn
about our deployment solution so one
thing I want to start with is but what
is scaling I caught you woulda scale up
or scale out mean and then from there I
want to go ahead and talk about Hadoop
and designing the cluster
what our requirements were and some
noteworthy projects if i get time to
them to get to them all right so scale
up you know you I hear a lot of people
talking about you know scaling scaling
with Hadoop and then they're talking
about in the terms that are using they
may say scale out but actually what they
mean a scale up scale up means word
vertical scaling so basically you have
say for example you make you making a
system beefier you're adding more cores
to your system you're adding more
powerful Nick card system you are you
have hardware accelerators are you
having this pci extension slots and
you're having dedicated you know engines
to the extensions not to you and you'll
be fing up your storage as well so when
you when you have this one system in
you're making it be filled that's called
scaling up right so what are the pros
and cons is killing up your unit gets
very expensive right you're single
system gets really expensive and and
usually people do that when you're
trying to have a hybrid solution so what
happens and when you have a hardware
failure now you have bigger problems
because you're selling this hybrid
solution system and how the failure
could happen could result in bigger
problems what are the pros you have
reduced licensing costs yes you don't
have thousands and tough sister nodes
now you have probably had tens or maybe
maximum hundreds of nodes now so you
don't have as many licensing nodes right
and you don't have to amend multiple
servers each node can provide a complete
solution like I said that you can have a
hybrid solution and so now moving on to
scale out it's basically horizontal
scaling you're adding more servers you
could add commodity servers what are the
cons footprint yeah physical footprint
is a calm cons of that rain do you need
Rackspace you need all these and also
you need the fabric you know your
network needs to be stronger because a
network io is going to be could be your
bottleneck of course increased licensing
costs ring and then meant maintenance of
each node you know when you go when some
when you need to have an
8 upgrade or anything out you have to do
it for every note on that on your
cluster and then you overall power
requirements complete system has its own
power requirements as well what are the
pros each server can have less
processing units and let's member need
so like a commodity hardware and now you
know commodity hardware the definition
has changed over the years to write but
think of it as cheaper system right
again cheaper right it stays there and
then great scaling crying because you
know you have lesser processing units
and you keep adding these processing
units and usually your processor units
and memory is not the bottle like it's
your that network right and fault
tolerance because now you have these
cheaper systems you can use them for
redundancy providing high availability
and you can scale out as much as you
want alright moving on to Hadoop a quick
introduction what is hadoop so it's a
distributed cluster this is the basic
definition it's a distributed cluster
it's flexible it's scalable scalable
basically can add new nodes as you need
them right and then fault tolerant
that's mean that's that the main thing
about her to bits fault tolerant you
have multiple workers your data nodes
and you have data redundancy you know
and you have data loss you avoid data
loss through replication then let's talk
about data parallelism right again
because you have the data distribution
and you have application it that leads
to data parallelism and and then the
last one is the aggregate system
basically have an aggregate is simple
it's very simple aggregation system is
very simple it's liquor in the reducer
class you have the aggregate package so
you have max min or even the total so
and you can you can have your own
aggregate aggregation systems as well
you can actu'ly that so what is scale up
and scale out with Hadoop means again
scale up is vertical scaling you add
more resources so you're adding more RAM
your hard drives to take a joke data
note and skill out on the other hand is
basically adding new data nodes right
in the entire system new data notes do
it scallop is usually for us limited you
know in case if you're buying a big iron
server now it comes at a larger costs so
server g the cluster that we have that
the processor that we went with it
enables us to scale out you know there
is the and i have i have the system
ranking one of the security of its
systems right you that's a 32-bit server
it's an appliance right there so we have
because of because of the system that we
have it kind of gives us good
performance network i/o performance i'll
talk about all these things later on and
the physical size is amazing as well
alright moving on to the core components
so it's a hadoop one have has HDFS and
MapReduce and talk more about them in a
free moment and hadoop to has HD thurs
in new york so what chase HDFS Hadoop
distributed file system it's written in
Java and it's written for very large
files I'm not talking about terabytes
embankment petabytes of data and the
it's streaming data access he so it's
basically high throughput not really
good for low latency and then think
about it's it's simple currency model
right so you can write once and read
often and the way you write to it so
basically not good for multiple writers
and the way you write to append to the
end so with it that okay hi fault
tolerance and data loss prevention i'll
talk more about that in a moment
automatic eighth edition and computer
and removal of nodes okay all right so
let's move on to the next this is how a
basic architecture for HDFS looks like
you have the master slave configuration
you have the name node and then you have
data nodes on racks right so so what so
what is what is the name node it's the
manager so it has file permission
basically take skin test file
permissions modifications namespace what
what you need to know is
that this information is stored
persistently so it's on your local
drives as namespace image and edit logs
and I've highlighted here because when
you're designing when you're providing
the heap space for the name node you
want to you want to know what's its
persistent data is right you don't want
to know the size of that so so that's
why we I highlighted it over here and
we'll talk about this later when we're
going into design phase so and then name
node also has the knowledge of the
location of all blocks so basically it's
all the files and all the blocks and
this is not persistent it is stored in
RAM again important to know and and and
the reason you mean you don't need to
have big it's not the data itself it's
just the knowledge of the block so you
know where are the blocks that's all it
is and whenever you start a note anyway
you have to create you know happen you
set the name node up you going to create
it anyway so it doesn't need to be
persistent all right so I was talking
about you have the knowledge is in RAM
it's up about all the blocks and all the
files so what could happen if you have
very large number of files you basically
could my memory could become a limiting
factor right so so what is so what has
HDFS done this hdfs Federation
introduced initiative in Hadoop too and
what it does it has multiple name nodes
each serving a portion of your namespace
so what's names what's a namespace
volume its metadata for the namespace
and a block of pool and block pool right
so what happens is that these are
basically independent name nodes and
failure of one doesn't really affect the
other but the data nodes have to
register with these name notes all of
them right so to access the Federated
cluster you would usually the client
side will mount the tables and map file
paths to name nodes so this is all
easily this is documented and if you
want to use the HT
ufs Federation there you know feel free
to research that more alright so again
continuing with the name node name node
is also responsible for ensuring your
fault and you know fault tolerance and
providing protection against data loss
so what that so how does it do that so
you have something called Brock block
replication factor and the default is
three so if there's a block failure it
will result in the application of the
block it tries to maintain the
replication factor so any blood failure
will result in application of it now
that's that's about the data node what
about the name node itself you know name
noted stuff like i said in the
master-slave architecture there's single
name node and it could become a single
point of failure so usually people says
yeah I have a secondary name node but
secondary name node does not is not a
backup secondary no named it only holds
a snapshot of the primary name notes
directory information and and it
regularly syncs up with a primary name
node so it's always lacking and and and
if something happens you know it data
loss it's almost guaranteed so how do
you how do you avoid that how do you
kind of take care of not to have it not
have any de los you have something
called called having you know having
multiple file systems so you persistent
data that I spoke about earlier that
needs to be not only on your local file
system but also on NFS and the rights
are synchronous and atomic so so the way
you could you could you could do
something is that says say something
happens and in fact by the way a
secondary name notes runs on a separate
machine so so if something happens you
know the name notes meta data and files
on the NFS are copied over to your
secondary name node and you run you
secondary name known as your primary
mode so that kind of ensures that you
you know you you are protected against
data loss
okay let's move on to data nodes they
are but the data is stored on the file
system of each data node the contents of
the HDFS file and split into blocks so
what is a block a block is a file for a
data node and and then you have periodic
sink cups with the name node to provide
information on this blocks and you know
it's a also provides heartbeat saying
I'm alive I'm doing good so that way you
know you did our notice working alright
so moving on to the MapReduce component
it supports multiple languages and it's
basically the programming model and also
data processing so the compute the brain
right now for general and in also
generates data so it's distributed and
the inherently parallel the way it works
is that do some federal map tasks and
I'll talk about that very in the next
few slides and inherently parallel it's
fault-tolerant again the inherent design
is fault on makes it fault tolerant and
map and reduce our basic map is key
value pair and you have a map function
in a reduced function for for your is
basically an application when you're
writing your application you have the
map function in the reduce function so
what is a consist of a job would have
your input data and and then the program
itself the audio applications the
MapReduce application and then a
configuration a job configuration file
tell us what what needs to be done and
the two task you have the map tasks and
then you have the reduced tasks so how
does the architecture look it's again
the master slave act architecture you
have the job tracker which not only does
the resource management and it also does
manages the jobs themselves and then you
have these task trackers running on the
nodes and and they're on their racks
right one principle is that you know
moving data is very expensive so what
you do is you move your compute
do your data so usually if it you know
people run the task tracker on the data
nodes and and alt and that and that is a
consideration that we have to keep in
mind would be talking about a unit of
work in for task tracker as well and
i'll talk about that in a minute all
right so how does a MapReduce job look
like so you have the input like I said
and then you have your map tasks and
then there is the brain the shuffle and
sorting and then there's reduce which
produces its own output and this is
basically a simple MapReduce a
simplified MapReduce job so what does
the job tracker do it coordinates and
schedules so you use coordinating the
tasks you scheduling tasks on the test
trackers and it also gives it a track of
the progress like how far and what
happened and of course these guys the
test track would send periodic heartbeat
back so you know the task tracker is
doing good or not so the way it works is
that if you remember the previous line
the match nap task will produce its own
output and it's not really complete you
know it so so if something happens
during that during the in progress or or
when it's completed but it's not done
further out in that stage of the job it
can re-execute it so kind of you know
you're not worried about d computational
loss because you have so many tasks
going on that you can have its kind of
kind of fault tolerant in that sense so
and what happens if you have if
something is if a failure happens when
there's an impress reduced ask you can
really get that as well because we
haven't yet to produce the output so we
don't be not we haven't started in the
HDFS or whatever so we are we can still
do it because it's disposable so to
speak and the failure detection i spec i
mentioned is to heartbeat
and it's similar to what you I mentioned
about in HDFS all right what about the
task trackers the run tasks and there is
there's a unit of work and basically
it's it's basically of a split which is
a fixed size piece of input so there's
one map task for each split and so I was
talking about this you know bringing the
compute to the data so when you have
this the split and you want to work on a
data block so what would be the optimal
size it should be similar to your data
block right because usually you know if
it spans two blocks or whatever it'll be
unlikely that HDFS node will store both
the blocks you know we have both the
blocks so it's advisable to keep your
split size equal to your block signs and
like I have already mentioned the other
two MapReduce produces the intermediate
app an intermediate output its local
storage your final output is what you
reduce tasks produce and that goes to
HDFS storage now so Hadoop too has
something called yarn it's basically you
can have it you could call it yet
another resource negotiator or you could
call it recursively like yarn
application resource negotiator so the
issue was found out by yahoo I think and
they found that job tracker is not
scalable beyond 440,000 nodes not only
that MapReduce itself is a kind of a
rigid design and in you know so so so
kind of an you hit the limit they
decided to design the something called
the yarn in and yarn is basically splits
away the application management from the
resource management and so now your
MapReduce is just another application
and it can run on top of yarn and this
is kind of simplified way of looking at
yarn again so your resource managers up
there it's kind of master but there is
also a masters for each slave
and so you have basically node managers
on each slave and for every application
you have the application manager for
example if you had a MapReduce
application then you would have an
application manager for MapReduce
application so if you had each base
running so you have an application
manager for your H face and these node
managers are managing these the
configuration are in the execution
alright so moving on to designing a
minimum cluster so these are the
requirements that we had Sergey came
came up with and like security is an
attack line to we are advanced
extensible security security is very
important to us high availability is
very important to us more not only does
some in software look even in hardware
we apart make our switches are provide
high availability as well so what is
high availability it's kind of like a
guarantee of downtown so it's like the
five nines of availability and it
handles detection of failure elimination
of single point of failure and kind of
reliable crossover and so so i'll talk
about that i'll talk about security like
i said and i'll talk about how we scale
out with a 10 gig Ethernet and then
we'll talk about Hadoop on vm this is a
common deployment and I'll talk about
that and then there's future work
because we have the system that we have
the 64-bit solution though we have it
has s RI o on the trip itself so we want
to use that and there are multiple
research has done reached researchers is
done and and we're still reading about
that there's another thing that we have
we have an SDN solution and it'd be
awesome to integrate Hadoop with sdn and
there is a scheduler that there was a
recently paper published it's called the
bath scheduler bandwidth we're
scheduling for Sdn so I don't have time
to cover that here but
a link so you know so if you're
interested in that definitely go read
the paper so let's design so first we
want to talk about the memory for name
node so say you had its simplicity
simplistic you had hundred nodes with
about 400 gigs of disk space so you have
about forty thousand gigabytes your
brought block or application factors
three your block size default again 64
megabytes so you have about 190 to say
about 200k blocks now so even if you had
two million blocks you know you wouldn't
you still wouldn't you would need about
two gigs because the rule of thumb is
you want to have about 1000 mb per
million blocks right so so you're still
all right so what we did is that we went
with our a 32-bit solution and we just
used about about 2.5 gigs or so and
we're doing doing all right right now
alright so moving on to the high
availability requirement so earlier I
spoke about that they are loss
prevention with not just having the
secondary name known but also being able
to use a remote NFS right but that does
not guarantee high availability say if
your system was down you would still you
know kind of going from a cold start you
would still be looking about 30 minutes
you know and to a new name comes back
online so how do we talk how do we get
high availability in Hadoop to they have
introduced the concept of high
availability and basically you have one
it's a simp I availability concept is
the same its support now Hadoop two
supports it so what's the concept of
high availability you have an active
name node and our active node and then
you have a standby node right and the
way Hadoop does it it shares the Edit
log so so if you bring up say you didn't
have your foot for you
running cluster you bring up a standby
node it'll read your complete edit log
and then any new entries to it and the
data nodes now have the responsibility
that not only send it to the to the
active name node but also send the
information to the stand by name node
and remember because because you you
know data nodes the data block
information is in the in is in RAM right
so it's the duty of the data nodes to
send those reports to both and then they
have there's a conservative failover as
well so basically remember high
availability you have detection you have
you know you know what happens if it
fails right and you have to cut the
failover techniques in there too so the
way Hadoop doesn't have to doesn't it it
has failover support by zookeeper and it
it meant so so what happens is there's a
persistent session maintained in
zookeeper and when if the session is
lost do Cooper knows that something is
not right with you another thing okay so
let me just so now we didn't have it but
we have it with Hadoop too right ok so
another thing is the concept of fencing
and the way Hadoop does it I'm there
other ways to do it do but what the way
up the easiest is having an sss fence
right so when you have the ssh fins you
can basically ssh into it and kill it
kill it basically if you have a if you
have a problematic active node that you
know that you want to kill another way
of doing it is kind of called literally
called shoot the other note in the head
and what that means is you have a Power
Distribution unit and you kind of
forcibly kill it so that way you're the
name node that was not responding
doesn't come back online and kind of
have make you have kind of spawn your
copyright because now you've already
bought the standby up into actor you
don't want the the old active to come
back alive so that's very interesting
alright so moving on to security so the
security the way we have learned about
security is the target talk in terms of
the rings of Defense and layers of
Defense and and these are the layers and
I'm going to talk about what's natively
available in Hadoop itself so what's
attend what's available for
authentication authorization
accountability which is audit and data
protection at server to talk about data
/ in most places probably talk about
data protection for data at rest and
data in motion so talk about that as
well so authentication who are you right
so the way I do in native Native Hadoop
the most common form is carbo's Kerberos
and it establishes you know it prevents
impersonation and then there's a there's
like the host identity and services this
in very simplistic form this is what it
is you have the user to services
authentication basically the mat leg
MapReduce tasks they use delegation
tokens and then you have services to
services as a service so for example
your ssl certificates and then you have
services to services as a user such as
proxy user so this is the simplest
simplistic form of authentication
alright authorization what can you do
what is your authority alright so
basically again Hadoop in its native
Native warm provides access control list
and you can specify you know there's an
ACL true parameter you can set and then
you can specify and it's for MapReduce
jobs it's for yarn it's for HBase data
so you have the access control lists for
all those things of course you have the
at the very basic you have file
permissions right so it's similar to
unix user and group permissions and
stuff like that so so now you had the
authority what did you do
so Hadoop has has a way of tracking that
as well and you have the audit logs you
enable enable more information if you
want to you can do that in the
configuration then you have the history
you know so the history of all the jobs
that are running your cluster you have
the job track for that you have history
for job tracker you have history for
resource manager and then you have the
job history server as well so moving on
to data protection so in so for let's
talk about data in motion first so the
Kerberos itself has something called the
sasl protocol which is basically Chavez
simple authentication and security layer
and and it just just by having Kerberos
it gives you the protocol access and and
it's as simple as just turning it on to
privacy basically in that protocol you
have different levels and then I think
by default it's the first one which is
just authentication the second level is
integrity and then the third level is
privacy and and basically you know so so
just because you have it available you
get it and the way Pratt what does
privacy mean again it's it guarantees
that the data exchanged between the
client and the server and that's just
Kerberos itself right that in any
direction between the client and the
server will not be readable by a man in
the middle you always talk about the men
in the middle so it's not readable
because it's encrypted so it's simple
something simple like that but what
about the HDFS data so HDFS data has
something called the data transfer
protocol does not utilize SSL so guess
what there's no since you don't have
that you don't have quality of
protection which is what I was talking
about authentication integrity or
privacy it took two different levels so
so how does Hadoop achieve that it wraps
the dtp with the sasl hand shaking
and how do you enable that you use that
setting in hdfs site and you get the
quality of protection and then you can
set of course you have set it to privacy
already so you get privacy there so what
about HTTP HTTP over SSL name note for
example name no do I and the MapReduce
shuffle you have to enable that by using
by doing that DFS HTTPS enable and set
it to true in HDFS site you can also
enable two-way as a cell and then you
can do the same thing for MapReduce as
well and that's how you do that there
now encryption of data at rest so that's
good work to be done there so now you
have right now you have just using your
OS or hardware level of encryption like
you would do for any other thing now at
surgery we have something we have the
SEF cluster basically a SEF file storage
cluster and what we want to do and there
is paper published I have links to it
later about how you can replace the HDFS
them the file system itself would set
file system and and we're looking into
that we just so that's so these are
these are the references that I wanted
to provide right now because these are
what I use to learn about these things
and and there's the final link that
talks about the safe file storage file
system and then Hadoop itself so that's
something very interesting to us at
server G ok so moving on to the scale
out with 10 gig Ethernet this is a
serious this is cts 1000 right here this
is it's block diagram it's very
simplistic level again so we have two
dimms slots pro controller and we have
those 10 gig right here and all right so
things of importance are the sass which
is the HBA and you have the storage you
have external Jabbar and
I'm not going to talk more about this
I'm just this one just to show and this
is the processor that we're using it's a
free scale processor keep core IQ
processor and this is our 64,000 which
is the 64-bit solution it's it's a 240
240 240 processor and and then it has
the SR I oh like I said available in the
back plane itself and the block diagram
right here so the t42 40 block diagram
we don't have time to cover that so I'm
just going to skip that so what's what's
few in what's future for us so basically
data traffic on on Hadoop especially is
a big deal and for cts 1000 we have it
set up such that we use the 10 gig
Ethernet ports and on on the chip itself
we have something called GPA which is
data path acceleration architecture and
it allows for mind rate trans traffic
well in for our next generation the
64-bit solution we also have like I
mentioned the serial rapid I oh and and
there's there is lots of papers on that
as well and it's basically rapid i/o is
basically something that you may be used
to like InfiniBand or something and it's
a low latency high bandwidth
interconnect we haven't done anything
with that yet its future work this is a
again how it looks right now for us so
we have about five on each side like you
see this is a 64 32 bit solution we have
two switches again for high availability
one is a redundant so active and standby
and and that's that's a simplified
solution right there and for CT CTS
64,000 the 64-bit solutions it's similar
hi again hi available switches future
work like I said this is what we want to
get to we want to use this arango
switches again redundancy for high
availability and we want to scale you
know through the black back pain
switches
and get to something like this so
basically we'll have about fourteen
hundred and forty cores or linked to 20
gig of the SRO that I was talking about
and you have about four hops maximum
distance between any two processors and
then you have redundant paths because we
have 16 switches alright so designing a
minimum cluster Hadoop's hadoop on VMS
that's what we have right now so so why
do we were chua lies you want to gain
elasticity you can basically quickly
Chloe your cluster right and then system
utilization you want to have you know
push the maximum utilization out of your
cluster out of your system itself single
system actually better security because
you can separate your competition from
your data right so you provide security
multi-tenancy consolidation of multiple
clusters that's multiple multi-tenancy
and of course ease of deployment you
could just take a single image and clone
it and deploy it right so what do we
have right now this in a very simple
form you can run the MS and the way you
will run Williams because of the you
know bringing your compute to your data
node you want to run your task tracker
and your date a note on one week vm and
kind of keep on doing that on on every
vm but does that really you know how
it's not really elastic it's a you know
if you wanted to add another vm right
for example you would need new data
right because you already have kind of
you had pre configured your cluster
already and new data will be the need
for adding a new node new vm right and
also there is not really separation it
didn't give us the ice in the security
that i was talking about earlier there
is not any separation of computation
from data so a more elastic way would be
something like this where you have your
vm running your data node
and then you have your task trackers
running and then to get this is highly
multi-tenant as well right because you
get the isolation your tat and tasks
your computer is different than your
data and you have the the isolation that
we're talking about and then when you
adding you know the elasticity part of
it is you can add more tests trackers so
so what do you have to do here you have
to find a good balance how many tasks
trackers for data node is what you have
to look for so I don't know right now
we're working with 3 is 21 and and you
know of course there's more work to be
then to find out the optimal solution
but that's what it is right now so I'm
going to talk about few North with your
projects HBase it's written in Java and
it's basically ones on top of HDFS you
know HDFS provides streaming access so
what if you wanted to do a real-time
read right right or a random so
basically you can use each base when you
have millions or billions of rows and
data is stored in tables you know rows
and columns it's not our DBMS since no
SQL its distributed database and it
helps you with low latency fast look up
what would it SQL on Hadoop we have hive
and now more recently we have Impala so
basically you have the hive is high cool
ql and it only uses MapReduce and you
know it's an analysis of any data stored
in the HDFS and compatible file systems
Impala is SQL query engine no it doesn't
have MapReduce only demons and slave
nodes and its interactive analysis for
any data store in HDFS or HBase
I'm going to talk about Knox gateway
it's very very upcoming project it
started as a parameter security and
kindest these are slides that I have
gotten from this presentation that was
done on Hadoop at hadoop summit and
larry provided these slides to me so
have a link to the slide that gives more
details and links with a SlideShare that
gives more details on these slides and I
just wanted to mention it very quickly
it had exposes REST API as an HTTP and
and basically it has an end security
simplified access and please go ahead
and refer to the link that I have
earlier because it talks a lot about all
these things in details I've just
captured the slides are just copied
actually the slides right here and
enterprise integration is also a feature
centralized control very very important
now for us at server G we since we're
using freescale which is a PowerPC for a
long time we didn't have the c2 compiler
and so we're thankful to OpenJDK PPC
export and I you know they have they
have done a great job and we use there
for a 64-bit solution we use their the
compiler what else do we do at Sergey we
help fiscal drive the IOT it's basically
a truck and literally a truck and it has
all these solutions and it's it's going
to be on the road for two years if you
want to have find more information on
that you can go to that link IOT
freescale com basically it's internet of
tomorrow and then they have cool
solutions there so if you have any
questions please feel free to email us I
was I had one backup slide that I you
know kind of talking about how do you
know your cluster so so think about your
hardware aspect and your software
aspects a hardware aspect would be power
usage
you know like you know how efficient is
your cluster space you know high density
like Nick for example right here cost
not just the processor but you're you
know SSDs versus hdds and Nick cards and
everything compatibility device drivers
firmware OS and bandwidth how much been
memory and IO itself like I said you
know Alex rapid IL signor roberto is
really low latency high bandwidth so
what about the software aspect you don't
want you know you want to have security
as your major major major thing be in
now it work for your cluster so virus
scanning you know network monitors
awarding man-in-the-middle encryptions
is over that so compatibility again your
solution for your hardware and os you
should be compatible or anything you
can't just have it as a disjoint
solution it has to be compatible with
what you are what you are running on and
what you're offering super ability again
is very important as well so that's all
I have thank you all for listening and
if you have any questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>