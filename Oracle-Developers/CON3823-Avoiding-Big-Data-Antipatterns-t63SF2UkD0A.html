<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CON3823   Avoiding Big Data Antipatterns | Coder Coacher - Coaching Coders</title><meta content="CON3823   Avoiding Big Data Antipatterns - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>CON3823   Avoiding Big Data Antipatterns</b></h2><h5 class="post__date">2015-12-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/t63SF2UkD0A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right well welcome everyone
hopefully you can all hear me in the
back there everyone good sound wise okay
great I'm so welcome thanks very much
for coming and making a photo session
I'm going to talk about Big Data
anti-patterns and I'm going to put some
the slides later tonight am so if you
want to you know follow me on twitter
than you can you'll have the link to the
slides later and by any case i'm a
software engineer I've been working
monday super systems now for a few years
i have a book called her dip in practice
i kind of contain some real-world kind
of recipes for working with Hadoop and i
have a blog that i've been pretty bad
about maintaining but maybe i'll get
back to writing some articles at some
point well in case and why should we
care but anti-patterns right and
specifically big big Atlanta Falcons
well this is an example of a real world
antepartum and action so you know here
it's kind of common to overload things
beyond their kind of maximum capacity
and then you have undesired outcomes
like this by which we all can comply
agrees play a bad thing to happen so by
understanding anti-patterns we're not
only learn about practices to avoid but
in the process we learn more about how
our tools work and including their
limitations so here's our agenda it's
kind of a mixed bag of items and there's
some higher-level anti-patterns and
including there'll be some anti-patterns
we're going to look in a look at some
Java code look at some algorithms look
at some system in terms as well so
hopefully a little bit of everyone here
well they can a general theme as well
identify an anti-pattern look at you
know why they shoot you know I kind of a
bad in practice and then hopefully look
at some mitigations that will help you
avoid them and I'm going to try and make
this kind of light harder so it's kind
of two main actors in this role you've
got Alex who looks a little like me but
it's not me even though we share the
same name I promise and he's kind of new
to big data and he's got a lot of
enthusiasm and he kind of reads a lot of
blogs but all the cool kids are doing
and then it's Jade like she's a seasoned
professional she been running big data
applications successful in production
for quite a few years so let's look our
first aunty part
and it's called its big data and this
was a little tongue-in-cheek and
light-hearted but it's something I have
encountered em quite a few times so here
we have Alex saying you know I have some
user data and I want to calculate some
basic statistics of that user data I
need a high do cluster to do this right
and Jade is asking well how much data
are we talking about here he's like it's
big data is 20 gigs so what's the
problem I mean this one's kind of
apparent to us all but there's some
Bible lessons here so as you're just
kind of punching out you think you have
big data but you really don't this is
big data this is your scale of data it's
a little different and to kind of bring
the hook point home I'm gonna run a
little quiz now so what do you think is
a maximum ram a single rack mounted
server can contain these days in our
data center who thinks is 256 gigs no
one okay how about 5 12 gigs no one okay
one terabyte okay there's a few well
you're all wrong that was a trick quiz
so this is kind of neat little a website
called your data fits in rom-com week
and you can put on your data size and
you can keep bumping up and up and what
ends up happening is that you can buy
servers now and the commodity servers
that have six tera bytes of RAM that's
amazing and the point here is that
there's a lot you can do with a single
host you don't need a cluster and
especially your data set is not huge and
there's another problem here you know
Alex's assuming he needs to build and
use a Hadoop cluster right if you're new
to big data and especially Hadoop which
is complex you're spending all your time
and a place in this guy trying to set up
and configure and make tuning it to
their kind of work these trying to do
and this takes time it is time like
you're taking away from actually trying
to solve your problem here so you know
the kind of bottom line here is keep a
simple tools like my sequel postgres
python matlab are the kind of data
scientist you know kind of standard
talkers but gray on decently large data
sets for sure 20 gigs
simplify your children you know don't be
kind of you know the new shiny thing
isn't always the best thing to be doing
with with the kind of work your data
size that you're working with okay so
the next one is called a single tool for
the job so often times we've become
familiar with a tool and you know and
big data just like anything else you
kind of start suffering from the big
hammer syndrome right like that you look
around you and all the problems start
looking like that Neal right you've got
a1 hamie let's go and I can solve that
problem and it's not surprising we're
like how many how many tools can we all
be experts at in a kind of understanding
inside out not many one or two for lucky
but the problem here is most big data
tools I cannot design to do one and
maybe two things well and then if you
compare that number with the various
types of workloads that we're trying to
do today there's a disconnect right the
type of work we need to do is kind of
extensive and varied we talked you know
range some low latency look up to batch
processing to graph processing to
analytics to full scans to et al I mean
there's no way that you know these
bigoted one big data tool can kind of do
all these things and it's kind of a
shame because no old world like 10 15
years ago we only had like one true
right and you know yeah I had its
limitations but we all kind of experts
at using a treme all new we're kind of
started to fall apart so this new Big
Data world is a little different this
your number here is kind of overwhelming
each category has like five or more
tools and they all appear to be near
identical there's a new tool every day
and you can let you spend all your time
just reading and researching the space
trying to forget what's going to work
best for you so we need a way to kind of
cut through the semester pig in the
right tools for you and this is kind of
no brainer but you know unfortunately is
no magic bullet here you can only define
you know the best in class for the type
of the work that you kind of do and best
in the class doesn't mean you a whore in
fact often is quite the opposite so
here's my kind of punch list so you know
before you kind of start reading this
and disagreeing with me on various kind
items here this is my opinion it
this kind of systems everyone and
production successfully at scale for the
kind of workload that I'm doing with
them so low latency lookups Cassandra
works very well memcached has been
around for ages it does a great job near
real-time processing and storm is great
now there's a bunch of other systems
either like new systems like Spock
screaming and Sam's and some of these
other things storm does a pretty good
job at scaling and my experience and
four interactive analytics this may be
again a bit of a surprise but vertica
teradata that kind of mainstays of data
warehousing they've been continuing to
evolve they do a pretty good job for
data scientists and people who need kind
of low latency quick access to a lot of
data and then fulfill scans etl batch
process and storing kind of system of
record data who do is this is kind of
its main.css what does really well and
then lastly for data moving moving data
between data centers between systems
Kafka is kind of where you want to be at
and I'm going to be diving into most of
them kind of during the kind of the
course of this presentation but I can I
take away his unfortunate there is no
one tool I can do all and don't believe
everything you hear i read you have to
verify for yourself what tool is going
to work for you and even in about how
your test don't spend too much time
money i can a political capital
investing and something that ultimately
may not work for you okay so let's talk
about data integration so here are two
saying i've got a bunch of clickstream
data and my application is generating
and i need to move into Hadoop she's
like great I know how to do that my
application is producing logs that's
where all my data is I'm going to write
a little etl thing that's going to load
into Hadoop and I'm done now tomorrow he
needs to use that same data and a
streaming system like storm she's like
okay well I've already developed it's
kind of one etl pipelines I'm going to
kind of leave that alone so I can either
write to maybe something like JMS my
application and then can a storm can
adjust that or my Hadoop loader can
become ain't it to do that can the side
effect right to sue JMS as well so
what's the problem here there's a
pattern emerging you can
engineering one of specific pipelines
and we're ending up with is a rigid and
inflexible kind of architecture and
solution I can guarantee if you went to
your CTO with this diagram that our
conversations are going to end well so
you know what we really need is
something generic a central data
repository in pipeline that isolates
consumers from the source and gives new
users and sim sisters and allows us to
kind of add new users and system and
consumers without any work to raise the
system and this is really bad cough that
comes into play and you know Kafka was
created by linkedin because they had
this exact same problem they had these
disparate systems systems and production
and the building is one of data kind of
integration pipelines they realized they
had anti-pattern in play and they kind
of bill at Kafka to kind of address that
you may be wondering okay so it was a
big deal but Kafka isn't it just yet
another JMS you know why invent
something you write so what's really you
know am the kind of the hallmark of
Kafka is really is through so Kafka but
year maybe a little more than a year ago
run from benchmarks on a three node cafe
cluster and they were to get producers
writing at two million over 2 million
TPS and consumers again over to mine TPS
filling from that renewed at em Kafka
cluster with average kind of latency at
the end of two milliseconds I mean this
is quite spectacular given but harder
wasn't kind of you know some high-end
hardware anything and one of the ways
that Kafka scales so well is that it
keeps things really simple I usually I
can a log based app and only am file
storage on disk which is kind of very
simple efficient way to write data out
and then as a side effect of that which
is a huge benefit sure they can leverage
open systems page cache so the producers
writing data to disk the page cache is
kind of costume as it goes along and
assuming that your consumers can keep up
with a producer it's hidden in a memory
in our desk so there are a few things to
look out for here first of all it does
use zookeeper to maintain some state and
you know this is which is into this is
yet another big data tool they need me
to learn and
to run and operate there's no ideal and
and then that whole back by nice thing
about the operating system the page
cache if you have a consumer that comes
along and replace data so it's going to
it's going to die cache miss and have to
hit desk then that's going to that disk
i/o is slow slow and then that's going
to screw up the page cache for all other
consumers I kind of reading able to keep
up a producer so that's a challenge and
then finally security is kind of one of
the things that isn't quite there yet so
if you're an environment where you're
dealing with sensitive information you
know maybe Kafka is not going to work
for you so the takeaway here is don't
write your own data integration it's you
know it's going to be taking away too
much time and again take away from
solving the problem solving and cough
cough is great as a kind of light weight
fast scalable way of kind of moving data
around your organization okay so the
next one is called full scans for the
win so Alex here saying I heard Hadoop
was designed to about this huge data
volumes great I'm gonna stick my date on
Hadoop and then I'm gonna run some
drawings something like slick star from
some huge table join or some other huge
table and do some interesting kind of
analytics type work so what's the
problem here well first of all alex is
right i mean hadoop was designed to
efficiently scan over large amounts of
data and the way it does is it takes
files like any other regular file system
I can stitch them up into blogs but
these blocks are larger hundreds of
Meg's and size and they get kind of
distributed around the cluster and also
replicated so you have n copies of each
block and your cluster and what that
means is a schedulers can actually have
a lot of flexibility when they're
pushing work out to the cluster and data
locality is something you hear a lot
about in Hadoop and it's basically the
concept where you push work to the data
is located so that data breeds are from
local node as opposed to other network
which is expensive so despite Hadoop
having all these goodies it doesn't mean
we can or kind of should forget about
all these optimizations we've learned
working with large data sets and other
systems desk I always still
slow is a biggest bottleneck we have
today and then typically on who do
pretend to run on spinning hard drives
or not an SSD so it's really kind of
where the most a lot of latency comes
entire systems so they can act you know
the thing to do here is to partition the
data horizontal if i petitioned data
like we used to do with our databases
right so you think about the majority to
the ways by which the data is going to
be accessed and any partition your data
on disk and that way they you know if
you if you aren't sure from our set day
is a great way to get going because for
the most part you've got years worth of
data chances are you only want to kind
of look up a subsection of that and then
it's kind of a no-brainer but you know
make sure that you include that in your
and your kind of where clause so that
your the query optimizer can actually
dig that partition iron or scan other
the whole data send now another
interesting thing here is projections so
projections will limit them of columns
are you're working with why why does
this matter well can a modern storage
engines and formats are pretty
sophisticated these days they can
optimize a read path to kind of skip
over chunks of data that's not being
used in the query and that means there's
less data with long to read path fewer
objects being unmarked and there's also
less data that's being sent over the
network when you have a distributed
function like a shuffle or join I need
to perform the other thing that's kind
of em problematic is the harsh join so
has joins in Hadoop work just like the
work in other databases which is to say
they'll super inefficient so imagine
you've got two data sets that you want
to join together you take the drawing
key you hash it you do a modulo of a
number of destination processes then you
send both datasets over the network for
the actual I'm going to happen under and
on the kind of receiving side so the
problem here which is with large joy and
it's a lot of data that's going across a
wire and never cayo is kind of the
slowest I mean obviously slower than and
disk i/o for for the most part I'm even
with a lot of the fast networks we have
our data centers these days and this is
yet another reason why filters
partitions predict as all the things
we've mentioned up until now is so
important to kind of reduce amount of
data that you
no shipping around so the thing we want
to do here is I'm out join now if I most
trying to drive to work both datasets
have to be sorted on a joint p but if
you do this this makes a join you know
stupid simple all you got to do is
stream all the data sets and do it in
line marriage it's very low overhead and
it's way more efficient in the hash join
because for the hash join has got a sort
both datasets push it over the network
marriage m and only then it can do the
joint so it's doing a whole lot more
than a marriage strong so to enable our
you're gonna have to have these bucketed
sorted data sets and then tell your kind
of creepy engine that you want to
actually you need to explicitly tell
most clearly engines that you need to do
I saw a marriage bucket drawing and I've
kind of included some configs here for
hive and finally you consider using a
corner data format like parking so when
you work with M data where you've got a
lot of columns and you're typically only
using a handful of them common data
formats and agree because they store all
the column values contiguously on desk
so this allows the column the storage
engines mobility to down to dynamically
pick the best compression codec for the
type of column and that you're actually
storing so you can actually get way
better and confessional with these
common data format sneak on with kind of
raw primary data data structures so you
know the summary I'm this is all kind of
common sense stuff but petitioning
filtering projecting your data these are
all things that go a long way to kind of
and make your work will be more
efficient taking things a step further
you've got kind of sort merge sort merge
bucket joins and then consider columnar
em data formats okay let's talk about
tombstones so remember wondering what is
a tombstone go do with the big data
right that's kind of weird so let's
let's have a look so Alex here says he
needs a fast persistent q he already has
Cassandra running and production
cassandra is fast and persistent so he's
going to use that so what is cassandra
cassandra is on no sequel database it's
modeled after amazon dynamo on google
big table kind of designed for
scalability
fast reads and writes and it kind of
favors availability / and consistency in
the cap theorem and the nice thing about
cassandra is you can take a cluster and
you can actually divide up into m
sections and enter kind of into regions
so that you can have like if you've got
east coast or west coast data center
half recommitting one side of the
country another half to meet other side
of the country and the Cassandra can
imagine takes care of keeping all the
data and sank and this is huge if low
latency and availability I kind of
critical parts of em your application
and the thing I love about Cassandra is
that most no sequel databases don't do
this and for my kind of work this is
something that I need so what's the
problem with using cassandra is a cute
well Q is typically need to support high
level of reads and writes so when you
delete it and Cassandra it doesn't
immediately remove that data from
persistent storage instead it performs a
kind of soft delete wear marks delete
records with things called tombstones so
this scheme allows for very fast greens
which is kind of you know great for
Cassandra but it come out I come out of
cost so the tombstones need to when you
do run when you do scans over records
looking for data these tombstones
actually incur and expands an overhead
when you're doing these reeds and in
extreme cases like use there could be
hundreds of thousands or millions or
tombstones and that kind of getting and
we're very varied and what you expect it
to be millisecond lookups and I are
taking the seconds and you know another
thing that's kind of surprising
cassandra is that deleted data by
default stays ran for 10 days which is a
long time right now there's good reasons
for this and the primary one is called
the kind of reappearing delete um I'm
going to call a syndrome but I would
suggest you do a Google I don't have
time to unfortunate go into the details
but it's kind of just like it's worth
just in a google search for reappearing
delete some cast and when you can reel
it about the background there and why
you know 10 days is how long deleted
atticus kept around four but in any case
it's a long time for they can these two
that tombstone data to kind of Gator in
any way of your reads so clearly you
know alex is norm using cut you know
Cassandra you
using Kafka that's pretty clear but if
you really have to use Cassandra oh if
you if you just can't get away from the
fact that you need delete heavier
workloads there's a few kind of trick so
you can kind of employ to kind of void
that from happening so what you can do
is they can basically de faire de leaves
so an acute Apes now you'd have a table
and Cassandra we keep em a list of all
consumers and their offset in a queue
and then you'd have these kind of
buckets where each but would represent
you know a range of time or you know
some some number of M elements I've been
a decir q and every time that wintertime
gets em again exceeded then you can
create another bucket and now you can
build some intelligence to say okay I'm
only I'm only going to delete all
buckets when I know all my consumers
have kind of moved beyond the hunt and
this is kind of similar to how Kafka
works internally as well so am the
summary here is going to be careful with
Cassandra with bottles are delete heavy
and design your schema and read patterns
to avoid hitting these these tombstones
okay so let's now talk about Java Andres
built in collections and kind of full
disclosure I'm not a mathematician it's
been a long time since I've been in
school studying that stuff so bear with
me you've been warned but Alex you're
saying I want to count the distinct
number of users are viewed a tweet right
she's like he cracks I his ID and says
okay this is simple I've got a method
called count distinct users there's an
iterable of strings representing my user
ID is being sent entered I'm going to
create a hash said it should be lambda
but it's no I'm going to iterate through
these users i'm going to add them to my
heart say i'm going to return back the
size so what's the problem with us well
this guy is a problem with this and you
know one of the biggest challenges when
building big data apps as an impact of
internet scale and specifically data
skew which is called be caused by people
like mr. Beeba over here so ninety-nine
point nine nine nine nine nine percent
your tweets are just gonna have a
handful of views but it's gonna be
outliers like this guy whose tweets will
be having millions of views so have a
little pop quiz for you guys again so
he's used hash said so what do you guys
think is used beneath the scenes in a
hash set so how many people think so
assuming is of type K right as a generic
how many people think it's an array of
type Kane nobody okay good how many
people think is maybe some sort of kind
of wrap around the key but it still I'm
ultimately an array okay good how many
people think it's a hashmap ok and then
how many people think is a treemap ok
there's a few so you have all mostly
right good job I'm impressed so yes
indeed hash set uses hashmap internally
here we're wondering like what's a big
deal and what's the big deal about doing
that well hash map has a pretty big
memory footprint and it's a change hash
table implementation and these map
entries I can heavyweight things that
kind of get dragged around and a half
send and if some math you can do so if
we assume that the average user name is
six characters long that's about 64
bytes so you already get a lot of
explosion just using that kind of Java
strings and then there's a kind of
formula that you can use to forget how
much space you're my stairs occupying so
how many items you've added to your hash
say is x 32 buttons and then the Ray
internally is x 4 bytes and they're both
added together so what that means and
that effect here is that if you've got
10 million users in the average length
of the username as 64 m six characters
that's one gigabyte of data
so that's obviously expensive and you
know takes up a lot of space so you know
there's a kind of the algorithm called
hyper log log that helps kind of em
solve this problem and it's that s that
kind of gives you an approximate count
so it's not going to be precise can see
not going to want to use this for
financial data but it works really well
so just to give you a sense 1 billion
distinct elements occupies 1.5 kilo bits
of memory so thank back to the hash set
right 10 million em occupied one gang
and now we're talking about tours are
mounted more occupying 1.5 killer birds
right that's pretty as a pretty nice
savings IAM so to give you a sense of
how it works beneath the scenes imagine
that you wanted the estimate number of
times a coin was flipped right so one
way that we could do that is to kind of
count the longest number of runs of a
head that we saw in a row so for example
if you're flipping a coin and you tell
me that you've seen maximum of two heads
in a row I'm going to guess you've not
been flipping a coin for too long but
even if you've been flipping a coin you
tell me when you've seen one hundred ten
hundred tails in a row then you've been
flipping a coin for the whole day and
the way hyper log log box is kind of the
same thing the same principle apart my
pleasure to hash chords so good hash
function is one where the base of the
output value are independent and each
have an equal probability of occurring
kind of like a can like flipping a coin
so this random uniform can distribution
allows you to extract a probable
probability distribution of our likely
occurrence so everywhere to hash eight
unique things then one of them will
likely start with 0 0 1 then we can
invert this so I've been a sequence of
hashes the maximum number of leading
zeros is too then there's a good
probability that we've seen eight things
now to can improve on this estimate the
hyper log log algorithm stores many
estimators instead of just one and then
kind of averages a result and this is
kind of called stochastic averaging but
what happens is we take the hash code we
kind of split into two and the least of
least significant bits are used as I
kind of offset into this register and in
the remainder we take we can't
of leading zeros and we add one and then
we take out the maximum of that and
whatever is already in a register and
store that value back in the register
and then we take our harmonic mean of
all these values kind of a fancy mean
and that basically ends up being the
hype log log kind of estimated count for
for your elements it's coming the
algorithm now there's a there's a really
nice implementation of that from new
star called Java hll and the thing I
like about this is that it's a curative
small cardinalities because it maintains
a hash data structure for kind of small
accounts and then it transparently turns
transforms it to hyper log log when a
certain threshold is met reached there's
a couple of other things that may be
interesting you so count min sketch is
kind of an approximate counting multi
map so we can support top K and in bloom
filter is an approximate set membership
data structure which is used in
Cassandra and each base and other
cameras sequel databases as a
optimization to determine whether or not
data may exist on on disk or not so can
the summary here is data skewers of
reality you know whatever justin bieber
and your data don't use Java built in
collections if you're dealing with you
know super large accounts and then you
know look at some of these approximate
is to make some algorithms if you indeed
dealing at that kind of scale so I'm
going to back away from the math now and
hope I didn't offend anyone and a
process here okay so this is the last
anti-pattern and is called it's open so
Alex he is releasing a new kind of em
big data application into production
he's done the prototyping he's done the
cording he's on the testing penis
testing monitoring he's he's ready to
ship so what's the problem here so the
problem is if imagine someone today had
access to your data center how to cure
would your data be and you know how sick
you were to be from not only someone
stealing it but corrupting our diely the
net and the problem with a lot of big
data tools is that there are so complex
but we tend to spend our time kind of
wrestling with them to design our way
out of their limitations and we kind of
forget the security is as important as
anything else if not more important now
the problem is a lot
the bigoted tools i just have little on
no security bill n or so complex that it
just kind of you know is intimidating
and non enable known bothers and turning
it on and then again it's kind of a
shame because no old-world things are
like super simple our databases hard
authentication authorization ACLs bill
and and we even employed people to kind
of manage them and make sure that these
policies were kept and current up to the
end and Alex me think what security is
not my job right why should I be doing
this stuff but you know you you know
hopefully you have some info SEC people
in your organization and up for going to
disagree and with you so the important
questions asked you are things like is
my data encrypted innocent emotion on
the ether is that on encrypted on desk
what can I HCL's on place to kind of
make sure that only the right people
have access to the right and data and as
this stuff turned on by default so
here's the kind of little chart kind of
comparing various some of the kind of
tools I've talked about so far and
unsurprisingly Oracle gets full stars
and run oracle roles we have to say that
but um you know it's been around for a
while so we kind of expect it to you
know get Phil stars and it does here and
Hadoop does pretty well here I'm going
to score all the kind of basic and
security things but it's really complex
to set up install so as a result there's
a lot of people running and production
that just don't have it turned on which
is kind of scary and Cassandra again
does a pretty good job it's pretty easy
to use but it's not turned on by default
m'sieu keeper not the best it doesn't
have a rest encryption and then kefka's
kind of outlier where it really you know
it was merely purely here and you've got
a couple of choices of kafka right you
can either decide to kind of encrypt
your data apply it to standing it to
Kafka and then deal with you know
unpacking on the other end you know you
live with the fact that your data is not
protected or use some other tool that
you know it's maybe going to give you
and sort of the security kind of things
that you need so takeaway here is kind
of in able security please it's an
important part of can evaluating and a
tool and you know if your product or
kind of em vendor doesn't support it how
a discussion with them
assembling is going to appear on the
roadmap okay so we're done I'm so the
kind of general themes here you know
don't assume that a big data technology
is gonna work for your specific use case
kind of test out your own hardware and
data and that's kind of common sense and
be wary of a new hotness I mean all the
vendors you know it all claim that they
can scale to vast amount of data but
you'll be surprised when you ask them
the hard creation of okay what kind of
em scale testing have you done in-house
they'll often times we're talking about
hundreds of gigs of data which to me
isn't big data so you know ask hard
questions about your vendor and if they
say they haven't scaled at scale test
let's kill asked him why and then to
make sure that kind of em scale testing
is a required part of your kind of you
know go to production plan that's it
thank you for your time everyone any
questions yes sir HBase well I'm so I've
kind of highlighted tools of hard
experience with here and I've heard
horror stories about a space by five
horror stories about everything you know
and i think it's now am I thank few
years ago had its problems and now it's
a lot more I'm hearing good things about
it I can't personally talked about it
with a hot dog I experience
unfortunately yes sir
so you asking a question but I'm keeping
around system I record data so I mean so
one of the one of the things that Hadoop
is really good at doing is keeping
around a lot of data i can reliably you
know back in the day like 10 years ago
we were all kind of bound by we're all
working on single machines and we were
kind of we didn't have much disk space
so we kind of naturally would have this
half of deleting data so if you know
given that you don't want to be doing
that with your kind of valuable things
like your your your logs and things like
that I mean Hadoop is probably the
cheapest way today that you can store
vast amount of information reliably and
securely so when I say can assist my
record data I mean kind of you know once
did it becomes too big of em all
TP databases and even em all up
databases you know Hadoop I think is a
de facto way to kind of store data
reliably over a long period of time yes
the question is any thoughts about
MongoDB well there's a lot of diverging
opinions on I think MongoDB there's a
fellow out there that there's a lot of
kind of consistency testing with some of
these kind of em no sequel tools and
Jasmine if you do a search call me
maybe and mongodb and he did a search on
that and found that the results were
kind of interesting I'll just clear out
the hand yes sir
so the question was you know Cassandra
does and they're kind of behind it seems
data replication between the different
regions and have I seen any issues with
and that across the network and I
haven't but then I kind of the networks
that we tend to deal with are pretty far
pipes i guess i've not heard a whole lot
of that either and beyond the cases of
we've had so i would be a great question
to ask maybe unlike sandra user group
okay well thank you everyone preciate
your time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>