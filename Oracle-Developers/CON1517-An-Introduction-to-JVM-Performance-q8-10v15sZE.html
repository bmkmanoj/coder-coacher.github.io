<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CON1517 An Introduction to JVM Performance | Coder Coacher - Coaching Coders</title><meta content="CON1517 An Introduction to JVM Performance - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>CON1517 An Introduction to JVM Performance</b></h2><h5 class="post__date">2015-12-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/q8-10v15sZE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello and welcome thanks for for coming
that late on a Wednesday I'm gonna weigh
competing with Elton John right don't
get to say that every day so thanks for
for coming to the session I pretty sure
you would like this and this is
typically a very popular topic I'm
talking about Java performance today and
I'll try to do it on a very basic level
without looking at machine code or
without looking at byte code and and not
because I don't think that you wouldn't
get it but the simple thing is that you
have to spend some time to get used to
these these concepts and and actually
they're not important if we talk about
performance what I'm trying to do today
here is that I'll look through some
concepts that are important it's a very
crucial for the for the hot spot Java
Virtual Machine when the java virtual
machine runs your programs and hopefully
you will learn how you can apply some
rules to to your daily working to help
the JVM to optimize your code because
ideally you know ideally your Java to a
machine would be this magical box we can
put in any source code or bytecode rada
and then out comes super performance
code super performant machine code and
no matter what you do it will run very
fast and that's unfortunately not true
because it's impossible to implement
right so it often helps to write code in
certain way to help the JVM to let you
help you write I have a disclaimer slide
here that everything I say today might
not be true in the future at least right
so so of course I do my best you can
always be wrong but what we talked about
today is first of all it's only true for
the hotspot virtual machine that's what
is running in the Oracle JDK and in the
open JDK right and I think most of you
work with hotspot but there are many
many Java tour machines there are for
example the dalvik virtual machine is a
very different virtual machine for Java
and it has different performance metrics
so unfortunately it's very difficult to
write a program that's both optimized
for for Java
I like default Java and for Android
right and also these things change we
talk about implementation details today
that's something that isn't specified
anywhere there's no
the java virtual machine specification
doesn't say you have to optimize code in
a certain way right you can do whatever
you want you can write your own virtual
machine and make it to whatever you want
as long as it'll base certain rules but
performance is not a criteria here so so
please keep that in mind because if you
go and stack overflow for example and
you ask is reflection slow that's like
one of the typical urban myth that is
still around right reflection is not
slow that spoiler to the rest of the
converter the rest of the session but it
was slow many many years ago Java 1/3
each or 1/4 they didn't have to the
heavy weights that the java virtual
machine has today right so so please
keep that in mind for for the future
life ok so what we want to look at first
is how a Java program is run and you
probably know that already but just to
be all in the same page let's just
quickly walk through it right so we all
write for example Java or any other jvm
language and then we have a compiler in
this case Java C which translates source
code to so-called Java bytecode and if
you came to my session on Monday you
heard a bit about my code and the thing
with byte code is that it is fairly
similar to the Java code the Java C
compiler that you all use for compiling
your java program is not an optimizing
compiler it just close to no
optimization right and the thing is that
it doesn't need to the Java Virtual
Machine is a way more powerful compiler
where this bytecode is later either
interpreted or at some point it is
translated into from byte code is
interpreted to machine code and even
after the translation to machine code we
still have a step of optimization
because modern processors they have
their own optimizations layers right a
processor is for example reading
prefetching memory right if you if it
wants to if you in a your Java program
read a field for example what you do is
you read data from somewhere in your
memory the processor might be smart
enough to figure out it at some point
needs this this information and my just
loaded already in some cash right so we
have two layers of optimization the JVM
self the just-in-time compiler and the
processor which is doing some
optimizations and developers don't like
to
into these layers because it quickly
gets a bit complex and complicated and
it's also difficult to obtain this this
sort of information it's not really
exposed to us we can look at source code
we can even look at byte code but
already obtaining machine code from a
JVM for the programs that we write is
this rather difficult to compare to two
other things right and today I want to
break through this layer with you and
look into the JVM layer and the
processor layer right and also one thing
I should mention is like Scala for
example if you compile Scala code the
Scala compiler might have to do certain
optimizations they might have to do
certain performance optimizations
because the JVM as I mentioned before
isn't this magical box where you can put
in anything the JVM itself recognizes
certain patterns and makes use of them
and it's poets it's capable of
transforming them in a more efficient
manner if they resemble the output of
the Java compiler that's what Scala
necess not necessarily resembles to tail
optimization tail call optimization is
an example the JVM doesn't support ya
resolving recursive loops right this is
a very functional program concept so you
have to basically translate this because
the JVM doesn't know how to do it so
it's the responsibility of the Scala
compiler to do that right okay so once
again this is hotspot how does hotspot
run your code when you start a program
and the JVM has three ways of running it
it starts out with an interpreted mode
and then it has two different compilers
these compilers were formally known as
the client and the server compiler and
you could actually specify which
compiler you would want to use by by a
parameter which was - client - server
modern JVMs don't do that anymore
modern try VM since Java eight I think
it's default you so called th
compilation
so both compilers are used at the same
time so the JVM in a way looks at your
code and decides if it wants to step
through your program or if it wants to
translate your program into machine code
right and most programs start out to run
in interpreted mode
the thing about hot spot is it's a
method based compilation dalvik for
example on Android is not compiling
methods as one piece of code it's
compiling loops in a way so it's not
important I'm not going to talk about
Android much despite I have mentioned it
twice already
but as I said before this is this is hot
spot right so we start out in
interpreted Boden interpreted node is
so-called level zero that's if you look
at compilation blocks and you read level
0 that means interpretation so the c1
compiler has three different modes it
has level 1 2 and 3 and the hotspot
virtual machine is doing compilation at
runtime because it uses profiling and
profiling means that the hotspot virtual
machine looks at your code it looks how
it looks like when it when it is
actually run and then based on the
experience of running your code for a
while it will basically get to know your
code in a way right how it's actually
behaving and then it will optimize based
on that information and it's a very
powerful concept and if you write a
program in C for example then you
compile it statically you compile it
once and only once and then you have
translated it and this basically doesn't
allow certain optimizations the C
compiler when it translates something
has to be active absolutely sure that
this one rule that it applies this one
optimization doesn't ruin your program
in a way the hotspot virtual machine is
able to wait until runtime and look
what's actually happening and then judge
based on that information and as a
matter of fact C one is collecting this
information and it then later gives it
to level 4 to see true compiler which
uses the created profiles to do
optimizations based on that so what this
implies is that C 1 on level 2 and 3
collects information that might make
your codes lower than minute for example
runs on level 1 and that's what makes it
really difficult to talk about
performance when you observe a program
because normally you don't know what
mode your code is currently running in
right so but how does this look in
practice let's say you start a program
public static void main
and you enter what will happen you will
often like we will always start out at
level zero let's say we walked a bit
through the call stack of your program
already we arrived at some method and
this method is used a lot and that's why
hotspot orbital machine is called
hotspot it determines what are the hot
areas in your code one hot me in this
case means it's run a lot and then it
will after a while of interpretation
start to profile this this hot code it
will say ok this seems important so I
have to make sure this is run fast then
it will be handed over normally in the
typical case it will move from level 0
to level 3 where the JVM collects some
information about your code like what
kind of classes do objects have like if
you have a parameter of type object it
will look okay what actual arguments are
assigned to this parameter right and
then after a while based on this
information it will compile it to level
4 and then basically your code is as
fast at best possible see to compilation
is however an expensive operation so for
a virtual machine it is always important
to make a trade-off between how much
time do I spend running code and how
much time do I spend optimizing code
because once the JVM is basically trying
to analyze what you're doing what your
program is doing this is time and energy
that it cannot spend on actually
executing the instructions of your
program right and that's why some
optimizations aren't performed by the
Trivium every optimization comes with
the cost with an overhead so so you have
to try to make a trade-off let's what
are some time compilation people that
Oracle are basically doing for the
living they consider what is a typical
Java program what does a typical
programmer do and if you do strange
things in your code then someone might
have said ok we could optimize that but
we won't because normally that's not
happening right that's the first rule
apply common sense basically if you want
to write performant programs write good
programs and yeah you'll see later what
I am what good means and in practice but
but it's a good good first rules but
basically the best I mean it's it's a
lame tip right right good
we'll be fast but in a way it's true
right
and since c2 compilation is so expensive
it might
however happen that if there's a huge
queue of optimizing optimizations that
this the virtual machine decides to only
go to level 2 because level 3 we're
profiling information is collected
collecting information also causes an
overhead right so you might just go to a
mode where you can't collect a minimal
amount of information unless and until I
mean the c2 compiler has capacities
again and then you move over to the
level 3 and then to level 4 this might
just happen because you your processor
is busy or your c2 compilers busy or
what also a typical case let's think of
a trivial method akita right what does
it get to do it reads a field and
returns it this is a no-brainer right
that's not much to optimize here what
happens here it goes to level 3 it says
ok this is trivial
I won't even compile this I won't spent
the energy and c2 I'll just drop back to
c1 this is the typical patterns that you
see in a compilation log compiling to
level 3 compiling to level 4 or the
other ones right
ok that's so much for the theory that's
basically the general framework of what
hotspot does right and but let's look at
something more more practical and this
is the central building block of hotspot
of the JVM and how it popped amaizing
code and the concept is called a call
side and you might have heard of call
sides right so what's a call side well
this is a call side for example every
time in your program very call a method
in performance speak this is called a
call so and not only performance speak
that the closer you are to the VM manner
and you often refer to this as call
sides rather than calling a method right
it's probably more exact and what you
all know about Java is that all non
private methods and non static methods
are dispatched virtually right so when
we call bar here on an object foo we
don't know if we actually observe the
class foo up there if you actually
observe an instance of foo or if you
observe a subclass right
and many languages have therefore chosen
to make methods non brutal by default
few if you wrote C++ code of you wrote I
think C sharp also has non virtual
methods by default they decided to not
make methods virtual but if all and the
reason for that is not that it's a bad
feature necessarily it's always good to
have the possibility at least to
override methods if you need to but this
is slow and why is that slow it's
because you have at runtime or or at
compile time you have to determine first
what method you will call when you reach
a call site can you just print hello
like an info bar or is the method
overwritten so we have an indirection
and in directions are bad because every
time we reach a call side we have to
determine what class is responsible for
dispatching a method and it might not
sound like much and it won't be much but
look at the Java program what is it like
assembled from it's basically method
calls in an object-oriented language
that's what a program consists of right
so if something happens as often as a
method call calling a method better be
as fast it's just possible right however
this is actually not such a big problem
especially not in Java which is single
inheritance and single inheritance
actually has implications of how we can
implement a runtime for language in this
case we have so-called virtual method
tables to dispatch and all unlike non
C++ which is multiple inheritance also
as with all method tables but they look
a bit different so internally the JVM
keeps track of a table where it
basically enlists all methods by an
index in this case we have to look at my
screen bar at index 8 and what we then
have is a machine address where we
reference the code to this method and
there we will just point to this printim
instruction right so if we go to a call
site and we observe a class foo which is
actually an instance of this class a
direct instance then the JVM just goes
to this class this virtual method table
goes to index 8 and calls to code there
so this is 3 in directions checking out
what class has an instance at
call set go to the method table and go
to the address of the method in in
question write three in directions
that's that's a lot for something that
happens as often as a method call a good
thing about single inheritance is that
if we have a subclass of foo then we
will find the bar reference again at
index eight so at least the JVM can yeah
point to a certain index in a table so
we be save us some extra trouble if
every method would be dispatched that
way if we would always go to a virtual
method table for every method Java would
be a very slow language and in the
earlier days Java was a rather yeah slow
language because the just-in-time
compilation I think they added hotspot
only in version one three right and it
improved a lot ever since so how can we
improve this and this is not what the
JVM thus but this would be a way we
could get rid of one apptimize ation and
one indirection by just maintaining a
cache of classes right could just say
okay the class foo has a certain address
for this method bar right and small talk
for example was implemented that way
small talk kept in line caches at every
call side and the problem is that these
caches are pretty big first of all in a
way and consume a lot of memory the
second problem is that we still have to
in directions so this is not something
that is very feasible for for hotspot
yeah because this is still you still
have to check the instance of a class
and then check the cache for this
instance and if you don't miss the cache
right then we have to do virtual method
lookup but in case that we always hit
the cache then we can just walk directly
to to the method implementation so what
we rather one 1/2 and this is something
that is very crucial to performance our
programs is a call cited is one SAS
monomorphic or linked a monomorphic call
side means that there's only one
possible class that is ever observed at
a certain call side so all instances and
Forwell will always be of class foo
right only foo and nothing else ever and
what the JVM
than does is it says okay we just go to
the cold side to to this system out
print on directly right we have
basically what the JVM does it adds a
guarding instruction that makes sure
okay
this is really full and then we jump to
the system.out.print old thing and this
is something a C program cannot do the
JVM makes speculative optimizations the
JVM looks into your program just says
okay let's just pretend we don't have a
virtual method here let's just pretend
that we can dispatch this as it was a
static method call disk will never be
overridden and only if this assumption
is never true then we have to have like
an assert and then we fall out and do
something else that JVM can do that
basically say ok and what I try to do
didn't work out but if this works out
most of the time and it does work out
most of the time then I win a lot of
Pilat of time for for not having to to
dispatch methods virtually right so and
this is also profile based information
right when we run and see one we can
collect information about what object we
actually observe within this foo method
here or to do something method here I
mean and we can see ok this is really
always the same class that we observe
you do we have a lot of polymorphism
here and based on that information from
the profile we can run situ compilation
where we can then put this a certain all
right so what is there the JVM knows
monomorphic also called link call sites
and as a matter of fact in a general
Java program about 90% of all call sites
are monomorphic think about get emitters
for example or setters or most methods
that you write right you have classes
that are never subclassed the JVM will
just consider these classes as yeah
non-virtual after a while and it will
just apply and this mono morphism that
we just saw there won't be a virtual
method dispatch it would be as if we had
a c-sharp program or a C++ program where
we always correctly said ok this method
is non virtual right this is the same
result in the end how
ever for example Smith specifically for
the data structures like sets or lists
we sometimes have by morphic call sites
and by morphic all sites means that we
have a call like let's say we call size
on a list object and then sometimes it's
an aerialist and sometimes it's a linked
list and then we have a similar
optimization and what we don't want to
have is so-called mega morphic call
sites and this is basically the first
way of avoiding slow programs is to
avoid megamorph we call sites where the
JVM actually has to do the virtual
method table lookup that I showed you
before but since the JVM is in depth of
model at all so it might be in different
states throughout the life of the
application so what will happen is that
once we start out a program we probably
have to look up the target of a method
and then when we go to JVM not we the
JVM gets confident enough that the
method is actually monomorphic we just
collapse the call side into being a
directly in line method call however
this assumption doesn't hold anymore we
just fall back we D optimize the program
to do over to a method lookup again this
is constantly happening your Java
program is constantly moving it might be
that you have a long-running application
it has been online for a month and then
you basically you change the
configuration property where different
code gets triggered right and then JVM
will learn ok something has changed all
of my assertions that are put into the
application suddenly fail all of the
time and then it will the optimize your
program but after a while it will
optimize it again and most of the time
it's ok but for some people it's not if
you have for example a trading system
and you have to be really fast because
otherwise you're losing a lot of money
then you don't want to have a
deoptimization happening while you're in
the heat of the market right so some
people have you know adapted be like
yeah they fire or fake data at the
application at certain times just to
trigger certain the optimization and re
optimization of the JVM right but again
this is why when you write your own
benchmarks for example and you look at
how code behaves you never know what's
happening right in your benchmark maybe
the JV
learning about your program while you're
running a benchmark and might start
optimizing which takes a lot of time and
you think that something is loaded it's
actually very fast so this is something
that is important to know about about
JVM or not only about hotspot actually
this is basically any Java tool machine
that uses just a time compilation okay
we've seen this before why is this it
doesn't sound so important but Mon Amour
fizzle is referred to as the uber
optimization of the Java of the java
virtual machine and not only mon amour
is emits its so-called inlining that is
important the thing is that inlining is
almost always only applied for
monomorphic call sites if this assertion
holds we can optimize the program
further we can just copy the
system.out.print on in to to do
something method and now we can just run
this as if we had copied the code there
ourselves so we don't have to do it we
can just write nicely object-oriented
programs the JVM does that the
optimization worked for us since we
could prove that we will always end up
calling system.out.println hello anyways
why not just copy this information to
where we otherwise would have called the
method and this often gives a lot of
space for doing other optimizations once
you have a lot of code in a method you
have a lot of stuff to think about and
as I said the hotspot regards a method
as the compilation unit it treats as a
main point if you can copy a lot of code
and one method you can do further
optimizations and that's the easiest to
understand actually oh no I have
something else before so now we have
this is the first really practical
advice so we have learned that Mon Amour
fizzle is important and what you should
learn from this is that you should not
introduce types into your program when
these types are not really useful for
modeling actual complexity or actual
behavior let's say you have a list
callsign you call this size right on
this concert here size being the call
side and let's say normally you either
have an array list or a linked list then
this is by morphic right by morphism
still means that the JVM there's a lot
of optimizations compared to a mega
morphic all set but now
not you of course but the colleague of
yours puts in this ArrayList here and
this is a fancy style of initializing a
list right and and you shouldn't use it
because it it creates an anonymous class
which carries a reference to its outer
class which might cause a memory leak
but in this case the worst part is that
if the list size call site now observes
this errorless sub class here we have a
third type and the JVM will fall out
from by morphism to mega morphism and
that forbids more way to talk or to
trigger certain optimizations but we
have seen before right so this isn't a
real type right
but the JVM will regard it as a type
because it said okay normally people
don't don't do these things so we don't
have to take into consideration that
people just create a lot of anonymous
types without using them also if you
reflect our program to represent the
same things as fewer types but in a
better delegation model for example this
might cause your program to trigger
certain optimizations because fewer
types means more inlining mormon
overfitting right so avoid types at all
costs actually and it might sound
strange but if the problem with
polymorphism is that if you actually use
it it might forbid certain optimization
and if it's necessary then there's
complexity that you have to to model
right so then it's okay but if this
complexity is only there because you
haven't written like clean and clear
coat then you pay for it by a
performance losses all right
so let's look at another practical
example let's look at this just yes yes
please yeah
no no um first of all we are we're
talking about very small performance
gains here right you should write a good
program and probably pays off more to
write a good program that it pays off to
do these sort of micro optimizations
it's just what I'm trying to teach you
yes that this has implications for
performance you should model if you
really need a linked list the linked
list might have so much better
performance implications for a certain
use case that this pays off right comp
and compared to losing the domani
morphism and actually another thing it's
just stay with me for this example and
then I will refer back to that what you
just said right let's look at this log
method write this log method is very
generic and log methods are normally
very generic because they they run
random arguments right and if you with
the knowledge that we just gained this
is a terrible method right we have an
array of arguments and then we iterate
over them and call to string on every
every argument and probably we will
observe a lot of different types here
right within this loop by design already
and to string here will be super-mega
morphic right this will be a blocker and
application so let's yeah here so let's
say we call log from somewhere right the
log method call here itself it's a
static method called static call is
always monomorphic a static method call
will always arrive at some some target
it's never virtual right and we just
learnt it for non virtual methods we can
inline code so what happens is that this
will be inlined
probably right what we now also have is
that we know the arguments and at once
we know the three arguments that we'll
always ever appear at to string here but
the JVM does for small loops a so-called
loop unrolling so it will replace the
loop with a copy of the method body like
this and what we have created now is
that we have reduced this megamorph
recall side into three monomorphic call
sites so the JVM will be able to do
further optimizations by basically
specializing each system.out.print them
for a certain type right and that's yeah
very handy because of course we could
otherwise argue if we have very critical
code like an algorithm that runs for
hours then this is something we should
otherwise do ourselves but since we
brought our code in a way that the JVM
is able to perform this optimizations
we're lucky and this gets there for free
after world think however about what
would happen if the loc method would not
have been monomorphic if you had some
weird logging logic where the actual
logging code is very nested somewhere
the JVM cannot figure out how to copy
this code into you actually do something
method this optimization won't happen
and that's why Mon Amour FISMA is so
important if you have a monomorphic list
there's very little code entailed right
but if you have a monomorphic algorithm
that is like a huge piece of runtime
code then not being able to do
optimizations by merging this algorithm
with the actual data might have a very
severe impact I have it's a very small
yeah if it's a very small segment of
code that is behind your your inlining
then it's not that bad mode of it most
of the times alright and also what will
happen maybe if you use a linked list
instead of an error list maybe in this
cases you call your algorithm from a
different method right and then this
might be copied into the algorithm and
basically create a new call side only
for the linked list right so in a way if
you structure your code properly then
most of the time it everything goes well
alright so I have these performance
rules and I try to ridicule it a bit
because these are twice is like right
you would go out now and say like this
guy told me to never use linked list
that's not what I'm trying to tell you
take these performs advices very
relatively you write good code and only
if you ever have a problem you think
about the performance implications or if
you have to argue with the colleague
while certain things should be done in a
certain minute and it's good to know
about these things but never actively
think about performance by writing code
so of course one type is good and many
types is bad right this is it is true in
a way right but it doesn't mean that you
should put
you code into static methods and not
write object-oriented programs right
that's important okay and this is
something I'm a strong advocate for
static typing and my probably spin one
never was a big fan of writing huge
business applications in dynamic
languages and here's why it's a lot
about performance again and what I'm
quickly want to do I know this is char 1
and we talked about Java I want to look
at a JavaScript program and we ate the
engine that's running Chrome and it's
running node.js right so how does nodejs
optimize code can it use the approach
that we just explained can it use mono
morphism and then these concepts it
cannot because there are types but
that's the only way to optimize programs
so what we ate actually does it that it
introduces types to your JavaScript code
so let's say you have this object foo
which doesn't have a type and we add two
properties to it
no J's will then create a type the star
is basically a type of the empty object
and no J's or we ate rather knows an
inheritance model if you define X first
it will define a type X that inherits
from star and then with Y it'll
introduce a type XY that inherits from X
and and star and it's in a way a sub
classing hierarchy so if you have bar
another object which is exactly
identical it has two properties wine X
but this time we define them in
different order we will get a different
type we will get a type star again for
for the empty object but now we will get
Y inheriting from star and YX inheriting
from Y and we ate won't be able to
basically connect these two type
together and to basically a mono
morphism optimization for observing
these two different objects and in a way
computers are strictly typed right you
shift bits and pies on the hardware and
then people like to refer to this as
mechanical sympathy right you should
think you write software that's running
that's run by computers computers can
just do so much and if you write a
JavaScript program the only way that the
travel script program will be
formalist by basically berry being very
strict about the shapes of your objects
which are in our form types right so
even if you don't write type programs
you will have to have structure in these
programs right if a program doesn't have
structure it's difficult to optimize and
that's true for that's why I'm telling
the story right structure predictability
is good computers can work with that and
JVMs as well right so you want to avoid
avoid a mess and explicit typing is a
good way of doing that alright so what
we could do now I desisted this is
basically just another side of the same
coin we could argue that if if
polymorphism is bad we just do branching
and branching is as a word for if-else
right so we can just say if that is true
instance of call instance off and we
have created to call sides right
however branching is another form of in
predictability and JVM has also measures
to to improve that and also the
processes especially so let's just
quickly look at this program what we're
doing here is we create an array with
20,000 elements and with a random value
between 0 and 99 ok this is what this
program does and then be 1,000 times
iterate over all these values and
calculate the sum of it but not the
actual sum but we will add the number if
the number is bigger than 50 and we will
subtract it if it's lower than 50 and
you might wonder how can you optimize
this program and there's some surprising
answer to that so yeah because one
problem that we have here is that this
is unconditional the JVM is basically
forced to do a different thing based on
a random value and since the values are
truly random here chances are 50/50 to
either do a plus like add something or
subtract something and simply the same
thing as with polymorphism if the JVM
doesn't know what's happening it cannot
do certain optimizations and that's a
problem here so in order to and the
processor kind of predicted stuff either
so in order to improve the performance
of this program we can sort the array
and despite us doing
additional work this program will run
faster and the idea is that if we know
that Aris ordered we will probably just
jump to the the some I'd like to adding
a number for the first five hundred
values on average and then we will jump
to subtracting a number for the next
five hundred values and since we
basically yeah JVM will be able then to
figure out okay we do now for five
hundred values we add the numbers so we
can just basically start adding without
the conditional right and then it will
at some point say okay this is not
longer true this condition bigger than
fifty is not laying out longer true I
have to fall back to subtracting and we
can also fast tracked us there's up
there this operation right so as with
polymorphism for branching so if else is
just the same thing be predictive if you
can reduce the complexity of your domain
to an extent that the JVM can figure out
for most of the time how branching works
you will be able to gain a lot of
performance here right okay so in a way
prediction is good if the JVM can figure
out what you're going to do it has a lot
of stuff to look forward to and it can
say okay this is gonna happen so I'm
gonna crunch all this to to a smaller
optimal operation which is resulting in
the same thing if the JVM cannot say
more than ten steps ahead what's
happening there's very little to go
through right the tray VM will not be
able to tell what's going to happen in
the next future and it doesn't have any
information about your program that it
can optimize that's very important it's
very crucial if you if you actually like
if you have an algorithm right and your
algorithm is alternating all of the time
between two different states the JVM
will have a problem rather do one thing
first and do it all the time then change
to doing something else and it's of
course again a very abstract way of
giving performance advice but that's why
people perceive performance as difficult
but this is basically the the moral you
can learn from this okay let's look into
a different optimization at the java
virtual machine and does for you
we have heard already does it spin true
for a long long time that object pooling
is not a good idea
object pooling means you keep objects
around for as long as you can and the
thing about objects is that they are
expensive objects have to be allocated
on the so called heap of the draw boots
on machines memory and that is basically
in a way space that requires
synchronization to a certain extent
because multiple threads can access the
heap
so you should be careful with whenever
you create an object if you look at this
example you will see that there's an
object created because this is more or
less synthetic sugar for this
instruction right we create an iterator
and then we iterate as long as there's
new elements and print out every element
from the list right and we allocate an
iterator and we want to avoid that are
dealing so how how can we do that we
could of course do the old form of
iteration we could just iterate over an
index and avoid the allocation I ever
escaped analysis helps us to avoid this
if the JVM can determine that an object
will not live very long it can often say
okay then I won't allocate it in the
first place and opposite to what was
done in the old days of Java where you
would keep objects around for as long as
you could in order to avoid an
allocation today you can try to not keep
them around as little as you can and
have an even better benefit by not
allocating the object at all the JVM
will in a way you raise the iterator and
just implement this the same logic that
represents directly in the method so it
will rewrite your code to not have this
iterator allocated at all so you
shouldn't worry about these small
iterations heaps go up small and to JVM
will honor that by deleting objects that
you don't actually treat as an object at
some point we will get value types in
Java and then hopefully this heap
allocation problem is reduced heavily
again but it's already today is reduced
to a certain extent by escape analysis
all right ok so the stack is good object
allocations bad it's generally wise
advice again don't take it too seriously
don't write programs that never
allocate objects in the end to JVM
hopefully can optimize your code anyways
okay let's look at another concept let's
look at that code emulation that code
elimination is often a very big catch
and benchmarks and what we do here is we
have written a benchmark we want to
measure how long it takes to to add
20,000 random values so what do you
think will happen here
well JVM will look at this code here and
say like okay wow-whee iterating 20,000
times of an object and be computing a
sum but then it says okay but we never
gonna use the Sun so I cannot execute
this I can just throw this away so what
your benchmark is doing now it basically
measures how long it takes to call
current time a least two times and
there's not a mistake in this program so
this this code will also be deleted so
there's another problem just in this
code and it is that currents of time
milliseconds is synchronized with this
time of your system but it is not
constantly synchronized it is only
synchronized Etten that many clock
cycles so what can happen is that you
call current time movies for the first
time and then the operating system tells
the JVM you're out of sync with your
time and it will reset the time to a
smaller value and this
system.out.println it actually print out
took -1 milliseconds right or even
bigger number so most cases will just
print 0 milliseconds because there's a
certain granularity so you shouldn't use
current milliseconds anyways but the
main part is that your code isn't
actually run and that's that's all again
a very powerful optimization let's say
we have a method that doesn't do
anything and and what the JVM will do at
some point it'll just say ok I'm never
gonna call this method again right I'm
just gonna remove this piece of the code
there is however a better way to writing
benchmarks this is you shouldn't write
your own benchmarks but let's look at
this example very fix this and if you
have looked at benchmark code this is
what you will see right you have in this
example what we first do is we do a so
called warm-up and I've shown you
profile based optimization before what
we basically do is that we ensure that
the JVM has learned as much as possible
about a
program so we iterate about our actual
benchmark code which is now contained in
the zone method and by doing that we
give the JVM an opportunity to learn as
much as it can then also we use nano
time nano time doesn't have a big
advantage but over there was in a real
application but nano time isn't
synchronized with the operating system
so if you compute differences of time
nano time is always better and then also
what we do is that we print out the
result of our complete computation
because that way we basically force the
JVM to actually compute the sum without
doing that the JVM could just again
discard our code completely but it can't
say ok you're not actually reading the
console right this is a benchmark so you
don't ever look at your console you
won't print it that's nothing that the
JVM can legally do then we print the
time so what we basically do is we have
a warm-up and then after 10000 times we
can be yeah certain rather certain
thoughts what that all optimizations
were triggered and then we can actually
print the results so this time we have
written a benchmark that we can actually
use and this is how people write
benchmarks more or less is most
benchmarks look like something like this
there's however a better way there's a
framework called jmh Java micro
benchmarking harness jmh is a part of
the open JDK and it is basically a way a
canonical way of implementing benchmarks
so what Jay mage does all the hard work
for you you basically have a setup
method where you initialize your code so
that you're ready to run the benchmark
and then you run the benchmark you
implement it just in one method and jmh
will take care for you that no
optimizations outside of this method is
triggered so for example we return the
sum and then jmh figures out for you how
this is this value is escaped in a way
such that they dead code elimination is
not applied it's a really really good
framework I use it for all of my open
source projects if I or if I have
crucial code where I care about
performance it is a really easy way
compared to what you had to do manually
to
or to ensure that the way you implement
things are the ideal weight to to
solving a problem all right okay so one
problem about measuring performance is
that if you run a micro benchmark and
people like to do that because they like
to prove that one way of doing things is
the fastest way if you run a micro
benchmark and you have two operations
and you benchmark them both you cannot
tell that these both operations trained
together will actually take the time of
the sum of these two benchmarks together
because if you run code together the JVM
can for example in the worst case you're
doing two heavy operations twice right
if you do the Tekken time the JVM if you
if you run it in a rock and then tell
okay this is the same thing I just
reused the results from before so you
cannot really tell from a micro
benchmark if something is fast or slow
in a certain context you have to be
really careful what you're measuring
right in the worst case it can even be
slower to run two things after another
if you have created a certain profile
because as I said before you have
profile based optimization where a
profile is built and certain
optimizations is triggered and since the
optimization takes time as well if you
have to do Mize first before running
your second piece of code this will cost
you right okay so again harness is a
good self-made code is bad right simple
rules um
one one last thing that you should know
about in my opinion every Java developer
to know about our intrinsics if you look
into the implementation of Java long
integer there's a method called bit
count and bit count basically counts all
the one bits of a number so this is how
a bit count is implement implemented in
the open JDK right
however there's an instruction a machine
code instruction called pop count that
is available on input I think so what
the JVM does is that it knows certain
methods and these methods are called
intrinsic if you copy this code into
your own method this bit count method it
will be slower than menu call integer
bit
for the simple fact that the JVM knows
this method and automatically reduces it
to a certain process instruction right
so it is important that if you know
certain methods especially in the
context of mathematics that the math
class and Java is a very important class
because it is often mapped to certain
ways of doing things on the processor
level on the hardware level all right so
this is another thing and you can
actually if you go into hotspot the
source code of hotspot there's a class
called B M symbols dot HPP where all the
intrinsic saw hotspot are listed so if
you if you write algorithms mathematical
algorithms especially find out what your
intrinsics are and apply them write as
much as you can yeah okay something that
the JVM doesn't do for you and I hope
it's trivial but but people
misunderstand that unfortunately
regularly this is a sorting algorithm
right and it's not a very good one what
this sorting algorithm does is it sorts
or shuffles values as long as they're
not sorted eventually they will be
sorted if the shoveling is truly random
but it will take a while the JVM doesn't
optimized algorithms for you algorithms
and what you ask before data structures
are you a concern if you for example
collect a lot of items in a list and you
probably will never iterate over them
for example to collect debugging
information or something it is often
better to use a linked list every lists
are almost always the right choice
but sometimes linked lists have much
better performance implications it's not
often something related to memory layout
for linked lists and error lists but you
should choose your data structures
correctly data structures are algorithms
as well this is something that the JVM
cannot do for you right and always keep
that in mind you have to still describe
your program in the appropriate way and
that includes choosing the right
collections that includes choosing the
right algorithms right okay so think
about your programs analyze what you
have to do and don't don't just do
random things this will cost you a lot
ok I don't have that much time anymore
so I'll jump about just quickly about
exceptions exceptions are terrible way
to implement
Oh Flo don't emulate an if/else with an
exception that you catch JVM regards
exceptions at something that are rather
rare and this is there's a great blog
post bollocks a should be left who's a
performance engineer at Oracle where he
analyzes or compares control flow by
exceptions to actual control flow so
what's here on the on the y-axis is
basically the average performance of an
operation and on the on the x axis we
have the how common it is to throw an
exception if you just look at for
example the yellow line which is just
dynamically created exception if it only
happens 10 to the power of minus 6 times
it is much faster than the black line
which is basically an explicit control
flow with if-else so if you use
exceptions to do really really rare
events then they are the right tool but
let's say you use an exception to
indicate you wrong password exception
right so from this class you can tell
that the black line and the yellow line
hits at 10 to the power of minus 3 so if
you users if hundred logins you have
only one yeah wrong login then already
the exception is then slower than the
explicit control flow so so really
exceptions are meant for rare cases ok
so exceptional control flow is a really
bad idea not only for aesthetic reasons
and code readability but also
performance reasons ok another thing we
want to look into quickly and then I'll
I'm almost through is false sharing
false sharing is again something related
to mechanical sympathy let's say you
have an object that is shared it's close
charity and has two fields X and y and
you have two threats where one only
works with X and the other one only
works with Y and false sharing means
that basically these two threats
interfere with each other despite them
not having a common value right that's
why it's called false sharing the thing
is that on hardware you don't read
fields you never read fields in hardware
level you read cache lines and cache
lines might have more than one values
they will always probably most of the
time they will carry much more than one
value this simple example we just have
five six values in there and let's say
the other ones belong to other objects
but the 14 and the 7 should be x and y
in this case right so so if we have two
processors and one writes only to X and
one only writes to Y then these both
processors will write to the same cache
line because they will both get this one
cache line that contains both values and
one says okay this 14 becomes 24 and the
second one says 7 becomes 1 then we have
a contention these both two processors
have to figure out how they change the
cache lines and have to synchronize the
information despite that these two
weathers don't have anything to do with
each other and this seems very
artificial but it's actually something
that I see all of the time what people
like to do sometimes is that they write
algorithms that use different threats
then every threat for example has a
value in an array and arrays are always
as aligned in one row so they most
likely share a cache line so if you have
caches that drive to different indices
of the same array this will often cause
a lot of performance overhead what you
can do however is there's an annotation
which is unfortunately internal so with
Java 9 it's not available anymore but in
Java 8 its it's called contended where
you force the JVM to put every value on
it that is annotated on its own cache
line the old way of doing that is and
you see that in a lot of like
high-performance libraries you see that
people add a lot of fields above the X
and below DX that way you basically
create a buffer where the cache line
will be taken but then you have this
that's not what you that's not
communicating you intend and you cannot
be sure if that is then causing a value
to to end up on a single hash a cache
line for for all fields right so that's
a bad way the content of the annotation
is much better but don't be confused if
you look like in Lmax 2
dropped or something like that you will
see it there right okay and this is
important really important if you look
at how much it takes to read something
from a cache line or to synchronize
something again with main memory which
contention always causes in the end then
that's times right if you need to read
something from disk like if you don't
write cache efficient code where you
basically have to exchange values with
with the main memory all the time or at
worst case with the file system let's
say you you have a swapping space right
so you write memory to disk and read
that memory again back from disk where
process instruction can cause one
nanosecond and reading from this four
megabyte can cost twenty the time that
it takes to execute twenty million
instructions and this also is really
important getaway of rewritten
algorithms for customers before these
algorithms were running for for ten to
twelve hours and then just by writing
them cache efficiently basically
changing just a few bits these two these
eight hours can be reduced sometimes to
one minute and it's not because the old
algorithm was slow the thing is people
like to think processes are super fast
today what people don't like to speak
about is that these times what it takes
to get something from a memory to a
cache these times have improved by about
ten percent over the last ten years
while processor speed has improved
magnitudes right but if your processor
never does anything because the caches
aren't filled then then your programs
will be slow and that won't improve
anytime soon
the reason that we have more and more
cache layers is exactly that is it is
because processors get faster but the
time it takes to transport information
through a processor didn't improve much
so that's important pardon what did you
say
yeah you miss you use amazing Thai
airspace right that's the problem with
the contention otherwise you could just
say okay we put everything on a single
tear cache line I I just take it at
people are leaving for Elton John not
because that's boring
alright okay we're almost through I'll
release you in five minutes and you know
what that is right that's now you don't
because I didn't before I googled the
relation either
that's Heisenberg's uncertainty
principle and what Heisenberg says is if
you observe I think an electron I'm not
sure anymore then you once you can
observe it you can either tell how fast
it is or where it is and the same goes a
bit for performance measuring if you
want to measure a program then you often
have to change the program in order to
make it measurable but by changing the
program you change the program that you
were observing so you're not observing
the same program anymore
so don't be fooled by profilers and they
almost all lie to you
profilers don't tell you the truth so
don't rely on on heap thumbs even on our
heaters maybe but almost all information
like we all love to look at big graphs
but it's difficult to take that out and
often this data isn't even correct right
so one example is that we instrument
code in order to make it observable
however I told you before that we
optimize charge the Trivium optimizes
code for us and that might trigger the
JVM to be in a inconsistent state
sometimes for example a constructor is
called after an object is allocated so
we have a short amount of time where the
object is allocated and exists but the
constructor wasn't yet called so if the
instrument code we might look this
object up but then it wouldn't be
consistent so we can never be never
allowed to observe the object in its
inconsistent state and for that reason
that JVM has so-called safe points and
save points are points where it is
observe aware it is safe of course to
observe a Java program from the outside
and this is how instrumentation works in
a way and a typical approach for
profiles is using a JVM TI the native
interface
where you can basically extract some
information about a program but let's
say we have a program that is either in
state a or in state P and we try to
observe it and if we have a sampling
profiler which then basically all five
seconds tries to track into the program
what's happening
it will probably observe this sort of
thing sometimes it's in state a and
sometimes they'd be right however if it
is never safe to observe program this
program in state P the JVM will simply
never allow the profiler to take control
where when the programs in state a right
so we will always only observe state a
instead of state P does that mean that
program is never in state P yes I have
to check I can't really read from you it
doesn't mean that the program never
isn't safe P of course not but our
profile I was never allowed to check and
so that's why all that the typical
profilers lied to you right so better
options in my opinion is the honest
profiler if you are really into
performance you should look at that
program at some point in life which
doesn't use JVM ti but basically UNIX
interrupts so it comes from the outside
it says to the JVM no now I want to have
a look no matter what and since it's
basically not changing the heap it's
okay to do that but the try VM would
know what for general application that
it is allowed to go into the program
another great profile this is Mission
Control from Oracle which basically
reuses information that is available
anyways by garbage collectors by
just-in-time compilers so it has an
unfair advantage over other profilers
right
yeah yeah I'm not true for all profiling
information it yeah Mission Control does
both yeah it's true yeah I mean of
course always walkthrough not even these
tools have the limitations and it would
yeah yeah no you're absolutely right I
should mention that the thing is again
it can do that because the openjdk knows
in a way that Mission Control won't
Drouin anything right so in that that's
important condition but if you write a
generic agent and hot spot wouldn't know
that the agent doesn't access fields and
and objects right okay so what you can
also do it as a great call a tool called
 watch it's a part of adopt open JDK
which actually allows you to extract the
machine code of a method right and
that's all always helpful but it's very
interesting to see how some code is
optimized and not optimized if you have
very crucial applications that can give
you really good insights all right
generally speaking what I want you to
take away today is that you should write
good programs you shouldn't care about
performance too much but it helps if you
think about it right okay with these
words
thank you that's me you can firm your
Twitter you can find my open source
project bite buddy which was created
using a lot of performance analysis and
documents for J is another open source
protected the road thank you for staying
with me enjoy Elton John
see you tomorrow maybe
I give another talk on the memory model
tomorrow at 4:00 if you want to
otherwise I'll be around I won't go to
Elton John</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>