<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Scaling Scala to the Database | Coder Coacher - Coaching Coders</title><meta content="Scaling Scala to the Database - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Scaling Scala to the Database</b></h2><h5 class="post__date">2015-06-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/eq3V0VWKpDA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so welcome everyone to our session
scaling Scala to the database my name is
Stefan I work at typesafe in Switzerland
and I'm joined here by my colleague
Chris who's at EPFL the University &amp;amp;
lisana in Switzerland where Scala was
originally developed dead working a
little bit okay so I'm going to do the
talking and he'll present some live
coding demo later okay so to get started
to first look at some key concepts of
what we're doing now you might have seen
a scene like this it's a a data center
in the early 70s an over here at Oracle
OpenWorld it was probably more like
Oracle closed world back then that's
that's the time when relational
databases were originally invented and
you're still using this technology today
it has proven to be very successful for
many applications but do you really want
to still use the query language that you
used back in the 70s the sequel language
which still kind of looks like the other
language is invented unlike COBOL that
nobody really wants to use these days
unless you're forced to or paid really
well for it so the idea of our project
and of several others before us of
course is we write the sequel so you
don't have to so what are you doing
instead
well you don't write your sequel code
you don't write any J pql and you don't
use something like the criteria API
which I know is nowadays being touted us
as the latest and greatest in in Java
I've had a look at it I think it's a
step in the right direction but doing it
in Java just looks so cumbersome you
really want to avoid that
go back to putting my my code into
strings rather than writing this stuff
so what you do instead is you write your
database code in Scala what we can see
here is just a standard Scala for
comprehension we're selecting for P in
persons yield P dot name I don't know if
you people know Scala many of you are
probably from Java background so this is
not exactly the same thing as a loop in
in Java
first of all in Java the looping is done
externally so the code does not really
have any control over it the loop is
done around it in Java it is internally
and you cannot just loop and do
something you can also yield so if you
do that in a over a list let's say
persons is a list of person objects then
you will get a list of these persons
names out of it so it's a transformation
of this list and in this case we're
doing it to compile it down to sequel
and you can see there's a very close
resemblance in the sequel code it's
almost the same structure select pete up
name from person P that's essentially
the same thing and it turns out that
this approach scales so here we have a
much more complex example using many
collection functions like filters starts
with group by map and length and all of
these can be translated very nicely to
sequel code this is real sequel code
that was generated by a slick by the way
we didn't make this up we just did the
formatting here for the slide so the
project we're using for this is called
slick it's the scholar language
integrated connection kit and it's a
database query and access library that
we developed at type safe and EPFL of
course it is based on earlier work I
have done on Scala query which was
targeted at Scala to seven I think when
I started two to nine and slick is now
the new product for Scala to 10 and up
and it is an open source project but we
also have a commercial
out there so that you can see that in
the list of the supported databases we
have all the standard open-source
databases at least a relational once at
the moment supported in the open-source
core of slate which is licensed under a
very liberal BSD license just like Scala
itself but you can also buy a typesafe
subscription that will give you support
for your servers and you can also use
the slick extension package then which
gives you Oracle db2 and in the future
also sequel server which is currently
still part of the the open-source core
so let's look at the the components of
the slick API that you're going to work
with there are a couple of of api's for
writing queries the main one that you
probably be interested in is the lifted
embedding that's what we've seen in the
slides before that's when you write your
Scala code and it gets translated to
sequel and this is a production ready
API that we suggest people to use
generally and slick we also have the
direct embedding which makes use of some
new Scala features like macros and is
therefore still experimental but that
could be a step into the future for
slick to provide a simpler model where
it can really take plane job playing
Scala code and translate that sorry not
Java we don't have a Java API we looked
at it it's it's just not feasible with
the language you'd have to write so much
code it would be the same as criteria
API and you could go back to putting
sequel and strings which by the way is
something you can also do in slick
that's what why why we have the plain
sequel library the plain sequel API that
also uses some exciting new features in
Scala like string interpolation so you
can just embed your variables in your
string with a dollar sign in front of it
but it's completely safe because slick
automatically turns that into a bind
variable so you don't have to worry
about sequel injection box and still get
the same simple model for defining your
queries and underlying all of these
api's we have the session management
which obstructs over the two different
models of JDBC session handling the data
sources and the old driving manager
and in the future also other kinds of
data sources and new and slick to dotto
we have the schema model which is used
to describe the database schema so what
we're going to do in into that o is add
a first to first a tool for SBT or a
build tool but you can also run it on
the command line or integrate it in
another build tool and it will take your
existing database schema read that from
the database and create the Scala source
code for your table definitions for
slick to use that in the future we also
want to do that with macros so that will
be pretty much the same thing as type
providers in F sharp so you can have
your IDE automatically connect to the
database and read your schema for you
and then either do that live or later
generate code when you want so now you
might be thinking hey they're doing
another ORM we've seen both before but
that's something we definitely want to
avoid there are problems with our ms
there's this famous impedance mismatch I
we actually had another slide here
explaining how the concepts don't really
match but those things are pretty minor
the main thing you have to watch out for
is through data retrieval patterns which
are unpredictable in the standard ORM
setting let's look at this example from
from the standard Sun or now Oracle
coffee store that you might have seen in
the JDBC tutorial we have a database
table for coffees we have a database
table for suppliers of these coffees and
we want to build a very simple web
application that shows you a screen of
the coffee names so we select coffee
name from coffees that's a single sequel
query the coffee name is also the
primary key in this case so there's
nothing else to fetch and then when you
click on one of the links what do you
want to do is to show the details of one
of those coffees so you want to show all
the data associated with this coffee in
this case it's just the price and you
want to also show the supplier but the
supplier name is not contained in the
coffee's table it's in the suppliers
table so you have to follow a foreign
key
to that supplier so we select everything
from the coffees table plus the joint
supplier name from a suppliers table
again the important thing here is in
order to show the single page we do a
single sequel query which fetches
exactly the data that we need so how do
you do that in an object model you model
this this kind of problem very
differently if you want to show a list
of all coffees you need some method like
get all coffees which gives you a siik
of coffee or if you're in Java you might
call that a list or collection of coffee
objects so what's really in there if you
call them if you fetch them from the
database is this a real cig is it
materialized or is it just some dummy
that might fetch the data later and what
about the coffee objects in there are
they real or are not all fields
initialized and will be fetched later
you don't really know but you can
configure that in an ORM the default is
probably that the seek is real the
coffees are not they're just stops that
contain only the primary key and the
rest is fetched on demand so let's roll
with this and look at this print links
method here that shows the overview page
so what we want to do is iterate over
all these coffees and then print their
name and a link to it but it's not
really important in this example so this
looks pretty good we have we know we
have to seek the seek is real so there's
no extra data access happening here
we know the coffee objects contain the
primary key which is also the name so we
can just print them and works great now
what if we will if we want to add
another overview page that also shows
the price because now we have a problem
the price is not contained in these
objects it has not been fetched yet so
when you want to show the price you need
to go to the database and fetch it and
that happens transparently of course
there's there's nothing you have to do
for it but you will definitely notice it
because your application won't scale
anymore because now in order to show all
these things you
have to do one query you have to do n
plus one queries for your n coffees want
to fetch the list and then for each one
a separate query just to fetch the price
and the reason for that is very simple
there is no way for the ORM to know that
you really want all of these prices it
only sees that you want the single price
of the single object at this moment so
let's go back and leave the price out
here what happens when we print the
details well again we run into the same
problem we have the name we have the
price probably or maybe it will fetch
the price now which is not too bad
because we need to do an extra query
anyway but then we have the supplier
that's not actually contained in this
same table it is contained in a in an
object supplier which is referenced from
your coffee object that's how you'd
model this in an object-oriented design
it just have your supplier as part of
the coffee but now this data is not
there this may be only the primary key
at this point or it hasn't been fetched
at all so when you want to show the
supplier name again you do an extra
query to the database you do not see
this in your code because it tries to be
transparent but you will notice it in
the performance so the problem here that
we're dealing with if that is that
you're trying to map low-level
object-oriented programming to high
level relational algebra so this this
might be counter in to it if you usually
think of the database as a low level
thing and of object-oriented programming
as very high level I mean object
oriented programming is high level in
the sense that you can abstract very
well over concepts it can abstract over
nouns but not over verbs that's the
other side of third of the coin and it's
still an imperative language do this
then do that there is no way for a tool
to really optimize much there because it
cannot look ahead it does not see how
the computations compose it only sees
what to do at a certain point in time so
despite claiming to be transparent it is
really not in practice if you care about
performance you even have the same
problem nowadays when you're accessing
data in RAM
that's pretty much the same thing except
it's not noticeable in most applications
fetching data from your first level
cache is tremendously faster than
fetching it from the main memory so even
if you chase pointers in memory you
still run into this issue but still for
most applications it is fast enough
these days so you don't notice but when
you hit the database it gets orders of
magnitude slower and if you don't notice
your DBA will and he will not like the
application you wrote so what do we do
instead well we don't use
object-oriented programming or modeling
we use functional programming because
that's a much better match for database
access you see here the main concepts of
the relational model what would is
kindig the things that are contained in
the relational model so we have the
relation which is basically in sequel
the table header the way you define a
table and that could be mapped nicely to
a case class in scala think of this as a
just a data structure that contains some
fields if you haven't used this in scala
soleus it's like a class with fields
named supplier ID and price and there's
a getter associated with each of them
it's immutable so you cannot set it
there's a constructor to create it you
also get it to string for free that
prints this in a nice way and you get
very importantly equals and hashcode
implementations so when you compare two
instance of instances of this case class
it will really compare them by the data
they contain and of course the attribute
the individual field or row in the
database is a field in this case class
then we have tuples or rows of data
sorry the attribute is the column and
the tuples are the rows in the database
and these just Maps map map to instances
of the case class we can use this
without a constructor in Java you would
write new coffee of whatever then we
have relation values which are basically
the contents of table in the relational
model they're viewed as immutable so we
map them two immutable sets so the
relation value is just a set of these
objects of these rows and finally we
need the relation variable which is
basically the contents of a table in the
database and we just use a variable in
Scala for that and we can make it a real
Val which unlike a VAR in Scala is
immutable you cannot assign any new
value to this because we want to keep
our mutable state in the database so the
point of this thing that we call
functional relational mapping instead of
object-oriented mapping or object
relational mapping is that it does not
try to fight through a relational model
and it braces it so we avoid the
impedance mismatch but you also get by
doing functional programming there is
composable queries so your database
queries compose the same way as
functions you write in scala for
operating on collections in memory this
is something you cannot easily do with
sequel it's not built for composability
especially if you put it into strings
and try to combine those that just
doesn't work and very importantly you
get explicit control of a statement
execution so slick will never go to the
database behind the scenes to fetch some
data that it pretends is there but it's
actually not there are very few specific
methods that you can recognize because
they take a session parameter and these
will go to the database and fetch your
data and they will fetch exactly the
data that you asked for so you always
know when you hit the database and what
you fetch and you can keep your
client-side you to your Java or Scala
application stateless you can keep all
your state in the database and use pure
functional programming on the client
side if you want of course you can also
funnel this into a more standard web
application model where you put it into
mutable objects and then modify those
that depends on what kind of application
you want to build
so I'll hand over to Chris now for the
live coding demo thank you
most of the result we were heading for
and the first thing you need to do when
you work with sleek is imported driver
you were importing the h2 driver because
for this demo were using the h2 which
ISM which is a nice which is a module I
defined for the purpose of this demo and
what it does is it defines this this
data model their computers their
companies that both both have IDs and
names and computers have a foreign key
to companies the manufacture of this
computer model and then we define case
classes Scala data types to hold rows of
these tables and we describe the schema
to slick we describe how the tables look
like and we describe it in a using a
Scala API that allows the Scala compiler
to type check queries so the Scala
compiler because of this will know what
columns exist what types they have and
then what operations are supported on
these types so if you make mistakes in
your in your queries you will get a
compile time error and here we see
computers have have IDs and names and a
foreign key and we also explicitly say
that it is actually a foreign key and
what it what it relates to
and while we do this as these classes
that extend the slick table class and
then we also define these objects
computers and companies which can be
then used as collection like objects to
have to to support slicks API that makes
them feel like you work with collections
on the database and then I also defined
this function in a DB which basically
creates the schema in the database and
inserts test data
and as you can see it takes a it takes a
session argument which means this
function can only be run if there is a
database session available and for the
purpose of this demo we're using H 2 as
an in-memory database that is created
when we first connect to it and dropped
again when we disconnect so I'll just
inert it every time I run run the demo
so you it's always starts fresh so
that's what we do I first tell slick hey
use this we use an h2 in-memory database
by the name test and we use this to
create this database object we could
also instead of this JDBC connection
string we could have used the data
source object which we could have gotten
from a connection pool or we could have
used a jndi name event here we use this
connection string and then this database
object has methods like with transaction
which starts a transaction and - with
transaction we pass a block of code that
executes queries that's also with
session which does not open it
transaction just gives you a connection
and this method with transaction
provides to this block the session which
this implicit session means it will be
implicitly available for everything that
needs a session until the closing
closing braces and the first thing we do
is we call this init DB which is a
function which takes the session but
since it's Marcos implicit here and in
the definition of in a DB we don't have
to specify it so it makes the code a
little shorter so and what we do here
now has a database as a connection so we
can just write queries here we can
define queries however without having a
connection in just write something well
query equals from let's take the whole
let's take the whole computer's table
for now so we query everything and then
in in this block where we have a session
we can just say we're eager to run and
well that run takes this query which is
a description of what I want to have and
turns it into an actual sequence of data
which we can then work with on the
client side and here you see what what
it created is a vector let's print
a little nicer so we call the Scala
vectors for each method and we tell it
for each element in this vector print
that individually so we get it line by
line
slick also comes with for each method
directly so we don't need to specify
this run-run turns the DES query into a
actual sequence and then for each would
be run on a strict like on a fully
existing collection of results if we use
slicks for each we we just work in batch
sizes so we can work with large amounts
of data so this does the same okay let's
let's make this little more interesting
products it's for example just to take a
couple of these computers not all of
them because that's quite a lot and we
just use the Scala collection API which
we mimic with slick it's the same take
means just take three of them and there
are things like sort and drop and things
like this
okay so let's see how we can associate
the computers with the companies so we
just use a Scala for comprehension and
we use the foreign key definition which
we put into the schema description
before and we can just say manufacturer
and we say okay we want out of this
computer as well as the company and
let's format this a little bit yes and
print it so now down here in the console
output you see that we get these pairs
this of computers and companies so let's
call let's call this billion computers
with companies and let's write another
query based on that so we can reuse this
part when we want that's that's where it
becomes composable you can even rather
complex queries you can put them
somewhere in the library and like reuse
those things and now we're going to
write another query based on that which
shows us how many different models each
company produce so we do a grouping by
the company and then we yeah we see how
many and so we use computers with
companies and that's also thing in
queries you get code completion by your
IDE which is quite nice so wait we group
these by so computers with companies
returns these tuples and we want to
group them by companies so we group them
by the second is the way to tell the
Scala group I had used the second entry
of this topple and the result of this
this grouping operation is began tuples
with the key first so the company and
the group of whatever belong to that
group second and that's the way to to
unpack these things in this
comprehension Scala and then we can say
hey for the company let's just take the
name and for the group let's take the
length so that's the usual kind of
aggregations you know from sequel just
that you use the Scala at the eyes
that's that's that's another thing if
you know the Scala collections ID is you
you mostly know how to use slick just
out of
box and it's abstracted over the
different dialects of sequel and so we
are here we have all these well yeah
let's let's sort them quickly by the
length so the query the second field of
the queries the lengths that we can
quickly start this okay so now it's all
sorted by the number of different models
which we have in our database which the
different companies built but let's
remove that again and instead we
parameterize this so we can allow to
like we put a parameter in our query to
to allow to get certain companies out of
out of it so wait we filter the company
computers with companies by the
companies that have a name that conforms
to a certain pattern so we say that's a
little better
and now we need to get this pattern
somewhere so we turn this value which is
like very X into a function which
returns the query which takes a column
of string that's the slick type that
talks that means a value in the database
a string value in the database which is
not existing in memory it's just
something you talk about right now so
now I mean now of course this doesn't
compile any meal because query is a
function that takes a pattern and let's
give it a pattern so now we see this
this this allowed us to have a reusable
reusable query that that allows to
filter companies by pattern and return
their counts and it's and it's really
that's that's really a bit game-changing
how you can put these components and
make them reusable and also have them
type checked and just just compose your
stuff in this way and not have to write
like long strings which of course you
don't I mean you could kind of
concatenate strings but if you don't do
that list it occurs to you okay so and
what slick does is this this whole for
comprehension and these compose things
they build up a representation in memory
that describes what's what's what should
happen and when you run it slick will
compile it down to sequel and run it in
the database and so we could use this
for two different values here now we get
first Apple and then IBM however when
you do it like this slick will compile
the query twice it's just a it's just
this value which you put into into the
query if the compilation especially when
the queries get larger and more complex
the compilation slick the compilation
time of slick can take a little moment
so if it come from more complex you
might want to cache the compilation of
the queries and reuse the compilation
and now I'm going to show you how to do
that
and that will actually the API for that
will slightly change and slick to the 0
which is coming out but the concept is
still there so let's say we have we want
to compile query and so we can there's
another for comprehension we say this
pattern this pattern is actually a
parameter to this query so we basically
call this query function which we
defined above and pass in this this
parameter thing which means pre compile
it and leave this open make a prepared
statement that has a place a placeholder
in this spot and then we can use the
file to query down here and slick will
compile it the first time either the
second time we use the precompiled
sequel code what we also always can do
either with the either with the query or
with the compiled query is we can print
we can print the sequel code that slick
produces for debugging it's also logged
using log back if you if you want to
hook hook into that but like you can
also do print line debugging Reggie
query let's still there oh yeah it's a
it's a function so it needs
there's a procurement so here we can see
what kind of query Slick produces so
select name and count and that's the
grouping operation it's very similar to
how to however you were to write it by
hand and you can also do this with a pre
compiled query and where you see this
placeholder here which is compiled in
there
yeah and that's basically what wanted to
show you just to give you a little
feeling how it how it is to work with
slick and I'll head
sorry no no in this case no because I'm
just printing the sequel statement is
yes
yes this parameters this parameters
object is just there to to pre-compile
queries to fill field like - we have a
function that takes several values which
it could pass to that function that you
can fill it in with this parameters
thing to actually pre compile it
actually I think you have a button there
this really needs to be a Balcom pilot
query oh right that is a function object
that you create so you don't want to
call it every time exactly that's like
what do you mean to the compiled query
like like you down here so this this is
just this this is just I didn't pass one
because I just wanted to see the sequel
code with a placeholder yes and below we
pass it and then it's let's run
does that answer your question
sure we can also come later and we'll be
able to chat about it sure and well you
would put a two pole and he would good
like this and it will it will be simpler
though and the new of it I were working
on will just allow you to just turn a
function into a precompiled just with
one call without without this parameters
thing so I said you could transform a
function which takes column off whatever
type arguments into a function that
takes the actual types as arguments and
execute the query under the hood okay
yes
I think so yes
what you get here too many arguments
okay I'm handing back over to chiffon
okay so let's take a look at the details
what's happening under the hood there
and slick how we're actually making this
stuff work I already told you about the
different API so in this case we're
talking about the lifted embedding and a
direct embedding that you use to write
queries in Scala and if you use the
lifted embedding that turns the query
into the slick ast directly so when you
compose these things that will
automatically produce the the ast nodes
behind the scenes when you use a direct
embedding the path is a bit longer so we
go through the Scala is T first because
the direct embedding is compiled with
the Scala compiler it's plain Scala code
and then the Scala compiler of course
produces a Scala ast on the way and we
hook into this with our slick macros and
these take the Scala ast and translate
it to a slick ast which we can then
process further at runtime in either
case you end up with a slick AST and you
run this through the query compiler and
the query compiler does all the
necessary transformations there it
optimizes the query and turns it into
the right shape for execution in the
database and in the end it creates
sequel code and that is also contained
in a slick AST so you can see that query
compiler also almost always transforms
an ast into an ast but the resulting ast
will mostly consists of a single note
that map's the result set back into
Scala and another node that contains the
sequel code to execute and then you use
the executor API to run this thing it
will go to the database and give you the
results so here's some code in the lift
and embedding it looks pretty much like
what we've seen before we have a query
that goes over the coffee's table and
selects all of the coffees which have a
price less than nine and then it follows
a foreign key to the supplier you can
navigate the foreign keys
please you don't have to to actually
write the join condition they're
manually and it yields a couple of the
coffee name and the supplier name and in
the end you run this so if you look at
the scout this could be exactly the same
thing you ride for Scala collections you
put probably not modeled a foreign key
like to ask you this you always get a
single supplier and here we get a say a
list of suppliers because that's how you
model it in sequel in a foreign key but
apart from that it's exactly the same
code with one minor difference you
called odd run on it on the in the end
and that's what takes the session and
actually goes to the database and
performs this so let's look at the types
of this thing because it's not actually
what you might think it is what you see
here so coffees is not a list of coffees
in this case when you're using slick it
is something like a table query of
coffees much is a basic query that
describes a database table so what you
get out of that is a coffees object that
is the table row object that you defined
in in slick and now we go to the price
of course the price in the table row
object is not really a double value it
is actually a column of double that you
get here because that's what we defined
was the column method in this table row
object and now it gets interesting
because you're calling this less than
operator you might think this is the
less than on double but it is not
actually this is the less than operator
that we define as an extension method
for columns that contain a double so
this is really column extension methods
dot less than and on the right hand side
this expects another column so there's
an implicit conversion that will take
the cup the double value that we supply
here and turn that into a column of
double by making it a Const column which
just wraps a constant value
so the foreign key is also some kind of
of query that gives us suppliers so what
we get out of there is a suppliers
object naturally that's the same thing
as for the coffees and then the tupple
is unsurprisingly a tuple of a column of
string and a column of string so you can
see your or that only the individual
columns have this column type
constructor in there the Topol is a a
real scarlet apple on the outside and
this result defines the type of the
query query takes two type parameters
the first one is column couple of column
of string and column of string that's
the same thing we yield at the end and
then there's a second type parameter
which is computed from the first one and
that is the type you see on the client
side when you execute the query so that
is a tuple of string and string this is
essentially the same thing as the type
that we yielded except that the column
type constructors have been removed and
this works recursively so you can have
nested columns in there and slip to that
oh you will be able to write h lists or
use your own container types that you
want and it gets a bit more complex when
we we guild complete tables not just
individual columns but essentially it
removes the column type constructors to
give you the type that you see on the
client side and when you run this thing
then you get a seek of this type of top
off string and string now let's look at
the same thing in the direct embedding
this is experimental and you cannot
actually run this code today it would
look slightly different but this is what
it might look in the in a direct
embedding so there you are really using
the plain types there are no surprises
in here the only unknown type here is
the queryable of coffee that we use for
the coffees table and that is the class
that is all the magic because when you
use that in as a generator get see from
coffees then this gets the sugar to a
call to this coffees object dot flat map
and
the seed supplier using generator gets
translated to dot map that's the way for
comprehensive work in Scala so these are
the methods where we do the magic and
these are implemented as macros that do
the actual lifting of the code now the
advantage of this direct embedding is
pretty much aerial messages you see the
plain types here so when you get an
error in your code you will be able to
understand it because it's exactly the
same as in standard Scala code that
that's one area where where this
illusion breaks down when you do a
complex type magic like we do in the
liftin embedding as soon as something
goes wrong the error message will really
show that we're not doing the plain
thing and and it might be harder to
understand we're also investigating
other solutions to this like a new
embedding that we call the shadow
embedding which might influence or
replace the direct embedding but the
idea is pretty much that we want to to
go there that you can write plain Scala
code and then have that translated to
the lifted embedding so that in the end
both can interoperate nicely because one
main limitation here is this does not
work across compilation units if you
call some function or some plain scala
function that is defined in another
compilation unit this will not work
because we need the actual AST of the
function in order terms translated and
we do not see that if it's in a pre
compiled class file
so for abstracting in a large the lifted
embedding is a great thing for doing
things in the small the direct embedding
might be nicer so that's a direction we
want to explore more in the future
so finally I'd like to give you some
insight into the query compiler and
another new feature that we're doing in
2.0 which is query scheduling so the
query compiler is based on immutable ast
s with some exceptions we modify the
types and it in the notes as long as
nobody has seen it we cache the stuff
aggressively so we really try to do
correctness first but then optimize for
performance in there
by not copying objects unnecessarily and
the very compiler consists of multiple
phases that transform a query compiler
state which is also immutable and the
drivers for the different database
backends provide their own compilers so
you can completely customize if you
write your own driver what it will do in
the translation process most of the
standard drivers that we ship only
change the code gen phase in the end
which does the final step of translation
from a normalized ast to the sequel code
but you can do more stuff if you need to
for your driver so this is what what we
do for a standard sequel driver you
don't have to to understand what all the
individual phases do there it might be
hard to read in the back as well there
are there four groups of phases that
were running essentially first there's a
cleanup phase which does some
transformations on the tree to make it
easier to work with for example if you
share part of the tree then you end up
with the same variable names in there in
different parts and you might want to
assign new unique variables stuff like
that happens in there next we need to
flatten the columns because at this
point you might have enesta topple
coming out of a a protection of a yield
clause in your comprehension you might
have tables in there but in sequel all
you can select in a in a select clause
is a flat projection of individual
columns so we transform the query into
that shape so that all the mappings and
transformations end up with a flat
sequence of columns then we transform
this into a sequel shape at this point
we still have have a monadic query model
because that's what for comprehension
schemas for comprehensions really are
designed to be used with monads and
sequel is almost a monad so in almost
all practical cases we can actually
translate this monadic query
to a proper sequel query but we still
have the magnetic operators at this
point and now we turn it into notes like
comprehension which actually represent a
sequel comprehension with multiple from
classes with a select clause where
clause and whatever goes in there and
then we have the code generator phase
which takes this final shape and turns
it into a sequence string and that is
provided by the driver but it is of
course based on a standard basic
implementation that the drivers just
override to change what they need now
apart from the sequel database backends
we also have a back-end for a memory
driver that's like our own in-memory
database that we wrote for slick it's
not meant to be used in in production
it's it's really not much of a database
it doesn't optimize anything it doesn't
even have indexes we mainly use it for
testing or the technology there so it
looks pretty similar we just leave out
the conversion to the sequel shape
because the memory driver operates
directly on a monadic query so we don't
have to transform it into the sequel
shape and it also has a code gen in the
end but that does not actually generate
any textual representation of the code
it just optimizes it for the query
interpreter which is then used to run
this query in memory so why are we doing
this well we do it for this query
scheduling again it's just a minor
modification of the the previous model
we split up this these initial cleanup
phases and in the middle we inject a new
phase to distribute the query to
different backends the purpose of this
is that you can write database queries
against different databases so you can
use a table from an h2 database another
table from another edge to debate
database and a third table from a my
sequel database and you can write a
query that joins data from these and
accesses all the different databases and
brings the data together and the way we
do this is that in this distribute phase
we split up the queries
we analyzed it for parts of the tree
that have to be executed on one of the
databases and other parts that need data
from multiple databases that we need to
run in memory using the query
interpreter so we just split it up then
compile all the individual parts with
the real drivers that we need for them
put them back as black boxes into the
tree then just run the standard
transformation which ends up with the
code gen for the memory driver or for
the query interpreter and then we just
run it through the interpreter it will
do all the operations that are necessary
to do to be done on the client side in
your Scala code and everything that can
be done on the server side in one of the
databases it's executed as a sequel
query there finally let's look at what
we were going to do in the future
so we're now just before the release of
slick 2.0 we hope to have a release
candidate out in October and possibly a
final release this will bring you the
the new table model the new kind of
table definitions that we saw in the
live coding demo it will also bring you
the query scheduling that I just showed
you here on these slides and it will
bring you the code generator that you
can use to reverse engineer your
database schema and create the code for
you
after that we're looking at no sequel
support we started working on a MongoDB
driver and we're also also dumps on some
initial explorations towards a neo4j
driver
so we already optimized their API to be
used with non JDBC data stores there are
different profile levels as we call them
and slip that you can use you can code
against relational profile or JDBC
profile and the more specific you are
the less choice you have for the
database on which your application runs
in the end but the more features you get
so if like relational profile is enough
for your application and you can code
against that and possibly run it against
MongoDB if the MongoDB driver also
implements the
national profile of course this might
extend to other data sources in the
future like web services or whatever
else once we have a model that can can
actually encompass JDBC databases and
different kinds of no sequel data stores
then we can really expand this to a lot
of other stuff and one big thing that is
coming up is an asynchronous API a
reactive API for slick right now all the
i/o that we do against the database is
blocking and of course a type C if we
want to push the reactive application
model that's what we do in slick that's
what we do in our car we do everything
non blocking so that it scales really
well and it's very efficient slick is
still blocking mainly because we support
JDBC databases and there is no non
blocking API for JDBC that is a major
limitation here and we have the idea of
designing our own API there unless
Oracle comes up with a standard which
would be great because it would get data
based vendors to support this but it's
bit different or even easier with no
sequel once we get there because for no
sequel there is no standard anyway so
even for a synchronous API we have to
write a driver that targets a single
database there's no easy abstraction
layer like JDBC so we might as well go
reactive and to the asynchronous API
that is pretty much what we we want to
do for the next major release of slick
that is coming up next year so that's
all we have here so if you have any
questions otherwise go to Schlichter
type safecom that's our website and
follow us on Twitter
okay so there is no support for store
procedures and the lifted embedding what
you can do is to use the plain sequel
queries and just wrap the sequel call
and have slick insert the parameters for
you which also makes this work very
easily what you can do in the lifted
embedding is take a database function
and with one or two lines of codes you
write a wrapper for that so you can call
your custom database function from slick
this this is a bit more complicated if
you get into aggregating functions but
for scalar functions it is really easy
to write you can also support your own
data types so if there's a custom data
type you can can write your custom
implementation for slick completely from
scratch or if you have a database type
that is close to an existing one or you
just want to map a database type to a
different type in scala then this is
also very easy to write with a mapping
type mapper as we call it
like virtually there various tracks
what's the normal way
so you you look at the immutable data
you have as a snapshot of the data at
the time when you looked at it that
moment so it represents the data from
that moment if the data changes the
database you would you would get it you
would always get exactly the data you
need at exactly the point in time you
need it and also not more you would you
can only get the fields you actually
need with or absence if they have to
like try to stay in sync because they
have want to have this local restricted
mirror of what's actually happening in
the database and they load some of the
fields and stuff like this sleek is very
explicit about that but it also means is
that it's also committed about that that
data you fetched before could out date
you write it bury that gets this data or
you have it very stored in some function
that actually does that you just call it
there's also a slick play slick example
project that you might be interested in
which uses slick as the data back and
for a play web application that might
give you some ideas of how to model this
in practice in a real life scenario
so enormous a a project done by is an
exit II the original office of play and
you have to distinguish between the play
open source project and play as part of
the types of technologies so we have
little influence over the open source
project they might want to keep a norm
in there when we're talking about play
as part of the types of technology so
it's play akka and Scala and all the
stuff that we support with the types of
subscription then we're trying to make
slick the standard Scala database
solution for that and we want to support
that better in in play applications and
also provide sample code like activator
templates that really target this usage
scenario yes we do want reactive first
this this is really something we will
invest heavily into for the next release
that's it's the major feature there yes
where the table is distributed over
several we had the same question little
while back it's it's an interesting week
we don't work on that at the moment but
it's a very interesting thing to look at
because that's what people do right when
the tables get too large they distribute
well once we support these kinds of
backends that will certainly be a
possibility we haven't quite figured out
how to design the API for that because
the the query scheduling using the
distributed driver has to use a specific
profile level at the moment the
distributed driver uses the relational
profile level so everything that is
below relational profile can be used
with that but it's very possible that we
also support other profile levels so you
might not have all the functionality
there it will be a subset of the
functionality of all the different
databases and data stores that you
support but I think it's possible to do
this it's it's definitely one of our
goals to support like Microsoft linked
like custom backends but that that is
neither documented nor very well
explored with non sequel database at the
moment but it's it's definitely
something on our radar yes
so we do not abstract over arrows so
when you get an error from the database
it is thrown as is at the moment it's
something we wanted to look into for
some time but it's always had to take a
backseat to other features so far it
would be nice to have a unified method
of obstructing over that if you're
targeting multiple databases what we do
is pretty much the the common sense
stuff like when you run in a transaction
and an error is thrown then it
automatically rolls back the transaction
but there is there's no common
abstraction layer for different
databases you will get the original Java
sequel or JDBC error out of that but
also since you are writing tight safe
queries certain errors will not happen
at runtime because it guarantees a
well-shaped query
okay then there no more questions thank
you very much for attending our talks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>