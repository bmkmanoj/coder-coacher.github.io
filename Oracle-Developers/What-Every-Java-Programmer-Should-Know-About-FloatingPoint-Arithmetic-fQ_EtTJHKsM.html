<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>What Every Java Programmer Should Know About Floating-Point Arithmetic | Coder Coacher - Coaching Coders</title><meta content="What Every Java Programmer Should Know About Floating-Point Arithmetic - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>What Every Java Programmer Should Know About Floating-Point Arithmetic</b></h2><h5 class="post__date">2015-06-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/fQ_EtTJHKsM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">our c4 pumpkin this afternoon I'll speak
about the hairy Java programmer to know
about 20-point birthday dear dachshund t
wonderful world
it actually is a wonderful
because it takes place in a future play
experience practice and you see
mathematics and engineering now this is
horrible state farmers walk no reason to
spend less time on today we see a lot
this week instead why make sure you're
aware that there are many other state
partner to make their discipline and how
he is for interest rates when those
virtual sliders and work Oakland or hi
Anna citron work last not least the city
five days ago
what is 2200 acres 20.3 music to the
systematic approximation
you'll admit like set of values as well
as a center operations on the
constellations
20:20 approximates the set of values in
the properties of the operations
department about as messiah like to be
there are species of ethical
communication to be precise
we're and credit and the number of
certain cases in at one point often
conciseness is there so all kind of all
these principles or sometimes
at all and out like a citizen point is
an approximation to originals so I
walked out but we'll use exactly look at
some point for mass together what
approximates copies will be taken care
of
and finally some lessons
first you start out with positive
integers and we just count now that's
not a very interesting game so we don't
stay there very long we quickly move on
to having all integers first adding them
up and then subtracting them that now we
get positive integers in zero along with
addition and subtraction and while this
team is very basic in retrospect it's
actually a fairly sophisticated
mathematical transition what we did in
creating negative numbers in zero was to
do something called completing the
addition operation right so we have a
subtraction it's a partial function if
you take two positive integers and you
subtract them that might not have to
defined answer right you subtract two
from one there's no positive integer
that makes sense
so subtraction is not complete over
positive integers so we prefer to have
complete operations so what we do in
this case is make up new kinds of values
that have the semantics we want so to
complete subtraction we created zero and
negative numbers and then we can profit
or take a loss in the case maybe for
negative numbers we don't stop with
addition and subtraction once we have
those we introduce the multiplication
operation and since we like to be able
to invert operations once we have
multiplication then we want to have the
vision so we can undo the operation now
multiplication on integers is closed but
as we know division on integers is not
all right sometimes it works out well
and sometimes it doesn't so what do we
do well we do the same trick again we
complete the function in this case
division and the new kind of values we
introduced a complete division our
fractions or rational numbers
although we still can't divide by zero
so okay now we have rationals and what
anyone can divide and we're starting to
look pretty good and actually at this
point we have a fairly interesting
mathematical structure called a field
these are the field axioms all fields
have these properties two of them and
the rational numbers are the first sort
of object like this we look into and as
you can see we do have closure under the
operations in question addition and
multiplication we have nice properties
like associativity so we can reorder
operations one context you may have seen
of the field axioms like this is when
people talk about compiler optimizations
because these are the sort of properties
optimizers rely on to rearrange and make
the expressions go faster so it's
because the underlying system say
integers obey the field axioms that
these are valid transformations
conversely if a system doesn't follow
these axioms then those sorts of
transformations would change the value
yet might not be valid for your system
okay so all rational numbers enough now
since we're approximating real numbers
you might suspect correctly that the
answer is no so is there a rational
number such that when you multiply apply
it by itself you get back to if we're
going to express that as a polynomial
would say is there a root of the
polynomial x squared equals two here's
this little proof that it's not all
right so let's assume the square root of
two is some rational number that means
it's some P over Q and of course P over
Q have to be in lowest terms so we can
square both sides so we have two equals
P squared over Q Square and then we can
switch things around and now we see
something interesting we see that P
squared is equal to twice something else
right happens to be Q squared but what
does that tell us about P squared that
means the integer P squared is even and
the only way you can get an even in an
even integer that's a square is if the
thing you're squaring is even to so now
we know that P is even okay so if P is
even that means P squared is divisible
by four because it has two in it so two
squared is four now we can look at Q
squared well we've just learned that
P squared equals four times something
okay so 2q squared that means a Q
squared equals twice something so now by
the same reasoning q squared is even
that means Q is even well now we have a
bit of a problem because we've just
shown that both P and Q are even and if
they're both even they have a factor in
common namely 2 therefore what we
assumed is incorrect we can't have a
value active which means the square root
of 2 was not rational now this was a
very disturbing to people when this was
first realized the there's an apocryphal
story about a mathematician pases who
figured this out and the people were
keywords traveling's with soap set they
threw him off the boat and he drowned so
but we don't have to get that upset
about it nowadays now you might think
well you know rational numbers were very
good why do we have to deal with these
irrational numbers maybe we can just get
by without them well consider something
simple like a unit square a square whose
sides of length 1 it's hard to find a
simpler mathematical object now if we
want to ask a very reasonable question
like well how long is the diagonal of
that square well we know from the
Pythagorean theorem that it's the square
root of this you know square the side so
we get radical 2 even when dealing with
something as simple as a square well
that's not very encouraging so here's
our first attempt at classifying the
real numbers we have the rational
numbers which are easy to understand and
then we have these irrational numbers
that aren't so we'd like the situation
to do with this taking roots a
polynomial seemed appropriate so why
don't we go back to our standard trick
and try to get all the numbers we can
from finding roots of polynomials and
when we do that we get a kind of number
called algebraic numbers algebraic
numbers are a field 2 so it seems we've
made some progress so if we want to look
at the classification now we have the
rationals Irrational's now luckily all
the rational numbers are algebraic the
reason is the rational numbers are a
over b and a over b is the solution of
this simple polynomial bx minus a equals
0 all right so we've classified all the
rational numbers and at least some of
the irrational was like radical 2 but
have we got
everything well let's take a look
consider this series here three three
point one three point one four and so
forth so this is the expansion of Pi
different digits and each member of this
series is a rational number and it
converges it gets as close to PI as you
want and it's bounded above now even
though it has these properties pi is not
a rational number it's even worse than
that pi is not an algebraic number so
there's no polynomial whose root is pi
as long as the coefficients of the
polynomial are rational and this is a
very famous theorem called the Lindemann
Weisser Strasse theorem from the 1800s
and you know if people are still
referring to the theorem with your name
in it it's an important result a hundred
years later so it turns out pi is
something else called a transcendental
number now again this might be a
unfortunate complexity can we just
ignore it if we ignore it we can't deal
with things like circles because amongst
many other things pi relates the ratio
of the diameter of the circle to its
circumference so it's a very basic
property of circles so to deal with
squares we need irrational numbers and
to understand circles we need these
transcendental numbers all right so we
apply our trick again with a more
sophisticated function these limits of
monotone series and that gives us as a
completion real numbers which includes
algebraic numbers and transitional
numbers and if we had much more time or
a real analysis course you can go
through and see that real numbers again
form a field they have those same
properties we saw before so now the full
approximation we have rationals that
you're assholes are one way of looking
at it but we also have the algebraic and
the transcendental numbers here now this
figure is really not drawn to scale
because almost all real numbers are
actually transcendental it's just that
we don't have names for most of them the
sum of the few ones we do have names for
things like pi e and combinations like e
to the PI now if you're a adventurous
mathematical gamer it doesn't stop with
real numbers there's many other thing
you can look at for instance there's
complex numbers we see these in in
physics or electoral engineering and
complex numbers you do something strange
you define a square root of negative one
so again you're adding to the real
numbers in that way there are extensions
to the real numbers like quaternions
these kind of fell out of favor
mathematically but are still useful for
managing rotations and things of that
nature and develop more recently in
1970s or something called Cir real
numbers so the real numbers go to
infinity and beyond they are the largest
ordered field you could have depending
on some assumptions and they are
invented by john conway who came with
the game of life and Don Knuth was so
inspired by this work from Conway that
he actually wrote a little novella
called surreal numbers now this takes
place as a dialogue between a man and
woman and is how 2x students turn on to
pure mathematics and found total
happiness so you can kind of think of it
a bit as a transient and it shades of
gray and that's why it has this adult
rating here so now that we've gone over
real numbers what are we doing with
floating point again it's a systematic
proximation to real arithmetic so how do
we want to approximate well not all
proximation x' are equally good so what
are the sort of properties we'd like
these approximations to have we like
some notion of determine determinism
reproducibility all else being equal
we'd like it to be fast if we have to
choose between an accurate and
inaccurate approximation it's better to
have an accurate one
and we'd like to be able do some kind of
analysis on it and we'll look to see how
the floating point approximation we have
satisfies these criteria so what does a
binary floating-point number look like
there's a binary significant times two
to the exponent so we have the bits here
and we say the floating-point format has
P bits of precision and for now we're
going to assume that b0 the leading bit
is 1 we say this is a normalized
representation there's a number of
reasons we do this makes it easier to
work with in particular it imposes a
unique representation on the numbers so
let's say you don't you
all pee bits of precision if you had the
be zero the first bit equal to zero you
could increase the exponent by one and
shift everything over right so there
would be multiple representations for
the same numerical value so if you
acquire normalization then there's only
unique representation and asked easier
to work with because of how this is
structured numerically floating point
numbers are sums of powers of two with
the caveat that the ratio between those
components is bounded so it's not just
in sums of any powers of two or any P
powers they have to be a limited width
apart in practice the exponent is a
roughly symmetrical range over positive
and negative values that means
floating-point formats can represent
numbers very small very close to zero as
well as very large far away from zero
and the format defines the set of values
based on the precision in the exponent
range so you can have multiple formats
that vary just by how large they are
now although we're approximating real
arithmetic each of the floating-point
numbers is actually a rational number
right because they can be represented as
a fraction we can write some power of
two that would divide this out an
important property in practice is that
for a fixed width so for a fixed format
floating-point performance is roughly
independent of the magnitude of the
operands and the result and of course
that's true except when it's not and
we'll talk a little bit about some of
the exceptions later than talk alright
so now that we've had an overview of the
real numbers in floating-point we're
going to start playing with a very much
a toy floating-point format so here's a
very simple floating-point format that's
good for slide presentations not too
much else that only has three bits of
precision and it only has three exponent
values minus one zero and one and these
are all the positive values of that
format written down here so we can
notice a few things a zero is a special
case so we won't discuss how that's
encoded right now but if we look at the
encodings are the other numbers here we
can see that they're all normalized
right the leading bit is one for all
these values okay
so the smallest value which happens to
be 1/2 has the smallest exponent minus 1
and then it has the smallest possible
significant right the leading bid is 1
and then the rest of the significant
bits are 0 so the next larger number we
just increase the significant value by 1
so it's so you read this as 1 times 2 to
the minus 1 or 1 times 1/2 so we can see
what the value is now this position here
is equal to 1/8 so 1/2 plus 1/8 and so
forth now we look at the values where
the exponent equals to 0 instead of 1
and they have the same sort of structure
the exponent stays the same and then we
just cycle through all possible values
as the significant and we'll notice that
for each setting of the significant here
it's exactly twice the value of the same
setting the significant with a smaller
exponent right so the first number with
an exponent of 0 is exactly twice the
amount of the first number with the
exponent 1 and again that's a
consequence of being encoding likewise
for the values were the exponent equals
to 1 instead of 0 all right so this is
what we get when we have P equals 3 we
can have another format where P equals 4
now when P equals 4 we have all the same
numerical values as when we had P equals
3 except we have another trailing 0 bit
in addition we have another set of
values where the new trailing bit is 1
instead so this has the effect of
between each of the P equals 3 values we
insert another value halfway between
graphically we can look at this too so
here's our real number line and we'll
start graphing the values were equals 3
starting with the exponents of minus a
minus 1 so we fill those values in so we
see those other ones less than 1 then we
fill in the values the exponent equals 0
and the exponent equals 1 now one thing
to notice here is that the disk
between numbers goes up when we change
the exponent so when we change exponent
values the distance between numbers is
twice as much as it was in the previous
exponent and that's a consequence of the
properties of encoding we saw before now
you'll notice here this graph there's
something seems to be missing right so
the distance between one half and the
next representable number even in this
toy format is much much smaller than the
distance between zero and that first
non-zero representable number so this
can be a bit of a problem when doing
analysis of floating-point so we
actually fill in this region with
something called sub normal numbers sub
below so these numbers have a special
representation and they're smaller than
normal numbers they smooth out the set
of values you have here now we can look
at P equals four so again we have all
the people's three values we had and now
we fill in the gaps between them so we
split the difference and then we get
there's a bonus an extra number a little
bit farther out on both ends okay now
the floating point numbers are imprecise
we only have a finite number of value so
for the we only have 16 positive values
for P equals three so that that's all we
have to work with including those sub
normal values so here we'll write them
all down so let's look at what we should
do when we have to return results say if
addition in this format so we take two
numbers in this format one half and one
and a quarter all right so those are
both numbers and we add them together
the exact value is 1.75 now in this case
fortunately 1.75 is also a representable
number so that's what we should return
if we can represent the exact number
surely that's the right thing to return
now we're not always that fortunate of
course how about we take something else
where we have two all right that's
representable and 1/8 we have that two
and the exact sum of course is to end in
a so we go down here so we have two and
then the next number is two and a half
so the exact value of this result is
bracketed between two values and it's
closer to the
- in this case so it seems reasonable
that we'd return the closest one
so we've returned - all right how about
this case we're 1 plus 1/8 one 1/8 is
not exactly representable but it's
halfway between to represent all values
right 1 in 1 and 1/4 so what do we do in
this case well to guide us in problems
like this we need something called a
rounding mode a rounding mode is a
mapping from the real numbers any point
on the real line to one of their
representing point numbers so we're
mapping from the real numbers to the
floating point numbers and to make it
clear that we're mapping only to the
representable floating point numbers
will take away the real number line here
so these are the only sorts of answers
we can return this is our full set of
numbers here again so let's consider
what happens around a particular value
say two and a half and our rounding mode
we're going to use is round to nearest
even round to nearest meaning that we're
going to return the floating point value
closest to the exact result so starting
at two and a half the next representable
number is straight so halfway between
two and a half and three is two and
three quarters so that means if our
exact result is between two and two and
three quarters the closest number is two
and a half so we're going to round down
to two and a half in that case likewise
on the other side of two and a half two
is the next representable number there
so the halfway point is two and a
quarter so if the exact result is being
two and a quarter and two and a half
we're gonna round up to two and a half
which is the closest representable value
so that same argument holds for every
other value there so there's a region
around each number a little bit more a
little bit less we'll round to the exact
value and we can see that by looking at
some particular examples so if we take a
value here like two and 1/8 what's not
going to round to two right it's gonna
round down to two all right so that
that's pretty clear now how about a case
like this where we're halfway between 1
and 1 and 1/4 this is where the nearest
even part to round to nearest even comes
into play
what rounds and ears even says if you're
in a tie look at the two numbers that
bracket the result and the one two round
two is the one with the last digit even
so if we look at 1 and 1 and 1/4 here so
one ends in a 0 and 1 and 1/4 ends in a
1 so what are you gonna round to you've
been around down to 1 right and likewise
if we look at the halfway point around
one and a half we can see that 1 and 1/2
is even so where are both of these
values going to go to I'm gonna go
go down to 1 and 1/2 now there is a
we've only done the graph out here 2 3
and 1/2 now we have results bigger than
3 and 1/2 so we should come up with some
some notion what to do here if we had
more exponent range the next
representable number would be 4 and then
we would again have a region around 3
and 1/2 that was bigger we round down to
3 and 1/2 so that seems to be a
reasonable sort of thing to do to add a
little additional rectangle here but
that does mean that we still have an
unanswered question about what to do
with the between exact results bigger
than 3 and 3/4 in this system because we
have no good value here in the set of
values to to use to return to that so I
suspect we'll have to complete a partial
function a little bit later than talk to
answer that question some of the
benefits around two nearest even this is
the closest to kind of a grade-school
rounding we use like well you know 5 and
up you at you round up to the next value
one benefit of this little round to
nearest even detail is that avoids drift
in certain times of summation loops all
right now with this toy format
we of course have rounding error when we
round we don't capture the exact value
so when we have two and we add 1/8 to it
the 1/8 rounds away and we get back to
the same thing happens in the P equals
three format when we add 1/8 to one but
you just get back one so we just take
that information away if we look at the
other format where we increase the
precision to and when a still rounds to
however one plot plus 1/8 we don't get
one back we get one underneath the exact
answer so this is a particular instance
of a more general thing we see where we
use a more precise format we get a more
accurate answer now precision is a
measure of how fine a distinction you
can make while accuracy is a measure of
error so to measure error you have to
have some notion of what the right
answer is and generally using precision
means you reduce your error as well but
they are separate concepts all right now
from always seen so far that this this
might lead us to have some concerns
about how floating-point arithmetic is
defined so let's say it takes like 2
plus 1/4 that will round down to 2
that's fine for a single operation but
what happens when we want to add up a
bunch of numbers instead let's take 3
numbers instead of 2 well in this case
we have 2 plus 1/4 so what's that gun
equal to all right so then we have 2
plus 1/4 what's that going to equal to
again all right let's say instead we add
up 1/4 we add up the second two values
first then we're going to get 1/2 and
then 2 plus 1/2 is actually 2 and 1/2 so
this is something very important we've
seen here that floating point addition
is not associative so the order of
operations matters and we'll see more
about that later than talk so as we've
seen from the rounding slide floating
point arithmetic is only a partial
function and we need to complete it and
the property we want to have is that
basic arithmetic on finite floating
point values there's always some
reasonable answers and we complete it by
adding a number of other special values
we have plus infinity minus infinity we
have something called nan not a number
and we also have minus 0 the reason we
have minus 0 is because we have both
plus infinity and minus infinity so that
you want sign symmetry on multiplication
alright so here's what the arithmetic on
n + infinity looks like now nan might
appear very odd but at one point you
know I is the square root of negative 1
and negative numbers appeared odd so
it's certainly unfamiliar but it does
actually follow sensible rules and the
summary is any time in real
arithmetic you would have an invalid
situation like zero over zero or the
square root of minus one in
floating-point that returns an app so
from the perspective a floating-point
nan is not undefined it's perfectly well
defined it's a result of these
operations now our nan arithmetic is a
fairly straightforward generally it it
tends to contaminate your result so if
you combine nan with anything else you
get an an back and nan has some unusual
ordering properties for any value a a
less than nan is false a greater than n
is false and a equal man is false
even if a is an app so this is very
strange but again nan is not a number so
it doesn't act like a number in terms
when you're comparing you infinities are
a little more familiar so if you add two
values that are big enough you get an
infinity so in our system two plus two
would actually equal infinity for P
equals three because it's bigger than
the limit and likewise if you divide by
zero you get the infinity arithmetic on
infinities is pretty simple once you
have them if you add something to
infinity it's still infinity and so
forth
now once you have infinities Nan's
sometimes come fairly close after so 0
times infinity is a nan that that's kind
of the best you can do and four
infinities they are ordered as you'd
expect negative infinity is smaller than
any finite value and positive infinity
is bigger than a finite value and
infinity is equal to itself unlike that
alright so now that we've seen all the
sets of values we have I'll take
questions at the end in the toy system
we can evaluate floating point as an
approximation all right again these are
the sort of properties we care about so
how about deterministic and reproducible
well if you look at the I Triple E 754
standard which has long been the
standard for floating-point it does in
detail to find the results of the
various operations you want it defines
the formats and the values and
everything so all this is is very well
defined and if you look at the
specification for CPUs and for this
purpose the Java Virtual Machine
specification counts as a CPU
specification you
see something that almost always defers
to I Triple E 754 in terms of the
definition so if you will pop open the
the jvm my virtual machine spec and turn
to the the page for the D add
instruction which talks about what to do
when you add double numbers it'll say
something to the effect of do what I
Triple E 754 says to do when you add up
to double numbers all right
so given that why does floating-point
have such an awful reputation for being
hard to understand if it's also well
specified in standardized well there's a
few reasons for that one is that those
field axioms most of them don't hold so
optimizations change the computer
results so it's not clear what's
happening to your code once it gets
running on the processor there's a
laxity in this language level mapping
maybe your language specification
doesn't say very much about how the
language level gets mapped down lower so
you just don't know what's going on
another issue often library functions
are not tightly specified so you're not
just using the basic arithmetic
operations you're using the math library
sine square root other things those can
change and a very important one base
conversion decimal binary and binary to
decimal conversion there's a lot of
surprises in there that cause people to
be confused about floating point and
we'll talk about all these a little bit
later all right so these are the field
axioms again the ones that are in Orange
do not hold for floating-point
arithmetic so we don't have very many of
them left so we do have closure under
addition that's good
we saw we've seen that addition is not
associative right it matters what order
we do things in there's not even an
identity element for addition so this
looks a little bit odd the reason this
doesn't work is because of negative 0
negative 0 plus positive 0 is positive 0
and you can tell negatives 0 and
positives 0 apart if you divide by them
so 1 over negative 0 is negative
infinity and 1 over positive 0 is
positive infinity so while they're
almost always equivalent negative 0 and
plus error are not always equivalent so
that's why this identity doesn't hold
so we're closed under multiplication so
you won't go through all of these most
of these fail for a few reasons
they can feel because of roundoff and
the floating-point operations or they
can fail because of the arithmetic on
the special values like nan and infinity
so if you think about what would Nandu
for zero annihilation so what's nan
times zero yeah right so so nan is not
zero so this one doesn't work for
example okay what about the job of the
programming language the Java language
specification does have a few sections
that talked about the floating-point
semantics they're short sections they're
relatively easy to understand much
easier for me anyway than the inference
stuff that's been added more recently
and what these sections require is a
subset of I Triple E 754 so you have to
use the I Triple E 754 formats you have
to have the full set of values all the
time
and subnormals infinities man's minus
zero all the has to be taken care of
with one exception you always have to
use the round ears even rounding mode so
that's the rounding mode that the
simplest that we saw earlier and the
base conversion is specified to be
correctly rounded all right so you may
have come across strict F P so a strict
F P is a very minor detail that most
people can ignore but I'll tell you
about it anyway it's a subtle detail
that harkens back to a time in the late
90s when there was an unfortunate
interaction between the Java
specification for floating point and the
floating point instructions found on the
X 87 CPU it was very impractical to get
the reproducible results needed so
strict FP was added when you needed
absolutely reproducible results and that
a fault FP was only slightly less
reproducible fortunately a few years
later this sort of concerns not very
relevant anymore once the SSE two
instruction set extensions came out and
in the opinion for and later in 2001 and
this is not really relevant on non at
x86 architectures so strict efi is even
more reproducible it is required in
certain instances for example in java c
we use direct FP to evaluate
floating-point expressions to use for
constant folding so the java language
specification requires constant folding
and you want to make sure that happens
the same regardless of what kind of
process
you're running on of note is that even
when we have math libraries that we want
to be reproducible we don't necessarily
have to make them strict FB so it is a
very limited you very specialized our
purpose of this a lot fire how about
Jabba's floating-point libraries in Java
the platform you have two libraries Java
line math and strict math and although
they define the same set of methods they
have different specifications in each
one and there's a trade-off between
predictability and in speed so in Java
like math you can use any implementation
as long as you meet certain quality of
implementation requirements in strict
math you also have to meet those quality
of implementation requirements but in
addition you have to use a particular
algorithm so this algorithm has been
very stable both over time as well as on
different platforms so this is your
reproducible result here
what are those quality of implementation
concerns we care about are there two of
them one accuracy accuracy of a math
method is measured in something called
hopes and hope is a unit in the last
place in the opal real number is the
distance between the two floating point
numbers bracketing it so we saw before
we had those discrete floating point
values so the distance between them is
an all of something in there and we say
a correctly rounded function as an error
of at most one half and hope and what
that means is you're always returning
the floating point value closest to
these act results and that's really the
best you can do but if you're returning
the closest value what more can you ask
for now most of the methods in math and
strict math don't have a 1/2 of error
instead they have a 1 alt err about what
that means in practice is that you can
return either of the floating point
numbers which bracket for true result
now often just that gross error bound
isn't enough to be useful
we need something called a semi monitor
since the requirement often it's
important to approximate the properties
of a function rather than just the
values so an important property is that
when the function is increasing we want
the floating point approximation to be
non decreasing we don't want it to go
down when the
going up so it can either stay the same
or go up we might not always be able to
have the floating-point approximation go
up because we don't have we might not
have a number we can use there right
while still meeting the accuracy
requirements so we'll see what this
accuracy requirements looks like with
the square root function so here square
root on the toy cools three range so
most a few things about it it's not
negative all right all the values are
above zero is nice and smooth and is
monotonically increasing all right so
this is always going up so here's what
the error a 1/2 old error bounds around
square root look like for the toy format
now the size of a no-cage is at an
exponent boundary and here the toy
values from before so and then this is
an output the result so we can see here
when the result is less than 1 the hope
is smaller so that corresponds to these
narrow boundaries and then when we go
above 1 the distance between numbers
double so the size are the old doubles
that's why you have this jump up here
all right so now we have one half open
one all bound so the one over ones or
twice as big as you'd expect and at
first we'll look at a not-so-good
approximation to square root so this is
a version of square root that meets the
100 pair abound it and it does exceed
the one half hole pair bounds so it's
not that good in that way but it also
has this semi monotonicity requirement
here right so between these two points
here the square root the true square
root function is increasing but
unfortunately our approximation is
decreasing so we wouldn't want to use
this approximation if we could avoid it
we much rather use an approximation that
looks like this so this is the correctly
rounded approximation to square root in
this format we can see and it's we can
see that it has the semi monotonicity
requirement it's either going up or
staying the same and it meets the arrow
properties
so this is the best we can do for a
square root of proximation in this
format now while it's easy to see what's
going on the screw looking at it
graphically that's not the only way you
can think about
you can also think about Oh just yes the
one half hope is a bounce sometimes you
have an exact value so there's no error
and other times you do get very close
but this graph is not the only way you
can think about floating point you could
also think about the square root
function as this table so these are all
the different floating point values in
this format we have all the negative
ones there the positive one and we have
the special values and we have the value
of the square root for each of these
values so this is all the information
about the toy square root function this
is it that that fully captures it a very
important aspect of floating point is
that while real arithmetic is continuous
floating point arithmetic is
fundamentally discrete you can think of
it as a table like that there's that's
all the information about the square
root function another important thing is
a floating point number only represents
itself without any other context you
don't know where the floating point
number came from as we saw with the
rounding example for maybe you got the
floating point number and it was exact
maybe you got the floating point number
and it rounded down from something else
maybe it rounded up from an exact value
you just don't know without any context
so all you have is a number the safest
thing to do is Exuma its exact and go on
from there because of this discreteness
from a certain point of view things like
floating point addition is just a big
lookup table now we don't just think of
it that way because we want to use the
correspondence with real arithmetic to
aid our comprehension and to help decide
what the right right answers alright so
we talked about these toy floating-point
formats what are the real floating-point
formats look like well they're a lot
bigger we have the float format that's
32 bits 24 precision bits 8 exponent
bits it has a range in binary and in
decimal the double formats bigger 64
bits much larger precision a larger
exponent means two so it can double him
represent values that are both much
smaller magnitude and much larger in
magnitude now while these are the exact
binary values for the format's people
like to think in decimals what are the
equivalent decimal
in terms of the decimal digits of
precision it's a little complicated
because the relative precision of binary
decimal varies at different regions so
for float it's between six and nine
decimal digits and for double is between
15 and 17 digits all the float values
can be represented as double because of
the way the format's work and here's a
fun fact over forty five percent of the
double numbers are actually numerically
integers so why is that the case well
what happens when our exponent is
greater than the number of precision
bits we have can any of those
significant bits be fractional they
can't so for to say if you have an
exponent bigger than 54 you know that
all the bits in that significant are
some kind of integer so floating point
can be too hard to understand because
it's almost half integers one reason to
you might think that double is twice as
big as float but that's not the only way
to think of it because of the increased
precision between two any representable
float numbers there are about 500
million double numbers so that means the
alt of a particular value as a double is
500 billion times smaller than the of a
float the error bars are 500 times
smaller and it's highly recommended to
perform computations in double instead
of float for that reason because you
have so much exposition the likelihood
you'll run into a numerical problem is
much much less all right with that we'll
talk a bit about decimal to binary
conversion while integers can be exactly
represented in any base the fractions
you can represent depends on the base
and you know the example we've seen from
grade school is that in base 10
one-third is a non terminating spatch
right point three three three three
three how about in base three in base
three a 1/3 is nothing special is just
1/10 all right so it's 1 times 3 to the
minus 1 all right so we see here that
it's a property of the base and many of
the floating-point sources of confusion
are related to decimal binary conversion
properties all right so why don't we try
to convert one tenth to a binary
fraction
all right well we'll start the
conversion algorithm going multiplied by
two you take the leading digit you keep
going that's all fine and then we notice
wait we have repeated state between here
and here so if we keep going we're just
going to loop through these values what
that means is that one tenth in decimal
is a repeating fraction in binary just
like 1/3 is a repeating fraction in
decimal one tenth is a repeating
fraction in binary now yes let's look at
the other direction going from binary to
decimal the rich initially this looks
more promising because alternating
binary fractions can be exactly
expressed as decimal the intuition is
that 10 equals 2 times 5 so all the
fractions in base 2 or base 5 can be
represented in base 10 and the proof is
kind of along those lines so 1 over 2 to
the K is 5 to the K over 10 to the K
right so 5 to the K is just some integer
and if you divided by 10 to the K you're
just moving the decimal point so there's
nothing that can cause a a non
terminating fraction there and since
binary floating-point numbers are just
sums of powers of 2 and the powers of 2
or their fractional powers of 2 or
integer powers of 2 all the powers of 2
can be converted to decimal without a
problem now whoa what happened in
practice if we wanted to use this exact
a decimal representation of the
floating-point fraction if there's an
exact representation maybe we should try
to use it all right well let's start
looking at what happens when we convert
you know negative powers of 2 to decimal
you know 1/2 you know that looking okay
but this is starting to get kind of long
so the smallest number we have in double
is 2 2 minus 10 74 so let's look at what
the exact decimal work for it's a
representation that looks like so it
starts out 4.9 4 and goes on for awhile
and it's actually not done here it keeps
going and going some more
okay so even without the leading zeros
which we've left out there are 751
digits in this digit block so it's a
little awkward to work with you know if
you wanted to print out a value to have
like this format specify you do like a
whole page to print this out so we don't
want to do this we want to do something
else instead so we want to step back
what are the properties we want to
preserve well if we have a round trip
like this start with binary convert it
to decimal and back to binary
floating-point we like to recover the
original value that way we can use text
for information interchange we don't
always have to use a binary format and
to do this the exact value is not
necessary we know that the decimal to
binary part has to deal with precision
in rounding so the approaches will use a
in exact decimal string for output but
one with enough precision that we can
recover the initial value so now as
we've seen there are many values that
will map to the same floating-point
number so it means there are many
decimal strings that represent
potentially the same floating-point
value we have that whole region around a
number that will round to it so there
are multiple strings for instance that
would round to a floating-point value of
equal to one so we could have 1.0 as
well as 1.000 and so forth so which one
we want to choose how about the shortest
one that seems reasonable but less
output and this is actually a general
trick using mathematics when there are
many possible answers in this case and
infant number answers if you impose
enough side conditions you can get a
unique answer and often have a unique
answer is easier to deal with than
having many answers all right and yes
we've seen before that multiple moments
go down there so how long should this
value be well it depends on the format
so it is a functional format and it also
does depend on where you are in the
range because of this floating precision
problem or between binary and decimal
all right
so if we have a the binary to decimal
conversion we use the shortest string
for that and that's generally inexact
then we have the decimal to binary
conversion and that's inherently an
exact but by being careful
this composite function is actually that
in any function so we take two inexact
processes and we get an exact process
back so that's kind of cool
and the takeaway message here is when
you see a decimal floating point string
think of its nearest binary neighbor
instead now you might say well this is
all very complicated dealing with
decimal strings for binary
floating-point is there something else
yes there is although you might be
forgiven for thinking that the cure is
worse than the disease there's actually
hexadecimal floating point literals
these were added to the revision of the
I Triple E floating point standard and
they've actually been in Java since JDK
5 now the structure the floating point
number is that you have a significant
and your exponent and there's a textual
representation that matches that a basic
structure so you have a hex significand
and an exponent so the advantage of hex
full hexadecimal literals is that it's
somewhat human readable it's obviously
unambiguous and it can be tied to the
format so we'll just do a quick example
here let's say we want to represent the
number 3 don't know is floating-point
one way to write that as a hex little
rule would be 3 times 2 to the 0 so 3
times 1 now this doesn't match how it's
stored in the floating-point number a
way to do it that matches how it's
stored would be this 0 X 1 so this is
the normalized representation so the
leading bit of one and then 0.8 so 0.8
in hex is equal to 1/2 okay so it's
leading bit set so this would be 3 times
2 to the 1st which equals 3 so this is
how forget to always store it as a
floating point number all right so now
we have enough background to to
understand some of the initially
surprising results we see in floating
point so for instance Y is 0.1 as a
float not equal to 0.1 as a double well
if we print out the hex strings 4.1 as a
float in a double will notice that the
double version is much longer and that's
again because 0.1 is a repeating
fraction in binary so when we use
forint with more precision we get more
bits at the approximation and because
between each float number we have you
know 500 million double values this
value as as I float is very far away
from the other double numbers in this
range so if you look at the exact value
of the float approximation for 1/10 we
get a number that's close to 0.1 about
the leading 8 digits are correct so that
again matches the precision but when we
use a point 1 as a double we get a much
better approximation instead we have
leading 17 digits correct and if we take
this float value and print it as a
double will again get something that
matches to 8 digits but it's pretty far
away from the exact double value when
you consider the double format all right
so let's say you add up one tenth to
itself 10 times what do you get well it
depends so if you take a simple
something like this where you just have
one tenth to itself and then you just
keep accumulating that sum if you print
it out at the end you will not get one
you'll get the biggest number less than
1 0.99 minute 9 so if you look at the
exact value of that it's has an air
about one part in 10 to the 16 now we've
discussed and you'd expect to get one or
you'd hope to get one maybe that's not
fair though because we've discussed that
0.1 isn't exactly equal to 1/10 it
equals this other binary value so if we
assume those binary values are exact and
add them up without rounding we get this
value here that's slightly bigger than
0.1 and if we were to take this value
without rounding and then round this a
double we would indeed get one back
let's say we decide to add things up
differently let's say we add a pairs of
values and then pairs of pairs and so
forth what happens when we do that
instead in that case we do get one so
this is another instance where the lack
of associativity of floating-point
arithmetic is showing up we do the
operations in a different order and we
get a different answer in this case we
get a better answer so once we know we
can get a better answer by doing things
differently the question is how
get those better answers well there's a
few things you can do to get a more
accurate sum you can sort the inputs
that takes extra time of course an extra
space and you can add the small
magnitudes each other first to reduce
roundoff you could use more intermediate
storage this pairwise methods end up
using more intermediate storage and you
can also use higher precision for
intermediate results so you can kind of
catch the rounding errors in the middle
and have a single rounding error at the
end and now you too can enjoy the
benefits of the secret robot Internet
based on this talk how about speed via
the approximation there is a very good
hardware support for normalized floating
point values add subtract multiply tend
to both run in not very many cycles say
less than seven cycles and be a
pipelined so every other cycler so you
can issue a new one so here are some
particular numbers from a recent x86
architecture divide is much slower
though so it tends to take a lot longer
and not be pipelined so these are
representative but what you find in the
mainstream processors so today
now what about other values like
subnormals and and infinities the
picture there is less rosy it varies
more the operations can be made fast
with some modest additional hardware
inside the CPU CPU designers don't
always choose to spend their hardware
resources that way however arithmetic
onions and infinities is very simple we
saw most of it on that slide where you
introduced names infinities so it really
is just a little bit of extra area to
make Nan's infinities go fast sub
normals are a bit more at work but it is
important to point out that the
representation used inside the FPU
doesn't have to match the format that's
used in the floating-point registers or
when you store the number so you can
partially decoded numbers to make them
go faster
how about accuracy
well we've seen here we have two formats
the double format has much higher
accuracy we can make very small
distinctions here so if we look at
the hope of one so what's the distance
between one in the next representable
number for float it's about one part in
10 to minus seven doubles much smaller
one part in ten to the minus sixteen
this is the same sort of effect we saw
with the toy formats or the distance
between numbers is much smaller when you
have more precision so at a micro level
this all seems to be not so bad if we
start looking at the operations the
basic operations are all correctly
rounded so if you assume the inputs are
exact you get the closest floating-point
value to the exact number so that's good
now the hard part comes in when you have
to move away from individual operations
and look at kind of what's the global
error so this is actually hard and you
might be relating your discrete
floating-point numbers to some
continuous process in the real world and
that's difficult and numerical analysis
is a very venerable if rarely love sub
discipline of computer science so how
many people have a cs2 green and didn't
take any numerical analysis classes did
not okay how many people raise your hand
then if you have a degree with or
without CS with without neural analysis
so yes I used to be mandatory I think
it's getting less mandatory as time goes
on one thing we can actually synthesize
higher precision operations out of
standard arithmetic then the reason we
can do have is that the rounding errors
are correlated they're not independent
and if we know exactly how big the error
is we can get rid of it so we don't have
too much time left but I'll just quickly
put up this slide here what this series
of additions and subtractions track
subtractions does is given a and B it
finds the high-order base of the sum of
a and B and in addition it includes all
the trailing bits is another word so x
and y don't overlap at all and they
represent the exact sum of a and B and
we're able to extract that because of
the rounding errors are correlated so
this is the sort of kernel you might
include in code that implemented higher
precision in floating-point now as you
see before simple sum can have very
pointing numerical behavior as we saw
with all those values kept round anyway
when you add them to two there's another
technique called compensated summation
has a much smaller worst-case err about
and it's implemented using a similar
technique to the one shown on the
previous slide you have the sum
contained in two doubles instead of one
you have the higher orbits of the sum
and then you have a running version of
the error bound and we use this
technique in the new streams library in
JDK 8 to help make sure that the sum you
get back is numerically pretty good it
does run somewhat slower but it is much
safer than just doing the simple
summation now we can go on to a few of
the lessons we've learned the main
lesson to take away is no one to expect
exact results in floating-point that
being confused about that is where most
of the problems come from and by default
you should not expect your
floating-point results to be exact
because you're having all those rounding
errors when you do do operations there
are some conditions though where you can
expect exact results in the double
format we can represent all the
contiguous integers between two to the
53 minus 2 to the 53 and two to the 53
so if you're operating on integer values
that haven't restored as floating-point
numbers in that range because of correct
rounding you'll get an exact result so
if you're your app if you're integer
valued add or multiply operates in this
range you'll get the exact value in
floating-point that's fine if you're
doing operations on powers of 2 since
that works well with a binary
floating-point again you can expect to
be exact there the special algorithm for
extending precision again you can expect
to be exact in certain cardinal values
of functions like sine of zero should be
exactly zero not even a very small value
now you should be cautious with money
probably in general but especially with
floating-point because you can run afoul
of some of the problems we've seen such
as the decimal to binary conversion
issues so you you can't necessarily
represent your fractional dollars or
pounds or euros as a double value so you
can get error there so if you're doing
that instead you should consider using
an integer type and instead of dollars
representing a sense or mils or whatever
the smallest unit is in multiplying
you have an integer value now you can do
that sort of computation as an integer
type and you could also do it as a
floating point type as long as you paid
attention to those range limits where
you can represent all the integers what
you really need to use decimal fractions
use big decimal that's what it's there
for decimal big decimal holds decimal
fractions exactly as well as decimal
integers now corollary of exactness is
checking for equality again unless
special circumstances hold it's usually
a bad idea to check floating point
numbers for exact equality unless you
know their exact you can check again
zero to avoid divide by zero but you
should be warned that if dividing by
zero is bad dividing by a very small
number probably isn't very good either
so you might want to be careful there so
here's the sort of thing that can happen
if you have a unfortunate equality check
so this is actually an infinite loop all
right you can see why from before right
if you keep adding point one to itself
it's never going to exactly equal one
and you just keep flying past the value
and actually eventually it'll stabilize
and not get any bigger so it won't be
very interesting so instead if you want
a counted loop if you want to add point
one to itself exactly ten times have a
you know the conical loop from doing
something ten times if you want to look
in the floating point domain say you're
waiting for a some kind of root fine to
converge or something use an ordering
comparison like less than or equal to so
you don't blow past the limit all right
for trig functions there's not very many
important values just very quickly so
you might initially surprised that math
dot sine of 180 degrees is not exactly
equal to zero and you know sign of math
op high is not exactly equal to zero so
is there a bug in the sign library no
this is not a problem
pi strands dental so it can't be
represented as a floating point value
and so it's not exactly equal there and
I've been hooked just briefly in the
Java platform we have a very strong
semantics around floating point the
language specifications and the VM
specifications have always been
very explicit defining the
floating-point semantics at both the
language and the class file level if
you're using other languages on the Java
platform
either's that compile the class files or
to other actually complain CPUs you may
not get the same sort of guarantees so
be careful there there's some other
references if this hasn't totally
exhausted you're interested in floating
point and since times up I'll be happy
to take some questions informally up
here all right thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>