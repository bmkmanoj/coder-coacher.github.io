<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Think Async: Embrace and Get Addicted to the Asynchronicity of Java SE 8  and Java EE 7 | Coder Coacher - Coaching Coders</title><meta content="Think Async: Embrace and Get Addicted to the Asynchronicity of Java SE 8  and Java EE 7 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Think Async: Embrace and Get Addicted to the Asynchronicity of Java SE 8  and Java EE 7</b></h2><h5 class="post__date">2015-06-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/gWhWnyrxnkU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">this is a section around whatever
asynchronous in Java EE and that
includes yo a six and southern where
there were some new features introduced
including asynchronous servlet
asynchronous restful services and so on
so I guess you have seen these slides
enough time already and I'm not going to
talk about any new features or make any
promises so you can ignore it safely I'm
going to have a little bit of
introduction first and then I am going
to talk a little bit about why do we
need to think about asynchronous
solutions why to think async rather than
synchronous development and synchronous
programming although i'm not going to go
deep into language level things like I
don't know threats etc etc what I'm
going to talk about largely about
architectural components that we have
and we can take advantage of like the
new racing servlet JMS restful web
services that can be executed a sink or
a sink ejbs that are introduced in ejb
3.1 then I'm going to show a little bit
of use case that very simply is going to
describe what can be done with all of
these asynchronous features that are
introduced into java ee 6 and 7 and of
course JMS which has been around like
since the beginning basically the reason
that i'm going to show you a use case
and imagine a use case is that going
through these slides and talking about
features and showing yokota snippets is
not going to make that much sense
without having a context on how these
are going to be used after that simple
use case i'll talk a little bit about
Jameis Jamie's has been around and the
specification never got updated for
around 10 years and after 10 years they
have updated the specification from 1.1
which wasn't maintenance release 22 and
it's going to be 2.14 Joe eight
I'm not going to go deep into JMS
details that needs a couple of sessions
of its own but I'm going to briefly
explain what JMS is and what are the
features that are added to jamies too
after that I'll talk about acing servlet
and how we can use acing separate and
why should be considered using
asynchronous servlets after that I
talked about a restful web services for
like a minute and the next 10 minutes
probably is going to be about
asynchronous restful services on how to
take advantage of that part and at the
end I'm going to just touch down on a
synchronization beams which are
introduced as part of did I bring
something as part of a JV 3.1 and
something that is not including the
agent and i just added was to talk a
little bit about the recent events which
is an asynchronous way of subscribing to
events coming from an event source and
then consuming them on the client side
we receive if you get time to do that
and if not this is the actual agenda
that I had in mind ok a little bit about
me I'm going to be a speaker here so to
give you a little bit of information on
who it is that green crispy i'm working
at oracle i have been working at in
glassfish team almost still a year ago
and then now i'm working in oracle cloud
organization i have a couple of books in
the topic of java java ee etc etc and
have been blogging for some time since
inception of glassfish and probably
since Joe a 1.3 I have been working and
providing solutions around the topic of
Hawaii and since sun microsystems turned
its application server into the open
source glassfish I've been pushing that
and providing support for that on my own
I tweet of course like everyone else
now why do we need to think I think just
imagine it like this that you are going
to get a coffee and when you order the
coffee you are going to hold your hand
like this until you receive the coffee
is that really how have you suppose it
is it should work know when you order
coffee you go on the other side and you
start checking your phone or if you have
a tablet you're going to start reading
something in or tablet or doing whatever
is that you would do so you're not going
to be just blocked after you requested a
coffee waiting for that coffee to arrive
because that is going to be waste of
resources on every level so what you do
you say okay I sent a request to get a
coffee and now I'm going to go and do
something useful with my time with my
resource until that coffee is ready so
lots of things in in our real
environment which we as software
engineers are going to put into an
automation are going to have some
asynchronous T involved that is why we
need to consider that okay there is
something there are some features that
provide us with asynchronous invocation
of things asynchronous requests and then
asynchronously processing the responses
another reason for having things done in
a synchronous manner rather than in
synchronous manner especially when we
are talking about larger components
rather than in 10 lines of code is to
isolate things so you can isolate the
processor who is going to process some
queries that you are sending and you can
have the part that create the query and
send the query in one place the one that
process the query in another place and
the one that we find the query in
another place and all of this is going
to be done through messaging without
having any hard coupling between each
other and of course you can tune things
easily and the maintenance is going to
be easier basically we are always
talking okay let's have something
modular because when it is modular
things are isolated and you can develop
it in isolation and we can maintain it
or find unit etc etcetera easier
on the other end of the thing when we
talk about a synchronicity someone is
going to use whatever service you
provide we are not providing a service
in invoice there are some clients either
human users order other AP eyes which
are eventually going to turn into some
interface for a human user who are going
to consume our services and nowadays
people are not looking to be blocked
when they put a request but rather they
expect to have some level of a
synchronicity involved so it is going to
be good for the consumer who are going
to take advantage of service that you
provide to have an asynchronous
interface in addition to do legacy or
the formal synchronous one that that one
may have okay so Java EE and
asynchronously since the beginning java
he had JMS and james is probably one of
the most successful and hopefully know
several respect guys here jameis is one
of the most successful components in
java ee and people has been used have
been using it since the beginning and
they have been using it with great
success because the interface was solid
and the design of the those interface
were easy enough to understand and easy
enough to use for different kind of
purposes even if you are going to have a
queue in in producer-consumer manner or
if you are going to have in a publisher
subscriber manner or whatever it was
flexible enough and easy enough to use
so it has been widely being used now in
several 83 and 3.1 they have introduced
some concepts one is the asynchronous
servlet which introduced in several
three and another one is a non-blocking
i/o for several 3.1 of course
non-blocking i/o requires the
asynchronous servlet to be present so if
your server is not a synchronous is not
going to be using non-blocking i/o
because that doesn't make sense then
there are some asynchronous teen jax-rs
jax-rs 2.0 provides a
increments service invocation from the
client side and asynchronous service
consumption or asynchronous service
provider at the server side so you can
develop your service your restful
service in an asynchronous manner so
that the client could invoke it and then
client will get a call back whenever the
response is ready for that for that
request I'm going to talk about each one
of these in details and at the end we
have a synchronization beans a
synchronization beans are basically
created to have a fire-and-forget or
fire and wait for business methods which
I'm going to go into details of it now I
mentioned that there is going to be an
imaginary design and we are going to
talk about that imaginary design
throughout this presentation for a
little while I was working with a
pharmaceutical company and we had this
really really big piece of software
running on oracle database as a
cartridge to query for a silikal data
that querying could take minutes or
hours because it needed to make a query
and then it's required to verify each
one of those drugs that gets from the
query with another web service and
eventually after lots of processing was
going to send back at the query result
so when I was thinking about it when I
was creating the slide of it okay maybe
it's time that I say that problem could
have been solved this way using the
asynchronous features that are provided
in Hawaii so there's a client client
send the request to an asynchronous
tablet asynchronous already received a
request and just release the thread but
before releasing the thread it is going
to send a message to a JMS queue that
this is the query that need to be
processed and this is the ID for this
current so somewhere outside the
container there is sitting a JMS queue
consumer and this consumer is going to
pick up this query message that we send
process it and start sending back chunks
of the queer
result as JMS messages to another queue
in the other q their seats and MD be a
message or even been and that message
that event being is going to pick up
these chunks of query row by row or five
row by five row as that server side as
that big piece of software is able to
produce them and send them back to the
client but the question is okay how can
an MD be sent back things to a server
client that is being done through
something called an asynchronous context
or a scene context when initially I
mentioned that the request arrives to
this asynchronous servlet and
asynchronous alright just released a
thread that is when it creates an
asynchronous context that asynchronous
context is the doorway to access several
response service requests and do any
kind of manipulation that is required on
those two object for example you can
start writing bits of response into a
separate output stream that you acquire
through these async response so just
keep this picture like in mind and I'm
going to go to each one of these
components with a little bit of details
so the MDB is going to receive these
chunks of results and then MDB receive
the chunks of result it is going to look
into that message that it received and
in the message there is an ID that idea
specifies what request has been made to
result in this query so there is a
client who sends the request and there
is an idea associated that asing context
as I mentioned so it's going to look
into the cache pick up the pace in
context for this request because the JMS
message that is received has the ID and
write back a little bit of response into
the output the string and on the other
side the client which is a browser and
has some JavaScript module in it is
going to pick up this response and
render it in in a proper manner or if it
is going to be an SS
we are going to have our servlet produce
SSE data it is going to be an SEC
consumer we will receive the event and
consume that event okay on the client
side because servlets are the basis for
every features that we have in the web
container so if if you're talking about
jax-rs stuff Jack source of are
basically using or taking advantage of
servlets and using servlet we can
produce any kind of response that you
want so when I mentioned about SSE we
can have our servlet develop to produce
SSE data or sse data format or it can be
a traditional long polling comet client
who is going to receive these chunks and
start rendering them okay with that said
I'm going to go and briefly talk about
JMS so jameis is is basically another
middleware in this whole stack of
middlewares that we have sometimes you
run your JMS broker embedded in your
container if you are using for example
something like glass fish and your glass
fish is running in developer mode you
are probably running your your gem is
procuring and embedded and sometimes you
have your JMS cluster your JMS broker
clustered outside the container and
connect it to the container through this
connector architecture so that's the
broker that's where the messages go to
that's where the messages get routed
that's where the messages get persisted
redelivered and so on and so forth then
with the JMS architecture we have the
message itself the message which is the
body or the artifacts that one party
needs to send to some other parties that
some other party might be just one
consumer or it can be multiple
subscribers so we have a concept of Q
which has the producers and consumers
included in it producer is the one who
is going to put these messages into the
queue and consumer is the one who is
going to pick them up from the queue and
consume them but when a message is
picked up from the queue
it is going to be just consumed by the
one that has picked it up it is not
going to be available for another one to
come and pick it up then we have the
publisher subscriber map model in which
anyone who has subscribed to a
particular topic is going to be
receiving the message that has been sent
to that topic and as I mentioned it is
accessible from inside the container and
outside container it means that it is
not something that is just working when
you have a java application server you
have you can have your tomcat and from
inside tomcat and not Tommy or tom cat
plus or water the name is from tomcat
you can send the messages to your JMS
server and from the JMS server you can
consume them in a joist if a standalone
client so the jamis too they added some
JJ mr. was sort of a post and evolution
and the revolution the the part is that
despite the fact that James API was
pretty simple to use it had lots of
boilerplate code and what they did for
gem is to is to remove this boilerplate
coding and to some extent adopt the
annotation and simplicity that that was
started with java ee 6 and continued
with joe a 7 so i'm not going to go
through the parts that they simplify the
api which they have done a great job
it's simplifying it but they have added
some features that i'm going to mention
here first feature which is really a
nice one is to have multiple consumers
being able to consume the message from
from the same subscription what does it
mean is that you're a subscriber you
subscribe to this topic and when the
messages arrive you pick up the message
and start processing it but processing
it takes so much time to do it one by
one you can have multiple threads now
and those multiple trees are going to
use the same source driver and start
consuming those messages so you won't
need to wait for each one of those
messages to be consumed in one thread
and go to the next another feature that
is that is delivery delay so you can
specify when
want this message to be delivered and
the broker is going to take care of that
you send it now you deliver it in 10
minutes they added a little bit feature
for a synchronicity of course that's a
topic of this talk and the asynchronous
is to send the message in an
asynchronous manner you may ask ok this
is JMS sending the message should not
take any time why would one wants to
have an asynchronous send the reason is
that when we have persistent messaging
it is going to take a little bit of time
for the broker to acknowledge that ok I
received the message and I persisted it
in the database so the asynchronous
message sent is going to release the
burden of waiting for that persistence
to happen so we just fire the message
send it and you go to send the next one
for example or you go to do something
other than that and then you will
receive a call back that ok the message
acknowledged and is now received and
persistent there is a message delivery
count it was optional now it is
mandatory so every JMS broker is going
to give you a count on how many time it
has tried to deliver a message and you
can decide based on that and ok this
message cannot be delivered and you just
put it in the in hold there are some
features added or maybe I can say there
are some features a standardized for MDB
configuration as part of a
message-driven annotation as well as in
the XML configuration that some of us
may still be using and those things are
like durability of subscription
destination type what is the
acknowledged mode is auto it's not
Ottawa and so on and so forth so this
makes jameis post easier to use because
they updated the API and more
straightforward because they have
reduced some of the confusion for the
optional features and they have of
course added a synchronicity which which
we are going to talk back
okay several at three and several 3.1 so
in server three we have the concept of a
synchronicity but why do we want to have
a single server then what does it mean
to have a synchronous servlet is it
going to just help us with what so first
and foremost is that when a client send
the request client is not going to
disconnect and send the request again
for this asynchronous several client is
going to be waiting there for the
response to arrive because this is HTTP
it is one way you cannot initiate a
connection from the server to a client
client send the request and when clients
and the requests we receive this request
in our servlet we look at the request
and we see that okay this is a request
that's going to take time to complete
what we do is that we release the
container thread that is processing this
particular request and say okay the
container third can go and pick up the
next request while we are processing
this long-running request that this
client hasn't client is still waiting
there but we are processing the request
in a separate thread and then we are
finished processing the request in a
separate thread we are going to tell the
container that okay we have finished
processing this particular request and
the container is going to wake up a
thread and say okay now is time that you
send back the response to this client
the response that you've received it is
it is pretty straightforward to use
asynchronous tablet excuse me
it is pretty straightforward to use
asynchronous omelette it is either an
attribute in your XML configuration I
think supported through or an attribute
in the annotation that you have web
server it asing supported through and
that is going to let you take advantage
of a single and servlet in developing
that servlet and there are a couple of
classes and that are going to help us
with with all of those a synchronicity
and then suspending and resuming the
request that I talked about and the most
important one is the async context that
we get that context when we actually
start processing request in an
asynchronous manner and then we have an
async listener which we can register
that listener to receive callbacks on
important events regarding this
particular request so asing context we
can use it to set the async listener of
course we can do request dispatching
using a Singh context we can access the
server at request and service response
and we can write down a chunk of data
into the output the stream of the
response or we can say that ok
processing this request is completed and
set it completed third time outs that we
can set and the callback is going to be
notified when a timeout happens and
these are all be done through a Singh
context and initially now if you
remember that diagram that I showed at
the beginning we had an async context
and that was connected to some box named
cash what we were doing we were getting
the async context in a stance and you
are putting the async context in a sense
along with the ID into that cash and
then we were sending the JMS message
including that ID and the query
parameters to the back end to a start
processing the query and of course on
the way back each JMS message that
carries some some bits of the the query
result had that ID so that we could get
back the right facing context from the
cash using the ID and start writing down
the results
so a single is an icing listener is to
just make sure that we receive callbacks
on important events what are the
important events important events are
when it is completed when there is a
timeout where there is an error and when
we start the whole aging process of a
particular request probably the
important one our error and time out the
error happens when processing fails and
we want to tell the client that okay it
has gone wrong so we write a different
type of response to the output the
stream to tell the client that okay it
has failed and the other one is on
timeout processing this request has
taken so much time that we just say okay
it timed out and then on timeout is
going to be invoked and we know that we
should send a response appropriate for
time out to the clients so they know
that the timeout happened for their
query because it took so long to the
process okay a little bit of code so
this code is showing to some extent what
happens in the async separate box data
showed in the in some of the initial
slides so we compute an ID for this
request however we want then we put the
request put the async context that we
get from a start async along with the ID
into the cache and then we send a
message that describe the query along
with this ID to that q that I mention so
it goes the j ms q that was on top and
now the back end processor is going to
pick up that message and stop processing
this query and then turn and send out
the response
okay oh please feel free to raise your
hand and ask questions or if you have
any comment not that much really the
only impact might be that if there is no
container thread available to process
something and the request waits for
container 30 to become available other
than that client won't see any
difference when we use async service
because from the client's perspective
client is still waiting to receive the
response on the on the server side of
the thing we are releasing the threads
to make sure that we get a little bit
more resources for the next request that
are coming in does that answer thank you
I'm sorry I I couldn't I couldn't hear
you
yeah of course so from clients
perspective nothing changes client is
going to open the request and he's going
to have it open and waiting for these
responses to arrive so a client won't
know that we are using an async
processing inside our our separate
container client is going to send a
request and they're going to wait for it
to arrive yeah exactly it's the
container threat container thread is
going to get released so it is it is a
grizzly is doing that at the lower level
using asynchronous i/o that grizzly has
implemented so I'm going to talk a
little bit about that but I'm not going
to go into into the container details of
how grizzly is doing using selectors for
asynchronous i/o but what happens is
that there are threads and there are
container threats container thread is
going to take care of processing that
when you send a request to the servlet
what happens is that someone should
start processing your request someone is
going to be holding this request until
it is finished to send back the response
what we do is that we release that
someone and delegate the work to another
thread outside the threads that
container is using to process the
inbound requests so
request timeout socket timeout is that
also honored by the stone yes yes it is
yes of course it is entirely independent
so what happens with this time out is
that ok I have been waiting for too long
for that separate thread to finish
processing this request in our case I've
been waiting for too long for the MDB to
send all of the query results and
conclude this so that is an applica
plication level and down in the
application server level we have the
time at configuration the number of
threads that container can have for this
particular task and so on and so forth
yes those time out or are at a low level
at the socket level this time out as at
application level so if this request
takes a really long time you know it's
still good time down yes yes of course
no not really if you set a time out in
your application server to be 10 minutes
while you know that none of the request
is going to take usually more than five
minutes it's going to be all ok but if
you say the time out in your application
server for five second while you know
that all the requests at all of the
majority of the requests are going to
take more than five minutes it is going
to defeat the purpose of course so there
are some some configuration that you
need to consider at the application
server level but to my knowledge none of
the application server uses something
less less than like 60 seconds for as a
default value for for the timeouts but
definitely I don't know all of them so
there might be different values so this
is this is at the application level it
has nothing to do what what
configuration you have at the
application server level
ah sure
okay that's that's a pretty good
question it depends if client is using
something like background Ajax to get
these chunks of response and then render
them of course client can do multiple
things otherwise if it is it is not
going to be possible because we are
going to be the one who is going to
write the output to the client we are
going to be just writing bits of out
with and they are going to start
rendering them it depends on how you
code at the client side it is not going
to be something as straightforward that
okay if I'm using a Singh servlet it is
definitely going to be a sink in the
client silent client is going to have
callbacks for this node client is going
to be a still the same old thing that it
was but in the server side we are going
to have this flexibility of of not
holding the container threat while we
are doing a business processing when we
go a little further I'm going to talk a
little bit about SSE and that is going
to probably give you some some ideas
about about I synchronoss steam client
as well as the web services if you get
the chance sure seem to be that this
weapon tempers are bigger thing of it
tells the reader application servers
don't read and use that concert anymore
what concept the concept of the rivers
will be web container thread pool they
use it because there are some limits on
at the aptly added at the OS level on
how many threads you can allocate and if
you really leave this unchecked you are
going to run out of number of processes
that can be open by the operating system
of course so what happens is that you
and aunt is pretty simple draw some
configuration in the operating system
soft limit and hard limit on number of
foil pointers or process pointers so
when you reach that number of process
pointers because you have allocated
gazillion number of threads is going to
stop allocating more threads
excuse me yeah of course of course too
many life threads not so it is not going
to be unchecked so definitely there are
checks there are limits on how many
threads can be hold open and running
you're still holding the fog yes it is
just going to release it is just going
to release it's just going to release
one resource in favor of another
resource it's just going to release the
container to it because you have set the
number of containers that that you're
going to process the inbound request to
25 and use a executor service threat
because you have a different
configuration for your executor service
or for your work manager it is a sort of
delegation of the task from one thread
to another thread and and the reason is
basically not to keep all of those
threads that are designed to or
configure to process the inbound
requests for taking care of the business
processing end of thing but rather
delegate the business processing end of
thing to a thread threads with different
priority with different configuration in
a different place and then get them back
and send them to the client whenever the
response is ready
okay non-blocking i/o so non-blocking
i/o goes down with the fact that we may
have a slow and fast type of clients in
in I or level meaning that i have an io
thread io thread is waiting and reading
byte by byte the information that is
being sent by the client or the io
thread is writing information or data in
a really slow manner and I'm still
holding that oil turret hostage to this
slow connection so what happens is that
container is going to tell us whenever
it is possible to write a proper chunk
of data into the client or read the
proper chunk of data on the request that
is inbound from the client and when that
read and write happens we are going to
get the call back to process that data
that that you are receiving okay so you
have added some some new classes and
some new methods to serve it I never get
bored of this way to several 3.1 the
reason I write listener are going to be
to listeners that container is going to
notify the application developer which
is us through those listener that for
example data is ready to be read or it
is possible to write data now and we
have some new methods for output and
input the stream to check to set this
listener check what is the status of
input and output the stream now we have
the on data available on all data rate
and on error for a read listener so what
happens is that server container is
going to be taking care of reading I oh
and dealing with IO and whenever it is
possible to read something from the
inbounds request it is going to tell us
that ok there is data available that you
can read and when the request has
arrived entirely and the server nodes
that ok the request has arrived the
whole
message completely it is going to tell
us that ok all data is read and you can
proceed to processing that data on your
own and if an error happened during this
time it is going to notify us that ok
something has gone wrong with reading
the data eg a disconnect or whatever
that that error might be and
we have a right listener and the right
listener is going to be used to write
the responses whenever writing the
response is possible to the output the
stream so that the client could could
receive the data it is possible to use
this a non-blocking i/o in that initial
diagram that I showed in the server side
of the thing but I basically excluded
that because to keep it simpler and
easier to describe with this set I'm
going to move to jax-rs to and what is
new with Jax RSQ in term of
synchronously and asynchronously which
is topic of this talk so restful web
services and the whole idea of using
HTTP architectural style to expose some
functionality is really getting the
momentum meaning that more and more
people are thinking about exposing their
services and functionalities through a
restful interface now independent of
what qualifies an interface that you
have just provided through your web
server as a restful interface there are
possibilities and drawer features that
are provided to jax-rs that you can use
to to make this service that you're
providing more appealing specifically to
people who are going to be using jax-rs
so why would we need a synchronous web
services the simple answer is because we
don't want to keep someone hold hostage
while we are processing your request
it's the same thing as someone entering
a starbucks and asking for a coffee and
then waiting there until that coffee is
ready or going back somewhere do
something and receive a shout out that
for example m your coffee is here come
and pick it up so what Jack Soros does
is that it provides the same level of
call back through restful web services
you have your restful endpoint and you
have your rascal client your respect
client is going to receive a call back
when you're restful endpoint has
processed the request and have the
response ready
send just imagine it like this that that
you have a little application and the
little application need to you are
reading something and you receive a
little bit of notification on top and
little bit of notification is from the
request that you submitted a while ago
to download the book or to get next
chunk of an audio book that you have
been listening to and that is something
that that can be sort of said as a
sample of using asynchronous T now it is
pretty straightforward to use to create
a synchronous web services asynchronous
principal services not web services when
you say web services people we think
about with the lasso so the starting
point is to have this a signal
sanitation on at the end point
asynchronous annotation tells that okay
this sub resource is going to be
processing the request in an
asynchronous matter what happens is that
it is going to take advantage of
asynchronous servlets to provide a
synchronicity at jacks or s level
because jax-rs is basically using the
servlet container and servlets and
back-end to provide the services
provides it is possible also to decide
whether you want to process a request
that has arrived to your web service
endpoint in an asynchronous manner or in
a synchronous manner you detect that is
going to take long to process it you
start processing in an asynchronous
matter and if you know that it's not
going to take long you're just going to
actually process it in you know for
straightforward manner lens and send
back the response now in servlet you
remember that there is something called
a sink context that using that asing
context we had access to everything
about that pair of request response that
you're processing an asynchronous matter
here we have something called a sink
response and using a sink response we
can decide when to conclude it went to
complete and asynchronous request that
hazard has been sent to our restful
service endpoint of course there are
some callbacks completion call back and
connection call back completion call
back just notifies you to single method
interface the sound that tells you that
okay
processing has been completed and the
connection callback is going to tell you
that okay what happened is that before
concluding this yes these two are
functional interfaces these two are
single methods
no York so there is a move to make more
of the interfaces especially the
interfaces that are being added now to
make sure that there are some interfaces
but they are not going to start changing
the interfaces that has been around for
this matter unless it is they come up
with something really a straight forward
without the spending that much resource
to do it so completion call back and
connection call back let us know status
of a particular request and the
connection call back is going to just
notify us that for example the
connection has failed the connection is
closed before we conclude the processing
the request so I've already talked about
asynchronous connotation and the async
response which provides recently the
callback and manipulating the response
the async response need to be accessible
through our restful a resource and that
is we are injecting a suspended acing
response into the sub resource and start
using that for further processing the
request now this is a little bit of code
that shows how it works so we have an
asynchronous a notation that tells the
container that this resource is going to
be processed the requests are going to
process in an asynchronous manner then
we use the async response to be used a
sink response to notify that the
response is concluded resume it or
suspend it the other way around it is to
use the execution context and using the
execution context is the programming or
programmatic way of deciding whether you
want to do it in an asynchronous matter
or synchronous manner in which you use
the async can't use the execution
context to notify to suspend and resume
the processing on the client side it's a
straightforward on the client side you
get two ways of processing the response
one way is
to get a feature and you're going to
pull the feature or wait on the future
until the response has arrived so you
check on the future to to see whether it
is done or not you do polling or you get
to be the time out and when it is not
receive it you didn't receive the
response in your desired time you're
going to fail it and the other way
around it is to use an invocation call
back the invocation call back is going
to provide you beat a call back when the
so-called response has arrived but the
point with the with the callback is that
if if you receive a failure response for
example 404 or if you see the 200 is
going to be a still a successful
response the only time that that you are
going to get the error on error of this
invocation callback is when something
fails at the client site for example so
any kind of response that you received
from the server is going to say yes you
do get fail I don't think so that is
that is not that's not how it is how it
should happen are you using jax-rs
definitely ok ok I can I can look into
that and when when did you try it is
like after the release or when it was
just a draft oh it comes from the I
offer no okay from the Oliver okay okay
I can double-check that
okay completion call back you get a nun
complete and for the connection call
back you get on disconnect if the
connection has failed before you finish
sending back the result now one thing to
mention about this asynchronous web
services is that you have probably used
a synchro so you have probably used
restful web services that are taking a
long long time to complete for example
if you try provisioning a service on a
virtualized environment you are going to
use a restful endpoint to provision that
service provisioning a service in
virtualized environment is going to take
a long time and that that is not
something suitable to use a async jax-rs
restful endpoint what it is most
suitable for is that when you know that
there is a single transaction single
step process that you need to take care
of and it's going to conclude in some
seconds in not in some minutes for those
cases you what you can do is to start
developing your own sort of small
request processing framework to process
this request a step by step through
different actions and then conclude it
when when when you finish processing all
those actions so for those cases you can
send back something like request is
accepted request is accepted and you
send back a location and retry after so
that people know that these requests
that they have placed on your service is
going to take a long long time to
complete and when it complete there is
going to be a resource in this location
that they can access to see the result
of this processing
okay ejb 3.1
so ejb 3.1 has a little bit of feature
that falls into category of this talk
and that is to have all of the excuse me
business methods of a ejb as a
synchronous or a single one of them
depending on where you where you put the
annotation to to specify a synchronicity
of the ejb so the good thing is that you
have fire and forget you can fire a and
an ejb method and forget about it and
that is going to be processed and and do
whatever it is supposed to do the bad
thing is that there is no call back sure
you're using cfd client will call server
imposter
okay so you're using container managed
beans a CMT and he's in a clustered
environment and one of the one of the
nodes that is processing yours is
failing what will happen to dare to do
to the whole request
I cannot comment on that I don't know
okay so there are two ways to invoke an
async last method one way is to do it
through a through a fire-and-forget you
just fire the method and it's going to
process and you won't get any result and
the other way is to get a feature and
then pull on the future or wait on the
future to get the completion of the
request that you have seen that you have
placed on the jb there is a synchronous
annotation of course that you can
specify on the method or on a beam and
the method returns a feature but within
the application server is going to use a
wrapper around the feature and death
trap is just for application server use
only and not for the end user and user
use and that's the asyncresult a very
basic code for using a sink ejbs is that
you start you start the you start
processing request and when there the
request is there and you sent back you
return back an async result of the
result this racing result is going to be
turned back into feature that the method
is returning and the client is going to
start using that feature to check
whether the result is ready or not on
the client side up the point is on the
client side you can inject or look up
ajb and when you look it up and you've
got innocent or a reference to that to
the ejb you can invoke the methods that
you want get the feature and then pull
on it or get on a time out until until
receive a response or you timeout and
the whole processing phase ok SS e so
the whole idea of html5 and all of these
components that that are part of html5
including like a web sockets
communication protocol or SSC have some
certain way of using them have some
certain effects on the way that you are
developing our software for example web
sockets with full duplex communication
negotiable to to just make sure that you
start with HTTP and then you go to two
plain a TCP level but the point with
something like WebSocket is that not all
components in your way or supporting
with the SS e is plainly on HTTP and it
just has a specific data format that one
should follow to produce SSE messages or
consumers to see messages so it is not
something that complicated to produce
those messages or consume them so what
happens is that there is something
called an event source and anyone who
wants to receive the events that this
source is producing them is going to
subscribe that even source and is going
to receive the events as they become
available so what happens is that the
events can be numbered the events can be
ID'd and whenever you disconnect and you
reconnect you can say that ok i have
received till 120 and i want to receive
the messages after 120 and depending on
whether the server side of this story
supports reconnection with resume point
you are going to receive the events
after the point at York yeah I've got
disconnected at the client site you have
the possibility to handle errors and the
New Year's Eve callbacks when a new data
message arrives or when an error happen
and so on and so forth so Jersey which
is the reference implementation of
jax-rs has some some internal APR not
internal proprietary API and those
proprietary API makes it possible to
have a jacks or s web service
acts as a SS e eventsource so you
develop your Jack source web web service
and acts jax-rs webseries can act as a
sec event source and send back sse data
but that is not part of the part of the
jack soloff's is just part of the jersey
that the property api in jersey that if
you use you need to use jersey libraries
in the client side as well as you need
to be sending chunks of data chunk by
chunk data is going to be sent as SSE
messages and SSE messages are basically
text you have a data and then the data
in front of it and then line line to say
that okay this is the end of message or
a single line to say that okay this is
end of first line of data and there is
going to be more line of data in the
same message so producing that is is a
straightforward using any kind of server
citing like servlets PHP's or whatever
so with that said I'm done with the
slides and if you have any questions or
comments or or something was missing
that that you were looking for I'd be
happy to to listen and hear
yeah of course and if but all modern
browsers supports it and if it is not
supported there are poly filling
libraries that you can use
especially stuff
yes when you complete the request is
going to be gone because that whole
business is concluded
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>