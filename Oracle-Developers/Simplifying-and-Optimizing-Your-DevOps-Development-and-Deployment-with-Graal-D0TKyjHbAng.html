<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Simplifying and Optimizing Your DevOps Development and Deployment with Graal | Coder Coacher - Coaching Coders</title><meta content="Simplifying and Optimizing Your DevOps Development and Deployment with Graal - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Simplifying and Optimizing Your DevOps Development and Deployment with Graal</b></h2><h5 class="post__date">2017-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/D0TKyjHbAng" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">alright I'll get going hi
so I'm Eric Sadler and this is Thomas
Burton ger we're here from Oracle labs
to talk about grow VM and this is the
talk that's kind of more practical
issues with respect to using graal VM we
have another talk tomorrow which is the
developer side stuff and then one about
using brawl in the database and of
course they don't take anything I say
into account for purchasing decisions
thank you
we're just from the labs anyway so so
the main focus that we wanted to kind of
have you think about a little bit when
you think about how we program and how
we develop applications is that you
probably don't actually get to run all
of your applications in Java or any one
at language we live in a multilingual
world right so if you--if you have more
than a few applications in your
enterprise you probably have a for at
least a couple languages you're running
we tend to use different languages for
different jobs you know most of the
people that are doing data science are
probably using R or Python if you're
building a web front-end you're probably
using a language like Ruby or JavaScript
and you know your business logic or your
server back-end is probably written in a
language like Java or C or C++ but the
problem is that of course most of these
most of the runtimes for those languages
are not very good at running other
languages you know you can you can run
JavaScript on the JVM today with Nashorn
but it's not very fast and it doesn't
run a lot of code like as one node it's
not as good at running Python most
people don't run Python or native code
very well in the JVM Microsoft has the
common language runtime up for a long
time but it doesn't really run languages
from Microsoft very well and and the
long tail of programming languages here
actually even the wedges the PI keep
getting smaller and keep getting more of
them right and so so java is like a
little over it's the only one that
really breaks 10% on the wheel this is
from the Tioga index so how do you deal
with this how do you manage this in your
business right so so our solution for
this is Graul and and the way to think
about this is rather than being a
runtime it's it's that runs your program
it's a met at runtime so it runs things
that runs programs right so what we do
is we provide this API we call truffle
and you build a language on truffle and
you build an interpreter and it's very
simple all it does is it tells
you know kind of it's what you learn
maybe in your sophomore computer science
class about how to define the semantics
of the language what happens when you
see a statement or an expression and
then how do you do branches and all that
sort of stuff you can write something
very straightforwardly and simple and
that that tells Grell what your your
language needs to do and what the
semantics are and then we actually will
learn from that and and watch what your
interpreter is doing to figure out how
to compile it and we have provided a
number of interpreters out of the box
you don't have to write your own
interpreter and we have ones for R and
Ruby and JavaScript and Python and PL
sequel and a number of others that don't
have space to show on the slide and then
you know what happens is as the
interpreter calls the API which is
written in Java we watch what the
interpreter is doing we learn the
semantics of the language and then you
compile it right so what this means is
that there's no inherent you know
expectation about what the language is
that the girl has and so we can kind of
treat all the languages together like
they're one big language and that means
that we have zero crossing costs when
you want to move from one line which the
other right so if you think about the
girl architecture then you know the
straightforward thing is to do is you
run your your application on the
libraries and the frameworks that are
popular for each one that we run OD on
rails etc each each of the the
interpreters has a whole ecosystem
around it which we validate but but the
hard part actually typically comes and
you see in most other situations when
you deal with native libraries and so
the native libraries are usually
included because the languages that are
the most productive that are constantly
being born every every year probably
have some kind of performance issues and
so they run some native code to maybe
you have access to GPUs or to use some
new instructions from Intel or to use
the operating system or just for simple
code performance and then you kind of
break some of the things that you wanted
from these higher-level languages you
wanted it to be dynamically compiled you
wanted it to be secure so that you can't
just write over memory in any other part
of your application that's say if you
want the the sandbox features of of Java
or you know Java Script or whatever
you're using
but our solution for the
is just another truffle interpreter so
what we've done is we have we call
internally as projects along but it's an
LVN bit code interpreter so any language
that that has an elevation which is
generally your static languages like C
C++ Fortran COBOL Russ Julia Swift
Objective C it has tons and tons there's
dozens of languages with an elevation
and it generates this intermediate bit
code kind of like Java bytecode and what
that does is it provides an interface
that we can interpret just like it was
source code we interpret that and we can
run all the native parts of the
libraries so the our native libraries
and the JavaScript native libraries that
the Ruby native libraries are all
running in that interpreter and you can
cross back and forth between each so
kind of the high level way to think
about this there are basically four
different ways we try to differentiate
growl so the first is we're multilingual
we've talked about that a bit
it's embeddable we'll talk more about
that a little later it's secure all of
the native code in particular is running
in the sandbox so you know if you're
downloading random libraries and they
include some native code from the
internet it's good to know that they
can't scribble over other parts of your
application and then of course
efficiency so then the next question we
get is well what's the status of all
these different languages and you know
so today we've done interpreters for as
I said Java JavaScript Ruby R and we're
starting on Python which you can see
will have available next year and PL
sequel we have also in kind of a
prototype form it's becoming kind of
alpha soon and then we're also starting
to look at doing the the microsoft
languages so you have a little prototype
project for that and then of course out
of the big wedge of the others
there's many languages supported mostly
by LVM but also of course Scala there
that we can pick up so you'll probably
be in a case where most of the language
is that you want to run on a server that
aren't legacy will be should probably be
able to run in Brawl in the next year or
so and the one one key thing that I
wanted to kind of emphasize a bit more
is that that because
you know we have these these language
these runtimes that are designed for one
language they have compatibility
problems not just performance problems
so you know if you want to run JRuby on
on the Java VM it tends to happen it
runs faster much faster than the regular
Ruby because the JVM has a good compiler
but what it doesn't do is run all the
libraries and particularly native
libraries in rubies so most people don't
use it because there's stuff that it
doesn't run
you know runs infinitely slower and then
the other problem that you see you have
a lot is that there's a performance
penalty when you cross the languages so
most people know this from calling
native code from J&amp;amp;I but calling native
code from let's say the JavaScript v8
engine or maybe these other runtimes
also there's a cost and then when you
have multiple VMs which is a typical way
people run multiple you lism you have to
separate heaps to size to a separate
engines to tune and manage and and and
these high language crossing boundaries
you know from maybe 10 X if you're just
doing a function call across language
boundaries to if your implementation has
to read data structures from a foreign
language that can be like hundreds of
times slower than Graul and if you're
actually running arrested it can be like
you know to call something another
process and you're serializing
deserialize might be a thousand times
slower so what we're gonna do now is
we're gonna do a little demonstration of
multilingualism so Thomas is going to
switch and show off a couple features so
one we're gonna show the languages
compiled working together in one runtime
and then we're going to talk a little
bit about boundary crossing costs ok so
Thomas take it away thanks Eric all
right so when you download chromium you
get a virtual machine that is capable of
running multiple languages and if you
look at if you downloaded and installed
chromium on my computer and if you go to
the binary directory what you will see
is it looks a little bit like a GDK
binary directory but it doesn't have it
has like the standard things of like
Java Java C and all of the Java tools in
there but if we'll find some binaries in
there that are not typically seen in a
JDK installation so I have here for
example and
a primary a note binary I have M p.m.
here I have a Ruby binary I have Ruby I
have a pison buyer in here and I also
have an R minor in here
so we basically not only allow you to
run the channel based off we allow you
to also run Ruby R or pison and I have
here a little example program that is a
typical small web server written in
notes yes and what I can do is I can
just use the node command from my
chromium installation and I will run
this first version of the server in this
type of situation I would just get the
standard request and will get hello well
from grouches and this turn i running
nodejs
ways to crawl compiler and okay so
that's not very exciting that's just a
standard noj's process but what you can
do is crawl now is you can combine no
chairs with other languages so no chairs
for example is the issue that well you
might want to use maybe a library for
big integer like Java is very good begin
to the libraries and in our situation we
allow you to directly reference Java
types from JavaScript so we can here
reference the big integer Java type we
then can make here calculations we can
just call Java methods as if they would
be JavaScript methods and I'm here
creating 10 to the power of 100 and I'm
adding another 42 to that and this shows
you kind of the the way we can mix here
JavaScript and Java if I run this now
what I need to do is I need to do a flag
- - JVM so I run now my node.js
application on the JVM with all of Java
available
in this scenario now I can go here and I
will oh sorry I was I need now the
second version of like that the file I
just showed you is a server to digest so
I can run this now and if I run it here
I will have my new output which is now
using JavaScript and Java to fulfill the
request so this type of scenario we
envision where you have maybe some
business logic that's written in Java
and but you want a front-end like a web
server micro-service front-end to that
business logic and that front-end could
be written in noches but it doesn't stop
there
you can also mix other languages in
addition to Java here and for this I
have prepared a small example with using
assert language because like in this
scenario I'm using the our language
which is a statistical language for
grading nice graphs and good statistics
and I can hear from my JavaScript
server-side application call out to the
our application I will do here some our
expression and even more so I can even
create a graphic where's our and I will
serve this graphic back to the web
server as an SVG I can just again like
start with this note comment but I will
give the - - g.b.m flag and in addition
to have other languages available I give
- - - polyglot flag I again I need to
start the web server here so when I'm
now going here and asking I didn't hear
the first request it will take a little
while because we are initializing the
whole our our interpreter at the same
time and we're loading the our package
is required to do the graphics but now I
can serve here the SVG that's produced
by our to the web server and up here I
have just an our expression
it's just coming from your coat their
subsequent subsequent requests here
while we have randomized the the graphic
subsequent requests will become a lot
faster here so in terms of development
tools it's only possible to mix those
languages but it's also possible to
debug those languages together so what I
can do is I can do - wants to be a -
once polyglot and - - inspect and if I
do this I will get here a link that I
can use in the Chrome browser for chrome
dev tools I will first do the first
request I will set off the first request
here just like that at the same time I
will let's just wait for the first
request to be served it was good yes and
but if I now could go to the chrome dev
tools
you see this cream
if I go here to the don't crumb death
cause I will get here to this file take
it that's not your normal development
environment no it's not that
let me just see if I can make it larger
right so is it readable approximately
yes so I can make a breakpoint here and
I can do here I am now I now I started
the epic and I'm now hitting the
breakpoint at the stage here and I can
do like step through I can do a step
into
I have a little private like started the
chrome dev tools twice now here and I
can I am now at a our function here so
it's an R if-else function and I can do
step into the next function call I'm now
at the our if-else function and what you
can see on the right is that on the call
stack I've you know in our function if
else but I can go also down the call
stack and he have my JavaScript
functions so have my JavaScript
functions here is my own like which is
no J's functions for handling requests
again I'm happy on my own function that
we just saw before and I've also D our
function on the stack and what I can do
is I can step through the our function
which is the our if-else function and
while I do this I can I can also watch
our expression product and watching our
expressions and I can go out of the
function again well I'm not in a
function I can go out of functions I can
go out of the function and I'm back in
my JavaScript function at this stage I'm
still into our function but I can
step out of current function step out of
contraction and I'm back in the
JavaScript function but it's basically
yeah I'm sorry about the confusion but
it's it is debugging R and JavaScript at
the same time in the chrome dev tools I
thankfully the chrome dev tools are
language agnostic so we can we can can
support both languages at the same time
right yeah so that was the particle
diamond rule I'd continue with some of
the slides before we go to the embedding
demo all right
so the next major thing we want to talk
about here oops wrong way is embed
ability so you know you can you can
growl VM was designed to that any
portion of the runtime the memory
management the thread scheduling the
code cache and so forth can be supplied
by the embedding engine rather than
having to be supplied by Grall and and
that's what was critical to allow
overall to run in something like the
Oracle database which has its own
resource management and and thread
scheduling mechanisms and it's code
management mechanisms so forth
so we'll demonstrate some of that
tomorrow in our session called
accelerating your database applications
with brawl both in the context of our
TMS and my sequel but the other
important thing to think about embed
ability from a relevant point of view is
that really you can embed it in Java as
well so you can run standalone in what
we call substrate vm but you could also
run it in Java and so in that case we're
using the Java mechanism store code
cache but it really kind of operates at
the same level as the database and in
the way the Grell works there's this
kind of a hybrid of static and dynamic
runtimes right so the substrate VM which
is it's just kind of Wafaa thin piece of
code you need to prop up a dynamic
compiler is that is the piece that when
you're running outside of hotspot that
we use to do some of the things like
garbage collection and code caching that
may not be fried by the runtime and then
what happens is substrate VM allows for
ahead of time compiled code so whereas
when you run crawl in hot spot hot spot
itself is native code and precompiled
but then most of java itself and then of
course all the growl up locations node
and ruby and r and and so forth or the
the native code even it's all running
dynamically compiled with the compiler
running in the context of hotspot when
you are in the substrate vm the other
thing that's kind of interesting is that
you have the choice of some of your code
can be ahead of time compiled and some
of the code can be dynamically executed
and any of your code can kind of move
back and forth across this boundary and
so this two layer design is really
important right so the host line
especially for embedded application so
the host language right which i think
right now mostly we're supporting just
java and native code in the host
language although I think we've done
some work for any JVM based language we
can do at the host level right now and
so you know we need to close world
assumptions we need to know all the code
you're going to run but you have like
100x faster startup like footprint is
like probably two twice twice as small
in general but the nice thing is that
native code is still managed right so
it's garbage collected it's Browns
checked it's secured against dock
overflows this is but you know where we
expect you want to run trusted code it
has access to the file system and all
the OS privileges but what we do then is
provide a whitelist so only certain
parts of the host language code can then
be accessible to the guest languages
right and and that's what's available
for your reflection so the guest
languages that are dynamically compiled
with an open world assumption
you know we runtime profile them to to
figure out the shape of the actual data
to maximize performance one of the
things that allows us to actually run
crawl to make T PCH query one run faster
for example than running a native code
is we dynamically find out what the data
shape of the columns in a database look
like and and compile and compress the
code at runtime to fit just the the data
we see in the particular application and
so you get that benefit benefits of a
dynamically compiled system and that
it's it's flexible the dynamic languages
are often more productive we provide
some facilities for memory boxing and
time boxing the guest language code so
when you're downloading random
of whatnot from the internet you'll feel
a little bit better about running it
there and and you can also say that only
certain of you know parts of let's say
the the trusted code base is available
to the dynamic languages and then the
other thing that's really important here
is that the native language parts of the
guest languages and the guest libraries
more importantly can be run safely with
with zero crossing overhead for those
extensions and so that's what we're
gonna demonstrate next Thomas is going
to show both and you can much more
aggressive ahead of time compilation
strategy then I think maybe is seen as a
JVM before and also more complete ahead
of time compiling strategy so while he
gets ready for that
I think that what I mean aggressive what
you'll see is just the difference in
startup time and in you know it's it's
kind of an order of magnitude difference
as opposed to this kind of maybe a 20%
50% difference you'll see with some
maybe other ahead of time compiling
strategies for languages like Java or
all right good so the example I'm
showing you here is it small it's a
small Java program it using some
streaming API to implement lists
directory functionality that's recursive
so it's walking the path and it's just
outputting the names of defaults and
their sizes if I'm here just executing
this file on a normal TV m and I see
here this dear how I get this type of
output but what I want to show you is in
particular if you time this then it will
take about well 300 real milliseconds
and 450 milliseconds for actual CPU time
to run this little LS command on my
current directory so that is a lot of
time and overhead and this is one of the
reasons why there's not many small
command-line tools currently written in
Java so what you can do with our
Eero t solution is you can run a native
image comment on a java application and
what this does is it will create close
to all the analysis of that Java program
so it starts from the main of the Java
program and does it close to all the
analysis of it everything that's
reachable it's kind of checks the types
in the flowing through system and then
it is creating a pre-compiled image of
exactly the type of code that is
reachable so what it created is this new
executable which is called list dear
here if you look at the size of this
executable it is only seven point six
megabytes and this executable is now
standalone and can run the same Java
program that you saw before what I can
do is I can just run this executable
here and you can always feel kind of
feel from seeing it it's a lot faster if
a time this it will run in here in like
12 milliseconds and in particular the
time the use of time the CPU time is
even even better like just three
milliseconds of user time spent in user
time if a computer is always just
running the original Java it is here
about a while about a 30x faster in real
time and about 100x a 100x faster in
user time CPU time it's not only that I
have your tool that's called I use new
time now to not only check the time but
also check the
the size the max resident size it's a
little bit hard to read here on this on
this setup but the max resident size of
the max Arthur size memory size occupied
by the process when running here in Java
it is about 200 megabytes and if we run
the same tool on the new version then
the total amount of memory occupied by
the process that anyone in time is only
70 megabyte so this is about a 15 X
reduction in our SS size memory
consumption by the process and but this
new process like one of the limitations
is that for this closed world image
building all of the code needs to be
available at image processing time so
you cannot anomic a load code later
because the code the final executable
that does not contain a runtime anymore
it contains a garbage collection so it
is garbage collected but it does not
contain a compiler or any dynamic class
loading but what you can do is you can
actually have an extension point of your
application still built into that
executable the way to do this is in this
example so here I'm using the same I'm
doing the same type of program but I
have extended it with JavaScript dynamic
evaluation so you can now for this list
directory you can give a JavaScript
function to the process that the process
will execute so I can create here a new
JavaScript context and I can create a
new JavaScript lambda function that I'm
getting from the arguments and I can
execute that so I can still create the
dynamic extension point with the truffle
languages what I can basically do as
shown on the previous slide is that I
have a pre compiled version like
pre-compiled parts which are the tower
parts and then I have the extension
points and this extent
next list function here this extent next
list program this is the default here
but in this program I can now say well I
want the size to be displayed before the
name and and then I can run any type of
JavaScript expression here I'm just
customizing the application now with
JavaScript and this JavaScript actually
runs very fast and in terms of the
timing here again if a time this I'm
down to like well real fancy for 43
milliseconds 42 milliseconds including
the parsing and execution of the
JavaScript if I would run here the
original Java version with which the
extension and I'm giving in the same
expression size plus name then it would
take me here 780 milliseconds this is
now without the boot image generation
process so they can this difference and
in terms of user time the difference is
even higher like 1.9 seconds of CPU time
versus just 11 milliseconds of CPU time
so this is just on a small example here
to show that you can like get a huge
benefit from this IOT compilation and
that you can do this here for common
line tools but one of the things you
might be interested in to use this for
is in the context of the cloud
deployments and in the context of cloud
deployments the questions they are like
can I have a faster start up and for my
lambda functions or for my micro
services and can I run more of those
micro services in the same process and
for this I have your little small script
that is trying to start this processes
up and it's waiting until the process is
actually finished starting off and can
serve the first HTTP request I have this
running here on my local machine on a
docker container then I can do talk
around here and you can see here that we
are very fast in starting up this new
each of these three milliseconds is
three millisecond between 3 &amp;amp; 6
milliseconds is starting up I completely
new process and its timing until that
process conserved the first HTTP request
and if we do this here with the hotspot
version so I can here instead of using
the pre compiled SVM based version of
those micro service of this micro
service I can run here the hotspot
version of this micro service and if I
start that then I'm here on taking about
20 milliseconds for the first few
startup of lambda functions and then it
gets even worse because the TVM
processes start to compete on on memory
and on CPU for the dynamic compilation
so this is the same program the exact
same Java program running here with the
traditional VM configuration versus
running here with the pre compiled
version and we hope that in this
scenario we can fit we can fit more
containers in Bourne server and we can
have also two containers be shot down
more frequently because if the startup
is just three milliseconds then you
don't need to have any stateless
currently not idle lambda function lying
around
no you did it with 20 oh yes you can do
this 100 you can you can do as many as
you want 100 for the Java one yes I'm on
four hundred for the Jovan well that's
not going to all right well while you
while we're waiting for this I'll sit
down here and okay this is gonna be a
while to set up that's a lot of Java to
start up - okay well let's drill see out
of that here
yes so the question is what limitations
are on this IOT process to work so at
the moment we are likely gotten a lot
better since 0.22 just like six six
months ago we announced 0:28
we are step-by-step removing limitations
if you have an application actually talk
to us we might be able to make it work
in the end we can make every single work
except for dynamic class loading so our
the point where we said it will not make
that work is tsunami class loading we
will still support some type of dynamic
extension with Java but that will be
wire like Java running on truffle which
will be some limited version of Java we
can read that we can run in the similar
way like I'm extending the application
with JavaScript
I could also like dynamically load a
Java there but it's not like technically
dynamic class loading we are supporting
parts of reflection not all of it not
all of it but you're starting to support
it but you for reflection for example
you need to do some declaration on what
type of reflection you want to support
because for a close broad analysis it's
a little bit of a problem if if every
function in the whole world becomes an
entry point so you would have to say
like well these are entry points for
reflection in this class or in this
package but we're removing the
limitations number one if you're having
a problem I'm happy to support you
another just on that point you know what
you do then is Java C for example does
some dynamic class loading for
annotation processes and whatever and so
we can ahead of time compile Java C just
by supplying when you build the java c
image these are the annotation
processors that we're gonna have okay so
then we can have the closed world
assumption and once you have the closed
world assumption and there there's be no
problems that we can't we can't fix
another question
any jvm language Scotland Scala well we
tested the problem in Scala yeah
not terribly because cherubi is
dynamically generating classes right we
are supporting calling Scala
we are also supporting groovy as long as
you like Ruby has a mode that avoids
dynamic loading
but for JRuby I mean you have to offer
Ruby which is the like 10x faster switch
back to the slides
all right so so in terms of performance
we talked about efficiency in general
across a wide range of benchmarks we are
we're basically on par for most
languages can definitely run Scala
significantly faster there's a talk
earlier it open world from the folks at
Twitter who are actually running in in
production because it's about 10% faster
even with our open source version and
the the closed source Enterprise version
is another like I guess 11 or 12 percent
faster than that and we continue to work
on this the fastest javascript is
actually Safari and so we're actually on
par with v8 and I think safaris about
10% faster than v8 and and and grow all
at this point and then as far as
application level performance in
addition to the Twitter thing I
mentioned we also have done a little
benchmark with web logic and we can run
hello world about 10% faster which is
actually pretty good given that most of
hello world is native socket set up and
pull down and so forth so the thing to
remember about the growled ecosystem is
that because it is multilingual and
embeddable there's kind of a
multiplicative add-on right so you can
wear any language and tons of different
embeddings by the way we've tested all
these embeddings here there's other ones
that we think are very possible we're
going to start looking into some more
database engines for sure but the nice
thing is you know your code can sit on
top of that right and get that benefit
so if I write a library I can write a
library in any of our languages and have
it available from any other language
right and and then I can run that
library in the Oracle database I can run
it in my sequel and I can run it in in
any of these other places we're going to
edit and so one of our goals with
especially the the open source version
of Gras
is to make sure that pretty much every
open source data processing engine can
embed growl as extension mechanism to go
run code there and so if you are working
on one of those engines we certainly
encourage you to check that out and
contact us and we'll be happy to work
with you on that so and then like as we
mentioned we do have a lot of open
source
interaction with the rest of the
community we've had significant
contributions from I think Red Hat did
an arm back in for us and Intel did a
lot of optimizations for some of the
later processors that have been
contributed and so let me check us out
on github we have an o TN download with
all the features and in all the
languages that are there and actually we
just pushed today the the growl vm
running in new york all database we call
it Oracle database multilingual engine
that's girl and so if you go to Ooty n
you can download that and you can learn
more about that and see demonstrations
of that tomorrow at 5:30 in the database
and then for people that are looking for
some of the more developer oriented
features of writing multilingual
applications as kind of a native part of
your development strategy thomas has a
talk tomorrow morning here in this
building on some of the developer
focused aspects of rawl and so with that
I'll open it up to some more questions
so growl is as we said embeddable on
hotspot right so it's a separate to
suffer compiler and the a the ahead of
time stuff is something that we're
working on using to replace like the
client compiler as well as the server
compiler because the right now if you
use brawl in in hotspot we have to use
hotspot to first compile growl and so
they start up a little bit worse running
in hotspot then with the current
compiler so but in JDK 9 they have a
public API called the java virtual
machine compiler interface so you can
just drop in growl jar and and bundle it
with hotspot and it's going to work
it'll have really good peak performance
a little bit worse startup and for jdk 9
and then you know we're going to
continue to find more integration points
with with hotspot going forward and I
think that they're gonna start
incrementally moving to you know we're
using draw more and more rather than the
existing compilers which are which are
pretty old and aging and so yeah that
but that's kind of in in process and so
we look at hotspot is one place we want
to run it but the Oracle database is
another place we want to run it I mean
those are obviously are kind of
premiere products that we're going to be
pushing this technology in but we
actually are also shipping growl with
one of our vertical applications called
Oracle policy automation so they use
that to run JavaScript as an extension
mechanism for configuring policies that
are being evaluated I think it's kind of
for the the government space that that
thing is running so it that's that's
another direction we expect to go in is
starting to see growl embedded more and
it kind of higher up the stack sort of
things so substrate VM yeah it's a
separate VM entirely and so it has its
separate you know you can run substrate
VM in hotspot if you want to run but I
say ahead of time compiled code there
and you know but it basically it
substrate VM it kind of has some of the
features of hotspot that I've you know
that are extractable so you know we
don't want to if you run Garlin hotspot
hotspot is providing the code cache not
providing garbage collection for you and
all those features when you run girl
outside of a hotspot you know we need it
most the other engines don't have
garbage collection or a code cache and
so that's what substrate vm provides
it's kind of that wall for thin layer to
keep the services up that the dynamic
compiler needs
right so what you have to do is when
your whatever your build or make files
or whatever strategy looks like you need
to output the LLVM bit code right so
that it's kind of like the dot o file
sometimes you don't actually pay
attention to them but they're generated
by your compile process so you have to
like change your make file to dump out
the the dot BC files which is the bit
code files and then we interpret them
just like we interpret JavaScript source
code so so if you all you have is an
executive all that's not something we
can run today so you do need to have
like at least a bit code around you
don't need to ship us you know the C
native source code but we do need a bit
code at least that's where we are today
we're starting to work on in the next 86
like in assembly an interpreter but
that's not quite there yes that may be
or two
that's you know like higher powers and I
are trying to figure out like you know
so in general it's an enterprise just is
basically some more aggressive
performance optimization so the only
thing he really knows differently in
growl interpret there's two basic things
in growl enterprise right now one is
some extra performance optimizations and
which right now about maybe a twenty
percent improvement but the most
important thing that's in the growl
Enterprise version is like the what we
call safe Seulong that runs the native
code safely in the sandbox so there's
bunch of stuff to do that and so those
those are the two main differentiators
we have for the license products that
you get when you run it in a database or
you run it in a license product rather
than just pulling it from github if
someone wants to replicate our
performance improvements and basic growl
you know like they were gonna do more
work to face a girl that's great we'll
have you know our our people that are
doing performance optimizations then you
know we'll continued like we expect the
open-source community to keep pushing
you know our our proprietary extensions
right and so you know we've got sixty
five people that are working on on the
growl stack right now or something on
that order and so you know we're not
afraid you know please contribute to the
open-source version and and and compete
with our enterprise version that just
means that you're up taking brawl and
you're part of the growl ecosystem and
there's there's gonna be other stuff
that we think that you know we can help
work those business out and you know why
maybe right now like the different you
know the performance differentiation is
is kind of a bigger part of the
enterprise story going forward we'll
start to see more manageability and
security staying things that are gonna
go and they the proprietary version and
I think that performance will be less of
an issue but that's that's kind of for
product management yet to decide exactly
what's going to happen with that but
that's that's kind of what I know now
if you if you're an Oracle database
customer you're automatically like
you're automatically fine right there
are other plate if you're like Oracle
policy automation customer you're fine
right so basically for the most part if
that product has I talked to us and
embedded Gras like you're fine and if
not if you talk to us you know basically
basically the way that Labs works with
an Oracle is we kind of think of us as
you know technology supplier to all the
product divisions and we want as many of
the product divisions to just take up
our stuff you know we don't have a
separate license fee for all and so as
long as like there's a you know some
sort of commercially licensed product
that seems reasonable we want them to
take crawl and the labs doesn't have it
so we are in a business unit that we
have revenue that we are measured by
anymore Oh Chris
so in coral 0.29 it will be in four
weeks out it will compile with profit
leader the regression was on the peak
performance of the ahead of time
compiled code at the moment its worst
and on hotspot because you don't use
profiling data but we will use profiling
data very soon like we have to prototype
of reticle ship it in 0.29 yes you run
your application once with a flag and
then we gather the profile and then we
compile and optimize based on that
profile and that makes a very good peak
performance the other advantage actually
is compared to hotspot let's says in
this ahead of time compiled code we are
not the optimizing so your performance
is actually more predictable which means
if you want to fix your current
application performance to a specific
profile and not have it change because
of some exceptional case that's at all
one more question
other binary size finely slice is much
smaller I mean so the binary size is
smaller than if you would like the
docker image you can create a docker
image from this that has one of this EOT
compiled code in it and it will be a lot
smaller than the smallest docker image
you can get with JDK
because the whole point is no no you
don't you don't need to chitti k you
don't you should just your binary your
binaries stand alone right you just
should know
oh so we were saying he says the JDK is
shared how much you know application
unique code per application would be
there right because you know if the JDK
is kind of pre-installed by the OSS and
shared across all the tenants well I
mean you can share like if you if you do
this ahead of time compilation you can
have like the code if you like well
obviously but yeah we are sharing it's a
similar sharing effect right
the Rishi of size between a felony OT
compilation I would cbot compilations a
little larger but not substantially I
would maybe say 2x but you don't need to
ship to JDK so if you subtract the GDK
then like usually you should get the
smaller docking image give like a docker
image with seven megabytes total that
runs HTTP server I mean I guess going
forward in the future we could look to
you know you generating rather than one
one executive all like the separate like
shared libraries for you know like let's
say the the parts that came from the JDK
but like the images are so much smaller
and mostly people seem to want to you
know bundle more things in their
containers so we haven't actually then
is an issue I guess we're getting the
motion that we should like right about
this but I know free to come up and talk
to us afterwards even more questions yes
thank you thank you thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>