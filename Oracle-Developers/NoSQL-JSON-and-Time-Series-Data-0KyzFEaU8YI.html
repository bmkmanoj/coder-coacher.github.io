<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NoSQL, JSON, and Time Series Data | Coder Coacher - Coaching Coders</title><meta content="NoSQL, JSON, and Time Series Data - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NoSQL, JSON, and Time Series Data</b></h2><h5 class="post__date">2015-06-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/0KyzFEaU8YI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">well first of all thanks everyone for
coming by this afternoon I'll just give
a quick introduction about myself I'm a
newt Shani product manager of Oracle no
sequel database and Berkeley database J
and I have my colleague who will be
talking with me Ashok Ulla and he is
senior sales consultant managing Big
Data technologies at Oracle
so before I start I'll just go through
very quick our safe harbor statement
mostly I don't have any future specific
roadmaps to share but if I speak of
anything which is coming in the future
consider that that as a figment of my
imagination you just you cannot hold
Oracle responsible for anything I say or
tell you during this presentation so
today's agenda I will be setting up the
stage by just going through a quick
history about where we were 10 years
back and what's happening in terms of
data format exchange in in the JSON
world and then how it is applicable to
Big Data and no sequel technology then I
will just give you a quick overview of
how JSON and big-dick Big Data
technologies are making its way into our
to know sequel database as well just
quick architectural background and we'll
spend some time on some JSON
representation in Oracle no sequel
database and all the good stuff that you
can expect from JSON so I'll speak very
briefly on that then as a case study
we'll spend some time on a financial
sector demo that we build using oracle
SQL database and Ashok will be spending
some time about how you can do something
similar on the time series data that you
capture on the Israeli
fast database systems so that's pretty
much the agenda for today
so starting right away if you all know
already that ten years back XML was or
still is very much applicable in data
exchange format right it is a great
improvement on where we started with the
standard general markup language
generalized markup language which was
mostly a free flow structure but it was
designed for the distributed systems to
talk to each other right so you can send
some information from one hardware from
one machine and you can pass it on to
the other hardware or a software but XML
improved a lot of things it has enforced
some constraints added some schema
definitions and and that actually drove
the internet adaptogen or at the moment
I should say adopted by HTTP language
and we know history I mean nothing that
you folks don't know but now we are
moving into a new I would say revolution
which is driven by JSON some would say
that it's better than XML mean I would
just say that it's not about better XML
still has its own contribution and it
will continue to have its own space but
JSON is starting to drive this internet
exchange things that are happening right
because a it is supposed to be very
brief I would say less verbose and
requires less computation power to
either parse or or work with those
format or messages and JSON is
considered as a language independent as
XML as well but the point I'm trying to
make here is that just because it is
less verb
less intensive on the competition and
easy to transfer these messages from
system a to system B it is becoming the
de facto nowadays and what's driving is
actually few few few bullet points
either JSON is driving this or this is
driving JSON but both are fueling each
other and I'll just want to quickly set
up a quick stage before we move on into
the Big Data space that the first thing
which actually making JSON really cool
is the API the Web API is and what I
mean by that is there are many new
companies out there which actually works
with the data and they quickly realize
that just keeping the data to the
enterprise itself is not going to add
any value so why not we share this
information to the outside wall right so
the ecosystem is built and that's what
exactly Twitter has done they created
web services so people can access the
same data that sits in their enterprise
system through these simple web services
or the API is and they came up in I
think in the version 1 with the XML
representation of the data as well as
JSON representation but it quickly
become evident for for these folks that
JSON is going to be the standard moving
forward version 2 is all JSON and the
other point is I would like to make is I
mean JSON is becoming the definition for
all these protocols like in rest you use
JSON as a protocol for conversation and
XML being the standard in the soap in
the soap which is web services but I
think JSON is taking its popularity
because of these kind of enterprises
going into the market and monetizing
their data by exposing it to the outside
world the second bullet point that is
effect
or making JSON very popular is the
Internet of Things
now Internet of Things is a buzzing word
as big data at the moment and in a
nutshell it's about your your simple
devices that we all use right they are
communicating or telling to a
centralized system how things are mean
if you speak of a refrigerator then what
you have stocked in your refrigerator
what's getting down in terms of
inventory and all that so these are the
things communicating through the
internet and it is kind of presumed that
JSON is going to be the data is going to
be the format of data exchange for all
these Internet of Things a because it
requires less computational resources
and and these devices come come with
very limited computational resources to
begin with right so the third part is
JavaScript I mean we all work with
JavaScript
it used to be some set of our internet
web page development but now JavaScript
is coming back and becoming mainstream
player in in the internet I would say
world and that is also fueling a lot to
the popularity of JSON that the the next
part is going to be the major topic of
our discussion and that is the big data
so the big data uses I mean there are
many different players many different
technologies it's a bag of technologies
right you name it analytics acquisition
and processing there are many many
different kind of technologies but there
is a great ecosystem of representing
your data in a JSON format and consuming
it through these technologies whether is
Hadoop or it's any no sequel technology
so moving into now the other topic of
the presentation that what is Big Data
and we'll just try to make a connection
between what's happening around us and
how JSON
is fueling and how these technologies
are fueling the JSON right so there is a
there's an interconnect between
everything here so if we look around
today everything is data fide right so
our thoughts our opinions our
relationships they are all expressed as
data right whether it's social network
is you name it we just express ourselves
through all these portals if you will
then there is a data fication for the
things as well the smart sensors that we
have the the geo positioning systems we
have in our car the traffic lights that
we see I mean all of these are producing
more and more data and I actually read
in in one of the article which says that
in 30 minutes
engine if light engine can generate 10
terabytes of of data that you now need
to analyze it because there is so much
sensors working together getting data
out of many different components of you
of the engine and then you need to work
with that fast velocity data coming at
to you right so that's the amount of
data we are talking about and then the
same way the process is also getting
data file right mean now I don't know
how many of you folks goes to route back
which used to be like a weekly chore for
each of us right now everything we just
right from the scanning of a check we
just send just a photocopy to the bag
and they just either do a debit or
credit whatever right and even if you go
for a walk in the morning right we want
to capture how much calories we burned
how long I went how many hops I made and
all that and then at the end we just mix
and match all these technologies or
processes into expressing ourselves
right so that in itself leads to doing
something interesting from what we just
gathered and I think that's what a big
data to me right so we are catching
all this interesting data which we have
not gathered or didn't kind of assumed
it to be important to begin with but now
we have the technology to capture all
these technologies in a in a
cost-effective way and and then do
something interesting with that so after
understanding what this big data space
is then many time people ask that Oracle
has relational database is it going to
be replaced by other Big Data
technologies or Big Data kind of tools
and products we naturally say that there
is still a need the the previous slide I
shared with you it had many different
patterns the new patterns that we are
seeing right so we are doing something
interesting something new with this data
but we still have the traditional
problem at hand and relational database
still do what it does better right it
has a sequel optimizer on top it has
many different standardization layer
built over 30 years so it continued to
harness whatever it does best right I
mean all our CRM inventory systems data
warehousing will continue to use because
of a reason because we need to slice
dice do multi-dimensional analysis on on
this complicated data but what we are
talking about now in addition to this is
the data that is coming either from
these machines as I mentioned sensors
processes and all those our our
expressions through all these social
medium so it has kind of a pattern it
has a requirement to first capture this
fast coming data and then process it and
do something with that data right and
over the time we would see more and more
technology companies coming in into this
big data space but my point is that from
the technology point of view you expect
that you get 24/7 support 24/7
availability from any
you used to a capture and then be
processed this data that you capture
before and most of the time you tend to
expect that the development process that
you are following is pre a Giles so you
would expect that your technology is
also helping the whole process I mean
it's not rigid as your data schema
changes you would expect that your
database schemas either evolved with
that or lets you store information
without any stringent schemas defined to
it so using all or or fueled by these
kind of new patterns of data capturing
and and utilization to measure
technologies as I mentioned before
Hadoop which is mostly for distributed
processing are actually a byproduct of
of these kind of applications and then
the second I would say the sector that
actually came into the into being is no
sequel database technology and what it
gives you is flexible schema definitions
it had by design high availability built
into the product right it's not like
here on a deployer system and then I
have to think about how I gonna put my
system on a data center B so I can get
like resilience from any data center
failure as well all these technologies
are built to solve these problems right
so they are solving it because they are
so simple to begin with they deal with
simple data and what they let you do is
store your schema less or our changing
information or data into it into into
its database store and as your
requirements changes you just add
columns without making any changes to
the database world or to the database
layer so there are many different no
sequel products out there many are
differentiated by how they are storing
information
you have key-value database you have
columnar database document and the graph
right but still there is no
standardization even at the moment but
what I can say for sure that all these
no sequel databases has one thing in
common is that they are storing
information in a denormalized fashion so
this is very important and and very
first kind of a bifurcation from the
relational world I mean it's not like
you normalize your data and then you run
your standardised sequel query you would
like to store whatever information that
is coming in just store it as a JSON
object and just save it in your database
system and then you need to access it
just pass in some quick keys and and
make a quick query based on the key that
he used while persisting your
information and you get the whole object
and then you can do whatever you want to
do with that object right JSON object or
whatever or a binary JSON and generally
the access is pretty simple as I already
mentioned the access pattern is you
store something you have a primary index
to it and you want to run your queries
using your primary or a secondary basis
it's not mostly going to be okay I have
an entity a and I want to join with an
entity B and then I will do a kind of a
predict predicate like a ver plaus and
then I'll filter it out it's not that
mean it's just very simple access and
that's how all the no sequel technology
or mostly no sequel technology behaves
now I'm just just an example here that
JSON object storing entity in this case
a person right mean if you have to store
a person and it's relationships of
relatives I mean think you will be
defining those kind of joints through an
intersection table in a relational
database but here you have a person you
can give this person as an ID as well
and that could become the key and you
can just store this just
object into any no sequel database so
that's the general kind of a patent on
how most of the new sequel technology
works speaking of the use cases where no
sequel makes sense over your relational
technologies a relational database
technologies I mean these are the three
buckets I would say the very first use
case that we are seeing is web-scale
transactional processing and this is
where you have fast data coming in of
very high volume as well and you want to
persist it whether it's from smart
sensors whether it's a call data record
I mean there is a simple schema there is
a definition to the data coming in but
your objective is to process at first of
all persisted very very fast and then
when you want to downstream it when you
want to work some on your data then you
would like to have low latency the
second is the web scale personalization
so this is where your social network
where you you persist a message and then
many of your friends they access your
message or your the picture that you
uploaded right so there is a
personalization aspect to the data that
you're storing and then the third bullet
is the real time even processing and
this is where let's say a smart sensor
is sending let's say health device is
sending some of your health statistics
so the first thing is you need to
capture the incoming events and if there
is let's say high glucose reading at
some point of time then you want to do
some action based on on the reading
right so you would first fetch okay
this device ID belonged to which
customer and then what was the historic
reading of this this customer so all
those things kind of falls into the
real-time even processing so I have two
use cases again just to give you some
more idea on
how no sequel and the JSON is driving
this big data space working together so
this is one use case which we are seeing
very much prevalent in the space that is
the online social gaming and everyone
knows that now if you play any game on a
web right so you have other players as
well in a virtual space and then you are
interacting with your players and you do
something so the problem statement is
you need to access all this data that is
coming at you at a very low latency it's
a very high velocity data right so
youyou don't want that if you move
yourself in a virtual IDE virtualized
world the action happens may be delayed
by a few seconds right you want it to
happen in a in a near real-time and of
course from the business point of view
you would like your servers who are kind
of helping out to host your applications
are 24/7 available and can manage any
kind of load right so no sequel provides
exactly that so I think my next slide
would would give you more idea so this
is a virtualized world where players are
interacting with each other and they are
also chatting I mean there is an
interaction they are moving into a
virtualized space so you are kind of
discovering people around you and you
are also interact interacting through
through the chats right so everyone is
saying how how things are going and
there is an interaction as well right so
what you would expect from the
technology point of view is a you would
like a technology at the backend which
can persist all the information that is
coming in very very fast and you would
like to start with the with the least
amount of expenditure right from the
technology point of view or from the dev
development point of view but you would
like to scale as your application
becomes popular right so
no sequel provides you exactly that so
it has Charlotte system so you can also
do availability by having multiple
replicas and they are actually as a set
by design you can also grow your cluster
by adding more capacity more storage
node to the system and mostly they
support schema-less or schema evolving
kind of a data format and from the
applicator from the application point of
view there is a great synergy between
html5 and JSON what it mean is that
whatever you see as a session you can
just persist as is into the no sequel
database and then when you want to
render it back then bring it back that
same JSON and HTML will just render it
because there is the same data format
going in the html5 as well again from
application all these interaction that
is happening you would like it to be
stored very very fast and as is without
much of the data transformation right
you don't want that you transform it
into some other world format and then
store it and then while deserializing it
you you do the reverse the second use
case is the smart meters analytics
something very similar but I would say
whereas the the social gaming was an
example for web scale personalization
because you are interacting with with
folks around you this is more of an
example of web scale transactions and
and here the problem statement is this
that we all have seen that we have power
meters but in third-world countries or I
mean I think everywhere you have a
problem where a you you have no
visibility of if someone is stealing the
power right so from all these utility
companies they would like to know in
real time that what's happening how
their customers are using and if someone
is about to steal or
stealing they can see a spike in power
utilization right so they can analyze it
that has something extraordinary
happened it should I be doing something
to safeguard or rectify the thing and of
course as I mentioned from my first
slide data fication right so many
different sensors many customers sending
all these feeds at a high velocity high
volume so you want to first persist it
somewhere so you can do analysis later
on right and no sequel technology is
kind of is the backbone to address these
kind of scenarios right a yes I think
the picture would be a would be a better
way to explain it so you have many
different kind of sensors and these
sensors they don't just speak in in a
very static format right I mean you
might have one vendor which might have
an additional fee lame and and you would
like your database layer to kind of
support that kind of evolution right it
should not that it slows you down just
because you have a different column so
the other part is that many different
sensors you just do a ETL extract
transform and load into some form of
format some form of semantics and then
you store it into the no sequel database
again from the no sequel point of view
you get horizontal scalability meaning
if you happen to have more and more
customers using smart meters you can
on-demand
expand your no sequel storage layer and
it will support it just scales
almost linearly I have some data to
share with you as well some of the
benchmark tests that we did with our
technology to support what I'm saying
and of course very easy generally this
is the use case where you have data
coming at to you in in some form of
time-stamped manner right and what you
would like to do is how the
being yesterday day before yesterday in
a week last month or last year right so
these are the things that you would like
to do with your data at the end of a day
and no sequel is pretty much equipped to
handle this simple data because you
don't need to have many complicated
relationships with other entities it's a
simple device and that device might be
owned by a customer so there is a simple
Association and all you need is to just
quickly gather or timestamp information
into it okay so moving in into how it is
applicable into what we are doing at
Oracle so the product that I'm going to
talk about is of course Oracle no sequel
database as you would expect it is
designed to be scalable it is a key
value database to begin with and all the
smartness is built into the driver
itself so it takes care of load
balancing you just add more and more
storage sister storage layer or nodes to
your cluster and driver takes care of it
where it's going to write and when it
comes time to read it it knows where to
read the information as well very simple
to install and manage the other features
that I would just quickly mention is it
has a support for flexible flexible
schema so you can just change whenever
you you need to make a change to support
your application layer and it gives you
an ability by using their by using it's
key to do some ordered scan queries or
unordered scan queries so I'll talk
about it very briefly that how it is
done and it has some native support for
large objects as well yes you can store
any size object as long as you can see
lies it into the no sequel database but
this blob support or a low of support
gives you an efficient way of storing
and
working with your objects and the driver
comes with either as a Java library or
are you you can use C library as well
now how the key looks like in Oracle no
sequel database is that what we have
done is we have bifurcated our key into
two different components major key and a
minor key and major key is used when you
are sharding your your or you are
deciding on where to write your object
right or your record so we use the the
full major key component which happens
to be just a collection of off strengths
so end of a day it is just a string but
you can have multiple strings in in the
both the components as in a major or a
minor but as I mentioned that we hash
the major key and then based on that
output we know which shot - right - and
then the minor key is used as a
localization format right so you might
have many different minor keys for the
same major key and all those different
attributes or if in in a relational
world you wanna do some analogy so
consider that as a column to the same
row right so minor key can be considered
as different columns all those different
records for the same major key written
down into the same shot and why because
if everything is localized on the same
shard you get better performance and we
can offer you acid transactional
compliance as well when you are dealing
with multiple records for the same major
key so in this example what I am trying
to show here is that let's say if you
want to build a data structure for your
car crash test and end of a day if you
know that you have to plot or work with
the timestamps then how would you do the
data modelling right so here I'm just
using a simple prefix for a key
isolation car car test or a crash test
as a prefix then this is the sensor
location left front there could be a
right front
rear left and right and all that so just
to tell me that where this sensor was
located then the idea of that sensor and
then I'm measuring some pressures from
those sensors so that's what I want to
capture pressure based on the timestream
right so I want to know how in my crash
test one what were the pressures on on
all the sides of the of the car and kind
of queries that you can do using this
key structure is you can either pass
let's say if you want to get all the
records all the readings for the left
side you might have many different
sensors not just one sensor by this ID
but you can get all the readings just by
saying give me all the records which has
same key as C T and L F right so that's
one one way of getting the values out of
it or you can decide that oh no I just
need to know what are all the readings
for sensor ID a or foo right or you can
decide that I'm gonna just get pressure
output for this sensor ID so see what
I'm doing is I'm just concatenating the
the other the rest part of this key and
and getting more granular information
about the readings I'm storing right so
that's that's how you build and you can
make some simple queries using the key
that we provide from the value aspect
you can store your value record like in
this case if it's pressure reading and
with that pressure reading I'm going to
store something else as well you can
define that as a JSON which we have
released I think in our r2 version to
release support for JSON and that
support is being used or provided by
using the Avro so I'll speak about a
very quick you can also store your value
part as a triplets RDF triplets if you
just want to do some graph analytics and
you want to store entity relationship to
end
right predicate and then subject
predicate and objects so that is done
using RDF triplets and you get sparkle
query on on top you can do much very
much graph kind of analytics using your
this kind of kind of relationship store
in no sequel database and we are working
on some another layer of semantics just
as you expect from relational database
so you can just define tables and
everything will be created for you
automatically so that's something which
is coming up in near future ok so just
quick words on the arrow why we decided
to go with the arrow as I said ever Oh a
is has a great synergy with the Hadoop
system and it provides you a very
efficient way of civilizing your JSON
objects and writing into the database
and then while reading it DS eliza's it
for you
the benefit is you do not put all this
logic in your application layer right so
out of the box we have binding api's
just use that and it lets you see lies
DC lies the objects and it is more
efficient it is efficient because the
field names in in any JSON objects that
say if you are recording billions of
these kind of records coming from a
sensor right all those billion records
will have the same feel name over and
over again isn't it good that you just
take or replace that field name and
replace it with a very efficient binary
coding and that's what ever does for you
right so it just reduces your disk
footprints as well and it supports
schema evolution so these are the just
some quick slides on what a schema
evolution means so in this case let's
say you are creating a simple customer
profile and you want to gather your
customers first last name date of birth
and year of experience right mean just
some abstract field names from no sequel
Oracle
no point of view you will just define
this schema as as a JSON and you will
use this DDL command to register it and
then once it is registered then start
ingesting data into it I'm just showing
you a tool which we call it data shell
this is more like just so you can get
get a hands on on how things look but
eventually you will be using api's or
the binding api's in the application
layer right so just to give you an idea
how things looks from the data shell so
you take or you define a key and in my
case I'm just saying I want to give one
prefix user just so I know that this key
belongs to user entity the second is in
my case I'm saying I'm gonna give M for
male and F for female and then some ID
user ID right and that the value is the
JSON object so I'm writing two different
value records into into the database and
once you have did the put which is a
right then you can just simply say get
me all the values that have a prefix of
you so see I have two records I use you
but with the different ID but because
while reading I'm not passing any ID I'm
just saying show me all the records
showing me everything that I stored you
know in a well formatted JSON manner
okay so tomorrow the requirement changes
and you say I want to make some changes
to my my schema but I don't want my
application to crash just because I
changed something so how can I do this
very quick so in this case what I'm
doing is the date of birth that I used I
want to replace that date of birth by
year of birth but I just say that if you
find any record with date of birth then
use that as a Elias and that's what I
did and on top I added one more
attribute called state right so I want
to gather something new to my already
sewn together something new but don't
want
to effect what I already have in my
database so what I do is again I disable
my previous schema that I registered
which is version 1 and then I register
this newer version of schema by simply
saying now add this schema and use this
arrow schema format and then using this
the evolved schema I wanna make sure
that my previous data still works right
so when you do the same get operation
the year of birth is basically what was
it sorry
it was date of birth date of birth is
now being displayed as year of birth and
state because I never stored that
information so it is coming as unknown
right but moving forward if I continue
to add state information as well for my
customers it takes care of that moving
forward right so what it helps is
developers to take the ownership and and
make or do a quick kind of prototyping
or go with the flow I mean it's a agile
kind of way of doing things not waiting
for DBA to make a change on the
persistence layer and then you do the
object relational mapping much more
efficient way of doing things so at this
moment I will just hand over to my
colleague Ashok Ola and we will talk
about a case study where we have used
JSON and time series data and we'll see
how orko no sequel database is being
used
yes please
question is is there any reason we are
using JDK 6 the reason being that what
we have built so far has been using geo
TK 6 so you might if you use anything
lower than this there might be few
features in the in the Java API is that
might not be compatible so that's the
only reason why not the later well later
so this is the cutoff so you can use
anything
above 1.6 right yeah I shall go ahead
thanks hutch yeah you have to turn it on
so yeah
excuse me you can take mine
yeah it's on okay so I'm going to talk
about a use case here it's time series
analysis using no sequel and the time
series data that we're going to talk
about is the stock tick data right so in
the stock world data gets generated
rapidly and it goes it grows to a
voluminous manner and it becomes a big
data problem
right so how do we use no sequel to
store the data and then do some sort of
analysis on top of it so what are the
big data challenges so in this
particular use case right so the
challenges our data is coming at a rapid
pace so you need to store in a storage
layer which is quick consistent so that
you can also read it back and do some
sort of analysis right and there are a
lot of customers who are trading and
they need to know what is the current
value and what is the historical value
so that I can do some sort of trend
analysis and how do you ensure that you
do all this in real time so this is a
challenge that this particular use case
that I'm talking about has so let me
introduce you to this application so I'm
going to demo this at the end and this
Oracle has not you know is not ventured
into investments yet so it is a
fictitious application so we've just
given it a name Oracle measurements Inc
so what this application does is the
goal of this application is to ensure
all this fast incoming data it needs to
be stored in a quick and consistent
manner and give a platform to the users
wherein they can do real-time no trading
do some sort of historical analysis like
your moving average no calculation based
on that they can do uninformed no
trading so that's the you know the goal
of this application the challenges of
there are lots of users who are
accessing it concurrently so you need to
have no very low latency so that no time
is money here right so if you lose time
then you are at loss so the value that
it brings yes
real-time data capture and that
real-time order processing and I know it
mentioned about no scaling automatic
partitioning of the data so based on
your major key a shard is selected so
based on that the data is stored over
there in the particular shard
so this enables easy you know elastic
scaling so if your data size grows which
obviously in this case because data is
getting generated at a rapid pace and
over a period of time it becomes a big
data problem so you need to think of how
do you store or how do you manage the
data so all you do is just add a new
node the whole cluster balances it out
right so and its lowest dollar per
operation because you don't need
expensive servers so you can use any
mid-range server come already Hardware
no sequel will work just fine
and it's a highly available system so
one was mentioned about that you've got
a master save architecture it's highly
available so all those value ads that I
just mentioned is being driven by Oracle
no sequel so Oracle no sequel gives you
super low latent rate and right right so
that is the reason why the Oracle
investments Inc our fictitious
application is able to service our
customers so you can easily perform
real-time trading we can perform
historical analysis you can get all the
data that you have stored so far do
moving average calculation and then you
can take an informed decision right so
that is the reason why we've chosen
Oracle no sequel and the type of data
that we store here is user profile
information which is the first name last
name user ID etc as well as the
portfolio information so what stocks the
user currently has we also store the
stock tick data which is getting
generated as we speak right so in our
case it's getting generated every 5
seconds but in the real world it could
be even faster so depending on you know
the bed and all those things that happen
in the stock world and the transaction
information so if I place any trade
requests by yourself that is also stored
in the no sequel database this is the
architecture of the application at the
top we have the presentation layer which
is a simple JSP so we could also use
html5 because as was mentioned right
html5 gels easily with JSON so you could
easily store the data which can be
represented very quickly on html5 but in
this case we've used JSP the middle
layer is the business layer which is
entirely written in Java so no sequel
exposes Java API s-- and you can easily
use it to get and put data so there's an
analytics component which performs
moving average so here it is and the
training component which performs your
real tried real time trading so
everything goes to the nose equal to DT
and then here we've got a stock tick
generator now obviously we couldn't turn
out tap into the real world so that's
not possible
so what we have done is we have written
a component but which generates data so
it generates it every five seconds and
pushes it into the nose equal a DB so it
takes as an input seed data from Yahoo
Finance so we have you know downloaded
few data from Yahoo Finance and we have
put it into the stock tech generator so
that generates the data so every five
seconds it generates random data which
is well within the range as given in the
seed file and then it pumps it into the
database so that data is used for your
intraday activities and all the other
historical data that is there in the
nose equal DB that would be used for
your trend analysis for moving average
so I'm going to go to the demo in a
short while so this is how they go to
store the data in the database right so
this here is the day's closing right so
we've got all the timestamp so when it
was closed what is the last value and
this here is the intraday data so every
five seconds you've got a new value so
be
and that the user would make a buy or a
sell right so this is this is a JSON
array and this is just a JSON field and
value so in the no sequel world how do
we store it we store it using a key and
a value so what is the key the key is
symbol followed by year month so this
constitutes the major key so remember
using the major key a shard is selected
right so you need to be sure whenever
you are designing your application so
the design level you need to be sure
what is the data that you are expecting
so based on that data you know to chose
what would be your major key because
that decides your data localization
right so here and you need to ensure
that it is randomized and so that you
don't pump all the data into a single
shot so in this case I've got symbol
year and month in the major key so I
have no data spread across at least you
know for every month right and date here
is my minor key which means I'm storing
all the data for a single day as a
single key value pair so if I want to do
some sort of historical analysis I can
just use one key get all the data and do
some sort of which are condenses all
right so that's idea but there can be
variations so you can think of probably
storing hourly data instead of days
worth of data because days worth of data
can grow it could be huge right so split
up across in an orderly fashion
so that you can increase the performance
so it's really up to you what you want
to know how do you want to design the
data all right so next we go to the demo
I'm going to switch the slides here so
yeah so this is how we do not quickly
start the database so I just want to
show you how we start the database and
how easy is it to actually visualize and
you can actually go and monitor the
database start up in a simplified
console here
there you go they've got storage nodes
and replica notes so under mentioned
about all those so you can check if it's
coming up so there you go it's up right
so now I can go ahead and start my
application so which is a simple Tomcat
application so when I bring up the
application I get the the webpage going
here so as this application is coming up
the stock tech generator that I
mentioned about the data generator it
gets a not kick dip and it generates the
data right so if you can see here before
I actually login so the left-most table
here this one so this is displaying my
real-time data so the stock tech
generator the component is generating
data and it is pumping it into no sequel
and we are reading it in real time
alright so all this is happening in real
time so once I log in I can visually see
my portfolio so there you go
so I don't have any data purchased as of
now so I'm going to make a purchase yeah
I know why that's ok so here I'm going
to make a buy so let's say before I make
a buy I don't I want to do some sort of
historical analysis so let's say I'm
going to buy Oracle so let's see how
Oracle has fared alright so here this is
this chart shows me gives me moving
average and so here is a moving average
graph so the black line is moving
average so if I point over there so the
moving average says $32 right so if at
all I'm going to make a buy
I'll see ok the price would be in and
around $32 so I might I don't want to
buy at a price higher than that so I
will go ahead and see what is the
current price ok the current price is
also 32 so I can go ahead and buy right
so I'll say I'm going to buy 10 stocks
of Oracle at the market price because I
know that that is the normal trend of
the data right so the order has been
processed this was all done in real time
and there you go so it's come right so
this is the beauty of Oracle no sequel
but in everything happened in real time
so I am getting data from my stock Tech
generator into no sequel my order data
is in no sequel my trade requests are in
no sequel so all this are independent
and this entire application is being
driven by no sequel so that that is what
I want to live with the message that I
want to live with that no sequel
databases are made for quick data that
are coming at at a rapid pace and you
want to store them quickly and you
expect them to grow in the future and
you want to do some sort of analysis by
reading them quickly
so that is the whole idea so much we are
open for questions
yes quotient is automatic partitioning
so what do you want to know about it
from the key value point of view or from
the storage point of view okay okay so
automatic partitioning from the driver
point of view what you would expect is
that your records your data is it's kind
of balanced on all the shots right you
don't want a hotspot to be on one
machine and then other shards or storage
nodes having nothing right so you won't
get better performance so what we do
from the driver point of view when you
write a record is we use hash algorithm
which we have tested with many terabytes
worth of data to make sure that this
algorithm kind of distribute the data
evenly right and and the way we have
done is we have done a scalability test
on many different sized clusters just to
make sure that everything is scaling
kind of linearly from the storage layer
right so this is one aspect from the
from the application point of view from
the storage point of view all these data
I would say is kind of partitioned into
the shot so we use term as a shard which
is a slice of your full data you can
have three shards having each shard with
one third of data right so the the
driver will take care of evenly
distributing your data in on to the
shards and from the sharp point of view
we take care of replicating the data
within the shards using that different
application notes right and we also
introduced one more feature I think in
release 2 which is on demand
elastic expression and what we do is
that you just add
new storage notes let's see if I have a
slide for that let's see if I can view
it okay so this is the elastic expansion
scenario that you start with single
shard with three copies for availability
reason but then when your business grows
you just would like to grow it without
any downtime right so in this case you
just add some more storage nodes and it
a adds those storage node or a shard
onto the same cluster and then evenly
distribute all the old data that you had
to begin with on the previous hole shot
and move it to the added cluster right
so it's all happening at many different
layers and what you get is as I said the
predicted predictability from the
performance point of view and it's very
linear linearly scalable any other
questions okay so question is how Oracle
no sequel database is different from
other no sequel database out there there
are as I said many different ways you
can differentiate one way I mentioned it
to you the way that I stored a key value
column in our documents graphs right so
that is one level of buckets but moving
forward I mean I don't think it will
matter a lot because we are moving
towards standardization right so
everything from your application layer I
mean you would not mind how your data is
stored as long as your s LS are met all
right so because that's what from the
application layer you expect that I need
10 milliseconds latency I need 3
millisecond latency and I want to have
primary indexes and secondary indexes so
I can run some some queries on on my
data right the way I say that we
differentiate ourselves is that a we are
the only relational database vendor that
has non relational database as well
right
so Oracle is the only which has
non-relational databases and what it
mean is that for you
we provide you a better integration with
other Oracle stack it's not like you buy
a no sequel database from open source
community then you put it in your
production and what you have is a silo
but it's not talking to other relational
database and you end up with your big
data sitting right on on to the no
sequel database but then you cannot do
anything what we are doing is we are
providing you the integration again I
should have a slide yeah this slide so
we have spent a lot of time and effort
doing integration with other Oracle
stack right here but we are showing you
is you have Oracle database for all your
fast data right you just capture those
data but then if you would like to
access your or store your graph data as
I mentioned the Triplett's
you can do it by using our graph
cementing product then we have also
integrated with event processing
remember I mention about the third
scenario where the use cases could be
even processing so we have a product
called Oracle even processing we have
done integration also done integration
with Oracle CONUS meaning if you need
very fast access in microseconds right
in memory database access then you can
use coherence for all your hard data but
if you happen to have more data that can
fit in the memory then no sequel can
provide you all those moving into the
cache and then processing it back into
the into the storage layer right so end
of a day again there is another layer or
another integration which is with the
our relational database external through
external tables right now what it gives
you or gives the enterprise customer is
ability to store any kind of data and
then talk to each other right you can
still say that I have a customer who
bought something so you have a
transactional purchase history of that
customer right sitting in the relational
database but then his behavior
information what what things they liked
what they were browsing on your web page
all that information sitting in in the
no sequel right and then you can join it
so I think that resonate pretty well
with the enterprise customer so this is
how we depreciate ourselves and then
there are other aspects as well which we
believe is the predictability in
performance so any further questions yes
please yeah is it similar or different
from or how different it is from the
Berkeley database so yes that's a great
question we are actually the same group
Berkeley database right and we have this
a decade-old battle-tested technology
that i guess you might already been
using in your smart phones which is
Berkeley database but it is a embedded
database B it's not a distributed
database to begin with right now if you
want a store massive amount of data
right so what we did is we said we
already have very stable technology
Berkeley database but it works only on a
single shard
let's use the same storage technology
and put a distribution layer on top so
we can have more capacity more
distribution so to answer to your
question it is the building block of
Oracle or sequel database it's not in
process I didn't get that what do you
mean by that
yeah so yeah I mean as I said it is the
building block you would not even know
but we are using or Berkeley database J
Edition under the cover so if you have
any experience with je how to tune it
mean that that is still applicable and I
should also mention that many other no
sequel that you might already been using
it whether it's LinkedIn who build their
own volley mode or Amazon who build
their dynamo no sequel database uses
Berkeley database as a building
- right so we decided why not we build
our own technology because we understand
our product better than anybody else so
that's what people Oracle not sequel
database any other questions yeah please
it's in the application here so yeah no
no you will have to get the data and
then do that yourself
I think the point we were making is that
you can do time series analysis right so
that is actually capability of the
product because you can have time stamps
in your key structure and then all you
have to do is do a query based on time
stamp right and what you do with that
that's that's your business logic
sitting in your application all you do
is aggregate at your application level
okay so yes last question yes please
yeah there are at the moment
not many aggregate functions in the
product so yes you have to based on the
key structure you just pull out all the
records and then then do the aggregates
on at the application layer but VR under
the disclaimer working on providing some
kind of functions as well yeah well
thanks everyone I hope that it was
useful for you folks thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>