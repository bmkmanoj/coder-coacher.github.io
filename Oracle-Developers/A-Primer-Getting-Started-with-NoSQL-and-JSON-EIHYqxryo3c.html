<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>A Primer: Getting Started with NoSQL and JSON | Coder Coacher - Coaching Coders</title><meta content="A Primer: Getting Started with NoSQL and JSON - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>A Primer: Getting Started with NoSQL and JSON</b></h2><h5 class="post__date">2015-06-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/EIHYqxryo3c" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay we're gonna talk a little bit about
Jason no sequel databases so the first
thing is what you know why are we
talking about this stuff and really
there's a few different things going on
there's a lot of hype in the industry
today
people are using Jason in a lot of
different ways as an interchange format
and the enterprise people are using it
because it's interesting and easy as a
browser format it gives you nice dot
notation in JavaScript people are using
server-side JavaScript there's a lot of
stuff going on and then also excuse me
there's a lot of things going on with no
sequel databases and a lot of confusion
over well what do you do with the no
sequel database and what is it good for
and how does it interact with the rest
of your ecosystem and then then you take
the two together and now what does it
all mean so we're gonna hopefully
address some of those issues in this
talk just first off I wanted to get a
sense for how many folks out there have
done anything with no sequel databases
okay so handful anybody doing anything
with Jason well way more great okay so
hopefully we can clear up maybe some of
the mystery around why even look at a no
sequel database and what does that all
mean okay this is the standard
disclaimer slide we may talk about
things we're delivering in the future
it's it's not a commitment or a hard set
of facts in terms of when we're gonna
deliver that okay hmm
so we're gonna talk a little bit about
the Oracle no sequel database I'm gonna
give a brief overview in terms of what
we do there I'm gonna talk about
something called Avro and when we get
there that'll become it'll become
evident why we're talking about Avro
because it relates directly to Jason
we'll talk about how Avro Jason and the
Oracle knew sequel database interoperate
together and then we'll talk a little
bit about how one might get started in
terms of development okay so what is the
Oracle no sequel database well
essentially it's a it's a very scalable
and we when I say very scalable it's
really designed to scale horizontally
across many hundreds of different boxes
so it's a it's really a automatically
charted highly available key value store
and key value store is an interesting
thing in and of itself because the key
value store storage engine that we use
under the covers is Berkeley DB if
you're familiar with Berkeley DB it's
it's an in process key value store
released a set of b-tree access methods
that are transactional but it's C it's
the most it's probably the most popular
download ever I think we're at something
like 2 billion downloads of Berkeley DB
it's open source it ships under Apache a
GPL and really it's a storage engine and
at the core of the Oracle no sequel
database no sequel or I'm sorry Berkeley
DB is our storage engine it's been out
of the market for roughly 10 11 years at
least the Java version that we use it it
solves mission critical workloads out
there for many years for folks like
Amazon and LinkedIn macys.com so it's at
the core of what we do
we're built on a pretty solid storage
and that engine that we feel very very
good about and so what we do is we
actually build a layer on top mmm that
really does with all of the auto
sharding all of the elastic expansion
all of the high availability and all of
the transparent load balancing that one
would expect with a no sequel database
so if you look out there in the market
you can find a lot of a lot of no sequel
databases a lot of open source and you
know almost all of them have you know a
few things in common and that is they're
designed to horizontally scale across
you know a set of commodity hardware
servers they operate in a cluster they
all operate roughly as highly available
auto sharda systems and we offer the
same capability with a with a few
different twists so we offer
actions at the shard level I'm going to
talk about that in a second one key
thing you note with us is we do offer
data center awareness so we make use of
the fact that we understand how to lay
out a topology such that the topology is
truly highly available and resistant to
zap to disaster recovery so one of the
key things you want when you're dealing
with data centers is the ability to
layout a store such that if any disk
fails any machine fails and Iraq fails
any data center fails right you still
can maintain some level of service for
your business right the other key piece
here is that we actually shipped with a
client-side driver that is really it's
just a Java jar file you just map it
into your JVM address space and off you
go
that driver basically just takes
whatever API requests you throw at it
and it will figure out which node to
route that request to so there's a lot
of logic built in that driver ok one of
the things that's pretty key about or
pretty central I don't want to use the
word key pretty central about these key
value stores is that they're very very
simple data models right so if you think
of you know modeling something in the AR
DBMS world you know you think and wrote
in terms of of tables and you think in
terms of PKF gate relationships and you
think in terms of you know what columns
are gonna are mi going to define as keys
when you look at the world in the key
value store paradigm what you're really
looking at is right how can I wrap my
head around my problem such that I can
define a key space to handle the queries
that I'm going to be able to need to
handle to satisfy that my application
requirements so the bottom line here is
that you know no sequel databases are
are mostly used for very very simple put
retrieve some range scan operations but
none of the date none of the no sequel
database is out there on the market
today are really meant for large-scale
ad hoc query processing you know
they very few of them have sequel
interfaces if they do they don't support
group by order by no aggregation so you
know before you even go down the route
of a note sequel database and before you
get to Jason and all that you know ask
yourself do I have a business problem
that is reasonable to actually solve
with a no sequel database okay from a
model perspective again simple flexible
key value pairs simple operations we
talked about that we do support ordered
scan of key ranges we do ultimately
under the covers have a b-tree so we can
do ordered scans we also support
unordered scans which are non
transactional order scans are
transactional we also have a unique
feature where we support a stream api
for large objects and what we do for
large objects is we actually take the
object and we consider a large object
say anything over a megabyte so let's
say you want to store MPEG files or JPEG
images so I need anything fairly large
we'll take that object we will split
that object into 256 K chunks and we
will actually spray the entire object in
those chunks across the cluster and then
we do that using a streams interface
also on the way back out as you're
actually reading from that input stream
we are reassemble we are reassembling
those chunks on the fly so we actually
spread the i/o load out across the
entire cluster and we offer Java as well
as C api's to this to this door okay so
to give you a little bit of insight into
how we deal with these keys and how that
turns into transactional operations so
let's take this example so let's say
we're thinking about storing an object
right we know some things about that
object we have an SSN number first name
last name clearly when you know we know
the SSN is going to be unique and we
have to make a decision in Oracle no
sequel database
okay we have to say which part of the
key is going to be what we call the
Bajor component which part of it is
going to be the minor component so the
major minor bifurcation in Oracle no
sequel is is key to both transactions so
the transactional boundary is off an
operation operate at the major component
level I'll explain that in detail in a
second
it is also central to the way we
actually shard and distribute the data
across the cluster so for example let's
just say you decided to use the social
security number as the major part of the
key so what does that mean that means
let's say I want to do multiple
operations against this person let's say
they they bought an item and then a
minute later they actually modify the
order so we're building a web scale
store for order processing and we want
to do that transaction like let's say we
can do that in Oracle no sequel if we
choose the social security number as the
major part of the key because what that
means is that all of the transactional
operations for SSN 1 1 1 2 2 3 3 3
excuse me will all go to the same shard
now once we get to the shard we can
actually have a fully acid compliant
transaction and the reason we can do
that is because every single one of our
shards an Oracle no sequel is a master
slave model and so if you think about
the layout of a cluster where you may
have two or three shards right each each
shard has its own master and set of
slaves like that and it is distributed
across a set of commodity boxes and
every time you actually hash one of
those major keys and get to a shard we
actually route all of the right requests
to the master node now once you actually
get routed to the master node that now
it's the actual rest of the key the
minor part of the key which composes the
actual full path or unique path to the
in the b-tree okay so a little bit more
about Keith's before we start to talk
about Jason specifically so when Oracle
no sequel keys are basically any
arbitrary string the application really
is responsible for setting up that key
space defining what that key looks like
but it's actually interesting to think
about a key and Oracle no sequel as a
hierarchy so if you think of a unique
path to a file in a file system right
essentially you have a hierarchy
separated by back slashes or forward
slashes depending on your file system
and it defines in no uncertain terms a
unique path to that single entity called
the file that you're looking for very
very similar in Oracle no sequel when
you actually start to look at the api's
and how things are defined the key is
really just a string alright and that
string is structured again as a path and
when you do things like sorting that
string it just obeys the natural sort
order of strings in Java so for example
if you want to use dates yeah I'm giving
a talk later today on how you do
modeling and and there we talked about
how do you actually you know structure a
string and a key space and a you know
figure out how to wrap your head around
that so things actually soar correctly
when you look at dates but that's a
little bit outside of this talk so the
value part of the key value pair in
Oracle no sequel is really an arbitrary
just byte array right and you really are
responsible for serializing that byte
array or for serializing your object
into that byte array with one unique
exception and that is when we look at
Jason and then of course you know the
full key to your data has to be unique
okay one other quick thing about Oracle
no sequel we do support RDF graph
anybody here looking at graph support or
doing anything interesting with graphs
okay a couple of people one one person
but essentially we we support the RDF
graph package on top of our system which
is basically ships with a sparkle query
processor and the Genna standard set of
ap is really the the graph paradigm is
to discover relationships you know in
this data it's essentially if you think
about it you know figuring out who knows
who and degrees of distance between
those people you could think of it
really as a recursive join and one of
the things key value stores are very
good at is avoiding that recursive join
problem okay so let's talk about Jason
okay so you know it's a document model
right it's very similar in structure to
XML right it's a it's a denormalized
document structure the interesting thing
about Jason today is that it's very
convenient to use from object-oriented
language it supports languages it
supports dot notation
it supports array indexing it supports
Maps it's very very easy to use in the
browser on the client side in JavaScript
now we see the popularity of of
server-side JavaScript nodejs and a few
of the other providers it's just a very
very convenient mechanism to use not
that different from XML just I probably
just more convenient from the
programmers perspective okay so what is
this thing called Avro and why do we
care so Oracle know sequel databases you
that Oracle know sequel database uses
Avro as its internal serialization
format for data and Avro has some very
good synergy with
and like I said it's really it's an
open-source package that is really
concerned about taking an object see
realizing that object extremely
efficiently getting it to do it well
they don't get it to this but they see
realize it and conversely on the way
back out BC realizing a bytearray back
into an object it chips is apache
open-source it was developed by doug
cutting and that's the guy who was one
of the early architects for hadoop it
supports c++ as well as java language
bindings and it has this notion of a
predefined and I put this in quotes
schemas and you know a lot of people
talk about no sequel databases as being
as being schema-less and flexible schema
and unstructured data and when you
really really look at these things
closely right you know all data in the
world has some structure to it right so
if you look at just random text data
right there is some structure for you to
be able to understand it in a natural
language right it has to obey
grammatical rules your brain has to be
able to parse it and make sense out of
it
so there is a notion of schema somewhere
embedded in the notion of being able to
make sense out of data and when we talk
about schemas and JSON data and Avro
data really what we're talking about is
how do we define the structure of this
thing so we can recognize it we can
serialize it we can do D serialize it
and we can actually make efficient use
of that thing and we're gonna learn more
about that in a second
so there's a bunch of different
serialization protocols out there I
don't know folks have heard of Google
protocol buffers or thrift but
essentially there's a number of
different ways to actually serialize
data there's something called Beeson
which is binary JSON so why did we
actually choose Avro so Avro as we're
gonna see is it's very very compact
and it's very very efficient and when
you're looking at actually storing data
in the world of quote big data when
you're looking at hundreds of terabytes
or petabytes scale you know saving a few
bytes here in there adds up extremely
quickly all of a sudden you find
yourself with a few extra terabytes of
data and then of course you want to
multiply that by two because you are
going to actually store replicas of that
data right and depending on how nervous
you are about h.a you may actually have
a higher replication factor that means
you're going to multiply it by an even
higher factor so one of the things we
looked at in this world is you know data
grows exponentially gets larger and
larger people are using replication
strategies to actually provide high
availability what can we do here that
would be very very highly efficient and
compact we're gonna see in a minute Avro
was the answer
it's got great synergy with Hadoop it's
getting very very popular on the Hadoop
side of the world so Google protobufs
was was quite popular for for a time and
I think still is as well in terms of
actual raw storage format on HDFS when
you look at actually the implications
there same thing as as a no sequel
database it's big data it's it grows
quickly it's very very hard to modify
once you lay it down onto disk so you
need a storage format that is going to
be able to let you deal with these
things like schema evolution and we're
going to get to that in a second
but essentially a lot of people are now
using abro as their storage format for
Hadoop which which we think is a good
thing because when you look at basically
taking data from any of these stores dos
sequel database HDFS Hadoop and then
look at merging that with other data in
your enterprise it'd be nice to have a
storage format that's actually
interchangeable and can actually play
nicely with all those different sources
and we
we think Avro is pretty compelling from
that perspective okay it easily supports
civilization from and to Jason strange
it is synergistic with Jason because the
schemas are defined using Jason but it
really doesn't rely on Jason and it's
quite nice in terms of being able to
serialize and deserialize Jason objects
it supports bindings in terms of
language constructs and I'm going to
talk about that in a minute but
essentially it supports three different
kinds of bindings to be able to get
objects serialize and deserialize
it contains this concept called schema
evolution so schema evolution of course
if you think about you know an
application that you're gonna write
maybe on an RD BMS or any any other kind
of store you need to be able to change
that schema over time gets even more
interesting in the world of you know a
cluster and maybe a large-scale cluster
because at any given point in time
there's going to be parts of your
application running on App servers that
may be using the old schema maybe the
code is relying on the old schema parts
of that cluster that are going to rely
on the new schema so right there's
there's no notion in the world of large
clusters in the world of 24 by 7 there's
no notion of bringing anything down you
have to evolve things on the fly as the
system's up and running so at any given
point of time there's going to be
different pieces of code that are that
are really relying on different versions
of the schema and the schema evolution
concept here is quite nice the schema
definition is actually tracked under the
covers with a with a small bite we're
going to talk about that a second and
it's always really the schema of the
writer of that data and the evolution of
the schemas are typically opaque to the
readers so
we're going to take a look at a Grove
ursus be sound so beasts on is another
popular storage format and just to give
you a sense of why we went the route of
Avro in terms of compactness and savings
so what you get in visa is you get all
of the field names stored along with the
data so you can imagine a degenerate
world where you have fairly large field
names but fairly small data items and as
it turns out you're actually taking up
more storage with your metadata than you
are with your actual data the benefit
you get for this is it's extremely
flexible you don't have to define
anything up front right you just take a
JSON document you slam it into a store
and you can actually start querying that
data but of course you know we all live
in the world of trade-offs for that
flexibility you pay a fairly high price
and it is tied absolutely directly to
Jason right everything is defined as you
know a Jason document just a surrealist
serialized form of it so how does that
actually play out so if you look at just
taking the JSON document with a single
field hello world what there you can see
the way that actually gets stored in
beasts on or serialized into a byte
array and beasts on and that alone just
in visa will be 38 bytes right so taking
hello world will cost you thirty eight
bytes to serialize that and get it to
disk if you look at actually doing
enough something like MongoDB add
another 17 bytes because they actually
generate a 17 by unique identifier for
each object so now basically you're
looking at 55 bytes to store a hello
world right fairly expensive but again
what you get for that is extreme
flexibility okay when we look at this
compared to avro right the field names
in Avro are stored once right so all the
metadata is stored one time right it's
not as flexible right as be sawn and
Avro addresses that flexibility with its
schema evolution
mechanism which I'll talk about in more
detail
it has synergy with JSON but it's
actually not tied to Jason you can take
any object and serialize it using a TRO
and deserialize it so let's take our
hello world example
so it's stored basically as that hex
string it takes six bytes if you store
that on Oracle no sequel database add
two more bytes because we use a small
integer for schema ID so now you're
looking at eight bytes to store hello
world versus 55 bytes in the two
different stores right so you're trading
off a little bit of flexibility but we
believe you're getting back a phenomenal
amount of storage so if you of course
you know if you're in the world of big
data and you multiply this out it gets
even more dramatic this is an example
coming from say a large website owner
they're storing you know 250 million
unique cookies you know they want to try
to identify the behavioral
characteristics of their user base let's
just say for example we have we've
defined this cookie using Avro you can
see it's a record it's got an ID it's
got a frequency it's got a last visit
which is the you know string date of the
last time the user visited and it's got
say a set of behavioral segments like
this user is you know interested in
sports or they're interested in finance
home mortgage some some sort of
behavioral segments and when you look at
storing something like this 250 million
versions of it which is not outrageous
when you look at in the world of folks
that collect online behavioral data if
you take an average size of this object
roughly 64 bytes and if you say the
average number of behavioral segments a
user is in is 5 so that's roughly let's
say it's an average of average length of
40 bytes so if you look at storing this
in a besom
serialization format you're gonna pay
181 bytes per object 198 bytes if you
add the unique ID for example if you
store it on Mago to be alright so
basically when you multiply that out
you're gonna get about 42 gigabytes of
storage and visa and 46 gigabytes of
storage in MongoDB now if you actually
store this in Avro you're going to get
about 131 bytes per object which is a
total of thirty point five gig just in
raw Avro and you're gonna pay a little
bit more if you actually store that in
dos sequel database right two bytes more
but essentially what you're looking at
here is roughly 44 percent less storage
by storing it using Avro on Oracle no
sequel versus storing it using beasts on
on something like MongoDB so just a
little little bit of motivation for you
in terms of why we actually went the
route of Avro for storing Jason okay a
little bit more about our schemas
they're defined using a JSON document
itself right
even though defining the a for a schema
is a JSON document so it's kind of
dependent on the JSON syntax there's no
real dependency between Avro the ever
architecture itself and JSON other than
the fact that it just uses you know the
convenience of JSON notation for you to
specify your schema so it's a it has
supports for some very nice data types
to be able to define what that object
looks like right records enumerations
you know unions of fields Maps arrays
there's another type called fixed which
basically says you know I want two bytes
of data I'm gonna put whatever I want in
there but make sure that this field is
two bytes it supports another thing
that's that's really quite nice called
field level aliasing so this is the
notion of being able to take a field in
a record
and say I want to give it these four
aliases I'm gonna define a field called
I don't know nickname all right so I'm
going to store every every person record
with a nickname and I'm going to give it
an alias that's and the alias is going
to be say secondary name so that means
all the code that that your write can
use the alias and then when you decide
to evolve that schema moving forward and
you decide to change the underlying
implementation say of the nickname to be
say the middle name of an individual
none of your code has to change right
it's all using the alias so it's a kind
of an it's it's very similar to the
whole interface implementation world in
Java where you can code against the
interface and then if you need to change
the implementation underneath the covers
or you add a new implementation under
the covers you don't have to rewrite all
of your code so it's really kind of a
nice mechanism and and something that I
personally haven't seen in in any of the
other serialization formats it supports
a sort order for both primitive types as
well as object types which of course if
you're going to be able to you know call
the equals method on two objects you
have to have a sort order and of course
if this if the object is stored using
C++ and you want to read that object
using Java you need a storage or
serialization mechanism that is going to
be able to support a sort order so that
you can actually call the equals method
and decide whether these two objects are
really equal and again it supports that
whole notion of schema evolution through
the concept of you know I write with one
schema I read with another schema and
again you know schema evolution is
opaque to the readers okay so let's take
a look at what this might look like so
let's say we're developing an email at
scale application
all right and just a little background
if you look at you know if you look at
the providers out there like Yahoo
Google a lot of these folks to be able
to support email at scale are doing that
through no sequel databases right you
cannot support 300 400 million email
users without actually sharding that
data scaling that horizontally across a
cluster of machines so it's it's a
fairly common fairly common paradigm if
you're actually going to build you know
email a scale to do it on no sequel
database and so what would that actually
look like if you if you went down that
road and actually used a pro to store
messages using you know JSON format so
here we have used the Avro specification
to define an email header as well as the
email message itself and being not an
expert in email format don't take this
too literally because I I have no I have
no background and actually you know x.25
email formats or anything like that but
essentially to give you an idea of what
you can do with a fro is you can define
a record give it a namespace so in this
case the email header we've given a
namespace as you know calm down
company.com backup of the X dot email
about Avro and we've given it a number
of different fields right we've defined
a from a to a sender a CC of BCC and a
subject and of course if you look at all
of those they're they're all typed they
all have to fall values what does that
mean so when we when we talked about
schema evolution right one of the key
things with schema evolution is the use
of a default value right and really the
key there is to false are not used for
the writers they're used for the readers
so if you happen to change your schema
moving forward
and let's just say you delete the BCC
field for example from your record and
what you're trying to do is right you
have some of your code on part of your
cluster that is using the old schema
right and part of the code on your
cluster is now using the new schema so
in your object you don't have that BCC
field in the new part of the schema but
you have it in the old part of schema so
if you read a record out that actually
has that defined in Avro it will fill in
if you're using the old code it will
fill in at the time of doing the read it
will fill in that field that is declared
as the BCC with your default value which
in this case is nothing and of course
we're using these aliases to define a
separate access method or a separate
interface to access those fields in case
we want to we want to change those later
on and then we define the email message
right again with a name space combat
company dot a couple yaks that email
about Avro we are referencing the header
by specifying the namespace that we
defined earlier and qualifying it with
the object name as the actual header or
the type of the header field and then we
just define the body as a strength so
that is a an example of using Avro to
define an object that you actually want
to store and we're to see in a minute
how we actually take Jason and convert
it into this Avro format okay so in
Oracle no sequel we support DDL for you
know adding and evolving and disabling
and enabling those Avro schemas and we
also support three object bindings one
is generic and it's basically it uses
string based getters and setters and you
know you can use things like you know
injection of control or if you're
familiar with spring it's really all
designed
this dependency injection design pattern
you basically have an XML file you put a
bunch of text in there you inject that
text into your runtime you know if you
if you feel the need to write some
extremely generic application where you
don't really know or care about the
contents of your record you can take
these strings right from an external
file and then you can actually send them
to the generic biding and you could say
you know get this filled using this
string and it will return you that feel
but of course you get back a generic job
a generic Java object and you have to
cast that for something useful
there's the concept of the specific
binding which is very similar to the
Google protocol buffer sort of IDL
compilation mechanism where you define
this record upfront you run the compiler
it spits out a Java class the Java class
has specific getters and setters for all
of the fields you've defined in your
class and then there's the Jason record
based binding which says okay I've got
this Jason object and I just want to
very easily get that serialized and put
into the most equal database and vice
versa right I want to read a byte array
out and I want to actually convert that
into a Jason record if you're familiar
with the Jackson parser you get an
object back call the Jason record you
can actually convert that to a Jason
node you can call to string on that and
you get something that looks like that
it's just a JSON document right you can
send out to your browser you can eval
that you can you can use JavaScript on
it okay so
so what do you do to actually develop
using Avro and Jason on top of the
Oracle no sequel database so we ship
something called kV light which is a
single node single instance version of
the Oracle in a sequel database oh it's
fully compliant and supports all of the
api's that are supported the Oracle in a
sequel database but it's something you
can actually get up and running and
installed on your laptop literally
within 10 to 15 minutes so you go to OTN
you download the Oracle of sequel
database you unzip it or you and you
want R it you know you put it on your
filesystem you launch KB light with a
Java Midas char and you're up and
running the other piece you need to know
is you know the admin command-line
interface very simple right you can
create a schema you can actually evolve
that schema moving forward or you can
disable it or you can enable it right so
first step right you install kV Lite
second step you bring up the app in and
you say okay I want to create this the
schema it looks like you know these
three fields and maybe it looks like you
know something like that you've
basically defined a schema that you want
to interact with with Jason and you
install that in the no sequel database
and then the third part is getting from
getting familiar with the API right so
there's a number of different api's out
there in the Oracle no sequel database
and really the ones you want to look at
are the ones that deal with actually
browsing the schema catalog because they
they get interesting to be able to find
the bindings that you want to use to
serialize and deserialize your data and
specifically the Avro bindings
themselves right the generic the
specific the JSON bindings if you you
want to get familiar with those because
those will actually be the things that
you interact with the most to be able to
get your data serialized in do you see
realised and then there's the data a
shell which will actually allow you to
introspect
into the store to be able to look at
what's going on out there right what
kind of date do I have in there what are
the keys I have what have I inserted I
want to basically take a look at what's
what's happening in that store so in
terms of the afro related ap is that you
would want to look at the ever catalog
ap is are the things that actually help
you find what schemas are defined out
there well findings are defined out
there do I have multiple bindings you
know what are those bindings look like I
can you can actually introspect the
schemas themselves and look at all the
fields that are defined in the schemas
again the generic Avro binding
interesting dangerous potato potentially
useful but again this is something you
want to be careful about when you're
looking at using bindings right generic
bindings can give you that flexibility
by you know writing code that really
doesn't depend on knowing the names of
the fields in your schema right you can
take those fields from somewhere else
right you can store the field names you
know in some other store and some table
on an already BMS or you know in an XML
file that you're going to read at
runtime but you know it's it's a little
bit dangerous right as those as those
things evolved as those objects evolved
you need to actually evolve potentially
those field names that you're pulling
from somewhere else all right so the
flexibility may be something you need
the trade-off is a little bit harder to
maintain moving forward right the the
specific Avro binding is a essentially a
POJO design pattern it's fairly
straightforward right you define a
schema and you run the compiler it
generates your Java object you access
that object at runtime specific methods
getters and setters are all there right
you remove one of the fields from the
schema magically it disappears in your
object moving forward right so the
getter does not exist right you get a
little bit of compile
time type safety there and then the the
Jason Avril binding right this is really
great for servicing you know Jason
requests either by the browser or you
know some internal app where you're
actually using Jason to support you know
things like an interchange format
between applications so what would that
code look like in Oracle no sequel so to
be able to write some JSON data so what
we've done here is we've created an
instance of an object mapper which is a
which is a Jackson class and then we've
actually got our hands on the catalog to
the no sequel store so let's sort of
took liberties here the actual store
reference is an instance of the KB store
object that we've initialized somewhere
else right so we've connected to the
store right and now we're actually
getting the Avro Catalog and then what
we're doing is we're saying to the
catalog object in the no sequel database
give me all the current schemas that are
in play right now
right I need to know what those are and
we get back a map and essentially it's
it's schema name by the actual schema
object itself and the schema object in
this case is an instance of an org hacci
Avro dot schema so that's actually
defined in the Apache Avro namespace and
then in this case we've said get me the
cookie latest schema now you don't need
to know the schema name and typically
your your codes knock at another scheme
and and we're going to look at an
example on a second that shows you how
you how you do this without knowing the
schema name but essentially you know
back to this notion of if you want to
use dependency injection if you want to
actually look at certain schemas there
is a way to get specific schemas and
interact with those at runtime and then
what we're doing here is we're basically
taking the content right which is a byte
array again
we're not showing the full code here but
let's say we have we have a byte array
that we want to actually store into the
no sequel database it could be an input
stream you know coming from a servlet
whatever and we get we get a handle to
the JSON binding and basically all we're
doing in a single line of code we're
calling the put method on the know
sequel database we're generating a key
for this key value pair and then we're
taking the JSON binding and we are
taking a jason record wrapping a JSON
record around that value which is the
byte array and we're using the binding
to actually serialize that Jason record
and put it into the no sequel database
conversely if you want to use Avro to
read that Jason data out right we're
getting our catalog from the store we're
getting our Avro binding this time but
instead of actually specifying the
schema name what we're doing is we're
saying alright catalog you know get me
the current schema get me the current
schemas and get all of the multiple
versions of the json bindings that are
defined there and what's gonna happen
here is when we actually get that single
json binding back what are we gonna get
we're gonna get the latest schema
binding that was used to write any data
in the store right so so typically the
way these things work is you always use
the latest and greatest binding as the
default unless noted unless you want to
read some older binding right and
essentially we're taking our jason
record right we're we're getting the
value from the store using a key right
which basically returns us a byte array
and we're using that binding to actually
create a JSON object and it turns into
adjacent record in the Jackson parser
world and then we're returning to our
caller a get JSON node
which is really the version of that
Jason document if you call the two
string method on that you'll get that
version of the Jason document okay
there you have it so what we're looking
at is just the data shell for no sequel
database so you can see some of the
commands that are actually supported
there you have aggregate of course you
have connect you can delete a key and
its value you can get a key in its value
you can load from a file you can
actually put a single key value and you
can actually show or return the schemas
that are that are in the store and you
can actually time certain commands so
it's it's just a little shell that will
actually allow you to interact with the
store to see what's out there in this
case I've done a get to find all of the
keys that I previously loaded into the
store I stopped at 25 so it pages the
results and then what I did is I took
one of those keys and I said well I kind
of I kind of would like to see what I've
written in the store by retrieving that
key and having it formatted as a JSON
document and so you can see you know the
key with cookie
whatever that idea is you can see that
there were five visits it's got a
frequency of five you can see that the
last visit to this by this user that
owned this cookie was that date right
for twenty to twenty twelve and this
user happened to be interested in sports
politics and is potentially a gamer so
you can you know the the data shell
gives you a little bit of ability to
introspect into the store and see some
of the data that you've actually
inserted into there it's a very useful
development tool okay if you want to
learn more about Oracle in a sequel
database come by tomorrow we're doing a
lunch and demo session we're going to do
some office hours development is going
to be there from 10:00 to 11:00
Andy Mendelsohn will be stopping by
so you're you can meet Andy Michaelson
who is the chief strategy officer former
CEO of cloud era former former VP of
engineering for our group will also be
there so please stop by if you have any
questions on know sequel database or a
bro okay so what do we do we talked
about the Oracle in a sequel database
what kind of a key value store that is
we talked about Avro why Avro is
interesting and why more specifically
it's interesting to us as a
serialization format we talked about
Jason schemas we talked about the
support for rich types field level
aliasing and then we talked about
getting started with development right
Cavey light the administrative API for
getting a schema installed as well as
the Avro api's and finally the data
shell to be able to introspect and look
into the store to see what's there
okay questions yes
so the question is is when we looked at
the what the get multi binding operation
right if you have multiple jason schemas
if you had say 5000 schemas installed in
your store would that be an expensive
operation and the answer is no is
because we we actually always know what
the latest for what the latest schema is
so we Kiwi and remember they the get
multi binding operation will return you
the latest schema that was used to write
data yes oh using Avro so if you look at
the actual hex
we'll get there so it's essentially it's
the bytes only for world alright so
think about how many bytes you need for
world plus two bytes to store the actual
schema in the sequel database right so
six so six bytes right to store those
those characters and then if you
actually look at you know doing this in
Oracle no sequel database right we add
another two bytes for the schema ID so
that gives you eight though yes so is
there any performance way is is there so
you want to know is there any
performance data yeah so I mean we've
actually run
we've run why CSB in-house to compare
Oracle no sequel against MongoDB we're
pretty confident with our numbers we
know that you know in terms of scale on
MongoDB there's a single lock at the at
the database level so when you look at
actually scaling a mixed-mode ycs be you
know something that is you know 50
percent read 50 percent update answer
it's a problem scaling that workload in
mago be happy to share the numbers with
you I'll give you my card and I can
certainly send you some of those numbers
but we feel pretty good about the actual
performance performance numbers yeah yes
so the question is is what if two
threads put data at the same time what
happens see in Oracle no sequel oh yeah
so we we use standard b-tree record
level locking right so we achieve acid
compliance let's say the record you're
talking about let's just say it has the
same major key so it's the same exact
record right same full key yeah so it's
gonna be last update wins and it's gonna
be fully acid alright because we
actually locked the single b-tree record
yes so you just specify the name of the
schema that you want to get right we
don't we don't give you any API to you
know specify the actual you know order
number of the schema whether it's the
third fourth whatever that happens to be
you would have like in this example you
would actually have to specify the name
of the schema you want
all righty</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>