<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Combining Collections and Concurrency | Coder Coacher - Coaching Coders</title><meta content="Combining Collections and Concurrency - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Combining Collections and Concurrency</b></h2><h5 class="post__date">2015-06-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/1NeZys7KEvc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so this is the session about collections
and concurrency we actually presented
this our similar session to this last
year and were asked to come back again
this year so my name is Chris Haggerty I
work in Oracle in the core libraries
team and I'm joined today by my
co-presenter Mike Diego he works in the
same team in Oracle so this is an
introductory session we're going to be
looking at some of the low level of
concepts and to do a concurrent
performance we're going to be using some
examples from the collections framework
to to demonstrate some of the issues
we're talking about so you know it may
not be all that exciting as streams and
other things you're going to hear about
this week so going to be talking about
it synchronized and the low-level
support for coordination between threads
in in the platform and the reason we're
talking about is and we talked about it
last year as well is because you know
every day we see mistakes and problems
in code like this so hopefully you'll
bear with us at least give us a chance
to explain some of the issues that that
that we see and how to resolve them
problems so most people have a mobile
phone in their pocket or are using it
right now and you'll be aware that it's
multiple cores in in most mobile phones
these days but that wasn't always the
case
a number of years ago back in the day
program flow was very simple and easy to
understand and typically it be single
threaded so your program would start it
would do some work and then it would
finish and if you wanted to understand
the the performance or analyze
performance of your application you were
more concerned with looking at the
actual the body of your algorithm that
was crunching your work from run to run
you attend to get the same output of you
of your program
that's not the case anymore for many
multi large-scale applications so we're
using some examples from the collections
framework here so I've just introduced
our first example which is a ray list
which we will be coming back to a number
of times through the presentation so
this is a very simple method to add an
element to an ArrayList it's not the
actual implementation from the JDK as
you'll notice the the add method here is
returning a void but for their purposes
of what we want to talk about it's it is
good enough so you can essentially
consider the body of the methods as four
separate steps you read the size of the
array we already delisting others we
write an element to the list we
increment the size and then we write the
size back to the list pretty
straightforward so I just want to
introduce that example and we'll come
back to that a number of times so
clearly there's only so much work to any
one CPU can do so if you're adding a
bunch of elements to an array or a
two-hour list as we seen in the last
slide if you've got complex objects to
take a long period of time to to to
create clearly you're going to be
bounded by how many objects the time it
takes to construct them objects and then
onto the list so in many cases to to get
something like that type of program to
run faster you're better off to add lots
of threads creating these expensive
objects and then stuffing them into a
list but if you are using a single list
you're going to have to be concerned
about the actual coordination and the
the overhead of adding to a shared list
so maybe you know you may want to
consider whether you want to use private
lists to to add your elements to and
then at some point in the future
periodically you may merge them into a
single shared list so your consumers can
get access to them but it's not as
straightforward there's a design
decision to be made here
about whether you want how much sharing
you want to do between your treads so
just to give a very brief overview
clearly Hardware has moved along
surprisingly in the last number of years
as you say most mobile phones these days
or even got multicores
but just to build up a little bit on
that you you typically have a processing
unit that unit itself kind of multiple
cores within it and then you will have a
multiple pipelines of of instructions
that can get executed when then decor
but thankfully for us a at the Java
platform level we abstract out a over
the hardware in Java line threads so we
don't need to concern ourselves with the
the low-level hardware itself although
that's important in most cases for most
applications which we can just deal with
with the java.lang treads so treads give
us a very nice abstraction above the
hardware typically a single thread in
the Java platform is mapped to one
underlying operating system thread and
they were just posing the question here
do we actually need threads in our
applications I think the answer as most
people know is yes it's pretty obvious
because you know if you've got an
application that's blocking waiting on
some i/o or some data from some water a
latent device you clear you don't want
to block your your whole application
waiting for that data to arrive and
there's various other reasons but that's
probably the most obvious and simple
simplistic reason why most people would
want to threads so skipping forward to
today I'm not sure if anyone was at the
open world keynote yesterday but you oh
I've heard a lot about getting the most
utilization out of your cores our work
has been done in Fusion Middleware and
develop different applications in Oracle
to better utilize cores but if you're
analyzing performance
a application these days you're no
longer or you may no longer be looking
at the actual low-level algorithm you're
more interested I guess in transactions
per second throughput the latency to
response times and again the utilization
of hardware if you've got multiple cores
in the system of course you want to use
them you can't just make your
application go faster by buying more new
hardware that runs faster you need to do
adopt it to take advantage of the
underlying hardware itself so I just
want to spend a few minutes talking
through the various difference I guess
programming models that that we have
sequential is the most straightforward
obvious model where you've got a single
thread is performing a single task and
it's not sharing any data outside of
itself and this is the most simplistic
model and most performant model but
again it can only get you so far
most applications these two days are a
combination of parallel and concurrent
so parallel is where you've got multiple
threads simultaneously performing
multiple tasks and again you may have
some shared data but typically you're
trying to minimize the amount of shared
data you would have here and this is to
some extent what a the parallel streams
API is trying to do and then if you go
full-blown concurrence it's where you've
got multiple threads they're all working
on potentially a single or maybe
multiple tasks and you've increased your
sharing here and now you need to concern
yourself with the contention between
them threads and the overhead of the
contention clearly there's no point in
throwing a whole heap of treads at your
your application and spending more time
with the overhead and coordination
between them treads as it would be just
to do the work you're trying to get done
in a single in a single thread model
so we go back to our ArrayList here
we've actually got the diagram
represents two threads trying to add
elements to the ArrayList both threads
are doing the same four steps we would
hope to get two elements added to our
list but if we've got two threads that
are racing together running at the same
time these steps can be interleaved and
depending on the actual interleaving the
outcome would may be different so for
example if both threads do the first
step together they will read the size
it'll have the same copy of the size
then they try to add an element it'll be
to the same index in the list d-link
romanticise where they'll both add one
did our own copy and then they may write
the size back so you expect two elements
to get added to the list but in fact in
some cases as the two threads race you
may end up with just a single element
additive lists so I guess what better
way to demonstrate that and with a
simple graphic it tends to get the point
across you know you've got your two
steam trains racing along and they
represented threads and eventually you
get an outcome that's not desired so in
some naive attempt people throw ball a
tile at the situation to try and you
know to try and fix it so in this case
if we add volatile to the size field now
we ensure that any threads accessing the
list get a consistent view of decides to
the no caching and the threads will see
no stale values so this helps to some
extent but it doesn't solve the problem
here because we still have an issue of
concurrent access to the list so what we
actually where we actually want here is
we want single single updates to list
one at a time we want sequential access
to our add method essentially
so good all synchronized well it's been
in the platform from day one but some
people still make mistakes or still get
confused as to what it's for
so I'll just briefly explain in this
slide what we're what it's about so we
still got the same essentially the same
four steps that are the body of the add
methods but now we've added two new
steps one at the beginning and one at
the end we get to lock on the list
itself every object in the in the in the
platform has an implicit lock when
you're synchronize you get a lock you do
your four steps so you read the size a
list you write the elements you
increment the size and then you write
back the sides and finally when you exit
the method you release the lock on the
object so this has given us this will
ensure we get serial access to the list
no matter how many threads try to add
elements add the one at a at a time so
the lock is not given as type of
security you may think it's given us
sequential access to the list just like
a barge through the canal lock a can
only any a single single barge can enter
at a time it can span the period of time
and a lot lock and then it will exit so
here's you can see on the lower right
hand side of the slide where if we have
a second thread at some point while the
first thread is trying to add something
to the list second thread comes along
and it tries to add something to list so
it'll actually get blocked out until the
first tread completes and then not only
at that point will it actually grab the
lock and anything can proceed to add its
elementalist
so I just want to go back there actually
so we've added synchronized to the add
method so clearly now any order public
methods or any other methods in the list
that affect state or read state of the
object or of the list I should say also
need to be synchronized so they also get
a consistent view of the list so moving
forward here we have sequential access
to the add method we have a number of
treads all trying to add elements to the
list each one is taking its turn adding
elements so this is the behavior that we
want what's not shown here is that
actually the add methods the treads can
try to add at any point but before a
previous tread completes its addition
and they just get blocked out until the
lock becomes available
so earlier we had thrown volatile at the
situation to try and fix the Deccan
science field to give us a consistent
view of the Desai's field so here I just
want to talk a little bit about if we
had left the size field volatile what
would be the impact on performance of
doing something like that so here we've
got the clear method it's also
synchronized because it's a modifying
state of the list it iterates over the
elements in the list setting each one to
know Rd indexed to none and then sets
the size to zero and if we had left size
volatile this that would actually hurt
performance of this method because each
time around the loop a decides field
will have to be reread from Kashi the
compiler doesn't know that we have
protected ourselves because we've
already synchronized on every method
that touches a red state in a list so
even even though size cannot be changed
compiler doesn't know that so it needs
to reread size every time to list and if
you've got a lot of elements in your
list is really hurt performance also the
jet can't do certain optimizations on
this because the size field needs to be
reread so it can't be hoisted out the
loop so it's just a gotcha as you're
going through we see these kind of
issues all the time where you may be
trying to update your code to make it
ready to be executed in the multitrader
environments and maybe you you're not
sure exactly what your outcome is to be
and your starter on volatile and
synchronizing randomly to try and get
things open running well it's clearly
not a good thing to do so once adding
synchronized to all the meta
declarations heard performance well yes
there is a a well no it won't hurt
performance but there's an overhead and
there's a cost to synchronization but
that costs is probably a lot less than
you think it's going to be
there's a lot of work and optimizations
been done in VMs over years to improve
the performance of synchronization so I
would guess I would say don't be afraid
to use it typically it's the right thing
to do for most applications unless
you're really really concerned about
multi-threaded performance synchronize
it's probably good enough there's some
optimizations listed here on this slide
I'm going to go through them all just to
say that typically locks are uncontained
'add and in a lot of VMS they optimize
this behavior by deferring the
instantiation of their low-level
heavyweight locking mechanisms in the vm
and maybe just putting a marker on the
object hazard indicates that a tread has
actually got the lock so that that type
of locking is very very cheap and very
fast in the VM also locks tend to be
held for very very short periods of time
in which case the VM are some VMs may
actually spin threads waiting for a lock
to become available in the expectation
that it won't take long and again the
the object here is to try and avoid the
instantiation of the hard or low-level
locking mechanisms in the operating
system so I guess the main key takeaway
I would like to say from this slide
anyway is don't be afraid of
synchronized it's typically the right
thing for most applications if you're
running in a multi-threaded environment
so with that I'm going to hand over to
Mike so so far we've been talking about
a ArrayList
which is a single data structure where
we're adding elements to it we're going
to look at another data structure now
which is a queue which is has both the
adding elements to it and also removing
elements in order so our add method is
modified a bit now it looked like the
last synchronized version that Chris
showed you except that we have a little
different behavior and
that is that the put and take operations
have slightly different strategies so in
the case of the foot operation the the
thing I care about is their in fact
space in the queue in order to add an
element on the take side we're waiting
for something to be in the queue in
order to be able to remove it
so unlike the add operation which is
essentially unbounded on ArrayList our
queue is in fact bounded so we can't
just insert an infinite number of
elements into the queue and obviously
when there's no elements in the queue
there's nothing for us to in fact take
so when those conditions occur we wait
for the when either the list is full or
the list is empty we wait for that state
to change before continuing the rest of
the operation we don't just fail and say
oh there's no room or there's there's
nothing available I just failed it's
better in this particular case to wait
for the conditions we care about to
erupt to arise now you'll notice that
one thing slightly unusual about the
waiting for the correct conditions to
arrive it's actually in a loop so rather
than being if size equals equals length
wait it's Wow
and you may wonder why that is and
that's because there in fact may be
multiple threads waiting for the
particular same condition you are and
there's no guarantee that when you're
woken up that somebody else wouldn't
have already satisfied the condition
that you had so for example we if we had
two threads both waiting for the put
operation and we didn't use a while loop
and we just used an if statement the
first one would succeed after that and
then the second one would just right
past the end of the queue because the
conditions that it assumed were true
that size was now
less than lengths are in fact not the
case so it's important that we don't
make any assumptions about the state
when we're woken up and actually recheck
our basic condition before proceeding
there's another bit in here other than
actually adding the element or removing
the element which is to notify the
notification is to the other side of the
operation in the case of put it so that
anybody who's waiting for an element to
appear in the queue can be woken up and
retrieve that element the it without the
notification the waits would just be
infinite and nobody would ever be woken
up to either use the available space in
the queue or to take elements that have
appeared in the queue and we'll talk
more a bit about notification and in a
future in future step so if we if we
look at the queue just a second here we
can see that you know things go pretty
orderly it's like the ArrayList where
threads are taking turns removing and
adding elements to the adding elements
to Hue and they work in a pretty orderly
fashion you know very much like the
ArrayList there's a little bit more
going on here in them it in the case of
the ArrayList though because in the
ArrayList case the synchronized resulted
in blocking on only one kind of thing
and that was contention multiple threads
trying to add an element to the same
ArrayList in this case we also have
blocking on starvation and this is where
we're doing the weighting in this case
we're blocked because we're waiting for
some condition to occur it's not just
that we couldn't have access to the list
we looked we got access to the list by
the you know entering the synchronized
Block in our method then we found out
that what we required wasn't in fact
available so it's not a question of
access it's a question of some other
precondition
it's the starvation case though was
entirely natural you know if you're a
consumer removing items from a queue
it's entirely possible that there are no
elements currently available
maybe the elements come in bursts for
example and that if you just wait a
little while elements will appear you
know the first case of contention is is
one that people are generally pretty
worried about if you're spending all
your time waiting on the lock and you
know you've got a thousand threads
waiting on a single lock they're going
to spend a lot of time you know just
trying to acquire that particular law in
order to do their work the other cases
starvation
may or may not be important you know in
the consumer case where there's nothing
available to process it's not
particularly interesting that the
threads spend time waiting for things to
appear if you have issues where they're
waiting for time for sorry lots of
things appear and then they spend a lot
of time waiting for it in order to
retrieve those elements and nobody's
actually doing any processing because
there's contention a lock that's far
more significant than just waiting for
something to appear in the first place
but in both cases you kind of do want to
be able to have the tools in order to
measure why you're why you're waiting on
a particular cube
so you know in this case as we mentioned
if there's nothing available for a
thread to take because the cue is in
fact empty and we'll wait until somebody
else comes along and put something into
the queue we've got two examples of that
in the latter case we see two elements
being put into the queue so whether the
the take on the far-right in the second
row happens it doesn't have to wait at
all because there's already something in
the queue when it arrives so it you know
if there's something already available
nobody's waiting at all so moving on to
you know there's an alternative way of
doing locking other than using the
synchronized which is java.util
concurrent lock and this is an explicit
lock rather than using the synchronized
key word here and we've restructured the
code for the take method a little bit in
that we now are locking on an explicit
lock object rather than allowing
synchronized to do it for us so that the
two steps we had on the original array
list of get lock and release lock are
now in fact explicit in this case there
appear as visible steps whereas they
didn't really in the synchronized method
you'll notice that we acquire the lock
right at the beginning and then we have
a little stunning tiny strange bit of
try finally lock unlock the reason we do
the try finally is is that we want to
ensure that no matter what happens
inside the code block that when we're
done the lock is released
normally the lock releasing isn't
automatic if we didn't have the try
finally and an exception was thrown
inside this method then the method would
exit without the lock being released
synchronized provides you some you know
additional guarantees and a safety that
explicit locking doesn't provide we see
you know some other things that are
slightly different rather than just
calling wait we're now we're calling
signal await we're waiting for
awaiting a particular signal and then
when we actually do something rather
than notify all we call signal all other
than that this is fairly much equivalent
to the version we had was synchronized
so we have one problem with our with our
signaling in this applet in this
application on cues is that we in fact
have two different conditions that we
care about
the first is threads that are looking to
put elements into the queue are in fact
looking for the condition space being
available threads that are looking to
remove elements from the queue are
looking for the condition that there are
elements available not everybody who's
waiting is in fact waiting for the same
condition right now when a condition
happens we have to kind of wake up
everybody in order to have them check to
see whether the particular conditions
satisfy that's the notify all our signal
all that we've got the problem with that
is is that so let's say we imagine we're
in a take routine and we signal all that
we've removed an element if the thing
that if this was just signal only a
single other waiter would be notified
however if that was a another taker he
would just look at the condition while
size equals zero which for him would
still be zero because the other guy got
the element and he would just go back to
sleep go back to awaiting however nobody
else has woken up in that particular
case so the after this after that point
the the program wouldn't make any
further progress because nobody was
being woken up the single signal was
consumed by somebody who didn't
particularly care about the condition so
if we switch to having two separate
conditions
the not full and not empty conditions we
can avoid that by having waiting only on
the conditions we particularly care
about and now we can be sure that when
we wake someone up it's somebody who
cares about that particular condition so
we can soothe modified the the take
routine again and now rather than just
waiting on a single condition we're
waiting on a particular condition of the
queue not being empty and when we in
fact remove something we signal on the
opposite condition of not full because
anybody who's waiting on the not full
condition is a put er and now he can
wake up because they're spaced in the
queue in order to be able to insert
elements other than that the routine is
fairly much the same as it was we've
added here and this could have actually
been on the previous routine or a
previous version of this code and
interrupt the exception interrupt the
exception is a kind of a system
administration exception type it doesn't
really get thrown by programs generally
I mean you can't it you can't interrupt
a thread to say something exceptional
has happened you should wake up and
figure out what's going on and probably
clean up and quit but it is not
something that is generally or shouldn't
be used for communication or
coordination among multiple threads that
should be used for overall
administration but the reason I
mentioned that though is is that the
interrupted exception in this example of
a type of exception that could be thrown
inside this block that we would in fact
be needing this particular filing Clause
to ensure that the lock was returned to
unlock state so that somebody else could
acquire it after the interrupted
exception was thrown so the opposite
side of the tank routine is the put
routine and you'll notice that we are in
fact waiting on the opposite condition
put is waiting for the cue not to be
full
and when it adds an element it actually
signals the not empty condition and we
can see here in an example that if we
start off putting a few things into the
queue some of the threads start to wait
because they are unable to acquire the
particularly this one ends up waiting
the third one here ends up waiting a lot
longer because in fact the three slots
in the queue are are full but once
somebody has come in and taken the an
element from the queue it can then
proceed the important thing another
important thing in here is and this is
more of an issue with our sorry less of
an issue with a bigger queue but our
small queue this happens all the time is
that a waiting actually only happens
when you're at the bounds so a waiting
for four putters only happens when the
queue is entirely full there's no more
space to put additional elements into
the queue and for takers they only wait
well the queues entirely empty the kind
of in-between space nobody is waiting at
all so all they have to do is acquire
the lock grab the current element and
off they go there's no waiting no
waiting involved we kind of talked about
the queue being of a fixed size in this
particular case that is that you know a
particular design choice that that you
would impose for particular applications
maybe it's appropriate to not use
unlimited amounts of memory there are
other queueing strategies and you do
need to consider that in the design of
your application and the choices that
you make really depend on what the goals
you're trying to achieve our if for
example you
have very bursty amounts of work so a
whole lot of requests come in at once
and then you have kind of an arbitrary
amount of time to process them maybe you
don't want to bounded queue if on the
other hand you want to you know control
your throughput rate and you have a you
know fixed number of resources which can
actually handle requests maybe you want
to bounded queue so that that you have
some back pressure on on things which
are producers in that they can't add
additional elements and are blocking
block from creating additional elements
and adding to them to the queue if in
fact the queue is full so it's important
to consider the trade-offs that you've
got and the particular effect you're
trying to achieve when you choose to
have a bounded or unbounded queue you
know the wrong choice you know that
general results and things like filling
up all the heap with unprocessed
requests if you had you know a thousand
producers and one consumer having an
unbounded cube probably not the right
answer because that consumer is never
going to be able to keep up so it's
definitely something you need to think
about in the design of of your
application and how you're gonna use a
particular queue
I'm bounded q in terms of what type a
linked list or right
yeah you can have typically the most
common unbounded queue would be a linked
list in that it doesn't have any
structural elements in its design that
imposes a particular bound on it
ArrayList is creative particular size
and if you need more elements you in
fact have to resize the ArrayList there
are other unbounded queues besides
linked lists but linked list is by far
the most common it's just a general a
general strategy you can in fact use
ArrayList like an unbounded queue if
you're prepared to accept the resize
that needs to occur when you go above a
certain capacity the Java ArrayList for
example doesn't impose a particular size
restriction on you there's no way you
can force the limit some of the other
queueing types in the concurrent
collections array blocking queue etc
they do enforce the particular size and
won't just grow automatically when they
get full they are in fact entirely
bounded I'm sorry I can't hear that at
all I have a cold and I'm
your soda question is is a linked list
better than an ArrayList better what
better than an ArrayList well certainly
for unbounded queues that's generally
the case for most other cases the cost
of all the additional pointers an
overhead that it'll eight that a linked
list imposes aren't generally worth it
the other advantage you'd want to use to
a linked list would be if you need in
place it you know removal from the
middle of the list at fairly low cost
but you know for other things of you
know just filling a list processing it
then throwing it away ArrayList is
probably going to be faster so a linked
list remains useful for you know
situational things most of the data
structures that are there you know in
the Java util libraries with the
possible exception of some historical
artifacts like vector and hash table
they have situational advantages that if
they're not entirely just historical
like vector or hash table you know it's
never never one-size-fits-all and that
that's part of the whole point of this
you know discussion of whether or not
you care about unbounded versus bounded
queues so these locks can be kind of
heavyweight if for example all we wanted
to do was increment size and we weren't
actually modifying the let's say it
wasn't ArrayList and all we cared about
was keeping a counter the size element
of that having these locks in order just
to surround a counter increment would
seem pretty heavyweight and it'd be nice
if there was a lighter weight
alternative particularly if all we
wanted to do most of the time was to
look at the value for size and we
weren't actually interested in modifying
it so you know to introduce yet another
thing beyond synchronized in lock
there's another alternative which is
atomic operations and these are
generally mapped to
atomic processor instructions atomic in
this case means that it's a unit that
can't be subdivided so if the operation
succeeds then you know that all elements
of that operation were successful and
the instruction that's most often used
is called atomic compare-and-swap and
what this does is that when you execute
an atomic compare-and-swap you say that
I expect the value of this variable to
be X and if it is then replace it with
this other value Y that I'm providing
you but if the value is not X when you
try to do this don't do anything just
leave it unmodified and we'll see that
you can in fact do this now this is a
method on atomic integer class increment
and get is actually provided by a atomic
integer but we're looking here at an
implementation that we're doing
ourselves so if we had size that was the
atomic integer how would we implement
incrementing the size so we we say get
the current value and then we do the
incrementation and incrementation is
happening in a local variable and here's
the compare and swap part where we say
check I that the current value of this
is whatever we read before and I wish
you to replace that with the the
proposed value that I have for next
after my local incrementation of this
variable and if this doesn't in fact
hold and this operation doesn't succeed
then we do it again until we in fact
succeed at this operation eventually
assuming that they're not you know ten
million threads all trying to do the
same operation one of the times we'll
get in and our value for current will
still be in the variable when we try and
replace it with our value for next and
then we'll
then we'll succeed and continue on in a
lot of cases where contention is low
this will in fact work the first time if
all we wanted to do was read the size
field we could just call size yet and we
wouldn't have had to incur any locking
overhead at all we'd be sure that the
value we were getting was in fact the
correct value for the size field so this
atomic allows us to increase performance
by taking advantage of the fact that
access is generally not contended making
read operations faster by not incurring
that synchronized overhead with the read
operations there's an implicit element
in the get here of that it's actually
reading the volatile value of size so
that we'll never get a state stale value
from the size variable at all and this
is important you'll see this example
here that I'm showing you in in old code
this is from before atomic integer
existed and what this was was an
optimization to improve the read
performance of a variable that you were
doing something simple long like this
size variable and what it did was it
made size volatile so that when I called
the get method it returned the then
current value of size but if in fact I
wanted to update the size field I
actually went to the trouble of getting
full synchronization so in fact we're
combining both synchronized and volatile
in this particular example the volatile
is helping out the read side ensuring
that the value that's returned is never
sail and the synchronized is happening
helping out the right side insuring that
well this case it's not terribly
important because the size field is just
a single field we're not doing anything
like adding it to an array as well but
I guess it still is important because
we're in fact incrementing we care about
the current value of size if we had no
synchronized on this at all it still
might be possible to interleave
operations on the get of size and the
increment and the right back so that we
could get to threads simultaneously
incrementing their local copy of size
and right in the back so the
synchronized actually helps out on the
right side in this case pretty
dramatically and if you care the quote
about hosted on your placed it on your
own petard is from Shakespeare Hamlet
and it says for tis sport
tis the sport to have the engineer or
hoisted on his own petard and a petard
is a bomb and it basically means you're
blown up by your own bomb so you know
we've up we've kind of gone
incrementally to from synchronized
through lock to atomic trying to reduce
the contention overhead that we have in
order to perform more more throughput
and avoiding that overhead of
concurrency but kind of the true Holy
Grail is to be able to have no
contention at all ideally what you'd
want to do is just divide up the task
whatever it is you're trying to do
between a variety of threads and have
them execute in parallel all together
you know independently of each other in
Chris's example that was doing something
like you know why try and add everything
to a single list why not have each
thread just build its own local list and
if you need at the end just gather those
those lists together but don't try and
have everybody fighting over a single
list all the way through the addition of
you know a million elements by ten
threads so you know if it's possible to
divide up our program in such a way that
we can execute most of the work in
parallel and and have little or no
contention
we're going to have higher overall
throughput you know this is kind of you
know called being on the right side of
Amdahl's law Amdahl's law says that you
know if you have n processors you're not
going to get you're going to get it best
a end time speed-up but it's almost
certainly going to be lower than n times
speed up because you're going to be
paying some coordination overhead and if
the cost of your coordination overhead
is extremely high
I mean we've kind of all seen the case
where adding more cores just actually
makes things slower and the reason is is
that you're paying a huge cost for the
coordination relative to the benefit
that you get of doing it just on a
single a single thread so what what
alternatives are there well as as as it
turns out there actually are
alternatives and those would be the
streams library that's provided in Java
8 and if you were you know a masochist
you could do it in Java 7 with the
forked rhyme framework which is you if
you have a problem that's in fact
decomposable you could take apart the
problem divided into its subunits hand
it off to the fork/join framework it
would have each of the individual
components of the problem being
completed on separate threads and then
at the end reassembled into a single
result streams library that's in java 8
makes that a heck of a lot easier and
the key thing is is that it focuses on
we only have to provide the logic for
what we want done to the data but very
little is said about what in fact or how
that operation is performed so in our
example here if we're looking for the
students with the highest grade the
parts that we're adding are the logic
that says we care about whether they're
enrolled and we care about their grade
and the operational care about is max
but we didn't really say how to divide
up the problem you know does this core
get these five students or you know
what's what's the best number of
students to give perk or you know leave
that to the framework it figures that
out for you and for something like like
max you know to find the highest grade
that's a problem which is very
parallelizable because all of the sub
answers of you know I looked at these
five hundred students and this is the
one that had the maximum grade that can
be easily combined with the results from
another thread there was no particular
sequential nature to the problem so it
is extremely parallelizable so
definitely you know if at all possible
avoid the problem you know concurrency
is sometimes necessary if you have a
problem that must have coordination or
is not parallelizable but in general it
if you can parallelize it and decompose
it you should you should look at that
for analysis of you know concurrent
situations about looking work intention
is looking where the weight is seeing
where your queues are saturated or you
know consumer threads are starved the
flight recorder tools that are available
with Oracle's JDK and other analysis
tools like them are absolutely essential
to understanding the behavior of your
program don't don't try and do it with
printf that the tools are just so much
better and so much more productive and
there's some good programming book type
resources available as well for you know
further looking into concurrency both
Brian gets this book into Uglies book
are a really quite good Doug's
is more focused on the gory details of
the implementation and why things are
implemented the way that they are
whereas brian's is as it says in
practice is more explaining how to use
the
two classes and and techniques
lastly Charlie hunts Java performance
book is is really very good at
explaining the big picture about doing
performance analysis and understanding
where your where your program
bottlenecks are so I don't know if we
have much time for questions if any yeah
we have been eight minutes for okay we
have a few minutes for questions if
anybody has any I'm probably get Chris
to repeat them for the mic because I
can't hear very well today
no questions no
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>