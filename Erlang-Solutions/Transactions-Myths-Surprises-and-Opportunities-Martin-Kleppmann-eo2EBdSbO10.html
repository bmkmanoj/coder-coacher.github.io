<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Transactions: Myths, Surprises and Opportunities - Martin Kleppmann | Coder Coacher - Coaching Coders</title><meta content="Transactions: Myths, Surprises and Opportunities - Martin Kleppmann - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Transactions: Myths, Surprises and Opportunities - Martin Kleppmann</b></h2><h5 class="post__date">2015-11-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/eo2EBdSbO10" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so back when I was thinking about titles
for this talk I was playing around with
a few different ideas and what I
eventually settled on was this
transactions myths surprises and
opportunities thinking you know they had
at least a modicum of optimism and
opportunity in it but actually my my
working title internally was more
something like transactions Joy's
challenges and misery with a subtitle of
consistency guarantees you want
consistency guarantees haha so you see
it looking at this topic has driven me a
little bit insane and so I'm going to
try and see whether I can drive all of
you insane to where the the forbidden
knowledge and in this talk anyway a lot
of fairly crazy things I'm going I've
got a lot of material so I'll have to go
through it fairly quickly but hopefully
we'll have enough time so at the moment
I'm writing this book for O'Reilly and
this talk is kind of an extract of one
of the chapters of the book so if you
find the kind of stuff interesting this
book tries to kind of lay out a lot of
underlying big ideas and principles that
you find in not just databases but other
kind of data storage and processing
systems as well it's not finished yet
but you can find an early release like a
beta release of the first eight chapters
at data intensive net so going back in
the history of transactions I like
looking back at the history of things a
bit and understanding where things came
from and if you look at the kind of
transactions you find in most relational
databases nowadays you can trace them
pretty directly back to a system called
system R that was developed by IBM in
the 1970s and a lot of the terminology
and the ideas we use for talking about
transactions really follow on fairly
directly from from the system forty
years ago and in in history like people
have been working on databases a long
time but that's basically been the
dominant model for a long time until
then in the mid 2000s or so
no sequel thing became trendy and then
of course the inevitable counter
movement of new sequel happened and but
if you think about this it's not really
about sequel like people have started
building sequel engines on top of HBase
and such like for example so it's
whether or not these systems support
sequel is you know there's a bit of a
data model question there but otherwise
it's kind of not that it is kind of
beside the point because the biggest
difference between these systems is
actually whether or not they support
transactions it's more like a movement
of new transactions versus yes two
transactions and if we're talking about
transactions we inevitably have to talk
about acid so you're probably familiar
with this acronym it stands for
atomicity consistency isolation and
durability so you think you know what it
means and what I'm going to show you in
the next half-hour is that you probably
didn't actually know what it means
because you know what does something
like isolation actually mean if you
unpack it in a little bit more detail it
turns out that actually this acronym is
one polite way to put it is it's more
mnemonic than precise which is how Eric
Brewer expressed it which is basically a
way of saying you know it's it doesn't
really have a well-defined meaning it's
just kind of a marketing term that we
use to very loosely associated systems
so I'm going to try and run through each
of the letters briefly and just expand a
little bit on what it actually means so
that we know what we're actually talking
about when we're talking about
transactions I'll start with durability
because it's kind of the easiest one so
back in the day when these earliest
transit transactional systems were being
built durability meant that the
transaction log is written to an archive
tape because discs were not that
reliable and at least with a tape you
could then if the disk crashed you could
then replay the transaction log the redo
log of this archive tape and thus
restore the contents of the database but
of course tapes fell out of fashion and
durability became known as the data as f
synched to disk so if stuff crashes then
hopefully it's still there and then more
lately this has kind of been further
interpreted and thinking well you know
discs on one machine is quite nice but
you know what that if that machine
becomes completely inaccessible due to I
don't know CPU failing and so you might
get it back again eventually but some
engineers have to go in to go into the
data center it's actually be nicer if we
just replicated the data across multiple
machines and thus if the data becomes
unavailable in one place hopefully it's
still available in another but anyway
this is all just to say durability means
that once we've written something you're
hopefully not going to lose it again
you're hopefully going to be able to get
it back again when that's correct so
that's a fairly simple one what about
the see the consistency this one is is
often misunderstood you might have heard
some of something called the cap theorem
which talks about consistency
availability and partition tolerance the
C in acid is not the same as the C in
cap they're two totally they both stand
for consistency but they have two
totally different meanings I'm not going
to go into a lot of detail on cap
personally I don't think cap is helpful
at all
I've written a whole paper about
explaining why I think it's not helpful
so you can find that on archive.org if
you are so inclined but just keep this
in mind what consistency means in the
context of acid is that the the data
went your transactions which execute on
the database will move the database from
one consistent state to another where
consistent means it satisfies certain
integrity constraints or certain
invariants like a an invariant might be
that's in an accounting system all of
the accounts debits and credits sum to
the same quantity or something like that
or it could be a uniqueness constraint
that you only have one user for a given
username in your database
the weird thing about this consistency
is that it's not actually something that
the database can guarantee by itself
it's something that the application can
guarantee by relying on certain
guarantees of the database but it's this
interplay between the application and
the database so you can't it doesn't
make sense to say that a database
provides consistency in the acid sense
it's the application using a database
that provides consistency so it's kind
of an odd one out in this acronym and
in fact people who know the evolution of
these stories have said that you know
the C was basically tossed in to make
the acronym work it was not considered
importance by the people who defined it
at the time so see we can more or less
ignore what about a the atomicity so if
you've done any kind of multi-threaded
programming for example you've probably
come across atomic operations like an
atomic increment or atomic
compare-and-swap operation or something
like that
confusingly the a in acid is not
referring to this kind of atomicity so
again we've got an overloading of terms
it's really unfortunate but in the
context of acid this atomicity is not
about concurrency it's not about what
happens if several things are happening
in parallel it's about full tolerance
it's what happens if something goes
wrong in the system for example
something crashes so the typical
scenario would be you have two things
that need to be updated together because
they kind of Express the same thing in
some sense or they are they are closely
linked to each other so I like to say
you've got some kind of auction website
where somebody can buy a product and
then when somebody's bought something
you need to say that the buyer of this
item is the person who bought it and you
also need to send an invoice to the
person who bought it or something like
that and now what happens if these are
two separate rights and the first one
succeeds and the second one fails
because something crashes or someone
unplugs the wrong network cable or
whatever something goes wrong and now
you've kind of made half the changes
that you wanted to make and once the
system comes back up again it's going to
be in an inconsistent state unless you
have some way of saying these two
modifications these two rights have to
happen as one unit they're one atomic
thing and so I want them either to both
succeed or to fail and if it fails I can
retry again and so that's really what
transactions provide is this atomicity
across multiple objects that is
atomicity on a single object like a
single row in a table a single document
in a document database that's basically
simple so and any any storage engine
will provide that but a key with
transactions is that you can kind of
wrap a little books around several
rights and say these all belong to one
atomic unit so this is what the a in
acid is talking about and a key here is
that if if something goes wrong during
the execution and the transaction has to
abort for any reason any partial rights
that happened there will be rolled back
or undone or discard it in some way so
really a better word than atomicity
would have perhaps been abort ability
because the important thing that makes
these transactions work is the fact that
they can be aborted and if they are
aborted then it's as if nothing happened
now abort are super helpful because they
collapse a whole range of different
error classes into a single thing that
you can handle so it doesn't matter
whether the transaction is abort is
because the application server crashed
or because the database server crashed
or because somebody unplugged a power
cable or someone unplugged a network
cable or the switch crashed or something
else went wrong in the network or you
got a deadlock between different
transactions or some integrity
constraint was violated all of these
things just get collapsed down to a
single thing which is the transaction
abort
so the whole point of this transaction
atomicity is really a feature to make
life easier for application developers
and that is you don't have to worry
about handling all of these different
error scenarios in the application
because they've been collapsed down to a
single thing and response if the
transaction abort is well you can just
safely retry it okay so so much about
atomicity we said that that's about full
tolerance but there is still the
concurrency issue and I skipped over the
IE earlier the idea isolation that's
where we're talking about concurrency
and this is where it gets started
getting really interesting so if you
look at the database textbook talking
about I said it will say that isolation
means serializability and the idea of
serialize ability is that each
transaction can pretend that it has the
entire database to itself that there's
no other concurrent
executing transactions on the database
or put another way the result of
executing all of the transactions is the
same as if they had executed one after
another without any concurrency in a
serial order and still named
serializability now if you actually look
at what the eight abases do in practice
we've got some support serializability
but they're also these various odd other
what I call isolation levels there's
things like we'd committed and
repeatable read for example and these
things a little bit hard to understand I
just like to I'm just curious is there
anyone here who thinks they can explain
off the top of their head what's the
difference between read committed and
repeatable read now one person so the
challenge with these things is this is a
configurable property of your database
transactions and if you've written an
application that relies on transactions
you have to understand what guarantees
you can rely on from the database and
which you can't so if you don't
understand the difference between these
isolation levels how on earth are you
supposed to write applications that are
correct and don't have race conditions
I'm going to show a few examples of how
do you things can actually be quite
subtle and so in practice what happens
is that these various databases have
various different isolation levels and
the default is often quite weak so read
committed is the default in about half
of the different database that was
surveyed here and like a whole bunch of
these databases don't even support
serializable isolation so that you're
forever stuck at some of the the weak
levels this is a very nice survey by
Peter Bayliss and some of his
collaborators which you can look at and
be depressed if you want anyway so I'd
like to give a bit of a flavor of what
the difference is between these
isolation levels are because I think
it's actually quite interesting and it's
it's good to understand this stuff so
I'll start with we'd committed which
sounds like you know that's what we want
of other transaction we only want to
read committed data that sounds like a
reasonable thing and the way read
committed is defined is that
there are two anomalies that are not
allowed to happen so an anomaly in
database speak is like a race condition
it's some kind of weird behavior that
would not be possible under a serial
execution so something that we're kind
of this this idea that each day each
transaction has the entire database to
itself where that idea becomes violated
and these two things that read committed
prevents are dirty reads and dirty
writes the dirty reads is what read
committed says in the name that is a
dirty read would be reading some data
that has not yet been committed so you
read some transaction rights some data
but it's not yet been committed and
another transaction reads it so
preventing dirty reads means you only
ever read committed data and that sounds
quite reasonable okay and dirty writes
maybe needs a diagram to explain so
let's have a diagram so imagine you've
got two clients a and B and they are
writing to the same database and there
are two objects like two rows or two
documents or something in that database
and a wants to set both objects to a and
B wants to set both objects to B and
they do this concurrently and so if you
look at X at first get set to a by a and
then get set to B and the final value is
B whereas if you look at the object Y it
actually gets the ordering the other way
around so it first gets set to B then
gets set to a and the final value is a
and now your two things have gone
inconsistent so you know this in the
auction website example you could end up
with an invoice being sent to one user
and the buyer being actually set to the
other user and so those those people
will not be particularly happy but write
read committed prevents this from
happening and so that's already good
news
so most most databases that have some
kind of concept of transactions will
prevent this dirty writes and typically
the way they do it is that it just take
a look so in this case the first
transaction writing to a certain item
will take a lock and keep that lock
until the transaction commits or abort
okay so I think you'd be forgiven for
thinking that
you've preventing dirty writes and that
he reads you know that's kind of and
you've got atomicity that's kind of what
transactions are right so what all of
these other weird isolation levels here
what I'm going to try and show is some
examples of rate conditions or anomalies
that can occur at we'd committed that
are prevented at some of the stronger
isolation levels and I'm quite as
obvious so take this one read skew here
this red skew differentiates between
read committed and repeatable read or
snapshot isolation and and so this is a
a problem that can occur under read
committed so the scenario is this say
you've got a bank and a user wants to
transfer hundred pounds from account X
to account Y and so what they have to do
for that is to decrement account X 500
pounds and increment account Y by a
hundred pounds and so the sum of all of
the accounts stays the same they've just
moved money around and now this is all
fine but now imagine what happens if
concurrently you have some kind of
process which just reads the entire
database and maybe that's like a backup
process that is just taking a copy or
maybe it's some kind of analytics
process or like periodic checking
integrity of the database and this is
going to first read the account Y and
it's going to get back 500 and then it's
going to read the account X and it gets
400 because by the time it gets round to
reading X that transaction which
transfers the money has already
committed so this is read committed the
data the change has been committed by
the time X is read but you've got this
weird inconsistency where you know
you're supposed to have a sum of a
thousand pounds in in both accounts in
total but actually this orange process
sees a sum of 900 pounds so it seems
like 100 pounds have just evaporated
from the point of view of this this
orange process so that's kind of bad
news and this is what repeatable read
and snapshot isolation were were
invented to prevent and so these are two
these are kind of the most confusing
isolation levels because the naming gets
very inconsistent depending on which
database you look at so the the
traditional implementation of repeatable
read in system our style 1975 you can
still find in sequel server nowadays for
example so if you use repeatable region
sequel server you get basically the same
as what system are did back then but
most databases actually do in snapshot
isolation rather than this taking blocks
the idea with this is that each
transaction gets a kind of consistent
snapshot that it can read from and for
that it doesn't need to take a copy of
the entire database it uses some clever
multi versioning on the individual rows
in the database but logically it seems
to do transaction as though it's
executing at one point in time so even
if subsequently that data gets changed
by other transactions this any
long-running read-only transaction will
only see the data at some point in time
and so like postcards called this this
repeatable read my sequel does it almost
right but not quite
Oracle actually calls this serializable
just to be confusing sequel server calls
this snapshot anyway so it gets kind of
inconsistent but the high-level
principle remains fairly similar across
these different databases okay but we're
still not at the top yet we're still not
yet at serializable so what is the
difference between something like
repeatable read and snapshot isolation
and higher-up serializable so here's
another example this our example is
called write skew and the scenario is as
follows imagine this is a dispatch
service for ambulances and you want to
make sure that you have at least one
doctor on call always all the time and
so doctors can trade their shifts like
it could be that at some point a
doctor's not feeling well themselves so
they want to ask one of their colleagues
to take over and that's a right
invariant you want to preserve here is
that at all times at least one doctor is
on call and so this is the transaction
that gets executed when a doctor clicks
on the web interface saying hey I'm not
feeling well cancel
take my shift please transaction is as
follows it first counts how many doctors
are currently on-call and if that number
is greater or equal to two then it's
okay for the current doctor like
currently one doctor one two three to go
off call so it then updates the doctor's
table and you know this seems like it
should be safe because in a serial
execution of these things you would make
sure that like one transaction executes
decrementing the number of doctors to
say one and then if a second doctor also
tries to to further decremented to zero
they won't be allowed to do so because
it's realizing oh there's only one
doctor you're the last doctor on call so
you're not allowed to go off call you
have to negotiate it some other way but
in something like repeatable read or
snapshot isolation this actually turns
out not to be safe so you could have a
scenario like the following where okay
you've got three doctors Alice Bob and
Carol and currently Alice and Bob are on
call and Alice and Bob are both
currently feeling unwell so Alice goes
into door into the system and says hey
can I go off call please and the system
says yes sure they're currently two
doctors on call so it's safe for you to
go off call and then concurrently Bob
does the same thing and Bob also sees
there are two doctors currently on call
so it's safe for you to go off call
so each of these individually updates
their own row in the database and note
that these are two different rows so
they don't conflict with each other
there's no lock or something like that
to prevent this and now all well both
transactions six successfully execute
and commit and now you've got no doctors
on call and so this is exactly the
situation that we wanted to prevent so
the pattern here is called right scheme
and there are several different ways how
it can occur but kind of one way
thinking about it is the following
pattern that you've got a transaction
which first reads some data and then
makes a decision based on the result of
what it is read like for example other
enough doctors on call and then it
writes something to the database as a
result of that decision and different
transactions
right there result two different places
in the database so those rights are not
going to conflict with each other but
the right may change the thing that was
initially read and so it could happen
that by the time the transaction commits
the thing that it originally red which
is the premise of the decision that it
made the thing that it originally read
may no longer be up to date and this is
the problem of of right skew so it could
be that the decision was made on a false
premise basically and serializable
transactions are supposed to prevent
that which then leads us to the problem
of well you know if all of these issues
these race conditions can occur why
don't why doesn't everybody just use
serializable transactions and problem is
that they're kind of expensive what
these historically they've been kind of
expensive because well since system R
came up with serializable transactions
the the one way of implementing them has
been two-phase locking so this algorithm
says that basically whenever you read or
write anything you have to take a lock
on it and you have to keep that lock
until your transaction commits or abort
and so this is a fairly pessimistic
concurrency control scheme in the in
this doctors example what it would mean
is say you've got this query which which
looks at all of the doctors on call
status you would now have to take a
shared lock on all of these doctors that
you've read so you could imagine in a
large query this is potentially taking a
shared lock on the entire database
effectively and the shared lock means
that other transactions can still keep
reading that data that's right but
nobody can write it nobody can modify
that data so essentially what you're
doing with two-phase locking is you are
blocking the entire database for writes
whenever you're making any large read
and well disk has kind of bad
performance implications and like if
you've got even just like a single
person running a backer query or some
analytics query it's it's not very kind
to stop writing or stop all writes to
the database for long
time and that was really the motivation
of introducing all of these week
isolation levels like we'd committed and
so on in the first place it's because
these locks were simply too expensive
and people wanted to get out and in fact
the definition of these isolation levels
was actually well the system our team
back in 1975 or so they just decided to
not hold the locks for so long they just
decided okay we're going to change our
long locks to short blocks that are only
held for a brief moment and then
released again that is why does in those
different isolation levels are so hard
to understand it because they're not
defined in any sort of sensible
semantics that is designed for humans to
understand that design they are defined
in terms of the implementation details
of a 1970s database that's why so
fortunately these things don't stand
still well they they kind of stood still
for about thirty years forty years even
but but eventually people did figure out
better ways of implementing serializable
transactions and kind of two different
approaches came along at almost the same
time one is implemented in an academic
prototype called H store which is
commercially now known as vault dB
a different one is called C erisa Balad
isolation which is implemented in
Postgres and foundation DB for example
so I'll run through each of those
briefly this is the first of all the H
store approach which is actually
remarkably simple and really people
should have been doing this stuff
earlier and that is if you want your
transactions to be serializable well
just literally execute them in a serial
order and if they are literally executed
one after another then no concurrency
exists like you've trivially
single-threaded the entire problem and
as long as you can make your
transactions fast enough it's actually
quite viable to take all of those
concurrently executing stuff and just
stick them one after another however
there are a few practical things if you
want to make this work this is only
going to be feasible if each transaction
individually is super super fast to
execute because otherwise your
throughput is just going to be a Bismil
so in order to make every transaction as
fast to execute you have to kind of
change the model a bit one thing that
you need to
pretty much as to keep everything in
memory because going going to disk and
waiting for disk to come back while like
you've got a queue of other transaction
it's waiting to execute serially that's
just going to be too slow you can do
things like aborting a transaction if it
has to read from disk and then we start
it again once the data has been loaded
into memory things like that you can do
and you also need to run everything a
stored procedures pretty much so there's
no way you're going to wait for Network
hop back and forth in some like ec2
crappy data center for like several
milliseconds
while this transaction is executing
again that would just drive your
throughput
down to zero so you need to be able to
simply NQ a transaction and say do that
for me please and the database has to be
able to execute that entire transaction
from beginning to end very quickly and
you have to partition the workload so
that you can actually make use of more
than one CPU core and more than one
machine and so on but this is a
reasonably viable way of building
systems so that's one way of achieving
serialize ability the other way is to
detect conflicts and abort transactions
if necessary if conflicts occur which is
kind of the optimistic counterpart to
the pessimistic approach of of two-phase
locking so this number three approaches
known as serializable snapshot isolation
and so the idea only came up in like
2008 or so but actually the
implementation looks kind of similar to
two-phase locking
you've also got these locks that you
take but with one crucial difference is
that the locks don't block anybody they
just gather information so that is in
this case where you're querying the on
call state of all of the doctors yeah in
some sense taking a lock on all of the
doctors but that block doesn't prevent
anyone from writing and from updating
those doctors it only helps the
transaction gather the information that
oh there's somebody who read this data
about which doctors are on call and
somebody else has modified this data
that who is on call
and so you can now collect these bits of
information about which transaction has
read and written what and then when a
transaction comes to commit you can do
an analysis on those this graph of of
different leads and writes and figure
out whether any conflicts have occurred
and there's some really clever
algorithms which if you're interested in
this stuff I encourage you to look at
the papers are quite readable anyway if
the if there's no commits no you
conflict then the transaction is allowed
to commit so this is basically the
overview of new ways of implementing
serialize ability I think they're quite
interesting so Postgres does this kind
of thing on a single node and so
Postgres is serializable transactions
actually have rather good performance
the only distributed implementation that
I'm aware of has been in foundation DB
but they unfortunately got both battle
and now the product isn't available
anymore which is a great shame so if if
someone wants to implement SSI in a
distributed way I think that would be an
interesting thing to do so that's me not
overview of these various difference
isolation levels most of what I've
talked about has been in the context of
a single database like a single
application running on a single database
but modern systems are unfortunately
more complex than that
so with modern systems you often have
various different services which maybe
each have their own database you might
be wanting to use several different
databases across several different
services people use buzzwords like micro
services I've been working on stream
processing systems which have many
similar characteristics where you're
trying to kind of break a large big
monolithic thing down into more
manageable chunks and platters breaking
down is that you've now got data sitting
across different databases so on a high
level these systems tend to look
something like this you've got some kind
of service which has some code in it and
probably got its database co-located
with that service and if one service
wants to communicate with another it's
going to make some kind of RPC calls or
some kind of rest requests to other
services and if you draw the same
diagram for stream processing it looks
base
the same except the arrows are streams
of events rather than RPC calls and
unrest but a key idea here is that in
this kind of architecture you kind of
draw a box around each of these services
and say okay we may have transactions
within one of these services but we're
not going to even try to have
transactions across different services
so we kind of put a boundary around this
so I was kind of intrigued by this and
thought well you know would it be
possible would it even make sense would
it be sensible to have some kind of
transaction cross concept across
multiple services of course I wasn't the
first to think of this people have been
thinking about this for a long time in
fact atomic commit protocols
dude this kind of distributed
transactions so two-phase commit is
probably the most well known so I do
naming is really confusing this is
two-phase commit earlier was two talking
about two-phase locking the names are
very similar but they're two completely
different things but anyway so this is
the idea that you can have some kind of
transaction manager which coordinates
across multiple services and multiple
databases whether a transaction gets
committed or aborted and people have
built these things but you know they're
actually not particularly and the
performance is pretty terrible
and so you know this doesn't seem
particularly great interestingly there's
just as a strong connection here to the
problem of totally ordering things so I
said with serializable execution you
want the system to behave the same as if
things happened one after another in a
total order well you can put things in a
total order and our systems which will
in a distributed system put events in a
total order but they are equivalent to
consensus systems so the zookeeper for
example provides a total ordering of
events and it does that through a
consensus algorithm all of these three
things are expensive and that means
every time you want to write something
you've got some kind of coordination
happening over the network sometimes
their fault tolerance of these things is
not great and what's more they have a
real
failure amplification problem and that
is if you want to transactionally update
data in several different services and
one of those services is going bad for
some reason like it's running slow or
it's currently offline or you deployed a
buggy version of that service or
whatever if you're trying to atomically
commit across multiple different
services that one for tea service is
going to bring down the entire rest of
the system which is exactly what you
wanted to avoid in the context of
flitting things across multiple services
you wanted to isolate the failures you
wanted them to not spread across the
entire system so if you introduced some
kind of distributed transactions here it
seems like you inevitably bring back
that failure amplification and it gets
even worse if you've got your systems
geographically distributed around the
world because now in order to do any
kind of atomic transaction commit they
are going to have to synchronously
coordinate over these really slow wide
area links so you know basically it
seems like distributed transactions are
not going to work particularly well but
well what kind of situation do we end up
in then what do we do instead and I've
looked at various things that people
have said about building micro services
and I've talked to various people
building these things and I've been
involved in building some of them myself
and the general idea I seem to get is
well you know if you can't guarantee any
sort of like Garrett any sort of atomic
commit across multiple services you have
to do things like compensating
transactions and that is you run the
transaction you do a thing and then if
later turns out that actually some other
part of the system failed then you run
another transaction they're saying
actually can you undo whatever it was
there whatever the effects were of that
initial transaction which is somewhat
like the abort the rollback that you get
with kind of traditional transaction
atomicity except that you're
implementing it yourself in application
code and that's certainly one way of of
doing things and you know maybe that's
really what we have to do but they're
still tricky part
that like how do you figure out exactly
what you need to undo and what has
happened if some other people have made
subsequent decisions based on the state
that you've now all rolled back and so
on it's you have to think about it very
hard as an application developer and
another thing that I've seen suggested
is that well if you can't guarantee
integrity constraints and consistency
guarantees like maybe you simply need to
introduce apologies that is for example
you've got a an online store and you
accidentally sell more items than you
have in stock and you know what you do
in that case well somebody's simply not
going to get the product that they
ordered so you sent them an email saying
hey sorry we thought it was in stock but
actually it's out of stock we'll refund
it for you and maybe give you a voucher
for five pounds off your next order or
something and businesses do do this all
the time
but if you think about it what this
apology is is basically a way of
recovering from constraint from
violating some integrity constraint so
your integrity constraints was you don't
sell more items than you have in the
warehouse but if you accidentally do
well you can apologize afterwards now
thinking about this the compensations
actions that looks a bit like atomicity
like the a in acid and those apologies
they kind of look like the Sein acid in
that they're a way of enforcing
invariable invariants about your data
after the fact admittedly but still
their way of getting some kind of
consistency back in your database which
leads me to an assertion you see we've
got here a and C and you could imagine
some kind of isolation and durability
coming up here as well it just leads me
to assert that every sufficiently large
deployment of microservices contains an
ad hoc informally specified bug grid and
slow implementation of half of
transactions apologies for this I have
you know no evidence to actually back
this up but I'm just going to boldly
assert this but you know there's still
problems here and that is we saw with
our discussion of the
from weak isolation levels that weird
things can happen like you can end up
with having zero doctors on call even
though you thought you had more than
that on call and so I'll run through one
final example of issues that can happen
in a kind of micro-services world where
maybe we want some kind of transaction
isolation and that is imagine you have
some kind of social network like thing
and scenario is that one of your users
broke up with their significant other
with their boyfriend or girlfriend and
so they unfriend their their ex
on your service and then they write a
nasty post to all of their remaining
friends saying how how nasty this person
is and the intention of the user is of
course that the unfriend happened first
and thus this nasty message doesn't get
sent to their ex but in a microservices
environment you've probably got like the
friends service handling the unfriend
requests and you've got some kind of
post service handling the fact that some
update was posted and now what if you
have a third service that is some kind
of notification service that sends out
push notifications or emails or
something like that based on events that
are happening in the system so you know
you could end up in a situation where
the unfriend event is delayed somewhere
in the system like maybe it's just a
slow Network or something can very
easily happen and so the post to friends
gets there first and it scatters email
notifications and push notifications to
all of the friends and only then it gets
the unfriend event and then it goes Oh
oops
I shouldn't have sent a notification to
that user so what we've got here is a
causal dependency in the end coming into
the system that is from the users point
of view the they expect the effect of
the unfriend event to take the unfriend
event to take effect first and then the
message post thing to take place but
this this ordering dependency is not
being captured in the rest of the system
and somehow that has been lost and
leads to violating the users
expectations so this leads to some
interesting area of research happening
right now and that is there are things
in between the kind of eventual
consistency no guarantees whatsoever and
the very strong serializable we're going
to do everything very strictly in order
guarantees and that is there are models
of causal consistency for example which
try to capture these kind of
dependencies like that the unfriend
should happen before the message posting
and the interesting thing is that
causality can actually be implemented
without this global coordination so it
doesn't require you to go back to the
world of two-phase commit and consensus
on every single right but I'll still
overheads to it so you have to track a
certain amount of metadata and like open
research questions around how
efficiently it can be done but causality
I think is going to be a very
interesting area to explore here in
future because if you think about what a
consistent snapshot means in a database
for example in snapshot isolation
well that consistent means that it's
consistent with causality that is you
don't end up with some state of the
universe where things that happen later
appear to happen earlier and things get
kind of reordered in weird ways so
causality is really the key to
understanding this and so in this whole
world of different isolation levels
there's actually a reasonable amount of
isolation that can be implemented
without consensus and two-phase commit
and distributed transactions and the
border is somewhere roughly here so
there's a kind of isolation level that's
just slightly weaker than snapshot
isolation which can include causality
and which can be implemented without
coordination so far only academic
prototypes of this stuff exists I do
hope that this these ideas make it out
into practice because I think they're
really going to help build applications
that are better more reliable that don't
have weird bugs if you interested more
in this stuff there's a whole long list
of reading this is page one of my
excessively long references list which
extends for page two and page three and
Pedro as well but you know everyone says
read
and here are some these are all
excellent papers which are all worth
reading so that's if you want to deep
dive into this stuff
there's there loads of excellent ideas
in these things if you want a kind of
predigested version of it I will
actually be giving out free copies of my
book so Riley has kindly printed off a
couple of copies of this for me to give
away so even though it's not finished
yet this is like the first eight
chapters in in printed form with a big
thing on the front saying it's not
finished yet
so at 12:30 backers just write up there
I'll be giving away signed copies if
you're interested or otherwise there's a
discount code as well but yes this is
dragged on for a fair amount of time but
hopefully they have been some
interesting ideas in there maybe I have
a little bit of time for questions as
well we do great
and yes questions okay the issues that
you mentioned in the for the
microservices the ad-hoc
implementation of transactions do they
applied to replicated databases as well
yes there's a closest link there so a
lot of problems that you would need to
solve in in having transactions across
multiple services would be the same as a
replicated database has to solve
internally
the main difference there being that if
it's a single data based and developers
of that database can implement the
algorithms within the kind of little
closed Garden in a sense of that one
database so they can do more interesting
things whereas if you want to have real
distributed transactions across
different services you have to kind of
agree on some kind of standard and so
like two-phase commit is basically the
lowest common denominator that people
have managed to agree on even though
it's actually a pretty bad algorithm but
anything like anything more powerful
than that has just proved unfeasible for
different vendors to actually agree on
yes how do you think we ended up with
this mess with this confusion with
naming and every vendor making whatever
they want of those terms of and those
definitions a few different effects I
think so one thing that I find really
strange but seems to be true is that on
the academic side the database community
and the distributed systems community
have historically been entirely separate
and basically not talk to each other
which means they've each evolved their
own kind of jargon with with the same
word meaning different things in these
different fields and nowadays that seems
bizarre because almost any interesting
database is distributed but but like all
really for decades they've evolved
separately I think we actually wouldn't
have many of their problems now had base
we talk to each other more in the 80s
but anyway hindsight is such another
effect is kind of well meaning standards
so like the sequel standard for example
prescribe these different isolation
levels like read committed repeatable
read and so on and try to describe them
kind of in sort of general terms that
would not be tied to any particular
implementation
unfortunately they actually failed at
that goal of describing them in a
neutral way and so as a way then like
for example postgrads wanted to
implement repeatable reads like things
but they wanted to actually use natural
isolation because it performs much
better than locking repeatable read
recorded repeatable read anyway because
that's what the sequel standard says and
it looks kind of similar enough that it
just about satisfies the standard and so
even though it's terribly confusing you
kind of ended up there where the same
word means different things in different
contexts so it's a lot of it is kind of
historical accident and hopefully we'll
be able to dig ourselves out of that
hole eventually yes exactly there are 20
computing standards let's create a new
one
hi so certain databases have introduced
the concept of time with every table or
whatever and thinking of them really
Keys database to come under them yes
does that have any effect on
transactions or is it purely a nice to
have for time traveling and
replayability and stuff like that
there's a bit of a connection there to
some extent it's an application-level
feature so you can do queries of our
past snapshot of the database which is a
useful feature to have in in some
circumstances there's a cost associated
to it because like your garbage
collection of old data is more
constrained so the link there I see is
kind of two snapshot isolation so with
snapshot isolation you've kind of
implicitly got a timestamp at which
you're querying the entire database
though that timestamp it's not normally
exposed to applications explicitly and
you can't ask for a transaction at some
past point in time unless that has been
kind of marked as like an I forgot some
databases have this concept at one
transaction can join another transaction
snapshot so there that kind of starts
going in that direction I think there's
some overlap but it's not exactly the
same thing as well
so I I can't imagine that for the
serializable model there are some
theoretical tools to argue about
situations but are there or at least is
there hope to have like some some sort
of theoretical tools like logic where I
don't know who ever to argue about the
other kind of transaction about the the
weaker transaction based models
yesterdays there's some theoretical work
surprisingly little actually I guess the
main reference there would be at all
ideas PhD thesis the first reference
here so you I think it was kind of the
first to actually build up a proper
formal model of what weak isolation
levels actually mean and then that's
other related things to dis are
Berenson's critique of number six here
they kind of introduced snapshot
isolation and have a bits of formal
model around it as well and then Peter
Bailey's and his folks which would be I
think it's the vldb paper number five
those those three papers are probably
the the most interesting in that regard
so a little thing that I did was a
project I call Hermitage or ellemeet ash
or however you want to pronounce it
which is on github it's just a small
thing where I basically took these
papers that build up a formal model of
what the different isolation levels mean
and turned it into a tests test the
suite of tests that I then ran on four
different data bases or five and just to
test like what kind of things would
actually happen it's really interesting
you get this this big matrix of
different check boxes saying like what
kind of race conditions what kind of
anomalies can happen on which database
under which isolation levels and that
work is just directly transcribing the
formal model that they've set up into
code thank you
hi I'm not really in through this
transactional stuff but is this
serializable snapshot isolation the idea
behind SDM software transactional memory
I don't know a whole lot about STM so
I'm not really the right person to
comment there so my understanding is
that STM tends to be within a single
process for multi-threaded communication
though I don't actually know how those
things are implemented nowadays
well the this locks that detects changes
so it seems kind of what what it is okay
yeah yeah said that that could well be
overlaps there someone else would
probably be much better place to comment
on it though I think so Phil says they
tend to be MVCC so that would probably
be similar to like post-crisis it's not
for the isolation type thing okay thanks
okay is it time for lunch okay alright
thank you for coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>