<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>David Turner - Some History of Functional Programming Languages - Code Mesh 2017 | Coder Coacher - Coaching Coders</title><meta content="David Turner - Some History of Functional Programming Languages - Code Mesh 2017 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>David Turner - Some History of Functional Programming Languages - Code Mesh 2017</b></h2><h5 class="post__date">2018-02-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/SbgMY0ap4p0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">right good morning everyone I'm going to
talk about about 60 years of history
going through right the emergence from
okay starting in 1936 with the lambda
calculus which is the universal
programming language invented before
computers existed and going through a
series of stages up to Haskell
what I want to show is the stages in the
development of purely functional
higher-order polymorphically typed lazy
languages of which Haskell is the
best-known example I don't I won't claim
to cover everything that happened in
between these two dates but some
significant things that had a strong
influence on me or or I had a hand in
let's start with the lambda calculus
right here we are the lambda calculus
which of which the first complete and
correct definition was in 1936 by Alonzo
Church is a tightness theory of pure
functions type lists because you can
apply anything to anything there's
there's no type discipline there pure
function constructively speaking is a
rule or method that converts an input
value into an output value and in a
theory of pure functions the input value
in the output value or both themselves
going to be functions and that sounds
kind of incestuous because there isn't
any data but you can make functional
representations of data so for example
the number two is a function that
it's a function and an argument and
applies the function twice to the
argument and the number three applies
the function three times and so on I'm
sure most of you will have seen all
these tricks true can be represented as
a function of two arguments that returns
the first one false as a function of two
arguments that returns the second one
and so on so you get functional
representations of data there are just
three rules in the lambda calculus alpha
the first rule which is alpha conversion
says that you can make systematic change
of bound variable and here note that I'm
using the substitution operation square
brackets Y for X in E which guy still
was talking about yesterday so what that
means is change all free occurrences of
X to Y in E and that's called alpha
conversion and in fact you there are a
couple of things you have to be careful
of first of all that you must only
replace free instances of X by Y and
secondly if Y itself is an expression
which might have free variables in if
any of those free variables are kerb
ound in e then you have to do alpha
conversion so you actually need our for
conversion to define this square bracket
slash operation so the whole thing is a
bit incestuous and it's quite easy to
get wrong and the earliest published
papers are at the lambda calculus the
ones before 1936 had error errors in to
do with variable capture but in the way
assuming you've got that right there's
alpha conversion which says that the
choice of bound variable names doesn't
matter rule beta says that you can apply
a function to an argument and what you
do is you substitute the argument the
actual parameter into the function body
so you have a 4x in
be so you need the substitution operator
to define beta reduction and each a
reduction is a kind of tidying up role
it says that if you have lambda X dot DX
where he doesn't contain any X's well
that's just the same as the function e
an instance of the left-hand side of the
be saw or Ito rule is called a red X and
performing those reductions in the
direction shown by the arrow if you keep
doing that you will get to a normal form
if there is one a normal form being a
lambda term that has no read X's in as I
mentioned already there are functional
representations of natural numbers and
other data and there are three really
important theorems about the lambda
calculus the church-rosser theorem says
if you start with some lambda term a and
let's say it will have multiple red X's
in it and you reduce it through one
method to get B but hitting on some
other read X's you can also reduce it to
B prime then there is a point of
convergence there's going to be a lambda
term C such which you can reach from
both B and B prime so you never get into
a blind out there you can always get
back onto the same path and that tells
us that when if normal forms exist they
must be unique that's well for
conversion
the second church-rosser theorem tells
us how to find the normal form if there
is one you keep reducing the leftmost
red X there are actually two ways of
defining the leftmost red X because you
can have one red X inside another you
might mean leftmost outermost or you
might mean leftmost innermost and it
doesn't matter which of those you choose
it's still a normalizing reduction
strategy you will reach the normal form
if there is one and the third important
fact about lambda calculus is bones
there I'm discovered by Corrado bone if
a and B are lambda terms with distinct
beta eaten normal forms then there is a
context C such that applying C to a will
yield true and applying C to B will
yield force force sorry lambda XY dot X
and lambda XY dot Y a true and false our
functions are carried here lambda X Y
dot X means lambda X dot lambda Y dot X
okay so that says if two terms have
distinct normal forms you can separate
them strongly you can do a conditional
branch on them and what an immediate
consequence of that is that alpha beta e
- conversion is the strongest possible
equational theory on normalizing terms
so it's not just a theory of pure
function of type list pure functions the
lambda calculus is the theory of
typeless pure functions if you added a
single extra rule between a single extra
conversion between normalizing terms the
whole theory would become inconsistent
because you'd be making true equal to
false okay so from the second
church-rosser theorem we can look we can
deduce something which is that in
general to find the normal form to be
sure of finding the normal form if there
is one you must substitute actual
parameters into function bodies
unevaluated which is what we now call
lazy evaluation where as Minh efficiency
disadvantage because if you copy the
actual parameter into the function body
and the formal parameter occurs in
several places you're going to get
several copies of the actual parameter
so you're going to do unnecessary work
so when people started implementing
functional languages they actually chose
to use call-by-value that is to evaluate
the actual parameter before you pass it
in but it's an incorrect reduction
strategy for lambda calculus because you
may fail to find the normal form when
there is one nevertheless from 1960
everyone did it that way the thesis of
Chris wodsworth who was a PhD student of
Christopher stretchy he and I were in
the same room but I was a year younger
than him he showed that you could
overcome the efficiency disadvantages of
normal order reduction by doing some
graphs instead of on trees so then what
you substitute in for what you replace
the occurrences of the four parameter by
is a pointer to a shared instance of the
actual parameter and that way you only
reduce it once I took that idea in a few
years later did it on Combinator's
combinational logic and lambda calculus
evolved together both in the late 20s
and 1930s combinatory logic you have a
small number of constant functions like
s and K and I and you can show that any
lambda expression can be translated into
an SK molecule by a molecule I mean it's
something that's just built with
application so you can translate
backwards and forwards between lambda
calculus and combinators combinators
have the advantage that they have much
simpler reduction rules you don't have
to worry about searching for bound and
free variables and doing alpha
conversion all that stuff so I showed in
1979 that you could compile a lambda
calculus based language which was called
sass all to SK combinators do graph
reduction on the combinators and then
you got a reasonably what it was an
interpreter not a compiler but
reasonably efficient as interpreters go
implementation of lazy evaluation and
then Thomas Johnson and then at
augustson went a step further instead of
using fixed
combinators like SK and I they extracted
program specific combinators from the
high-level language from the program
text by a process called lambda lifting
and then you can compile the reduction
rules for those program specific
combinators and this is the way to go
if you want to do lazy evaluation on
stock hardware whereas my method would
have worked if you wanted to build
special purpose hardware that did SK
reduction and some people tried that
arthur norman at cambridge built an sk
reduction machine Burroughs corporation
like I consulted for in the 1980s built
an SK reduction machine but Thomas
yongsan and Leonard Augustine's method
is better if you want to compile for
stock hardware for Intel chips or
whatever and that was further developed
by Simon Paton Jones into the spineless
tagless G machine which underlies the
Glasgow Haskell compiler GHC so over a
period of how many years is that I don't
know about 25 years lazy evaluation was
implemented more and more efficiently
until it became practical in production
use okay so now going back into our
timeline we started with the lambda
calculus in nineteen six in 1936 the
first functional language that had a
significant user community was Lisp
developed by John McCarthy at MIT
starting in 1958 his famous paper in
1960 is called something like symbolic
let's just find it yeah I ought to
remember that recursive functions of
symbolic expressions and their
computation by Machine very famous paper
of 1960 right unless you have symbolic
data which consists of words you know
just like Fred or David or it could be a
number so atoms and you combine things
by pairing so you can build pairs of
pairs of pairs of words so you can get
arbitrary lists trees graphs and so on
you can do anything you want with with
just atoms and pairing and these things
are called s expressions s expressions
are a variable size and they can outlive
the procedure or function that creates
them had a very farsighted decision of
John McCarthy in 1960 was not to burden
the programmer with allocate and dispose
instructions he wanted even though it
was a complicated thing to do at the
time he wanted the the Pro the runtime
system to automatically figure out when
data was no longer required and and
garbage collected so probably the most
important single thing in Lisp I would
say was the invention of the garbage
collector right so you had the s
language which is just made from atoms
and pairing and then the M language was
a first-order functional language for
manipulating s expressions it has comps
car and cutter for building and
decomposing pairs
it has tests conditional expressions and
recursion and that's all and this is
computationally complete McCarthy showed
that you can code an arbitrary flowchart
as mutually recursive functions ok
now M language programs could themselves
be coded as s expressions and then you
have a function called eval which
interprets the s the s expression which
represents an M language program so you
have meta programming using eval unquote
and Macarthy if you read his paper he
was clearly of the view that it didn't
matter that you didn't have higher-order
functions because you could get the same
effect by using eval unquote by doing
meza programming which is actually a
slightly different thing from
higher-order functions right some myths
about Lisp pure lists never existed
this had assignment and go-to before it
had conditional expressions and
recursion it started as a dialect of
Fortran - - which McCarthy added the
things that he needed for programming
with symbolic expressions second myth to
be disposed of this was not based on the
lambda calculus despite its use of the
word lambda to denote functions
McCarthy's inspiration was clean his
work on first order recursion equations
the M language was first order the first
list manual I encountered this was in
the early 70s was the Lisp 1.5 manual
which I think was the first widely
distributed list and I believe you can
still get that historic manual 5 manual
and if you look at the syntax of the M
language in it you will see that its
first order you can't pass a function as
a parameter or return it as a result but
you could pass a function as parameter
by quotation by passing in the S
expression for its code however this
gives you the wrong binding rules for
free variables you get dynamic binding
instead of lexicographic if a function
has a free variable say Y in F equals
lambda X dot X plus y Y should be bound
to the value in scope for Y in the
surrounding Tec
not in the place from which F is called
and it wasn't until scheme came along in
1975 that we got versions of Lisp that
did that correctly that had static
binding rather than dynamic binding
nowadays all versions of lists by lambda
calculus based right okay on the other
side of the Atlantic also in 1960 Algol
60 was being defined the inter
international algorithmic language which
began in 1958 and reached an important
fixed point with the publication of the
Algol 60 report the Algol 60 report is a
masterpiece of precise technical writing
written in English rather than in maths
but it's it's a wonderful example of of
the use of natural language for precise
description and in particular right
Algol 60 allows nested textually nested
procedures so you could have one
procedure to find in inside another
procedure and so on to any depth you can
pass in procedures as parameters but not
return procedures as results and the
point about that discipline is that you
can still use a set a stack it's only if
you can return procedures as results
that you can't do it all on the stack so
one of the design aims I assume of Algol
60 was to be able to do things on a
stack another very interesting thing
about Algol 60 is if you look at the
parameter passing rule the default rule
was called by name which means you
actually copy the text of the form of
the actual parameter into the procedure
at E at each place and the and Peter
now's description of how you do this
covers exactly the cases of variable
clashes and the use of alpha conversion
that that you need for the lambda
calculus whether he'd actually studied
the lambda car
or not I don't know but Algol 60 in its
parameter passing mechanism was much
closer to the lambda calculus than this
call my name was the default and it and
it's got the rules right to avoid
accidental variable capture so it had
Lex with graphic binding Randall and
Russell 1964 the book the implementation
of Algol 60 which describes their
adventures in what I think was the first
complete implementation of Algol 60 used
two sets of links on the stack the
dynamic chain linked each stack frame to
the stack frame from which it was called
but the static chain linked each stack
frame to the textually containing
function call which might be much
further away on the stack and you acted
so the dynamic chain tells you how to
return to your caller the static chain
tells you where to find the bindings of
your free variables so that was an
important step forward and then later
the static chain got optimized to
something called the display which was a
little vector with all your free
variables in okay now the next step was
made by peter land end supposing we go
one step further than Algol 60 and say
I'm not only going to allow functions to
be passed in as input I'm going to allow
functions to be returned as results then
you have the problem that a free
variable might be held onto after the
function call in which it was created
has returned so it won't be on no longer
be present on the stack and Landon
solved this problem in his sec D machine
by taking these things called the
display which Randall and Russell had
invented and putting them in the heat so
a function is now represented by a
closure
which consists of a pointer to the code
together with a little vector of the
free variables and closures live in the
heat and the secd machine was a very
general mechanism for implementing any
kind of programming language Landon
Landon wrote a series of papers in the
early sixties describing the
relationship between lambda calculus and
programming languages the first one in
1964 was called the mechanical
evaluation of expressions hadn't
introduced the secd machine sec d stands
for stack environment controlled dump
the next what not the next paper
actually the two papers later in 1966
the next 700 programming languages
describes an idealized language
effeminate family or perhaps I should
say language template because you could
choose what basic what constants and
basic operators you wanted to have so
you might have numbers and arithmetic
operators if you're doing numerical
analysis or atoms and comps car and CUDA
if you are doing symbolic computation
you could plug in whatever you wanted
for the basic constants and operators
but around that was this apparatus that
was universal
I swim stood for if you see what I mean
and it was in three layers the first
layer which Landon called church without
lambda was a sugar a sugaring of the
lambda calculus and with a few key words
Lautrec and and where you can do
anything you can do in the lambda
calculus without actually using the word
lambda without actually using the lambda
symbol so you can say expression where f
of X equals staff instead of lambda X
dot staff applied to expression and he
also introduced a nice syntactic idea of
indentation to indeed
a block structure which has been copied
in a number of languages including
Haskell gets rid of all the clumsy begin
ends that you had in algal in the next
layer landon added reference musical
variables and assignment or references
and assignment and in the third layer he
added a very powerful jump operator the
J operator which allowed a program to
capture its own continuation but it
wasn't as simple as call CC you have to
read the paper it was called 1965 a
generalization of jumps and labels so in
1965 Landon had already arrived at the
idea of course of capturing your own
continuation via this J operator and you
can do any kind of weird control
structure including backtracking using
the J operator okay so I swim was a
sugared lambda calculus class assignment
plus a generalized control operator and
also in the I swim paper the next 700
programming languages paper was the
first appearance of algebraic type
definitions which landon introduced in
words was a few years later that burst
fuller invented a notation for them at
the end of the 700 languages next seven
hundred languages paper there is a
discussion among the audience which is
printed at the end of the paper it was
in C ACM and in which Christopher
Strachey raises the idea of doing all
your programming in just the inner layer
just the first layer of I swim
that is what straight he called a DL a
purely descriptive or denotational
language and he speculated that you
might actually be able to write all your
programs without either assignments or
jumps which is indeed true as we all
know now right Iceland was was an
unemployment it concept it actually two
things got implemented that were based
on it
Pawel at MIT by our sevens and GERD
Duncan at Argonne National Laboratory by
John Reynolds and they're both very
similar I got to know Powell because
when I started as a PhD student in 1968
Joe stoy who was working with
Christopher stretchy gave me a tape from
MIT which had the the Powell interpreter
on it so that was my introduction to
programming languages was figuring out
what was going on inside Powell and the
garbage collector was the hardest thing
to understand it was a compacting
garbage collector okay so I had all
these funny variables in it called s II
C and D and I wondered what they were
for a bit until I read London's paper
okay so Powell was in three layers
applicative Powell was sugared lambda
calculus and it had shallow
pattern-matching you could say that kind
of thing let X Y Z equal expression so
that would take apart a three list and
give names to the parts imperative
Powell which was the next layer that was
El pal adds mutable variables and
assignment and then J pal had his first
class labels and art Evans had the idea
instead of using the J operator you're
just allowed to set labels L :
anywhere inside a function and then you
could pass out the label and jump to it
later which means you could jump back
into a procedure invocation that had
already executed which is a way to do
backtracking so you could do all kinds
of amazing control structures with first
class labels and the PhD project that I
was originally given was to find an
efficient way of implementing powell
including these weird jumps and well i
never found one because it means you
can't have a stack at all
everything has to live in the heap
because your stack frame can't be thrown
away when you exit it because someone
might jump back into it okay
Powell was typeless in the sense of like
list has run time type checking first
class labels allowed unusual control
structures co-routines for example the
standard example where you need
co-routines is you've got two trees of
different shapes and they have leaves
and you want to go round the fringe of
the tree and see if they have the same
leaves in the same order but they're
different shapes and you can do that
with co-routines
because you have two different recursive
functions two different invocations of a
recursive function each recursing over
one of the trees and they're talking to
each other whether with a resume you
know interrupts and resume and an
example where you need backtracking is
writing a general-purpose parser and one
of the things on the palate a from MIT
was a five line package for parsing an
arbitrary BNF which used first-class
labels to do backtracking okay so when I
I
I never finished that PhD which was
implementing pawel efficiently and on
the basis of a promise that I was about
to finish my PhD which wasn't quite true
I got offered a job at Andrews as a
lecturer and I gave a course on
programming language theory using all
the stuff that I'd learned from strategy
and Scott in my first term and during
that course I invented a simple
denotational language which was
basically the applicative subset of
powell just as a blackboard notation to
show them what you could do with a
sugared lambda calculus and my colleague
Tony Davey surprised me by implementing
it over the weekend he implemented this
in this
so then we had to give it a name and I
called it Santander is static language
and I reimplemented it in BCPL so as we
could get better error messages because
it's hard to debug a program if the
error messages are coming from a
different language so Sasol was the
applicative subset of powell with two
changes I introduced multi-level pattern
matching so well it will see an example
in in a minute
and whereas in powell string had been a
primitive datatype I decided to make
strings just lists of characters which
meant that all the this processing
apparatus you have Const car could be
said I called them I called it calms
head and tail and map and so on could be
used on strings so it unifies string
processing less processing so those were
the two ideas I had that were
improvements so sasol was originally so
it dates from about January 1973 it was
a call-by-value language it was
dynamically typed it had let and rec no
explicit lambda I which makes it easier
to teach if you don't have any explicit
lambdas curried functions and with a
lambda calculus like notation left
association the left association rule
for function application the data types
were integers truth values characters
lists and functions all data had the
same right so first-class functions and
I used it for teaching functional
programming instead of less and its
advantages over list for teaching FB
were first of all that it was a pure
sugaring of lambda calculus with no
imperative features and no avowal quote
distractions it had the correct scope
rules for free variables which in those
days list didn't have and it had
multi-level pattern matching which is a
big plus for readability I can just show
you this example
so we want to we've got a pair of pair
look at the let expression I've got a
pair of pairs so it's pair of pair a B
and pair C D and I want to rearrange it
in the way that you see so I say let
ABCD equal X that names the parts and
then put the match together in a
different order and the equivalent
equivalent Lisp code is above and you
can see it's much more opaque I cheated
slightly because you don't have to say
car of car of X you can say R of X with
two A's and so on but even so it's much
less readable than the pattern matching
so multi-level pattern matching was a
big step forward and I don't know if
sattell was the first language to have
it but it was certainly an early
language to have it
Sasol like lisp had runtime typing why
was this this van similar languages were
used for computation over symbolic data
so they had to write functions that
worked on lists trees graphs and so on
and this leads to a need for structural
polymorphism if you are recursing say
you're reversing a list you don't need
to know what the type of data is and at
that time the only way to handle this
was to delay type checking into a
runtime polymorphic typing hadn't come
along yet so here's a nice example of
what you can do with dynamic typing
that's very hard to do in a statically
typed language so I've got a a curried
function of an unknown number of boolean
arguments that returns a boolean result
okay and I want to know if it's a
tautology that is is it true is its
truth table true in in all the rows okay
so you can test that in this very simple
way you don't know what its arity is
it's a curried function of unknown arity
so the first thing you do is test if
it's a boolean
a boolean function of zero arity is just
going to be true or false as a curried
function so if it's a boolean then
that's the answer
it's a tautology if and only if it's
true whereas if it's a function then
what you've got to do is apply it one
layer to true and false and check that
both of those functions which have era
c1 less are themselves to autologous so
2 to 9 tautology tester it's very
difficult to do in a strongly typed
language at this time I'll show you how
we can try and do it in Haskell safley
evolved gently during the 1970s
I dropped rec &amp;amp; recursion became the
default mutual so everything was in
everything else is scoped an idea which
went from assassin into Miranda and then
to Haskell
so you don't have to worry about you can
recurse arbitrarily without announcing
that you're doing so in 1976 a still
underwent a small revolution it became
lazy and in fact three papers appeared
in 1976 about lazy evaluation there was
constant evaluate its arguments by Dan
Friedman who's sitting here in the front
row and David wise there was a lazy
evaluator by Henderson and Morris and
there was my lazy SAS or manual also
1976 so it seemed that lazy evaluation
was an idea whose time had come
I I got it from the lambda calculus I
decided to use normal order something
equivalent to normal the reduction and
at the same time I introduced I already
have pattern matching
I used pattern matching now not only for
naming the parts of a complex structure
but also to define functions by case
analysis with different patterns on the
left hand side which is which idea I got
from John Darlington
i implemented lazy SAS all using the
lazy version of the secd machine which I
got from a book by by wolf bird called
recursive programming techniques I don't
know if that books still in print
it was very inefficient they lazy secd
machine and so when I got to Kent in
1977 I reimplemented lazy sass all by
translating to SK combinators and doing
combination graphic reduction and also
during the early 80s I added
floating-point numbers and list
comprehensions why why laziness first of
all for consistency with churches lambda
calculus the second church-rosser
theorem says that if we want to be sure
of finding the normal form we need to
substitute in actual parameters our unev
alyou ated it's also blaze evaluation is
also better for equational reasoning you
can substitute equals 4 equals without
worrying about non termination and it
allows you to write programs using
infinite lists because the infinite
lists actually come into being piecemeal
as you explore them and that gives you a
way to do interactive i/o and lazy
evaluation renders I I came to realize
it rendered the exotic control
structures that I'd played with in in
powell unnecessary for example you don't
need co-routines because you can solve
the equal fringes problem by just
creating the fringes of both trees as
lists and comparing them element by
element but because of lazy evaluation
the recursive exploration of the trees
will just happen piecemeal and only go
as far as you need until you find the
first point at which they differ so
later the infinite lists get rid of them
and the laziness gets rid of the need
for co-routines and the list of
successes method
replaces backtracking there's a famous
paper by a Phil wobbler of 1985
replacing a failure by a list of
successes which has a very elegant
series of examples showing this is a
general method for getting rid of
backtracking now I actually had an
example in my 1976 manual which was the
8th the solution to the 8 Queens problem
using the list of successes method and
some level I clearly understood that it
was a general method because that
becomes apparent in in in my KOC paper
but I didn't have a name for it and it
may be giving a name to things enables
people to see their their generality and
to use them rights at all got exported
to a quite a few sites oh I didn't know
if I can make this smaller those are all
the places that were running SAS all
this would be in the mid eighties this
was the combinators implementation of
sass all those were all the universities
and there were three companies so 24
educational sites 3 commercial there may
have been other ones that I didn't know
about those ones I knew about so I think
Sasha was almost certainly the first
lazy language that acquired a
significant user can be a first purely
functional lazy language that are quite
a significant user community though how
much time have I got 15 ok fine right
going back to the 1970s again stuff was
happening in Edinburgh in his paper on
structural recursion these slides will
be in the proceeding somewhere people
will be able to get at them weren't they
yeah through the website all the
references are at the end on the last
couple of slides ok so there's a paper
of burst from 1969 in which he
introduces a formal notation for
algebraic type definitions something
like that ok there's a type 3 which
as an atom which is an ill tree and a
node something a constructor node which
takes an atom into trees and returns a
tree so there you've got a notation for
declaring algebraic data types and
bursal introduces case expressions and
structure and structural recursion for
proving things about algebraic data
types structural induction I mean
structural induction John Darlington who
was a PhD student of the Burstall
decided to replace the case expressions
by multi clause pattern matching like
that example which is the Fibonacci
function and that was the idea that I
borrowed for 1976 ass'll interestingly
NPR which stood for new programming
language John Duncan's language also had
set expressions you could say this kind
of thing settable even X is the set of
all n such that n is in X and even n get
the idea they were set so I assumed
duplicates got removed they weren't
particularly efficient m but the idea of
NPO was that you could write down a
specification in this very high-level
language using set expressions and then
do transformations to turn it into an a
recursive algorithm and there's a couple
of papers by Bertrand Arlington in the
1970s so new programming language was
first order strongly typed purely
functional and call-by-value NP a new
programming language evolved into hope
1980 which was higher order strongly
typed polymorphic purely functional and
it kept multi equation pattern matching
but dropped the set expressions so hope
was an important landmark the first
purely functional polymorphic higher
order language
also at Edinburgh in the 1970s ml was
being developed by Robin Milner and his
colleagues as the meta language of
Edinburgh LCF that was logic for
computable functions was was a
computer-based logic for reasoning about
domain theory and and international
semantics and ml was the meta language
in which who wrote tactics for
constructing proofs and ml had it was a
sugared lambda with lambda let and
Lautrec plus references and assignment
it had types was strongly typed with
type sums type products and type
recursion and it had polymorphism as the
most important thing about it
Robin Milner invented a polymorphic type
discipline so that you could do static
typing on things that were polymorphic
o-over structure as a very important
step ml was a call-by-value language it
didn't do pattern matching you you had
explicit selectors but you know go left
go right and so on and then from that
standard ml evolved by the union of the
hope stream and the ml stream and ml
became an important standard it's a
higher-order strongly typed polymorphic
new type functional language but it's
not pure it has references and
exceptions okay meanwhile back at Kent I
developed a miniaturized version of sass
all for teaching which was an experiment
I wrote though it was an interpreter
which I wrote around 1979 to 1980 and it
was used for teaching up until about
1986 it was a version of sass or without
where it just had top level function
definitions which is actually quite an
interesting challenge to program without
any
variables and I I dropped it after a
couple of years and put the we're back
in you really do need local variables
there's an interest it was an
interesting disciplined programming
without chaos II drops conditional
expressions because the idea was it was
going to be a sort of functional basic
so it was line oriented and had a built
in line editor and so I replaced the
conditional expressions by guarded
equations something like like this
example okay
guard so will now be familiar from
Haskell except Haskell puts the guards
on the left-hand side of equals instead
of on the right okay
combining pattern matching with guards
gives a significant gain in expressive
power KRC also had list comprehensions I
took over John Darlington's
set expressions turned them into lists
and the lists were lazy and list
comprehensions with lazy lists are very
powerful okay and actually there is a
website there Cousy Lang org which has
some papers and the resurrected software
a colleague of mine translated my
ancient BCPL into see the software is a
bit wobbly across the garbage collector
sometimes falls over but it works well
enough that you can see what was in
chaos II and what you could do with it
if anyone wants to look at that Miranda
was sattell with the where chaos II with
the where's put back in plus a
polymorphic type system which I lifted
from the original ml standard ml hadn't
appeared yet
Miranda was developed in the early 80 to
86 combining dance with we're raised as
a puzzle about the scope rules and the
where Clause has to govern a whole right
hand side with all the alternatives
rather than just one expression okay
so going back to the yeah if you if you
want if that was a more complicated set
of expressions on the right hand side
there and you wanted to have a local
variable the where would cover all of
the guarded right the whole set of right
hand side so and the way that that's
come out in Haskell is that you have
less expressions and we're definitions
so in Miranda I'm entered the where
definitions okay another thing that had
to happen once I put these things
together
was you needed a lexical distinction
between functions and constructors so
that you could distinguish pattern
matching from function definition so the
idea I hit on which was to was to use
initial capital letters for the
Constructors a simple idea but it caught
on
Haskell does exactly the same so node X
Y was staff is a pattern match you're
naming you giving names x and y to parts
of staff whereas lower case node XY
equals stuff is a function definition
okay so Myranda it still it still exists
it's lazy purely functional has list
comprehensions its polymorphic with type
inference and optional type
specifications then there's a 1986 paper
which was originally in sigplan notices
for december 1986 an overview of Miranda
it was first released in 1985 with
subsequent releases in 1987 1989 it was
quite ly quite widely adopted but it was
an interpreter not a compiler and it got
gradually replaced by Haskell because
Haskell had a compiler ran on far more
things and was faster also Haskell was
open source whereas you actually had to
buy
a license for Miranda which obviously
impeded its bread okay there's a website
Miranda dot org dot uk'
where there are papers and again the
software is there it only runs under
UNIX but that includes cygwin so you you
can if you want to play with Miranda you
can find it there okay so now let's jump
to haskell that has many similarities to
Miranda the most immediately noticeable
changes are are to lexical changes the
the guards are switched to the left-hand
side of the equation and that's logical
because the guards modify the patterns
you introduce variables in your pattern
match and then you can have several
tests on those variables so that's
logical and the other important lexical
change is I had this clumsy star
notation which I got from the original
ml for polymorphic types so in Miranda
the type of Mac was written like that
doesn't look too bad with two type
variables but if you've got something
with five type variables in it gets
awfully clumsy fill Wadler had the very
simple idea of using lowercase letters
and then you have uppercase words for
type constants uppercase words for type
constructors more generally and
lowercase words for type variables very
simple idea but I think it's the nicest
single thing in Haskell is the
introduction of single letter variables
for type there was instead of all these
wretched stars there's more important
things in Haskell than that
Haskell has more or less everything that
was in Miranda with the same or similar
syntax in most cases but it added very
important new features type classes and
melodic i/o being the two most important
features and also a module system with
two-level name
I'll say a little bit more about tight
class in a moment Haskell has a much
richer syntax so whereas my languages
have all tended to be rather minimal I
only give you one way of doing each
thing Haskell is more generous you have
let and where you have conditional
equations and case and so on yeah you
get several ways of doing everything
okay so the most important change in
Haskell the most important thing that
Haskell introduced that as far as I know
no one had done before is type classes
type classes are extraordinarily
powerful but also add greatly to the
complexity of the type system so you
know either like them or you don't I
whether I like it or I don't depends on
what I'm trying to do if I'm trying to
do something that needs type classes
then I do it in Haskell I've got one
example to show you how can I have got
time for one example oh I'll do it okay
you remember the tautology program in
SAS or well here it is in Haskell using
type classes this was my first attempt
there you go
it's a you have to introduce a type
class okay if you stare at that for a
bit you can see it's got two instance
declarations which are the two branches
of the conditional it actually doesn't
work I tried programming that up and I
got error messages where I've where it
says problem here you can't actually do
that kind of instance declaration in in
standard Haskell you can have arrow as
an instance but not some particular form
of arrow who thought I mean it's got the
instance the instances have to be
distinguished by their top-level type
constructor and not buy stuff lower down
as well in GHC there is there's a flag
for everything and there and there's a
flag called what's it called general
generalized instances or
think I remember what it's called which
would enable this to run but if you want
to do it in standard Haskell you have to
write that and here is my case for
typeless languages that's what it looks
like in sastra so typist languages
haven't gone out of fashion Lisp is
still here
Erlang is typeless and that example
shows you why some people like it okay
I'll stop there to give time for
questions
so we don't have time for a lot of
questions but I'll take at least one I
think it might be too early in the
morning for that now to what extent did
the prophecy of the next 700 languages
come true said it again to what extent
did the prophecy of those next 700
languages come true yeah it didn't
I guess because if it had come true all
programming languages would be sugared
lambda calculus plus references and
exceptions and I don't you know I don't
think you could say that it did come
true well it came true in the form of
Haskell so there were three or four
programming languages that came out of I
swim not 700 thank you I always think
you did 700 languages on your own though
few of them anyway time for the break
before we do got a question are we gonna
do this with one more question what do
you project for the future of functional
languages what remains to be done Oh
dependent types has got to happen I mean
well it is happening
I think dependent types is the next
major step to have a language in general
use that that has dependent types
because that way you can you can have
proofs specifications and proofs in the
program text and also you can do a lot
with dependent times as addresses is
mainstream and production ready now
you heard it here first all right thank
you so much dr. Sona</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>