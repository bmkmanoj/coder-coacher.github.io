<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Working with HTTP/2 in Elixir - Peter Saxton - Elixir.LDN 2017 | Coder Coacher - Coaching Coders</title><meta content="Working with HTTP/2 in Elixir - Peter Saxton - Elixir.LDN 2017 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Working with HTTP/2 in Elixir - Peter Saxton - Elixir.LDN 2017</b></h2><h5 class="post__date">2017-08-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/zqzkrUVfv-k" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I want to talk to you today about
http/2 and elixir the source for this is
available and some of the slides so if
I'm going too slow when you're getting
bored you can you can find spoilers
there so I want to talk about this slide
little bits there on Tuesday lunchtime
this was the only slide I had and I
asked her after work could I take a bit
of time to finish off my slides and wrap
up a project and they handily said yes
however this was still at the time I
thought I was doing a five minute talk
so and so I need to say thanks to work
for letting me have essentially a 24
hour panic slide session and some of
these may be as much of a surprise to me
as they are to you so let's go back so
the UM this is the introduction to the
ATT b2 spec it's from 2015 which
actually surprised me how long ago it is
when I sort starting this project
there's two words in here which I want
to focus on so it's um this
specification describes an optimized
expression of the semantics of HTTP that
we know and love second bit on your own
but um so there's two things here so
it's optimized and it has the same
semantics so semantics the meaning of a
word or phrase the whole purpose of this
project originally from Google and
speedy and sort of later as a standard
was to not change this picture so you
have a client you have a server client
makes a request client gets a reply very
useful easy to understand horrendously
inefficient in certain cases if you make
a new connection every time you want
something and you want a hundred things
which is common for a new website today
yeah it's very slow so the goals of HTTP
two were to keep the same request
response model so this means no new
methods no new headers URLs at the same
meaning and essentially there's no
change in its use case for the
application
mostly they got these goals I would
argue that no change in use of the
application is probably not true and
really quite a lot of places but we'll
get onto those later
so the things they've added to each to
be to to make it up
mais de binary protocol had a
compression I won't read this less
because we'll see a bunch of them later
on okay so here's a here's a comparison
of the two this is a talk about HTTP too
so I mean you can guess what's going to
happen the one on the right is 8 2 B 2 1
on the left two days to get 1 if I hit
play on this eyes actually this is over
but there we go
conclusive proof that it is always
faster to use HTTP 2 - ok so I just want
to ask a few questions of people in the
room today sort of before my talk
obviously who thinks that there's no
value on HTTP 2 for what they're doing
in their work who would put their hands
up for that so that's oh this one ok so
there's a there's no use there at all so
who would like to use 8 to be - who
would put themselves in that bracket ok
I know maybe 1/3 to 1/2 of people and
then who's currently using it ok so we
there's a good number of quiet people
because that's only a smattering of
hands so can it be used today so it's 2
years after the specification browser
support is pretty good there's reasons
why sort of Opera Mini doesn't support
it you know they're much less sort of
capable phones so for all the mainstream
browsers so chrome Chrome for Android
Safari they all have it so this sort of
sub green on ie and Safari
is because they only support it with
secure connections so with HTTP s which
is fine because you should you should be
doing that anyway so you can consider
that total support if you're sending
sterlington serving secure web sites ok
so on the server side so how do we do
this
from Erlang or elixir so all three of
these are Erlang service cowboy you
probably know it's the one that sort of
behind most Phoenix applications there's
two others lucid in chatterbox but when
I last look at them there were they were
very low activity so we can essentially
discount them from this list Joe is
focusing on cowboy
so cowboy 2.0 I knew was meant to
support HTTP
- when they looked into it a bit more I
discovered it it had been sort of a long
time in the works for how its would be
best to say this so the first
pre-release was actually before the
specification was finalized to be fair I
don't know if that was 2.0 for something
else and then just a few months ago they
got the first release candidate but
there's still work to be done and there
were good reasons why Cowboys struggling
with this it's trying to sort of work
with backwards compatibility it's got a
lot of established user base but after
seeing this I decided just write my own
reinvent the wheel
it's a classic classic strategy which
pays off every time so so I want to talk
about a bit about just building ace ace
is a project which I started it was a it
was originally just an HTTP 1 server it
got a little bit of use in a couple of
places but I wanted to upgrade it so to
make the job easier
I felt so first things I didn't support
anything to do with HTTP 1 it's you can
use cowboy for that you can wait like we
needed to sort of curtail what we were
doing somewhat so that was one thing
which made it easier and I also wanted
to focus on making it efficient for
streaming so in HTTP 2 you can still
model the whole request coming in and
getting sent as a whole response but
like the benefits come from sort of
being really sort of comfortable with
streaming so I wanted out to make a sort
of first-class citizen of the interface
so yesterday was busy if you see down
here I've got a new release of ACE just
pushed it this morning
this means that what I'm talking about
is not a lie because quite a lot changed
and there's the last two versions around
the interface so that's good so it is it
is usable you can you can have a play
with it and I would encourage you to do
so so how does it look like so in eighth
this should look familiar you build
requests so this is a get request which
has no body this is a post request it
does this is a response so with your
status your headers and the body and if
you don't have a body it's false but I
said I wanted to focus on making
streaming easy so the thing that the
interface I've ended up with is requests
and responses can have a last key which
is the body and the body can just be
true which just
fun mentally says it's the same request
that you're used to but the body is
coming later so if you're streaming all
you need to do is put the body as true
or false so and obvi and no content
response has no body so complete
responses are bonds which have the
response body to be false or ones with
the binary so that is one assumption if
you do give the binary body it's assumed
to be the whole body and it allows you
to fall straight back to a nice simple
request/response simple map of function
so then using these client sides so the
client-side code looks like the
following you need to start a link to
what you don't need you need to start a
process to manage a connection to an
endpoint within that you then start a
stream and you can then send your
request send the data over that stream
and again there's helpers to compact
this if you're sending such a small body
you wouldn't you wouldn't need to stream
it again so I said there's a you know
there's no change of the application
layer these things do start appearing
the fact that you're using streams
eventually show up like if you if you
use a simple client that was to make a
connection to the server every time then
you would have lots of HTTP to
connections to the server each managing
one stream and that would not be any
benefit on the server side so on the
server they get the the analog of the
messages it's say the processes have to
be started up so they would respond to
the messages so they're stateful so any
process can be a server it just has to
have a start link function so this is
what's expected from ACE and when you
receive a message you get at the same
format so you've got a request you've
got a stream identifier so you know how
to send it back so when you get a
request to get endpoint here you can
just do service end response and then
anything else we can do server sends
response and this is a 404 in both cases
I've actually used a complete response
but the streaming API is exactly the
same so it's service end data service
end trailers etc to start a server you
give it a way to start workers and then
the configuration arguments so this this
module needs to have a start link and
for every single new service that every
single new client Lee connects there'll
be a new worker to process its requests
okay so this pebble is to remind me to
take it take it easy to be calm I have
no idea how long this talk was going to
take so okay that seems that seems about
right
moving on so the implementation so how
do we implement these these things in
elixir how are these requests in
response which I hope you'll agree look
reasonably similar to api's you've used
before how do they get between the
client and server quicker so the first
thing is it's a binary protocol
so finally protocols they're easier to
pass you don't have any of this rubbish
about new line characters they're more
compact they're less error-prone I think
that's probably a case-by-case basis but
certain problems don't show up so you
can't have a request splitting attack
where so that's an attack where a
request body looks like the beginning of
a new one and some passes I get confused
and they and they start doing the wrong
thing with it it's definitely harder to
debug because if you print if you stick
in an i/o inspect as we all do as Joe
said I mean it's gibberish means nothing
to you working with binaries and elixir
is really easy so we all have well I
love pattern matching so this is how you
pattern match to a string you can have a
fixed front section and then an
extensible tail so this will pattern
match and then when we print town we say
London pattern matching with binaries
and lexico is really quite a long way
beyond this so and this is how you write
binaries that cannot be represented as
characters so there's not strings so in
this case the syntax here says for each
row of 8-bits encode this number in that
number of bits so this looks like we
have one well there will be seven zeros
and a one and then another seven zeros
and a one and if we try to pattern match
a value which takes 16 bits there'll be
one 256 bit and one one bit so we get
the total value 257 we can then use this
this sort of matching to do some really
quite interesting things so in this
match here the first bit is 8 bits which
are a length so we have 8-bit encoding
of a length in the same match we then
use that length variable to say that we
want a part of the binary which is
exactly as long as what the first 8 bits
specified it would be and then we have
the rest so that's just a tail so if we
Pat and match this here where we say we
want to and elixir we get the first two
characters of elixir and then the rest
and if we try to match this one where we
have
eight and elixir there's no match so
this this expression cannot be fulfilled
with what what we're passing to it so
this is this is really powerful this
allows you to write work to work with
binary protocols incredibly efficiency
efficiently in elixir so the unit of H
to be to a frame so everything is
shipped in a frame and the format of a
frame is you get 24 bits for the length
8 bits for the type another 8 bits for
flags this reserved bit stream
identifier frame payload and the match
for this insides the code is basically
the same thing this is one pattern match
where we say take the length take all
the remaining pieces we want a payload
which is the same length as the length
and we also want our max length to be
less than the length in a guard so
that's a single at the end of this we
have a deconstructed frame that was
everything we need and just to
completely wrap off this function I have
the failure cases as well so when the
length is greater than max length that's
an error and then if it does not match
we return the whole buffer wait for some
more to be pulled off the socket and try
again so how we work with binary
protocols and elixir so the next
important thing for performance of HTTP
2 is the fact that we can have multiple
streams so that picture we show right at
the beginning its modified I told you I
wouldn't know the order of these it's
modified to to this one where there's a
single connection this is one of the
abstractions which works very well the
clownin server they do exactly the same
thing they just queue up things and
they're put in as the connection
managers as the connection thinks it
would be most sensible to send them
their stuff around like how it packs
them and how it chooses to but
fundamentally it doesn't matter like it
gets it it gets it to the other side so
your streams are largely depended the
Reaper and the this means that if
there's no more if there's no more
content to be sent on response 1 the
server can just get on with the job of
sending response to so if there's a lot
of background work to be done generating
a page it just it just does it and the
only other assets are sent down so it
already provides a large amount of
improvement performance improvements
that are in HTTP 2
so multiplex dreams they reduce the
round trip and they solve head-of-line
blocking which is the thing that if one
request is not complete you can send the
other ones there's still head-of-line
blocking at the TCP level so when they
say solve it that's as air quotes I
think is appropriate for that because
there's just a different head of line
blocking blocking problem but it is
still an improvement his a he's just a
sort of diagram of of what that would
look like so if you wait sequentially to
request things obviously takes a long
time the latency being that these arrows
drop there were solutions towards this
in HTTP 1.1 so there doesn't look a huge
difference between these two which is
actually what I'm saying HTTP do gives
you that's because in this diagram I
think it says yes so the red one is a
large resource and the small one is a
small resource so an HTTP one if that
large resource it was much larger than
the pink one say for example you've got
a thumbnail and like a video clip then
there would be a big difference between
left and right so this pink one arrives
before the red one more than so this
pink one arrives before it would hear
more than the cost it takes to delay the
large file so this can be a big
improvement this diagram doesn't really
show it off and then even further one of
the features which we'll talk about
later is this push promise which allows
for even more okay so each of these
connections are isolated this is a
really interesting article by Joe
Armstrong he talks about making a server
which manages 2 million users and the
takeaway is you don't write a server
that manages 2 million connections they
all get their own process so you write
an airline OTP application that is
running 2 million servers and each
server each client gets one of those
servers and therefore that server is
much simpler to write and then you can
fall back to the power of OTP and
supervisors for the rest of it so you've
you've isolated each user which really
shouldn't need to worry about each other
this is yeah this is basically just a
sort of a picture of that so this is
what the servers look like and then in
ace we have these process called
governors and they're just there to make
sure the population of waiting servers
remains constant so we can always take
new connections you wouldn't want to
spin up too many servers and have them
idle and you wouldn't want to let that
pool grow
with http/2 this got more complicated
because within each connection every
stream stream being a combination of a
request and a response they should also
be isolated so it's now a system where
every single client ends up with its own
process to manage its connection and
within that and that connection that
manages a bundle it's not a pool a
bundle of processes each managing a
single stream and those stream servers
are the things we talked about right at
the beginning so they're the worker
processes they get spun up and actually
do your business logic so here we go so
this is a client so isolating streams on
the client side that's a client's
responsibility when you call this second
line the stream you get back the process
that called this is the worker that's
responsible for this stream so if you
call this 10 times then it will be one
process managing 10 streams on the
client on the server it is it's much
better sort of managed for you so that's
a module and arguments which we passed
in the beginning so this was your custom
module and these were the arguments
departs pass to start link so for every
single new stream in ace we can use the
wonder of supervision and we have so we
turn that much of arguments into a
specification it needs to be transient
so that's because they die if they die
we can't can't bring them back up if the
stream goes yeah and this is called for
every single one so that's how we make
new workers so for every single stream
so yeah so we sorry that was a slight
slight miss mitigating guiity so this is
when we start the server we build a
supervisor why it makes more sense this
supervisor is started when the server
started and they supervisor is given the
specification for how to start workers
then when there's a new stream coming in
and we call open stream we ask the
supervisor using the specification we
previously gave it to start a new worker
this worker is then stored inside a
stream strut inside our connection which
stores what we said are the streams
should be doing but we let the sort of
application process which is the stream
do whatever it once that we don't we
don't put any
constraints on that it's up to that to
follow the specification and that
separates quite nicely the two the two
concerns like making sure the protocols
obeyed and doing your business logic I
think we are lost to slide somewhere
it's very curious
okay so streaming data streaming data is
advantageous because data can be sent as
soon as it's available
large requests or responses do not need
to be buffered to memory this is one of
the things which led to plugged to look
different to other sort of middleware
layers and it loves notably Ruby coming
from that background streaming is not so
simple to work with you can't write this
function anymore which just turns your
request into a response and for a lot of
occasions that is very convenient so
that is that is a cost there and the
biggest problem with streaming is
whenever two processes both up streaming
and down streaming you've got a lot of
intermediate states so this is just an
example of what a server might receive
it could receive a request then it's in
a different state because it shouldn't
receive a second request that would be
protocol error it could receive some
data then halfway through
then it could start sending its response
but then received more data so it's
still an open up stream and it's now an
open down stream it sends a promise
which starts a different stream keeps
any more data received a trailer it's
still open for downstream but the state
is now closed up streams have it
received any more data that's a failure
and in any of these processes either end
could send a reset going please cancel
this stream at which point enter a new
state and you have to timeout for the
frames that coming in so this is
definitely more nasty than this like if
you can get away with this it's much
nice the thing to do this is from the
documentation this is the full state of
a stream heard the worst thing about
this is it's not even this is not even a
true represent all the states this open
state with this heart closed and half
open are when either end has declared
that is stop sending information there
is really a half idle state where one
side has sent has sent the headers that
are going to start it but the other side
is not and this closed state as well
also has about three sub states which is
was it closed by a completion or closed
by reset because if it's closed by a
reset
because of race conditions it should
accept some more data because it could
have been in the pipes but if it was
closed by a completion it should
definitely not receive any more data and
that should be a protocol error so so
that was that was fun
fortunately state machines are well
understood so okay I think this was the
slide I was looking for earlier actually
so here we go there here we have the
separation so the stream state lives in
the connection process in which we have
a representation of every stream the
stream worker is application controlled
and fundamentally completely unknown so
we there's a protocol of communication
between how it should talk back to a
connection but we don't enforce that it
we just essentially kick it out if it
does it wrong so a state machine in
elixir now there are a few libraries for
state machines importantly there's a
state machine which is a process and a
state machine which is a data structure
if you're newer to elixir making the
distinction between those two things is
very important you don't always need a
process to do a state machine you can
just do a structure and then sort of map
through and keep a reference to the new
one see how long we are so those hits
this is how we start our state machine
so our state machine is just data
there's no there's no process involved
there's a few libraries which help you
do this
I think there's that there's just one
called FSM I found that they just
weren't there weren't even necessary
like with elixir pattern matching you
have a map you have a status key you put
the value in there and everything's good
to go because you can have a
multi-headed function so this is our
receive headers function insight for
dealing with the stream so we get is to
get a stream in its state we find out
what its actual status is I don't use
the key state because then you have a
state where the state key I try to use
status for the name of the state and
then the state for the internals but yes
so these are all the cases as I
mentioned I had to split it up so idle
is is it idle on both sides of the
channel or is it only idle on the remote
side so this is what we think our local
state is and this is what we think the
the remote peers state should be for
what they should be sending so if it's
idle on both sides on either sides have
sent information we know that we need to
turn this headers block which has
arrived here into okay
request and we then forward it to our
stream worker if it's not idle on both
sides but it's idle on our side we know
we must have previously received
something that must have been a request
so we know we can forward a response and
so on and so forth through all the
various states that weigh in we then
just return a new state so if we've
received a request we now know that the
open side of the state is sorry the
remote side of the connection on that
stream is in an open state and our state
is still idle because we haven't sent
anything and this is how in ace
we manage the complexity of that state
machine and I would argue in many ways
that this code it's quite quite long but
it doesn't it's not that much worse to
look at than the actual diagram of the
states in the first place and it's
because of the joy of these pattern
matching cases here the dividends were
huge for that and the same is true for
outbound messages so for outbound
messages when we want to send data we
can only send data when we've already
opened our streams we've already sent
something in any other state it's a
protocol error and we would terminate
that worker and send a little closing
message going unknown error internally
okay so the next one the next one is
flow control this was the slides I added
this morning so just see how long I've
got I think we've got time to talk about
this so flow control is the fact that in
HTTP two each stream can be blocked by
the client or the receiving peer so it
will it advertises how much more data it
would like to receive before that data
is sent that means that not only can the
server send other data if it's stuck or
not dealing with a request if a client
stuck with dealing with a response if
it's being slow to process and it
doesn't have space to buffer more of
that particular response it can stop
sending updates saying I want to receive
more the service should stop sending on
that stream but will still continue
sending on other streams so the other
responses can be dealt with in parallel
again a lot of the stuff around HTTP 2
makes it incredibly friendly for the
small process model that's available in
elixir so in flow control I think that's
what I said
but probably better so this is saying
that yeah so data is only transmitted
once it's been acknowledged that it can
be received in summary this does add
some incredibly nuanced effects to our
state because we now have a point that a
stream can be in a closed state ie it's
sent ahead as block it's sent all its
data and it's closed the stream and no
data has been sent because it's been
queued in our system so if our worker
process for the server goes this is the
response this is the body it shouldn't
send anymore but we've not yet sent it
on the wire and this is managed by
adding to our state this concept of a
queue so we have when we receive data
we'd like to send we actually queue it
so we don't send it we then later call
ascend available so whenever there's a
window update or a new message we can
look at the connection and we reduce all
of our streams and go what can you send
and if anything we add it to this out
trace or out tray is just our it's an
array of frames that we would like to
send down to our client so this is how
it gets updated so when we receive so
the window update frame is the one that
a peer will send to say I can accept
more data and then the stream that it's
on it's an error to threat to have a
waiting and accepted amount higher than
this number so that's just an error case
that happens otherwise we we update our
state with this increased window size
and we send available frames that
increasing this window size make
available the important well the
interesting percent available if you
don't send the frames so this is this is
good practice with functional languages
and it's just a nice thing to do with
you should separate code that does side
effect from the one that calculates what
side effects should be done so in all of
this code we never actually call the
socket we just return this list of
frames this is really helpful but sort
of queuing in this particular case but
it's nearly always like a good idea to
separate those two concerns in your
system these frames struts which have a
protocol that allows them to be cast to
binary so our sending code is then
incredibly simple and it's the stuff
that's most annoying to test
we actually had to send up sockets okay
so what about the rest of these I'm not
gonna talk about them there's not time I
think yeah there we go they're most of
them are in from an eight-string
priority that's an optional
specification again to reduce it it's
just not done so if anyone wants
something interesting to do that that's
completely fresh so what lessons did I
learn from building ace I can't wait any
we can't remember let's see oh yeah
right library applications okay right
library applications when you're writing
a library perhaps this should say a
library application as your main
application can't do anything for those
of their you have slightly newer to
elixir a library application is one
doesn't start processes so libraries you
can call them and they have function
within them which may or may not start
processes but they don't when you add
them to your mixed dependencies that
start their own tree of processes the
reason I say write library applications
can be when possible as it means that
you can start them in a much more
controlled manner so in this case a a
service should be a classical sort of
application with its own supervision
tree but actually I found it wasn't
necessary like it's very easy to as long
as I provide this start link function
which starts a server you can add it to
your supervision tree incredibly easy
it's when you do your mix new you get
that bit in the mix file that says
worker one here you put in server start
link and you put in your list of
arguments and you're done why is this
valuable well it means that you can
start lots of them so I have a whole
bunch of test processes that start a
service on port 0 and use HTTP poison to
call it which makes it incredibly
thorough test like I'm actually using a
proper client to test a proper server
and not relying on an abstraction level
in between and because election airline
are amazing I can do this in parallel
and I can spin up 100 servers each with
100 can wait in connections and it's
fine
you just run one so this is my test set
up started well in which in a test setup
this would have to be port 0 so they
wouldn't clash the rest of the time it
would be port 443 so this is just a typo
both times
and all you need to pass to the test
suite is a port to call to like check
that what your application said it was
going to do it's in fact doing this
leads to the second point which is I
guess equally controversial and then I
was feeling sleepy
don't use configs s for your libraries
if there's no globally available process
tree and you can always start it with a
list of arguments there's basically
nothing left to configure because I'm
not starting anything but it doesn't it
doesn't make any sense once you've
adopted the previous point with the
previous point of passing everything
into this start like so the options take
everything your application that uses it
can use config exs to work out what
those are but for my library I've not
found that particularly helpful so it's
really easy to jump straight into
configuring everything you're like oh I
want this slide between super flexible
I'm gonna make it configurable it can
still be configurable without using the
config file perhaps that's the more
nuanced way to say this
say this lesson and second one is limit
your API so this was just a really
little thing use module doc false
there's a lot of stuff there and I'm
gonna have to probably depreciate
various functions as I go forward my
internal modules I don't want to get
angry if I change how they work and this
is me telling you that I'm gonna do it
all the time so if there's module talk
false and you don't see the functions in
the X Doc's then why use them if you
want but I'm not making any guarantees
about that point
this makes the hex project on hex Doc's
look much nicer they sort of follow five
you have the service the server and the
client to work with you don't have the
frame internals exposed yet because that
would be too much to maintain right so
we can see how we're doing when we get
to okay next steps this seems to be
perfectly times excellent so next steps
face so I've been working on this for a
while the first thing is a rax adapter
this is one of two web interfaces
available in the lick sir it's it's one
that focuses on simplicity and it goes
back to using request to response
directly so it just provides a bunch of
simplicity rax is a project I'm working
on
hence why this adapter is in progress
it's leads us on neatly to
Oh point two which will be a plug
adapter plug adapter is best known as
the other where blue here I need a
Vulcan elixir perhaps yeah oh no plug is
essentially the adapter you should be
using for anything at all unless just
like you've got to Sunday afternoon to
explore an alternative there's a bunch
of things which need doing to this so
plug as a as an entity doesn't expose
server push it's not available HTTP one
that's a design question of what that
API should look like if you use HTTP two
without server push where you're you're
eroding your benefits so what that would
look like would be interesting again it
doesn't expose flow control I don't know
what that would look like it's a it's a
thing that needs to be sorted and this
is why I said that HTTP two is not
necessarily a simple step up from the
first one some of the abstractions are
trivial they hide and you don't need to
see that it was binary data but some of
the extra things you do need to see and
then the final one leads this a patch
send file to attend file is the ability
to write in kernel space a file to a
socket that can't be done anymore
because everything on this socket has to
be in frames so send file doesn't do
that so it just doesn't work so that has
to be patched in some way or another
help wanted if anyone thinks has been an
interesting thing to do I would happily
like review pull requests like are busy
on the other one at the moment but I'm
very keen to like talk more about this
further to that some more clients would
be interesting quite a few api's serve
HTTP to and we don't use them so just
more clients think the client side is
definitely simpler that could be an
interesting thing to do like which api's
would be particularly useful to start
migrate to this point hmm this was a
poorly rearranged slide here so we'll
have this one first G RPC hands up in
the room who knows what G RPC is I only
heard of it recently but that sir that's
quite quite good a quite a few so G RPC
isn't our PC framework I'm interested in
this because it only needs HTTP 2 so
it's not in the short time you're gonna
be able to use ace for anything related
to your web frameworks because I don't
yet fall back to HTTP 1 and you don't
want to lose those clients Oh
we found a use case for G RPC this is
very much a work in progress in the
elixir community and I would happily
help out with this this if project of
anyone wanted to like sort of take it on
I was working towards this I think this
would be a really nice thing to get done
explaining to your PC as this diagram
which you saw earlier
so it's an RPC framework that's what
this diagram shows finally a gen stage
so this is um I think this is really
interesting as well
I've not started I was a bit busy
recently um gen stage looks like this
and gen stage has this demand driven set
up where the consumers they ask for the
amount of data that they can they can
send and then the producer sends that
much data and no more to the consumer
with HTTP to one of these steps could be
done remotely
you could have an H 2 B 2 connection
between a single producer and consumer
or a bunch of producer and consumers on
separate streams and you could translate
the gen stage asking for messages into a
representation of back pressure in the
window updates available in HTTP 2 so I
think this would be a really fun project
this is one I will probably try and look
it on my own given time whenever that
will be finally this is a blatant like
come talk to me guys
Doc's dialyzer Erlang interrupts they're
all that they all need doing particular
interest in the airline interrupts I
joke about the whole reinvent the wheel
thing ace is not ready keep using cowboy
however I think a healthy ecosystem
probably has two maybe three
alternatives it would be nice if one was
like owned by the Alexa community
getting it to interrupt with airline
would make it as valuable in a lot of
areas so you know I'm serious about
pushing that one forward I think that
could be that could be worthwhile
so yeah that's me thanks for listening
I'm Peter and you can find the crowd
hailer all over the internets
I'm one of two developers at curl so
we're both lead developers
we're using Alexa everyday so that's
really nice it's working really well for
us yeah come and talk to me about that
if you want they were really nice to me
yesterday so I will give them a little
shout out we're reinventing the future
of payments with user names so that's
that's fun these are the links to the
code so this is a copied slide these
were bottom two aren't even relevant
well they are but the top one is ace
that's the that's the server other
things I'm doing with it related to the
web are below they both have ace back
ends so if you want to see it noose you
can do that any questions
what told you everything you need to
know oh there's one in the corner
hey that's great use in production yet
yes are you planning to so we are not we
don't really have many customers the
other play with care so production is a
vague term for us at the moment I am
aware that someone's actually using
racks in production somewhere else
That's not me that has happened there
using the earlier version of ACE that
was for HTTP 1 truly it's not it's not a
replacement for web services because you
don't have that fallback there are some
interesting discussions I've had around
G RPC I think that could happen much
quicker but yeah it has been used in a
couple of couple of little places so
when I learned HTTP 2 I had to read a
bunch of online protocols and
specifications and there were what at
least to me not very friendly took a
really long time to learn them so do you
have any recommendations for an easier
way to get your head around how http/2
works because it took me way too long to
like realize all of those things that
you listed right oh it depends what
you're what you're doing to implement it
you need to read the specification
there's no getting around that the
specification is at least at this point
only one it's quite a complicated
specification but they they've not
revised it yet so if you start at the
beginning and read to the end you know
you know everything that's really nice
if you don't if you're not implementing
it I assume there's a point where you're
going to implement it using some other
service that thing should be the one
that provides you the documentation
that's what I would say like I don't
know if they exist very well yeah
there's a couple of nice things on the
nginx website but they have quite a
subset of use cases for it like they're
not about making applications that use
it they I think they read your your
assets and and pick out link tags in a
sort of cunning manner but yeah if you
wanna get understand it you've got to go
to read the RFC and it's not that bad it
wasn't that bad was it
yeah but yeah but yeah if you just want
to use it it should be the docks of what
you're using that's what I would say
anyone else make him work for it have
you played with gun and what's your
opinion of that as a http/2 or generic
HDTV client I think I've probably read
more of the source than I have actually
used it I think it's it's okay I did not
start this actually trying to write a
client I was just trying to write the
server and I was using I think Cadabra
which is an HTTP to client to test it so
I was doing that thing of exposing on a
port and testing it there I could have
used gun I think I was very good the
only reason I switched to making both is
because because of the I think this is a
well-thought-out better the protocol
both sides are incredibly similar so the
stream code and the connection code are
just the same for both sides of the
connection so I discovered with very
little work I could write a client and
then it was just nice because I have an
ace dot request thing which I put in I
ship it and it comes out as the same
thing on the other side yeah I mean gun
is probably far more ready to use today
like I think it works like I said that
there's problems with the server side
working on cowboy
again coming back to this G RPC thing if
you don't need HTTP one you might wait a
long time for cowboy and gun because
they have the fallback like that's
what's essentially slowing down their
development but of course if you need
that and it's a better it's a better
thing to use
so this is a really bad question because
I don't really understand what you said
at one point you said you mentioned
there was a problem with HTTP 1.1
anything was ahead of line or something
like that
head of line blocking yes and that went
away and then it was but it's still
there in TCP who do you like I'd have no
idea that meaning right that's not
exciting can you talk more about that I
wish I had some kind of whiteboard to
draw a diagram I'll just wave my hands
appropriately a head of line blocking is
when you have so you have two computers
talking to each other sending data from
A to B head of line problem is when B
has the data it needs to do something
but it doesn't know about it because the
protocol is limited it or the data could
be sent perhaps there's a better way of
doing it so the client they saw they
were two requests and they're cute
one of them's really big one and really
small client the this other computer
could use the second response or request
but it can't get it because the protocol
says you must ship everything is an
atomic chunk so you might be able to use
that but it can't get it because it
can't be sent HTTP to solves that the
disordered level of the requests because
one can be blocked technically pipeline
but TCP works by sending packets in
order and those packets they all have a
stream identifier so it could be that so
when you lose a packet the TCP level
goes send me the packet I'm missing but
if that packet contained only data for
stream one there might be in like
genuinely on your machine all the
information for streams 2 3 &amp;amp; 4
but TCP is going on oh there's there's a
break so it has to then wait for to be
reset and if you want to switch to that
you then have to look at quick which is
Google's next experiment on HTTP over
UDP yeah amazing thank you anywhere else
No
all right well thank you Peter that was
really exhausting Thanks
[Laughter]</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>