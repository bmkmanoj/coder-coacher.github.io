<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Philip Wadler  - Propositions as Types (Lambda Days 2016) | Coder Coacher - Coaching Coders</title><meta content="Philip Wadler  - Propositions as Types (Lambda Days 2016) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Philip Wadler  - Propositions as Types (Lambda Days 2016)</b></h2><h5 class="post__date">2016-03-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/aeRVdYN6fE8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">let me do the important thing first
smile
I wanted to do that because this is
absolutely an amazing audience how nice
to come to Cracow and see a room full of
smiling eager faces so I'd like you to
all give yourselves a round of applause
this is really amazing so thank you to
the organizers for inviting me and thank
you to you for showing up and are you
ready to learn about the hilarious
subject of computability theory see I
told you is hilarious so let's do this
an algorithm is a sequence of
instructions executed by our computer
now today we think of a computer as a
machine but computer used to be the name
for a person the man or the woman who
executed the algorithm algorithms can be
found in Euclid's elements in classical
Greece and eponymously in the work of
al-khwarizmi in 9th century Persia but
it wasn't until the 20th century that we
get a formal definition of algorithm
when proposals by Alonzo Church Kurt
girdle and Alan Turing all appeared
within a year of each other
it's like buses you wait 2000 years for
a formal definition of computability and
then three come along at once why did
this happen
at the dawn of the 20th century one of
the foremost proponents of formal logic
was David Hilbert in göttingen and he
had a goal his goal was to put all
mathematicians out of work what he
wanted was an algorithm that if you put
in a formal statement in mathematics you
could compute and determine if the
statement was true or false so you
wouldn't need mathematicians you could
replace them with computers which were
still people back then and this was
called the anti drunks problem because
it sounds better in German now his
belief that the introduced problem was
solvable basically presupposed that
logic was complete
now Keats famously said beauty is truth
truth beauty
okay so Keats tells us Beauty if and
only if proof and completeness is very
similar right if you have a logic that
is sound and complete that means
provable if and only if true what is
provable is true and true provable okay
and this basically was why it was
figured that there had to be a solution
to the introduced problem but in 1930
Kurt gödel delivered his proof of the
incompleteness of logic and that meant
that pardon my use of a technical term
here but Hilbert was screwed so what
girdle did was he found a way of
encoding in um as numbers statements in
logic and proofs in logic and then he
could write down a numerical statement
that meant this statement is not
provable way they right as soon as you
can write that down you are in serious
trouble right because eat the statement
is either false or it's true so if it's
false then it's provable and you've
proved something that is false
which kind of undermines the whole point
of doing proofs I you want to do proof
so that you know that it's true so you
really don't want to be the case that
you can prove something that's false so
what's left as soon as you can write
down this statement the only possibility
that's left is that the statement is
true which means it's not provable so
there is a true statement that is not
provable so that's not as bad but it's
still really annoying especially if
you're Hilbert now as long as people
thought the anti dooms problem was
solvable they didn't need a formal
definition of algorithm it would be like
justice stewarts definition of
pornography I know it when I see it
right you could just look at it and see
if you had an algorithm or not but if
you wanted to prove there was no
algorithm that could solve the
introduced problem then you needed a
formal definition of algorithm so you
could show that nothing in this class of
things can possibly solve the introduced
problem so the race was on to come up
with a suitable formal definition of
algorithm and then show the introduced
problem was unsolvable so the first
definition came from Alonzo Church and
he defined the lambda calculus and in
1936 and he came up with Wright church's
thesis the claim that what you can write
down in lambda calculus exactly
corresponds to what is computable what
is an algorithm and in using this
definition he showed that indeed the
entire problem was undecidable now
lambda calculus is the world's oldest
programming language being defined back
in the 1930s and it's the coolest
because it was defined a decade before
the first storage program computers were
built and it's very small right here is
the complete syntax of lambda
so it has just three constructs
variables abstraction and application
now you've all worked with functional
programming for years I've worked in
functional programming and people in
industry have pretty much contrived to
ignore everything we've done right they
just don't pay much attention to us
but all of a sudden right lambdas have
become trendy right and all of a sudden
your gang languages like Java and C++
and Python are all boasting that they've
now added lambda so here's Duke look at
the icon for Java looking rather smug
about getting lambdas into Java okay
well congratulations Duke you have
finally caught up with where church was
in the 1930s so Kurt girdle remember him
came to Princeton to visit and he had a
word for church's definition he thought
it was thorough well actually he had two
words thoroughly unsatisfactory
so um Church said fine you rip out your
own definition and I will show you that
mine is as big as yours and girdle did
this so he came up with the second
definition recursive functions and
Stephen Kleene II who was church's
student published the definition with
app tribution and proved Church and
cleani proof that indeed the two
definitions were equivalent they defined
exactly the same things so Church went
back to girdle and he said see look your
definition my definition they're the
same and girdle said hmm my definition
is the same as your meshugganah
definition mine must be wrong men
the impasse was resolved by Alan Turing
who came up with the third definition
what we now call Turing machines and the
really interesting thing in Turing's
work was not just that he defined Turing
machine it wasn't just the math it was
the philosophy he actually gave an
argument that what a Turing machine
could do was the same as what a computer
could do we remember a computer is a
person following the rules of an
algorithm so he had things like well
there must be a finite number of symbols
because if you had an infinite number
written in the small space you couldn't
distinguish all of them and the memory
must be a small number of symbols
because you can't sorry what you're
actually considering at one time must be
a small number of symbols because you
can't tell the difference between nine
nine nine nine nine nine nine nine nine
nine nine and nein nein nein nein nein
nein nein nein nein nein nein nein in a
single glance and things like that so he
did some philosophy to explain why this
was a good idea and indeed he proved
that his definition was the same as
churches and hence as girdles and this
finally convinced girdle that maybe he
had been right after all
so philosophers argue a lot right and
one of the things they argue about is is
mathematics invented or discovered I've
discovered that they really like to
argue about this right I've said this
before and talked and then I get lots of
arguments about this and people have one
point of view or another but just
sticking to the ordinary senses of the
word right I think the fact that you
have three independent definitions that
all turn out to be equivalent if you
powerful evidence that you're not
inventing something but you're
discovering something there's something
that was there before something inherent
it's not just made up so Kurt durdle was
28 when he undermined the work of
Hilbert who was at that time 68 turing
was 23
when he resolved the dispute between
church who was 33 and girdle who was by
then in ancient 30 so all you young
people in the audience you know what to
do
please keep explaining to your elders
when we've got it wrong okay
so you've now learned about
computability yeah you can applaud if
you want to or not and now what I'm
going to go on to do now that we've got
our grounding in computability is so I
said okay here's evidence that something
interesting is going on you have three
discoveries and they all turn out to be
the same thing but there it turns out
that lambda calculus so and by the way
this was using something called untyped
lambda calculus so it has two variants
typed and untyped and typed lambda
calculus it turns out is exactly
equivalent to certain kinds of logic
this is really cool this is yet more
evidence that you're discovering things
rather than inventing things and there
are lots of names for this it's
sometimes called the the dhk
interpretation it's sometimes called the
Curie Howard isomorphism and the most
standard name for it I think is
propositions as types so let me tell you
about propositions as types so we're
going to start with this fellow gajja
Jensen and in 1935 at the age of 25 in
his PhD thesis essentially he did three
things he introduced natural deduction
which is the main way of formalizing
logic that we use today he introduced
sequent calculus which is the second
main way of formalizing logic that we
use today and he proved
a kind of normalization theorem they'll
tell you a bit about he also by the way
he was the first one to use an
upside-down a to mean for all so here
are his rules and his insight so write
formal logic goes back another hundred
years right so it goes back to bool and
it goes back to Hilbert and Russell and
Whitehead and several and piano and many
others but Genson right after people
have been doing formal logic for like 50
years
he finally had a very important insight
which is you should write it down with
rules in pairs and the rules are called
introduction rules written with an eye
and elimination rules written with an E
and so this is the way
Jensen wrote them down originally and
we're going to focus on just two of the
rules the rules for implication and the
rules for conjunction for and and here
they are so these are if you look you'll
see they're exactly identical except
that Jensen wrote his letters in German
and I'm going to write my letters in
English so this is the rule for
implication a implies B and what he says
is the rules come in pairs so one rule
so this line here in fact stands for
implication at the meta level so a a
funny thing B means a implies B that's
actually a backwards C for consequence
it says B is a consequence of a so a
implies B what does that mean well
you're allowed to conclude that if
assuming a so that's what this a in
brackets means we've assumed that a is
true we don't know that it's true we
just let's assume that a is true if
assuming a is true you can prove B then
that means a implies B so that's an
introduction rule it has a implies B
below the line and the rules come in
pairs how do you create a thing and how
do you use a thing how do you construct
a thing and how do you d construct a
thing
so the Deacons
renewal is called an elimination rule
and this says what can you do if you
know a implies B well if you know a
implies B and you also know a then you
can conclude B and this is a very old
rule it goes back to the classical
Greece and in the Middle Ages this was
called modus ponens so are these rules
kind of familiar to people you should
serve not if it's kind of familiar and
if it's not kind of familiar and you
have a question you should ask a
question are there any questions right
so I'm going to get a little bit
technical so you need to work with me
it's your job if you don't follow
something to ask a question okay and
I've always had people come up to me
after this talk and they say that was
great I didn't follow any of it so if
that's you please ask a question that's
your job to help the other people in the
audience to also follow up not just
yourself okay yes question this is funny
little axes as a superscript yes right
so I stands for introduction this is an
introduction rule and then e stands for
elimination this is an elimination rule
and then you've noticed with your sharp
eyesight that there's a little X written
here and a little X written here and
what that means is this assumption is
what we're called discharged so that
means when you're done with the proof
we've just proved a implies B there's no
other assumptions like we're not
assuming a a implies B it's just a
implies B with no assumptions so this
assumption has been discharged and it's
discharged by this rule so to indicate
that this assumption is discharged by
this rule they're linked by writing an X
on both of them that's a very good
question
thank you any other very good questions
okay but yeah
here the zero and one so well right so
the this is the second set of rules
remember I said the rules come in pairs
but in this case right we've got one
introduction rule if you have a proof of
the day and you have a proof of B then
you can conclude that you've got a proof
of both a and B so that's how you
construct a proof of a and B how do you
deconstruct it well there are two ways
right what do you know if you know a and
B but one of the things you know is a
and the other thing you know is B so
we've got two rules so I've put
subscripts on them to separate them and
instead of counting one two I'm a
computer scientist so I count 0 1 yeah
all right okay when somebody asks a
question I will try to say the question
out loud yes thank you and if I don't
please remind me just shout out repeat
the question and I'll do that okay so
this was Jensen's key up question yep
oh right so somebody wants a further
explanation of the two symbols so this
is a backward C and just dance 4 implies
a implies B meaning if you have a proof
of a then you can conclude you have a
proof of B and this is a and B meaning
both you have a proof of a and you have
a proof of B and in the black here
written letters it doesn't mean the same
or yeah okay this is the rule that
introduces an ampersand and you can see
that because there's an ampersand below
the line and this rule says this is one
of the two ways of eliminating an
ampersand and you can see that because
there's one above the line but not below
and similarly for this rule does that
answer your question yes yeah so this is
just the name of the rule right if you
met the rule at a party that says hi I'm
an inference rule you
I'd say what is your name and it would
say I'm and elimination zero so here's
an example of doing a proof so I'm going
to prove that if you know BN a then you
can conclude a and V well da this is
completely obvious right of course from
be a you can conclude a and B because an
is symmetric well yes that's true you
know it and I know it but we're trying
to formalize logic as formal rules so
can we conclude this obvious fact just
can we prove it just using the rules we
can get it right so if we can that's
good and if we can't that would be sad
but we can and here's the proof so
assume B and a well from BN a I can
conclude a right and from BNA I can also
conclude be right but now it's got a
proof of a and I've got a proof of B so
I've got a proof of a and B and now I
can just so now well it's done so far as
I said assuming BN a I know a and B and
then this these two assumptions are both
labeled Z and then this rule is also
labeled with a Zed so this rule
discharges the assumption it says with
no assumptions at all I know that VNA
implies a and B okay so it's symmetric
question sorry is there any kind of
constraint on the implication rule so
can a reference something that we have
in B or the other way around
oh or what happens with the variables
kind of a present a variable would
appear strange except reference appears
in B and so on those are great questions
and I'm not going to answer them I'm
just going to talk about some very basic
symbols but we can talk about that
afterwards if you want you can get into
very interesting dependencies and things
called dependent types but I'm not going
to talk about that today so great
questions but I'm not going to answer
them
now the key reason for putting the rules
in pairs is that they guide how you
simplify proofs
so in fact Jensen didn't disk so Jensen
want to simplify proofs in order to show
that there are no roundabout proofs what
he wanted to do is he wanted to show
that if you could prove something and
you had no assumptions that the only
formulas that appear in the proof are
going to be sub formulas of the things
that you proved so what is a sub formula
well not too surprising what are the sub
formulas of BN a implies a and B they're
all the parts of it so they would be B
and a and a and B and then the parts of
those so a and B and then a and B don't
have any parts right so and indeed the
only things that appear in this proof
are sub formulas of B and a and of a and
B so he wanted to show that that was
always the case now if you look here
right this proof seems to have a formula
in it a implies B that's not a sub
formula of its conclusion so he needed a
way of simplifying proofs and how do you
simplify proof it turns out whenever an
introduction rule encounters an
elimination rule you can simplify right
so if I've proved a implies B well that
meant assuming a I can prove B and then
here I have a proof of a and I conclude
conclude B so that's a better way of
doing the proof right instead of
assuming a and concluding B hey I've got
a proof of a so everywhere here I said
assume a I can just replace it by saying
oh here's a proof of a and then from a I
can then prove B so now I've eliminated
this formula even more simply right
got a proof of a and B well what does
that mean it means I had a proof of a
and I had a proof of B and let's I use
the first elimination rule so I can
conclude a hmm
is there a simpler way of proving a oh
yeah
look I proved a over here I'll just use
that proof so we can simplify proofs and
let me give you a simple example of
simplifying a proof so here's the proof
we had before that B and a implies a and
V and now here assuming B and assuming a
I've got B na so let's say I'm just
going to have these two things as
undischarged assumptions then from be a
I get a and B by did it really a
roundabout way right first I have BA and
I've got B a implies a and B so all
these other things are not sub formulas
of my assumptions B and a and my
conclusion a and B all right so I've got
B a and B but then all this other stuff
that's not sub formulas of that so can I
simplify the proof Oh we'll look here is
an arrow in sorry and implies
introduction and here's an implies
elimination so those are next to each
other so I can simplify it and what does
that mean it says well here I assumed BN
a but here I've got a proof of B and a
so just substitute this proof for the
two assumptions so if we do that we end
up with this tree here right so we've
gotten rid of this big formula now right
and we've just substituted this bit on
top in those two places and having done
that this and illan and introduction
will has appeared on top of two and
elimination rules like this so I've got
and introduction followed by and
elimination and and introduction
followed by and elimination I can
simplify that so oh yeah from assumption
a and assumption B I can prove a and B
so there's the simplified proof and I've
got rid of all these unnecessary sub
formulas now
the reason Jensen was interested in this
is he wanted to prove consistency right
that you cannot prove false so false is
basically just as simple in the language
and the thing that gets tricky is
negation right the way you say not a is
you say a implies false so then if you
had a implies false and you had a you
could conclude false so we want to show
there's never ever a proof of false and
that you know that might be hard because
maybe you could prove a implies false
and a for some a so if you normalize the
proof in the way that I just showed you
then this extra formula a implies B or a
implies false would go away right it's
not a sub formula of the conclusion
false in fact how many sub formulas are
there of false right not very many
false is a sub formula of itself and
then there's nothing else right so it's
just like the old saw what part of no
don't you understand right there are no
parts of false so by normalizing proofs
you could say well if there was a proof
of false it would have to consist of
false on the bottom line and nothing
else above it oh and there are no proof
rules that look like that so we're done
so he got consistent that's why he was
interested in simplifying proofs in
having no roundabout proof so that he
could show consistency now interestingly
the proof I just showed you is not due
to gansan the reason he invented sequent
calculus this other main formalism which
I won't show you today was to show this
normalization property he couldn't show
it directly it was another 30 years
before in 1965 prophets wrote down the
proof that I showed you today so that's
kind of ironic right he needed a
roundabout proof to show there are no
roundabout proofs but eventually it was
done directly and by the way I'm working
with a variant of logic here called
intuitionist or constructive logic which
doesn't have the law of the excluded
middle but
don't worry about it okay let's go back
now to Alonzo Church and lambda calculus
now the previous version of lambda
calculus is untyped lambda calculus and
that's important because in untyped
lambda calculus you can write down any
possible algorithm which was the goal
right that's what we needed for the
introduced problem um
typed lambda calculus is a bit different
it guarantees that everything that you
write down terminates so the types are
enough to guarantee that everything will
terminate now you need both of these
things right they're both useful right
if you want to be able to write down
every possible function then something
that lets you iterate forever and
possibly get into a loop is essential
and the halting problem tells us you
couldn't always decide if you're going
to get into a loop but it's quite nice
to know that your programs terminate it
turns out an awful lot can be expressed
in typed lambda calculus in particular
Church actually he didn't invent lambda
calculus because he wanted a programming
language which is maybe not too
surprising since they didn't have
computers back then he invented lambda
calculus because he wanted a sort of
macro language for logics and it turned
out to be a really powerful macro
language it actually did introduce
inconsistency and let you prove false
which as we saw was sort of bad so it
turned out the way to get rid of that
was to ensure that all the land
expressions would terminate that they
wouldn't get infinite loops and so
that's why he invented typed lambda
calculus it's very close to getting rid
of paradoxes and on things like
Russell's paradox was also Wrestle
invented a type system to solve that so
they're closely related ideas in fact
I'll say that more asking about that
later if I don't get back to that so
typed lambda calculus a function has a
type so remember we had lambda
expressions lambda X n um and that says
I'm going to write its type this way
a funny symbol B sometimes this is
written in a scenario and it says given
an argument of type a we return a result
of type B and then this is function
application given a function that takes
an a to a be and given a value of type a
if you apply the function to the value
you of course get a result whose type is
B I'm going to extend lambda calculus
with a pairing operation so if M is a
term of type a and n is a term of type B
so like M might be one of type natural
number and n might be the care the
letter A of type character then this is
1 comma a which is a pair whose type is
the first component is a natural number
and the second component is a character
so just the pair type or the record type
and what can you do with a record well
you can extract the first component
which if it's an A and B record will be
of type A or the second component which
if it's an A and B record will be of
type B and then you can type your
programs right so here's a simple type
of program it takes a B and a pair and
it returns an A and B pair and what does
it do so lambda Z this is our function
argument you're all probably familiar
with lambdas since this is called lambda
days so lambda Z says n is an argument
of type B and a and we'll return a pair
consisting of 2nd Z and first of Z okay
and so what does this do it takes a pair
and it returns another pair consisting
of this second component of the original
pair followed by the first component of
the original pair so given a B and a
pair it returns an A and B pair and you
can see this is just the swapping
function given a record it swaps the two
components
as you execute a program it still
retains the same type and we can see
that by looking at what happens is your
executors program so lambda xn is well
typed because X has type a and then
given that n is a term built from acts
like so example right lambda Z here's
our term which is built from Zetas it
refers to Z twice as it happens and then
M is are some argument of type a how do
we evaluate the function applied to the
argument you just substitute your
argument your actual parameter for your
variable the formal parameter every
place it occurs so now we build up a new
term which is n with all occurrences of
M replaced by X and that will be well
typed because we can just put the proof
the M is well typed in place of the
assumption that X has type a and
similarly if M has type a and then has
type B so I've gotten MN pair of type a
and B and then first of n and would of
course be M how do I know that's well
typed oh yeah I knew that M has type A
so I can conclude by the same proof here
that ms type a I know here
that M has type a I just copy over the
proof so as I reduce the program as I
evaluate the program it remains well
typed so here is our swapping program a
pair to a YX parent I'll just assume
that Y is some variable of type B and X
is some variable of type a and then I
can produce the YX pair and then apply
the swapping function to YX
what should that give us well let's see
to evaluate this I replace the formal Z
by the actual YX and that gives me this
term which is again well typed and now
I'll extract the second component of YX
which will be X
and the first component of YX which will
be Y and I get X Y and that's well typed
because X has type a and Y has type B
that's what we assumed to begin with so
we I can evaluate a program and notice
that because I'm always getting rid of
sub sub formula it's always getting
smaller and therefore it's guaranteed to
terminate you need a bit of a
complicated induction to prove that but
you can and in fact Turing was the first
person to prove that every program in
typed lambda calculus must terminate so
he showed the halting problem is solved
so he both show the halting problem is
unsolved for untyped lambda calculus and
solved for typed lambda calculus and you
need both they're both useful so at this
point right I'd like you to all reach
under your seats
the organizers should have left some
rose-colored glasses there so please
reach under your seat and put on your
rose-colored glasses they're there right
you guys did do this
ah sorry okay well pretend that you're
wearing rose-colored glasses and then
all the red stuff would go away and you
would just see the blue stuff and that
might look kind of familiar right
okay so there's you'll have seen this is
one-to-one correspondence between proofs
in natural deduction on the one hand and
programs in simply typed lambda calculus
on the other hand and this is sometimes
called the Curie Howard isomorphism this
is a drawing do Deluca card le called
the Curie Howard homeomorphism so right
functions correspond to implication
Cartesian product pairing corresponds to
conjunction it turns out that you can
also show that disjoint union
corresponds to conjunction to
disjunction or and the empty type
corresponds to false that makes sense
right they would say there are no proofs
of false so false is the empty type so
this is named after Haskell curry who
first wrote this down for a different
version of logic and then William Howard
wrote this down for simply typed lambda
calculus he first came up with the idea
in 1969 and it was circulated as Xerox
sheets and it was finally published in a
Festschrift to curry in 1980 so we see
this is completely obvious right well
it's completely obvious because I've
shown it to you and I could show it to
you because all these other people had
figured it out once you've seen it it's
completely obvious but at the time
Denson wrote this down it certainly was
not completely obvious but he didn't
even know how to do the proof in the
form I showed you but once you've seen
it it is entirely obvious right so we're
indebted to people like curry and I
think especially the Howard who put it
down in a very simple form that really
did make it completely obvious
and here's his paper in which you wrote
that down so what have we see right we
have this correspondence which says
write propositions that is formulas in a
logic correspond to types which is where
the named propositions as types comes
from but it's more than that right
proofs correspond to programs and it's
even more than that right because we saw
that normalization of proofs corresponds
to evaluation of programs so it's not
just that propositions correspond to
types but that all of the structure
comes along as preserved they're really
just exactly the same system which is
why it's sometimes called the Curie
Howard isomorphism all the structure is
preserved so proofs correspond to
programs and normalization of proofs
corresponds to evaluation of programs so
you are probably thinking the same thing
that I thought the first time I saw this
which was a
that's kind of funny look at that
there's a particular logic and a
particular programming language and they
just happen to correspond that must be a
coincidence but well no it's not because
yeah it works for natural deduction and
typed lambda calculus in fact
intuitionistic natural deduction and
typed lambda calculus but it also works
for type schemes and the hindley-milner
type system used in ml and in fact type
schemes were first discovered by the
logician hindley in 1969 and by the
computer scientist Robin Milner in 1975
rediscovered them embarassingly working
at the same institution where Hindley
was it also works for polymorphic lambda
calculus which was called system F what
was discovered by jihad in 1972 and the
polymorphic lambda calculus by John
Reynold when he rediscovered it in 1974
and again Reynolds complained that he
went talking in France about this at
Gerard's home institution and nobody
told him about system F so what have we
learned we've learned that the Curie
Howard is a double-barreled name that
guarantees that every interesting
functional language will have a
double-barreled name because it will
have been discovered once by a logician
and once by somebody doing computing and
it works not just for that but for lots
of other things so that modal logic
which was first invented by Lewis in
1910 there are many many variants of
modal logic but one of them corresponds
exactly to monads which are now widely
used in Haskell and F sharp and Scala
and other systems for managing things
like state and exceptions and
concurrency it turns out girdle's proof
that classical logic can be embedded in
intuitionistic logic corresponds exactly
to continuation
passing style which is often used for
compiling programs due to work by guys
steel and most recently something else
discovered by Sharad linear logic turns
out to correspond accession types which
is what I'm working on currently which
were discovered by Honda in 1993 and are
a way of describing protocols for
communication between processes so it
doesn't just happen once it happens
again and again and again
pretty much every interesting logic will
have a corresponding interesting
programming language feature and vice
versa so things i've not mentioned here
is that classical logic actually then
corresponds to the into continuations to
the call/cc operator in languages like
scheme and racket so all sorts of things
correspond and pretty much every
functional language you can name has
some of these ideas at the core of it so
I mentioned a lot of these ideas show up
in in Haskell for instance but pretty
much every functional language you name
not all of it right is based on lambda
calculus we can't say that all of it is
discovered but what we can say is that
there's a core that is discovered now
most of you work in languages like Java
or C++ or Python and those languages I
will claim are not discovered they are
invented right looking at them you can
tell they are invented
so this is my invitation to you to work
in programming languages that are
discovered and somebody asked before
about dependent types it turns out when
you extend two dependent types so that
you now get logics involving for all and
there exists and this was one of the key
ideas that howard has that that
corresponds to a programming language
feature called dependent types so this
means that you can encode very
sophisticated proofs pretty much
everything you can name as a program and
so the standard way of getting a
computer to check a proof for you or to
help you develop a proof is to represent
that proof as a program in typed lambda
calculus and all of these systems are
based to a greater or lesser extent on
that idea so things like the proof of
the four color problem that was done in
coq by uh Fournier is based on these
ideas okay so that's kind of the
technical stuff that I wanted to say if
you want to know more about this there's
a paper of the talk which has finally
appeared it was in the communications of
the ACM for December of last year that
will give you other resources that you
could follow but also vataj is giving a
closing keynote which will build on
these ideas and so come to that if you
want to learn more about these ideas
he's nodding so I think I've got that
right okay so I mentioned that let me
take home doing for time good I
mentioned that Turing's big difference
from what had been done by church and by
girdle was philosophy so I'm going to
conclude by indulging in a little bit of
philosophy what would happen if we tried
to talk to aliens we've actually done
this right this is a plaque on Voyager
which is a there's Voyager there in the
background behind some people so you can
what sighs people are compared to
Voyager and then these are lines showing
the distance of various pulsars from
salt and the length of the line is
proportional to the distance and then
the lines are marked with numbers in
binary and the number in binary is the
frequency in terms of hydrogen atoms
given off by these different pulsars so
that is if aliens saw this they could
look at it and they could go ah all
right what's going on here
and here's my claim so aliens looking at
this bit on they might or might not work
it out right if you know it depends on
what their perceptual system is like
what the aliens are like right if Star
Trek is right
aliens will look at it and they'll go oh
people look just like us except they
don't have pubic hair um but it's dark
Rick's not right right they might look
at that and go oh I don't know what that
is
this bit they can probably figure out
right they can probably measure distance
they can probably work out the binary
number system they might be able to
guess that this means a hydrogen atom
and that this is a frequency of hydrogen
right let's just hope they don't look at
that go oh that's where they are look at
that go they look tasty and come over
here but that's what happened in the
film Independence Day right aliens come
and they want to invade the earth and
the aliens are destroyed by a virus a
computer virus here is the virus
captured from the screen so you can see
that it's written in Java
actually no the film came out in the
early 1990s which was before Java had
spread through the known universe so
it's probably written in C and you can
see it's written in a dialect that only
has open brackets but not closed
brackets so how likely is it right the
aliens will understand Java or C well
not very likely
what about lambda calculus right they
probably won't write it with a lambda
but they'll probably have the same
abstract syntax right they'll have the
same concepts they will have discovered
the same thing so we could give them a
computer virus written and lambda
calculus
except of course functional languages
are very secure so you know should we
conclude by saying that lambda calculus
is a universal programming language well
let's talk about that for a minute
right it's become common to talk about
multi versus so you see this referred to
even in the works of Salman Rushdie and
this is from a play called
constellations which has only one stage
direction in it right normally a play
has all you know pages and pages about
the stages laid out this way there's
only one line in this which says a
horizontal rule in the text denotes a
change of universe so this is a play
that takes place in many many different
universes so we're used to the idea of
many different universes now um right
people actually talk about different
universes for saying well you know it's
kind of funny because the weak
electromagnetic force is just enough
that matter could form if it was a
little bit stronger or a little bit
weaker we wouldn't have matter why is it
like that and then people say well it's
like that because if we didn't have
matter we wouldn't be here to see it so
they're actually lots of universes and
some of them might have greater or
lesser weak electromagnetic for
but ours has one where matter forms so
scientists actually talk seriously about
there being multiple universes so you
might have a universe with a different
weak electromagnetic constant what about
modus ponens if a implies B if you know
that and you know a you can conclude B
now your imagination might be greater
than mine but for my imagination I
cannot conceive of such a universe there
might be universes with different
electromagnetic constants but there will
not be universes that don't have modus
ponens and therefore that don't have
lambda calculus so what does this tell
us it tells us we cannot call lambda
calculus the universal programming
language because calling lambda calculus
the universal programming language is
too limiting okay so that's me right and
just to conclude I want to remind you
but this is lesson you should take away
from this talk when you have a tough job
then you should think that this is a job
for lambda calculus
so I can't I'm left of my shirt hanging
out because otherwise I'll pull off the
microphone but we have a little bit of
time for questions if there are some
great questions during the talk with
people have more questions I have one
question you mention it a lot of
different logic system and you said that
each such system implies some new
features in programming languages so can
you like pretend or in I mean predict or
invent some new features in some
programming language like a new feature
- Haskell a long right now right so how
does this help you out so one thing that
I said I would mention later and didn't
so thank you for reminding me of it
I mentioned Russell's paradox right
Russell's paradox corresponds to where
you can't have a logical proof system
right where you could prove false I said
that course wants to non terminating
computations which course wants to the
fixed-point operator which corresponds
to recursion which is what lets us write
any possible computable program so it's
so powerful that even paradox something
that breaks logic corresponds to a very
interesting feature of a programming
language and I mentioned briefly this
idea that linear logic corresponds to
session types so right the most
important problem right now in computing
is how do we deal with concurrency and
distribution right I'm walking around
with four computers in my pocket how do
they talk to each other how do they talk
to the warehouses of computers run by
Google and Amazon and Microsoft
elsewhere so communication concurrency
distribution are the most important
problems and there are lots of different
ways of dealing with those problems all
of which pretty much look invented some
of them are very clever inventions right
some of them I really like like pi
calculus
but they all kind of look invented they
all look different so can we discover a
way of dealing with concurrency and
distribution so it turns out right that
linear logic corresponds to session
types session types in fact were based
on linear logic so they're not a
completely independent invention and
they have to connectives that come from
linear logic and to other connectives
read and write which are kind of
important or send and receive which are
kind of important for protocol send a
message and receive a message which did
not which Kohei did not base on linear
logic except 20 years later thinning and
Kara's discovered that actually send and
receive correspond there are four main
construct constructs in linear logic two
forms of and and two forms of war and
Kohei had one of the forms of antenor
but not the other one
turns out send and receive correspond to
the other one discovery took 20 years to
figure that out
blindingly obvious once you see it takes
20 years to find it out so that's why
I'm not working on session types because
this is logic perhaps guiding us to a
discovered solution for dealing with
concurrency and distribution did that
answer your question
good other questions and and you want to
basically discover a machine that we can
basically use for computations for for
logic what what's the intuition behind
such a machine what would be the
intuition behind a machine that is
discovered yeah so it turns out that
there's lots of work on right as we saw
simplifying a logic corresponds to
evaluation so you might expect that
logicians had actually looked at
evaluation models because they were
looking at ways to normalize proofs and
indeed they're things like the craven
machine which you could consider as a
discovered way of evaluating lambda
calculus any other questions all right
then please help me thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>