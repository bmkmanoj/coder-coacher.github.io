<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Function-Passing, A New Model for Typed, Asynchronous and Distributed Programming - Heather Miller | Coder Coacher - Coaching Coders</title><meta content="Function-Passing, A New Model for Typed, Asynchronous and Distributed Programming - Heather Miller - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Function-Passing, A New Model for Typed, Asynchronous and Distributed Programming - Heather Miller</b></h2><h5 class="post__date">2015-11-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/QvJSOv5TJSk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so today I'm just going to talk about
something that isn't actually research
that I've done during my PhD it's more
it's a project that I was working on for
fun here and there it could be the worst
idea ever it could be an interesting
idea I don't know but it's I think a fun
idea and it applies basically two topics
it brings together two different topics
that I worked on over the course of my
PhD one is a serialization actually so
how do I provide better sort of language
integrated support for serialization in
a way where it's a performance and be
more errors are caught statically so I
don't want to be you know finding all
kinds of runtime class cast whatever
errors due to serialization and and also
very importantly this project this this
thing that I had worked on was actually
quite flexible so while things happened
at compile time it was all based on type
classes and statically composing you
know instances of these type classes and
bringing them together and to make these
Pickler Combinator things but because it
was so you know composable it was easy
to actually also change the format so
how these things are represented so I
could actually statically generate code
that would you know and swap I can
easily swap into formats so I could I
could statically generate basically
binary you know basically binary message
messages in line you know where they're
used at JSON things like this right so
it was quite performing at the same time
was quite flexible because it allowed
one to change how how there's how their
what they wanted to serialize was
represented and also you know well in at
least two different ways one in the
format and also to in sort of
customizing the actual object that you
wished serialized so this was a cool
project and and in the second project
that sort of come bring comes together
to make this cool idea that I'm going to
talk about today are these these funny
things called spores so we all many
people maybe know that that closures are
a little bit tricky to serialize
especially in an object-oriented
language like Scala so we had this this
solution called spores which I'll cover
in more detail soon but the idea is that
these things could then be you could
basically guarantee statically that
these things would be serializable oh
so together we were experimenting with
these two to his two things the
serialization framework that you know is
a little bit faster and can give you
some static idea of whether or not
something can be serialized and along
with this the serializable functions to
build a interesting maybe weird little
programming model that that that that we
hope one could build some kind of more
functional distributed system on top of
and and just due to the design of this
thing we we think that this model
actually simplifies things in a number
of ways because you know due to the fact
that it's mostly persistent to the
immutable fault tolerance becomes a
little bit easier things like in-memory
caching becomes a little bit easier and
because the model by default pushes more
types throughout more layers of the
stack we think that debugging becomes a
little bit easier as well but like i
said i'm going to say once more and then
maybe four or five ten more times that
this is a fun little research project so
everything is currently under
development and changing and it's an
idea right now it's not like the
greatest thing that existed it's just a
project so the fundamental idea it's not
it's not you can kind of think about
this as if it was an inversion of the
actor model but not really it's a good
it's a good way to kind of think about
it to start right so how would you how
is this possible how am I going to be
doing this the idea is well if you think
about actors you know they encapsulate
both state and behavior and they stay in
one place typically they don't always
have to but usually they stay in one
place so you kind of have this message
handler that deals with all of the
different behaviors that you want you
know to be applied to some state and and
you can think about these actors as
exchanging data and commands through
asynchronous messaging this model which
right now the the operating name is
called function passing it's actually
stateless and it's built on persistent
data structures and our idea is keep the
idea that the data actually stationary
unlike actors and and to instead
exchange the the behavior so send around
functions over the network through
asynchronous messaging and this is no no
new model for concurrent processes or
anything like that it's just a new new
different way we think of dealing with
tributed data in a functional way
actually I usually like to bring up this
this note on distributed computing
because you know they 10 years ago are
actually more 11 years ago they they
bring up these ideas that well you know
the idea of remote objects were bad
because differences latency memory
access partial failure and concurrency
make make merging these local and
distributed models both sorry local and
remote models both both you know as they
say unwise and unable to succeed right
and they say that instead a better
approach is to accept that there are
irreconcilable differences between local
and stupid computing and to be conscious
those differences at all stages of the
design so rather than trying to merge
these local and remote objects and
provide one interface we want you know
we in this paper 11 years ago they
argued that engineers should be able to
be constantly reminded of the
differences between the two and know
when it's appropriate to use remote or
distributed our sorry I'm sorry local or
remote objects and and the reason why I
bring this up is because this is
actually a sort of seed for the ideas
that were in this in this in this model
we use the idea of a function
application and you know accumulating up
function applications and sending them
over the wire using like laziness or a
delayed application as a way of using
the network or at least you know using
remote objects so this is actually in
the programming model so you might ask
ok well what does it look like if I have
to draw a picture for it of it well like
I said sorry before I do draw the
picture that there are two concepts like
I said earlier stationery immutable data
and these functions that you can move
around you can move to the data that are
on different machines so we call the
stationary mutable data for now silos
and we call I guess it for a lack of a
better name and we called the ability to
move these functions around spores this
is the name that we got from from this
previous work that we had done earlier
so I'm going to start by describing what
the silo things are then I'll go on to
describing a little bit about what these
four things are but if I have to start
with a picture you can think about the
style I know it's a little
this little bit dark in here I'm sorry
the slides are a little bit dark but you
know I hope you can see so the idea is
you can think of these things as as
having two parts so a silo is actually
the piece of data it exists somewhere on
a different machine typically or on the
same machine as this other thing that
points to it called the silo roof so if
you're familiar with something like acha
you have these actor refs they usually
point you know this is what you operate
on this is your handle to your actor the
same is true for these the silo things
so the idea is that you have a reference
that points to the style of thing you
actually can't operate on the silo and
the silent reference can be you can
actually operate on the silo you know as
it is you you operate it on it always
through the silo ref and this Tyler f
can be on many it can be on any machine
it can be on the same machine as the
silo or on a different machine as a silo
and you can make these interesting sort
of directed acyclic graphs that
represent interesting computations and
you can build them up on one machine
have them done on another machine and
they can be one of these these these
sort of trees can can actually span
between different different machines so
they all don't have to live on the same
know they can be broken around broken up
across different notes and there are
three main operations on the silo f
things there's something called apply
send and flat map so like I said this
I'll RF is the handle to this this the
silo thing and right now I'm just for
the sake of I know it's really hard to
see I'm very sorry but just for the sake
of this example I'm going to to we're
just going to pretend that they apply
takes two functions or to force rather
than just one in reality it takes just
one but like I said for illustration
purposes to is going to be prettier to
look at and it returns another silo ref
so it returns a reference to another
possibly remote piece of data oh yeah
it's exactly what I said what's very
important though is is that these things
are are this the supply method is lazy
so when you use this this thing is used
to build up a graph of computations that
you can then later send over the network
so it defers application of the function
to the silo and then returns the silo
ref with info on how to later
materialize that silo somewhere else sin
is the eager operation that actually
forces the thing to be sent over the
wire so this is this is basically the
handle for network communication so and
it returns a future so whatever the
result is of this function application
is it's complete it's a future that's
later computed and flat map of course so
sometimes need to move data around and
this flat map function this flat map
Combinator exists so you can send a
spore to a silo and the result of
applying that spore could create another
silo ref on that machine so it's a wave
of armed sorry on that node so I'm sorry
as a silo rep so it's a way of basically
moving data around and just like just
like the apply method it also returns to
Tyler as i said yeah i just went through
that so here's another here's here's a
maybe a little bit more detailed example
so I'm going to just iteratively get a
little bit more detailed with these
examples but I they're all still very
basic so let's just say we want to
create a new new silo and a new sila ref
if I have some Sylar f of type T that
points to an existing silo of type T I
can apply this function and then put you
know apply this the spore and and then
send which forces it to beam it forces
the the function to be sent over the
wire to this silo applied to this this
silo on the other end and for a new silo
ref to be on machine to over here on the
machine on the right a new style RF to
be created before this whole thing is
actually applied and a new siler was
computed I have a reference I have this
new side of the ref over here so over
time as this thing is is eventually
materialized I could still do stuff to
the silo roof of type s and and it's a
handle it doesn't even have to exactly
point to something that already exists
eventually the this this this graph will
be completed
as these things are materialized so
again it's also important to note that
these things can be on on different
machines you don't have to have all you
know just one machine for or you don't
imagine you don't have to think about
this model where you have one node that
deals with all the style of rafts and
another one that deals with the style is
this just for the sake of illustration
you can break these things up in many
different ways now the other the other
little important bit that I'd like to
mention is spores well what are these
things exactly this is a research
project that that I had worked on
basically last year and you know it's
all about trying to solve this trouble
that we have with closures especially in
a language i scala but not limited to
languages like Scala it exists across
all different sorts of languages and
paradigms so I'm just going to enumerate
sort of some of the issues that that
that closures have especially when
operating within a distributed or
concurrent environment so in scotts
especially in languages like Scala it's
possible to accidentally capture non
serializable variables like this this
this in closing this reference so if i
try to serialize something that
accidentally captures that and if the
enclosing the type of the enclosing this
thing is not serializable then you get a
very confusing runtime error typically
they're also the number of compiler
specific translation schemes that can
create an implicit references to objects
that are also not serializable so you
write some code and compiler translates
in a way that you didn't expect and
there could be an implicit reference
that you didn't realize is there which
also causes a serialization error and
then there's also the possibility of
having transitive references that might
inadvertently hold onto excessively
large object graphs which of course can
create memory leaks nobody likes that
capturing references to mutable objects
we can do that in Scala because Scala
you know is basically Java with lots of
functional features added to it right so
I can capture a reference to something
that's mutable and have all kinds of
race conditions happen in a concurrent
setting and also a knowingly 11 major
problem which doesn't seem like a major
problem is unknowingly accessing object
members that are not constant so certain
things like method calls which in a
distributed setting can have logically
different meanings on different machines
these are actually this is actually
matic and to give you two quick examples
of some of these these problems so this
is something that does happen in spark
so I don't know if anybody is familiar
with spark it's basically it attempts to
try and improve on you know how do piÃ±a
being a lot nicer to look at and being
more concise and then be doing things in
memory so it's a lot faster if anybody
has anybody seen or used to park our
people familiar with it okay cool so you
can think of so in spark this rdd thing
it's basically like a distributed
collections like a distributed array so
if this array is too big to be a number
on one machine you can have bits and
pieces all over the place you treat it
as if it's it's one array and you can
call a number of high order functions
you can use them you can there's a
number of higher order functions that
you can you have access to like map
which of course applies the same
function across all shards of data on
your on your on your cluster but the
problem here actually is is in the
function that we passed to to map so we
have this enclosing class is my cool RDD
app which as you can see it doesn't
extend serializable for going to use
java serialization and it captures this
/ m thing here so this closure perhaps
is this pram thing here and actually
this this in this case this whole this
you you end up spark ends with crashing
with a not serializable exception
because my quill RDD app is not
serializable so this is the normal
problem that looks totally innocuous it
looks totally fine but of course when
you try to run the code it doesn't work
a logical problem that sometimes happens
it is well at one point it was happening
in acha basically the idea was if you
mixed actors and futures sometimes you
had this funny problem this phonological
issue due to the fact that futures are
our closures basically what would happen
is so this is a message handler and this
is a certain well this is um sorry this
is this received message and there's you
have this this request this handler for
this request thing and let's just say it
takes really long time to compute
whatever should be done to this request
message type right so of course being a
smart program and we're going to wrap it
in a future because that throws it off
to basically usually another thread and
then it takes some time to compute over
there and when it's finally done then we
finish oops then we then we you know we
basically if we put it in the future we
can we can free up the message the actor
to move on to the next message so what
happens is that this immediately returns
before actually this code is executed
and the actor goes on to handling
another message so what could happen in
this case is that let's just say there's
a this in this example after we compute
this long-running result we want to send
another response back to the person or
to the to the actor that originally sent
this message so what can happen in this
situation is that since this this code
is executed later somewhere else the
meaning of the sender thing can change
so if the actor moves on to processing
another message sender is now someone
else so if if Mary sent Tom so this
actor is Tom if Mary sent this this this
message that we're processing now and
says ok send me send it back when when
we're done and then we throw this off
into a future and then we start handling
a new message from from Bob basically
what can happen is when this is finally
computed this actually means Bob now and
so Bob receives the message that Mary
should have received so these are
capturing issues basically so spores are
a closer like abstraction and a very
lightweight little type system designed
for use in these kind of environments
and really the overarching goal is
well-behaved closures that can control
environments and avoid various hazards
and this is this is basically achieved
by enforcing a certain syntactic shape
which dictates how the environment
should be declared of a how the
environment of a function or a closure
should be declared and then providing a
little bit of additional type checking
to ensure that types being checked have
certain properties like serializability
for example and and it's really the most
important point here is that spores
encode extra type information
corresponding to the captured
environment in their type and so this is
the really ugly longhand ver
version of what these things look like
the basic idea is that there are two
parts to these poor things so first you
have this kind of looking this keyword
looking thing but within it you have
both the this first part which oops
sorry which is a sequence of local value
decorations so in this case is just one
and this this represents the actual
environment of the function this is a
I'm sorry of that that this spore is
allowed to kind of capture and carry
around and then you have the actual
actual closure itself and and basically
the rule is that this thing is allowed
to capture anything in its header but
nothing else so if it reaches outside of
this this this scope then you have a
have a problem and you fail to compile
that's very simple little syntactic rule
it can actually so like I said this is
the long hand ugly version but it can be
made more concessional show you in just
a second how but what these what's nice
about this is that it requires all
captured variables to be declared in the
sport header so you can't accidentally
capture all of Wikipedia by calling
Wikipedia at length doesn't actually
pull in Wikipedia it actually computes
the length and then captures that and it
to like us yep so the initializers of
the captured variables are then executed
only once which is upon creation of this
this functions for things and and then
references to captured variables also
another benefit is that the reference is
to capture variables don't change over
the course of the sports execution there
are a number of rules about what can be
in this this header thing here so of
course you can't have VARs so references
that can change over time nothing can be
lazy up there and that just means that
your environment is kind of effectively
immutable and it's also important to
note that the evaluation semantics are
exactly the same as if you didn't have
this this sport thing in there at all so
if I remove this poor thing here these
two pieces of code are evaluated exactly
the same I don't change I don't do any
any writing rewriting at compile time I
just do some extra checking and like I
alluded to there are some there are
these coercion from this this this from
normal closures to sports so it's so
it's so it's it's it's easier basically
to include these things in existing
api's so
shun literals can be in regular
functional roles can be implicitly
turned into spore if the function
literal supply satisfies the rules where
which means it doesn't do all kinds of
extra capturing oops so just to
summarize a little bit so this is quick
the highlights of these poor things but
just to summarize some benefits of these
things is that the environment these
captured variables is declared
explicitly and due to these these rules
where these things can't be changing
over time it's basically fixed at the
time of the spore is created so it's an
effectively kind of immutable
environment and it's possible to
statically ensure that everything these
poor things because they you can also
ensure i didn't show you exactly how but
it's possible to statically ensure that
everything capped is captured has a
certain property for example being
serializable so i can attach something
called a property to these things and
that makes it possible for these things
not to be so dangerous when they're
being shipped around the network and
also to make sure that everything that
they capture reach out and do it's
always serializable which is of course
very nice if you're trying to send these
things around the network there's a
paper that we published last year and
also you know with with the actually
more with this with a type system that
goes further and talks about excluding
certain types and things like this but
what may be more friendly to read is the
scholar improvement proposal that we put
together on that on the Scala
documentation website okay so those were
the two pieces like I said there was
this these the silo things these these
data containers with with references
that could could point to them called
sila refs and there are these these poor
things which I know I can serialize and
center on the network what kind of
programs can I build with these things
so I have a cute little example and it's
very very limited because it's actually
really hard to visualize these things I
noticed so let's just pretend that we
want to implement a little distributed
list a little spark like thing that has
you know two operations map and reduce
so this is to just give you some picture
of how things would be looking
underneath the hood if you if I already
had the logic for map and reduce and I
wanted to basically do a little Matt Map
Reduce on on one of these distributed
lists that I've created so this is again
all happening kind of on two machines
here but you start by using this apply
method and and in these so these are all
silo refs so let's say I have a base
piece of data a base a base list that I
want to then apply some transformations
to so I can use the silo ref thing I
already know what the logic is for you
know this distributed kind of map thing
and this is this F would be what the
what the what the user supplies so this
would be the the function that we would
pass to map like I said this doesn't
have to be two separate things this can
all be one but just for the sake of
illustration imagine that there are two
and let's just say I'd like to do a few
more I'd like to you know apply this
function f I don't know multiply
everything by 2 and then just reducing
some everything right so this is the the
logic that I would like to build up and
apply and I can I can do this over time
and I can build up this dag and also
like I said it doesn't have to be just
one chain of operations I can also take
if I knew the reference to this thing I
can start applying stuff from this node
here I can make my start you know using
a different map or or whatever else I
could build a I can build a dag that
also stands different between different
machines but what's important is that
now let's just say i would like to
basically do this reduce thing i have to
use this this eager send operator here
which whoops which at this point
assuming that this that this base piece
of data is already materialized what
descend operation does is it forces
basically all of this stuff to be
applied together okay and sent over the
wire so this arrives wherever this piece
of data lives
and I know that I have to apply these
things I can also get rid of
intermediate data structures there's
ways to do that but for the sake of this
example I'm just going to say apply
these these functions one by one okay
and I get these intermediate silos
materialized which all reference each
other and also make another kind of dag
and when this is done I've created so as
you as you saw so this is a map map and
reduce so i end up with you know several
intermediate data structures one
representing a list one representing a
list and then the result of this reduce
is just as a silo of integer and because
San returns a future when this thing is
eventually completed and this integer is
eventually available it's it's basically
sent back and this this this the the
type the return type here is a future of
int so if I had Val you know all of
these operations together the return
type of all that whole that whole
expression is going to be a future of
int and this eventually gets completed
with whatever is computed remotely and
return so you might ask okay i mentioned
earlier that that you know this could
help you know in sort of fault tolerant
situations well it makes fault tolerance
pretty easy because silos and silo refs
relate to each other by the means of
this persistent data structure so you're
doing everything as a as basically a
transformations on a dag and you're
basically applying functions to data and
you're leaving the data where it is and
actually this dag of function
applications represents basically all
the computation that your that you want
to do to this distributed data structure
so you can from from the land of
databases you know you basically have
these things called lineages and so just
traversing the silo ref data structure
or even the silo data structure you can
complete you can you can end up with
some one of these lineage things and
since the lineage is composed of these
spores that means you know that you can
serialize them you
and that means that you can you can
persist them on the machines that
they're on or transfer them to other
machines and you can come up with all
kinds of different strategies for
handling failure okay so I mean that's
the basic idea so you might ask well
what's the status we have a we have an
implementation of this model it's a
little bit more it's a little bit richer
than I showed to you I don't actually
think I put the URL up there maybe it's
on the last slide but it's the project
the whole project is just called F Dash
P it's a horrible name I know given that
it's the same as functional programming
sorry but this is the working name of it
now but on the sporah side of things we
had a limited prototype implementation
until februari of 2015 that wrap
actually changed quite a lot since then
we've released several times we have
expanded test suite and we have a new
module called spores and pick spores
dash pickling which basically brings
together both the the spores the spores
as I describe them to you and this
pickling framework so that you can
ensure that spores are pickable if
that's somehow clear and you know we
have a lot more implementation
experience since we actually wrote this
sip and we we plan to update it soon but
if you would like a more detailed
description of what is allowable in
these spore things you can you can find
that here it's on the on the doc website
for these these things are called sips
they're scholar improvement proposals
that's that's listed there so that's
that about covers it so this is like I
said it's just an idea it's something
that we're playing with and to summarize
kind of what it is it's the idea is that
you have these data the data and these
things we call silos managed using a
persistent data structure that can span
different machines and all operations
including the ones that we provide by
the earth that could be provided by
prospective system builders are spores
so everything is serializable which
means it's easy to keep track of what's
been done and to build up a record in
case you have to recover from some kind
of failure so we think it would be
easier in a situation where you can make
your problem mostly immutable or you
have a way of
computing from some base piece of data
that it would be simpler to build
mechanisms for fault tolerance oops but
that's that's about it so if you guys
have questions I'm happy to to go
through them Thanks oh wow sure I will
do my best to actually do that
yep good sure oh yeah sure so right now
I don't do anything the so that I said
this was actually discussion that we've
had many times before so let's just say
that you have a you know something where
you your it's a long-running computation
and you just everyday maybe you create a
new node whatever like a new silo on the
same machine by just default by whatever
your job is so if I compute something
like three weeks ago and every single
day I make a new one of these big silo
i'm sorry i just realized i didn't
repeat the question i will try harder
next time so you know you could have
many many of these intermediate data
structures that are no longer necessary
but the only problem with that is then
that if you start if you just look at
this like a persistent data structure
that evolves over time if you start
charting off pieces of it then it would
be difficult to ever then you have this
nice guarantee where you can rerun
computations on give just given some
persistent piece of data that you start
with so it's actually like a fundamental
disagreement that I keep having with the
my co-author basically you know what we
should do about this I think that we
shouldn't do anything about it my
opinion but not his that we shouldn't
really do anything about it and since we
have the ability to just compose
functions together and kind of stage
things yourself you can get rid of
intermediate data structures yourself
maybe not if your if your if your
computation is a job that you know
computes a note every day or something
like this but you know in a situation
where you don't you can you can
basically get rid of senseless
intermediate data structures by doing
your own kind of staging by saying okay
well I don't need to go apply map seven
times I can basically compose these
things together and apply map once then
you can get rid of intermediate data
structures and save memory by yourself
but yeah this is basically a design
discussion at this point exactly what to
do I it's in debates phase if you have
an opinion on it I'm happy new year is
it huh anybody else
okay you begin with classic words you
need to think about what's going on
think about is how you minimize
communication and one standard way of
doing that is keep all the data in one
place and send stuff to it tip so what I
really like you're doing it plays that
spray yep yeah that's cool I'm happy you
like it I can repeat the observation the
observation is that one way of so I'm
probably going to get this wrong so you
can of course interject if I if I didn't
rephrase it correctly but and that you
know one way of dealing with this
problem that I brought up earlier about
the difference between remote and local
computation is you know you can always
kind of reduce this problem by keeping
as much data in one place and this model
sort of fundamentally kind of encourages
that or does that by default a little
bit and requires you to move data
specifically if you if you need to via
this flat map thing actually is that
about right or did I get it wrong okay
right so it allows people to
fundamentally tackle this this design
problem where we're to keep the data
just to repeat it for the remote yep
yeah so this is what I was kind of
discussing just sorry I'm so bad at this
so the issue so is there an issue with
garbage collection when you basically
end up in a situation where these things
are creating silos remotely you could
have many of them for example on one
machine that you know makes you run out
of memory and everything crashes it's
horrible so the answer is yes this is a
problem but then again the the issue the
basically that the trade-off here is
while we allow you to stage computations
so you can use the supply thing to kind
of I mean in this example that I showed
you I made basically an intermediate
data style for each for each apply node
but there are ways to basically compose
all these things together and do
basically come up with one one node so
you can compose all your functions and
then send them over the wire and do your
own kind of staging and reduce
intermediate data structures so that's
one solution it's manual though the the
benefit to this solution is that you
don't have the product you can always
sort of assume that you can safely
reconstruct your entire computation from
some piece of persistent data and don't
have to worry about pieces from last
month going away because of some kind of
garbage collection on your on your on
your persistent data structure so it's a
fundamental sort of I don't know
discussion right now about should we do
something about this and there's this
trade-off for basically you kind of lose
the ability to do this nice fault
tolerance if you start removing pieces
of your of your of your of your data
structure if you start moving some of
these silos
so the question is like what is the
reference if this reference is somehow
deterministic then actually oh oh you're
asking what the silo reference thing is
well I mean this thing in its
implementation it's it's really very
similar to these actor reference things
that we use in Anaka and it really just
contains kind of look like information
about where this thing is and it's a
little object that has all these methods
that you can call to queue up all of
these these these operations so you make
a little dab with these kind of cellular
f objects it really just contains
information about where to find the
actual data that's all there is to it
yep so the observation is this sounds
really similar to spark so what are the
differences it's actually so you're
right that you have something and I can
call like map or reduce on whatever that
thing is and it creates this lineage
like thing that's that's where it is
similar to spark spark is an enormous
distributed system which you know is
like this big and my this thing that I'm
proposing is something that's much
smaller kind of like a middleware that
would be actually more useful underneath
something like spark so if this model
existed before spark existed this would
actually be useful for a number of
problems that exist in spark namely the
capturing issue and also the fact that
everything is typed you have typed
communication so I was actually at data
bricks for a little while trying to
solve some serialization problems that
they were having and if I had just a
little bit of type information anywhere
in the middle of this enormous stack of
code that they have there could have
been so many things that could have been
improved but just because it's an
enormous stack of code with many
different people with different sort of
typing disciplines all working together
you have different parts of the code
base that wander away from this typing
discipline and this this model would
enforce basically more levels of the
stack to be better typed in my opinion
and also a lot of communication errors
trying to figure out Oh some actors sent
the message to somebody and the whole
system is hanging and we're wondering
why it's because that guy didn't
actually receive that message and we
didn't you know he didn't her another
guy received it and there was no message
handler for that type and everything
just waits and times out right so it
basically is something that you could
build something like spark on top of but
not spark at all yep
okay so to summarize to try and
summarize your question you said okay in
this example there was just two machines
and what if I had these things spread
around more machines what about
reductions right is this all handled by
the silo refs right now yes that I have
to say that this is part of the
implementation where we're doing this
bookkeeping right now for you so we yes
so okay it's a little more difficult to
explain we don't provide much more for
you beyond just that but in the example
that where i made like this little
distribute collections thing there's a
lot more than just this little picture
so there is bookkeeping that has to be
done about how to do these reduction
trees you can do them with silo refs and
this is some bookkeeping that we we had
to implement which was not fun in this
layer that we built on top of this
little model so that's in the lair of
the thing that says what reduced means
because we give you send we don't give
you reduced you implement reduce
yourself as much as I just said assume
that you have a reduced operation and
the reduced operation has to figure out
how to do a reduction tree given the
silo things yep okay so i think that's
it i think right a time thank you
everybody for coming and asking good
questions
you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>