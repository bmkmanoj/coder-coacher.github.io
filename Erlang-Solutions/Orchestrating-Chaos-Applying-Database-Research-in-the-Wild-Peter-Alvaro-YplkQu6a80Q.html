<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Orchestrating Chaos Applying Database Research in the Wild - Peter Alvaro | Coder Coacher - Coaching Coders</title><meta content="Orchestrating Chaos Applying Database Research in the Wild - Peter Alvaro - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Orchestrating Chaos Applying Database Research in the Wild - Peter Alvaro</b></h2><h5 class="post__date">2017-03-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/YplkQu6a80Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thanks for coming my name is Peter
Alvaro I'm gonna be talking today about
some work that my colleagues and I have
been doing both in the lab and in the
field applying some somewhat old ideas
from database theory to reasoning about
resilience in particular reasoning about
whether our large-scale fault tolerant
distributed systems actually tolerate
faults and about as recently as six
years ago I used to begin talks on the
subject of distributed systems with this
sort of prediction about the future I
would say you know in the future in the
near future all systems are going to be
distributed right and that's that's
problematic because even though we've
been working on distributed systems for
some thirty some-odd years we haven't
really figured out yet how to program
them or or had a reason about them and I
think it's it's easy to it's easy to
argue now that we're already squarely in
that future in which pretty much all
non-trivial systems are physically
distributed that is to say that they
require the cooperation of some
significant number of machines to
fulfill their function and also
heterogeneous and what do I mean by that
I mean that distributed systems nowadays
tend not to be monolithically
architected entities right they tend to
be cobbled together from a variety of
components components that may be
exposed different guarantees right as
we've known for at least 30 years
distributed systems are sort of
devilishly difficult to reason about in
large part because of the uncertainties
that they expose one of the sort of key
uncertainties is uncertainty about what
subset of our compute components are
going to fail mid computation and the
and and the ensuing difficulty of
reasoning about the sort of correctness
and completeness of our computation
despite these partial failures and some
time ago maybe as recently as ten years
ago there was a great deal of confidence
that we could sort of relegate
responsibility for solving these really
hard programs to a rather small
priesthood of genius programmers right
the library writers right we could get
the people who really understand
concurrency and really understand you
know partial failure you know the Erlang
people and we could get them to you know
write lib Paxos for us and Lib right
ahead log and then we could just get on
with our lives
but do in parts of this sort of
aforementioned heterogeneity of modern
large-scale systems even if we're lucky
enough to to have a collection of
it's that are correct written by
geniuses individually verified um as I'm
gonna argue in this talk fault tolerance
is really an end and property of an
application a property that doesn't
necessarily hold under composition right
so taking a bunch of correct components
off the shelf is only half the battle
right so in the limit right it's kind of
ugly and so I'm gonna provide motivation
for this perhaps somewhat controversial
claim that fault tolerance is a is a non
compositional property and I'm gonna
draw that motivation from two different
places in the stack we're gonna kind of
start at the top of the stack and look
at this emerging world of applications
built on top of micro service
architectures right everybody's doing
micro services nowadays it's it's the
cool kids are all doing them right and I
think like there's a variety of reasons
why people think this architecture is
great but the people who think this
architecture is the greatest are the
managers and the product people right
because it enables really agile rapid
releases releases where individual
services are decoupled from the other
services right it wins because it's
productive but it also wins because it
provides a really nice sort of
complexity hiding abstraction to the
programmer much in the same way that the
actor oriented programming model does
right it allows programmers to focus
their attention very narrowly on the
correct behavior of the little people in
the computation the individual services
by putting on our blinders
you know we focus on a correct service
and of course the dream is that we get a
correct application by composition of
course if we're gonna write a correct
microservice a little white box that we
understand very well we can't be
completely solipsistic we need to know a
little bit about our immediate
neighborhood right because a
microservice depends on other micro
services into which it calls to to
derive results and combine them to
provide results to its caller right so
we have a white box but it it sort of
needs to call into a bunch of grey boxes
we need to reason about how the our
immediate dependencies can fail because
it's gonna be our job to mask those
failures to the services that depend on
us right so there's a large variety of
sort of best practices in the micro
services world for for ensuring that an
individual service can tolerate Fault in
its dependencies right so we we do the
bulk heading pattern we do the circuit
breaker pattern we
founded retries we do we do reasonable
defaults and so on and so forth right
and of course we additionally think very
hard about the API that we publish to
our callers but in a real world
microservice architecture we know very
little about who the heck is is calling
in to us in fact to share a little
anecdote from some contract work I did
recently at a large micro service
company there was a sort of active
project at the time that I was there
mining console logs in spark to try to
determine the global transitive closure
of this calls graph which is baffling to
me having worked in industry something
more than ten years ago where there
would be like a there would be like an
architect right who had the picture of
the call graph in their head and like a
micro services world the things are
evolving so quickly but nobody knows
like what the fuck is going on
and yet the site is and yet the site is
running right so it's this is kind of
the view from the point of view of a
program of a micro service we have a
white box that calls down into some grey
boxes and it's called into by some black
boxes and provides some contract to them
but this is not the view of micro
services from the point of view of a
client or an application running on them
in that case the view looks more like
this right a request comes into the
system to some root node and it sort of
fans out in a non-trivial way across
these services and it's easy to argue
that like from the point of view of that
client this this interaction fault
tolerance of this interaction involves
reasoning about the totality of the
interaction and if all these boxes
uphold their SLA s that doesn't
necessarily mean that that user has a
good experience right in the limit you'd
sort of need to see what happened for
every contingency for every combination
of those things that could be working
incorrectly does the users experience
always work right which is obviously
intractable so we'll come back to that
and the intractability of that of that
sort of testing problem in a moment but
before we do I wanted to provide a
little bit more motivation for my claim
about non composability of fault
tolerance from much lower in the stack
from the world of protocols which is
like a world that I think you all are
much more familiar with and I wanted to
take a look at a fairly well-known bug
that was reported back in 2013 2014 by
Kyle Kingsbury on his blog call me maybe
he was studying the Kafka reliable
message queue and in particular the
replication subsystem the replication
protocol used by Kafka
now this bug was a surprise to many of
us in the community because Kafka's a
great system written by really smart
people and sort of done right it was
built out of three
very well understood classic even and
into cases individually verified
components right so primary backup
replication a primary replica does not
acknowledge to a client that a right is
durable until it gets acknowledgments
that that right has been terribly stored
on all of the replicas and this means
that such a system can tolerate the loss
of all but one replica without violating
a durability guarantee right very nice
of course some replicas might be slow
they might be limping along due to
problems with their hardware and so we
need to be able to make progress in the
face of slow replicas so it's another
classic technique to use time-out base
failure detectors if somebody's not
keeping up after a while you take them
out of the replica group and you do a
reconfiguration you get on with your
life totally reasonable right and of
course to make that reconfiguration
strongly consistent and atomic and so on
they use zookeeper a strongly consistent
metadata store to store and propagate
that view of who is the current set of
replicas and among them who is the
leader all totally reasonable right so
three correct components you go on them
together into a slightly new protocol
that you might be able to argue is
basically primary backup right and a
really nasty bug emerges in which rights
which were acknowledged at the client
our loss forever despite the fact that a
replica group of three replicas suffered
only one crash failure man that
shouldn't be right and so may be blowing
the punchline I specified this
replication subsystem in an acute little
language called Daedalus and I passed it
into a verification tool called Molly
that I'll be talking about a little
later in this talk and Molly told me the
following it said well you know given
the protocol you told you you described
to me there exists this counter example
to the durability property and the
counter example unfolds in the following
way
due to a brief network partition that
isolates two replicas B and C from a
replicas group of three ABC after a
brief exchange of control messages
replica a becomes the leader arguably
that's the correct thing to happen under
this circumstance but it also becomes
the only replica
in the group it's a singleton replica
group arguably not the correct thing to
happen and a right at that critical
moment a client issues a right to the
primary and the primary note in full
compliance was primary backup doesn't
acknowledge the client until that right
is durably stored on all the replicas
unfortunately he is the all the replicas
and so of course at that point he
crashes and the data's lost so we had
one crash the data's lost man how the
fuck did that happen right so maybe a
better question to ask is how do we make
sure that stuff like that doesn't happen
like nowadays
let's see some me we kind of nowadays
have two ways of solving this problem
and there those represent kind of
extremes on a spectrum right one is to
like roll up our sleeves and do formal
methods right like let's model check
this thing explore the state space and
sort of assert that all of the states
that we can reach are good right we've
learned we learn Pro melih right we
learnt ela or something like that
this approach isn't adopted very much in
practice as I think you all know for
reasons both superficial and fundamental
the superficial reason is that these
tools are really hard to use you know
who is like a steep learning curve and a
lot of programmers will have a lot on
their plates or unwilling to put aside
the nine months it might take to become
an expert in CLA but the assumption is
that if they did right you would start
seeing some returns and that would be
great but that's superficial we can all
become experts in whatever tool right
it's just a matter of how we spend our
time the oh my slides are out order
that's okay but the more fundamental
reason that model checking isn't done in
practice is this compositionality
problem right it's like you can't just
model check the components and put them
together and hope that you have a
correct system because the glue you use
to put them together and the config you
provided them with is probably where
you're gonna fuck up so the fact that
they're good components of doesn't help
the other alternative would be to put
all the components together and then run
a model checker that will never work
because of the state explosion problem
if the system that we're studying is
non-trivial early large okay so what's
the alternative
well the alternative is approach is
based on testing right you put the
system together and then you run the
system and see what it does and if
you're smart you may be like the people
at Netflix do you perturb the action
soon as you go so you run your
integration test over and over again and
you inject random faults and little by
little maybe as you inject random faults
some bugs come out and this is really
attractive to programmers right because
unlike this it's sort of a it's sort of
a pay-as-you-go investment the more time
you put into writing your assertions the
more time you put into running these
integration tests the more bugs you find
but as a curve starts to level out
you're faced with this question okay you
know how long do I run this thing for
what am I done right it's like the hard
bugs you're unlikely to find randomly
stabbing into the space of failures as
I'm gonna argue now okay so let's talk
about that space of failures remember
that micro-service graph that let's just
imagine that it had about 100 services
when I did a contract two years ago at
Netflix we studied for some time an
interaction that I'll talk about later
in this talk called app boot that
involves 100 communicating services so
you ask yourself how do I get confidence
that this interaction works correct for
everything that could go wrong well one
answer could be you you see what it does
for everything that could go wrong the
problem with that is is that there's two
to the 100 things that could go wrong
right every and the set of all subsets
of those services right and so you maybe
you're on ambitious maybe you just care
about like what does my thing do the
right thing if any single service fails
and that's tractable maybe it's 100
executions let's say these integration
tests run in the order of minutes so you
could do that but if you turn it up to
four faults you're looking at three
million if you turned it up to seven
faults you're looking at 16 billion like
this obviously isn't gonna work you're
gonna need to have some strategy to
intelligently search this massive
combinatorial space right then the
approach that chaos monkey took which is
a damn good approach is to do it
randomly right nice thing about random
is it works equally good on any
infrastructure a problem with random is
it doesn't work particularly good on any
infrastructure random testing looks like
this I'm gonna like run my integration
test and as I run my grace tells me to
throw darts into this combinatorial
space some of the darts go deep and I
look at you know whatever one two three
four combination some combination of
four failures
some of them go shallow and sometimes
you get lucky and find bugs right
there's a lot of bugs in there to find
right so you know you get pretty lucky
but you know what if I said there was a
pin out here what
you're gonna split that pin with one of
these things and give up any hope of
like making this dartboard black/white
just jumping into it a million times
right the Sun will be burnt out this
isn't gonna work so what else can you do
well companies like Netflix and other
mature companies like them tend to do a
combination of random search and what I
might call like a heuristic or engineer
guided search and that goes something
like this the chaos engineer walks
around to the different teams that build
the services and they say tell me about
your service what are the dark corners
where do you think it could go wrong how
can I push it into an edge case right
and then I ends up looking like they
kind of build a model that the main
knowledge and you do a sort of engineer
guided search through this space going
sometimes deep sometimes shallow and
achieving decent coverage now now the
shortcoming of this approach of course
is that unlike random it doesn't scale
at all it requires intimate knowledge of
the actual services under study it's
only as good as the intuition of the
domain experts and it only scales with
humans right so obviously what we'd like
is some kind of system that gives us the
best of both approaches that has like
that feels like testing to the
programmers but that gives us returns a
little bit more akin to what we might
find from formal methods right and I'd
like to advocate today for an approach
that my colleagues and I have been
developing in the lab for you know some
some years now to three years called
lineage driven fault injection
the idea being lineage driven fault
injection is that you know asking the
big academic questions what is fault
tolerance
really right fault tolerance as we know
is really just redundancy at some level
right like a system is fault tolerant if
it provides more ways of getting to some
good or expected outcome than we
anticipate there will be failures in the
execution and that redundancy could be
played out sort of in space we put our
data on many replicas or in time we make
multiple attempts to do something
mathematical redundancy in the form of
error correcting codes right redundancy
the program text in the case of K
version programming like I challenge you
to give me an example of a fault
tolerance technique that isn't
redundancy sort of stretched out in some
dimension right so so the trick would be
if we could recognize by studying a
systems executions like how it provided
redundancy we can understand the classes
of fall to the qatal array and the
complement right we could
understand like what faults it might be
sensitive to how it's redundancy might
not be adequate for its application
domain okay
that's the idea but how do you know
redundancy when you see it well the
approach of lineage Rafal injection is
to begin with lineage to begin with
traces of successful executions the way
I like to think about this is you know
instead of doing like what a model
checker does which is reason in a
forward Direction like begin with some
initial state or States and a transition
relation that's hopefully finite you
sort of take all the transitions and you
enumerate all the states and you say
they are all good right if the set of
states is too big to check all of them
that doesn't give us very much guidance
about how to carry out the search so
instead why not sort of start in the
middle of the maze and work our way out
let's start with some successful
executions and ask ok well why was the
execution successful and and what wrong
turns could I made along the way ok so
imagine that we have some way and I'll
give you some evidence later of how we
could have such a way of collecting
lineage explanations of how a system
produced its outcome so a storage system
like Kafka might have some successful
outcome like I did a write and it was
durably stored right so we asked ok well
why why is that predicates true well it
might be durably stored because it's
stored on some concrete replica replica
but there might be more than one reason
why it's durable it might also be stored
on some replicas B right so we have
evidence of redundancy providing support
for this high-level predicate that the
system did the right thing
ok we keep turning the crank ok but why
was it stored on replica a well replica
a because the client initiated a
broadcast that went to replicas ANP and
in fact it may be stored on replicas a
for more than one reason because the
client retried that broadcast after some
time out or something like that so you
see as we unroll this lineage we sort of
recursively ants ask and answer these
why questions we get a graph that
describes how we got to some good
outcome from some initial states and
then the question just becomes can we
devise you know like how could it not
have happened can we devise the seer a
set of cuts on this graph that like
intuitively breaks the flow to the good
outcome right and so for example to go
back to this problem of the
the combinatorial space there's sort of
four things that could have gone wrong
in this trivially simple execution the
replicas could have failed and the
broadcast could have been interrupted by
like say a network partition or
something like that so that's 2 to the 4
or 16 different experiments that at
least naively we would need to perform
to make sure that this thing work
correctly but looking at this graph for
a moment it's easy to convince ourselves
that many of those 16 executions are not
interesting for example the execution in
which we crash replica a and interrupted
broadcast 1 is not interesting because
it's easy to see that we our system
would have produced the correct outcome
anyway right we have evidence of the
redundancy that makes this an experiment
that wouldn't be worth wasting money
resources time performing and by
contrast though this execution in which
we interrupt both broadcasts looks
promising right because it breaks all
the flow to that good thing so it would
be worth using our infrastructure to
inject those faults and see what happens
right so ok so that's fine that's great
how do I convert that intuition into
something that is efficient for a
computer to sort out on their own one
approach is to sort of enumerate the
individual paths through that graph in
this graph because it's so simple
they're just paths in general they'll be
trees right enumerate all the individual
paths that are sufficient to produce the
outcome so here's one clients as a
broadcast it gets replicas hey that's
enough to make the right stable so we
can just kind of write that down as a
boolean formula describing what could
have made it not happen replicas it
could have failed and then that path
would be invalidated right or broadcast
one could have been interrupted and then
that that path would have been
invalidated ok but wait there's another
way to get to the good thing via
broadcast to an n replica a so we can
just kind of cons that up with an and
write we say well that path could be
broken if we fail replica a or broadcast
to and to break the outcome we would
need to break both paths so we can
connect them with an and right so we
just kind of turn the crank on this and
oh look there's another path and that's
another path and now we're done and at
the end of the day we've cons DUP this
boolean formula whose variables
represent faults that could be injected
right and and which just happens to
already be by construction in
conjunctive normal
which is convenient right we can pass it
to an off-the-shelf Sat solver and just
say give me the solutions for this
formula the solutions to this formula
are going to correspond to cuts on the
graph that are interesting for example
here's the solution that we saw a moment
ago right interrupt those broadcasts
that's cool
so now it we can't stop here we don't
know that this set of faults produces a
full tolerance bug right we need to go
inject it and indeed when we inject it
we shouldn't be surprised to see that
the program under study reveals
additional redundancy that we didn't
know about in that additional lineage
graph right so we've prodded the program
and forced it to reveal additional
redundancy
even if these broadcasts fail this thing
was going to retry K times right that's
not a problem we can just enrich our
formula now we have a more constrained
formula that has a smaller number of
solutions so in this way as we prod the
system with these hypotheses we either
find a bug or we find evidence of
additional redundancy that gives us a
better model of the system and makes us
make more intelligent choices about
faults in the future right so the
property that we wanted and indeed the
property that we get is that as we sort
of cast darts into this dart board first
of all we're not casting them randomly
we're casting them into regions that we
can prove according to our model our
weaknesses and sort of the facade of
redundancy right we're casting them in a
smart spaces but even more importantly
than that when we cast the dart it casts
a shadow over a region of the search
space that we no longer need to search
and in this way every experiment reduces
the number of future experiments we need
to perform but a really high level the
recipe for LD fi looks something like
this you begin with some good thing that
happened the right was durable and you
work backwards from it right so you
start with success and this recursive
process of asking and answering why
questions yields a lineage graph which
we can mine in the way I showed you to
derive a conjunctive normal form formula
which we can pass to a very fast
off-the-shelf Sat solver and at that
point one of two things happens right we
we get some solution and we use our
fault injection infrastructure to test
that data that's set of faults and one
thing that could happen is it exposes a
bug like in the case of our previous
example the right is no longer stable
good now we can stop now we provide some
of these lineage explanations to the
programmer and they can go do their
debugging right or if we don't find a
bug the system has surely revealed
additional another way if you like that
it has a bit sleeve to get to that good
outcome and we can use that to enrich
our formula so we keep going around this
circle until we exit here or we exit
here because our formula is
unsatisfiable that means there's no more
interesting fault to inject then we can
stop our verification right so you know
some of you might be thinking I'd like
to buy your product yeah so there is no
product it's just papers but it's okay
because you can just build this and
people have built this at a bunch of
places and you can do it it's easy right
all you need are some basic requirements
like you need some way of knowing
whether an execution succeeded or failed
maybe that's just a couple of assertions
in your integration test if you're
netflix maybe that's confirming that a
user succeeded in streaming data because
at the end of the day that's what you do
that's your end and you know a
touchstone for whether the interaction
succeeded or if you're like uber maybe
you you confirm that riders and drivers
were successfully rendezvous or whatever
so the the measure of success is very
likely to differ from enterprise to
enterprise but but it's something that
should be possible to define as a
predicate right
you also need some way to actually
perform the experiments you need a fault
injection infrastructure luckily as I'm
an argue in a moment these
infrastructures are maturing rapidly in
large part as a reaction to the chaos
that is the corollary of the emergence
of micro service architectures okay and
similarly we need some mechanism for
getting that lineage for getting those
explanations and I'm going to talk a
little bit about this later in the talk
- luckily these kinds of systems call
graph tracing systems in particular
becoming very mature and then finally
you need a way to go around that loop as
many times as you need to you need to
replay interactions now if you're
running integration tests in a staging
environment replaying interactions is
trivially easy but when I was at Netflix
you know Netflix Cowboys that they are
they do all their testing and production
and it turns out that I mean that's
great that's great fun right but you
can't replay stuff in production because
the site's changing no two requests are
the same so we had to solve some
serious problems being able to emulate
replay over a world in which no two
requests are the same it's a very fun
subject there's no time for it in this
talk if you're interested talk to me
about it offline so when I first set out
to sort of come up with a way of getting
all of these desiderata I took a sort of
I was a PhD student you know I took a
boil the ocean approach I was like well
you know what people really need to do
Chris knows about this is throw out all
their systems code and rewrite it in my
toy language Daedalus and if you rewrite
all your code in my toy language I get
there's a very straightforward program
transformation that gets me fine-grained
data lineage in the standard database
way I have a god-like control over the
simulation of the distributed execution
so I can inject faults at arbitrarily
fine grain and then you know I just I
just encode that middle bit that I just
described where we where we go around
this loop and solve so sort of if you if
you do everything in my academic
language all this stuff falls out for
free you can imagine what they said at
Netflix when I arrived in June and said
okay well all you need to do is you know
rewrite all your systems code in
Daedalus so I didn't work right I only
had two months at Netflix so I kind of
needed to take advantage of what was
there essentially I had to say you must
have some way of explaining your
outcomes you must have some way of
perturbing executions let's try to like
make this fit with whatever you have oh
but before I get to that the boil the
ocean approach works great so I studied
like a lot of well-known protocols
delivery protocols commit protocols and
Kafka itself from the distributed
systems literature old stuff new stuff
and I found that Molly which was the
name of this prototype of LDF I found
known bugs in many cases using an order
of magnitude fewer executions and an
order of magnitude less wall clock time
than than random approaches so like this
approach definitely works in the lab but
it's not particularly reasonable if you
if you need to rewrite all your code so
as I was saying a moment ago the
approach to is sort of like you know
make do with make do with what you have
well it turns out that in a micro
service based organization like Netflix
because nobody knows what is going on
there's some really mature observability
infrastructure that's becoming quite
standard right so this
approach to call graph tracing that many
of you may know from from Zipkin was
first pioneered by google and their
system called dapper it was very quickly
open source by Twitter into the system
called Zipkin Netflix has a clone a
Zipkin called what is it called Sal uber
has a clone as if Kim called Jaeger and
these approaches are becoming so
standard that there's an emerging
standard sort of led by Ben siegelman
the guy who developed a pro called open
tracing so it's probably gonna become
like a common open standard for for
tracing large scale micro service based
systems you also need some way of
effective situating the false right and
as I indicated these things are becoming
really mature too so this approach was
pioneered by Netflix with their system
called chaos monkey and later fit
LinkedIn has a fault injection
infrastructure based on chaos monkey
Microsoft also has a fault injection
infrastructure so does uber so does page
or duty and sort of many other companies
that I can't mention due to NDA's and
friend EA's and stuff like this so it's
almost like a reasonable expectation at
least within the micro service world
that this kind of experiments enabling
framework is just gonna be available
right so in the approach here is you can
just you take that LD f5 that core of
the other approach and you snap it in in
such a way that it consumes call graphs
they provide it with its models of
redundancy and then it's hypotheses can
be used to parameterize your fault
injection framework that's exactly what
we did at Netflix we snapped these two
things together and we automated it we
just let LD f I run it watched the
system run and watch the good executions
happen it formulated hypotheses about
what could go wrong and then it saw what
happened when they when it did go wrong
right there's one wrinkle and that's
that the graphs that I showed you before
explicitly capture redundant
alternatives in fact the whole approach
that I showed you is that you're only
able to prune the space of failures by
recognizing the redundancy right
to a first approximation a call graph
like this here's one from Netflix
doesn't have any redundancy it just
tells you what services and their causal
relations between each other were
required to satisfy some request it
doesn't tell you that
for example this service had a backup or
something like that right so this
wouldn't work by itself as a framework
for providing explanations but we
realized after some time at Netflix was
call graphs certainly do reveal
redundancy they just reveal redundancy
over a longer time scale let me give you
an example of what I mean so we look at
this graph and we say well gosh it seems
to require everything so I'm just gonna
flip a coin and have a hypothesis that
if I crash this EC map LTE service
whatever the hell that is it would cause
this whole interaction to fail an API
proxy to either timeout or return an
incorrect response right so that's our
hypothesis so we go effectuate that
hypothesis we go injective fall when we
inject that fault the system provides us
it turns out with a new call graph that
looks something like this what just
happened here
well what happened here was EC is evey
cash it was a cash right this was a cash
and in the event that that the cash is
down we do the same thing we would have
done in the the system does the same
thing that would have done in the event
of a cache miss it re performs the micro
service computation that replenishes the
cache all right so here and now if we
think about these two graphs
collectively over time we sort of route
both graphs under the same route we end
up with explicit redundancy in our graph
right so the system doesn't necessarily
reveal its redundancy upfront but if you
prod the system in the right ways it
reveals that you're gonna see over time
and you're able to build a sort of
long-lived model of how the system
provides multiple ways to to get to a
good outcome okay so I mentioned we
studied this interaction called app boot
app boot is essentially the set of
interactions that happen when a user
first initializes a device to be used
with Netflix so it you know checks your
account credentials it does DRM stuff it
loads your bookmarks it loads
recommendations it kind of primes your
device to stream and it's a moment of
truth for Netflix right because if sine
goes wrong here people might abandon and
go use Amazon or something like that
right so it's really important for them
to ferret out the bugs and the belief at
the time that I joined was that well
there aren't any bugs in Abood because
like it's the most studied interaction
and the whole thing right now a bhoot
uses about a hundred services so the
sort of naive integration testing
approach would involve performing two to
the 100 experiments that's like a 1 with
30 zeros after it so that wasn't gonna
that wasn't actually going to happen
well with LD fi we explored this this
sort of fault space performing only
about 200
along the way we found 11 critical user
facing bugs that had never been surface
before so there's lots of bugs in there
it's a question of guiding your search
through the space to actually find them
by building the right models okay looks
like I have about 10 minutes left so I
wanted to talk a little bit about some
of the fun stuff that I want to work on
I have a team of currently three PhD
students two masters students and two
undergrads there's a lot of great stuff
we want to do but we need help
so tell me if any of these ideas sound
interesting to you okay what do I mean
by this uh yes I really believe in the
LDF I approach the idea of building
models of redundancy from explanations
and using that to search a fault space
but I think that the particular places
that we've been pulling explanations
from are fairly shallow and narrow so
I'm inclined to think LD if I could work
at a bunch of different levels right
like and so the question I want to ask
is the the problem with call graph
logging right is that there's this
instrumentation burden like you can't
use Zipkin unless you open up your code
and put in the right tracing points and
this is a burden that not everybody's
willing to undertake you know some
companies like move too fast and break
things too much to instrument all their
RPC their whole RPC layer there
threading libraries and so on and so
forth right so what if what if we used
some os-level provenance collection
techniques I know that Margo's ulcer at
Harvard and some of her colleagues have
been working on provenance collection
that draws graphs explaining how system
outcomes happened at the level of things
like file descriptors and packets and
things like that so if we could collect
sort of hypervisor level provenance
could we extrapolate these application
level explanations from those low-level
explanations like we know there's signal
in the noise but can we simplify OS
level provenance to give us something
like application level provenance may be
even more interesting sort of the
advantage of that is that although you'd
incur a an overhead burden you wouldn't
have to incur that instrumentation
burden to take advantage of an approach
like LD if I may be any better question
to ask is well what if what if you don't
even want to do that what if you want
this to be completely transparent could
you construct things like explanations
from unstructured
well Facebook has been doing some really
interesting work in this space with a
system called mystery machine the idea
is if your facebook you have so much
data flowing through your system that
you kind of an infinite set of logs and
so what you can do is you can do this
really brute force thing where you sort
of postulate all of the possible
relationships between different
components you say I'm gonna postulate
that components a and B are a happens
before B and B happens before a and a
and B are mutually exclusive and a and B
are concurrent and anything you can
think of and then as you see logs right
logs provide evidence that can't
contradict some of your Cartesian
product of hypotheses and in the
fullness of time you've contradicted all
of the hypotheses so the example right
is like hypothesize that a and B are
mutually exclusive but if I find one log
in which a and B are running
concurrently well you can forget about
that right so in the fullness of time
you end up with the actual explanation
of the causal relations between the
nodes it seems a bit far-fetched right I
don't know if it'll work but it would be
cool if it would work and it's probably
worth working on another issue with LD
fi as we use it at Netflix is that
sometimes even the space of solutions is
too big to search exhaustively and in
that case what we'd really like is some
kind of prioritization of that space I
want the next fault that I explore via
fault injection to be the most likely
fault to actually happen right and in
order to do that I would need some way
of reasoning about the individual
probabilities of different failure
events and so we had constructed this
problem as a sort of decision problem in
which you know each of these clauses is
represents how you break an individual
path you could fail a or B or C and then
the conjunction of them is how you fail
the whole graph and we ask a solver is
there a solution to this formula well
probably what we'd like though is to
convert this into an optimization
problem
give me the most likely solution to this
formula and we can actually do this in a
pretty straightforward way so we can say
we can represent each of these clauses
is just a set of elements and so we have
a set of sets and what we would like to
do is come up with a set of faults that
hit each of these sets at least once and
to be the smallest such set of faults
this is a well-known optimization
problem
the minimal heading set formulation for
example ceh is a minimal hitting set
because it hits all of the sets and
there is no solution that hits all the
sets that has a cardinality less than
three so that works and we can encode
that as an optimization problem in the
following way right I'm not gonna have
time to go through the details but we
can sort of just take this problem and
say we want to minimize the set of
faults that we inject the sum of all the
faults subject to it has to hit the
first clause and it has to hit the
second clause and so on right so now we
can get the smallest cardinality set of
faults that is consistent with our
hypothesis but unfortunately this
assumes that all faults are independent
and equally likely which are both not
true right so we would like to
parameterize an approach like this with
some kind of probability distribution
over faults and some kind of model about
independence like if let me show you
what I mean so if we did this naive
thing it would appear that this outcome
supported by three alternative paths is
more robust than this one which is
supported by two but that wouldn't be
true
if these three were all on the same
computer and this was on two different
computers and these two appear to be
equally redundant because they're
supported by three ground facts but
those three ground pacts could be all on
the same rack or they could be spread
out across three racks and two data
centers right so we need this kind of we
need the likelihood and we need the
containment topology information one
approach would be to say now what we
really want to do is if we know the
probability of an individual fault we
want to take the product of its
probability taking it to either one or
zero and we want to maximize that
probability unfortunately those of you
who are familiar with optimization
problems this is not a linear objective
function but we can make it into a
linear objective function by taking the
sum of the product of whether or not we
inject the fault and and and and and the
log of its probability
so that solved the problem so now we
have a linear objective function and we
can essentially say to this thing give
me the most likely fault that is
consistent with our hypotheses okay I'm
almost out of time but the last thing
that I want to work on it may be the
most exciting thing is this problem of
searching the space of inputs
so my tool everything I've described
today has just sort of assumed that our
inputs are fixed we just know our inputs
to our system a priori and when I
originally pitched LDF I sort of glibly
argue it's ok like LDF is its orthogonal
to input generation there's lots of
great tools quick check and fuzz testing
and colic execution use those in concert
with LD Fi but it's a white lie because
some of the most juicy bugs involve the
really fine-grained interleaving of
fault events and input events
so in some sense I'm in search of the
pulse to my quick check like the quick
check to my pulse I should say how do we
co search the space of inputs and
failures and just very briefly in my
last three minutes I want to tell you
about one of my favorite distributed
systems bugs a bug that LDF I couldn't
find until we solve this problem and it
has to do with cord who knows cord the
DHT not too many people know cord so
Kourt is a DHT that involves nodes being
arranged into a ring on the unit circle
ok one of the most important properties
that that a cord ring needs to uphold is
that no schedule of execution could
possibly bifurcate the ring into two
rings because if that were to happen
we'd have a split brain situation the
two rings would never fuse again and in
the original cord paper which won best
paper an SOS p flawless victory they
proved this so-called one ring property
but of course they proved it with human
language and it turned out to be not
true okay a Pamela's a of a researcher
an AT&amp;amp;T proved it to be false and she
said here's an execution in which we
start with an ideal ring there's two
nodes 60 and 25 the successor of 25 is
60 the successor of 60 is 25 and for
fault tolerance reasons every node not
only has a successor pointer but they
also have a successor of my successor
pointer and these trivially pointed
themselves okay everybody with me so
then she says okay well here's an
execution in which two new nodes join 31
and 19 and the first step of
stabilization is to rewrite your
successor pointer so 30 comes any points
at 60 and he says Oh 60 who's your
predecessor okay let's rearrange so we
get in the situation in which we have a
ring again the next step would be to
repair the successor of the successor
pointers which you'll note are the same
but then right at that moment those two
nodes fail and so we don't have
successors so we say our successor must
be the successor of our successor
and we're done so this example is
fascinating to me because to get to it
you needed to very carefully control the
insertion of input events and the
insertion of fault events and so I'm
actively seeking smart people to try to
figure out how to how to take LD fi and
do something similarly cute with Co
optimizing the search of of inputs and
faults and this is non-trivial obviously
because as I showed you the set of
faults is sort of intractably large and
the set of inputs is well it's infinite
isn't it so I'm not sure I'm not sure
how we move the needle on this okay so
I'm out of time but I wanted to thank
you for hearing me out on this somewhat
academic this somewhat academic
excursion I wanted to also thank all my
collaborators kin and contributors who
have made this work possible in
particular Netflix and uber who paid me
to work on my research at their
company's elastic who paid one of my one
of my students and an internship gremlin
who were actively collaborating with and
NSF who I'm delighted to announce have
have decided to fund this research for
at least five more years so so yeah and
thank you very much oh you're so kind if
you're interested in reading more you
know scholarly stuff or maybe even just
a tech blog about how this stuff played
out in efflux I encourage you to read
these and you know if you're sick of the
grind and you want to go back to school
and like work on hard problems and and
not have to have bosses I know a really
great place and a really good advisor so
come talk to me thank you
speechless yeah I mean I probably I went
over it pretty quickly in the beginning
but maybe I went over it too quickly I
think that like the nice thing about
hardware I mean model model checking is
a slam-dunk for hardware because it's a
finite state space you can just search
it now it might be very large but with
enough computational resources you could
search it right when we deal with
software first of all sometimes we're
looking at infinite state spaces even
for single site systems and it's also
the case that the systems to which this
work is applying are non-trivial a large
distributed systems right so state space
it's just completely unsearchable I made
this argument don't know if you're here
for in the beginning of the talk that
you know it's tempting to say we can
model check the protocols and then build
up the system from correct protocols but
I made this argument and supported it
with some evidence that fault tolerance
isn't really a compositional property
like if I pick up a couple protocols and
I putting together a new protocol I have
to glue them and I might have screwed it
up
and so you individually checking the
protocols doesn't quite work and and
when you glue enough of them together
you have too large of a state space to
explore that's not to say that there
isn't still that people aren't still
pushing the needle on model checking for
distributed systems so I have a
colleague at Chicago named hardy Hardy
Canali who's been doing some great work
on model checking for large scale
distributed systems but most of that
work involves limiting the state space
by encoding domain knowledge of the
system under study you know in via
annotations into your system making some
kind of assertion along the lines of you
don't need to explore all these
reorderings for example because this
logic is commutative and so the problem
with that is you have to be an expert in
the system to provide those annotations
and if you're mistaken you sacrifice
soundness so I think like model checking
may very well at the end be the solution
but it's certainly not there yet and
this is an alternative technology
possibly complementary technologies
okay yeah sorry for running over thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>