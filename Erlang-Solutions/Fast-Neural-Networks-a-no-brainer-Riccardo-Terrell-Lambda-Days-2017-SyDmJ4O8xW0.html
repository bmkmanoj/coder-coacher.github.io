<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Fast Neural Networks… a no brainer! - Riccardo Terrell (Lambda Days 2017) | Coder Coacher - Coaching Coders</title><meta content="Fast Neural Networks… a no brainer! - Riccardo Terrell (Lambda Days 2017) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Fast Neural Networks… a no brainer! - Riccardo Terrell (Lambda Days 2017)</b></h2><h5 class="post__date">2017-03-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/SyDmJ4O8xW0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone thank you for coming so
you already know my name's first-ever
networker and no-brainer it's pretty
catchy title right so the agenda I will
cover to be about what kind of promise
no net results and what is not a network
little bit then we see actually how does
it work and why I think neural network
is a functional we see how implement a
smaller net or using functional approach
and now to paralyze never network all
the material on github so and then
provide the link you can download and at
the slide in the source code
well this light really just mean this is
my email and my Twitter account just to
touch base for me so a lotta material
but this is the main three objectives I
want it today convey it is a Mary can
you hear me right so they tell it really
is to a soft compass problem and is
actually turn out to be based on simple
math which was surprised for me of quite
satisfied and it or now actually that
I'm I'm fanatic about you know
parallelism and concurrency and when I
also saw that in your directory is easy
to paralyze I would just you know
jackpot for me but I'll be about you how
many you have made about neural network
what you doing here well so the first 10
v
10 slides probably are like into two
neural networks so for pretty much all
of you can be pretty boring so I want to
try to to move on then we are familiar
with F sharp all right cool so the
source code that I will show later in a
slide is a using F sharp however is
pretty much very self-explanatory so you
need to know I've sharp too
understand that the code and however I
use F are mostly because you know I can
feed this decoding one slide which is
nice so motivation for me
first there was the buzzword right here
on nitro super super fancy but I'm not I
don't have any math background and I've
like probably beginner machine learning
I've been you know introduced few times
for few projects I did some and some
work related but recently where I work
is a company based in San Francisco we
do natural language interfaces for
support and analytics and we start to
introduce neural network for natural
language generation so I was able to get
more sports and is a very fashion into
topics right so I said to read blogs you
know and books about our nectar you
found all these interesting Greek
letters that we saw the lambdas all that
it was like I have no idea what this so
my motivation were really the best way
to learn about something it to speak
about it right so that's what I'm here
when I introduced also to machine
learning I found out something very
fascinating right so first of all if
you're familiar machine learning think
commit or you are you know you have a
bunch of possible algorithm to use to
solve your problem however there are so
many algorithms I did to meet the main
challenge is figure out the mapping
watch algorithm to use to solve my
problem right
so there are bunch of those my linear
regression k-means just to name a few of
those and the main comment that you
always like this friend expert so you
asked him a one should I use here right
they probably give an answer but the
biggest mistake you can do is that you
ask somebody else you're gonna give a
different answer right so again it start
to be really confusing and of course we
have you know supervised and know
supervised kind of you know umbrella
that can divide all these algorithms we
know but it also turn out that it's not
that easy because even if you have
supervised in a supervised
even the same agreed work can be
implementing a supervisor on a supervise
man that life is over
so analysts have to be complex but so to
me the motivation by the electrode is
that when I was really make my research
I was watching this video about James
McCaffrey I don't know if you know about
this guy but he's working micro source
research and one is though he mentioned
that neural network can be considered
almost like a multi-purpose machine
learning algorithm right which means
that you can use much in a neural net or
to solve a different kind of problem
right so you can see like a sweets knife
right so you can design pattern
recognition of classification use neural
network so you can call it with much set
of other algorithms just winner or
nectar so to me was like well I can
focus to learn well narrow nectar so I
can solve different kind of of domain
problems which is just nice by the way
little tip that I found out Microsoft
provide in a you can search online for
this it is just independent Microsoft
it's just a table that you can navigate
through and help you to figure out which
algorithm to use right you just answer a
bunch of questions just using decision
tree kind of thing so and I found is
very very useful so definitely I
recommended so briefly what kind of no
problem never never let us all it's not
a brighter
I know people complain about my red
color so so they let us all another
bunch different problem from you know is
a paddle recognition for a CR of course
natural language consisting for what I'm
being exposed in majority of my my work
and probably the most famous one or you
know magazine and feni's you know that
business study were DNA pop DNA pattern
or
no try to predict the future stock
market to become rich but even when you
go online not today you know your
favorite ecommerce store you can some
other Commendation right and so how this
is work well the system we really try to
figure out the similarity between the
people that buy you know the same items
so really it classified based upon your
child
you mean your cart your items in your
cart and similarity with people that
bought more or less the same items to
find out probably what you're going to
buy with a proposed to buy an extra I so
proposal some coupon and so forth it
really what it does really represent a
way to predict probably is not really it
can be predict like base time but only
pattern recognition or classification
right Sophie this is a passive because
example you're like a spreadsheet here
with a bunch of information of this case
student with different kind of you know
informational agenda to create age and
you want to attack this poor well
marinated is able to classify all this
information similarity that can be used
in the end to figure out the most likely
these guys don't play baseball
okay so what is neural network how many
you remember this good because I don't
wanna feel that old right but so right
guys pause on your later time ago right
so they would click clipper in Italian
was a little it was like it clipper I
don't know but you know provides
information above course you know now
let to reserve you think about software
you know that he pays of the neurons
that make prediction and able to decide
what to do right so probably this is
what you in your mind you know no matter
the TX 808 oh yeah so we think about a
literally a system that train and evolve
with the time right think about you know
I'm from Italy so I play there you know
soccer example here but if you want to
start to play soccer you like to grow up
to make baby steps and you train and you
start to kick the ball and then you get
better with the time but you keep
training training you know understand
your three ball and until reach your
goal right so you get better and better
right so you can tell me what this is
good so how are you able to attack the
racket because the shapes probably the
color the you know cute the little eye
there which is great but how you then
tell the Machine to understand this is a
cat so what do this looks okay well you
say blue dark suit and a red tie and you
think is a person but now you're more
information right so now your brain is
able to detect when I mean I see this
this ear there so yeah must be kept so
from now on whenever you even see
somebody where you know dark suit in a
right time you know it's a cat right so
and this is pretty much how our works
the brain during the time you know
your evolution evolved and understand
different part because when you were a
kid you didn't know that was a cat right
maybe scratch you then you know and you
figured out with the time and in the
brain
you know evolve mature and and all these
information are assimilated and you
understand that it was a cat because of
where it not at tied right so so the UM
brain unit can be described as this a
biological neuron network right is this
interconnected system of neurons it was
made in no signal and nearly elaborate
the pattern to figure out there was a
cat or you want to kick the ball through
electric signals right so in this case I
don't even answer right
I think it's dentists receive the signal
and based on those input elaborate and
find out a signal to the axon to reach
the next neuron so they are all
connected until you reach the final
result and basically is a is a brain you
know a biological neuron receive input
from source combine all this information
during the day way and figure out was
defined resolved but the great part what
is very powerful is that is a adaptive
right so a grow and self learn it get
better with the time with a training so
here what we're going to talk more
details coming soon but think about
really another matter is a set of
organized nails right that relies a
subset called layers so in this case in
the slide here we have three layers
actually it would be two but three
layers of three four and two neighbors
right and the idea is that all the
nerves from the come from the input the
elaborate some signal it passed to the
Narrows of the second layer and so forth
right and there is a value that the
arrow we like Cairo right
the arrow they connect neurons to
the next neurons are called wait so the
connection with and they're gonna see
what doesn't mean with that but step
back is immense really we must see a
really base of simple math right
symmetric operation and some linear
algebra and an equation that pretty
simple so if you know how to conduct the
result of X there you're good to go
you can you really know neural networks
when he took a shot the vocabulary and
the promise that you know never met with
such a big technology that you know
using different kind domain from you
know business businesses doctors you
know academics and developers and I read
those who use a different terminology
right so for instance narrow without
note right and the connection or the
connection called synapse it in a
biological term in many ways just a
guide to print out this probably keep
cross your desk sometimes is easy to
understand so how does this work anyway
any questions so far I don't want to
keep it by the end but some type of you
better keep in the context so alright so
this is our fully connected the network
composed by three four in two layer
three layers nodes right so the first
layer the input layer is a normally not
counted that's why I said test probably
more cozy like a two layer Network and
as I mentioned really you can see how
this network consists of interconnected
neurons right and usually a group in
these in layer and they impose noticed
emote that is is a pass from one layer
to the next where is ELISA said feed the
the next theory propagate to the network
layers until the last that is a which
you know you get the output the values
that most likely is
represented your result and this class
is called feed-forward right in this
case so they ultimately the result
really depends on the weight is a
measure the connection between the
narrows and something called activation
function and we're going to see
activation function in a chemist's light
but really what doesn't mean that during
the process we were taught probably in a
back propagation that this process of
feed-forward in he got to the end figure
out you know the the error between the
want the result in the real output
figure out the error and then feedback
from the beginning to the neural network
to self training so it'll be more in
details here the most positive concept
as I mentioned that you know itself
train but the first the the second grade
column right is something called where
are different names I call it
normalization it's something very
important when you implement your
network is a way that you map the input
value was a sort of numerical
representation that it will be in line
right so this actually is a very
important point when you set implement
your neural network so as a measure is
about find you know the best sector
value of weight narrator the most likely
compute the output in a matching you
wanted resolved right so initially the
way that actually was something that was
interesting to me beginning that when
you set your way to just random values
right and then when you start to train
your system this way this value just
update and anga better until reach the
point that you're able to reach the
result that you were looking for
they in this case the the neural network
you know inside the white box there you
can see there are like several
connections in this case the connection
county narrator who is going to be
important specially literally it's
counted like you have three
six and in five so gonna be three times
six plus six six plus six twenty five
point five so in this case going to be
fifty nine connection you can simply
measure as we see later how the second
explode in a huge number of different
connection the hidden layer the one in
green and saying something like is what
actual responsible to the computation to
figure out the result of the neural
network okay so I think I've too much
lied about the boring stuff but I think
it's important to cholera become
constant here so perceptron is a you
know simple concept it's just the
building block of the end of the neuron
network I as a is the node right in this
case we have three input value which is
zero one zero two zero three that are
passed to the perceptron and this one we
represent the input and the normalized
like can be any arbitrary input and in
the value between you know the the input
value and the person John is the weight
which is important to calculate another
result in this case the final output for
each node is the sum of the product of
the weight and the input value and then
apply the activation function and that's
you're gonna get the result for that now
you can see here beside back propagation
we'll see you later
ready you can see how these are just a
bunch of matches operation right
especially during the training process
it's just of self updating and some
computation and in updated value the
weight which is operated by magic
mattresses so already you can see how
easy could be diplomat don't worry about
the code is actually these we're gonna
see later in
in spended version i should be a remote
provocateur use some mutable up today
well okay so yes sir as I mentioned the
connection and the between input and
weight of much matrixes operation
briefly about activation function this
is just you know function it to apply to
scale the out to output the general
vector in a proper range and several
which one to use just try it right the
most common one is a the first one the
sigmoid however seems and again this is
come from what I write on internet so it
must be true that Sigma star seven and
some sort of problem with a saturation
of the error in some circumstances so
assume that the the tangent the
hyperbolic tangent function is tired to
get a more favored over the sigmoid so
training process as I mention we talked
about I measure about back propagation
there are several other the most of em
one is the back propagation a really how
does it work simply you get the input
you constant till the output and
calculate the error because you know the
final value right you calculate error
and a unit feedback neural network
passing this value and adjust the weight
until the arrow doesn't change anymore
this table since that the the last one
this warm organization is a bit better
aware is lower and the most common is
the back propagation because it is easy
to implemented it you know the most
mentioning paper
so little tips here that I you know
learning my oilskin some nice guys you
want to share with you I assure you when
you initialize your weight they're
random so but you were less know the
range of the way that can more or less
is close to your output so that's
actually going to help to be faster but
now choose the right number of layer or
number of narrows for each layer this is
a this is a tricky because there is not
any rule of documentation that can tell
you which one is better than the other
right is a it's about testing is about
trying until you know it figure out the
best number and however the bigger that
the layer the more flexible and
adaptable in the system so you know
usually you say ok I'm going to just use
a bunch of you know there was a layer
but then you're gonna run into issues
such as overfitting the the inner
electro so is about compromise it really
keep trying and figure out the right
number and the only thing I want to say
about this table you know some time you
run in sickest answer when you have like
some day that can be you know later or
allowed and depend very similar to the
data what kind of issue you run of
course if you have a type of set that is
very different with just a small amount
of data that's probably you have some
issues there ok so any question about
this part that was mean everybody knows
1l night was so probably I should
correct me if I say something wrong I
don't know okay
which one deep in our life work okay do
you think this is a specific for your
domain or Ruggiero okay cool thank you
all right so never later can be faster
right so that's why you wanna run in a
paralyzed neural network so as I
mentioned the connection in narrator can
be explored right so here in this slide
are northing there are over 300
different connection so when you run
this in some machine it can the
computation can take a while right and
what I like to think is that if you
really know light to the bay is upon
your brain I hope that you know you can
compute different thing at the same time
I don't talk about multitasking I'm not
good in multitasking but for instance
did you drive the car most like you can
organize the music playing your radio or
you see the red light you brake mean
there are a lot of thinking up in the
same time so the same working you know
in a neural network can be the brain can
be can send different kind of signals
simultaneously and most likely prometo
can be computed you know in parallel so
there's also a be provocative but I want
to say here
so note they are connected chatter right
so and they communicate each other
sounds familiar
of course right it's just vodka do that
so agent anybody know what is an agent
the same with this should I say it
no yes yeah okay so agent is just you
know computation right he's a he's a
user a synchronous message passing
semantics which means you push messages
is no block he from outside the world
and compute one message at time though
is a is a used mostly for you know
concrete application and the the benefit
about agent is that you know it's single
thread inside so you compute one message
at a time and you avoid any sort of risk
conditional deadlock they could you know
parted a bit full of the concurrency
model traditional concurrency model
computer immense to the time and the
other messages just buffered so they're
now lost so the great part is that
communicate chatter right so if you
think about the rail network they
communicate with the neighbors you know
in the only the output enables sent to
the next layer and communicate each
other right so this is very much in line
you know with the actor mode actually
display anybody knows they're different
between a generic sir just the
correction for is not part well so they
the agent just in process right is can
be accessed with a memory address
instead of actor to more like all the
process kind of thing anyways
so and this is the best animations light
and ever done look ready
okay I do it again so so you can see
that this mod right and really if you
think about it know as an agent right
well yeah so you can really map the two
model together right so in their worldly
benefit about the agent that I found out
when I implemented is that you can
actually either remove agent almost
on-demand when he's running so when I
was mentioned about detector a number of
nodes neighbors or or layers where
actually this actually could be a great
model because you can just you know a
business or API around to remove an
agent as it goes to figure out a
shoulder ailment of the result so in
this case is that F sharp no compatible
code just you know conceptual so we have
an agent there to accept a function and
you can see the age really receive a
messages synchronously compute the input
with some activation function again this
is very concept oh and then pause the
output you send out so then you can just
create your sort of link function you
can connect agent right so when you put
an input to an agent it compute and when
it's done send and dispatch to all the
agent part the next layer right and you
can see here just pretty simple the hand
you just put a couple for each and you
create your so this is great right this
is awesome right
okay provocative I know I know this
probably It or Not sure that pretty bad
deter nowadays performance-wise because
unless you really need to build a system
like this for some sort of you know
requirement I would avoid it because the
performance early based on the
computation of the connection no the
connection per se so it wasn't terrible
the performance but definitely there was
some
it wasn't that fast so and I put some
slides here also because I think that
when you read the Nelnet early day the
mapping is seem natural right note agent
yeah but is a the representation wasn't
that happy handed as I wanted you can
talk me later
all right so I'll be why never it was
functional right well you know that
we're functional programming and to just
right so you can seekne laterally as a
function mathematical function that gets
em input and get some output right so
how we can implement it let's see how
briefly can implement a small neural
network right well first you know we can
create some upper function in this case
you know depends just to add the bias or
they measure the bias the by is just
this constant that I really personally
no no why probably you can tell me why
but it just exactly noted paper that you
should just the bias which is a constant
between you know one or two or whatever
so we have this function that append the
value to our vector we pass which is you
know our input then we have our you know
function okay we open the constant 1 to
0 for the bias and then we have the
layer function here which pretty much
apply I give an activation function okay
as arguing we have the weight which is
repeated by matrix and our input which
is our vector right so just simply we
have the product of the matrix and
vector here the input in the weight we
map passing the activation function and
we ran our append bias
that's pretty easy right so next step we
have we know our sigmoid function which
and if we have some layer in this case
are presented by some matrixes I have
three layer there right then we have
these two function here which I said to
know you know you just compose neural
net with our layer function we pass the
layer function with our activation
function and our first matrix and just
use they know the compose operator here
and you build your neural network and
then of course you have your branch
function in this case that you pass the
layer and any arbitrary input and you'll
get your results back
but the compose operator think that's
pretty clear so I should turn out that
with a simple you know function it's
pretty powerful because now we can fold
over a given a list of layer when he was
a couple of layer couple of matrix and
the function of direction function and
then we just you know again with the
composer operator fold over and build
something more complex so at this point
we can pass any arbitrary tapo with
activation function and the magic
represent my layer in pass by combine
them in there run it right so this is a
so what's the priority how easy
composable can be you know neural
networks use a functional programming
and of course you rather use a CUDA with
some input and you'll get your results
back
so this is our probably the biggest
piece of code are gonna show and I sure
know one you know to to go to much
detail but really what I want to show
here that really they are ready to now
to be no a bunch of matrix operation
right so in this case I just I'm a very
pragmatic so I like functional
programming or light you know all the
nice feature immutability but also you
know I think that purity is good a
certain limit so in this case I use some
immutable immutable type inside and I
live in purpose just because you know
when you want it when are the factor
encapsulating the function why not right
and again we can talk about that later
I want to just tease a little bit anyway
so all these a bunch of for loop or you
know matches cooperation I can be easily
paralyzed right so it really is the
nature of the neural net to write that
allows distribution of the work in a in
a with a very small effort to now
however while distribution of the
computation
they provide computation can be
accomplished easily sometimes know there
are computational Kotetsu the neural
nectar provides
the choice or the architecture can be a
bit more complex right so it is easy to
implement some sort of neural a tool can
be paralyzed but it's all about found
the right pattern for your to solve your
domain you're going to see what I mean
in the next slide coming
however just basic is they are para
network I mean a neural network are
pretty much mastery parallel right
because if you think about it this is
look I will be familiar this pattern by
the way it so it looks like a no Map
Reduce kind of pattern right we have
sort of input in Map Reduce to the next
layer Map Reduce the next day and so
forth so most likely you can apply the
same
the same pattern as well right so for
instance here in the F sharp you know
MapReduce we can create our parallel map
function the piece fake here stay for
parallel sequence so it take a function
with map function in our input right I
just apply this function parallel to map
the input a the same we have for the
reduce function right again we have our
input with the reduced function and we
run in parallel the reduction the reduce
function and then again we can compose
you know Map Reduce reducing function
planner right we have a our to function
and we combine them with the with the
two inputs which if you remember here
the symbol a little bit decomposition
aspect that we have in beginning with
neural network any question about this
part okay
all right so how long by the way I miss
left
okay so whoa I guess seven right okay
motivation all right
f sharp event calendar I have the link
here it's a great event I run every year
and what it does it just you know people
sell the soul for Christmas to write a
blog the specific day this year is great
because we build a pub sold the blog you
can download it but for the blog
actually I was working with neural
network so I decided to write something
using their own network which is the
classic TSP problem
but in our neck which is a bit different
right so TSP actually if I find out that
it's not just a fun algorithm but is
used like a medicine from the farm some
sort of genomic pattern or DNA sequences
so fun very interesting but anyways so
in the beginning I you know wrote the
code in parallel and I have like a
pretty good performance as expected
however well actually a friend of mine
it is this I'll just found a bug and
actually but anyways thank you to fix it
it was even faster anyway so then I
found I'm a paper junkie so I found this
paper online right and the paper pretty
much apply you know the divided conquer
approach to the speedy right and what
you use is the user neural network of
using elastic network pattern which
pretty much instead to use a a
two-dimensional point user a ring
initial and span the ring to figure out
the best pattern butter now they
actually the study used I think canis is
network and a stick something many ways
what it does is get all the point and
try to and use underneath came
a fire to classify the the city of the
points right in district and then
running parallel all these district here
to found the best pattern and then found
the closest point to merge emerge
together some sort of with a blind
function so of course right quick demo
let's run the demo alright so I'm not
going to go in detail about the code but
again are gonna show the link like that
you can download it so the idea the deal
we have these are point America fear
right yeah so he let some set about the
city iteration arrows and initial linear
rate can be arbitrary so using the
setting you won't have like some sort
the number of narrows should be like
four to five times the number of the the
city and the iterations should be like
about five times about the number of the
narrows in this case that put twenty
five thousand let's put like fifteen
thousand which is way more but anyways
so I initialize here and a start and
this is like today in parallel bus in
the back you take three second to the
four hundred millisecond to run right
fifteen hundred fifteen thousand
iteration right so now we can divide all
the city in cluster around discussing
follow a more though and then merge
together before right well let's run it
first all right
it was eleven hundred which is like
three times faster which is pretty nice
right and this is a job with two cluster
right then you can of course increase
the number and you can go crazy let's
say six probably is not even good but
anyways yeah but five hundred ministry
so what is cool about this approach this
technique is that if you start with zero
iteration right you start with the
traditional approach you just get a mess
of wine right because I tried to figure
out which line touches the city and in
the end the best in all paths instead it
means start well start with establish a
circle right and then the next iteration
is each iteration actually of two two
steps is a expend one side know whenever
the point to touch the best city and
spend in a back try to almost like an
oval size to found the other best city
in the other side now the direction
the point here is that even if neural
network can be massively paralyzed is
always about find the solution for your
domains right there are like in this
case you know I found this paper and it
seems actually the right solution for
approach so always try to go a step
further or we try to implement I know
we're running out of time so I'm not
going to go through the code but I want
to just close up with the link that
probably useful so humbly well i'm no
repeating that the resources here with
the paper and what is it also the next
way by the way I write in a book
functions on coins not met this is the I
have to do it right you get coupon here
to get 3539 discount here and this is
the link yeah that's all now have some
you know I could there you go so this is
a the link then a workshop that is a
actually is cool if you want to
implement yourself there is two two
folders wonderful implementation if one
actually tell you step by step
complement to yourself so I recommend it
when a fun and some time learn from you
know F sharp try with the workshop first
play a true if any questions shoot in
email right and then see how it goes
right that's all</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>