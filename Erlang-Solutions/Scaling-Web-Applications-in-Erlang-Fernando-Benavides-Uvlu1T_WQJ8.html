<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Scaling Web Applications in Erlang: Fernando Benavides | Coder Coacher - Coaching Coders</title><meta content="Scaling Web Applications in Erlang: Fernando Benavides - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Scaling Web Applications in Erlang: Fernando Benavides</b></h2><h5 class="post__date">2012-06-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Uvlu1T_WQJ8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">a little bit about me first I'm a
developer since a long time ago when I
was ten years old I started working on
qbasic and basic for Commodore then I
worked in visual basic c-sharp net Java
screen name it all that kind of stuff
after a dr's doing that I switched to
functional programming I let all that
object oriented world aside and I did my
thesis project in Haskell and I found
Ireland and started working in a company
in Argentina called Nova means after an
ER there I moved to Anaka and in Anaka
we are basically a company that builds
into end applications with iOS clients
airline servers for them and Ruby for
the admin tools my part in that as you
can guess is the airlock Megan today I
will talk about how do we scale process
web project that had an HTTP API and a
component that keeps clients connected
to the server for long periods of time
it seems to be a narrow scope but it's
we found that is a design pattern we use
in a lot of systems almost every iOS
obligation we have a socket that
connects to the server to receive
real-time update and also has a social
and has an exit epa api to deal with it
it lets the HTTP API lets you connect
with other systems and be like
independent of how its implemented and
the circuit is a sir it provides a like
optimal way to connect because it lets
you send small pieces of data through
tcp without the burden of other things
around samples of these obligations are
for instance chat rooms
social sites where you get updated of
what's going on in your network or my
favorite one sport site when for
instance there is a game play you're
going on somewhere and you get updated
of what's happening there ok and what
will we do with this kind of sites today
is we will try to optimize the way we
use OTP behaviors TCB and HTTP
connections and they and we will tweak a
little bit the underlying system
configurations you will see in a while
what we will not be out in this talk is
we will scratch the surface but we will
go in deeper detail on how to handle
multiple multiple nodes on machines or
which databases you should choose for
the different projects you have to show
you the process of scaling an airline
web server I will too I will present you
a sample application as you can guess
since I'm an Argentinian it will be
about soccer of course in this
application the general idea is like a
soccer match is being played somewhere
but you are working so you cannot be at
the stadium and then you want to get the
updates of what's going on there because
you're a fan of one of the teams there
is a reporter in the stadium with the
device he can tell you what's going on
so you need a system that connects you
to him this system basically receives a
stream of events from the reporter and
broad customs will cast them to tons of
users that are connected to it the basic
requirements for this kind of system is
there are many concurrent users using
the system at the same time and they all
connect at the same moment at the
beginning of the match they keep their
stay connected for two hours and then
after that you have no users at all like
they drop until there is a next game and
they want to know the things that are
happening in that match exactly when
they have
it's an annoying feeling to hear
somebody shouting a goal through the
wall and you don't know what's going on
and if the application has a refresh
button you hear the refrig I want to
know what's happening okay so these
people really really want to be updated
in real time this kind of system is a
sample system but it's a very very
similar to many of those we have in the
in our company okay our let's start with
a general design idea what we will we
have there is a match process that will
receive the events from the that the
reporter is seeing at the game every
client will have a connection which are
those boxes on the top right and they
also can use the API which is a web
server and the much events will be
stored in the database that's because if
somebody connects later they want to
know what happened before so they will
use it through the API and the prodigal
I made it like a readable prodigal so I
can show you the idea is basically you
connect to through telling it to the
server it welcomes you you tell you that
you tell the server who you are that's
v2 a group are gone and the server start
streaming event one after the other
first it gives you the status of the
match with the players who's winning
whatever and then whenever there is a
goal it sends three lines of text
indicating the data that role the same
for a penalty whatever so basically it's
a a communication where you say I'm this
guy and after that it's all on you all
the communication habits on the other
way the server sends you event okay the
architecture for a system like that is
something like this I will go in deeper
detail for on each of the processes the
first one is the client listener if you
I guess you are familiar with the
non-blocking tcp server that was it
wasn't a nerd doin internet a long time
ago it's it's a gin and B server it
listens for TCP connections and whenever
a client comes in it candles that
connection to the client supervisor that
starts a client process that has that
connection and delivers the events the
clients are Jennifer sent that basically
receive events from the user process and
delivers them through DCP to the
connected clients there is a mochi web
server it's here it is shasta box but
mogy work has a several processes
working when you started but to
represent it is just a box it's a muggy
web server started like the usual way it
processes API call request then we have
the this is a little forced in this
sample obligation but it matches what
happens in real life more robust
applications where you have a for
instance in this case we have a process
per user because we thought a user may
watch several games at the same time so
there is a process per user and a client
process per game basically the holes
user information that it gets from the
database when the process is started and
it keeps track of what matches you watch
what team you're a fan of all that kind
of stuff em the most important part it
subscribes to a genuine dispatcher which
is the process on the far right there
it's an event dispatcher I will talk a
little bit more about that in a moment
but for each much that's being played
there is one dispatcher and the users
subscribe to that one and finally we
have a DB process with a connection we
use the rebus just because it was there
but we
silly the idea is it has a connection
process that handles the TCP connection
to a rather server okay it seems like a
fairly a properly designed system and so
we try to use it like just that we have
put it in production and try to use it
and we find out that it's simply using
airline to build your system in not
enough to ensure that it will scale to
whatever number you want just doing in
our like it's not enough you need to
like tweaker you need to do it right to
be able to handle the amount of users
you plan to handle to do that we started
we needed to test it so what we do we
say that what do we want to test we
wanted to see how many connections we
can get so how many users we can handle
at the certain moment also we need to
know how many of them can connect at the
same time remember I told you they
connect everybody will try to connect at
the beginning of the game so we needed
to be that see number needs to be bigger
anybody familiar with a batch event will
know an MC are the same thing here and
also they need to be informed about the
events as fast as they can so that's the
average response time it's a little
tricky here because as a there is no
like responses is not request response
the only response we have is the first
event that's for sure because when you
connect then you say your name the
server will retry we reply with the
status of the game so we are measuring
just that one to measure these things we
use we create our own test client that
connects through TCP and acts as a user
we use a batch event for the API calls
and we use a
the first time we did it we did this I
used ends up because that's what I have
at that time but now there are a lot of
other tools say for instance bigwig that
was the spawn fest winner which is with
us kind of the same thing and lets you
know which processes are have the
longest message queue or using more most
of the memory of that kind of stuff so
we started testing let's say the first
type the first step in every like scale
testing here is to establish a baseline
to know how well the system performs to
do that we create the ultimate testers
we install the system on a clean machine
we run this the test on the servers
local network that's important it's like
we know it's not the real deal at the
end of the day but running them in a
local network means that network
problems doesn't affect don't affect us
so what we do is we run in a local
network but we require better
performance that we will actually get
that there and basically we test and
increase the end and see values and we
just again and we increase with values
and we test again until we find the
boundaries and the last one is important
too when we test the system we automated
clients we learned we should never
forget to have somebody else watching
the system to know if it feels right
because even if it responds to
connections at the speed we won with the
amount of connection we want maybe it's
not responding properly so we need to a
human I to tell us that the system is
really working while it's responding to
those connections okay so let's make a
guess we assistant designing like that
on a clean linux machine a big machine
how many connections do you guess we
will have like like that just install
and run how many people
we'll be able to watch a game when it's
happening who say one hundred thousand
one hundred thousand ten thousand okay
no a thousand she's like that a thousand
yeah yeah it's a thousand and also that
a RT is not a typo it's really 26
seconds it's a long long time you can
help if you are really really fast you
have two events having a penalty and a
goal and you find out when they are
already playing again so it was really
bad number and it is a really good man
we know we can make it better for
instance the first thing we we think is
okay it's a machine problem it's a
machine broke that machine is not tuned
up to use to be able to run our systems
so let's tweak it let's let's make it
better let's improve the operating
system configuration because as you know
like regular operating system linux OS x
are prepared for the general case so
let's make it work for our case and also
let's improve the air long vm
configuration for instance having
processes letting let it run with a big
number of processes so there are no
dropped clients because of that so we
turn up the file limit in linux the tcp
connection limit the backlog sighs
everything we can to let them know that
let the Machine know that we will be
only interested in running our system
there with our problems with our
configuration and so let's bet again how
many users how I tell you it improve but
how much almost like that 4000 but it's
slower as you see it's instead of 26
seconds now it's 31 seconds you can make
two goals in that time it's still that
but we can improve it we can improve it
by tweaking airline stuff so let's start
with a kind of a configuration one you
see in this slide we were only able to
handle for other time okay if you
increase your backlog you are telling
this gen DCP or machi where ATT be that
runs on Jen TCB that you want to be able
to receive more of them queue them up
and process them later we do that oh
yeah we do that this way basically when
you start a gen DCP listener you can
tell it how many how long the backlog
debug log Q will be and the same thing
for McGee web HTTP start another thing
remember we have one connection to the
database that's that works but you don't
need that you may have several
connections if you handle them well so
there are some tricks in there some
things need to be queued up but for
instance in this particular case where
all that happen is like reading and
there is only one process writing you
can have all the connections you want
and that and in that way you can
paralyze database request we did it like
this we turn the internal state of our d
week process from one PID which was the
Redis connection to a list of them and
we okay and on the init function we
start them over like with a map but the
trick is here we will we go round
robbing the reddest connections using
one at a time and the thing that's in
the middle of the yellow of the
highlighted parts I will go back to that
in a while but basically it lets you run
it lets you process handle call calls in
parallel so for each one we use a
different radius connection on the other
hand they are you have a listener like
we have a listener for the TCP client
why one that's a good question we can
have several of them and if we have the
if we need to let users know just one
port for instance for the HTTP API we
can use nginx or other stuff in front of
us that do their round robin so we can
have several Maquis web servers and
several gently TV listeners on our
obligation this by balancing the charge
among them we that this way when we
start we turn out our list and replying
listening to supervisor it starts a list
of listeners that are the same as the
client listener was one per port and we
end up with an architecture like this
instead of one listener we have a
listener supervisor with several
listeners instead of one web we have a
web supervisor with several web
listeners and instead of one DB
connection we have several connections
and that really improved stuff now now
we are talking we have a small town club
but we can handle eight thousand users
500 of them out of time and we reduced
the AR T at half so it's improvement but
we can do better we can start with a 0
DB behaviors that this is my favorite
one I love Jenni van and and even so I
find it has its two or three things that
may be better let's stir by let me tell
you how to CP how
Jenny vent worked just for just so you
understand the rest the following slides
basically genuine works this way you
have a process which is the dispatcher
that starts when you do genuine start
that process is basically a gen server
with several states one per subscriber
is of those state any time an event
comes in handle event function is called
for each of those modules that are
subscribed and the when it's called it's
past the a corresponding event so
besides that you can add a supervisor
Chandler because those hundreds are just
functions are models with functions
nothing else there are no process
involved but if you want to subscribe a
process or something like that you can
add a supervised handler that thing
links the supervisor links the genuine
dispatcher to the process that's making
a call to subscribe and it links them
that means whenever one of them goes
away the other one is notified when you
have 23 subscribers that's okay when you
have thousand subscribers maybe but when
you have a lot a really big number of
subscribers each time one of them goes
down every other subscriber is notified
that makes a lot a lot of messages going
around too many of them so the solution
i found was too instead of using
supervised handler i simply add a
handler and I monitor the dispatcher
that way I the only notification that
goes through is when the dispatch of
light which is the one that's important
I do it this way like when there you see
I've handler it was an odd sub handler
before and basically after that I
monitor the the manager I keep a
reference to that monitor so when I get
the down
info from the monitoring I know if that
if it belongs to the dispatcher and I do
something about it another thing to to
question this behavior is like the thing
is i told you when an event comes in the
handle event function for the different
models that our subscriber is called one
at a time gen event has a list of them
so if you have many subscribers and if
that handle event function does a lot of
work the cue the delivery queue goes up
like higher higher and also if the
events come in a faster than processing
rate the dispatcher keep message queue
goes higher and higher and higher and
higher and events take longer than
longer and longer to be distributed so
what you can do is you can add genuine
repeaters which are basically gen events
themselves that are subscribed to the
man dispatcher they are many so the you
distribute the work when in event comes
in it goes through the main dispatcher
then it goes through the repeaters and
then to their subscribers and the
repeaters works in parallel here is a
sample code of genuine Riveter it's
basically the first the first function
startling happens on the process that
need that wants to start this very
bitter and the other to happen in the
main dispatcher code so that you first
start your own dispatcher and then you
subscribe with the repeater to the
original one then the notification goes
all the way to your clients you handle
the notification holes from the remains
of the main dispatcher to handle event
here and then notifying through your
dispatcher to your clients
this gets a little more complicated to
explain but basically every repeater is
started in in our system on the match so
the match has similarly be there someone
main dispatcher it sends events to the
spatula those are those events are
repeated and the users subscribe to one
of the repeaters and that got us a huge
reduction in a RT like a huge one and we
been able to handle more a connections
at the same time so we are improving but
we can keep improving we will try now
with the famous gen server yeah so this
one is one of those things when you find
it it's like a revelation you see the
you were you I was able to work for a
year and a half with Jen server without
knowing that function and when I found
it I was WOW like you can go out of
handle call and keep the server working
and replace somewhere else it was
amazing it's amazing like you don't stop
you don't block your server server and
you can go like replying whenever they
want to they have a chance it's awesome
and it's easy to do if there is one
condition your state you need to modify
your state inside the chain server
because you need to do that no reply but
for instance in this case the only
modification was to move the various
connections list so we can do all the
rest of the work in a fun process there
so basically the database the database
hand server is just starting processes
to reply to the clients ok another one
this is
it took me a long time to understand
exactly what is doing and I think I
don't I I like okay it works it really
really works but I guess it's like it's
something like airline one gave us the
OTP guys gave us to help us but it's
like this it blows like this when you
hibernate your process you the memory
conception from that for that process in
the video machine waste go down with
really low so we do that in every we can
do it in hundred cast we can do any
handle girl whatever and that means that
Jen server goes to sleep and therefore
its memory is compressed so you can you
can have more gin servers in the in the
same machine using less memory and
another one this is a this is a hockey
trick but the thing is sometimes for
instance our user generous took a long
time to initialize because they were
going to the database to find the
history of the abuser so somewhere on
the internet I'm sorry I didn't find it
again and I go and I know I don't know
how to give credit for but I found it
somewhere that if you do you can do this
you can in it the you need reply has a
third tableau element which is the
timeout if you use a cero timeout after
right after you go away from that and
the Chancellor is started and whoever
was calling Jen server start or stop
link can keep going on you let that Jen
server will receive a timeout the same
thing if you can use whatever number 0
is because we wanted
initialize it right away but if you can
if you want initialize it like 10
seconds after that you get the idea is
that that barometer is if the server
stays inactive for that many
milliseconds you will get the timeout if
you use zero you will get that time out
immediately that means you can move your
initialization goal from the init
function to the handle time out to
handling photo time out there for
whoever is called engine server start or
startling or whatever it's not blocked
because they its blog until I need
function returns so whoever is calling
it it's not blocked it can keep doing
stuff in our case it was a supervisor
trying to start users remember everybody
connection at the same time it was in a
really long queue of users that it tried
to initialize with this methyl it were
like tak tak tak tak tak tak one after
the other so so we get a big improvement
with that we can handle now 40,000
connections 5,000 at the same time and
we reduce the LT time to consider a
really good value 25 milliseconds it's
really good but as you guess we improve
it even more with one thing this is a
little trick that we were discussing
that with child we will we would love to
see that inside ODB we don't want to do
this but it's a the way this is like
this you have simple 141 supervisors and
so you can start triggering like you
they are the number in every supervisor
you can start a child but in this
particular case its uses to start a lot
a lot of times so so a lot of taylor
many of them what happened this that
supervisor has a list in his in each
state so if you have a lot of children
it's time you want to start one of those
the memory of the supervisor increases
and it takes longer it takes longer and
longer and longer so again the same
trick as before you use a supervisor
hierarchy we called out in the middle we
call it managers and basically the in
this case together did you use a
supervisor has a group of supervisors as
children and whenever you want to start
a user you started on wonderful children
we couldn't use a round-robin we because
we have no access to the internal state
of a supervisor unless we ask for it so
we use a random you just pick a random
one it works very well and so the
architecture turns out like this instead
of a client supervisor with a lot of
clients have a claim supervisor with
several managers each one has a piece of
the client process we have the same
thing with the user supervise and and it
turns out we can handle now 5,000 50,000
users 8,000 of them at a time and that
soccer stadium is that cigar stimulus
from one of the most important soccer
teams in Argentina we can handle more
people that that stem you can fit so
let's the last list of tricks this one
is a particular one there is a choosing
a login system is very important it's
like those things where you can screw
things really really bad or do it really
work what you need to do is you need to
choose a login system that lets you that
informs you of the relevant things when
you are in production but doesn't get in
the middle of your scalability purposes
when we leave this this when I build
this system I did a new lager
I so in anagha we develop our own login
system called elog it's good but now i
recommend laggard lagger is a good one
and basically the idea is to be able to
know to be alerted about errors but to
keep the login information the lower
possible another trick this is
complicated for me to explain but I will
do my best the idea is for instance we
have a dispatcher on where clients needs
to where users need to subscribe so to
find out dispatcher that was started by
a match process they needed to ask the
match process which dispatcher they
needed to subscribe it was a handle call
like when a user came in it it asked the
match okay I want to watch that much
what subscriber where is the subscribe
hey where is the dispatcher the match
goes find it so it's dispatcher returns
and user then subscribe to avoid that
round trip we can register the
dispatcher with a proper name is a
naming convention so we know what that
is beforehand without going to the match
so the user can simply subscribe to that
process like this instead of a gen
server call on the first function which
as to some magic to get the dispatcher
name and when we started on it in the
match process we resisted it that way
now we can handle the same kind the same
amount of users the same connections at
a time but we improve the speed of the
AR T and this is where the trick I told
you the first time makes sense it's not
that every event is faster it's just the
first event it takes a smaller time to
retrieve the first event because we
don't
you that handle call in the middle okay
so the final part it's we have a system
that scales in one node it works pretty
well let's do what we know we can do
with airline which is adding some other
notes again as first I think like okay
just start another note start
application in it and everything works
we know the case because there are
several things you need to consider
maybe for instance in our case we don't
want several much processes listening to
the reporter because if that way the
reporter must tell the first one the
second one the other one is complicated
we we don't want to write the events
several times in the database either we
want to write them once so what in this
in this symbol stage I'm not talking
about a lot of complicated stuff like
nodes going down not going out all that
other things that are messing with it in
just the simplest possible case just to
get it running we needed to repair the
system we need to decide which processes
we need to register locally globally how
they need to connect how do you need to
communicate and we came with this
basically we connected the match
processes using pc to and we split their
database into one unique in the system
like globally resistor DB rider and one
maybe reader for now then we started 40
sorry there is a the architect the
resulting architecture basically the
only change its notice is that divided
the database structure and then we will
we started for nose in the same machine
connect that into them we were able to
handle a thousand a hundred thousand
users 32,000 of them at a time with the
same a RT that means a bigger sign that
one
barcelona's stereo okay let's recap we
started wheel a thousand users for other
time 26 26 seconds to get their first
message and with small improvements one
step one tiny step at a time and nothing
so drastic alike we didn't really sign
the whole application nothing like that
we moved to a thousand eight hundred
thousand users thirty-two thousand at a
time and with a in average response time
of 10 milliseconds which is 100 times
better for the amount of users eight
thousand times better for the the
conquerors in value and 2023 2600 yeah
English ten twenty six hundred times
better for to get the first event and I
should remind you this is an iterative
process its relative in the sense that
it goes one step at a time we will
improve him slowly but also iterative in
the sense that it gets better with every
system you scale because you knew you
you learn new tricks new stuff to do and
for us it was really useful this all I
told that story I told you about a like
a fictional system which is real you can
download from Gib help but I mean it's
not the real live system happen to us in
a real-life system in more than one like
in for at least two production systems
and the second one was a fast process
like we didn't even know this for the
first one it was like a month of work
and it gets bare metal number ok that's
it questions
oh yeah pretty significant increase
again do you know why that increase not
really I guess it's because for instance
we listen to four times the the ports
that we were listening before we have we
have four times the connections to the
database we increase the same things we
were increase it before we increase them
again all those all the things maybe
yeah
yeah they're there are things that we
were considering like for instance the
gin event add supervisor handler it's
something that maybe fix her Oh to be
level the supervisor hierarchy it's
something that we would like to have
like from the OTP itself yeah there are
many things we improved because we need
it but it will be convenient if they are
already on the on the provided all to be
journeyman behavior yeah
basically a library called a two part of
what it does is it takes the patterns
like zero time out not a pattern but you
have to check out two but he tries to
take those things
I will definitely go to that until i
talk yeah thank you very much thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>