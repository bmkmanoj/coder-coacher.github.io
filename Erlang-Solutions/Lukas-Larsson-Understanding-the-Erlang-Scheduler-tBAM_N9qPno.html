<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Lukas Larsson - Understanding the Erlang Scheduler | Coder Coacher - Coaching Coders</title><meta content="Lukas Larsson - Understanding the Erlang Scheduler - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Lukas Larsson - Understanding the Erlang Scheduler</b></h2><h5 class="post__date">2014-02-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/tBAM_N9qPno" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello and welcome to the erlang
solutions monthly webinar my name is mud
in Malaysia I'm the solutions director
here at erlang solutions today's webinar
represents a continuation of a series of
webinars we are organizing across topics
of interest in the world of Erlang and
dealing with solutions based on the
Erlang programming language today we
will talk about and try to understand
the Erlang scheduler as many of you will
know Erlang is all about massive
concurrency and situations where large
numbers of occurrences happen at the
same time given that each of such events
can have millions of instances a chain
of 14 Erlang has been dedicated to
scheduling these occurrences so as to
make optimal use of the system our
webinar today will focus on the Erlang
scheduler and on how the Erlang virtual
machine determines what to run where and
in what order
as with any live event please excuse any
second issues we may encounter today to
start by telling you a bit about erlang
solutions we are a product and services
orientated company completely devoted to
the Erlang programming language and
since our founding in 1998 we have
worked with organisations and
individuals using Erlang helping evolve
the language and supporting people and
businesses using it today we have about
80 people across our offices in London
Stockholm Krakow Budapest and Seattle
and working on projects across the globe
we are keen on creating value and
competitive advantage for our customers
across industries and through the unique
features and characteristics of our
language a language
we're very ambitious as well in
development of Erlang based products and
we work to create lasting partnerships
with our customers some of the products
in solutions we market include Mongoose
I am messaging platform the react
distributed data store and other
solutions applicable across sectors and
problem areas where Erlang makes sense
now I'm pleased to say we have a speaker
joining us today in whom we have
carefully selected as best suited to
tell you about the Erlang scheduler and
the optimal ways of harnessing its Lucas
Larsen a senior consultant with Erlang
solutions has first encountered Erlang
all the way back in 2004 during his
studies
he has since spent many years working
across airline projects and across
sectors and industries nowadays Lukas is
a consultant with within the Elango TP
group Eriksson mostly focusing on
developing the Elan VM please allow me
to finish by saying you are welcome to
post questions throughout the duration
of the webinar by using the chat
facility we have over 500 registered
attendants for today's webinar so we can
expect a number of good questions raised
and a lively debate our speaker Lukas
will answer as many questions as time
allows at the end of the webinar if any
questions do go unanswered you are
welcome to raise them by email using the
following address webinar at Erlang -
solutions comm that's webinar at Erlang
- solutions comm if you're interested in
learning more about Erlang which to
establish whether it may be a solution
for the challenges your own business may
be facing and please feel free to
contact me directly my email address
will be displayed in one of the final
slides of the presentation where we'll
share with you today the same goes for
any other questions you may have a feel
free to contact us I would now like to
hand over to Lukas Larsen who will be
glad to start us off however
continue this presentation about
scheduling in the virtual machine in
this presentation I will try to
introduce you to the different ways that
we in the I language machine try to
schedule different activities from an
online developers point of view so that
you can get a greater understanding of
how you should program your different
systems in order to get the maximum
performance out of your system and also
to understand when the virtual machine
doesn't behave as it as you expect it
should so the things that we're going to
be looking at is processes corks and
timers so the different kinds of
scaleable activities that we deal with
in the Ellenbogen machine we're going to
be looking at deciding what to run or
where to run it and when we're going to
run things I want to be looking at load
balancing so how to load balancing
algorithms works we're going to have a
quick look at the async thread pool and
see how it scales its work we're also
going to talk a bit about profiling and
we're going to close up with talking a
bit about why you should stay away from
writing myths that are running from a
long time so starting up with processes
well Allen process is executed Allen
code as you know and they are spawned on
the same scheduler as the process that
you are running so something you might
want to and in mind if you're running
some kind of spawning factory where you
have one process spawning a lot or a
supervisor or something so they always
born on the same schedule that they are
in run
it pushes one something that runs out of
reductions or for some reason offenses
are received with are no messages
it goes into a beef trap so a built-in
function for some reason decided that we
should yield for execution of other
processes or we we see that they support
signaling processes are pre-emptive this
means that we can preempt them at any
time and that we want to so that we can
allow other processes to run at some
time now each scheduler that we're
running on has a run queue that it if
track of to figure out which processes
are being won so you can look at the
different schedulers particular one two
and three and up to the number normally
the number of course you have in your
system or execution movements actually
that you have in your system and this is
a FIFO queue so the first process that
goes in is the first one that goes out
it will always consume from the
beginning and new things being put up
the end so this is the simplest view
unfortunately in reality it's not this
simple when looking at the different
processes and things we add things like
priorities so we have four different
types of priorities and processes in
Allen we have a max a high a normal and
hello so Alan priorities are work like
that it's a hard priority meaning that
max if there is something of a max
parity it will always be run before
something of a high priority and a high
poverty will always be run for something
on a normal or low priority normal and
low is kind of a shared priority level
where a low priority item will be
skipped eight times before its lot about
execution while the normal one is
executed each time these priorities are
not really used in your model Allen
programming as much and I wouldn't
recommend you to use them but they're
there if you for some reason find that
you have a process that needs to be
executing for your system to be
responsive and you don't really care
about the other ones for instance when
dealing with overloads and areas where
you want to have one process that's just
throwing away load quickly and not
caring about the other ones moving on
we're talking about reduction so a
reduction when we're trying to decide
how long a process you'll be running a
reduction is a decreasing integer that's
decremented once for every function call
in the current implementation this
interview starts at ten thousand and
when it reaches zero it will signal that
hey now we're done executing things that
they commented this function calls
garbage collection in decremented number
depending on how much garbage was
collected by the comb
collection call this we'll also do
different kinds of bunting of this or
decrementing of the reductions in order
to say that okay I just reversed a list
that was two megabytes long this will be
more expensive than doing a reverse of a
list that was just three elements or
something like this for me and that's it
about processes so let's move on to the
next item
so ports is the interface we had to the
world outside of the a language machine
they are created on the same schedule as
the process or port that creates them
only some of the activities related to
ports are actually scheduled on
schedulers if possible when a process
tries to do something on a port then it
will try to it would immediately take
the lock on that port and try to execute
it immediately and if it if it can do
that then it will do it if not then that
activity will be scheduled for later
this is in order to decrease latency
because normally you only have one
process that's interested in one port
port as well as processes runs until
it's reached 2000 reductions or until
it's out of tasks normally I believe one
port task is about two hundred
reductions of worth so you run ten tasks
and then it's the next ports turn to run
ports part non-preemptive
so we have to use a lot of tricks tricks
within the LinkedIn driver in order to
execute this so we use things like
non-blocking i/o or scheduling things in
your sink pool in order to keep the
execution parts of one task really
really small a guiding number that's
kind of being thrown around there is
about one millisecond per call
personally I think one millisecond is
too much for this but it's a general
good thing to do the adding distribution
uses this basically the same makin a
seems a sport for scheduling so when you
start to distribute a line note it's
just create support that we've been
running on and you're communicating with
that TCP port to another thing so it's
gets carryin in the same way and useless
reductions in a very similar way as well
now talking about four tasks we sorry
wrong side here ports are put in a run
queue as well on the different schedule
so you have ports belonging to the
schedulers and they are executed
interleaved with Processing's talking
about different port tasks we have what
I like to call Alex space Alan Griffin
or VM driven now or Alan be in space for
tasks so Alan torture his things that it
process does to a port so command
control and stop things that are driven
from the virtual machine is things like
ready input where the output as in ready
and many others there's also like
timeout and flush and things like that
when all of these different tasks are
being run we are acquiring the port look
so it's only possible to run one of
these things at once in a virtual
machine what it looks like basically is
that a process doing something like for
instance support commands to a process
support it takes the port lock so that
no other process is allowed to do this
at the same time and then after a while
it returns true now the thing about what
command here is that it's a synchronous
call to a port which means that it will
return true if the command has been
successful it will however return
something like bad or something if the
command is not successful
however being in a synchronous language
in nature also has a dying primitive
that you can communicate with ports and
this is an asynchronous request now this
helps scaling and are quite a bit if
you're using the what command the Bang
operator instead so we take the lock
there and then you execute it
this becomes important when you have
many different processes
interacting with the same port so for
instance if you have a process running
along here it does a port come on it
takes the port clock and then you have
another process on another scheduler
that wants to do the same thing this
will realize that hey I cannot take this
port look at the moment when it does the
port come on and then it has to go into
a waiting State because it cannot know
if the previous port command is
currently running will for some reason
terminate the port so that I should
return the bad org or change the state
in the port in a way so I cannot I have
to wait for the reply on the port c'mon
and we can have many many in many
different processes there waiting for
this port lock and this obviously
becomes a scalability issue because the
latency of all of these processes would
be increased now when it's done with the
truth the next process starts in the
queue can do a do it's for c'mon take
the look and return true there and then
we can do all of them now instead when
if you're using a synchronous message
passing here we can do a port bang come
on and we get we take the log but the
next process that's running when it does
is the port come on it can continue
running in the background so we're here
decreasing the latency of this process
and it's just scheduled tasks to happen
to this port of entry and then all of
the different processes can do the same
thing here and once the port clock has
been released we can execute execute the
different parts and a task protected by
this port lock and the processes that
are running they are just moving along
doing whatever it is they want to be
doing so this creates
lowers latency in the system is quite
nice unfortunately it has a problem
meaning that we cannot really know if a
task has happened or not because it's a
synchronous in nature so if for some
reason the port has died while executing
a task before we have no way of knowing
if it's happened so even though it's a
shortcut we can take it's not always
that useful especially if you're using
something like
CP which is a reliable protocol here so
you want to know that you deliver the
TCP message on the other side but if
you're using something like UDP and you
don't have any guarantees anyway then
you might use for dying because this
will scale up and you don't really
really know if the other side has
reached something anyway execute a
little different parts so port tasks
driven from the virtual machine space
are normally created by things like kqe
Apollo select so we're doing some kind
of select on a file descriptor or an
event object on Windows to figure out
okay so do we have any i/o that needs to
be handled one limitation in the current
implementation is that only one
scheduler can do a well a select command
at the same time so all of the other
schedules will be sleeping or doing
other things and on one scheduler will
go down and do the pole we have an item
in our backlog at your TP team where we
want to increase this and try to make it
possible for more schedulers to do this
phone but it's kind of tricky to make it
work well we'll take some time and
hopefully this will decrease the latency
when reacting on new i/o events we've
seen some good benchmarks that are
promising but it's not easy to do so
moving on from the ports and all those
things we're going to the next scalable
entity which is timers so a time when
you're talking about timers there is not
really it's not the timer module as such
that's important here but it's what we
call diff timers so the timer's set in
place when you're using things like add
links and after some timer at the start
timer or menu to receive after timeout
so these are these three I believe are
the only three ways you can insert this
timer into the timer data structure and
the four things list
below there so now what Co can set timer
and returning from check io is the only
times that the a language machine is
updating its perception of time so the
only when you call now or work lock or
set the new timer will be will recall
what is called guitar and final day so
actually calling asking the operating
system what's the time now and how much
time has expired since the last time I
call them so on and so this is when we
do these things you try to do it as
little as possible doing this update of
time because it's quite expensive and we
have a big lot in front of this get
tolerant time of day which means that if
we have many schedules calling this at
the same time
it will get quite hairy
unfortunately not doing it a lot means
doing it at least I would imagine ten
times that every millisecond or
something like that because it's being
called quite a lot by a lot of code all
the time I also would when you want to
talk about time versus own I want to
recommend reading the documentation for
timers that we've written a new chapter
in the documentation in seventeen point
though about timers and timing and how
it works now time adjusting works so if
you want to have a look there in the
it's in the long run time systems use
the I there's a new chapter about how
now works and in the limitations of now
and lots of different things there so if
you won't have a look at that I
encourage you to download 17.0 and see
what you can do there timers internally
in the web machine are handled by a
timer wheel we have one global timer
wheel for the entire system and I've
formed averted schedulers and we have
one lock protecting this so this again
is a bottleneck in the scheduling so if
you have a lot of schedulers putting
timers inside of this timer wheel at the
same time you can get contention on this
lock we have some ideas about how to
solve this lock but until we've solved
the gap tolerance time of day lock
this look will not really help us all
that much we have to solve both locks in
order to get any performance or an
escape scalability gain out of this
checking and when a time has passed is
done when we're looking for a new
process to schedule so in the scheduling
loop it's not actually being done when
we are updating time but it's being done
when we want the next process to
schedule this can be done because we
know we do not give any guarantees about
timers being that case if you do a timer
and say it's going to I'm going to
happen off to 100 milliseconds it's not
a guarantee that you will be actually
allowed to execute after 100
milliseconds so therefore we can
postpone doing this we just guarantee
that you're not allowed to execute on
this time but before 100 milliseconds
but you might be put up the underground
queue that's very long or something like
that so it's how long a time it takes
there is there but it will should never
be less if it is less than there's
probably somewhere okay so that was that
about timers let's see yeah so and when
we're trying to decide what and where to
run so when which process of I'm going
to run home and which what are we going
to run and so on
we have what we call the schedule loop
so in the C file called L process let's
see there's a function called schedule
and these seven bullet points is
basically a pseudocode of that function
it's quite short and not very hard to
understand if you can read C basically
what what we do in the loop is that
first we check if any timers should be
triggered so if somebody some other
scheduler or ourselves have advanced
time then we check should any turn
timers be triggered and if any timers
are triggered and we activate the
processes that are associated with those
timers we might check balance now might
is there because
checking balance is done by the first
scheduler that's allowed to execute
2,000 full process executions so when
two thousand times two thousand
reductions have been consumed by that
scheduler that's when we want to move in
and we do a balance check exactly what
happens with demanded biometric I'll get
into later as a third step we try to
figure out okay should any processes of
ports be migrated from this schedule or
to the scheduler from anywhere else we
execute any auxillary work so axillary
work is work that's internal to the
schedulers so if one scaler once another
want to do something that's like
internal like finalize a code loading or
doing a free of a memory or setting up a
trace pattern or something like this
that's called auxiliary or works it
might check i/o if it's allowed to and
then it executes support for 2,000
reductions and executes the process for
2,000 to duction and then it repeats
from the beginning the only things that
might vary is the number of ports and
processes that are being allowed to run
if for instance we are running realizing
that the amount of Port us is
never-ending
we will try to increase the amount of
port work that we're doing because we're
prioritizing ports about processes we
want to consume all of the port work as
possible for fast as possible and put
processes on the back burner until the
ports queues are cleared and then we can
just exited processes so that's the main
scheduling loop now let's
lit about load-balancing so what do we
do in order if we have one scheduler
with 100 tasks and another scheduler
with just ten and a third one that's
just run out of work so the goal here of
the load balancing is to make sure that
we always end up in a state where run
Cubans are being consumed so they should
be empty as much as possible while
schedulers are not the waking and
sleeping all of the time it's very
expensive to put a schedule to sleep and
then wake it up again so we want to have
work as much as possible and then we
want to compact it to as few schedulers
as possible
now compacting it has multiple benefits
we save energy by being allowed to sleep
the other threads we can put CPUs we can
actually tell the operating system that
it it should shut off CPUs this is
something you have to do if your allen
code if you want to do that it's not
something the a virtual machine does but
you can't do it and I know of products
that that do it and it's also nice for
memory locality because if we're running
in the same thread we will have very it
will be a higher chance that the data
will be in the cache that's one it's
very nice and now the main means of
doing load balancing that we have for
spreading out tasks is task stealing so
the almond scanner is mostly at all
screaming scanner where it scales
different things so if we have a look at
these things if we can see that scaler
everyone has some port tasks some
different priority tasks and scale it to
a support askin scaler three is out of
work so this is a candidate for steaming
work from somebody else now when
stealing first of all we put a lock on
ourself saying that nobody's allowed to
steal any work for me until this lock is
removed so we don't want schedulers
stealing work from their scheduler that
just stolen work from somebody else so
for instance here we can have scanner
three steals a work from the front of
the run queue of scheduler form
it will always try to steal on
schedulers that are higher in number
than itself first because we want to
compact load down to the smaller scalars
the smaller scalar numbers and so it's
teens that job and starts to execute
that and skin YouTube consumes it sport
job and now it's out of work it looks if
it can steal skew they're three no it
cannot because there's a lock on top of
it then the moves on steals can your
fourth work and take that over there now
scan there for scale III consumes it
work it tries to steam from scaler for
realizes hey there's nothing there then
it moves on to steal the highest
priority job off scan your one so it
wraps around there and tries to take the
job
scan your warning and then move it over
to itself and that's basically how the
class stealing works it's quite simple
in the different place and one thing you
might notice area is that you can have
some kind of priority inversion here
because if if you get all of the max
tasks on scheduler one and then
scheduler two three and four will steal
low priority tasks from the other
schedulers before it goes over to the
max port max part the some other load
balancing techniques are used to
mitigate this because we try to move max
jobs earlier and so on but it can happen
that we get something wrong in like this
so the priorities are really per
scheduler rather than being on a
system-wide level now this is task
stealing città stealing is there in
order to spread the work that you want
to do different parts now we wanted to
have some compacting abilities first
feeling is notoriously bad at compacting
work it just tries to spread everything
out as much as possible so what we have
is something we'll call in emigration
migration project that looks at
different things to set out migration
paths for processes so it looks at the
new run queue length power priority so
it's looking at these things
to saying that okay we should my
migrates to processes from schedule
three to schedule one of high-priority
or something like that it looks at total
process reductions that is consumed by
the that's scheduler and lots of other
things and it keeps a history of the
seven lost rebalances so it knows kind
of what's been happening in the system
for the last seven balances and if we
calculate this we can see that
so one process runs for a fraction of a
millisecond maybe a half and then your
time start by two so one of these three
balances happens maybe once every second
or something like that when you're
running forth on a system and it will be
less happen less frequently the just
active your system is now these things
you cannot really affect all that much
there are two things to flex that you
can do so you can shut off the migration
logic all together so that you get talks
running only as they are stolen so
you're trying to create anis and even
low that's possible in your system and
so if you want to do this use the plus
SEL fly so schedule compaction and that
will spread the work as much as possible
if you set that of course if it's true
which is the default and it will try to
compact the load in 17.0 we introduced a
mechanism that we call schedule
utilization balancing which is basically
a more fair way or action attempt
actually to make a more fair way of
scheduling work on the different
schedulers when we're not running and
follow full load so if you wanted to
have for instance a very nice top graph
where every core is using 25% on your
eight core machine and you want to put
this option off it's not much better
than just using toss feeding but it's a
little bit better
that's it about the migration paths and
so on so the sync threads work various
similar to the schedulers they are
therefore long-running jobs so if you
have for instance something that you
know that's execution heavy or something
like that then you want to run it on the
things that normally CPU intensive work
you don't want to put there but you want
to put something that looks for a long
time in a call in the language machine
we use this for file i/o so talking to
the disk costing this can be notoriously
bad and the implementations of
non-blocking i/o have historically been
bad on operating systems so if you're
writing to an NFS of some sort they
sometimes they will gluck even though
it's missingness column and so on one
thing that might be good to know here is
that the radius sync also saying that in
the sync task is has been done it's
actually done in the scheduler loop and
not done on the synchronous thread so
you have all of the things you want can
want to deal with their skin with it
let's see next tasks we're talking about
profiling so profiling these things is
quite difficult and you have to be aware
of the entire stack really in order to
figuring things out the Yellin virtual
machine provides the lock accounts so
being able to clean to see which locks
have been triggered and if we are
contention somewhere and so on quite
helpful you have to enable this by
configure and we also use tracing and on
an operating system W can use things
like CAF grind or profile G prof
whatever tickles your fancy there to
figure out what's happening and these
are the old profile is the tube that we
mainly use to figure this out these
things out ourselves
Dutton gut feeling how you do it with
the lock country you do like this
configure been able what come to me
start up this is a small test that I've
been doing so I create a process that
tries to send
less appears to another process and I
won 1000 other processes that it tries
to do things and we see here we have
very low contention on time the time of
day look that I wanted to call you see
before so this is basically doing calls
to now we can see that we have had a
collection collision percentage of 81%
which is very high we can look even
further to see it that it's in a
specific place in the C code and then we
can see that it's when I call the diff
now that all of these things happening
so if you're suspecting that you're
having a lot of conflicts you probably
want to do this tracing you can also do
like a trace to see where the different
processes and ports I guess are being
run you do issue this command along
trace all true and then running
scheduler ID and time stamp I don't
actually know why scheduler ID is not
documented but you can see there if you
add that option to tracing to see which
get you are the different processes are
being scanned in and out on so you can
see in this example you have to know the
scale your ID to now this is used by
percept together it's data and as always
been doing this tracing trace to a port
and try to label it as code if you can
and so that's that about the profiling
that you have to do unfortunately
there's not all that much you can do
about profiling you just have to
understand the stack and try to figure
out what's happening with your system
moving on we can have a look at seeing
so why should you not be doing
long-running niffs
in the virtual machine the answer to
this is something we call thread
progress anomaly and I'm going to try to
explain this this is kind of complicated
stuff as such so what
if we say that these lines represent
different schedules we have a process
here that's running so it's in the
running on that scheduler we have
another process that's running on
another scheduler now this process wants
to send a message to the other person so
it sends a message over there
now while this process is sending that
message the other process terminates for
some reason now when you terminate
normally you would well before we used
progress zone and you would terminate
the process and delete all of the
structs and see memory pattern things
for this process immediately now we're
in progress we can schedule work that
execution to that the nation to be later
so we see hey it smashes we change the
state to exit in here so that all of the
other schedulers know that hey now this
process is dead we issue what we call a
third progress event there so we update
the internal progress and then when we
know that all of the other schedules
have executed this thread progress then
we can free the process tracks and so on
and we know that everybody have a has a
common view of doing these things now
what happens if no thread progress is
being made by schedule for a while so
this is what if you're in a live and
you're calling an if/then no thread
progress is being made so what happens
badness happens some of the consequences
that happen here is that memory could
not be freed so when I said before we
have something called delayed dialogue
so a allocation made on one scheduler
and then a pointer is passed to another
scheduler then the the allocation work
of that pointer has to be passed back to
the original scheduler and then being
done there that work will not be done
this is things like messages Peaks that
have to be moved to different places and
lots of data being is being delayed it's
been the lady allocating this way and
this is a great benefit when we try to
scale things because we don't have to
have a lock on the actual alakay
also code loading setting again tracing
is delayed a sync thread works like fine
read and writes acknowledgments are
being delayed by this so if you're
blocking for a long time and I missed
and no progress is happening then you
these things will not happen until you
finish executing and also the schedulers
perception of time gets very skewed and
can lead some to some very strange in
ours where schedulers are working one
scanner is working at 100% and it
refuses to wake up the second schedule
quite odd so what should you do to
alleviate this and make sure that your
NIF calls are very small generally I one
millisecond should be enough don't make
sure to bump reductions using consume
time size in 17.0 my friend Steve has
introduced something called turkey
schedulers into the virtual machine that
you have to enable it's an experimental
feature that might solve this you have
to modify your Neph in different ways
read the documentation for how to do
that so continuing with different ways
you can optimize things so use the
important bang if you can plus SP P so
that's schedule parallelism tries to
enables you to schedule more tasks than
you done before this is off by default
so if you put that on you can get some
greater scalability in your port
handling scheduler bind type is also
something that you can configure so
normal normally you would have these
schedulers not bound to a specific
processor so they will the operating
system will bounce them about and things
if you set s PT to default by and DB
then you will bind all of the threads to
the processors this will give quite some
bit of firm
performance increased by normally maybe
10-15 percent if you're not using it
only use this if you're alone on the
system if you have some other process
that might be completing for CPU
resources you do not want to use this
but if you are alone and nothing else is
in the system recommend using this
systems with a small memory footprint
and that's running SMP you can use plus
mu T force which basically removes all
of the delay the eye look and thinks and
puts one scale one allocation instances
for memory and it can help with memory
fragmentation issues and so on and
scaling when you're in a small system so
we're talking something embedded with
less than 500 megabytes of RAM or one
gigabyte of RAM something like that
really small and also when you're
running on the Numis is to make sure to
set the scheduler topology to be correct
so that was it for this presentation I
hope you've enjoyed it the different
places you want to have to look at
things is the processor C schedule check
balance just balancing logic and our
process that H has lots of defines
trying to influence scheduling so if you
feel that some dissipation that we've
made about where things should be
scheduled for how long and so on you
might want to influence those so thank
you for that no one say well I'm sure
everyone yeah Thank You Lucas I'm sure
everyone will join me in thanking you
for a very inspiring talk on the Erlang
scheduler now many thanks to all of you
as well who have joined us for the
webinar before we finalize the webinar
itself I'd like to sort of do the honor
to at least some of the questions that
we received and there's been a great
many of them by trying to ask Lucas in
the remaining available time to answer a
few of them now Lucas sir the first
question we have
Aleksei basically asking what is the
average latency of a time or event and
that totally depends on the load of the
system at the moment the timers
if you build a lot of runkis and you
have long run just you expected latency
to be more than it would be but if you
have no thank you so you're running on
80% CPU utilization or something like
that the utilization should be very
small
the in latency should be very very small
thank you for that just to try and run
through because we've received in a
pretty incredible number of questions
the next question from Anton is there a
sampling profiler for Erlang code not
that I know of no okay thank you for
that the next question just to sort of
run through them does each Biff have
reductions has reduction related logic
hard-coded individually no not all of
them I believe that there's a default
that if incremented maybe ten or
something like that and then the ones
that we know have been known to cause
issues have additional reductions
encoded into them right thank you for
that Lucas one of our listeners it is
asking for recommendations in terms of
schedule time to bindings that you've
mentioned so what would be your
recommendation in terms of schedule
bindings
well the bindings if you're talking
about the binding types I would always
use the default binding which is I
believe thread node node spread or
something like that I would use there I
haven't experimented too much with it as
we don't have unfortunately don't have
all that XO take hardware in our lab to
experiment with different new market
actors they think the most fancy one
we've been using is the trial era but I
wasn't at the OTP team when they did
those experimentation I don't know what
settings they had there I know that some
people have had quite
a big difference in performance when
they try to set the topology and making
sure that a topology configuration of
their Numa system is correct Thank You
Lucas here's a really interesting
question from Andy what is the aim of
the new schedule balancing mechanism
plan for Erlanger 17 so again what's the
aim of the new scheduler balancing
mechanism plan for langar 17 we don't
know if it's going to be of any benefit
of people in general it's to solve a
very specific problem at a customer that
interaction customer that we have I
would doubt that it would solve problems
for people in general using just turning
off the compaction algorithm will
normally kill the same result but it
will be a little bit more even on the
different parts the this customer within
Ericsson has their own reasons for
wanting to have this which con fortunate
we have sometimes have to do things that
we don't agree with Thank You Lucas
before I ask the next question I'd just
like to apologize to everyone listening
to the webinar seeing the number of
questions we received it's quickly I
will only be able to answer a tiny
percentage of them but the next question
would be from one of our listeners are
there still issues in a situation where
a scheduler cannot steal tasks form from
overloaded schedulers in a system that's
not running any native code that's not
been written by the by us I don't think
so but if you have lifts that's running
in code for a long time and so on yes
there are still issues there there are
some options that you can use to
alleviate this problem where for
instance I believe there's a flag called
plus SWF that Eakins call schedule a
wake-up frequency that you can set that
will basically try to go around this
problem the solution really is trying to
is are
or together with Steven Oscars attempt
to get these dirty schedulers working so
if we get those working and people can
run their knits in May 30 context then I
don't think this will be a problem
anymore thank you for that just to try
and answer at least a couple of more
questions
there's an interesting one a listener is
asking in case of many schedulers
checking for expired timers is there a
way to deal with contention a schedulers
will generally not check for timers at
the same time a when we check for timers
you will one scheduler will go in and
update the value saying hey I checked
for timers up to this point in time and
the other schedulers will see this there
might be a problem there might be an
issue thing with two or maybe three
schedulers answer this at the same time
but then you would have to have a system
with maybe 64 cores or something as the
main point of contention is when you're
inserting timers into the wheel at a
point and let's say it's a hard problem
to common because we have to know what
the time is at the point when we insert
a timer in order to know how and when
should it expire and figuring out how to
do a system-wide
time thing that's scalable across things
is quite hard I know they've done some
work in the was that the virtual machine
in that question is written for Java the
origin that they can make a tolerant
time of day that's scalable but we have
not had a look at 14 that solution into
our line yet and until we are very
scalable a tolerant time of day there's
no reason for us to remove and
complicate the solution for the time of
it because basically we still have a
look that way we will be hitting Thank
You Lucas now in the interest of
remaining time a couple more questions
is the dirty scheduler were paving the
way towards a time-based schedule for
Erlang or are the reductions simply the
route that the OTP team is sort of
betting on
we we've been thinking about doing a
time-based scheduler thing unfortunately
it's very hard to find a cross-platform
solution that works for all the
different platforms that we have and the
one with productions we have right now
works fine it's unfortunate for the with
the overhead of counting reductions on
the final we would rather have have the
possibility to do timing based
scheduling and with some kind of high
performance counters that the CPUs
provide and so on we can do that but
with the large amount of platforms that
were were supporting we would have to do
a specific solution for all of the
different chip architectures and so on
and it would be hard to maintain make
sure that they work as far as I
understand I'm not an expert on this
part but that's the arguments that I've
heard anyways here's here's a bit of a
rhetorical question but one interesting
to to answer from your perspective Lucas
what do you see as the main channel
challenges you know ahead of us in terms
of the airline scheduler it's growing
and scaling in the different parts and
making sure that memory management I
mean as we are scaling in course we're
also scaling in the amount of memory
that were being used and we need to find
some way to be able to really define
when we want to do copying semantics and
when we want to actually pass references
within the language machine and trying
to figure out a way to decrease the
amount of memory that's allocated an
early stand used by different schedulers
especially if we're moving into a really
big systems were a lot of course and
lots of CPUs then we need to see ok so
how far away is the CPU would it be do I
want to exit it directly or do I want to
make a copy of that memory and send it
over how do I want to do so getting some
way to figure that out would be the next
big challenge I would say the exactly
agreed and that's the type of research
that you know we had erlang solutions or
undertaking when on a daily basis and
through the work that you've been doing
now to finish off one one question that
sort of stands out is do you see a
solution for the possible memory leaks
in situations of running relatively
long-running myths well dirty schedules
again is the solution there we are
we cannot run the job that is dirty in
nature so it's not really a proper
interruptible work on a scheduler that
needs to do these things so running this
work on a dirty scheduler would be a
very good solution I would say okay just
just to try not honor the the flurry of
questions coming in one final we can
pick up a listener is asking whether
spawning thousands of timers has an
impact on all the scheduled schedulers
are only the ones that processes that
started them yeah I can repeat the yeah
basically the listener is asking for a
spawning thousands of timers has an
impact on all the schedulers or only the
ones which serve processes that started
them on all the schedulers I would say
so if you have for instance for
scheduler is running and you want to
create timers on which for processes
running on these four schedulers then
they will you have contention on that
look all of them so it will affect all
of them this room this is the reason why
we have a global look that all of the
different schedules have to access there
so it won't affect all of them and you
can see if these things is a problem in
your system by just enabling the lock
counter that I was talking about before
and see if you get that contention on
this look that's in the timer code Thank
You Lukas and we do need to sort of wrap
up the webinar given the time that's
available but please all join me in
thanking Lucas for a very inspiring talk
on the scheduler now many thanks to all
of you as well who have joined us today
and please join us again in March for
our next webinar dealing with a topic in
the world of Erlang following today we
will be sending you a short survey to
make sure that we capture your feedback
of today's webinar please also note that
the recording of the webinar and the
presentation that was used and shared
will also be available to you
collect underlying solutions these
corporate websites WWE
- solutions comm thank you once again
and we look forward to seeing you on our
next webinar</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>