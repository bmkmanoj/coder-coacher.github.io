<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Detecting Recursion Schemes in Haskell Functions (...) - Adam D.  Barwell (Lambda Days 2017) | Coder Coacher - Coaching Coders</title><meta content="Detecting Recursion Schemes in Haskell Functions (...) - Adam D.  Barwell (Lambda Days 2017) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Detecting Recursion Schemes in Haskell Functions (...) - Adam D.  Barwell (Lambda Days 2017)</b></h2><h5 class="post__date">2017-03-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/StVSC7ZSNIk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello oh there we go excellent it helps
if you turn it on it seems so yes thank
you good morning just mmm so yes in the
last talk Kevin asked you whether or how
many in the room deal with powers know
about parents and that sort of thing and
quite a few of you put your hands up so
I'm going to go out on a limb here and
sort of say that the majority of us are
going to think of parallelism as as a
generally good thing I mean the device
is all around us our power capable these
days and if we can make things go fast
that we can either solve larger problems
or we can do more things with it
however as has been often the case for
the past couple of decades we have seen
that the actual act of introducing
parallelism has always been sort of the
main trick to it so let's let's take a
simple example of a fairly common one
matrix multiplication here we're doing
the dividing conquer the divide and
conquer approach whereby we taking our
two matrices and we are splitting one I
have a horizontally vertically or both
horizontally and vertically so I'm going
to represent this in Haskell and perhaps
the most obvious way of representing
matrix is very a two dimensional list
and because we're doing divide and
conquer we want to first split our
matrices
up until a up until the certain size and
then we're going to join them together
so we're going to multiply the the
leaves of our tree and then we're going
to join together the Rhea fix shall we
say the the split matrices in certain
ways according to how we've divided them
in the first place so so looking at
looking at join join is a fault split
far enough is none fold which Kevin also
mentioned earlier but let's focus on
join for the time being so join is a
fold we can have a fold over a tree
where page is the bit which which which
really is the the split matrices and
multiplying is a simple matrix
multiplication function defined using
transpose and dot so so yes so and we
can paralyze this quite simply by
switching the fold the sequential fold
with a parallel fold so here i've
defined a strategy using the strategies
library in haskell whereby each of the
leaves are multiplied in parallel and up
to a certain depth of the tree we also
do the join operation itself in parallel
and then after that it's sequential so
we have some control of the parallelism
as well and if we run this on our 28
core machine we get some measure of
performance improvement and this is just
by replacing a powerful a fault a
sequential fallen sorry with a parallel
fault
no no extra meddling necessary now you
might argue and in accordance with if
you look at the legislature you know
these speed ops aren't that exciting
especially on the 28 core machine so
what we could then do is we could then
go on and play around with the parallel
fold itself so we could adjust the the
depth to which we do the join operation
in parallel we might adjust the the size
of the matrices that leaves we might
play around with which bits of the fault
that are being part we could also do
things as heavens or rather do I should
say system does by splitting it up and
playing around with the fault finding
alternatives or we could use altar of
assistance entirely we could go for the
par monad we can go for we could use
Eden we could call out to see for
example or if we're something kind we
could throw this on for GPU regardless
the nice thing about this is that all we
need to do is just switch out our fold
for whichever parallel version of it we
want
and so that's quite nice and this goes
back to what Professor Hugh said earlier
on this morning so yes thank you for
underlining my point and of course this
is applicable to other recursion schemes
or patterns we we know about so map
unfold BSP on and so forth the slight
inconvenience with this however in is
that we may not start with a fault
so although functional languages are
very nice and have support for
higher-order functions and so on and so
forth
we don't always immediately use them
especially for types we define ourselves
so a cursory look at the spectral set in
the in the Haskell no fib suite which
comprises examples of Haskell functions
or sorry not functions programs kernels
of real world programs of which there
are 38 programs in there they all do
very different bits and pieces they've
been written by the various different
people I was amused to notice that my
supervisor has one in there as well and
if we as cursory look at this tells us
that at least 19 have more one or more
functions in them that could be
rewritten as I ever a map or a fault and
that's only looking at maps or fault and
we find that most of these cases of not
having an explicit map or a fault or in
cases where the the types they're acting
over are custom
whereas as you would expect to find maps
over the standard lists are well fairly
fairly regular so there are any number
of reasons why we might not actually use
a fault in the first place we might just
be trying to work out what are actually
right offing in the first place for
instance we might so it might be a
Friday afternoon and we might sort of go
oh I'm only going to use it here it
doesn't really matter if I define it
properly alternatively these functions
could be very close to being and map
fold unfold so on and so forth but not
quite they might have a function in the
wrong place for example and of course
that's not necessarily to say that even
if we do find all of the
is inexplicit shall we say recursion
schemes in our programs that's we're
going to get masses amounts parallelism
not every single recursion scheme is
going to lead to great speed ups but if
they are there we can play around with
them so it would be nice if we could
find these instances preferably
automatically I think at the prett and
yet the the computer to do this for us
so to do this and that's primarily what
I'm interested in to do this we're going
to protect nee called Anton unification
which is the jewel of the more
well-known and humidification if you
know it first described in the ninth in
the 1970s it's been improved upon and
built upon since and is primarily used
these days in clone detection and
elimination
so here the haskell refactor at least in
in wanna miss earlier incarnations had a
clone detection Eliminator which used
the ant unification algorithm and we see
a similar thing in in Wrangler the
Erlang refactoring tool as well and
there are plenty of applications of this
to imperative languages not that we are
interesting imperative languages
regardless and so the prime idea behind
Anton unification is it finds the least
general generalization of two terms so
an example let's say we have two terms a
plus B minus C and five times B plus C
if we answer unify those we get an
actual unifier here in T and which is
which is the G as I say the least
general generalization between T 1 and T
2 so here we see that B and C are in the
same place in both T 1 and T 2 so they
stay the same
however the other parts of our two terms
are different which means we need excuse
me which means we need to introduce
variables as substitutions and so one of
the one of the core ideas behind answer
unification is that we can get back from
our aunt unifier to our original term we
can reconstruct it and so in the case of
T 1 we might replace alpha with a Beatle
with past
and gamal live with - as you would
expect and so that's that's a very cool
part
and so these substitutions arise from
the generation of the unifier itself so
how do we actually apply this to sort of
real code it's very nice sort of playing
around with this sort of thing so let's
go back to the moment the matrix
multiplication example so if you recall
join is a fold and we have and we can
defining it that way however that wasn't
actually where we started off with we
started off with this which is it's the
same function it does the same the same
thing it's just it's not defined in
terms of a fold it's defined using
explicit recursion but to get the the
advantages of sort of the simplified
parallelism we would like to be able to
represent as a fold so using a
unification we can do this so it would
be helpful of course knowing what the
fault looks like and it's pretty much as
you would expect so you have two
functions one applied to the node
constructor and one apply to the leaf
and so if we ant unify both fold fault
tree and join whereby we take the body
of each clause and we ant unify the the
matching the matching expressions we are
given or we produce an anti unifier
function which looks something like this
and yeah so that's that and so because
of this principle of answer unification
whereby we can get back from the add
unify to our original fold and what we
would like to be able to do is well so
we can in principle redefine both fall
tree and join in terms of our anti
unifier function with the with the with
the substitutions as arguments here and
so what it would be nice is to go one
step further and take a look at our anti
unifier function and our fault and our
original fault tree the patently
unifying against
and we well we can sort of see in a in a
in an informal way we can sort of see
that our anti unifier it's pretty much
the same as the fall tree the only real
difference you know in the are in the
are in the recursive calls because of
the difference between they're more
formally they unify up to the UPS
they're recursive calls so long story
short the unifier is basically the same
as the fall tree which means we can
replace the ant unit of the call to the
ant unifier function in our join
function with the fall tree and so we
give back to originally where we started
from
now of course this is a very sanitized
and very simplified example for the
purposes of this of this thing and so we
have applied this to other examples and
indeed I've got a brush segmentation of
this using hair again the haskell
refactor we've been applying this to a
range of examples of range of functions
some of which from the inspired from the
ASCO Prelude mostly from data don't list
if I recall correctly and because the
obvious ones but also to some of these
larger toy examples such as matrix
multiplication and body and quicksort
and in the same way that we've seen for
matrix multiplication we get speed ups
for embody and quick sources of matter
but I'm still working on unfold so yes
check back in later but of course these
are still fairly small and they're
they're fairly you might argue
artificial so what we would like to be
able to do in future at least now that
we have this core idea is to apply it to
two more examples of more real examples
so maybe starting off with no fib sweet
I know there's a real part of the of the
new fib suite but also sort of Pascal
programs out there in the wild
scary fortunately and also we would like
to extend our approach to more patterns
to using more pattern so I was
interrupted right implementing unfold by
having to write the paper so I'm halfway
through this at the moment but there's
also other things I would like to I
would like to play around with now the
more sharp-eyed amongst you will have
probably have noted that our original
joint definition is fairly close to the
definition of fall
and that is that is one point we would
also like to address so to make our
pattern detection more flexible so using
well-known techniques such as reduction
rules rewriting rules and in creation
reasoning so on and so forth so we can
really apply this to actually proper in
the wild at least that's the that's the
plan at least the time being however
what we have is or we have an approach
which uses an T notification to discover
recursion schemes or patterns
higher-order functions in Haskell code
and this will work for pretty much any
well theoretically it will work for
pretty much any higher order function
you can find and say hackage or of your
definition because of the the generic
Ness of a notification
we've got a prototype of this as I say
in here and I'm going to keep building
upon that and we have applied this to a
number of examples larger ones and
demonstrated that this is at least a
stepping stone for parallelism and
indeed we can we can we can couple this
we have to feed castro's which Kevin is
just presented approach was something
like scale NL or Spock in Scala which
then takes these these these horrible
functions and does all the hard work of
paralyzing inform you so you don't have
to get a PhD student bashing his head
against Haskell for days on it much
nicer for me so yes and so but the point
is we can use this for privatization we
can also figure out you can use it for
other things but we're mostly interested
in parallelization and so it it brings
us up to the to the to the to the to the
starting line shall we say so yes thank
you I think I think that was pretty
quick
question so define ourselves correctly
you know the regression skis as a
stepping stone for your presentation
continue and the problem was that I have
to get free I know quite so so these
these so we can we can mechanically
derive the the actual recursion schemes
themselves that's that's known that's
fairly easy to do dare I say it and
before I get lynched but yes so we know
how to do that the point I'm or the the
thing I'm trying to do is work out where
we can actually use these recursion
schemes but where they have in in
Haskell code or in any other code for
example theoretically but where they
haven't yet been used so the idea of
basically making them explicit
yes well that would be that would that
would be the ideal case yes but for
whatever reason and this is probably
different for other people we see that
not everybody will sort of go in and
define say whether it's a part of the
function site class now I mean if you're
if you're an excellent Haskell
programmer who does this sort of thing
day in day out and dreams it then yes
you're probably going to use it and my
stuff is is neither here nor there to
you but we do find in in existing code
that there are that these that these
recursion schemes or not use perhaps as
much as we would perhaps like them to or
ideally see them to be
so it doesn't it doesn't really touch it
doesn't really touch on that I'm looking
at this a purely semantically so it's
already late now obviously when you
actually come to put it in the
parallelism bit you're probably going to
be making stuff less lazy but we don't
really actually address the laziness now
there is the there was one little corn
case if I've got the time for it I
noticed when I was actually implementing
this thing so let's say maximum it finds
the maximum say number in the list for
example you can actually implement that
in such a way that you would find a fold
over this but which you stop at the
singleton list and this will work just
so long as you don't it won't complain
just long as you don't pass this an
empty list however if you rewrite this
in terms of a fold because it
effectively skips or your original
definitions as effectively skipped over
the the empty list case the folds will
always contain so there are a couple of
caveats and corner cases here which I do
have in the actual in the actual paper I
mean this is sort of a hand waving
approach but but yeah so there are but
for the most part laziness doesn't
really get in the way which makes change
so I so admittedly the the paralyzation
I whipped up for this example was fairly
quick and so I it's it's not it's not
the best in the world let's put it that
way we do still get the speed up at 28
but indeed yes which is why I sort of
had the which is why I had the slide
well okay this may not be the best in
the world but we can still swap out this
fold for a better approach
so maybe using something like skele now
or to be castro's approach which will as
we saw in a previous talk actually find
the best way of writing it well that's
the problem is not only common but yes
so this was what it was
III think he might have been yeah yeah I
mean I owe you I only use the what is
that fifteen 1552 square matrices yeah
yeah but again so my original sort of
focus on this was not the was not the
actual paralyzation I leave that for
other PhDs in this department to play
with I don't like to get my hands dirty
with that as it were I much prefer
playing with with the programming
languages and sort of seeing what we can
what we can learn from how programs are
written so my main focus on this was the
how do we actually discover these
recursion and patterns in arbitrary code
and then I'll hand it off to somebody
else and they'll make it go fast
unfortunately I couldn't quite do that
for this one so you should some example
way after you are B minus C and B plus C
yes great beware the same places
yeah I guess that's why you mentioned
equational reasoning is a way of
searching for these jet possible
generalizations do you have a feeling
for how important that is in being able
to make so that's a that's a very point
unfortunately so yes so one thing I did
find is when I was writing the the
examples and doing the implementation
and thing was that they did have to be
in a fairly specific structure as as as
you student li noticed but what we can
do is we can use sort of fairly simple
refactorings so one of the one of the
simple ones is basically taking infix
and making them prefix that's that's
just the way of normalizing it shall we
say so I do believe there's going to be
a decent amount of or at least a minimum
amount of writing to begin with there
get it into a form even if that's only
relatively simple now you could say for
example so quick sort for example you
yeah the the the the one liner the
Haskell version we can actually do or we
can actually derive or infer or discover
a divide-and-conquer version of that
from the the one liner and we can do and
we can do that as part of sort of
refreshing ting isn't it or rewriting
and so on and so forth so I think
there's going so as intuition I caveat
with that so the intuition so far is
that this is going to be sort of varying
depending on the style of the of the of
the of the original function or program
you put into it but ideally what I would
like to sort of focus more on is so
there's I sort of alighted or sort of
glossed over sort of to two stages
really so the Anton unification gets you
or inferred substitutions but then we
sort of have to take those substitutions
and work out whether or not they can or
how they how they look as actual
arguments to the eventual fold so a part
of that using equation reasoning if
we've got these these substitutions and
if they they don't quite match up the
idea is we can sort of automatically
shuffle these these substitutions into a
form where you get something like Oh F
equals at least that's the that's the
current thinking so yes and that's that
sort of why I'm going to focus on more
now that I've got this this this core
idea</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>