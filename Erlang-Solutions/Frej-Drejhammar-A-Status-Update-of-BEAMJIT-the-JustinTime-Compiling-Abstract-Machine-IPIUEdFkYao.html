<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Frej Drejhammar - A Status Update of BEAMJIT, the Just-in-Time Compiling Abstract Machine | Coder Coacher - Coaching Coders</title><meta content="Frej Drejhammar - A Status Update of BEAMJIT, the Just-in-Time Compiling Abstract Machine - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Frej Drejhammar - A Status Update of BEAMJIT, the Just-in-Time Compiling Abstract Machine</b></h2><h5 class="post__date">2014-07-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/IPIUEdFkYao" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so the title might be a bit misleading
as this is not the status report it's
just not the main thing on my talk but
I'll try to talk about some not too
obvious details about the implementation
that I haven't talked about the year
before this year and the year before
that and also say a bit about what we
have in the pipeline for in the future
right so as Robert said I work at six
but this project is financed by ericsson
where i work together with the loss
Rasmus on but enough about those
preliminaries this talk is mainly an
introduction to how beam get the jet for
ailing which we've been developing works
and a detailed look at some of the
not-so-obvious parts of its
implementation and the talk is
structured like this I'll start with a
background into just-in-time compilation
then I give a high level view of beam
yet before digging deeper into two parts
of it in the implementation namely the
beam jetta we're optimizations process
we do inside LOM and also our compiler
supported profiling and then I'll round
off with future work and questions at
the end just-in-time compilation is an
optimization technique that is has been
around for more or less since forever it
was the first reference I know about is
MacArthur's list paper since 1969 and
it's a fairly common implementation
technique of almost all virtual machine
based languages has it today there are
more jet implementations for example for
JavaScript than you
can bother shaking a stick at and just
since last year there are been a number
of new ones and beam yet as our jit
system for airline is called why do we
want it we already have beam and I think
we have cost is somewhere in the
audience we have hype i should say well
compared to ahead of and ahead ahead of
time compiler such as hype a jet for
alan will give us a number of increased
flexibility in our usage of airline for
example we can do tracing without
requiring our system to switch
completely to fulham emulation we can do
cross module optimization and our
compiled beam code modules are no longer
our platform independent some something
which is not the case for hype neither
do j to require or beam kit require us
to strongly kappa our compiled binaries
to put a particular build of the
emulator and also being a jet it
integrates naturally with code hot code
update so when we started being Jayde we
set up a number of goals well the first
one which i think is the most important
that is that we're lazy so we want to do
as little manual work as possible we
also want to preserve the semantics of
plane beam so if for example your
program crashes and you get the best
actress you should get the same stack
trace as if you're running purely
interpreting we also want to stay in
sync with the beam the standard beam
implementation so when they do things
like adding maps we shouldn't do have to
do anything with our chit implementation
and want this process to be more or less
automatic
and also we want of course native code
generator that is state of the art and
our aim is of course to eventually beat
hype performance-wise at least in the
steady state when we have done our all
of our native code generation so how can
we do that well I already touched it
before we want to automate this process
we don't want to have to do any manual
work to catch up to the current bean
version and will also say that we're not
really interested in implementing that
our own native code generator student
generators who want to use off-the-shelf
tool for that and what it looks like
when we started was that the tracing jit
compiler was the way to go and what that
is I'll cover in I think two slides so
what do we have to work with them well
we have the beam the plane standard
virtual machine that they're long run
some it's a register machine 8s around
150 instructions when they added maps I
think they got two or three more they
are at low time specialized into around
450 larger instructions using a peephole
optimizer and this is to cut down on the
instruction decoding overhead
instructions are fairly complex so they
are just like they can do a lot of
things it the beam is hand written in C
some parts are automatically generated
the most of it is handwritten see and
from jit implementers you there is a
nasty thing is that this
an instruction set is not concisely
defined anywhere it's defined by the
source code so to be able to do this the
key tools we are using are from the llvm
project and l of m is compiler they call
themselves a compiler infrastructures it
contains a collection of modular
reusable compiling toolchain tools and
it uses a low-level representation to
represent the code you want optimized
and generated natively native code for
and LLL a.m. comes with a C compiler
which is mostly GCC compatible called
clang and a very nice nice thing about
clang is that it's also available such
as a library so you can give leap and
that library is called lib flying and
you can give it to see module and get
back to abstract syntax tree for that
see module and that is a key tool we've
been using for implementing an automatic
process doing this automatic processing
of the beam beam implementation which we
need for our jit implementation and I
said briefly before that our cheating
strategy we are using as a tracing jet
and a tracing it works by figuring out
execution paths in your program which is
most frequently executed and we do that
by proof by first profiling profiling a
program to high find hot spots or hot
posts through it and when we have found
such a hot path we trace the execution
flow from that hot
on that hot spot and when we have found
a good phrase we turn that into native
code and then we run the native code so
this has the benefit of cutting off many
execution paths which never occur in our
program and with that out of the way now
it's time for a high level view of binge
it and I'll going to be describing some
parts of its implementation the how we
do profiling how we do tracing how long
do native completion I'll use
concurrency to mosque some of the
overhead incurred by just-in-time
compilation and also at the end I'll
cover some my show us show you the
current performance briefly the inner
workings of beams it as it's an a
tracing jit compiler it uses lightweight
profiling to detect when we're at the
place which is frequently executed and
when we found such a place we trace the
execution flow until then we found
something that will believe is a
representative trace our program then we
come pilot trace to native code and when
we're then running our native code we
monitoring execution to see if maybe
this trace wasn't good enough and maybe
we should try to extend our trace
internally being it consists of four
main components it contains a profiling
interpreter which is more or less than
standard unmodified beam interpreter but
it has been extended with some parts
that are automatically generated and
inserted into it by some C preprocessor
magic
we have a tracing interpreter which we
use to record the using did the
execution execution flow and then we'll
also have a clean up interpreter which
we use to get back to the profiling
interpreter if we abort tracing halfway
through a beam instruction and the
reason it's done with these three
different interpreters instead of just
one where all this functionality is
baked into one interpreter is mainly a
performance thing because it turns out
that if you're merging everything into a
single interpreter our a/c compiler
finds its work very hard to optimize the
code so for from and that is mainly due
to many entry points into this
interpreter loop and that absolutely
kills performance the compiler spills
more or less everything onto the stack
and never dares to allocate things in
registers so therefore the somewhat
strange Mazda interpreter in
implementation we also have a code
generator which uses llvm for
optimization and native code generation
and ed uses a fragment library of small
code snippets which are extracted from
the profiling interpreter to turn a
trace into native code so that is the
internals of binge it I've said that we
are doing lightweight profiling and we
do that with help from the compiler so
we have the compiler identify locations
in the program and we call those
locations for anchors which are likely
to be the start of sequences of
frequently execute
pin code and then the runtime system
measures to execution intensity of these
anchors and if the execution intensity
is high enough then we start tracing and
exactly how the compiler finds the
anchors is something I'm going to cut a
cover later in this talk it's one of
these not obvious details so when we
find something that's it intensely
executed we start tracing and as I've
hinted earlier tracing uses a separating
interpreter and during tracing we record
the beam program counter and the
identity of each basic block we execute
inside our interpreter and a trace we
consider trace successful if we reach
the end we started from meaning that we
have found a loop or we are scheduled
out and to limit memory consumption if
we're doing multiple passes through a
trace we make sure that we actually
follow they were already recorded trace
so we can actually say that and we use
this to determine when we have a good
enough trace or a representative trace
because when we are just following along
the same old path this trace will stop
growing so we say that one this we have
passed through this anchor and I'm n
times without the trace growing then say
that this is a representative trace now
we can start native compilation and
native compilation is done by first
gluing together LM mi or fragments which
we have extracted from our from the beam
implementation
and that those fragments when we glue
them could get together with insert also
guards into the code flow to make sure
that we stay we are staying on this
trace we have recorded if not we insert
a call to this cleanup interpreter that
allows us to execute until the next beam
instruction boundary and then return to
our plain old standard profiling
interpreter and when we have our tres
represented the seller FM I are we
headed off to el avance optimizers and
int optimizes forest and then eight and
its native code and we have one extra
optimization which is not found inside
all of em which is which encodes beam
beam specific knowledge and that is one
of the other details i'm going to
correlate it today a drawback with using
llvm is that it's pretty slow so it
takes a while from the time you have
produces tale of em I or code till until
it's optimized and available to run and
we can mask some of that overhead by
using concurrency so we run our code
generation optimization and I Takoda
mission in a separate thread and while
we are compiling code we temporarily
disable tracing because it's there's no
use in producing a huge backlog of
possible traces when we're perhaps
compiling the first race and yes
so where are we now if we consider beams
its performance currently the figures
i'm going to show you are just four for
the single core version so it's no
station p support SP support just
started working last week so i'm not
showing figures for that currently it
somewhat hit or miss although there is a
lotta more hit than miss
performance-wise looks like we're almost
removing all overhead for instruction
decoding which is good for short
benchmarks of course the tracing and
compilation overhead or mainly tracing
overhead dominates and we still have
some strange discrepancies which we
can't explain but hey this is a research
project we're working on it when beam
get that's good we the performance looks
more like this so in this graph there's
a bunch of benchmarks for each benchmark
there are two bars the left bar of those
two shows synchronous compilation
meaning that hey we have a trace now we
compile it and we stopped we stop the
world and compile it and then when we
have it ready we continue the right bar
shows a synchronous compilation when we
say we find a trace we shall we hand it
off to a separate thread for completion
and then we continue interpreting until
the code is available and we can also
see the red part yeah sorry what we're
showing here is normalized execution
time and execution time is normalized to
the execution time of the benchmark
running on the unmodified beam so a
value of one point Sierra means that
were just as fast as the unmodified
system a value of 0.5 means we run in
half the time a value of 2 means we run
in twice the time and what you can see
here at least that some of them were
around sixty percent runtime of the
unmodified system and if we start trying
to figure out what actually happens it
turns out that it's mostly instruction
decoding overhead that we're managing to
remove let's see you might notice some
in some cases where strangely enough the
a synchronous compilation takes longer
time than running the program with
synchronous compilation like this one
the fifth from the right no not that one
that one and that is one of these
discrepancies we can't really explain we
suspect that it is an effect of
profiling still going on remember we
only stopped tracing my things are
compiling so we might be unlucky in that
we are having a program which starts
executing we find a trace we compile it
and then we go on interpreting and then
we change to another part of the code
and then we have maybe another trace or
selected another trace and that way we
pay for the tracing over head multiple
times and that's why we're actually
slower when we should probably when one
would think that
a synchronous compilation would cut out
the the compilation overhead but these
are what we call the good benchmarks we
have also a bunch of other benchmarks
which are really really bad and the
black Bart's then they are outside the
scale so they are truncated but you see
can see we have things benchmarks but
we're over 10 times as slow or 10 times
slower than down modified system and if
we investigate that it turns out that
most of them are very small so our
overhead for tracing dominates and you
might ask when warm white on you just
increase the size of your benchmarks
then well if we start doing that then we
could make the overhead for the
completion or return in logic like just
increasing arbitrary small but just
increasing the size so we have decided
not to do that mainly to keep ourselves
honest okay there you have it that bend
it in a nutshell so now to some of the
more interesting implementation details
llvm gives us surprisingly good
optimizations and they are
state-of-the-art they are surprising
good at eliminating redundant tests and
things like that but it they can't can't
help us with a frequently occurring
pattern which is something that occurs
in most airline programs and I'm going
to illustrate that with a hypothetical
beam instruction we have an instruction
that takes a register and an immediate
value and ads the ads that immediate
value to the register and that's it
it will its implementation looks
somewhat like this it fetches the
registry index and the immediate
immediate relative the program counter
then it does the update and then it
updates the program counter and then it
goes on to the next instruction if we
encounter that instruction in the
recorded trace it will look like
something like this that we have the
code for a previous three century we do
the loads then we do the register update
we update the program counter and then
the code for the next trace entry comes
but remember this is airline the code
area is constant so the program counter
always points to something that's
constant and in the trace we store the
pc values for this particular block of
code and we have guards to check that
where are on the trace and we have we
know the pc on entry teach basic block
so why don't we just do the loads at
compile time so this part will then just
become a single thing update register
fixed index with a fixed value and then
we even know where the program counter
will end up but remember if the next
instruction looks somewhat like this
then we'll also be able to help demise
above optimize away the program counter
update and this is an optimization when
we implemented this it turned really was
something that increased performance
tremendously previously I said that
compile the profiling was helped along
with the
by the compiler so now it's time to look
a bit deeper into that part first by
describing why we're profiling then
describing how we do it and beams it and
then I'm going to show you some examples
that show that profiling or deciding
where to insert these profiling
structure in these anchors is not
necessarily as simple as it seems but
first why do we do we profile well we
want to find frequently executed code
and also frequent if we manage to
optimize something if we manage if we
direct our optimization effort at that
parts that are most frequently of the my
free most frequently executed that will
of course gave us the best bang for the
buck and that is standard JIT thinking
and traditionally you have considered we
have considered inner loops as a good
target they're frequently executed and
the compiler can usually flag a loop
construct and that's it's good because
if you have compiler support then your
runtime can be stupid and it doesn't
have to be smart enough to figure out
what this loop and I said before we call
the flag locations in the program for
anchors and when we have found our lack
anchors we at runtime condemns it we
maintain a timestamp for each anchor and
we measure the intensity execution
intensity by incrementing encounter each
time where we encounter an anchor that
what was visited recently otherwise we
clear it and when the account becomes
high enough then we start tracing we
also in order to in some cases
find traces which are or anchors which
are poor places in which to start the
trace so we have makin it to blacklist
them so we are not bothering starting
traces from that location anymore so
I've sidestepped the issue of where to
place anchors I just say that I'll start
of loops airline does not have a
syntactic looping construct apart from
list comprehensions but they don't count
because they are compiled to Taylor
cursive functions or recursive functions
and as someone once said to iterate this
human to recurse divine we use recursion
for our looping needs so the simplest
way to support profiling is to have the
compiler insert bank and anchor at the
head of each function but is that enough
of course as I asked it's not if we look
at this example here we have a stupid
way of multiplying by four and we see
that compiler has inserted than an
anchor at the head but how many loops
can you see in this code there are
actually two loops here one first when
we're recursing down and building things
on the stack and then when we have
Boston out in the base case and we're
doing all these additions and our tracer
will perfectly find an anchor and start
tracing when we're building the stack
but when we were we are collapsing this
tag
then it won't trade your tracing at all
so therefore we have to have the
compiler insert and anchor also
following each call which is not in that
tea in a table position so as to be
inserted right there after the
multiplication or the recursive call and
it's that enough well you can probably
guess it's not what about a ten tenors
remember that at race starts at an
anchor and ends when we reach the anchor
we started from or when we're shedule
out if we're having an event and then
Taylor looking somewhat like this we
stand around waiting for message we when
we get a message we handle it this is
not this is how you write in an airline
but what it's compiled to is actually
somewhat more like this we have an
opcode to wait for the next message and
one then message arrived we do pattern
matching on it and then dispatch to our
cases and if it doesn't match any of
those patterns we have we postpone the
liver of that message so in this case we
would be shaded out then suddenly
message arrives we're settled in we do
the pattern matching we call recursively
call our handler again and hey where we
encounter an anchor so maybe we are
triggering tracing and then suddenly and
then directly if there are no messages
in the queue we will be shuttled out
that's not very good because we're only
babe we will only be able to natively
compiled stuff we have a trace for and
this is a very stupid trace it only
contains
a call for waiting for the next message
so what should we do then well we have
the compiler insert an anchor after each
time we receive message and then things
look much more healthy in that were
scheduled outweigh informations you
arrive we are scheduled in we encounter
the first anchor and then that means
that we can start tracing so then we go
on doing the pattern matching then we
call our handler again and then we're
 it out and suddenly we have a nice
nice trace that actually contains the
code you want to be cheated right so now
for the last couple of minutes in of my
talk I want to cover some of the things
we're working on right now and we have
planned for the future the first one is
full S&amp;amp;P support currently I said that
we're just the benchmarks I showed were
just for the single core version and for
being for us to be able to run on real
world benchmarks we must get SMP support
working we also want to have a much
smart chip that knows about Biff's which
it doesn't do today and will also want
to do some more beam specific
optimizations so the first one is full
S&amp;amp;P support currently our prototype only
supports profiling and tracing by a
single scheduler although all share
dealers can run native code if they are
active currently breakpoints and tracing
and code update is broken so that is
something we feel that we need in order
to be able to run really benchmarks and
allow people to actually test this on
real things currently will also limit
the JIT to the inner interpreter loop
that means we miss our OPEC to us and we
want to extend our set the same
techniques to also cover Biff's and that
is something we are going to need in
order to eventually be type also talking
about the hype another optimization we
feel that is needed is that we want to
in first or give Alabama optimizers some
knowledge about the island heap so for
example we're now building objects on
the heap which may be not used so for
example if you have something looking
like this you have a function that
returns a tagged return value which you
almost immediately rip apart because
you're only interested in the our part
nowaday right now we are forced to
create that tough will on the heap and
then do pattern matching although our
optimizer will be smart enough to see
that hey this return value it was
created somewhere it was inserted into
this newly created temple and then it
was immediately ripped out pattern much
out of that Apple the optimizer is now
smart enough to actually use that return
value in a register but it's still
trading this stuff on the high on the
heap so we want to be able to have the
optimizer remove this redundant heap
object of allocation and that is an
optimization that hyped us today but as
far as I understand it that can only do
it for
module local functions which are not
exported but as we're running a tracing
it we should be able to do this for more
or less any functions that bathes in
this way and this is also something
that's good enough tomorrow if we get
this optimization working it will deal
with more or less all kinds of boxing
unboxing that is performed on the heap
well that's all i have for you today so
questions
I have you tried to compare the
benchmarks with hype yes and until
almost very recently we've been so bad
so it was embarrassing and not very
meaningful but lately we're at least
getting in the same neighborhood for
some of our benchmarks and I should
mention that the venture we are running
on are the hype benchmarking sweet so
this is not strange that type is good on
them
yeah I'll just wondering you mentioned i
think the slide before this one was
cross modules yeah how's that going how
would that work with the module module
loading that is one of the things that
are right now broken but it will work
because we're actually tracking modules
or when we're creating traces we track
the modules they are passing through so
if you doin up a code update and purge
code we can actually evict native code
that passes through those modules and
then we just had to retrace and then
produce new code okay so if you reload
just stop start over again yeah yeah
okay any more questions oh oh thank you
and we're waiting thank</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>