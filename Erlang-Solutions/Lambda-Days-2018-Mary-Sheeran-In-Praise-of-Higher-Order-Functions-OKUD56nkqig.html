<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Lambda Days 2018 - Mary Sheeran - In Praise of Higher Order Functions | Coder Coacher - Coaching Coders</title><meta content="Lambda Days 2018 - Mary Sheeran - In Praise of Higher Order Functions - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Lambda Days 2018 - Mary Sheeran - In Praise of Higher Order Functions</b></h2><h5 class="post__date">2018-03-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/OKUD56nkqig" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">this talk is called in praise of higher
order functions and of some friends and
heroes and my intention with this talk
is that you go away with a new hero so
I'm hoping you'll pick one of the famous
people I talk about in the talk and go
away and read some of their papers or at
least go away and read one of the old
papers I talk about it that's my aim for
the talk mmm
why do I want to talk about higher order
functions apart from personal
fascination one is that it's a big part
of the functional programming experience
now I'm a high schooler so the small
bits of code I show you will mostly be
in Haskell this is a quote from the
website called learn your Haskell calm
which is a great place to go away and
learn Haskell if you haven't tried it
before and they say higher-order
functions aren't just part of the high
school experience they pretty much are
the Haskell experience so it's a good
idea to study them I'm going to talk
about three higher-order functions only
map fold and scan that's the the topic
of this talk but there's plenty to be
said about these three three functions
and I'm gonna start with some history
I'm gonna go back to 1959 III thought if
I'm going to talk about map fold it's
gonna better found out who invented them
so I went back and I looked around and
read some papers and I found this page
of the Lisp manual from 1959 March 3rd
1959 and it's got a definition of a
function called map list and on the top
there's some text some documentation and
on the bottom there are two different
implementations of map list and I'll
read out what it says on the top it says
map list of LF so it's got the list
first constructs a list in free storage
whose elements are in one-to-one
correspondence with the elements of the
list L the address portion of the
elements of the new list at J
corresponding to the element at L
contains F of car of L
means head the first element of list in
Lisp and then there's an implementation
down at the bottom which he calls slow
map list and it says if L is the empty
list then returned the empty list
otherwise return cons of F of L and map
list of coder of L and s cutter means
the tail of the list list and this
actually doesn't match the documentation
because it I would have expected to say
at the bottom mapped list a cons of car
of s of car of L and that list of Qatar
Abell but it doesn't say that so this is
this is a little bit of a surprise a
mismatch in the documentation so it's
not surprising that you I look further
in the list manual and I come to a new
version of map list
and it looks like this and my hope was
that he would change the implementation
of the function to add the car but no he
changed the documentation to remove the
car so he's defined a function map list
which takes a list - a list of 80 list
of B functions a function and and if we
visualize it like this map list takes an
F and first of all applies F to the
whole list and then F to the tail of the
list and then F to the tail of the tail
of the list and so on so that's what map
list in list was and I was expecting a
different kind of map that would just
take a function and apply it to each
element of the list of course you that's
called mapped car unless in Lisp and it
can be defined in terms of map lists but
still I he didn't get it the way I
wanted it to be but still he went on to
win the Turing award in 1971 he's John
McCarthy who wrote those two manual
pages is like the father of artificial
intelligence and the developer of list I
have had lunch with him once but
otherwise not really met him I think
some people in the room have bet him
like for example Phil so if you want to
know more about well yeah so if you want
to know more about John McCarthy talk to
Phil afterwards um and I find it a bit
fascinating to discover these two manual
pages from the 3rd of March and the 20th
of March 1959 because I was born exactly
halfway between those two dates on the
10th of March 1959 oh so I was destined
to be interested to higher order
functions you see because this was being
developed Maps was being developed what
you know in the middle of it all so
let's think back it to 1959 though for
you it's a very long time ago even for
me what computers look like in 1959
here's an example of a magnificent
British computer called the Ferranti
Pegasus it was this particular one is at
the Science Museum in LA
unfortunately not still on show and it
was built in 1959 it weighed three
thousand eight hundred and twenty kilos
it costs 50,000 pounds despite not
having any input or output devices and
it had 56 words of storage 40 of them
were sold 26 of Pegasus 1 and 14 of
Pegasus 2 and it was considered a huge
success so this is what your computers
look like in those days and the
interesting thing about this computer is
that it was regarded as the first
user-friendly computer spice it's 56
words of storage okay and the logical
design of this computer was done by one
Christopher strachey so I'm hoping this
will be your new hero and mr. Strachey
was Renaissance man so that's a new kind
of hero right
um he was from the British aristocracy
and he seemed to be able to do
everything he was the first person to do
computer music at all that's preserved
it was the British national anthem that
he got the computers play and he was
also a super hacker in the positive
sense a great programmer and what I'm
going to talk about is some a program
that he developed so in 1961 he started
to experiment with higher order
functions so map was already known but
he developed two higher order functions
that he called in those days r1 and r2
but they were fold or unfold l2 versions
of reduction he lectured about them in a
summer school in 1963 in Oxford and then
David Barron wrote up the lectures from
a tape recording of the of the lecture
to make a paper that was published in
1966 and the title of the paper is
wonderful it's called pro
drumming those were the days when he
could write a paper called programming
okay and this paper introduces map
unfold and and shows a number of
different examples and Olivia Delvian
Mike Spivey wrote a paper in 2007
nominating this paper as the first
functional Perl so this is like this
paper from 2007 and I CFP the functional
programming academic conference
nominates this old paper as a Perl so
it's like a meta pearl okay and the dom
be inspired paper is very nice it
explains the examples that are in the in
the 1966 paper let's look at some of the
higher-order functions that are
introduced there
the first one is map this is written in
a language called Cpl which was not
implemented at the time was too it was
too fancy and too difficult to implement
later there was another language called
BCPL which led to led to see so house
map to find it's defined the way we
expect it to be defined okay map of F on
L is if the if the L list is empty then
return the empty list otherwise return
cons of F applied to the head of L and
Mac of F applied to the tail of L so
this is a recursive definition of a
higher-order function map that applies F
to each element of the list we would
write that in Haskell like this so map
takes a function from A to B and a list
of A's and returns a list of B's if you
apply map F to the empty list you get
the empty list if you apply map F to a
list whose head is X and tails its tail
is X's you get a list whose head is f of
X and whose tail is a recursive call of
map applied to the tail of the list so
if we you know ask GHC I the the intern
the Glasgow Haskell compiler interpreter
what is map of a function times two or
double applied to the list 1 to 8 we get
2 4 6 8 and so on now because of my
history I have a bit of a tendency to
visualize functions like map with
pictures
I know it's disapproved of by many
people but I like to draw pictures to
think about these higher-order functions
so this is like a an example of a higher
order of map the the list comes in at
the top it's its head is X its tail is
XS and it supplies the blob the F to
each element of the list so straight and
perÃ³n and straight you introduce map in
their paper they also introduce a
function they called list iteration or
fold that we call it fold these days and
here's the definition of that it says
lit applied to F and 0 and the list is
if the input list is empty return Zed
the 0 otherwise return F applied to the
head of the list and a recursive call of
lit with F and Zed applied to the taalib
list this is the function we would today
called for our school terms and it's
defined here so it's it takes a function
from A to B to B and the B which is the
zero and the list of A's and returns a
single B if you give it the if you give
fold or of F and that the empty list
you get said if you give it a list whose
head is a and tail is a z' you get F
applied to a and fold or of F set and
the tail of the list that is a z' and my
visualization of functions gives I just
turned off is it still going
the blue box is an F applied to a and
the result of the all the orange boxes
which are a recursive call of fold are
applied to the tail of the list so this
is a way to visualize what fold or does
and if we asked HCI what does fold are
of x on the list one two eight gives us
it gives us what
what would you name this numbers 8
factorial yes and here's what's
happening it's it's taking the list 1
cons to cons 3 and so on cons the empty
list and it's replacing each cons by the
function x and it's replacing the empty
list by 1 and then it you get all the
multiplications and you get factorial 8
so that's what fold or does to a list it
takes the conscience and replaces them
by function and it takes the empty list
at the end and and replaces them by
whatever is 0 you told it so let's look
at another example that straight she has
in his paper what if we do fold or of
cones and something let's say the
singleton list X so that square brackets
thing is a single - list containing X if
you do that you get a function which
takes the X and sticks it on the end of
the list that you've given it and I've
got to visualize that so that the thing
with the boxes is a visualization of the
fold are that does that and on the right
I'm going to draw us a different way the
X is coming down and coming around and
being added to the end of the list and
let's use fold or again let's do fold or
of app and the empty list and else what
does this do anybody guess it takes a
list in what does it do to the list
reverse it reverses the list this is an
example also from straight cheese paper
so revs of L is just fold or of app and
the empty list and L so you can do a lot
of interesting things with fold or in
particular you can define map so mat of
G of L can be defined as fold or of an F
the empty list now where the F is takes
an x and a y it applies G to the X and
then comes us that on to Y and i've
visualized that again so if you kind of
follow each input to this fold or it
goes through a G and then it comes out
the end so it's a map you can define
fold or in terms of you can define map
in terms of fold or so we need to
bothered starting with map we could have
started with folder 1 final map like
function instead of so in the definition
of map we on the right hand side we suck
an empty list into the fold our ok now
we're going to add a 0 instead so now
we're going to say map a of G and some 0
is fold or of the same F as we had
before but with the 0 on the right hand
side and this does a combination of map
and append that plus plus is the thing
that combines two lists to give a longer
list
so with folder or you can do something
that is like a combination of map and
append and we'll use that again later in
an example so now I'm going to talk
about the kind of main example that Dom
V and Spivey talked about in their
admiration of the 1966 paper and the
main example is Cartesian product so it
relates a bit back to Phil's talk from
yesterday he talked about product
so we'd like to make a function which
takes a list of lists and gives us all
the ways of T of all the ways of taking
one element from each of the lists so if
there were two lists only it would be
like Phil's product that he had
yesterday was a product of two things
but now we have a product of many things
so for example if we have the list a B
and then some more stuff how do we
compute this product we take the product
of the more stuff and we stick an A at
the beginning of each element of that of
that result and we stick a bit at the
beginning of each element of that result
or recursively if the first list is a
cons a's what do we do we take the
product of the tail of the list we stick
a onto the front of every element of
that result and we also take the product
of the the act XS that the tail of that
list counts onto the list of elements
that we have already so this we're going
to get a program that looks like this
this is this is the the first program
that Strachey gives in his in his paper
so what does product do it's it takes a
list of lists okay if that is empty then
the result is going to be a list
containing the empty list that's what
those funny square brackets are on the
on the first line of the definition if
the
the first list is empty so if the head
of the entire list is the empty list
then the result is the empty list this
is a bit tricky and if we have ex-con
sexist in the first list and the rest of
the list is accessed then what we're
going to do we're going to take product
of X's and stick X at the beginning of
each element okay so that's a Mac of
ex-cons over that product and we're
going to append to that the product of
the remainder of that list
Collins exes okay so we've got two
recursive calls of product here so we've
got what we do what they do in the DOM
be inspire paper to try to explain
straight easy answer is say let's take
some of these the calls of product that
are where we know that we have a head
and Italian but that's called call some
of the calls of product H so that's all
we've done here so now we have three H's
down at so some of the cook that the
second call of product has been replaced
by an H which we now define so now let's
look at this function H recursive over
its first input and so the but it calls
product on its second input this it this
is not good so we're going to call
product many times on the same input as
we were occurs over the first the X
context is so let's take that product
instead of doing it many times let's do
it once and for all in the definition of
product that's what happens here
we will do product once and for all so
that's that's reducing the number of
times we call product so we still have
the same behavior of this program in
fact I had to type them all in to
convince myself that it was really true
and very worryingly one of the papers
and straight she's one of the functions
and stretches paper was not correct so I
spent a lot of time trying to figure out
why that was um so we make a step where
we reduce the number of calls to product
and now if you look at the very bottom
line of this you have a map of something
followed by an append and we saw that
before I showed you map a before so
we're going to replace that by map a
right so we're getting somewhere
so now let's just do some coloring on
the on the code we have some blue code
which is the definition of product we
have some black code which is the
definition of F and we have map a but
each of these bits of code can be
implemented with fold R if you look at
the blue code it's exactly like the
definition of fold are if you look at
the black code it's like a definition of
holder so we didn't just replace the
blue code by a fold or and the black
code by a folder and then wrap a by a
folder and we end up with this the music
a piece of code which is an
implementation of Cartesian product
using three nested instances of folder
now in sin the straight G paper he just
says he just shows us the very first
recursive definition then he shows us in
fact a rough a wrong definition that is
more efficient but I think that was just
a transcription error that when Baron
was transcribing from a tape reporting
of a lecture okay and then he just shows
this this he says our even our second
our second program is not very the first
program is not very efficient so we just
rewrite it this way and then it turns
out to be folder and there's no
explanation he he just wrote this
he just knew to do this and this despite
the fact that he had no computer on
which to run any of this well he was
running some of his programs by
transcribing them into Lisp and running
them but but there's no evidence that he
ever ran any of these fold our programs
well but he managed to get them right
anyway which is quite amazing so if
you're interested in in history of
computing go away and read the straight
paper it's fascinating or if you can't
trace that read the dole V and Spivey
paper which is also like a very nice
paper so we're about in 1962 or 63 still
I'm aged for what else was happening in
computing at the time there was APL how
many of you have ever heard of APL how
many of you Helmuth Iverson developed a
language called APL in the like early
60s like maybe even beginning late 50s
but he didn't implement it for six or
seven years this was a difference
between then and now and it was used
inside IBM as a hardware description
language and they used it to verify by
pencil and paper development or
calculation some of the implementations
of the early IBM machines which i think
is as a hardware a person that's amazing
but APL has something like fold or it's
called reduction and it's written with a
kind of slash so it says slash X is the
same as Z equals and then it says x1 and
a dot we call it dot L x2 dot L x3 and
so on that's the same as as fold our he
wrote a book in 1962 which is a very
interesting book to read some of it is
online though I think not all of it
he liked very concise notations of
programs and there was a special
keyboard and indeed that still is I
notice from the company called dialog
that sells
an APL interpreter still so you probably
can't see it but there's lots of funny
squiggles on these keys okay and APL
programs are extremely short into size
and Iverson's interest was in notation
and he viewed these as mathematical
notations as an age to thought so he he
won the Turing Award another Turing
Award winner here in 1979 and he wrote a
great paper at that point called
notation as a tool of thought that's
another paper every computer scientist
should read it's very interesting I've
seen an APL compiler that was like
written in APL there was like one a
four-page just of squiggles right so it
was that even right I think it was a 17
class APL compiler written entirely in
APL so it's an amazingly concise
notation perhaps a bit too concise
sometimes so we're still in 1960 two or
three and anybody recognize this person
it's John Backus another of my heroes
so in 1963 he was at the top of his game
he was made a fellow of IBM in 1963 and
the reason for that was that he had I
think between something like 54 and 57
developed the first he developed Fortran
and the first Fortran compiler so that
is the first high-level programming
language and the development of the
compiler was done with about ten people
somebody said he even hired
mathematicians and women so it was a
group of about 10 people who made the
Fortran compiler and they had a they had
a whale of a time they had a just a
fantastic time and I have seen recent
compiler talks that basically say we
haven't done much better you know since
1959 we haven't done much better than
Backus did in his his group and so that
was a tour de force to make the the
Fortran compiler and he was awarded the
Turing award in 1977 for that and for
contributions to ways to describe
programming languages
for example the Backus narrow form and
he wrote a great paper which we talked
about last year in our keynote - called
can programing be liberated from the von
Neumann style a functional style and
it's algebra of programs and you should
all go away and read this paper it is
the first half and he it's a manifesto
for functional programming so even
though he had you know won the Turing
Award for what he did with Fortran this
paper is a manifesto that says roughly
we've got it all wrong and we need to
start again
so he it's it's it's quite backers in
its style so he he says conventional
programming languages are growing ever
more enormous but not stronger inherent
defects at the most basic level cause
them to be both fat and weak and one of
the defects that he talks about is their
inability to effectively use powerful
combining forms to will to build new
programs from existing ones and that's
what higher order functions or combining
forms as he called them are all about
last year at the very end of our keynote
we talked about four important points a
functional programming related to Jones
why functional programming matters paper
but also they're very closely related to
what Backus says in his paper and the
four are are combining forms which this
talk is about and if you have combining
forms and you define them for example
over lists or over trees you are talking
about whole values rather than talking
about individual but he talked a lot
about individual words going over the
monoamine bottleneck which he named okay
so combining forms and whole values kind
of go together and his his thesis is
that you have to define your combining
forms in such a way that they obey
simple laws so that you can reason about
your programs a fourth important point
is functions as representations and I'm
not really going to talk about that here
except to say that in the Cartesian
product example that I showed you
if you use a functional representation
of the list Allah don't use then you
only need to fold ours so go away and
play with the Cartesian product and
you'll see and I'll give a quick plug
for the fish stock because that's really
about functional representations room
one at three o'clock today don't see the
fish talk what is an example of a simple
law here's one both in code and in my
pictures map of s composed with map of G
is the same as map of s composed with G
now with my hardware designer view view
of things
this doesn't make a lot of difference
they're kind of the same circuit if I'm
taking a you know a map and making it
into silicon you might say but if we're
talking about software these two
programs can have very different
behavior and because you might think of
it as there's two iterations over a list
so it's like having two loops on the
left and one loop on the right or you
might think of it as there's an
intermediate list produced between the
two maps okay so when you're a
programmer it's important to be able to
apply these kind of transformations and
you'll get programs that behave
differently they'll have the same
semantics the same input-output behavior
but their performance might be very
different their memory use might be very
different here's another example of such
a simple law fold R of F and V applied
to map of G axis is the same as fold R
of F composed with G and V and axis and
F composite of G is function composition
backward function function composition
as as we'll talked about yesterday and
these again will have for similar
reasons possibly very different behavior
as programs now where we're at
look we're asked about 1979 here now 70
1980 I was a lek trickle engineering
student at that time I didn't see any
functional programming in my
undergraduate degree but I did see
formal method
which was very new at the time - and I
got sent off I was in in Ireland I got
sent off to to Oxford to study formal
methods as a for a master's degree and
this is we were the second cohort of
master students in Oxford and this is me
with all the man
I actually have noticed that at the time
and nobody really said anything but this
is us on the very so Oxford's the
program research group was where
straight she worked until the end of his
life but unfortunately the end of his
life was in 1975 before this but
straight she's spirit really dominated
at the PRG so you could say that the
master's course that I took was telling
us straight she's view of the world okay
it was fantastic and on the left bottom
of those rows you see Joe Toye who
worked a lot with Strachey he taught
denotational semantics on the course he
also taught functional programming on
the course which is where I first saw it
and it was mind-blowing a fantastic
course we did a lot of reasoning about
programs and in the middle you have Tony
Hoare who was following on very much
from Strachey's view of the world so at
the end of my one year masters I got
asked to stay and do a doctorate and I
said rather I was a bit surprised but I
said yes and I moved into a house in
Oxford with a young man called John
Hughes it was just a shared student
house my supervisor had said you know
John John need somebody else to live in
his house so we went and the rest was
history and in about 1981 we were we
were driving to a workshop in Edinburgh
and in John's ancient Ford Cortina and
he told me as he was driving about
backus's paper and I had had spent you
know several months already at this
point trying to design a hardware
description language and then he told me
about backus's paper and combining forms
oh well that's it that's what
we need for a hardware description
language ah got me into all this you
might say and so my work was about
taking back as as his ideas and applying
them to the description of circuits and
using the kinds of laws that he espoused
to reason about the behavior of circuits
here's an example of one of these laws
if you have if you can push a blue blob
through an orange box and get to blue
blue blobs then you can take something
that looks like this
and gradually play with the blobs push
them around oh sorry
and end up with this and this is a law
if those blue blobs are unit delays for
example then this is a rule about
retiming of circuits or pipelining it's
sometimes fall and you can see example
an example of all of the combining forms
that I ended up with in my description
of circuits that I didn't have in my
description of programs that triangular
thing came up all over the place in
circuits and in fact it was implemented
we were talking to G you see at the time
about regular circuits and they had
triangles of utilize all their circuits
okay so it was nice to be able to play
with real circuits um in those days we
had to draw the pictures in our papers
by hand with a with a pen and we had to
submit the papers with on paper with
little nose hole pasted on pictures okay
so it was really a different process to
write a paper in those days but
eventually I did manage to publish a
paper in 1984 about all this and I
presented it that it was called once
they called Lisp and functional
programming I think the name of the
conference was it was in Austin and
Backus was there and he came up to me
afterwards and he said that was the
nicest use of FP I have ever seen and I
thought okay now I can die I could die
and go to heaven at this point okay um
and in the end in the same year actually
in 1984 I got a doctorate for doing that
work and John got his doctorate at the
same time so that's us looking very
young for my doctorate my in
Colonel examiner was this person Richard
Byrd on the left and I want to tell you
about the bird merchants formalism so
the bird merchants formalism was you
might say an approach to reasoning about
programs much influenced by the ideas of
Bacchus and by by other people so the
idea with bird madness was that you
write down an inefficient version of
your program and you apply simple laws
to the program until you eventually get
to a more efficient version for example
in 1986 Richard wrote a monograph he was
at the programming research group also
he wrote a monograph called a theory of
lists and you should go away and read
that too it's beautiful so this is
programming as an intellectual exercise
for simplicity and beauty is important
it's just wonderful stuff they tended to
use quite complex notations - a bit like
APL and FP so it was sometimes called
squiggle and it was a bit like a secret
society there was a there was a journal
called the squiggle list and and which I
have a few copies of published in the
80s and early 90s and it says on the
inside of it
you can't subscribe to the squiggle list
you either get it or you don't
so this is the secret society on an
island in in the Netherlands in 1989 so
burden mutants are you can see there are
leaders there in the middle of the photo
and we are the disciples and one of the
disciples was a young man called Graham
Hutton who actually was my one of my
first students in in Glasgow and I would
say that he is probably the person who
took the torch of bird mertens and that
approach to reason about programs and
and stuffed with it and one of the
people who invest so he writes beautiful
papers about programming examples he's
also written a very nice book about
program
Hostel so if you want to learn how
school that's the way to do it and he's
written a paper called this is a 1999
called
a tutorial on the universality and
expressiveness of fold I told him I was
going to show his wedding photo
so that's him looking very Scottish I
asked I asked permission to show it and
he said yes please do you know there's a
story about this paper I wanted to call
it fold but the editor wouldn't let me
let's it's published in the Journal
of functional programming and so he says
as a result not many people have read it
it would have been better to call it
fold and and this paper is is is very
beautiful he talks about the universal
property of fold so Phil talked about
universal properties yesterday so this
is the universal property of fold these
two things are equivalent so that thing
over on the right is an equivalent so if
you know that G of the empty list of V
is V and G of X cons axis is of f of X
and G of X is that's the same thing as G
is fold of F and V that's one property
of fold and another property of fold and
it brings up is if what he calls a
fusion property of fold and that's a bit
like my retiming law that I showed you
before it says if you know that H of W
is V and H of G of X and Y is the same
as f of X and H of Y so this is one of
these can you push the blue blob through
examples then you can rewrite the
composition of H and fold our as just a
single folder and I'll show you that in
pictures because it might be easier to
understand it is for me at least so if H
applied to W is the same as V that's
that the two left the left of the
picture and if you can kind of push an H
through a G converting it into an F then
you can take an H and push it through a
fold of G and convert it into a fold of
F now why is this important
why do we need to know things like this
we're
sitting and we're trying to calculate
our programs in the Birdman style we
would like laws that encapsulate
inductions we don't want to apply
inductions when we're doing these
calculations of programs we would like
to be able to apply laws that replace
inductive proofs so that's the whole
point of this so you're you're all you
know developers and you're thinking to
yourself this has nothing to do with me
no I I don't write so perhaps I don't
want to calculate my programs but there
are ways there are ways to make this
easier so here's another plug for a talk
this afternoon in room 3 at 1555 Max
who's here will talk about quick spec
I'm not gonna say very much about it
except that it allows you to find out
laws about Haskell functions so you have
a bunch of Haskell functions you tell
quick spec which functions you're
interested in here we're interested in
map fold composition the empty list of
cons and it goes away and thinks and it
tells you a bunch of laws about those
functions I've taken away some of them
because they were related to the
definitions functions but here are some
interesting ones for example that
function composition is associative
the one about map of F composed with G
is the composition of maps and the that
one I told you earlier about the
composition of folder map it tells me
that and in fact it told me it in a
nicer way than I had it written down
before so I had to go back and change my
earlier slide so that's a quick spec is
extremely cool and go to the talk and
hear more about it so why are combining
forms are higher-order functions it
useful and important they capture
patterns so they are they capture
patterns of computation so they are form
of abstraction an abstraction is
important for programming after computer
science they support the whole value
programming and that can be important
also in the presence of parallelism for
example data parallel programming
and the enable reasoning by the
programmer because they if you define
your combining forms I think of them as
being like Lego and if you define them
so they fit together nicely and have
nice laws then you'll be able to reason
about your programs but here's an
important point the enable reasoning by
the programmer but they also enable
reasoning by the compiler and that's
important
so there's a whole tract of research
about that so we have another hero here
Phil wrote a very famous paper called
deforestation transforming programs to
eliminate trees in 1990 and then there
were some so this is about avoiding
intermediate data structures and then
there was a follow-up paper from about
1993 called short cut deforestation by
Gil launch Bri Paton Jones I think and I
thought to myself maybe I'd like to find
a paper that I can that really makes
this clear what this is all about so I
found then actually quite a much later
paper from 2001 the Haskell workshop and
it's called playing by the rules
rewriting as a practical optimization
technique in GHC and it's about the
rewrite rules that you can add for
example as a library developer when
you're using GHC so that's you can add
laws about your functions which then GHC
will apply during compilation and in
particular this approach was used to do
what's called list fusion which is the
getting rid of intermediate lists and
what they did was they rewrote all the
prelude functions the the kind of
standard functions in Haskell in a new
way so fold our remained as it was the
same definition as before but they
introduced a new function called build
and build of G was just G applied to
cons and the empty list
we've seen that before and then the
point is that fold our and build when
they meet each other you can cause the
intermediate list to disappear so fold
our of K&amp;amp;Z applied to build of G you
don't have to first give build the cons
and the empty list and then later
give give K and that you can just
immediately give K&amp;amp;Z to to G okay so you
can avoid an intermediate list by
figuring out where folds and bills meet
I think of them as kind of meeting and
the list the intermediate list
disappears and it's a very nice paper
very easy to read and it's just
beautiful okay okay so we've talked
about map and fold and reasons that they
are interesting there's one more
higher-order function I'd like to talk
to you about so we talked about fold or
there's a there's a kind of fold out
which has the arrow flowing in the
opposite direction in the diagram and to
get to scan which is my final
higher-order function you just take the
fold out and you output all the
intermediate values along the way so
it's like many calls of over-over
reduction here's the definition of it
scan L of F Z and the empty list is the
singleton list containing that and scan
L of F Z and ex-cons axis is Z cons scan
L of F and the new 0 is f of Z and X and
XS so scan L so fold took a function a 0
and a list and gave you a single value
Scannell takes a function a 0 and a list
and gives you a list in fact it's one in
this definition it's one longer than the
input list so let's take an example if
we apply
Enel of + + 0 2 2 4 6 8 10 we get what's
called the prefix sums of that list we
get 0 we get 0 + 2 we get 0 + 2 + 4 and
so on
so we get 0 - 6 12 and so that's called
prefix sounds and if we draw the picture
it looks like this okay so the the
there's a recursive call of Scannell on
XS that's the orange part and it both
produces all of its outputs and produces
know that the f' produces an output and
also passes an output on to the
remaining as gano
so Scannell was figured figured in a lot
of the examples that were done by the
squiggle errs okay and if you want to
read a nice paper about the the use of
algebraic laws in calculating programs I
think this is a nice one to choose this
again is Richard Byrd I think the the
paper is called algebraic identities for
program calculation and it looks really
at map fold and scan and shows
calculations of programs involving them
it's for the computer Journal in 1989
and and so here's an example of so we
saw map map fusion and we saw map fold
fusion and now there's a fold scan
fusion okay so this is again about
plugging together our ways of combining
functions so this is what it looks like
in text so if you combine a fold and a
scan you can rewrite it as just a fold
and this might be important so on the
top we have a scan on the bottom we have
a fold so that's the composition of a
scan and a fold and what what's writing
it as a fold is about is just you know
combining pairs of these boxes okay so
you can write this then as something
involving F
and then a fold of those yellow things
those jeez okay so they fit together
nicely and you can review it as a fold
that's the point of this and there's a
famous calculation that Richard did
involving using this law and a number of
other ones I'm not going to go through
all the details this is what it looks
like in the paper and it starts with the
program applies a law gets a new program
and so on and so on it's called a
maximum segment sum so you take you take
all the segments of the list you map sum
over that and then you take the maximum
and this is the Fibonacci of program
calculation if you want to study program
calculation you should look at this
example and the very last law that he
applies of there I think there are seven
or eight laws applied is the the
Scannell fold fusion that we saw before
and the whole point of this it apart
from the beauty and insight that you get
from doing these calculations is that
the original program is order and cubed
and the final program is order n so you
are changing the you have the same
semantics of your program in terms of
input/output but you may have very
different performance of the programs
okay so go away and read that paper too
and scan is of interest for other
reasons than reasoning about sequential
programs when you add parallelism to the
mix scan becomes especially interesting
so if you are in a sequential machine
the data dependencies demand that you
just do if you're trying to do a scan
the data dependencies demand that you
just do each F in turn but but
algorithms people have studied scan for
a long time and they've developed a
special diagram for showing compositions
of functions that the perform scan so
the diagrams show data flowing in and
down and along the wires think of
abstract wires the
dots are the F's or the functions okay
so this is a picture of a sequential
scan it does time is flowing downward so
it does it combines the first two input
values and then it combines and produces
some output and then it takes the next
input value and then the next element of
the list and so but it turns out that
even though that looks very sequential
linear there are if you know that the
function is associative then you can do
it much more efficiently in fact you can
do it in log depth okay this is
surprising and interesting and these
these things get implemented as circuits
all the time so if you look at a
microprocessor and do a heat map of a
microprocessor the hot spots will be the
prefixed circuits because the Intel
doesn't like to make slow circuits so it
makes very very shallow sir
and this particular version so here
we're we're inputting 0 1 2 and so on
and as each box we're adding 2 values so
for example here 0 1 gets added to
produce 1 1 2 gets added to produce 3 so
we we add together each adjacent pair
and once we've done that it turns out
that we can just recursively call the
same function on the even elements of
the list and on the other ones of the
list okay so we get then a log depth
scan program or circuit and in fact
there are many many different ways to do
log depth scan circuits it's a whole
research area of its own and it's
fascinating I've worked I've spent quite
a lot of time in the that black hole
it's a very interesting black hole but
I'd like to come back to a paper by
Strachey this is it this is the entire
paper so not only has he written a paper
called programming he's written a paper
called bitwise operations it's fantastic
you see why he's my hero
right we're trying to reverse
the bits of half of a word into the
other half of the word okay so that's
what he's trying to do here so that's
there's a picture at the bottom which
I'll try to explain so for some reason
we have 11 bits it's probably related to
the Pegasus but we have 11 bits and we'd
like to reverse them over into the other
half of the word so let's imagine that
we have them reversed okay and that's
count the distance that a has to travel
and that B has to travel and that C has
to travel so a has to travel one B has
to travel three C has to travel five
let's figure out what is the binary
representation of those distances and
then in a program what we're going to do
is first move all those together that
need to move 16 and then move all
together those that meet need to move 8
and then 4 2 and 1 so this is a log
depth bit reversal program from 1960 so
it basically contains the same idea as
the scan circuit that I showed you just
before it's very similar and that was
from 1973 so straight she invented
nearly everything I think it's fair to
say one of the the power of scan is has
been very well described by a professor
from CMU called die Blaylock he's
written a completely fantastic paper
called prefect sums and their
applications and he explains basically
that nearly whatever you want to do in
the algorithms line you can do it scan
and he has long lists and programming
examples and it's fantastic read that
paper too and if you go looking for
cheap if you read GPU papers these days
you will find that they can take
pictures that look just like the scan
circuits that I've been showing you and
scan is very important in the GPU GPU
world too and you might regard scan as a
kind of encapsulation of a very standard
form
of computation and and a colleague who
was at Glasgow at one point Edinburgh
first and then TAS go when some of us
were there was called Mariko and he
introduced the idea of skeletons as
capturing generic patterns of
parallelism and he wrote in in his PhD
thesis he first of all wrote his thesis
and then wrote a book later about it it
turned it into a book examples of such
patterns are stencils wavefronts
divide-and-conquer tasks farm and so on
and the idea is that we give the user
these cut higher-level ways to describe
programs and it's up to the runtime
system to provide good implementations
for the different platforms of the
various forms of computation and there's
a nice survey paper about that so it's
it's taken it's it's it happens both in
the not in the functional world and in
the non-functional world so this paper
is about both of those and these
skeletons that murray and others studied
are closely related to higher-order
functions in fact if you look at the
process of that book Murray says thank
you John Hughes for explaining to me
that skeletons are tells you really to
higher order functions so where are we
now have we solved everything you know
am i telling you that we you know you
just need to go away and program in
terms of higher order functions and
we're all done and I fear that we're
it's not the case so let's think about
the systems we need to program today
parallelism is here to stay and just
getting more and more and heterogeneity
is here to stay you know we're going to
have all kinds of crazy accelerators
FPGAs GPUs and so on we might have good
ways to program individual types of
accelerators but now we're faced with
the prospect of programming this mess so
I think it feels like we're back in 1959
in order to program this mess we need to
think about a lot of low-level details
to get it right so I think so Backus was
interviewed in 2006 a little bit before
he died in 2007
and he was asked you know what what
should programming language designers do
now and he said this the question of it
still seems the programming is a pretty
low-level enterprise and that somebody
ought to be thinking about how to make
it higher
really higher level than it is and I
think that's what we need to think about
still so if we're going to program that
mess we need all kinds of ways of
dealing with the fact that we need to
deal with low-level details without
overwhelming the programmer we need to
talk about expressing and controlling
locality of data security it's become an
issue we hear heard about smart
contracts and blockchain yesterday we
need to ensure correctness for them for
these kinds of reasons and we need to do
things like control power consumption so
the days when we could just view a
single sequential computer or something
abstracted we could easily program are
gone and there are some advantages to
this because it means that we
researchers can get money to think about
these problems so we and this the
information secure we functional
programmers in the information security
group led in the project led by
Alejandro Russo we just got 31 million
Swedish kronor from the Swedish
strategic research agency to think about
exactly these problems so we're gonna be
hiring doctoral students so if you fancy
coming to shomers and working please do
apply but once we've advertised um so
I'm gonna end with this very famous
quote from strachey so we were told this
quote all the time when I was a masters
and doctoral student in Oxford and it
says it has long been my personal view
that the separation of practical and
theoretical work is artificial and
injurious much of the practical work
done in computing both in software and
in hardware design is unsound and clumsy
because the people who do it have no not
any clear understanding of the
fundamental design principles of their
work most of the abstract mathematical
and theoretical work is sterile because
it has no point of contact with real
computing and I I tended to think at
that time our hesitation it's very nice
for him to say these things but you know
I don't really but then I have now since
discovered all the amazing things he has
done like designing computers and so on
so now I understand that he was speaking
from real knowledge when he admonished
us to bring together theory and practice
and then I hope that you'll go away and
read some of the papers I pointed to you
and then we can continue in lambda days
to bring together these two parts of
computer science thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>