<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Going Barking Mad with DalmatinerDB  by Steven Acreman and Heinz Gies | Coder Coacher - Coaching Coders</title><meta content="Going Barking Mad with DalmatinerDB  by Steven Acreman and Heinz Gies - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Going Barking Mad with DalmatinerDB  by Steven Acreman and Heinz Gies</b></h2><h5 class="post__date">2016-09-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/1lC8m3omo4I" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">well anyway thanks for coming it's going
to end up with the database this is a
double act so I'm going to take 20
minutes of your valuable time and then
Heinz is going to come in and get
through the final section and hopefully
I can make it all the way through in the
onesy but before you're asking why am I
in a onesie cuz some of you that are
observant it's probably noticed this
this is the official outfit of dommatina
dB it's got spots all over it down
mattina is a word you probably can't
spell or pronounce but it's the German
word for Dalmatian and Heinz picked that
word for the database because it's a dog
with a lot of spots lots of little
points in the database that was the
reasoning behind it anyway and I work
for a company called data loop and we
use it I'm not annoying developer
believe it or not my background is a
sysadmin I pretty much that that was me
about 10-15 years ago I used to just sit
at a desk people would ring me up and
ask me how to fix things and I would say
turn it off and on again and I got quite
good at turning things off and on again
to the point where I
I wrote lots of automation to turn stuff
off and on so they call it DevOps
nowadays but back then it was just
Python and fashioned and other weird
stuff so I'm going to tell you the story
of how I became enlightened and started
to use Erlang and ultimately Heinz's
Heinz's software so about well in 2013
at the end of 2013 a series of life
choices and had led me to to launch our
Fresco's cloud product this was about
the so it's getting very hot this is
about the another six or seventh time
I'd launched a SAS product for some
other company and at that particular
time like any DevOps person I was doing
one or two things repeatedly I was
either helping people we deploy software
or helping the monitor and measure it
and I found myself at the end of a
three-year gig at al fresco helping them
launch their SAS product which was an
online document management system you
could upload fire was very sexy but I
kind of got a bit bored of that stuff
do any of you guys recognize any of
those and those logos do you know
anything about Nagios or stats d or
graphite or sort of stuff so basically
what I was doing was what I've done six
or seven times before putting together
the kit car of rubbish like people was
half completed hobby project set up off
from github I'd take them like they
would gospel
fit them together in weird and wonderful
ways and before you knew it you had like
a franken monitor or the KITT car which
is what I used to call it and I've done
that loads of times and I thought you
know I need to make a choice now what am
i doing next the choice basically was
either either go consulting where I
could have been wonderfully happy I you
know I could have billed for 20,000
pounds a month easily I you know lived a
wonderful life or go and live an
impoverished life as a person doing a
start-up and end up in a onesie which is
where I am in life at the moment so
obviously never been very good with
decisions decided decided to actually go
ahead and do the startup so that was
three years ago now started a company
called data loop the idea would be it
was going to be all things to all men
you know unbounded scope we were going
to take all of that kit car of
monitoring tools and actually do it
properly in a single app we'd take the
Nagios that would that would do the
polling you know that big binary that
would run check scripts on remote hosts
that would let you know if things were
up and down it would be the things that
developers could use to shovel metrics
in and you could see pretty graphs and
it would be the one place that you went
to so we started off with a I think it's
very fairly narrow spec which was let's
tell people when things are up and down
which is what you can do in Maggio's
let's get some pretty dashboards let's
do some alerts we had some ideas around
adoption I've always been a fairly lazy
person and through my entire career I
mean I know the only reason I learned to
program was to make sure that I never
had to do a donee work so I would just
program things the computers would do it
for me and I could sit back and do other
things so that's what DevOps is
basically we thought that we could build
a system where we could entice
developers to do most of our work for us
they call it self-service nowadays
so the idea would be that we would have
a platform we could give it to anybody
in the entire company and they could
write their own custom monitoring
scripts they could instrument their own
applications and they could get it all
into a single place and we put
enterprise-e stuff and secret sauce in
there just to make it all work so with
that in mind we went off and started
building it we got better at
architecture diagrams later on this is
this is a very early one but you'll see
there are a few Erlang bits in there
there's a bit of a hint that we were
going to go in that direction
we started off for the first year pretty
much working on our agent so the idea
was to make it very very simple we built
an agent in Python we've got it rolled
out by config management all these
different boxes it would spin up start a
WebSocket connection back to us as a SAS
service and we'd have bi-directional
communication and we could push down
scripts to the boxes like you did with
now gos except it would be simpler and
also we'd have a port where you could
get data being shoveled in so that's
what I'm scared of touching that
actually that's what that bit is there
that's our data loop agents connecting
into an exchange that's all written in
nodejs
in fact most of it was we call it a
prototype but at the time we really
meant it
and this is where the third-party agents
would come in and push all of the
metrics in all of this stuff in the
middle is all no js' all talking to
rabbitmq so lots of Erlang in that
somebody decided to put MongoDB in there
we were naive at that stage it's Colin
calling there he did it and at the time
we were using react so we never wanted
to build a time series database so I'm
not entirely sure why I'm here but in if
we decided we were just going to have
the minimum functions we just have key
values and we would forego any of the
crazy analytic stuff just to get the
product working so we'd get all the
agents connecting in sending in time
series data and we could create
dashboards and alert rules and actually
get the product release and then we do
the hard bit later because who wants to
do the hard bit first so that was where
that was this took about a year we
managed to convince a cup
of companies to buy this this didn't
exist but they bought it anyway and and
then we started to build it behind the
scenes and it was all working pretty
well when we just had a couple of
companies on it then what happened was
we launched which I was against to be
honest but but we did it anyway and
things went well I mean for a company we
started to we make her money by selling
agents so agents would get installed
across machines we'd get a few dollars
per agent for every agent get that gets
installed so we really like it when
people take our software and roll it out
it's like cha-ching every month it's
pretty cool we did that this is the
whole of 2015 what you'll see there is
started off nobody really cared that's
what always happens when you launch a
product you put it out there and people
like oh I don't really care about this
and then we managed to convince a few
people to try it and then what happened
with that blue line there that massive
one was we made a couple of sales and
immediately a few thousand machines just
got dumped on the system overnight which
we didn't really anticipate what really
happened was it all caught fire
like the whole thing exploded we managed
you know it would be nice just to get a
few of these things on there luckily we
managed to hide it
like any good SAS service as soon as it
caught fire all of us behind the scenes
were bailing out putting water on it
doing various various crazy things so
for four months we had a very very hard
time we we tried not to put any more new
users on the system although we were
actively selling it we were also
actively deferring people to try to move
them off and that that's particularly
where the fire is so if you guys can see
there where the flames are that was that
was the problem areas we had issues
basically with with our nodejs workers
mostly they seemed to be a bit unbounded
you know you'd run this big event loop
thing they would pretty much pull
everything they could offer for the
queue and go sideways forever like a
like a big IOU system you know I promise
to do this at some point and then it fit
up
to like a gig of memory and then crash
and then which is okay because it start
again but we got a bit fed up with we've
tried to tune all of these nodejs
workers and fighting all that stuff so
we did what every company does when the
problem gets hard we put readers in the
middle and and that's that saved us that
was enough to buy us some time to fix it
properly so yeah the metric workers with
pull metrics off of the queue they'd put
them into readers which is massively
fast even even we couldn't outpace that
at the time and then another set of
workers would take it off of readers put
it into our database but it would batch
up the metrics so that's the kind of
times we go on about this we kind of
cheated it's hard to do like an
oscilloscope of maximums and minimums in
a database when you're writing if you
can batch them all up and then sort of
play them a fairly low rate that's over
a period of time that's much easier so
ya know GS workers weren't scaling we
had big memory management issues we were
we had big physical 8 8 core boxes so
we're running eight no js' processes on
those boxes one per core what you'd find
would happen was we had to put some cash
inside the nodejs stuff but the memory
management wasn't brilliant you only had
about a gigabyte and memory to play with
if a nodejs processor ever gets above
1.5 gigs that's the end of it you have
to shoot it in the head and get a new
one we yeah we had big caches if it got
up to over a gig it would do we'd
garbage collection things over and over
and then we'd be doing crazy things and
with flame charts and trying to work out
what what it was causing the the garbage
collection it wasn't it wasn't a great
situation to be in I hope you can't read
some of those words they're a bit rude
but we went through a we went for a
comparison of different languages
because we realized probably needed a
bit of a fundamental change you know
you're sticking Reedus in was one thing
but the problem was in the workers we
probably need something more applicable
we looked at Java that's the one crossed
out I think people generally agree Java
was a bit depressing you know they would
prefer not to ever have to touch Java
again because they've done it at the
last job's C they looked at Pauline told
me if we don't we've C we'd need at
least five times as many developers
to write all the code and we looked at
we actually tested go with Hecker which
looked quite good at the time but they
seem to discontinued it now and in the
end the Erlang stuff won out for us
because you could do you know you could
actually see what it was doing live in
production and it seemed like the sort
of thing that it was more scientific you
know we would we would look at nodejs
and you'd end up with some problems used
to go it's just like that we wanted to
end up in a situation where you could
actually look inside and and say it's
doing that because of this and not end
up in Wonderland
so yeah but about six weeks later from
the initial let's take this particular
metric service to having not touched an
Erlang book having read it I think tom
has picked it up in a few weeks rewrote
that worker so it did the same thing in
Erlang no more crashes much more fault
tolerant reduced the number of boxes in
half so that was quite good and then we
came to sort of the final bit of the
puzzle we obviously we kind of committed
now to rewriting all of our nodejs
back-end stuff in Erlang to make it more
fault tolerant and make it faster and
easier to to troubleshoot but we still
and we we still wanted to remove Reedus
we didn't really want to have that
single point of failure or to have to
build a load of sharding on top of
readers so that we could have many of
them
you still get like a you know a bit of a
gap if you lose the readers box so what
we did was we decided we quite liked
react we didn't necessarily want to
build around database but we started
looking around at different options in
flux DB at the time was pretty immature
it didn't really do clustering when it
did do clustering it only clustered to a
size of three which wouldn't have been
any good for us and I think we didn't
want to have just three and then there
were other ones based on Cassandra open
TS DB if you've ever run Hadoop I never
want to run Hadoop really so we would
look we looked around the guys had this
idea of there was a bit inside react
that was quite good
like the react core bit that did the
ring and the distribution and we quite
fancied the idea of building a you know
a very thin layer on top of that and
then we found online through an internet
search that somebody hit was crazy
enough to have already done it and all
I'd hidden it behind a name that nobody
can say or even search which was lucky
for ask us you know it wasn't it hadn't
been acquired by VC and cost lots of
money so we found this repo we started
to commit to it and yeah this is
basically the story of Douma t-midi be
getting rid of readers having a decent
metric store that's very fast so that we
can just write metrics off the queue
directly into it something that we can
add features to there isn't a massive
product that you know we can actually
iterate on for our particular use case
and that was one of the other things
that deferring of those advanced fancy
features was catching up with us there's
only so long you can delay the
inevitable with developers wanting
features and operations are happy like
you give them a web interface that's
slightly better than that yes they love
it but as soon as you start giving a
developer an interface and it's not
quite amazing and they won't use your
product say we want all those cool
features so we got erlang solutions in
that was kind of one of the projects
that we kicked off we had a team of
three people on the back end that were
quite keen on using Erlang we wanted to
fast ramp them up so we got erlang
solutions to sit on site with them we
kind of trained them
we started working with them on the on
the database and them and the workers
and other things Hines came in and
started to do some consultancy as well
we got rid of Reedus we reduced the
number of metrics workers again now down
to pibbly little amount it's gone up
again unfortunate we got more customers
but it seems to be a never-ending battle
but yeah new new matrix work had
directly into the dog this time no
Reedus in the middle and then we started
adding more analytics features so when
we first met Heinz it was pretty much a
graphite replacement which meant it had
dotted metric paths
you just had a hierarchy that youing
encoded it works very well if you're
clever and know every answer up front if
if you've already got it solved and
sauce down your head it's a brilliant
solution you definitely don't need
metrics dimensions at that point most
people will start off with a thing to
measure and then over time will want to
decorate it with tags like you know this
is a production metric this is in u.s.
East one you want to do the way across
that's the cool bit so we work with
Heinz on adding Postgres as an indexer
to it so all
of the metrics would get written into
the database but we'd also the first
time we saw a particular dimension we'd
write it into Postgres as well so that
you could do these fancy analytics
things you could take you know from my
production from my production cluster
where it's an application server in
production select one of my time series
like my New Relic and throughput and
then you can do really complicated you
know like graphing calculator types
things on them combined series together
sum them up and derivate them transform
them and do loads of weird things that I
never needed to do as an operations
person but apparently are absolutely
required as a developer which is more
where we're heading and you can do other
things like time shift them you know
what's my the number of division of
visitors on the site divided by the
number of items purchased and then shift
it back 24 hours and overlay it so that
you can see yesterday's purchases versus
today you can do all of that we cheated
again on the front end so the back end
kind of skipped ahead and got a lot of
features data loop itself we only had a
single developer at the time on the
front end
we've got more now but but we kind of
cheated by adding the graph Anna
interface onto data loop so that you
could do all of these functions and we
could test it all now that's kind of RIT
and being written and pushed in but yeah
this is all kind of and we the most
important thing is we contributed all of
this back so we didn't just take it like
a lot of software companies do and kind
of use it internally everything that
we've done here we pushed back into
Heinz's repo it's free we package it up
you can wear the very short installation
guide install a few dab unit Debian
packages get it running on your server
and you get what we're running behind
the scenes in data loop it's probably
overkill unless you're running a SAS
business but it is quite cool and you
get all of these fancy features in the
app as well where I normally get heckled
because databases are one of those weird
topics where people like to hate on your
opinion you know you can come I can come
up here and say how good it is and
somebody go have you tried Kairos TV or
have you tried in flux or why didn't you
try this this is brilliant I got fed up
with that so we tested every single one
and they're all rubbish
so so there all we did them we did a
massive blogpost on it we've got the the
world's biggest Excel spreadsheet it's
got about 40 different factors that we
took into account I mean they're not all
rubbish
some of them do things quite well if
you've got a different use case like you
don't necessarily want to run a
distributed system there are single go
binaries that you can run in certain
cases if you've already got the soup
stacks it's probably the onesie isn't it
the curse of the onesie but yeah this
will be in the presentation you can look
at it you can agree or disagree but it's
it's factual we've tested all of it we
think that for our use case now Mateen
is pretty good it's very fast so we get
about 3 million metrics per second on a
single node box and Hines will go into
I'm not allowed to ruin that
but yeah have a look at that we got
better at data architecture diagrams
over the years but this is sort of it
changes everyday because we keep adding
new services in production but but we're
still using RabbitMQ three years later
that's still how we we push all of our
data around it's like an event bus it's
all we push metrics as well we've still
got the ways of getting metrics in
although now we support dimensions so we
support Nagios and graphite but also
Prometheus and in flux DB metrics can be
pushed in as well we're we're heavily
reliant upon down mattina with the
Postgres indexer so you can do all those
crazy analytics features and a lot of
our services now are in Erlang and are
if they're not they're being ported over
we're still for any new stuff we still
tend to prototype it in node.js very
quickly get it working and then
immediately realize that rewrite it once
it's kind of in and working that's it
seems to work out well that way see it
pretty happy we're hiring as well if you
want to come and if you want to be the
one wearing the onesie you can come and
join join data loop probably not much of
an advert is it yeah things are pretty
good we're a big Erlang obviously
advocate so we'd like to to get some
more Erlang people in there and now over
to science
hello ah wonderful
I am I wanted to talk a bit about
Diamantina DB and I will not go into the
technical details because that would
take about two or three hours and for
those interested I'll be here the next
two days just come up and we can talk so
I want to tell you this story how
dommatina came to be you know this
interesting story the whole thing
started about two years ago on the day
and actually two weeks earlier what I
usually do when I don't write databases
is I have developed a cloud
orchestration system which lets you set
up your own private or public cloud
manage a ton of hypervisor and virtual
machines one of the requirements a
customer had was we want to collect
metrics for all the hypervisors all the
VMS store them in a second precision
forever and they had over 100
hypervisors in three datacenters and I
don't know how many virtual machines at
that point so they wanted to measure
everything can i anyone used illumos
here no okay
there's this wonderful tool called case
Tet which gives you kernel statistics on
literally everything and and it ended up
being about a thousand metrics every
second per virtual machine plus another
let's call it 2004 the hypervisor itself
which quickly sums up if you want to
push this every minute so I went out and
tried to find different solutions
starting out with a pretty small subset
of the metrics
volved for about 2,500 in a test system
and I tried in flux TB which blew up at
about 3000
I tried graphite which blew up at about
7,000 I tried Kairos TB which handled at
25,000
with about 60 gigabytes of memory and 2
to 3 seconds response times to queries
which wasn't great and then two weeks
later I happen to be in Stockholm at the
early user conference and one of the
great things that usually happens here
is you meet really smart people and I
had this crazy idea you could just store
this in a file and I talked to someone
who I greatly respect who was here that
day and said he isn't today I don't
think he actually knows that it's his
fault that I started this and whoever
knows data
Enes he is like Erlang guy and a lot of
performance work and brilliant and
really nice person and I talked to him
about it and he said yes that could work
and then it was a point where I decided
I'll just give it a try of that smart
guy things that can work then it has to
work it turns out it does and so and I
am I thought about it talked about the
problems and I am of the opinion that
algorithms beat execution speed so I
went with the language everyone tells us
is slow I'm pretty sure everyone here
has heard Erling is too slow once or
twice in the lifetime and and I don't
think airline is too slow I think
algorithms are slow in the worst case so
at this point I found this yesterday I
had to put it in here it's a little
snippet of code where people apparently
thinking that execution speed matters
the most so it's a go and go as fast so
we don't have to think so what happens
here is they calculate some statistics
the sum of a set of metrics the mean of
a set of metrics and the count of a set
of metrics and
I hope I really understand what meanness
but the mean is because I'm really
scared that this actually makes sense
because it looks so stupid to me that
it's scary so instead of just dividing
the sum by the count at the end of this
they do some tricky mass was multiplied
applying the former mean with the former
count at the new one and then divided by
the count plus one and then increment
the count this is just people not
thinking and that happens if you are in
a language which is supposed to be fast
you don't think about the algorithm you
were like oh this is fast that is how it
came to be and it works really well so I
went out to reinvent the wheel because
there are a million databases and I
wanted to do it without reinventing the
wheel because I'm absolutely a lazy
person I think that's a personality
trait we share it's one of the few I
hope and so I figured what all can I
avoid doing so I think I'm running on
illumise which is a Solaris theory of it
so I can use the ZFS I don't have to
really care about compressing the data
the data file system does this for me it
doesn't better than I could ever do
because those are smart people's I am
NOT and same goes for checksumming the
data I don't have to care about
integrity of my file because a file
system guarantees me my file is either I
get an Arabic or it's correct I don't
have to care about snapshotting and this
stuff because my file system gives it to
me it's all work I could avoid I was
really happy about not doing that work
distribution you can't expect to handle
the load I was seeing there without
going to distribute your system out a
single box can handle a lot of metrics
and he spoiled it but you can't go
we just scale horizontally you have
vertically you have to scale
horizontally at a point and this
brilliant people at better which again
are pretty much a lot smarter than me
solve this problem they released three a
call which is amazing you get the
distribution you get the cluster
management it gets scaling it does it
works wonderful you get the whole
and CLI which lets you manage access
their ad stuff and they even documented
it so I can tell him when he annoys me
just look at the best of things it's the
same wonderful another thing I didn't
have to do then this Annoying bastard
came up with dimensions which is utterly
stupid because as you said smart people
can just make up the pet and be good
with it and no don't tell him I said so
it's actually pretty cool I didn't want
to write an index I don't want to do
queries which is like we're a esque
equal B and/or blah blah blah but
there's Postgres and it's amazingly fast
for this stuff so why not outsource a
problem to someone who's good at it so
dimensions just go in Postgres and
normal SQL occurs can be executed to
resolve dimensions blazingly fast and I
guarantee you a lot faster than I could
ever done works wonderfully so there's
one thing which wasn't solved for me and
that is the data layout on disk but
again I borrowed from someone who had
done it before I bought from graphite a
graphite they have this simple whisper I
think it's called flat files works
wonderful if you think a bit about it if
the data is in an area on the disk you
had sequential reads and writes for
everything and they are just faster than
looking up transversing a tree or
something like that
and the wonderful thing is ZFS has a
caching
it's called Ark which is an amazing
cache that does effectively what you
would do is memory mapped files just a
lot better because it knows about the
filesystem and the files so it will read
the stuff ahead of time if you need it
it will keep them in the memory it will
keep them if you want in a SSDs second
layer it's awesome look at the looking
at the compression it turned out that if
you start a 64-bit value for each point
in time it compresses down to one byte
not never the average but in most cases
yes an outlying West was hired there are
some outliers where slower but Road one
bite is really really good and since I'm
lazy and I don't want to do complex
stuff it's as simple as it possibly gets
reading the sixtieth 64-byte value in a
file to get the sixtieth value in your
data is really simple there's not much I
can do wrong I'm really good at I can do
that so this is just a lot of stuff I
hadn't to do pretty much it's the
database I didn't do and and I want to
actually encourage everyone when you
build something to try to be as lazy as
me and because 90% of the time someone
has done it before perhaps not to 100%
of what you want to do but perhaps 95
and perhaps those 95 percent are so good
because they have been around for a few
years that the 5% extra you would get
when you do it yourself doesn't make a
difference
so if anything you take wisdom with you
today other than seeing this dude in a
dommatina costume then let it be try to
look if someone did it before and use
their stuff instead rien of reinventing
the wheel what ten minutes oh that's
wonderful and the second thing that
obviously had to be done from scratch
is the query engine it's a sql-like
syntax a bit adjusted for time series as
time is always the querying most main
querying factor it is a streaming query
engine which gives you the advantage
that if you compute over a large
quantities of time you can start getting
results very quickly and keep the system
busy doing computations while you wait
on reads for example and if you read
multiple metrics at one time you can do
the calculation on the first while the
second gets fetched so you're very
effectively end up using your CPU time
using your memory
you would think that sending messages
during the query processing is slow but
it's amazingly fast an airline it is
really really good so a service
infrastructure my ass no that didn't
work let me try this again yeah here we
go I don't know why the red world
surveillance infrastructure I think my
assumption is we are talking about in
front decisions regarding state and its
location it was better so the modality
is very strictly divided into stateless
and stateful components and when I say
stateless I don't see configuration in a
state in this case because it's Eppley
its configuration state and application
state so bear with me and accept that
status is okay if there's a confirmation
file the front end is entirely stateless
there is no data stored in the front end
it just goes over the network grabs the
raw data from the database and does a
computation the proxy which you haven't
heard about until now and it's also
stateless the proxy takes in a variety
of different protocols like open TST be
in flux graphite metrics - oh and some
other one I forgot and and translates
them into something that mattina DB
understand which is awesome since you
can just take your existing system and
point it there and the metrics get
stored in a much faster database so
those are status they don't need to keep
state which makes them simple you spin
them up if you need more you shut them
down if you don't need them and then you
have two stateful components they have
Postgres the indexer and you have
dommatina DB the database and both only
serve really the purpose of keeping the
state they don't do computations they
don't do fancy stuff they are just a
state and that thing makes them really
easy because they don't do much either
they just do their one thing indexing
for Postgres storing metrics reading and
writing them for dommatina so come back
then it turns out the metal bar whatever
this word is I choose worked out pretty
well might be an accident there's a very
small API between this components and
this API is very very stable it has
changed once in the last two years only
to add compression and please see the
difference between an external API was
au which the user has and an internal
API which is between the services and
the interesting part here is that when
we did the stuff with the dimensions we
didn't need to change the database at
all the database remained untouched and
running while the dimensions were added
they only were added in the proxy to
write them and in the query engine to
query them the query engine could keep
going to the old back-end without any
change and that was really really really
nice seeing that a lot of changes we did
we have totally changed of the query
engine didn't matter because the
protocol stated same you had a
separation between stateless and
stateful wonderful there was barely any
down times ever I think because of
changes since they only where in the
stateless components the logic is there
the stateful components which serve for
ingress and egress remains the same no
downtime so this is another light nice
trick I learned during doing this I
would totally do it again for my next
application and if anyone built
something think about doing the same
I'll think about not doing the same
that's ok that's up to you so in the end
this is how you end up this the
architecture with what you end up you
have the front end which handles the
request from either visualization the
dommatina UI graph on your internal
stuff or any other system that queries
it and the query engine reads from the
mattina to be the metrics and from
Postgres the indexes works well if you
produce data you can use tachyon which
is a system I built for the cloud or you
can use the dommatina proxy
which is a translator and they were
stored in this two databases it works so
now I want to show off and will actually
raffle off one of those wonderful
dommatina suits because it didn't fit me
and to the person with the best guess
I'm talking about performance so this I
can actually don't worry I can't ruffle
on this because he gave it away and we
did performance benchmarks on his system
with this specs 16 cores 1 out of 10
gigabytes of memory and 10 terabytes of
disk and it turns out that you can get a
2.5 to 3.5 million metrics written every
second on a single box the next best
best system which is Prometheus
advertises very proudly eight hundred
thousand metrics assists a second that's
in order of magnitude difference and
this is really nice especially
considering it's in written in a really
slow language because Erlang is slow and
here you see it as really slow only 3.5
million metrics a second damage I wish I
would have written it and go and thanks
to the rear core architecture its scales
are pretty much in a linear fashion that
is thanks to bash or not thanks to me
because adding a new note it gets
distributed it writes to the new node
and just goes up to nodes will do nearly
double it three nodes will do nearly
triple that so this is for write
performance and now we go to read
performance and and now I will take
guesses about the 99 percentile read
performance for a query currying let's
take the eight whole square because it's
interesting you carry eight hosts over
the last 12 hours and aggregated by one
minute per season so anyone wants to
take a guess how long that takes if
you're the only one you get a free
onesie no one wants okay
if I take the onesie of the table who is
willing to guess no one you are all
horrible people I will randomly pick
someone of you to give them a one see if
you don't guess it doesn't work you're
shameless
okay first single host we are at 13
milliseconds as a minimum 14
milliseconds as a mean 16 milliseconds
at a 95 percentile and 18 milliseconds
at a 99% I was 21 as a max if you look
at that not only as a time really small
but also it is really tightly packed the
difference between the minimum and the
maximum is just 10 milliseconds if you
look at the other systems the minimum of
the maximum are orders of magnitude
apart which is thanks to Ireland's
wonderful concurrency and the streaming
query engine you can do the computation
and steps and don't have this outliners
while one is still waiting the other one
can already work and the blue ones are
the winners and as you can see even
beating in flux DB which is written in
go and does not need to go over the
network to fetch the data we
outperforming them the same picture
paints if you look at eight hosts or at
one day all the hosts which were ten
over one day you have a very tightly
packed very fast query response okay
this is me showing off and now I wrote a
blog post this morning in the plane
because I did a lot of benchmarking it's
called the lies we tell if you want to
know about all the lies and benchmarking
I discovered and did myself because I'm
a horrible person
- and then you can go there and read it
and say something to it and know why all
this numbers really doesn't matter and
I'm just showing off
ok that's it try dommatina DB if you
need a time series database or project
file if you want
run a cloud or get hired by them they
are really nice people no no no there
are nice people they're good people and
any questions am I allowed to do that or
do you want to do that okay yes so the
question is do you store the dimensions
with the metric associated at a time or
at all times
and the answer is yes and the dommatina
dimension system is inspired by ideas in
metrics - oh so the idea of metrics to
us that you have to kind of dimensions
dimensions which are part of the metric
identity those dimensions do not change
they are hard-coded if that if one of
the identity matrix changes then it's a
different metric the other are the
metadata metrics they can change without
actually affecting the identity of the
metrics but they cannot change they
can't change but you will always read
the same metadata at all times
so like you could have a metadata metric
host up or down which would give you the
last written state of this metadata not
the state at that time if you want the
state at the time you encode it in our
metric
I haven't looked at the metrics - Oh an
open TS DB because when I tried it they
didn't exist yeah my step the question
is what's your current bottleneck for
the query engine
it's my stupidity followed by just raw
amount of data it's pretty much mmm it's
it depends on the size of the query it's
either CPU or network bounce</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>