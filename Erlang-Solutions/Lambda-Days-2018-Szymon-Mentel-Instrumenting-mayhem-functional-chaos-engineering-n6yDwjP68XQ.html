<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Lambda Days 2018 - Szymon Mentel - Instrumenting mayhem: functional chaos engineering | Coder Coacher - Coaching Coders</title><meta content="Lambda Days 2018 - Szymon Mentel - Instrumenting mayhem: functional chaos engineering - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Lambda Days 2018 - Szymon Mentel - Instrumenting mayhem: functional chaos engineering</b></h2><h5 class="post__date">2018-03-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/n6yDwjP68XQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone few words about me I've
been working with Erlang for nearly 5
years and in recent years with Alex co2
and been involved in developing
containers load testing tools recently
and that's how I got into chaos
engineering and today I would like to
tell you a little bit about
orchestrating mayhem and before I start
I would like to ask the question if
there is anyone who have heard of chaos
engineering please rise your head ok
quite a few people how many of you have
applied it ok not that many I can see
two maybe three hands all right
so I'm hoping is gonna be interesting to
him I would like to start off with some
what why and how in terms of what the
chaos engineering actually is white why
bother and some basic techniques of how
to apply it and actually from this talk
my goal is to simply introduce the idea
and encourage you to go and try some
basic principles and to see whether it's
something it's worth investing in when
you develop your systems if you look at
today's systems it may you may think
that they are quite complex especially
when it comes to micro service
architects architecture when to serve a
single request multiple services are
contacted and what's more it happens
that different services are being
developed simultaneously and they are
being deployed at the same time let's
say within a day and that's why we can
say that chaos as this is inherent in
big distributed in large distributed
systems and even if the individual
individual services function properly
they are tested by the teams that ship
them then then they are integrated at
some integration software
and then they are deployed the
interactions between them may lead to
some unpredictable outcomes when they
are in production and specially when
compounded with some real-world events
like server crashes net splits discs
failures and such and to give you an
example let's consider the graph above
where we have a client that hits micro
service a which in turn contacts
microservice b ec d e and at some point
C fails and we have a fall back in place
which is F but it turns out that for
some reason I wasn't able to handle the
load that C was it builds up a queue and
eventually fails and chaos Engineering
is about experimenting on a system to
provoke scenarios like this one so that
you can learn the system behavior in
turbulent conditions ideal in production
and this is the formal definition of
what chaos engineering is so it says
that it's is a discipline of
experimenting on a distributed system in
order to build confidence in the
system's capability to withstand
turbulent conditions in production so it
is empirical because it's about
experimenting it's about learning system
behavior because it says about building
confidence and finally is about
experiment again as it's about
experimenting at scale because we are
talking running experiments in
production and this example could be a
valid chaos engineering scenario in
which we deliberately disrupt C to learn
the weakness of the fallback F and now
why bother at all
what's the rationale for chaos
engineering first of all it's to build
trust as I said today we have great
development velocity a piece of
innovation and it may happen that one
person is no longer are no longer able
to capture the the whole system and
that's why there is no there may be no
trust in what we would we ship into the
production and by experimenting you want
to build that trust over time then it
comes per activity and it means that we
want to be proactive and discover
weaknesses before they manifest
system-wide you would like to capture
them in some sort of controlled
environment and finally it's it's
cheaper because we may prefer to put
some fraction of users at risk than to
event to experience some them to
experience the entire business taken
down that would hit all our customers
and before I get to some principles of
how to apply it just a note of on the
note about the steady state steady state
refers to some property of a system that
it's then tends to maintain over time
within a certain range of values or or
according to some pattern for example we
can say that that the human body has a
property that it keeps some Deepti body
temperature at some level and we say
that our body is in a steady state where
our body temperature is a dud level it
functions normally and the grab of the
graph above represents steady state of
some system where we have request
response time around 40 milliseconds and
that's something we have learned over
time by observing our system and we know
that when this property holds our system
functions normally and then we have
another graph of the of the same system
representing the same metric but this
time we have some spikes
because in the in the first place we
have learned what the study studies now
we can say that our system is no longer
functioning normally something wrong is
going on and having that set chaos
principles dot-org the same aside that
that that were you can find the
definition of what chaos engineering is
devised some four basic steps of how to
practice the discipline and the first
step is you need to learn the steady
state of your system so need to observe
it over time pick some key business
metric that is let's say related to the
money to the money that the business is
making something that is very important
and get to know what the normal state
means for your system or the business
once you have it you set hypothesis you
hypothesize that the steady state will
continue when you break something in the
system and here's where we have the
third step which is about introducing
some variables like real real world
events like server crashes due to disk
failures and so on and once we introduce
them we can learn something about the
system either we can learn that it's not
able to to cope with the failure that we
are introducing or it did it or it did
it well and the last step is to try to
disprove the hypothesis by comparing the
de steady-state with the de learned
behavior so we are comparing the same
metric but one was but but one was that
that we learned by observing its normal
state and the other one is referred to
the situation when we introduced some
fault and you also have something that
we call control group so this is the set
of users that hit our system where it's
a normal state or this this part of the
system that is not affected by any fault
and then we have experimental group
which represent the set of the users
that are affected by the failure that we
are
introducing and now I would like to tell
you how Netflix does it who practice the
discipline and let's assume that they
would like to answer a question how they
are how they are API handles failure of
writing service and the normal path of
such a request would be like lie and
hits the gateway then the Gateway cause
API and finally API calls the writing
service writing service is some service
responsible for assigning scores to
movies something like that and because
they observe for their system they know
where what what is their key business
metric they know the steady-state and
what they would do
using chaos automation platform that
this is the tool that we used to run
cares experiments they would spawn two
additional API clusters one for the
experimental group and the other one for
the control group and then using the
same component they would instrument the
gateway to direct fraction of the
request the normal path that the blue
ones and some fraction to doubt then
they would decorate some fraction of the
requests to go through the control
cluster and finally some fraction of the
requests to go through the experimental
clusters then would scale this these
decorations appropriately and finally
they would they would enable some fault
at the experimental cluster which would
be simply drop all the codes from the
experimental clusters from the
experimental cluster 2d rating service
so that all the customers that go
through the experimental cluster are
affected by this is injected failure
and that was the first part which
discussed some basic and high-level
concepts and now I would like to tell
you how we are there like solutions
apply these principles to one of our
systems that we develop and share with
you some learnings that we have gained
from the experiments that we run first
of all what we do we have sorry we have
a man whose I am
server which is a messaging service that
allows clients exchange messages
which is used for example to build
Internet communicators and it can run in
a distributed mode and then we have a
cluster of nodes and this is first
possible place where something can go
wrong I mean there is an interest this
is interesting for us because we may get
a we may have a nice plate and let's say
the node are no longer no longer yet no
longer able to synchronize we also can
talk to external authentication service
and this is another there is this is
another potential place to experience
some failure we also have a database
which is used to store which is used to
archive messages and again for some
reason we may not be able to write to
this database and finally some
installations would stream records of
users activities to some streaming
service like Kafka and again this is
another potential place where we expect
some fault to happen and this is a set
of injection points so places in our
system that potentially can introduce
some some problems if if something bad
happens and these are deserty this is
the base for us to set some hypothesis
like if something bad happens
let's say the connection between the
nodes in the cluster we assume that
something I mean and we would like to
try this hypothesis and we seem like
failures by injecting some code into the
running Erlang VM or by tampering with
light or by directly tampering with the
servers that run the services and to
carry out this experiment we use our
load testing infrastructure what we have
is load testing scenario that we fed our
load generator with which in turn put
some load on the server and we learn the
steady state and this is our control
group then we use the same load testing
scenario to to run another load test
which we which is the experimental one
but this time we inject some faults into
the running system so our approach that
will be different than than the one from
Netflix because we are spawning two
separate load tests that that I
configured the same way and and we
compare this steady state from the
control load test from with the metric
that we do we have got that we have
gathered from the experimental load test
and now I want to share with you three
hypotheses we have tested first one was
related to a database failure and we
said that failure to write to the
database won't disrupt the service the
basic functionality which is which is
that the users can exchange messages
using the B server and the dice as I
said the database is used to archive
messages and
and in the system we have worker
processes that keep connection to the
database and a supervisor process on top
of on top of them and to simulate that
fer and to simulate that failure what we
will do it would kill the entire pool of
these processes that interval and see
what happens
and this is the this is an example code
that we used to do that so what we what
we did we define fault fun anonymous
function that that represents this fault
and what it does it grabs all the worker
processes from the supervisor then steps
over them and kill them while wipe on
one at interval and then close itself
itself recursively and to enable this
fault we get this fault fun spawn it on
a note that you would like to effect and
when it's done we simply kill the
process that was simulating the failure
and what we learned from this experiment
this is this is the request response
time of of our service and this is where
the chaos started this is where it
stopped and the service was not was not
disrupted so we proved the hypothesis so
we learned that even if the database is
not available available we are still
able to to serve users but what we have
learned was that the the performance
turned out to be better I mean the the
key metric that we are looking at and it
turned out that when when the pool of
workers connecting to the database what
not was not available the the reads and
writes the database were simply dropped
and that's why we saw lower load on the
server that was the first experiment the
other one was the hypothesis that we
said was delay on the connection to the
authentication service I won't prevent
the users from logging in so we said
that we saw the hypothesis that if for
some reason we have a delay on the
network to the authentication service
the users eventually will login and
similarly as with the case with the
later base we have internally we have a
supervisor and a pool of workers that
call the authentication service and to
simulate this delay we added it at the
workers level and to achieve it
again we defined a function basically we
use the mocking technique so what we
what we what we did we put a mod instead
of the original module that was calling
the authentication service and added
artificial delay so what we did we
defined that delay of function dot take
user and a password and added some sleep
so that we have this delay in place and
then we would call original function
that eventually caused the
authentication service and then through
mocking we would replace the original
authenticate function with the with the
one that we made up that had the delay
in it and set an expectation on a mock
so that our function is called every
time a user tries to log in and finally
when we are done with the experiment we
would unlock the mock and here the
steady-state was that eventually during
our load test all the users log in but
time that this hypothesis was disrupted
because as you can see these two lines
certainly not affected note we have
three nodes in total in this experiment
did not affected nodes had users but the
one we run the default on had no users
it turned out that that small delay
caused the that the users were not able
to login and what we what we learned
from it was that there was no proper
retry mechanism at the load generated
level because we would expect that
eventually because there was load
balancing in place all the users get
into the system but it didn't happen
and the last hypothesis of the test we
said that if there is a network glitch
on the connection to Kafka
which is the streaming one of the
streaming service we use so that all the
records of users of activity are locked
if there is a glitch on this on this
connection eventually order messages
will be delivered so our server connects
directly to the Kafka server and to
simulate this filer
we used IP tables we simply install the
rule that would drop the packets the
incoming packets from our server and
erupt that rule into into our elixir
code that we used to represent defaults
so we would literally add a role to
input chain of an IP tables and that run
that command on the server that was
running the Kafka service when we were
done we would simply remove that chain
from the promoted rule from the IP
tables chain and our steady state here
was that the number of messages produced
by our server equals the number of
messages consumed from the Kafka server
and what happened was that we produced
one message and then because we didn't
get acknowledgment our application we
tried to to send that message three
times and then the network recovered and
on the on the side of the Kafka con
consumer we've got four messages and
this time the our hypothesis was
disrupted again because we produced one
message with write three times the same
message and we've got four messages and
we learned that it was the de Kafka
driver which had its internal retry
mechanism and we were like doubling the
retry so first we produce a message
there was no connection was broken
it was stored in the Kafka here and then
every retry from the application layer
was treated as a as a as a new message
and that's how we eventually we've got
four messages
and these were the these were the basic
techniques and now how to how to
actually get them executed if we have if
we have coated them like here with Alex
view and we have a few options so one is
you can get into your shell and simply
type it in but it's not handy if you
would like to automate the process
that's why we can make use of of our PC
which comes with the airline VM and we
can instrument our folds by by calling
the node you would like to effect and so
we could also wrap our falls into an
anonymous function that you would then
spawn in a process on the affected node
this fall SCADA could also be weaved
into the code and triggered by some
metadata carried carried along the
messages there is that are exchanged by
by the users but it is not the the
approach that was solution to our
problem
and this brings us to the last part of
this presentation which discusses how we
attempt to automate this whole process
because what do we in the first place
were we were applying this these faults
manually by logging into the shell and
typing in these faults or let's say in a
semi out semi-automated way and we took
an attempt to automate the process and
just to recap we carry out these
experiments by running two separate load
tests and then we compare our key key
metric and and judge whether we have
some we have some problem when there is
a Fault in the system or not and this is
what we treat as a chaos experiment and
then we try to do then we develop a tool
like that that would automate our chaos
we call it typhoon and this is the
knowledge this is an analogy or
application that comes with with the
front-end and with this typhoon web and
the backend typhoon that consists of
three components the first one is chaos
chaos manager that is like orchestrates
everything and then we have fault
injection which which is responsible for
for managing the faults and finally we
have infrastructure which orchestrates
the load tests and the infrastructure
component comes with two models the test
setup which defines how the load test
should look like
like config files what what docker
images to use how to scale it and the
infrastructure component we would talk
to our load load test automation tool to
spawn the test to get notifications on
its status and to get notification
whether it's whether it fails where it's
done and it also has tests apology that
defines some metadata of a test itself I
hold our the IP addresses and ports what
are the services named when the test
runs then we have then we have default
injection that comes with a fault
protocol
technically it's an elixir protocol and
that's our make main extension point
meaning that with we we put an effort to
make it easy to add a new fault the
system so that if we discover any any
other injection point that would be
useful to test we can easily add it and
the fault injection also talks to the
load testing infrastructure to basically
inject these faults either by m1 in some
code at the node to be affected or by
tampering with the system that runs some
service you would like to disrupt and
finally is the chaos manager that
defines the experiment model that has a
reference to the control load test with
experimental load test to be set up to
defaults and fold off folds offsets
which define when in time the faults are
to be injected when the particular load
tests load test run and you would start
an experiment by providing some
configuration throughout through a web
form and then we would build an
experiment and run and based on that
build the set up around the control load
test and that's how we learned the
steady state and then it would run
another load test which is the
experimental one inject folds as is it
is running and finally learn behavior
and compare and compare the two and now
some elixir beat and as I said we have a
fault protocol which provides single API
for all defaults and for now it just has
one function which is apply which takes
default struct so data type representing
particular fault and a reference on the
test so that we know at which test to
apply the fault to
and all the I need to implement the fold
we need to implement that in the
interface basically and this is an
example fault and here we use or we also
use embedded schema and and change that
to to make it easy to validate
parameters that come from the UI so that
it helps us to to build default struct
and as I said each fold needs to
implement default protocol and this is
basically all I have for you we talked
about case engineering as a concept and
then I showed you some basic techniques
how to apply it and finally our journey
to automation and I encourage you to go
try to try to explore it and see see how
it works try to apply some basic tech
techniques to your system and if it
works for you try to put it into your
continuous integration pipeline and
benefit from it and that's all I have
for you thank you
I have to I must admit something when
you ask the beginning has anybody done
ever ever done chaos in chaos chaos
engineering I didn't raise my hand it
was a mistake actually because he turns
out that they actually did it I just did
not the right name for it my question is
about typhoon is it something you did
for yourself or is available someone or
is it or internal tool now it's an
internal tool we are working on and then
I question about Netflix am i right that
the experiment on the life system under
your users yes they do so that so the
test fails when user starts start to
complain or something how do they
measure it so as I as I said so at least
that my knowledge is based on on some
blog post from the from the previous
year so what they would do let's say
they would they want to test some some
component they would spawn additional
two clusters of this component one for
the experimental group one for the
control group and then they would just
put a fraction of users to both of them
so that to minimize the D blast radius
and then these two new clusters report
the some some key metric that they
measure and then when the experiment is
done they compare the two and they know
what there's something went wrong when
went wrong or not that's how they do it
I hear how much time do you need to
spend on the design of the actual
production system to make all these
possible because we saw that you are
injecting some like like physiologic to
create Falls or to add new behaviors at
what moment do you these
decide when to actually change the
diaper production system to do so so the
question is if I understood correctly
how long I spend on on designing these
folds and how much effort put into it so
not so much the force themselves but
deep like for the production system to
allow injecting the Falls
so in airlock like like might be easier
but maybe in other technologies is not
that easy to change dynamically the the
behavior of the system you know in
production you'd have to make a new
release or sub like this but okay so in
the question is how how long it takes to
adjust some system so that it's able to
to run some faults in it yeah so in our
case what I was presenting that there
was no change required at all because we
were injecting anonymous functions in
the running system so no change no
recompilation nothing that was our
approach but another way to do it is to
weave some some triggers into your
system and that would require
recompilation rebuilding and another
release with some some code other that
would react on some metadata carried on
with with the request so in our case we
were running the same system the same
code when experimenting and not
experimenting you are using anonymous
functions okay thank you get into the
system come on I wonder if you can share
a little bit about how you stress test
specifically the message broker and the
container scheduler
we are not specifically stressing you
mean the The Container scheduler in
context of docker yes you mentioned dr.
young Kafka as the message broker so my
question is how do you apply this chaos
engineering strategies in the context to
stress test those parts of the of your
architecture the docker container
scheduler and the capcom message broker
okay so our aim is not to particularly
see how it to stress the d2l scheduler
we try to customize everything so that I
mean on the doctor level so that we are
able to put the load we want on the
system and once it works we have it's
like a part of our scenario that's the
part of the configuration how to how to
customize the docker containers and what
we actually do if there's something
special we do is we are pinning a CPU
cores some containers to to improve
performance and then once we have that
once we have a stable load tests that
that build that yield repeatable results
then we apply chaos engineering
principles to to these load tests so
this is first first we stabilize the
whole load Lotus the whole if I am
getting you correctly yeah you are
saying that in in the docker case you
are running one container per CPU so you
are bound to the CPU that you have on
that particular machine or that or I
guess the amounts of containers that you
run on the scheduler in your cluster is
bound to the amount of course that you
have on that particular cluster is I'm
getting that right or
wrong what I what I'm trying to get here
is trying to learn about your experience
with multiple doctor containers like
with multiple yeah docker container
bicycle that's it and in he is not in
the context of docker by itself but if
in the container technology that's to
basically okay so it's not like that
that we are pinning one core to
container we what we do we know how many
cores we have on our machine and then we
what we do now we simply assign the
number of course evenly to to the
containers we have on the machine so
that it's not so that they are not
overlapping because when we when we were
when we allowed all all the containers
take all the cores we were seeing some
performance degradation and then we like
customized it so that every container
has a has a range of course to use and
it helped a lot
as I understood the failure that you
prepared and that are controlled by the
tough typhoon right is a failure of the
particular element element of the system
like the internet connection to server
one or another it is did you check the
interactions between the failures like
like I mean that something is broken and
then something else is failing or is it
just when one by one test for now it's
one by one but the intention is to is to
compose these failures so that we can
test what happens if two of them happen
simultaneously that's from you know the
plane the place goes down when two
things fails the planes are designed to
I mean I have a plane it's usually
handles when handles well one failure
rights the problems becomes when there
is another one so your system could be
also exposed some problems when you have
two failures in the same yeah that's
that's perfect that's perfectly correct
that's what we are aiming at to to be
able to run multiple failures at the
same time not one by one that's just the
first step to discover some weaknesses
once we have them once everything seems
to be working fine when we apply the
feathers one by one then we start
experimenting applying these failures
like mixing them together and things
like that that's that's that's nice
robotics hi so are you aware of any
technologies on the infrastructure level
that would support fault injection I was
like thinking some service mesh or like
anything similar because this your
approach depends on their like platform
to inject faults and I thought it would
be nice to have it on infrastructure
level
so why presented one one thing that we
utilize iptables two blocks on traffic I
also know that there is no don't
remember the tool name exactly maybe
something like in madam my made me wrong
here that allows you to
to put some delay on the network
interfaces and I also know that in case
of Tucker there is a tool called block
ID I think that also allows you to
tamper with the network level and
Netflix also has some open-source tools
on the github that you can check and see
if they apply to your use case but there
are some there are some tools definitely
not allowed to do things like that I
have time for one more question
okay so unless I'm missing someone I
think I thinkwell well thank you Thank
You Sherman thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>