<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Building distributed applications: riak_core vs partisan - Mariano Guerra | London Riak Meetup | Coder Coacher - Coaching Coders</title><meta content="Building distributed applications: riak_core vs partisan - Mariano Guerra | London Riak Meetup - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Building distributed applications: riak_core vs partisan - Mariano Guerra | London Riak Meetup</b></h2><h5 class="post__date">2018-02-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/BKqR3EJAV78" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so welcome to everyone I'm so big
thanks to air like solutions who
sponsored this event for the venue the
food and the drink and some small
announcements as well coke being San
Francisco
it's on the air like an elixir factory
saffron that's coming up in March we get
some keynote speakers or announce to us
we go to Costas secona so I decided the
title before writing the talk and what's
a hard challenge company check it out so
some philosophical questions who are we
how do we share knowledge how to
communicate with someone and who is
responsible for this these are questions
that every node in a distributed system
usually ask himself and questions we
have to reply for answer when we are
designing a distributed system so let's
explore like these two systems recording
partition kind of in terms of these
questions so first comparison I may
introduce just in case you didn't know
them react or is a framework that allows
developers to build distributed
applications using a pattern no was the
dynamo style architecture defined by the
vanilla paper which I will mention later
and it's the core of react
it was just extracted and exposed to it
as a framework is used quite a lot in
many projects and it's a really powerful
and easy way to get started building
distributed systems getting all the
complexities out of the way as long as
your problem fits the pattern on the
right partisan it's a new comer newer
the brick or is it was built by a team
of people me mainly you may know
Christopher Michael John from last
because they needed something that would
scale more to more notes than what
distributed are long enough which is
between 1600 which usually it's okay for
almost every single developer in the
world but they are trying to do that as
what they call planetary scale systems
so they wanted an underlying platform
that would allow them to scale to
thousands of nodes and right now I think
the benchmarks they run between a
thousand and four thousand nodes already
and for that they need to change
the basic part of the clustering cluster
membership for reasons we will see later
right now partition is used mainly for
last a project which is built by a group
of researchers around but you can use it
for your stuff and the comparison I
tried to compare them because me as
somebody here pointed already it's hard
to compare but I found a way that almost
beats these questions I mentioned who
are we is what's called cluster
membership which is who are the members
of a cluster who are alive where are
their IP port and stuff like that
then how to communicate how to share
information but also a simpler one is
how do I send a message to this other
node which are two different things and
one sub beliefs and then on top is where
the similarities end
which is why reoccur is more of
framework and partition is like baseline
for distributed systems but we will see
how can we make it similar in general
terms we have the membership layer at
the bottom that provides point-to-point
communication also just cluster
membership members but since it knows
who are the members it can send messages
members then on top of that we build the
gossip layer which is broadcast
communication I want to share this
information with all the nodes in the
cluster how do I do that
then you have gossip and on top of that
you can build two different parts
metadata which is you need to store some
information about the cluster itself for
your application and for running for
your problem and for general
stuff like permissions users groups or
more ephemeral information that metrics
and use usage by user for example for a
limit and then the question of who is
responsible for this or in another way
where is this is provided like routine
and we will see how can we solve those
so how can we make them more similar
let's go back you can see here this is
the Gaussian layer of record for
historical reasons there are two gossip
implementations on record one the new
one a plum tree for real core metadata
which is what synchronicity we have to
point out I think and it's the new stuff
based on research and papers and stuff
like that and the gossip the wrinkled
gossip is the legacy which words that's
why I didn't touch it
which is used to go seek the ring which
is a structure we will see later so how
can we make them similar well first just
use one tricky because they are to
gossip implementations they do the same
so we could like make the initial two
layer similar as long as you choose in
partition and then we could we could
take the top layer from record and put
it on top of partition because it only
talks to country so we could make them
see we will see today how far can we get
so let's start more insulated the talk
will be a little bit of theory to
understand what I'm going to show you a
references to research
yourself and demo with called working
stuff so let's start with membership
where Korra uses distributed early which
comes with darling you really have it if
you run the definition a loosely
connected nodes in the sense that it
waits until first reference to another
name another note when you do spawn of a
model the function and arguments on
another node it will try to connect to
the node or if you do a net admin link
to that node then you establish a
connection with them know that needs the
directional and
you can disable this this transit
connection passing the VM we connect all
falls when never and all the disconnects
either because it goes down or because
you run this connect node it gets out of
the cluster it move and the nodes are
connected all 12 whenever one node
connects to another one and joins in the
network the in the cluster every other
node would have a an active connection
with other that's why this user and is
so transparent and easy to use and ECU
you don't even have to think I won't
even use it but when you have all to all
connections then at sixty or a hundred
nodes you have five thousand active
connections and all of those connections
are sending heartbeats between them so
as it grows exponentially then you start
having problems and how much it scales
above sixty depends on the workload
you're running but usually it's
mentioned between sixteen hundred where
you can scale if you have if you have
seen the EPMD process and you try to
kill it
it because it starts with any other node
and provides a mapping
the node name that you have and the host
and port and all these you can see all
the notes into your cluster as long as
they share the secret cookie run in the
notes function there's their special
kind of nut called hidden which are note
that when they join they don't connect
to every single note this if you want to
connect one to another in a cluster to
do some operation and some management or
together some matrix you don't want to
actually join the classroom you just
connect with the hidden cloud and you
can list those not with notes hidden or
nodes connected so let's say the
distributed arnica membership
implementation which is the one that we
have Cora uses and so since this project
is a research to build bigger
distributed systems they need to
implement the wrong in parties and the
membership layer is configurable because
they want to test different scenarios
the default is distributed earlier so if
you can use parties and we distributed
our line and you get what you got from
red core that's why that's how you can
make it really similar to the tool
building blocks to create core then we
have other three hipper view which is
the interesting one static is you
specify the cluster in the configuration
you tell the notes and I bits and parts
in the configuration and you will never
change and another one called
client-server which I have a research
but it sounds weird it's it sounds like
a snowflake like a two or more levels of
service I don't know it works but I like
this is the he probably is the one you
are going to use if you want to have
clusters about what distributed among
supports yeah this is the paper for a
purview it's really breathable there are
papers that are not is to read this one
is released to it I like it a lot and go
and check it and then the section for
explains yeah
if you want to skip everything go to the
section four and it's explained there I
will go over a little bit it has each
each node internal node has two views
the active view at the passive view
active view impossible you are sets of
nodes the active view is knows not with
which the current node has an active
connection and whenever it goes down it
must lead them to the passive view and
takes another one from the passive view
and moves it to the active view and
whenever this node receives a message it
broadcasts that message to all the nodes
in the active view and this algorithm
assumes recipe and to calculate how many
nodes you will have in your active view
and how many your passive view those are
the formulas login plus C and C for C
equal one and key K equals six which is
what the people in the paper use and a
network of 10,000 nodes each node on the
network will have five nodes in the
active view and 13 nodes with the
passive view and as I said active
connection on all the nodes in active
view every time you receive the message
you for every node in the active view if
the connection fails or gets
disconnected that not moved to the
passive view and another one is promoted
to the active view and periodically each
node connected to a neighbor and shares
his view of the of the nodes it knows
about of course you keep a limit of how
many nodes you have interacted on the
passive and this is calculated by
priority for example if an you know
joins it has more priority because it
doesn't know about anybody so you want
to be friendly
and yeah and it's just so the other
question is how do I communicate with
somebody with someone in records
insidious is distributed our line just
say or gem server stuff like that it
works because every node knows about
every other node so it just works we
don't have to change anything
of course the framework provides
instructions on top which we'll see but
if you need to have a gentle server
register with the name is unknown and
send me the message in good work in
partisan we said that the membership is
configurable and you have four on three
of the four distribute the darlin static
and clean server but I do not clean
server but every node can reach all the
other nodes but on he purview you may
have to know that only transitively see
each other so you cannot send a message
to know do you have to use what
partition provides to say because that
message make up between more than one so
you use the peer service forward message
they don't name the remote register name
and the messaging it will reach look now
we are on the third level which is who
is responsible for this and in this in
this case we have two different use
cases metadata which usually is spread
all the notes know about all the
metadata it's small data in that sense
it's not your main data store but like
users and groups and permissions and
configuration and routing I call it
routine which is a request lands in and
out and how it knows who which node or
which server is responsible for handle
that request for example if you get not
one receives I put on the key I know too
get on that key you expect to get to get
to the same note that the put otherwise
you so to answer this the who's
responsible for this
the Dynamo style architecture is the one
used by red core which is a paper by
Amazon which also it's really readable
and basically you have any notes for
example here for notes and you partition
a ring which is goes from 0 to 2 to the
160 in but and a number of partitions in
this place 32 and assigned each slice of
that range of numbers to one node and
you take care of that now not every
single neighbor is not the same node so
for example if you say you have a key
and you learned here and you get the
three partitions the three next
partitions then you will land on three
different nodes right so this is a way
to to partition and replicate data and
be sure that you're learning and of
course if you ask for five partitions
and you have four nodes then eventually
you will get the same but usually a nice
treat for you how much is your the usual
way W 3 2 pencils it some people using
like 7
so how do we this that I explained with
my hands and with the ring which is you
get a request and you decide which part
of the request is the one that will
route the request to the correct node or
nodes so you have the key what's called
the key which can be any attribute of
the request it can be the user it can be
the host name you can be the key can be
the value it can be everything and you
land in some part of the Ring and one
node is responsible for that and you can
ask for more than one node and I think
that's what's called preference list
that's this part so how do you do that
in code you just have to keep you ask
for the reference list in this case I'm
asking just for one and then you say
okay send the comment comment can be
that's what your application does if
it's a key value store it will be put
together delete if it's a
and then the the module that implements
the so that part was the routine which
is who is responsible for handling this
request but the other side is how do i
gossip information because it is the
term used in the papers which is one
node I add a one user not one and how do
I make sure that that user new user is
read to all the node so now every single
node can authenticate that useful
I could implement users and partitions
them so I could have one user on the Sun
nodes and then use the routine to decide
where to send the authentication column
that's a decision in react it's use some
metadata because usually you don't have
so many users and groups you
to chart but you can and for the gossip
this is easy because both use the same
algorithm called
Plumtree well record is easy to
implement what's called real core
metadata and parties and has country
that doesn't have any metadata but it's
really easy to to implement we will
today
cow so what's real core metadata it's a
namespace key value store it's
replicated to all nodes eventually and
synchronously it stores the information
in memory and on disk that means since
it every alternate is stored in memory
you shouldn't be storing like a key
value store with metadata because memory
and also it only works for the local
right to succeed to consider successful
then we'll a synchronously replicate so
it shouldn't be for your critical stuff
on your metadata this plan tree
algorithm is comes from a paper called
epidemic well test twist which is like a
tandem this paper depends on the purview
paper or assumes something they keep you
for so let's see how come 3d works
exactly when you are want to go sip some
data you have three pieces of state the
eager said the lazy said and the message
you you will not see the you set is
whenever you get a request to rock us
something you will push that data
automatically to all the notes in the
USA the lady said is eventually you will
say to those notes in the lady said look
I have a new value for this key but you
don't send the data you say I have date
a new date
for this if the other know this
interested it will ask you to send the
data so it's more lightweight these
happens after a timeout and they can be
good so you can send many I hats in one
request so how does it work
imagine you have a 5 node cluster 1 2 3
each has eager lazy and messages so the
node one has the node 2 and 3 in the
eager set and the two the one for let's
say we asked they don't want to
broadcast a message a with a payload so
what we will do is it will send the
broadcast to the eager set which is 2 &amp;amp;
3 and this node 2 &amp;amp; 3 of course won't
return the the world s 2 not 1 because
but will send it to the units that they
have which is 4 and 5 and now 4 &amp;amp; 5 well
5 will broke us to the eager set it has
which is 1 4 because 3 it received a
message from trilling so it won't say
this back and after that note 4 sends
the message and then something called
prune happens which is not one already
has the message a so it's the second
time it receives this so what will
happen is no one will say look remove
you me from your eager set because
there's our redundancy there's make a
cycle I sent something and it came back
so there's a cycle and this graph so
remove me from your able set move me to
the lady said and now no one has
notified in the lazy set because from
that after a while the lazy push and
what can happen imagine the note 3 dots
so not one does the broadcast not too
does the broadcast but of course no
trees dead so it doesn't
and since it's the only one that has not
fire with eager set it doesn't send a
message after a while the old one which
has no file in the way she said since
and I have with only the key it says I
had a have you seen this and note 5
since it hasn't seen that message it had
an old one to the USA so it's like
rebuilding the tree so something bad
happened it was working the World Cup
was working well which is optimized for
where the message
alright faster but since now it sees
that another note says look you're a
lady said and I've seen this message
have you seen it and it didn't see it
it's not five things okay
I should have received these methods
from node 3 and I haven't so maybe twist
it so now I will move no one to eat the
intercept because I receive the message
first one not one and it said yeah I
don't have it so please give it to me
and then it sends the payload and after
a while not for also sends the message
and no fibers it to the lacy said
because it already saw the message 1 so
it's not the fastest one so what country
does it's its energy it spreads the
broadcast to all the notes and the notes
started into the ger sent the the notes
that were they received images such as
the fastest so it optimize for the first
flood of messages and then it heals the
three whenever some those die if they if
they see a lacy message a delayed
message that they didn't see they think
ok I should have received this already
from other notes so maybe they are there
it's quite quite
read the paper I'm not really good at
playing it but what happens if the note
3 gets back it didn't so did those
broadcasts depending on the use case and
what you're broadcasting you may be able
to wait a little bit and you will get an
update on that value which is my case
for example if you're generating user
usage rates every 15 30 seconds and you
lose a message you waste 15 seconds and
you get the new value so you don't need
to reconstruct the messages to do so but
in case of of if you're storing users
and groups and permissions you want to
if you go down and app again you want to
see whatever happened when you were now
so for this there is something called
activity entropy which uses a data
structure called marquetry and this in
which is because you can read it but in
C you serve it the hash you see whenever
you commit something
it is the hash of all the hashes of all
the folders and the files and for each
folder the hash of all the folders and
the files so you start from the bottom
of the tree and start hashing and then
hash the hashes until you get one half
and then you can compare just that hash
and if it's equal it means that
everything below that part of the tree
is the same and if not then you have to
start going over the tree to see which
hashes are the same it is everywhere
could get some VC funding yeah
so how hush two words in yeah we call
metadata is a structure like this this
is the structure which is used to
reformat the data but it applies to
almost everything you have the segments
which are story kneadable DB which is
really hard to make compiled in Windows
and this is what hey we cannot have
recurrence because we need for this the
second hashes are also stored on the DVD
Andy what's called the upper hashes are
contained in memory so the hash tree has
three operations insert update and
compared insert this whenever imagining
these are keys you update a key and it
changes the hash it has you insert the
key you insert a new key it gets a new
hash you update one update another one
and now you have to recalculate the
hashes of the of the intermediate nodes
for the hash that
changed so it's like bubbles up until
the route changes the hash of the route
changes so now you can with one single
exchange of one hash and when compare
you can know if you have the same data
than any other node on the cluster so
they the compare operation is that
imagine you have node 1 and node 2 and
node 3 you know example and no tree goes
down and goes up and says do I have all
these metadata that you have and it says
ok let's see do we have the same root
hash no so something changed ok let's go
under them which let's compare these
four hashes which one changed the third
one so now let's compare that node ok
the first one and here is word on record
it's different
at this point all the segments is sent
and
there's something called what you get
you register a callback and you get
calls for all the keys and then you can
reconcile the keys that you need so it's
really efficient it's really efficient
to check like no three goes down and
goes up faster two seconds and checks
one hash and knows that nothing changed
it's really cheap and it's really good
to get fast comparing small amounts of
data to the places where something
changed so let's so what if you want to
play
[Laughter]
let's say I I created three templates
rebirth the blades which set up projects
one for air color one for partisan and
one for that and today we will see the
real core and partisan ones but the last
is the party some template so first you
go to I will add the link later you
clone the template on the folder and
then you have it so you can do we
weren't really new that's the name of
the do plate and the name of your
project in this case and so it will
create some files and you CD and it's
for the lazy and for me but whenever I
want to try something I want that clean
vehicle project to try it so you may
derail what this does is it combines
three releases with different
configurations so you can run a cluster
on your machine because the thing that
changes is the name of the note and the
ports otherwise I have to learn I would
add the dr. thing because twisting does
so now it's compiling a little delay
which is the part that takes the most
after that finishes let's split this so
I can start
so what I have useful I use curl to
activate because I have many versions I
recommend to you I have a blog post on
how to setup curl and use it so it's not
hard so now that make the real world and
you do make one you don't make that one
console and you don't have to memorize
it or take notes because it's going to
read me and so release and now you have
three notes but they are not a cluster
they are just three notes you have to
join them to the let you run their real
joint
yes
no this is a problem with urban version
so now I'm doing the join which if you
see the main fight is just three
comments but it's easier to just and
then you check the cluster status you
have to read out and two are joining and
that doesn't mean that you're slow it's
that you have to acknowledge the
joinings it's not like drawing and they
join a plea it's like a stage you first
join them which puts them in the state
of joining and then you have to check
the cluster plan which is what's going
to happen and then you have to commit
the plan which is not joining anything
to the cluster so you see something like
that and you it will join two nodes and
it will transfer 21 because it's a 64 we
know so we are happy with our change so
we commit it read the main file it's
just so you run comments and it will
start joining them and you will see the
messages they're joining
and you see their status and it starts
transferring it starts a process starts
transferring one we know that one that
we know transfers is sub with the next
one and so it's when you're transferring
in order to play dates slower but that's
why it's careful
step-by-step and after a while you will
be 32% and now that we have our
Craster what can we do if you want to
stop you do that and we can ping it
somewhere here we can think what this
will do is it will hash some random
values so every time you make ping into
landon some other you know and it will
reply you with pong and with which
partition handle the request that's by
default under the plate because i cannot
go further than that because i don't
know what you are going to build but
let's see how to add a key value store
in memory key value store with that so i
will the link is on the slides i
implemented in memory key value store
using ets which is one letter away from
being a persistent key value store if
you add bits and since the cool thing is
that if you have a cluster of 64b notes
that is 2 gigawatts so 2 gigawatts by 64
it's a lot and if you do a 256 V notes
then you have a lot of so the difference
that's the real me so what do we have to
change we go to to the main module which
exposes the API RC kV and we have 3
methods functions methods get put delete
yes we are exported three functions and
the functions don't do much they call
same to one with the key and the comment
that's the common denominators get key
the key and put key and value and what
center one does is what decides it
hashes the key because we went to every
time you make a request for the same key
that he should then at the same we get
one we know to handle that and then we
send a comment to that pin node and we
and on the the place on a real core
application when where you put most of
the code is module which is like Jen
server with different callbacks so on
the state and the state of each we know
we store which partition are we and now
we need two more fields the table ID the
ETS table
and we had three more closest to the
handle comment function one will match
pull key and value and will do when its
insert nothing fancy the gate will look
up and delete and since we are that's
all now you have a scalable distributed
dynamo styling memory key value data
store like 15 lines of code and only the
high pass words at the beginning are by
by a framework that is being used in
production here I what I'm not having is
the handle which is whenever you join a
node we have to move some keys to other
be notes that's explained on the e-book
I wrote called the little via color
there's a section where it says how to
implement but it's it is a strangest
part of red corner then the callback for
the thing of because you have to do it
by by small increments so it starts
calling you and you keep like feeding
them and telling them to continue and to
keep calling you and till you're finish
and when you're finished like you know
this one and what you do if while you're
migrating keys you get a request for
that key that we they thought about that
for us but it's still you have to think
about the thing because it's and it's
the worst thing when you say if I
receive a put on a key that I made
migrating what do I do
i forward it and I started and I wait on
the new node until that migration is
finished and then I write it or I say no
I'm busy
forget it or I store it here it's
but it's another problem of the
architecture is a problem of your
processing comments while you're
migrating data from one server it's hard
so that's how to implement the key value
store AmeriCorps and now let's see how
to implement it will be hosted based key
value store using Plumtree on partisan
yes and no yes in the sense that each
node so it will be so that we were
talking with him what happens when you
want to have some data loss it's a
subset there there's real core you can
do it with the windows you just get end
be notes and you send it there but with
partition of country and even with last
there's no control saying I want this
and for partisan so you do the same make
several three consoles every door in
status here is different it will look a
little bit different because and you can
also join yourself you can use the
members you can use the connections and
the ping here is different the ping here
is tell me the note because there's not
nothing on top to do routing or anything
to build it yourself so you can do the
pink as long as somebody else decides
where that thing should go and here how
do you do the this requires a little bit
more of code because we have to
implement behavior so we had to function
in this case set and delete we could
implement more they only to take up the
broadcast Handler and say set first set
so it says locally devalue and then
broadcast divided to all the other nodes
and get just as good and to implement
the broadcast handling for your logic
you have to implement Ali Haider called
partition plan 3 well Cassandra and it's
not really hard to export these
functions what do you do when you want
to roll cast data what do you do when
you will have to merge data that came
from a broadcast a call back to reply
whenever you receive some data if you
already saw the data from the graft is
whenever you didn't solve a message and
you want it and it's changed its
porosity and the entropy which you can
read the react or metadata exchange and
one cool thing you can do which I think
it's on one of these and let you have
recon as a dependency to your project
and you will trace the module and then
you three your broadcast and then you
see all the calls and then you can
understand what's going on and then
you've got a good button
I did a change on react or metadata that
you can specify that you want to gossip
one key but you don't want to store it
in the disk and how I learned where to
put the code is by tracing and seeing
what's going on and and the charts I
showed the in the slides are basically
me trying to understand what was going
on so you have to implement like a
chancer with all these callbacks and in
my case I use an DTS to remember which
which messages I saw to read reply to
the east tail and the merchant you could
you can use vector clouds you can use
your duties what I did is I remember
which not came and for each note when
you run cause you have increasing
numbers so when I receive something new
from you I check if it's newer from your
note perspective and it's if it's newer
I merge it and if it's not I rejected
that means that you can only broadcast
keys from your note and then somebody
has to merge them in some meaningful way
it's but it's like just inserts and gets
it's not really complex and that will go
seek the keys to all the notes so you
can do the reads locally you just
broadcast and finally I think
what I did is so I said earlier that to
make it to make both really similar
we could take we could take the these
200m over here
the example I showed you just now is for
this the only difference is that react
or metadata keeps one copy in memory in
ETA and starts one copy on disk for for
persistence and it does that and the
remaining one to implement would be two
for the real career so what I did is I
extracted the rip color ring that Earle
and all the dependencies and I
implemented a plant we broadcast hunger
on top of parties confused myself so
what you can do now is set up a
partition project and use the react or
ring this dynamo style to to route the
requests so when you do it
right now it's an experiment of course
and you start the cluster you join it
like you do with the partitioning and
then you can transfer notes so whenever
you join the cluster for now this is an
experiment we have to explicitly
broadcast the ring whenever you go
change of course later it will be done
anytime you want
and then you can transfer be notes of
course that's only what this will allow
is for you to on any note on a partisan
cluster to say which note should handle
this request all the migration that you
say it all day assignment referring
notes to note that are not the same and
keeping them separated and distributed
and everything still has to be migrated
but it's possible to extract it the ring
data structure and gossip it around
using plan three like because they
recorded everything which is so what I
do here is I create Oh for me what you
need if worth try three times and it
works on my machine and then what I do
is you can bring the ring to see what's
going on you can list all the members so
now I join the cluster so what will
happen whenever I broadcasts a ring is
that the three rings that right now are
different will merge into one because
they're really occurring has effective
work inside the data structures so it
knows how to merge itself so it says
merging ring from three and I need to
see it again and it's stable because I
really saw that now all the members in
the ring not in the cluster so it's two
different memberships and now I see I
print the ring and now they have solved
the be notes the A's are because that
and now what I do is
I get some indices the first two
witnesses so I can transfer them and I
brought us the change and they'd
received it and then I bring the ring
and you can see a weedeater it means
that nobody has one being out there that
was transferred and I can print them in
all the other notes and see that so now
we can do the routing I mean the
underlying technology core and partisan
will scale so that was I think a lot
yeah well reoccur assumes that it can
see all the know something but you can
do it the hidden node yeah but what I
thought at one point was if you do like
that sixty-four V not ring and then each
be note is has a cluster a 64 Vinod
cluster hidden yeah we need it and so
you have to hop in 64 you have many
nodes and it's not like 65
interconnections but it's what partisan
does yeah I mean they are experimenting
with that I think yeah but it wasn't
merged they there was a project color
which was trying to do that but it
wasn't much but now their protein is
experimented using the hash tables
because the main road were facing is
below the global process register when
you want to that is the scale or the
part that makes it complex to change the
underlying cluster membership without
breaking so they experimented using
level with the STS I think
I think they are really they have
something to show you something I think
they should announce like a test for the
SD is actually if you go into virtual
synchrony there's a paper from the guys
and they have something that is
versatile to what they tried to do min
his D and he's a token of its guess
20,000 and what it does is you form a
cluster the ring and there's a leader
and that does leader for that cluster
then they they also participated in
lower level of plaster and you have like
three or four layers because key please
yeah so yes like a trade or try so so
basically you have five five minutes
here in the ring and one of them belongs
to another for the cool thing is if you
do some three or two hop structures
Hindus whatever each like route that
handles a sub cluster has to know the
routing for itself and then it can know
all the other routes I think but it
depends may use case for me I'm I mean
with react or an already okay I just
interested in how to use because yeah we
use it and it's interesting so yeah
there are many ways you can do like
distributed hash table which there's one
by Jasper
tht then you have advanced and you have
depends on what you want to do there are
different ways of doing it but depends
on the specific place personally I like
the the leader you would well yeah you
could join you can join non hurlan notes
to react or cluster using C notes I
guess Summerson I know Emerson wrote
this paper they have Summerland but I
think you can there's no things stable
and tested us make or open source or or
you have this but for a specific just
classic close to taking quite a bit yeah
but I don't think if they have the
pattern they take more for multi P
yeah the only other one the distributed
systems framework is audience in that
which is used but it's a different part
it's like it's like a transparent
message-passing legging Arden in the
sense that you say you send a message to
this process and but it also handles the
lifecycle of the process so if the
process is not alive it would be spawned
and if it's a while without doing
anything it will be slapped the state
persisted and when I run a systems so
it's a little bit higher level level to
be but it's there's a project called
dance which is which is built on top of
parties and I think it is it uses a
party Passport
so there's a lot of cross pollenization
in your life that's what the cool thing
if you want to try like you have react
assemble for consensus accidents you
have advanced for the northern style you
have react or for dynamo you have last
for last they take themselves so you
have many different really will test
frameworks to try different patterns of
experiences and of course
we has specific multi data center
replication which is just I don't know
what it is yeah it's it's not like it's
not like a routing of B notes but it
does some synchronization rather than
system and I think it's essential that
you have heard that the central clusters
and then it does the idea because they
want to they started to wind ASP for
machine zone which was a video game
company and they wanted it to
synchronize their words for video games
on mobile that you're playing a game and
you go to a tunnel and then you go up
and you want to synchronize whatever you
did whatever happened outside so yeah
it's for less slowly eventually
consistent state since that like nodes
can go down quite heavily and heavily
converge to a state it's it's hard it's
not hard it's more restrictive the set
of operations you can do with the CDT
are really restrictive because they have
to be commutative no matter if the order
we apply them they have to converge to
the same time but if your pattern fits
yeah in case you're interested
I implemented a because I was thinking
what
there's no less code to read that
implement something something so it
permitted a role based authentication
access control which is stolen from a
little bit from real core security and a
little bit problem yeah so I just copy
the interface and it uses less so with
gossips the users and groups of the
permissions you see in series so you can
play with some real use case or last
partition if it doesn't work it's not at
fault because this is here that I'm
using their massage with advertised and
this will you just do an operation and
one now then you will go see in
conversion you should persist it
somewhere if you read I think a whisper
not persisting because it will be as
long as there's one note alive it will
then later be replicated to all the
notes when they do what I would like to
persist it and I don't know how to if
you if the whole data center goes down
and goes up and then you start lifting
the data from the store to the CRT if it
will generate a lot of conflicts
that's this so this is like a use case a
real in this case on last or partisan
you can play and you can view the
cluster and you do the operation in one
and you check in the other one the data
so yeah a lot of code I will posit it
all these means I have some on the last
slide and so they recover kV and the
partition KD with world cos is here
the reoccurring on parties are the</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>