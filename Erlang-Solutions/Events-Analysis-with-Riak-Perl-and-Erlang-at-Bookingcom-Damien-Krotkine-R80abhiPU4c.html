<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Events Analysis with Riak, Perl and Erlang at Booking.com - Damien Krotkine | Coder Coacher - Coaching Coders</title><meta content="Events Analysis with Riak, Perl and Erlang at Booking.com - Damien Krotkine - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Events Analysis with Riak, Perl and Erlang at Booking.com - Damien Krotkine</b></h2><h5 class="post__date">2015-11-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/R80abhiPU4c" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so today I'm going to talk about how we
use react to store and analyze even the
other flow at booking.com so as also say
damned american i'm a senior software
engineer at booking.com i'm known as
dance on github and some other internal
services you can reach me on twitter and
19 feel free to give me feedback on
these talks and other topics so i do
work at booking.com i guess nearly all
of you know booking.com if you don't
please raise your hand okay no hands Oh
1n ok so we are number one website on
accommodation so we are a very big
website basically we have more than
800,000 Odette's actually these figures
are not up to date in more than 200
countries a lot of rooms are booked
every day like well more than 800,000
rooms booked every single days so that's
quite a lot of course we have millions
of guest reviews a lot of offices and we
are now reaching 9,000 or 10,000 people
from which we have seven more than 700
people in working in IT in big office in
Amsterdam so we have a very big IT team
so basically it's not a small website so
I'm going to start with very quick and
rushed introduction about evens what
they are on how we use them for so when
when the user browse is our booking.com
website of course there is interaction
with the booking platform but also
clients are smartphones constants and
also we have special clients which are
authors basically they use api's and
other non web means to connect to our
platform to change rooms availability
prices and so on so these are just only
three out of many examples of
customers reaching out and using our
platforms so on our platform we have so
this is a very simplified view of our
front end of course but you can be
represented like that web mobile and API
of from ted's subparts which which is
replying to the users and why they are
doing so of course the front end is
using a bunch of stuff in in the backend
10 plants databases caches message
queues and so on and why they are doing
so these back end sub systems are
sending events to our centralized even
storage okay so that should be pretty
straightforward for definition of what
and evens flow is of course we have even
from web mobile API databases caches
load balancers availability trusters
demons that do stuff with emails of
course and a lot of other sub sub part
of our platform basically we try to have
even from everything from our back-end
now what's an even even is only a piece
of data which provides information about
our subsystems so technically it is a
deep hash map data structure so key
value data store to which values can be
anything skaara arrays or yet again
recursive key value structure and this
piece of data does not do anything or
personal it just provides information on
all the subsystems all are even have to
specify a tungsten type and subtype and
then the rest and the rest is specific
to the subsystem which emitted the event
and we have so many different subsystem
and every developer can change what such
subsystem sent as an event that at the
end we cannot draw any schema of our
event so our even flow is schema-less
and this is very important for the
specifics the properties of our pipe
pipeline so this is an example of a big
event we are very big even very small
event and evens of any size so basically
here we can see this is a this is a hash
structure with timestamp type equal web
subtab equal up so this is a big event
coming from the web the main web
application it has of course data center
DC a data center number and this is
specific to the web application even
type so we cannot dry we cannot draw a
schema out of this but here we can see
that we have the list of requests the
time it took and here we have warning
messages a bunch of very interesting but
very big stuff here on the opposite side
of the scale we have a very small event
which comes from our availability
cluster and availability cluster is a
group of clusters actually that is
responsible of knowing which how many
rooms at what price is available for a
given time so here we can see that there
were seven queries with latency of 21
millisecond I think from this cluster so
this is very small so now the property
of our even flow is that evens are of
course read-only we never changed them
technically we could but we don't scheme
RS we have more than 50k even per second
which adds up to four billions for
billions even spurred a network usage is
more than 150 Meg per second actually
one year ago we were throwing 1.5
terabyte of events per day in one year
we have switched to 62 right so it is a
huge growth and we store so we want to
storage for the hot to be considered as
the hot storage which will keep the last
10 days or eight to 10 days and for the
rest we also send a copy to Hadoop for
very slow but very long
we use our events to do real time graph
for instance that's the first usage we
try to draw graphs of all our parts in
our subsystems so yeah we want a general
platform health check that humans can
actually feel and see so this is an
example I've removed this case of course
but this is an example of a graph here
we can see the number of event and the
number of even with warnings and errors
very simple graph here we have another
one where we can see the top 10 abnormal
user type so here we can see that we
have the type is strange users that do a
lot of visits but they never actually
booked any rooms so these are probably
competitors trying to see what kind of
our tenants and prices we are doing all
teams at booking.com are creating and
managing a lot of graphs that then are
gathered together in dashboards and then
we have dashboards or dashboards and
stuff and all this is created created by
using the events flow so we have a big
fetcher group of factors that are
consuming the the flow and using the
values and into the events to create to
create metrics and send it to graphite
we have a very big profit installation
and because this system which is
fetching and graphing is part of our
back-end then we can graph it as well so
we have so called meta graph so it's
kind of a bit crazy maybe you are
graphing too much but at least we know
what's happening we also use events to
do short-term analysis from 10 seconds
to a few hours for instance when we
deployed something in production we have
a very quick way to work back and we
have automated
processes that use the constant even
flow to detect any issues any breakage
so that we can roll back very easy on
average we do production code deployment
hundred times per day so apart from that
we also have an anomaly detector which
is consuming a window of events last two
minutes for instance and applies
algorithm to detect strange behavior
like if we are under dealers or hackers
attack we use that to to detect these so
unaware adithya detector is a very
important piece of our security and and
back-end and basically the evenflo is
verb and so I've very important to us so
it has to be there also we are
data-driven company so we do a be
testing probably no one disease so every
developer can create an experiment where
it will change the color of a button on
the front end for instance and then it
will and end result if this pattern is
making me more booking or less booking
is coming back into the evenflo back to
us so we can decide if it's a good
feature or not so let's take a very
brief look at the overview so here we
have front n plus back end press
everything which is sending so these are
called producers producers of events
they are sending events as UDP to a
first layer of boxes which are the
listeners so you can see that we are
sending of course we are sending events
over the wire on the network so the data
structure that I've shown you in memory
needs to be sterilized at some point
before we can send it via UDP if you
have questions on why we use UDP you can
talk to me after the conference after
the talk away expand your wife then we
have a second layer which are called
reader caters that actually take the
events from these boxes and send them to
our global storage this way of doing is
very classical it's a bit of historical
and could at some point be changed to a
more q oriented system but it was there
so we used
now we need to aggregate our events so
basically we don't want to send our
events individually because it's kind of
a waste of a resource it's not a big
deal for fetcher to fetch one second of
event and then find the even see it
actually needs inside that globs so the
granularity is the second and we want to
group or events as soon as possible so
to be able to do that we need to
sterilize our event over the network
long time ago we tried chasing but for
few minutes only because Jason didn't
work for us at all it's very slow to
encode and decode and it produces big
results that is so basically the only
good feature of Jason is that is human
readable but it lacks a lot of features
like being able to have binary data
inside the structure of being able to
refer to a previous reference of data
structure so constancy it's not really
possible to have a recursive data
structure and Jason so we created
surreal in 2012 which is a new binary
data citation format that provides high
performance schema-less theorization so
this is the key it's a new series Asian
format because we didn't find any that
would suit our schema as a type of data
structure otherwise we could have used
things like protocol buffers or other
formats but or dig City you can check it
out on github it's a very nice piece of
software very efficient let's go back to
our logger we have a bunch of events
that are streaming so sent via UDP on
our logger first layer of machines the
colors are the different types the
loggers are responsible for doing the
first aggregation by simply accumulating
events in different silos different
stats / time so nothing very complicated
after one second we take a slice of the
events and we we take the group of evens
/ type for this epoxy second and we
realize and we compress them to have the
smallest size possible and we send it to
the storage
of course we have many boxes doing that
now back to the storage what kind of
strategy we use we wanted storage
security so it's it's it's the number
one feature because so many critical
things rely on our even flow we are
needed storage security we also need a
massive right performance because we
want to be able to take to intake the
events flow in real time and especially
when the website is in a strange
situation when our platform is under a
ddos attack for instance or with
something weird happening the the even
flow goes crazy it can run but this is
exactly at that point that everybody in
the company wants to look at what's
going on so they want to fetch like two
times or three times more than usually
our events so we need a very good right
performance and also actually a bigger
read performance on top of that we
required easy administration because the
team setting this up and managing
managing it was very small and we need
to scale because every every year over
two years we double everything at
booking that's not from both and evens
are actually at the higher walls so we
will bankrupt contenders and which was
react because because it is very robust
I mean it's the probably the best ring
clustered solution internal for business
it has good and pretty predictable read
and write performance as we will see it
was the easiest to set up an enemy
straight so we went for it apart from
that which are enough to to make our
decision it also has advanced features
are produced triggers secure indexes she
oddities we didn't use all of them but
yeah it was like a future-proof product
so this is react maybe you know about it
very briefly it's a clustered storage
system built on community hardware we're
all node serve data so basically
as an external user you pick up you pick
one random node and you do request
against react and react to take care of
doing replication of the data for you
handling the network partitions when a
nose goes down when it comes back
managing hidden hand offs and so on so
it's very easy to use and to
administrate and it uses gossip protocol
between nodes yeah so it's a distributed
system it's written in along when react
when when we want to store something in
react we send a key and value to random
node and react is applying a hash
function on the key so on the name of
the value and from that it reduces
primary nodes in this case three because
by default the replication replica
number is three but you can change that
and it sends the data to these primary
notes when you fetch data it does does
it do positive way so very simple stuff
it's not using consistent hashing it's
based on a simple Hadji so react is your
key value store keys are unique in a
given name space and then space our code
buckets values can be either 0 pack so
react doesn't want to know really what's
a add value it can be text Jason binary
whatever or it can be sorta teas which
are really great but we didn't use it
for this project I used it in another
project so here we have simple keys and
simple binary values react is very
flexible and you can choose how to store
data on these go in memory it provides
at least 30 main back-end bit cask
leveldb and a memory back end which is
based on along ets data structure and
which we decided to go with the big cast
back end because it's a simple back end
which is easy to understand and it
provides predictable performance so
basically it uses up and only files
which are open and everything is
appended to to these fines even when you
want to change the value or delete
value it happens the comment to execute
at the end of files and after a while
these files are closed compacted and
data is expired while doing compaction
and that's it we move to new files the
keys are loaded in memory which is not
the case with leveldb but here we don't
actually have that many keys because we
are aggregating our event so the keys
have to fit in memory which provides you
a good predictability when you want to
fetch data fetching data from big task
means to find the key in memory and then
fetch the data from the disk but big
cask uses the file system cache so the
data is probably already in memory and
if not you're going to reach your data
with one disk seek maximum which is not
the case with liability as Allah
Willoughby has other good points so this
back end is perfect for sequential data
and this is why we used we used it for
our even storage I'm going to go very
quickly through these slides so these
are the numbers that we needed when we
started so we needed to store 6200 bytes
of data basically because hundred get by
our application factor 3 and so on we
went with two clusters each of them with
12 notes at the start now we are more
than 32 notes per cluster with tribe
CPUs ahead lot of memory which is great
HR bite it took its ok and only one
gigabit network speed which is an issue
and we are trying to c5 time how are we
worked on this issue so we would have
been much more comfortable on a tent ng
network and we are getting there but
when we started and now we had to cope
with one gene at one so you remember
saving this even flow to the storage
which is going to be react now we have
to think about the data design so here
when we are we at the end of the
relocator layer we are strong the
group of evens which represent one
second of events into a react so we have
a blob of events and we have so one blob
is / epic so the number of seconds
timestamp then the data center number
the type and the subject is also cell
type but this is just for implementation
so nothing very fancy there and so we
group or event by these combination of
characteristics that gives us a binary
blob if this blob is bigger than 500 k
we check it again so that we have values
that are small ish to straw into react
because you don't want to store big
values in react so we are going to store
these drops into the bucket name called
data because why not and the key is
going to be just the concatenation of
all the properties using whatever
separate or you want okay so this is
very simple the value is of course the
blob the list of events for these
characteristics serialized and comprised
that gives us two hundred or more keys
per seconds but not ten thousand which
is great because then we have a reduced
number of keys however in react if you
don't want to use additional indexes or
solar secondary indexes you need to know
exactly the name of the key to be able
to fetch evens and consumer fetcher is
is not going to know all these
properties to fetch the data so what we
do is after we have set the data to
important bits we create another key a
meta data key which is going to be in
the meta data namespace and the key here
is going to be much simpler only time
stamp because all consumer knows about
the time they want the even from and the
data center which also is something
known and the value of this key is going
to be the list of long data keys so here
we can see that when we store data we we
are going for instance to so these three
keys or three probes for this epic and
data center so we start by writing these
and once they are written then we
rewrite the meta data
here and you can see the keys very
simple easy to understand and value
points to data keys okay so it's going
to be a two layer data architecture
writing that up so yeah that's what we
do so we push the data then the metadata
that means that here we can we can have
some sense of atomicity basically if a
meta data key is in react then you can
be pretty sure that the data is are
there it's not guaranteed because it's
an eventual consistency but still so we
do that in parentheses on the pseudo
code but it's very easy you instantiate
a real client you put the data here so
this is the namespace name and this is
the values this is the key names and
after that you do the same with me da da
da so nothing very complex here the push
strategy is to use many boxes from the
relocator layer to do paralyzation using
many machines to be able to to send
using all the nodes of the clusters and
we want to maximize iosh basically
network usage so to do so I've written a
new cloud clients using any oven which
is a very efficient even to oriented
library so new clients will react and by
using that we are able to maximize the
performance of our network when we want
to beat data well it's very easy we
decide which epic which a timestamp and
which data center wants to be we want to
read from and we read the meta data for
the given epoch and DC that gives us
this value and then we fetch them in
parallel from the data namespace while
doing so if the fetcher is only
interested in emk type for instance then
of course it doesn't need to fetch the
web key so there is the first way of
filtering out unwanted even types so
again yeah when we fetch data with from
the metadata and
and following the links we read from the
data namespace in power again that would
be here i have my rear client i use it
to get the timestamp and DC from the
meter data in space which returns me a
list which I sorry clear text metadata
which I Spit on pipe that gives me an
array while I'm at it I grab only the
type i'm interested in to from that
array which gives me a future array on
which I loop and I use the client to get
the data keys from the data in space
alright so here we have managed to fetch
one second worth of event great but if I
want to last ten minutes then we do a
very stupid things for which react is
kind of optimized for we simply
enumerate all the seconds from 0 to
minus 10 minutes and we do massive
parallel fetch from react to get all our
data and react react actually excels at
handling huge number of requests per
second so this is the recommended way of
using react it's no use to try to do a
bulk fetch or use MapReduce to get
things out of their power and fetching
is the way to go so I set up these first
12 notes cluster with all our veterans
and using the data to do graphs anomaly
detection and so on and the constant
even stream going in and basically these
boxes we're doing nothing like using one
up to two cpu at the time and the disk
i/o operation we're kind of low because
in terms of disk i/o ization we are very
low like was very hard to see actually
the disk usage so it works basically
very well and here we can see that the
free space is reclaimed I have set up
the exploration of the back end to be
one day when I started so we can see
that we are cramming we are pushing
events into files and then at the end of
the day
we are doing a massive exploration so
I've combined the compaction of the
files because react wants to compact
files to be done only after the data
inside the files are expired and there
is a shortcut into the big cast back end
so that if the last data written is
actually older than the exploration it's
not going to try to compact the file it
knows that all the data is to also it's
simply delete the files so it doesn't
use any CPU to do that so real time
processing outside of react this is the
normal way of using our even flow at
booking.com basically we fetch as I
explained we face either the last 10
seconds or the last minute or the last
hour out of react by enumerated all the
epics and fetching them in part and we
do streaming so we basically we can
categorize the the usage of the data
into very short-term real-time oriented
fetching and up to batch oriented
fetching so we have streaming of data
center graphite every second so we have
bunch of crystals fetching all the data
and crunching the data to send to
provide we have a sort of anomaly
detector which is gathering data in real
time but creating a moving window of say
two minutes to pride this is its
algorithm we also have our a be testing
which is kind of streaming the data and
then we send stuff to Hadoop every
minute or actually every hour I remember
and so we are moving away from where
your time then we have mine on request
done by people trying to find issues and
but batch fetches and so on so basically
we have all kind of different strategy
in fetching so we cannot really optimize
our storage for these or these orders
and that ends up being a huge numbers of
rig requests so this is a way to look at
it you can see that these all these all
these
systems and clusters are fetching from
the even storage and they may be
actually fetching the same data at the
same time because they needed so it adds
up in terms of bandwidth usage which is
starting to be an issue because we are
limited network bandwidth of one key
however it's real time because so with
few seconds lag you can and even is
stored in less than once again and it's
available just before before few seconds
so the issue again is network saturation
instead of doing real-time processing
outside of react we are actually doing
some of our processing inside of react
to try to limit the network usage so we
are not doing everything inside react
but more and more we're trying to push
to do that inside Maria what does that
mean means that the idea is that instead
of fetching data we text a lot of
network bandwidth then crunch the data
to produce a very small number for
instance everything we sent to graphite
are very small numbers like averages and
stuff like that number of warnings and
so on and then that produces a very
small result instead of doing that let's
bring the code to the data near the data
try to use data locality to quench the
data by the CPUs on the react nodes and
then the result is very small and then
we can fetch this result which is going
to be very few bites and drastically
reduce the network usage so when you say
bring the code to data usually we need
to actually look at what it means what
text time is fetching the data out so we
saw that this is pretty clear but also
decompressing the data is taking a lot
of time because we are using heavily
cellulite and compressed data it takes
four seconds for instance to decompress
one second worst of data so you want to
prioritize stuff and you want to be
careful of the CPU time you use to
decompress data so when you say bring
the code to the data the idea that
immediately comes to mind is use
MapReduce so react provides
quite a nice bar peruse feed you so I
played with it and I managed to set up
MapReduce jobs and it was working fine
and actually achieved I achieved to have
real time are produced I was able to
create and run MapReduce job job on one
second worth of data which would execute
in less than one second so this is real
charmer produce quite different from how
to produce one sense however the issue
is that so the good thing is that
network usage is reduced to almost zero
because the data is quenched on the
notes however if you have 50 people
trying to fetch to crunch data on the
same epic and the same time stamp they
are going to fire 50 different by
produced jobs and because MapReduce jobs
are isolated because that's the design
of the future they are all going to
decompress the data in memory 50 times
and so we are actually over using our
CPU and it means that it's effectively
useless for more than five to ten my
produce otherwise you need a huge
cluster so this was not the way to go so
I looked at another feature of react
which has which are hooks we are pre
commit and pasco meat hook which
basically allows you to write code and
put that in a call back on the react
infrastructure and have that callback
triggered an exit executed when data is
about to be store or has just been
stored I use the latter so first commit
hook execute code from you just when the
data has been stalled and I use that to
crush the data on the node so this is
how it goes react a sense that so this
is the relocator box basically sending
the data to react here by choosing a
random node react stores the data to the
replicas and then here we have the node
hardware on which react service is
running react is triggering the post
commit hook which executes
my code and my code receives actually
the key of of the data has been stalled
so this is the meta data key and it
sensed the key to a companion rest
service which is on the same note but
which is completely different from react
see some raised stupid quest service
that we have implemented which receives
the key and fetch is the key the data
from react and then applies all our data
processing data processes on the data
which is decompressed in memory which
means that the data is fetched once
decompress only once in memory and all
our data crunching jobs are executive on
that particular beats and that that
means that the data is decompressed only
ones and so all these jobs produce a set
of very small results that we can send
back to the client or actually send back
to react because these are very small
values so this is the code of the hook
code I'm going to go very quickly over
it basically the API of the hook a pasta
meat hook is that we received a react
object from which we extract the key and
burkett so this is the meta data key and
value so from the from the key name we
can extract the epic and the data center
and from the value if we split on pipe
we get a list of data keys ok and we
send all of these to our REST API by
calling the centrist function so here we
are talking of maybe how good bites very
small data center west is basically
building URL to a horse named so and it
the body is the data keys that we encode
and Jason in Jason because we don't care
about performance here the value is so
small and we send it as an HTTP request
with this option sync equal force
meaning fire and forget so we have
effectively decoupled the reactor this
these data crunching service so that we
cannot bring down react if if we cannot
reach the service or if there is some
data processes that goes wrong so this
is very important for us to decode them
on the other side we have a red service
with an imperial using PSG I which is a
pearl implementation of wsgi went on for
Python depth so the web server is Tom on
using pre force I'm not going to show
you the code because it's very simple
basically we received a key we
instantiate or we use a react client we
fetch the values we decompress them in
memory and then we receive a list of
data jobs to execute on this we do that
and then at the end we sent the results
back to react it's scalable because the
more nodes you have the more CPU space
you have the more memory you have so if
it's too slow just add nodes and there
is a nice trick instead of having the
post commit hook cooling the companion
on the same node because then the
companion is going to fetch the data
from the primary keys the primary notes
from this key but there is a big chance
that the data is not on this note so it
will have to fetch from all the nodes
inside the network and then store the
result instead of doing that we change
the arrows so that the react post commit
hook because it knows everything about
the ring can send the key to actually
one of the primary node for that key and
then the companion is going to ask react
for the value but it turns out that the
value is on the same node so here we
have hundred percent data equality and
the value never cross the level goes on
the wires inside the cluster and outside
of repressor we are only fetching the
very small values a lot of advantages
that I've listed so that our quality
that I do compress only ones and so on
and so it can be written in any language
which is great we use / but you can have
your rest service in Java or Python or
whatever
disadvantages it only works for
streaming data for incoming data you can
say oh I have a new data job I need to
execute to the past two months of events
that's not going to work it's by design
we needed a streaming data crunching
solution and you cannot easily do cross
second data because the values are
spread over all the nodes so it's not
something already worked into we don't
need that but it's not very tribal to do
let's go back to the bandwidth program
because here it works fine if we do
that' crunching on the cluster but this
is kind of new and we are doing some
data crunching that way but a big chunk
the majority of data crunching happens
by fetching the data out of outside of
react as I've displayed so we cannot
migrate everything to be done on the
cluster so let's try to optimize the way
data is fetched out of react so this is
called the bandwidth problem let's have
a look when we fetch the data from react
into the by using the standard way we
issue okay we pick up one roll of node
here and we say please give me that key
react is going to issue the request to
the primary nodes and because that's the
way we've computed because we are
read-only data when the first request
which this node then returns it to the
client so by default react is optimized
for speed okay request all the primary
nodes as soon as one of them which which
has written it this is because you
configured that that way you could
configure it using forum or to make sure
there is no conflict or so on but here
we have read-only data which have
internal checks on so we can check for
conflicts and data culture that was if
you are in the bad case if you're in the
good case by rendering on only choosing
a node it happens that you reach out or
primary node in which case react is only
going to issue
request on the two other pro he knows so
in this case we are only using two times
the outside network usage but basically
we can say that roughly on average the
inside network usage your cluster is
three times the outside network usage
because of this primary nodes and this
replication usually it's not a problem
but in our case it is because we are
limited on 1g network and we have so
many people willing to fetch out of
react that they are saturating the
network outside of the twister two
solutions for that one of them is to
optimize the query for network usage
incidents of speed there is a
little-known parameter that you can set
when fetching from react is n val equi 1
which means that it says to react
pleased when you do this request
consider that i have only one replica
and not the default 3 by doing so react
is going to fetch the data only from one
primary key and if you are lucky and if
you actually first poked at a primary
key then the data is actually not using
any of the internal network usage that
works only because the data is read only
of course there is no conflicts and we
have checks on to detect corruptions by
doing so the practical network usage was
divided by 2 which is a huge game the
other solution is to stop choosing a
node one only but try to always fetch
from a primary node so when we saw
affect something in react we pick up a
random node by default react is taking
the bucket name and the key name and
applies a hash function that gives a
result a big name and using the ring
status it can know where the primary key
is ok yeah and and so we can store or
fetch phone repair manuals eid is to do
the
is processing on the client and before
we speak to react so that we can have
the knowledge of the ring status and do
the apply the hash function on it and
directly know which primary keys are
there so we can reach them and be in the
good case out for these slides how do we
do well by default if you look at the
default configuration of react you can
see that the default hash function is C
hash STD key phone which is fun fun in
this module react or YouTube react is
open source so I went to github and
looked at the source code I found the
occurrence in this file and here this is
the function which is called and we can
see that it calls SH á which is
actually a shy one implementation so
very simple react use Chauhan we we know
how to that to do that on the client
side so that's one part of the stuff
done the other part is to know about the
ring stages so this is a bit more
complex but basically i created an
hour-long app which is loaded when react
starts and which hooks itself into web
machine which is the HTTP API of react
to add a new endpoint of the API and
when we create it returns the ring
status because there are sort of cluster
stages because the cluster architecture
the crystal topology doesn't change that
more that often I mean we don't add a
new node every minute we can cash this
result for let's say 10 minutes or one
hour and so we are not obliged to query
react all the time to get the ring
stages and we can do the hash function
on the client so this is the results of
these new endpoints I have added to
react and it returns a list of bignum
which are actually the result of the
hash functions intervals and mapped to
react notes so when I apply my ass
function so sha wan on the serialized
concatenation of key and bucket strain I
get a number so usually a very big
number but for this example I've taken
this one we can see that it is between
zero and this huge number so this means
that the primary node is 16 and because
replication is three then we simply
react text the next two nodes as the
next primary nodes so by combining these
two features ring stages press applying
shawan on the client then we can know
exactly what the primary node for this
key is and we can poke react directly to
the primary node so we are never in the
bad case and we can reduce the network
usage even more a word of warning this
is possible only if your notes are
properly monitored we already had
zookeeper there so we use that because
when a node is down you don't want to
spam it with a lot of requests and then
fall back to picking up a random node
and also you have to make sure that the
data is requested in a uniform way
otherwise if everybody is always
interested in to the same data for the
same epoch and if you don't have enough
keys if you that I is not spread enough
on all your key namespace then you are
going to create a hotspot on three notes
on one second then three next notes for
one second this is something we want to
avoid as a result network usage even
more reduced comes to the conclusion of
this talk we used only the react open
source version we use the other
information that is available on in the
community we didn't buy any training
even though I sure would have robbed us
to do so but yeah we were a very small
team of self-taught engineer and react
was a great solution post its rawest
it's very robust I mean if you have to
remember one thing about these stories
that react is very released fast
predictable and scalable and in addition
to that it is very flexible and hack
ever
as I have I hope demonstrate it two
years and it helped us to continue
skating and we're probably going to
continue using that for some years thank
you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>