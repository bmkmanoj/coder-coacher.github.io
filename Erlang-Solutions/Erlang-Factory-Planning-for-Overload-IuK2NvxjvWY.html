<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Erlang Factory --Planning for Overload | Coder Coacher - Coaching Coders</title><meta content="Erlang Factory --Planning for Overload - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Erlang Factory --Planning for Overload</b></h2><h5 class="post__date">2014-03-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/IuK2NvxjvWY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">right so yeah blending for overload this
is a topic I think is fairly important
in whatever system we design but often
forget about so I'd like everyone to
raise their hands up just from beginning
to have one just raise one hand up if
you want wait
all right so if you've ever had the
error though keep it up keep it up kiddo
if you've ever had the error log or
blowing up you can take your hand down
if you have ever dab blocking operations
that we're backing up either writing to
a database or to a socket you can take
your hand down also if you've ever had a
message queue that explodes you can take
your hand down
I see no hand left up all right so
everyone has had issues with overload
and pretty much everyone has seen that a
heap a lot cannot allocate the bytes of
memory in deihi message before which
happens whenever you run out of memory
the way to see the system in that way I
like to compare it to a bathroom sink
usually in informal discussions I use a
toilet but that's yeah bathroom sink is
a bit nicer for the kind of data you
push through it
so you always start at the edge of the
system where the input comes in and you
go deeper and deeper that system until
you get the output and the output here
is something down at the drain so in
normal operations you will have a bit of
data coming in the data will come out at
roughly the same time everything is fine
everybody is happy you can brush your
teeth spits in the drain whatever it's
gonna be fine from time to time you get
temporary overload maybe to people who
are putting water at the same time in
their system or maybe just more usage
and whatever and usually it's fine you
go you're going to have to sing
accumulate water in it you're gonna add
your system accumulate stuff in buffers
in memory and after a while it's gonna
subside and when it goes the drains will
quickly empty up and then regular
operation can follow so you're able to
deal with a bit of overflow temporarily
the problem comes when you have
prolonged overload so when the upper
bowl keeps going and going and maybe
you're popular maybe your whatsapp and
you just got bought for 19 billion
dollars and more people are using your
application and before when that kind of
stuff happens there is a breaking point
at which the system can no longer cope
whether it's going on
so yeah idioma is gonna get full and
full and full and you're very happy
about it and then all of sudden you've
got a crash dump everything is spilling
on the floor your system is dying
so yeah that's overload for you and a
very very short manner so what do we do
when we see that kind of message the one
with a crash dump and stuff going very
bad
the first reaction usually I guess is to
raise you limit it took too much memory
so just just give it more memory I'm
just gonna solve the problem right if we
made it bigger it's gonna work it's
science
what could be wrong with that so you run
it that way and then a couple of weeks
later when it had the time to gather
more data in its temporary overflow it
dies again so then you start going
outside yeah well too much memory and
then you look at the process cues and
the process heaps and you see that oh if
something in the edge of the system
basically is taking too much memory so
let's optimize now so you start taking
the edge of the system using hmm looks
like the buffers are getting full and
you limit didn't help it so let's make
bigger buffers so you build a bigger
buffer then you wait 2 or 3 weeks you
put it in production and eventually it
crushes it again so you look at the new
crest and say yeah the buffers are being
so clearly there another problem and you
start looking at the drains at the
bottom of the buffers and you say yeah
well maybe it's time to optimize that so
you spend a couple of days or maybe a
couple of weeks optimizing them away
then you have this fantastic new system
in production that's now much better
than it was before you let it run for
two or three weeks and it crashes again
so you start looking at it and say well
looks like the drains are actually
emptying is still tiny pipes so you
spend a lot of time optimizing that one
and that's where you do the hard corrupt
invitations and you get the profiler out
and you shave milliseconds out of
everywhere and so you deploy this kind
of Hulk of a system that's terribly ugly
not only again but it runs kind of fine
so yeah yeah super big pipes and
everything and you can brush your teeth
forever without a problem
eventually you let it run and it runs
two months without a problem it's fine
you're a genius and then he crashes
again then you look at it and you say oh
yeah there's this kind of drain at the
bottom that I don't control this is
actually going to sewer system and I
don't have the means to fix that so
what's the conclusion about that is that
I get paid to fill to solve the wrong
problem so yeah for all that time we've
spent weeks and weeks and months and
lots of time and company money to fix a
problem that actually if we study the
system a bit would have seen the drain
at the bottom and this is the kind of
overflow or overload that we cannot cope
with this is something external to our
system if you're working in the real
world that might be an API to an
external system from a different company
and as much as it will be fun to storm
into their offices and kick down the
Ruby walls and install Erlang it there
to make it work faster you just can't
you have yeah sometimes at the disk that
is getting too full the disk is not fast
enough and you could change hardware
might be a bit costly it could be the
bandwidth you have in your system
there's something at some point that
gets output from your system otherwise
you're writing death null in a very
complex way and whatever gets out from
your system has a rate or maximal rate
which is can be consumed at some point
it might even be the maximal rate at
which humans could consume the data at
some point so yeah if you don't want to
be paid to solve the wrong problem
overload must be planned for and it's
absolutely important overload is planned
for because it defines how you died and
given how many people lower their hands
at the beginning of the talk it's also
the fact that defines how often you die
now it won't cover all the cases you
dies because all your failures are still
going to give you a very intelligent
failures but a lot of them can be
blocked or planned for or well just
prevented by planning for overload it
defines premature optimization and
basically the entire process with drain
I mentioned is premature optimization
and we have developers or
Engineers often have the idea that you
need to measure before optimizing and in
that case we even measured we had
symptoms we had dreams we had buffers
that were full we have mailboxes that
get filled and we optimized in a way but
we just measured around the wrong thing
we measured where the problem was
happening but never took into
consideration the internal limits of the
system that we cannot modify and each
system has that kind of limit at some
point past which it's not worth
optimizing you have to grow it a
different way compress your data or
whatever you can do but there's a hard
limit given by hardware or how many
nodes you have something like that that
you cannot go past and if you don't find
that point first every optimization
you're going to be doing is going to be
done for nothing and for a lot of money
which is kind of fun for your resume but
it's kind of bad for your employer it
defines your margin of error and the
margin of error that we have when we
operate a system in production I like to
see it like a kind of a balloon that you
came inflating so when you blow with the
Loon it gets bigger and bigger and
bigger and at some point when you blow
in it there's a pressure coming back and
you feel that it's about to explode so
you just stop there tie the knot then
keep it there and you look in the next
balloons you blow you blow them roughly
the same size and at some point you
figure out that oh you can make it maybe
a centimeter or an inch bigger in
diameter and it won't blow up so that
becomes a new margin of error and the
margin of error is that buffer space
between where you're at right now and
when the balloon explodes in your face
or in the face of a children at the
party which you cannot want to avoid a
bit more so yeah as you go you blow it
more and more and more and more and at
some point the margin of error is going
to be razor thin and if you're running
anywhere close to a cloud storms are
frequent and it's not only things you
control that restrict the margin of
error running in the cloud is sometimes
blowing a balloon in a room full of
needles and sooner or later it's going
to blow the balloon up in your face even
if it were safe for three months and you
change nothing and it's still the same
volume so it's important to define a
margin of error because if you plan for
overload you're able to fix that margin
of error in a way that's always safe and
lets you detect problems before they
trash systems entirely it defines your
API and to define the API I want to come
back on this a bit later just keep it in
mind that your API will be impacted by
how you plan for overload no matter what
you what you want to do and if you don't
plan for overload when you do your first
API and then you figure out that you
need to do it later
you need to change your API and that's
not simple if you have customers that
depend on your API and then they build
their code according to it and you end
up changing the semantics of it not just
the way the calls were made you're just
gonna piss off a lot of people and
probably won't be able to get people to
migrate there and won't be able to
handle overload ever so you really
really need to plan for it and in
general defines what is engineering in
your team all these tasks figuring out
how to die figuring out optimization
figuring out what's your margin of error
figuring out how you expose data to
people that's engineering so if you're
doing engineering overload is something
you want to care about you have to pick
what has to give ideally all the systems
we develop are the perfect system that
keep accepting all the data in the world
forever with no pressure and keep
churning out good results all the time
with no pressure in reality that doesn't
exist it's similar to databases with the
cap theorem and the cap theorem you have
to pick between consistency and
availability to different degrees you
cannot have both while tolerating
partitions for overload you have to pick
between blocking an input which is back
pressure or shedding load which is
dropping data on the floor so back
pressure is a bit like having a bouncer
in a nightclub that just keeps people
out it decides that to be safe you can
have this many people inside and they
have to look good or something like that
for dropping data on the floor actually
if you look at a hydroelectric dam
they have a spillway which is usually
something a bit lower in the dam but on
the sides that less overflow water just
go around you don't need it to produce
that much electricity and it keeps the
dam from breaking and killing everybody
in town it's kind of useful
if you like the people in town so you're
in the end it's a business decision and
this is something that's kind of boring
to say in engineering you don't you just
want to work on hard problems and solve
hard problems and be a hero and IRC or
something like that but it's a business
decision and ultimately whether you can
pick on blocking user input we're
dropping data in the floor will depend
on the kind of problems you're trying to
solve and what your customers are
allowed to do people working in
advertising are more often than not able
to drop data on the floor or list and
blocking input whereas people doing
ecommerce websites have more options to
block input than sharing load for
example so for back pressure the most
basic form is simple back pressure and
simple rx4 back pressure is something
that if you work with Ruby Python even
Java and other languages that are mostly
sequential is implicit all the time you
call a function from a given place and
then it just blocks until it gets a
result all the time you have a
restricted number of pools because
otherwise it's too expensive so whatever
kind of input you do the back pressure
is implicit and on these systems
whenever it gets overloaded you will
rarely see the node explode in the dying
fire what you will see is a bunch of
customers telling you the website is
kind of slow what's the problem with the
website my p99 is really bad and that's
going to be because of the back pressure
is going back to the TCP socket and then
they see the back pressure up to their
own application which might be their
browsers and you need to do it all the
way down and languages like Erlang or no
js' and JavaScript you don't have that
you have that kind of systems where
everybody takes a server puts all the
settings to the highest maximum possible
and then you get a billion requests
accepted
caught in the memory header of your node
and then you have the little data base
at the end has to chug through
everything and you just keep piling it
up and up and up and up all the time if
you don't do any kind of back pressure
whatsoever so nodejs there's absolutely
no way to that kind of implicit back
pressure because if you do you just
block the entire server for everyone at
once in Erlang you can do it implicitly
by using functions like gen server call
equivalent to Jennifer Sam or whatever
when you do a call you will look on the
response that you're waiting for from
for the process and if you do it all the
way down you can restrict the number of
colors at the edge of the system so that
may mean you can accept at most 100
concurrent requests and you're gonna
have back pressure implicitly built in
your system that way so that's gonna be
a way to automate it it's gonna slow the
user down and yeah the problem with
slowing the users down is that well it's
kind of like back pressure mechanism if
the back pressure mechanism is implicit
there's no explicit way to explain the
users who are the why they're getting a
back pressure right and there is a fun
issue in our language is the time-out
management so if I have the head of my
at the edge of my system right there and
I put a five seconds delay and then I
have the next step there and I put in
another five seconds deal a maximum and
I have the deep and assistant there
still a five seconds Neely even if that
one takes 4.5 seconds to execute if I
took one second to get from there to
there I'm gonna fail and explode on the
edge because the timeout will be there
even if none of the other time outside
surely failed so when you go on the
airline mailing list and you ask what do
I do about these case how do I pick
timers you have a bunch of people that I
knew well put the timer to infinity
problem solved the problem with that and
I want to call Pat hell and that one is
some applications developers may push
for no timeout and argue that it's okay
to wait indefinitely
I typically propose they set the timeout
to 30 years instead then the developers
kind of react on why through the year 30
years is kind of ridiculous and then I
ask why is 30 years silly but infinity
reasonable and now every time I put a
timeout somewhere I'd say it's 30 years
reasonable and if I say no
usually I want to say well maybe 15
seconds is enough to kill after a while
right
so yeah put it in timeout to infinity
it's kind of a non-decision it's just
not taking a decision you're pushing it
until later and you're basically making
sure that the behavior remains and
defined which is not something you want
to do when you want to plan for overload
if you leave it in the find you're not
planning you're just shoving it under
the rug so yeah put a timeout and
usually you want the timers at the edge
to be bigger bigger at a time or that
the timeouts deep inside the system now
something you may want to do is say well
every request I want to do will take 15
seconds maximum because 30 years was too
long and everything past that can be
said to infinity because the timer idea
is just taking care of the rest
so if you do that you basically have one
big timer interest has back pressure for
no very good reason so what's
interesting is getting into an explicit
form of back pressure which is asking
for permission
for that to be done you need to really
know what your bottleneck is like your
real real assistant bottleneck not just
the one that you put incidentally
because we're all bad programmer asleep
down you have to figure out the part at
the end I decide is it okay if I send
you more data and if it's okay I'm gonna
push more water down the drain if it's
not okay I want the little problem we
can get into with that is that if you
really really put your your sensor or
your probe or your permission asking at
the bottleneck itself you're gonna have
this kind of uninfected the time that's
just gonna be wobbly because the time
that the edge of the system makes it to
the bottom there's a delay there where
you're just gonna accept a crapload of
requests then you're gonna deny them for
a while then you're gonna accept more
just gonna go off on enough enough
enough enough like that there's gonna be
pretty bad so what people do in these
cases usually is that you just set a
buffer and the buffer in that case is
just a pipe you ask a bit higher there
is that okay are you being overflowed at
the limit of that buffer if I need
empties up a bit gets a bit for and
something like that and the buffer will
basically make sure that you get better
utilization of the system so that's a
lot of stuff without any kind of code
there's not gonna be more code than that
that I'm going to point it out to code
if you really want to see it so if you
want to implement that kind of explicit
back pressure mechanism you can use it
with processes and that
stable you use it to ask for permission
and then you need to DD up for your own
system a solution invented it's doable
it might be super flexible but there are
things that exist already so if you're
blocking on memory
there's mem sup in the West Mon that
 with OTP if you're plugging and CPU
CPU sub with OS money no TP this stuff
is actually about total disk space so
this is something you may want to do and
when you do an explicit backplate
backpressure it's possible you have more
than one bottleneck depending on the
circumstances
so it's possible before accepting a
request or deciding to service it that
you asked for more than one of these
whether you can or not there's the
overload module in sasl I don't
particularly like that one personally
because it has a kind of a fancy
calculation about send rate and whatnot
that I'm not the biggest fan of but
might be interesting in the open-source
community we have all vigor who wrote
the job system that was made to control
at the edge of the system that kind of
back pressure mechanism J Lewis yes poor
Lewis understand from erlang solutions
looked at that system tried to test it
finally bunch of bugs and road safety
valve and safety valve that does that
kind of management where you have the
buffer you want the kind of overload you
want you can pick your dropping strategy
and it's going to tell you if it's too
busy or not so if you have to do back
pressure control on your airline system
and you will have to do either back rush
or load shedding safety valve is
something you should really really look
into before you get started or before
you get started when you're sick on a
rewrite of the system because the first
one keeps crashing all the time right
load shedding or in other words I don't
even need these requests where are you
giving them to these load shedding is
something that we don't see nearly as
often as the core business of a system
but we see from time to time in every
system one example for that I think our
metrics some vlog messages if their info
messages if their warning messages you
may want to drop them during overload
but you don't want to throw up error
messages or critical messages for
example so one of the first
implementations you can have for a load
shedding is random dropping and the
random dropping is something you can
implant
as a receiver but it's kind of
inefficient because it means you still
have to keep to get all the data inside
the system before throwing another floor
random drop is really really nice when
you do with producer size and that
happens the best example I have is with
metrics whenever when I was working with
advertisement stuff you would get
something like 10,000 requests a second
per node and each of these requests
might be generating 50 log messages or
50 metrics messages and if you count
that gives you 10,000 times 50 that
makes a shitload of requests just to get
logs and to some extent your system is
going to be more busy handling logs and
handling metrics that it's going to be
busy doing it's really its core business
so random drop is not more complex that
these two functions may be module called
well drop random and random is the
function there it just gives you a given
raid and I want 95% of your requests to
be dropped what do you do with a random
drop that's fairly neat is that if
you're keeping only 5% of the requests
at a volume and this is something Brian
Trautwein treat a bit this morning you
get a large enough sample size that all
your data is still relevant but you
literally only use a fraction of it so
if I'm keeping only 5% of my requests I
can multiply every value in the logs I
have by 20 and get something that's
fairly equivalent to the full size of
everything so random drop is something
that's being used in stat store for a
logging in the metrics there I think
folsom does it with some of its counter
for some of its counters for sampling
and you can correct me I think XO meter
does something like that also yeah yeah
I'm either just something like that also
so all these metric critical systems are
doing it so if you're using one of these
tools you're doing load shedding already
in parts of your systems another
approach is a queue buffer and a queue
buffer is something that you can use and
random drop is something you can use
also in backpressure the difference
between back pressure and load shedding
when you use a buffer like that is that
in the case of back pressure it tells
you we can take more data in the case of
load shedding it drops data for you so
for a queue buffer you get more control
and random drop the messages will came
out in the same order and when the
buffer is full you
have the choice of either dropping the
messages that are the oldest or the
newest and this is usually something
that you have the choice of doing
entirely based on business rules you
have so for example if you're hiding a
card system because card systems are the
cornerstone of the internet if you're
having a card system you probably want
to keep the oldest requests first they
were the first clients in line those are
the one you want to serve those that
can't make it you may shed them on the
ground it's not too bad if you're
dealing with logging or recent or
current events or news at that point it
might be more interesting to drop the
old messages and just keep serving the
new ones and a nice system for a load
shedding will be able to give you the
option of doing both yeah the messages
will be in order
but the problem we have with queue
buffers basically is that if we have one
of these messages that takes long to
process the entire queue gets to slow
down and if all of them are a bit slower
the last message should come and we'll
have all the buffering times of all the
other messages in the queue accumulated
in there so you have tagged buffers and
this is something I discussed in a talk
here in these exact same room two years
ago telling you that I don't do a lot of
new stuff or something where my talks
but yeah
stack buffer is something you do if you
need to have better latency overall so
the difference we're stuck in queue
buffer is that you will always drop the
newest requests and you will always read
from the newest request that's in so
when you have a few outliers that are
very very slow only the requests are
already in the buffer aren't going to be
accumulating time and having a bad
service time and if you're ready to drop
the request that's not a big deal
because you can just clear the entire
buffer and that would not be a big big
relation of service so you keep getting
them first and first all the time that's
better for a low latency but that means
that you cannot have ordering in the
messages that you're receiving if you
need ordering stack buffers are
basically out of the equations because
you're gonna read the message in a
different order than what the common
implementations for that oh yeah blogger
does it on Oh T P errors for cascading
failures so
you're callin message or whatever those
are actually never dropped but for OTP
errors that happen if you crash big
Supervision trees lagger has a mechanism
in place where it accepts a given number
of messages per interval of time and if
it's higher than that we'll just drop
whatever your sees until the internal
takes over and then it will keep
accepting them over and over again so
that's a form of load shedding that
lagger does is the form of low chaining
I've put recently in recon to do tracing
and kick the node when it's tracing too
hard so you can do that kind of stuff
there's a discount application that's
the one I presented roughly two years
ago and at the difference with discount
is that it's like a stack buffer but the
stack has a value of one in there and
what that gives you is basically the
idea that if the request will be dropped
you know it instantly rather than maybe
losing it so it can be used both for
back pressure and both for load shedding
but in practice this more or less used
for load shedding and then there's the
application Bo box that we developed at
Heroku this year basically PU box is a
mailbox for your mailbox and it's
something that we extracted from the log
Blake system we have hat Oroku and
basically you have this process called
appeal bugs that a process starts if you
have a process that's really really
really busy all the time it keeps
receiving messages in its mailbox every
time you do an operation that has to do
with your business rules you won't be
shedding load you'll be accumulating it
and that's what will cause your message
queues to blow up and up and up in size
until eventually the entire system dies
so the way to do that is to add more
processors this is a bit of the
grandma's solution for Erlang when
something fails just add more processors
and wait until it works so P o box gives
you a new process that will only receive
messages it will store that stuff into a
buffer that you have right here and the
buffer can be either a queue buffer to
drop the new s a queue buffer to drop
the old s messages or a stack buffer it
does both of them and if you don't touch
the process it just receives the message
and drops them as they come around all
the time the drops and drops in and
trust them the process that does
important stuff can tell peel boats tell
me
whenever you get a new message and you
really receive one notification tells
you you've got mail you can call the
processes they'd send me to mail and to
send the mail you have a function that
lets you filter and pick whatever is in
the mailbox that's a bit less powerful
and then selectively receive but does a
similar work the peel books can then
send a batch of messages and we'll
report how many we're lost what's
interesting with that lots of messages
that you can use that as a feedback loop
somewhere in your system to say I'm
currently dropping message so apply back
pressure it's possible to take hybrid
approaches if you can afford it and
we'll see baby how you can afford it
when we discuss the API is a bit so the
way to work with peel box is that yeah
you have a filter function and you just
say you're now active and filtered them
so this filter function right there is
called with a state of 25 and what it
does is basically send you 25 messages
that are not empty binaries whenever
there's no message of you it's richer
skip skip basically tells peel box that
I'm done with the current batch of
selection returns all the message to you
and keeps buffering with the rest of the
mailbox intact drop and messengers of
something tells you to drop that message
I don't want it you can just drop it
right away go to the next one but keep
calling the filter function and as the
state doesn't change and though the last
one telling you okay that's a message I
want will let you know basically that
you want to receive the message in your
process decrease the counters for that
one yes will return 25 messages that are
not empty binaries we use peel box like
that in production system at Heroku that
have to do with output and logging so
when you're in the cloud everything
likes to turn to mud from time to time
for no really good reason and whenever
we're doing logging it happened that
yeah from time to time the throughput of
just doing IO format or just standing to
a file or something like that will drop
by as much as 50 percent for periods of
three to six to seven hours for no
reasonable reason probably just noisy
neighbors on the same hardware so we
move all our I Oh format calls and our
lagger logging calls to an application
called batch i/o and batch are you is
basically peel box
with a process that just buffers all the
messages we receive in batches of four
kilobytes which tends to fit very well
into a given buffer window when your
right to disk or something like that and
yeah what it does is that it basically
just has this gigantic buffer of 40,000
messages like I said you don't
necessarily want to set a low limit you
just want to set any limit at all that's
kind of reasonable on one credit system
so 40,000 messages was perfectly fine
for log messages to be backed up it
takes a lot of memory but it doesn't
kill the system so we put in 240
thousand and whenever it's full it just
drops messages until the i/o system or
the noisy neighbor eventually disappears
or get booted of the platform and then
we're able to catch up and we went from
having maybe one crash every two months
due to logging system getting backed up
not counting the degradation of the
system because if you're using i/o
format or a lag or message or something
you're getting overflowed it becomes
blocking and all your operations
including logging become slower so
basically when these other replications
will slow down the i/o system our
logging that was there to debug stuff
was actually causing bugs and bad
performance of the system that made it
go entirely as synchronously and I
haven't checked in months because it
works fine but after like 2 or 3 months
over 45 to 100 nodes rolling over over
time I think only two of them needed to
drop messages and had filled a buffer or
two up to 40,000 the rest of them never
used the entire buffer space but we just
raised throughput and made sure that we
can never ever again crash due to
logging taking over and it's no longer
ever going to be blocking us whatever
operation we're doing just because we
saw that there was another load in there
and took measures to correct it there is
a very very important question in all of
that is how do you tell users about back
pressure or load shedding because if you
don't tell them they just think that
your system is kind of all wonky and for
no reason and they hate you and then you
get support calls at 3:00 in the morning
and it's urgent and you have to take it
it's kind of a sad story so yeah you can
block on sessions for backpressure if
you ever go and read it this is what
they tend to do when they're getting
overloaded they allowed you out
they put this
I read only and you basically have a
degraded experience but still an
experience that can be had on the
website in some cases when you have an
API or service you can keep new people
from starting a new session or just
throw out all the rate at which you
accept session which forces the people
to wait a bit and just slows down the
operations of the system a bit blocking
sessions is not the most optimal thing
it's generally better to put usage
limits however high and these usage
limits are both valid for loadshedding
or for backpressure more often than not
though we see them for back pressure and
usage limits can be absolute for the
node which means I'm able to handle a
thousand concurrent requests after which
I'm blocking or they can be set by user
and by user is usually much much nicer
because if you have an important
customer you can give them your limit
that's twice the size of the usual
customer and they're gonna be happy
about it and then you just found a way
to make more money because your city
you're selling an upgraded service for
some people everybody's gonna love you
and your company the the danger of
limits being set by users is that if we
go back to that margin of error we had
in the balloon that's about to pop the
more users you have the absolute limit
you put on your system is also growing
it's a bit like ISPs that love to sell
more bandwidth that they're able to get
on the system because they know that
most people won't be bad citizens and
then torrents come out and Netflix comes
out and then they're really really
depressed because all of a sudden
they're no longer able to deal with it
so yeah you have to be careful when
you're sitted by users so that the
margin of error you defined as the safe
space to which your application can
survive doesn't end up shifting past the
actual safe point at which point is as
if you had absolutely no overload
planning at all it just took time to
implement it and that's similar to
having a bad premature optimization
right so yeah per user limits are fairly
nice but you have to be very careful and
consider them from time to time either
you need to add capacity or you need to
progressively reduce the limits on all
users which tends to piss them off when
you're doing load shedding telling
people about lost messages is actually
fairly efficient people were not
when you lose data if it's not vital
data or their credit data or your baby
is born congratulations then you lose
the message they're going to be fine
with a few log message disappearing for
example and if you tell them well there
was an overload probably due to the fact
that you're consuming too slowly you're
producing too much people are usually
fine with that explanation and they're
gonna debug it and say oh yeah that's
kind of slow I found a bottleneck there
they're gonna make their application
faster and they're not gonna call you so
log Plex has l10 messages that tell you
the drain for the logged messages you
have right now is not fast enough and we
had to drop that many messages and we
let them know how many we dropped if
you're using different part of the
system that can be the same another
important point is to respect the
end-to-end principles and this is more
of a general tip than something about
overload planning the end-to-end
principles is something from the late
70s to the early 80s that led to the TCP
protocol more or less and we do with
implicitly in our line when you talk
from a process you're another one when
you send a message you don't get an OK
telling you that yeah the function call
whatever operation you wanted to do
succeeded you need to do with manually
and send a message back that you will
receive that tells you how actually did
the work you wanted me to do it's not
something like I received a message you
wanted me to receive and then I threw it
on the ground but you have no way to
know it's really about I did the job you
wanted me to do here's the result an
end-to-end principle like that has to be
respected in the api's especially when
you have more than one item in the
dependencies change something that will
happen in Erlang is you're not careful
is that you're gonna send a message to
one process that's gonna tell you ok I
handled it but then the actual work
actually depends on another process
depends on that other one and if you
don't respect that final call back at
the end it tells you yes the absolute
final test you wanted to do succeeded
you can't have screwed and what happens
is that if you don't respect that and
you shed load there's no way for the
user to know whether the operation
worked or not so they don't know if they
can retry it or not they don't know if
retrying it will actually break
something so respecting the interim
principle is something you have to care
about and make idempotent a api's and
that's the point about api's I wanted to
make earlier so when you're shedding
load or you're denying
well when you're denying a request due
to back pressure usually it's not a big
deal what will happen is that you will
have to consider having an error message
it tells you you're currently breaking
the API rate limit we gave you which is
part of your API and you have to think
about it because something that's
extremely sad is that if you put no
limit on the API and then suddenly one
day you put a limit on the API all your
customers are going to be fairly
ingredient thing that you're screwing
them over so ideally you start with a
limit right away because it's much much
easier to give more leeway to people
then remove it from them without making
them angry in terms of in them protests
if you're using a low chaining as a
mechanism and you're dropping requests
on the ground that's perfectly fine but
the client has to be able to make the
request again and make sure that maybe
it will be reapplied maybe will fail
again and being a damn potent basically
means that if I made the request one
time it may or may not work if I make it
two times the result will be the same as
if I made it one time finally basically
it's a guarantee that one or more time
having the call being made won't make a
difference with just having been being
made once and if you're dealing with a
card system again that means doing
something like every operation you do
for buying some things or for
reimbursing something has a transaction
ID for example and that transaction ID
is something that you put in the request
and that lets someone try to delete the
same attempt at the same item 15 times
if they want and if it's been registered
one to say that transaction ID is
already taken and actually your
operation was successful and that's fine
and if you don't think about that and
making that part of your API it's gonna
be very very hard to retrofit that kind
of stuff back in there but that's
actually essential to making something
usable once you plan for overload and if
you don't plan for overload you get
trashes all the time and you have to do
up during the night questions
all right
currently what we do at least in the
logging system that I manage is that
everyone is equal in these systems if
you're generating more data you actually
have to just drain it fast enough what
we found is that most of the time our
system is actually fast enough to
accommodate the biggest loads in the
system for example in the logging system
it's easily doable to do more than
50,000 messages a second without a
problem per user and it's rare that
someone will do more than that except
when maybe Ruby on Rails is crashing
repeatedly and that's like a DDoS attack
usually people are fine with not
receiving all the messages in that case
so in practice we haven't had that need
but something we get and we gave is that
we have a support team and we gave them
a tool which is basically a devil train
so when people send messages on the
Heroku nlog Plex you get a drain that
tells you that lets you reroute it to a
given end point and what we have is that
kind of death knell endpoint that just
spins through the messages as fast as
possible but still reports losses so
that lets customers know that giving an
optimal drain this is what would happen
and usually that's that lets us know
that they're possibly consuming data too
slow and that's what causing the system
to back up and forcing us to shed load
and when that doesn't happen it happens
once and was internal with a team then
we need to go around and optimize a bit
but so far using that system that lets
our support team tell the customer
actually the problem is on your end we
might optimize it won't solve anything
it will just give the bigger buffer that
eventually will need to shed load anyway
so depending on the system and really
the way I see to do it is that there's
no good way if there's an obvious wave
and I don't need to tell you and if it's
not an obvious way then I'd probably
need to look at your application to give
a general tip it's not something it's
either obvious or not at all which is
kind of shitty but that's how it is the
other question yeah
I don't know all the details because
that happened before I was born but
basically with TCP there was a lot a
bunch of customer let's say in-house
network protocols and whatever that were
developed in the 60s and 70s and whatnot
and there was a paper about arguments
about system arguments about end to the
end to an argument about system design
or something like that that was a super
interesting paper that basically just
looked at things like whenever you're
writing something to transfer a file
from a computer to a different one it is
of nearly no use to just have every node
halfway through the point tell you that
the transfer went right it is of use to
know if it worked in the end with a
given hash or a checksum that tells you
yeah it worked or not
everything else in between is
superfluous or an optimization basically
because if you don't have the end-to-end
check you have no proof that let's say
pass the network and unto the disk
something didn't go wrong and if
something went wrong after it left the
network and went onto the disk you don't
know about it
so the interim principle is really about
you don't know that it worked until the
job is finally done you got the final
message about that and TCP is kind of
like that because you have the
acknowledgement and the sequential
sending and everything that's done based
on the end-to-end system you won't get
to be able on your note to send more
messages at a time that different things
depending on the settings you have but
yeah basically you don't have the UDP
stuff where messages are received out of
order or some of them are missing you
have a system in place it lets you
confirm that at the end of the protocol
everything is right but yeah I cannot go
into more details than that because I'm
not knowledgeable enough about it yep
okay so Kenji asked for people or
watching it recording if I have examples
about that kind of stuff being
implemented in Erlang there is one
example of back pressure that's
implemented in Erlang and if you watched
Lucas Larson's presentation from relying
solutions about how the schedulers and
airline work you will see that whenever
you're sending something to TCP buffer
or rather a port process whatever you're
sending something to report and that the
port buffer gets full the Erlang VM will
desk a jewel your process until the
buffer has free space again and if you
don't do that the process just won't run
again so that's back pressure apply to
your process directly to force it not to
generate data that won't make to the
network in some cases that may cause
problems because you don't want to get
that back pressure because you're
yourself victim to it and I you have to
deal with that and that's something that
we did with peel bugs at some point but
yeah that's one example of something
that's like that no TP I think that the
telecom industry in general has been
aware of these principles for a very
very long time and that explains why the
memsaab cpu sub and all these
applications are already in the note I'm
guessing that a lot of systems that the
teams that Ericsson develop have these
kind of things in mind and if you're
writing any kind of system it's really
system dependent you will get a limited
amount of resources maybe you're going
to be cash registers that you know
exists in hardware stores or in
brick-and-mortar stores in the world and
you can put limits on them but outside
of that directly in OTP I don't know how
many of them exists I think more or less
it's about the implicit control flow
that you have which answer recalling
that kind of stuff yep
yeah
does who descender know about that now
the sender doesn't really know about
whether you're dropping the oldest or
the newest one when your load shedding
the sender that the sender doesn't know
when it was dropped may just know that
it was dropped the P the person who
knows is the user the end-user to
receive the data and you may tell them I
dropped that many messages because I
could not handle them but usually it's
none of the business of the sender to
know whether was drop earlier or later
than something that's really something
that the consumer cares about not the
producer most of the time all right well
thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>