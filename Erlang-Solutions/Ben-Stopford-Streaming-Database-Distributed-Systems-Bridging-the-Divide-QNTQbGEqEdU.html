<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Ben Stopford - Streaming, Database &amp; Distributed Systems: Bridging the Divide | Coder Coacher - Coaching Coders</title><meta content="Ben Stopford - Streaming, Database &amp; Distributed Systems: Bridging the Divide - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Ben Stopford - Streaming, Database &amp; Distributed Systems: Bridging the Divide</b></h2><h5 class="post__date">2016-11-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/QNTQbGEqEdU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everybody thank you all for making
it in at this early hour obviously
morning people so I appreciate the
determination to get here today we're
going to talk about streaming database
and distributed systems bridging this
divide so this is really a talk about
stream processing and looking at it
really from that from where stream
processing has got to today specifically
this sub branch of stream processing
which is known as stateful stream
processing and then we're going to have
a look at how how that that can be
applied to kind of more general problems
rather than just streaming ones so yes
there's me I'm an engineer at confluent
which is the company behind patch kefka
and and i work on as you work on the the
team that builds the distributed log so
most stateful systems have to pull
something from one of these three worlds
if you're in the business of building
stateless systems then you are very very
lucky because stateless systems are nice
easy things to build in many ways well
they're quite a lot of them end up being
stateless by really just pushing the
state to somebody else's concern but
yeah if you do that then you probably
not going to find that this very
interesting we'll be talking pretty much
exclusively about the concept of
Stateville systems and which tend to
have to pull these various different
elements together so concept of a
database the concept of spreading stage
across a bunch of different machines and
the communication that results from that
and then this idea of having event
driven systems we don't always have to
do polling so today we have two goals we
want to understand the staple stream
processing soft now and in the near
future and then we want to make a case
I'd like to make try make a case for SSP
being a more general framework for
building data centric systems so
data systems come in a bunch of
different forms everyone's kind of
probably familiar with the idea of a
database i'm sure everyone uses one if
we look at a LTP data database so that
could be this could be postgres it could
be Mongo to certain extent but this idea
the idea of a database is really
focusing on a consistent view so
databases generally give you some
concept of global consistency but then
the actual underlying the internals are
really designed to support updates and
queries across individual two tuples so
what that means is they're focusing on
selectivity the ability to take a large
data set and narrow it down to something
that this relatively small so the tool
that tends to be applied here is indexes
that's kind of what differentiates them
we look at the analytical database this
could be something relational like red
shift or it could be something like
spark that's something that focuses
effectively on aggregations and it's
going to do that via really via table
scans so you're typically going to that
you're typically optimizing for getting
a large amount of data through a query
engine so there's a couple of kind of
broad categories of databases we can
also think about messaging as being a
data system and Jim Gray who's a
academic in the who's head of jim gray
yeah we got a few he once read a paper
saying that all messaging systems
essentially end up being databases and
this is because the contract of a
messaging system actually ends up having
to look a lot like a database actually
really like an OLTP database because
they have to provide this concept of
selectivity so if you have primitives
like message selectors or routing Keys
named QP you actually look at your
actually introspecting a stream of
messages in a very similar way to the
way our database query works but they
tend to focus on limited state so
tend to be slightly like relatively
ephemeral um distributed log is a
different type of system so kathcar
which i work on is an example of a
distributed log conceptually it's very
similar to it to a year a messaging
system so you have this idea obviously
the state streaming through it with
immutable records but it builds in a
couple of other concepts so one is that
it the underlying structure means that
you can retain data long-term in a way
that's much harder to do in a
traditional messaging system actually
that parallel is very similar to the
difference between an OLTP database and
an analytics database secondly you can
it also execute as a distributed system
so by being a distributed system we have
both the ability to scale out linearly
as well as add in concepts of fault
tolerance so stream processing is this
idea that we can manipulate concurrent
streams of events so it comes from a
back background in complex about event
processing which has a pretty long
history dating about 20 years or so and
in its core it tends to be sort of
ephemeral so this idea that we have some
machine and we have a set of streams and
we're going to perform some computation
inside this engine across these various
different streams and I'll put something
else so stateful stream processing which
is this particular branch of stream
processing actually blends together a
number of these concepts in one kind of
product as it were so search it's a kind
of slightly more general framework for
building these kind of systems so what
exactly is stream processing we can
compare it back to a database right so
if we think of like a typical olt btp
database will have these constructs that
have data and data some data in a file
and an index in a file when net and
those will be
reversed by a query engine stream
processing is a query engine much like
the query engine here but it's something
that's designed to operate over an
infinite stream so that on the database
size it's a finite data set over here
it's designed for an infinite data set
and the thing about dealing with an
infinite dataset is you have to bound
that in some way so that's why you bring
in this concept of windows so windows is
very simple idea it was like we have
these streams and we're bringing them
into this machine which is going to do
our computation and all the window is
really saying is how many of these boxes
am I allow going to allow into the
machine at the point of time that I'm
going to do the computation so it's
effectively really just a kind of
buffering so we have to put a window
around around our infinite streams to
bound our computation and this buffering
allows us to basically allows is do a
few things balance of computation but it
also allows us to handle late events so
where we have street infinite streams
are typical in any particular problem
that you apply this to this data may or
may not turn up when you want it to turn
up so if you're correlating three
different streams you may you know one
story might get held up that system
might be down or you know is from a
mobile phone and the guys gone into a
tunnel or whatever it might be so you
have to have this concept of dealing
with late events and that again it's
done effectively by buffering so
different stream processors have
different sort of ways of describing
this but the concepts pretty much
general so what we only ending up with
in stream pressing is a continuously
executing query of two streams of data
coming in some query which is going to
execute over some temporal window and
emit results at some frequency that's
kind of a general the general approach
and what comes out is a derive stream of
something else so if we put something a
little bit more concrete onto that we
might have a stream of orders and a
stream of payments and let's say we're a
where some web company that has payments
which are paid which are settled in
different regions and we're interested
in how quickly those payments are
settled across these different regions
so we take this average completion time
across orders and payments with a group
by missing over over a one-day window
and missing every second so we get this
concept of completion completion time by
region as a result so that's kind of a
basic basics of stream processing now
there's another concept there's a little
bit like this that's a materialized view
in a database and when use materialized
views good this is going to be much
easier so the materialized views kind of
interesting because materialized view
takes two tables orders and payments and
takes this query and then mana
physically manifest that as a separate
table on disk right so we have a table
now which represents the completion time
by region and actually that this will be
this query will be executed every time
these tables change trigger and
materialized this this this this
materialized view so why do we do this
anyone have a go why do you buy to use
materialized views for performance
exactly so we're able to take to do this
computation effectively real time so
that instead of every time a user does a
query we don't have to repeat this
computation it's already prepared for us
inside this materialized view so
stateful stream processing as actually
the process of creating materialized
views it's essentially what it is so we
can have it's a little bit different
because it blends these concepts of
state table and string so we can have
effectively a stream set of streams and
a table so that the table is effectively
it's actually effectively a window and a
stream with an infinite window and we
can pull these in we can perform this
this query we can do and aggregates we
can do a join between these things and
then we can choose to materialize that
materialize that view either as another
string which somebody else can listen to
or as a tabular form which a user can
query so you can see that in this with
this this sort of stateful construct we
have this ability to blend streams and
tables both as inputs to the computation
as well as the output that we the output
pattern that we want do we want
something that's going to take to
continue to be streaming or do we want
something which is effectively an event
store we can query so the way to think
about one way to think about this is to
think of a table as being a stream with
an infinite window in this case it's a
cut it'll be a constantly updating table
so s staple stream processing is
effectively about creating materialized
views materializing them either as a
table or is another stream so in its
core though very similar to a database
query engine right we can do have the
idea of streams we have join join
operators view so you can translate
things during internal data model domain
model predicate so we can do filtering
aggregate functions etc and then
typically you can run these things over
multiple machine
that's unsurprising but you can actually
do that in two dimensions so you
actually scale effectively in two
dimensions you can scale out
horizontally by effectively spreading
this is this is this is like sharding so
some subset of your data will be dealt
with by this processor another subset
would be dealt with by this processor
excuse me which allows you to scale out
horizontally but you can also add you
can also scale forward by adding
different levels to your chain so you
can actually end up with a kind of graph
of processing which spans many machines
so a staple troops is that when you were
a stream processing can use it can use a
brokered technology I'm actually going
to just jump to talking about brokered
technologies you don't have to use a
broker technology and many stream
processing engines don't they use
implement things like back pressure to
deal with some of the problems of
overloading downstream nodes but staple
stream processing engines pretty much
always user distributed log because they
need the state fulness which they can
hear it from that distributed block so
we need to kind of introduce this idea
of just a distributed log and categories
is an example of the distributed log so
this is a really really simple idea like
conceptually it's almost so simple that
it's hardly worth talking about but you
can literally just think of it as a file
right it's just like a big distributed
file when you add messages you add them
to the end of the file so new messages
come on the end here when you want to
read a message you have an idea of an
offset within that conceptually
distributed file and you jump to the
offset that you were last reading at and
you scan so this has said this has
actually some pretty nice primitives in
that it forces you to do sequential
access effectively as you probably know
that the
sequential access is about two orders of
at least two orders of magnitude faster
than addressing things randomly jumping
around and address space or disk there's
actually a matter of whether or not it's
a memory or on disk the performance in
differences is equivalent and so by
pushing you down this this root of just
doing sequential access you end up with
very very efficient operations so
actually Catholic is really just about
reframing a problem such that it is only
ever solved with sequential access
actually a lot of I mean that's log
structured merge trees or another
implementation of the same kind of
pattern so the nice thing about this is
you can you can really you decide to go
from whether you start go from the start
of the log or the end of the log doesn't
really matter it's effectively over 14
read caching aside so we have to
introduce these other concepts it's one
more concept and that's this idea of a
compacted log and this is the messaging
systems way of representing a table
effectively so if we have let's say we
have a stream of orders and you have you
create an order you might update the
quantity of that order or you might
offer update your delivery address or
something you end up with three separate
versions of the same order ID if you
represent those in a stream you would
have all versions right because it's
immutable messages but you can have this
so that would be a topic in CAF go and
you can also have this compacted log and
that's much like a database table I'll
provide you with only key it will only
keep the latest version for any key so
all of them or all of the versions that
have been updated and replaced or
effectively just thrown away so the
latest key only so we have this concept
of a compacted log and hence something
which looks a bit like a table
so beyond that the log is actually a
distributed system inherently a
distributional is unsurprisingly a
distributed system and that means that
effectively we can have a bunch of
different system a bunch of different
processes writing to it we can actually
have the distributed log itself spread
over a number of machines with
individual partitions much like a
distributed key value store and then
actually this bits kind of cool I'm I'm
not really going to go into enormous
detail but you can actually then spread
a single topic this is really important
from the perspective of data systems you
can take a single topic and you can
spread the partitions which hold the
data across a bunch of different
machines that doesn't sound that's
exciting until you actually think about
what you need to do to do that you
actually need to be able to have the
concept of leadership and leader
election inside these reading processes
and and the way that actually works is
well the reason that you want to do that
is so that you can survive failure so
these would effectively be these could
just be standard to consumers reading
messages or the cupola stream processing
engine so if we get failure here we need
to put a reroute data to another machine
so what this what this ends up meaning
is that there are actually two concepts
of leader election this is two cons
separate concepts of a leader one which
actually exists is private to the
distributed log that's how it deals
internally with with failure and
recovery and then there's another layer
there's another layer of clustering
another layer of leader election which
is inherited by anything which pulls
data out of Kafka so actually these
processes will have if you have multiple
consumers just regular consumers in a
group
you will have one of them will be a
leader and that's a protocol which is
inherited from from the distributed log
itself if it fails it will rebalance
stable rebalance somebody else will
become the leader and the kid and a
consumer actually has the ability to
affect that the result of what happens
next it actually has the ability just
the leader can spread the data anywhere
he wants the data to go so the log is
important for stateful stream processing
because it maintains this history push
it acts a bit like a distributed file
system and maintains history in a kind
of push based way it has these
primitives for infinite immutable stream
and a table so ends up being a little
bit like a streaming engine it is very
much to a streaming engine or HDFS is to
Hadoop it's a little bit more than that
because what I mean hcf is just a file
system whereas here we're actually
taking this concept of membership and
inheriting it from the log so these
processes can be actually really simple
and independent and they're actually
Harriton that concept of net of
membership and what is effectively work
distribution from the lock without
needing some external you know external
work manager process so stay proceed
processing is a bit different because it
uses stateful it's based on a stateful
look but there's a little bit more to it
than that staple stream processing
engines also use a local key value store
so they actually have a database inside
each of the stream processing engines
and this is actually this turns out to
be quite important this allows it to do
a couple of different things
firstly it allows it to cash a stream on
we can cash a stream on disk so this
means effectively you can have very
large retention windows and that means
that you can avoid the uncertainty of
not of that can come with late events
secondly you can actually cash cables
remember a table is just it's just a
stream with an infinite window so you
can actually but it's actually separate
concept so you can take a table which
you know if you ask a question of a
table you will always get a definitive
answer no matter how you know how how
old that table it's because you're
retaining entire data set so it allows
you to take this this concept of tables
and cash them in the process with the
stream processing engine so there's a
representation in the in the log and
then there's a cache of that inside each
engine in the case of a table it's
actually cashed in its entirety so this
means you can do join operations between
stream and a table very efficiently
within a single process so here we have
an example of an infinite stream coming
from a distributed log through a stream
processing engine and we might do a very
simple joint operation between that and
a table so that might be for example a
stream of orders and this could be a
table updating table that contains
customer information so much this would
be a very simple enrichment acres
exercise where we're adding customers on
to our orders now that's still not quite
enough because if we're doing an
aggregate compensation very simple some
for example um we actually need some
persistent state local to the engine
because this is actually a stateful
computation if we fail in the middle of
a window then if if we didn't have some
some
think that was tracking where we got two
in that computation then when we
restarted we might get a different
answer so you need the ability to
basically persist your current value and
that again is used uses the key value
store that's local to the engine but as
soon as you make that key value store
read/write right you've got a problem
what happens if it fails so the solution
to that is that the the key value store
is actually backed up to the distributed
look if you end up with this process
where you have effectively beta and
distributed log tables and streams
cashed in this layer and then the
ability to write intermediate state into
what's called a state store which will
then flush that data back to the log so
that it it inherits all of the
resiliency properties of the distributed
look so this allows us to do a few
things which kind of interesting because
it allows us to basically separate the
concept of data processing and view
remember that view can either be another
stream or it could be a query which we
could be something that's stateful that
where we can execute a query and get the
latest version so if that might be I'd
in our dashboard or something like that
so we can query these views from
anywhere or we can represent them as a
stream so what happens on failure we've
talked about this idea of leadership
because each engine is inheriting this
concept of leadership from the log is a
the log will effectively cause this data
to rebalance to another engine but they
even that can cause problems because if
this goes down let's say this is
actually ends up cashing a terabyte of
data it could take quite a long time for
a terabyte of data to rip to rear epic
eight to another machine and we probably
don't really want that so one of the
nice things you can do is actually this
just happens by default is that the data
will be pre
over here so every time you make a steep
will change every time you make a
stateful change inside here you'll
effectively get replicated to another
sort of hot processor such that if when
a failure records you can literally just
switch membership over wait for it to
catch up and then another processor can
take over so the result of this is that
we get a couple of primitives for
basically sharding and replication so it
shining replication is could have two
primitives that you typically need to
build a distributed database they're
kind of most distributed databases have
primitives for that in some form so we
have this idea processes and they hair
it inherit a lot from the log so we can
just basically write something a stream
processing engine which can then run as
an independent unit and we can inherit
this idea of clustering and this brings
us to this idea of slightly more general
framework for doing distributed
real-time computation now unfortunately
there's a problem the problem is
correctness and if you've ever who's
built distributed systems with messaging
don't know who actually company who does
stream processing Anastos if you do yeah
so if you built your put your hands up
pretty much everyone's built something
with messaging so when you when you use
a messaging system yeah you're so you
you always have this this opportunity to
create to duplicate data and that's
basically because of it comes down to
the two general generals problem
essentially so if you get if you have
let's say we have a graph like this and
we lost this particular node we can fail
that data across to another machine
but we don't have any way of
guaranteeing we can guarantee at least
once delivery you can guarantee that
data will definitely make it across but
we can't guarantee that it'll only make
it once so that's when typically you'll
implement this idea of idempotent so
you'll actually with the item POTUS is
just d dupid deduplication so we have
some deduplication process going on
inside here it actually turns out that
deduplication isn't enough idempotent
assistance isn't a general solution to
this problem the general solution is to
use transactions anyone ever anyone ever
used XA 0 got one excellent how was it
that's exactly what though the premature
answer expected so XA is like a back in
the days of messaging you could
basically change to get the
transactional resources like a
transactional messaging system and
transactional database using an
independent transactional manager and
it's sort of in the days of ejb when
strange things happened so you can do
this in a slightly different way and
using this concept of distributive
snapshots and there's a few people who
have done this and there are a few
flavors of doing it pretty much all of
them are based on this this paper by
chanderi and Lamport about distributed
snapshots I'm not going to go into this
into too much detail because it's
reasonably complicated we've had a guy
working on it for a year but actually
the broad concepts pretty simple so that
the problem is you need to actually be
able to transactionally ensure that data
is not duplicated but you also need to
be able to do that across multiple
streams and effectively that the core
concepts pretty simple you have markers
much like you would in a database
transaction to have these transactional
markers and you have a graph of
computation and you send you send the
markers down the various different
output streams maintaining very clearly
the off
sets so the nice thing about it about
obviously processing engine is
everything has an offset so you can kind
of follow each stream according to its
offset and then save a snapshot of the
input that you've got to but then you
actually need to omit that across
multiple different streams and this sort
of transactional way so there's a bit
which I emit from here which is there is
actually a transactional coordinator
element that's really about the
allocation of ID's if you actually have
your own ideas you don't need to do that
but then effectively what happens is
then each each downstream processor has
a buffer and the buffer simply looks at
these transactional markers and has
rules to effectively obviously abort or
commit so basically only commit or
proceed with the computation once it's
it's a chit once has received the commit
marker so it's really just this sort of
contactless concept of buffering and
these various snapshot markers moving
through a topology over time okay so
that was kind of like a load of stuff
about staple stream processing and yeah
so take a little pause at that point and
ask the question why did you just like
why did why did you just sit through all
this stuff was there any point to it
hopefully so there's a few things that I
think we can we can take from
understanding or trying to understand
this concept of staple stream processing
so one is that we can we can posit that
streaming is actually a superset of
batch so by batch processing I mean I
mean historically it guesses it was done
in relational databases and then made
big by Hadoop I guess but so we can we
can make an argument says that streaming
is a superset of batch because streaming
obviously provides the ability to give
you real-time computation but it can
also do historic computation so
databases are very much oriented around
the idea of looking historically looking
back in time this is actually pretty
much just
I'm active the way they evolved right
databases rolled around files so they
are inherently retrospective but if we
take something like an MPP batch engine
so you can think Hadoop with redshift or
whatever your flavor of choices and
something like an MPP streaming system
that's over a distributed log batch is
just streaming where you set the offset
20 you just start from the start of the
log every time and actually these this
is doing a tablescape mo this is
basically table scanning across many
machines that's how an MPP batch system
works whereas a distributed load it's
the same kind of concept but it builds
in this concept of being real time so
dreamy engine is really a superset of
badge it's not really strict it's not a
superset of a database because databases
provide this concept of a global linear
linearize above you linearize herbal
concurrency model which you don't get
inside a stream processing engine yeah
even with chanderi Lamport so that's
kind of one concept we have this idea
that maybe there's a different way of
thinking about data right we can think
about it in terms of of TV streams
streaming as being a superset of batch
as another thing we can take them this
which is this idea of separating the
store and the view ok so this engine
part is pretty lightweight but it's
stateful all right so it doesn't have
any responsibilities for storage in
itself but it has this concept of state
and then there's a couple of things we
can take from that firstly we have this
separation of Kirk concerns so it's
familiar with MVC as a pattern yeah
everyone I would hope so so you have the
same kind of concepts where we
effectively have this Model View and
controller and they're all effectively
separate and there's a physical
separation here as well between those
two layers
we can also think of this Asik urs who's
familiar with that one yep great i love
this crowd so i mean this this is an
implementation of c QR s right so you're
right you right to the model your read
store with its index is stored on a
different machine and constantly updated
because really as ever we are creating
this idea of materialized view streaming
is all about credit materialized views
so it is really an implementation of c
QR s so if we go back to that original
idea of a database being a query to
accrue engine with data and an index we
now have something that's kind of
similar where we have data a query
engine which can be distributed and then
this idea of a localized view which is
effectively an index on a query but we
can manifest that view obviously as
something we can query in real time or
something which we can distribute to
another another layer of processing in a
kind of event driven architecture it's a
third thing that we can take from this
which is that decentralized approaches
and our little bit more general so
rather than pushing your processing
inside an appliance so remember like
it's funny over time this idea of
pushing code to data or data to code
it's kind of like varied in terms of its
kind of trendiness remember it like in
the 90s I work with a guy who got very
very excited about the fact that you
could put store procedures in databases
because they're really close to the data
and then that stopped being something
that people did bit later and then I
hear mid-2000s the data grids which was
about pushing code to the data and then
who do which is again so this idea of
pushing data into appliance it's kind of
attractive in some ways but there's
actually some a distinct advantage to
going the other way so this idea of
decentralizing your processing
pulling the data out into a set of
distant different views so this is what
Martin clapman would call turning the
database inside out anyone see Martin's
talk on turning the databases that out
yeah it's a great talk so this idea of
taking taking a distributed log and
decentralising the processing into a set
of independent views so you can only do
this if you can actually do fast data
transport between machines and have the
tools for caching these these various
different views in an efficient way
handling failure etc but this is
actually a little bit more empowering
because it's not focusing everything
into a single appliance and we this
isn't just about analytics right we can
actually apply this to more general use
cases so what we're really thinking here
is that it's more than just a database
where we've added some push
notifications whether you're whether
you're looking at is a sort of hulking
great multistage streaming analytics
platform where you have a series of
interviews and results tour you'd run
queries etc or at the other end of the
spectrum is just a simple micro service
application which wants to be able to
blend these ideas of local state shared
state and have the idea of hot instances
just stay hot stateful instances we have
this kind of whole range of different
use cases that you can apply this to and
really at its core is about these
composing these primitives so query
engine primitives for replication and
partitioning and then distributed
systems concepts membership and work
distribution they put it together and we
have this kind of potentially general
framework for a venture of and data
competition um so you can go forth and
build a streaming database that's like
just for you
that's kind of the idea so if any of
those ideas resonated with you I'm
really just repeating various ideas
which other people have come up with
there's a bunch of information out I put
this up on it on the Internet there's a
soap at Helens Martin clapman there's a
bunch of these are a bunch of use cases
and if you want to see how these things
actually work in reality some of this is
a bit some of this is I said there's
some of this is a bit more forward
thinking and some of this is stuff
people do now I said some of these
things are just evolving as we speak um
but yeah that's it for me anyone got any
questions
hi I had one question about the windows
in the initial example you had you had
to time scales that you basically it
coughed out data every second and it was
looking back over a day yeah I kind of I
toyed with asking the time but I thought
maybe you'll come to it later are those
things kind of predefined like you know
before you kick off the stream processor
it you've defined those or can can you
have a computation that says oh I might
kind of look over a longer window or a
short window depending on what's going
on or kind of I might cough out data
when something's triggered or is it you
know basically that kind of there's an
optimization there that says actually
pick pick your windows first and then
we'll work within them so but I think it
depends on so the answer is that it
depends a bit on the implementation I
can only really talked about like our
invitation our implementation is
actually relatively simply simple in
this regard and it's a good question i
mean it's the the and this the whole
space is kind of evolving so at the
moment you're that's that parlance is
actually really the kind of beam style
parlance and we we don't have k screams
doesn't currently have a concept of
triggers so you're a little bit limited
more limited in terms of your control of
when things are emitted but that's
changing so it's just I mean the
background here is so k streams is AK is
a stateful stream processing engine
that's part of kafka it comes in the
distribution it's just really a client
of kafka there are others like Sam's o
which are a bit older and a bit more bit
more mature but yeah it's definitely
something which is evolving so yeah that
the description here is general was
relatively general and actually I
totally simplified I usually merge the
concept of windows and retention periods
because well actually there's a join
joins also have a concept of windowing
and it I didn't really want to get into
it but they're kind of the same kind i
should be the same sort of but it gets
bit complicated getronics plane or three
um yeah any other questions that adds
that sort of once
we get crappy the end into anarchy do
you have any advice on maintaining like
a complexity in a streaming system where
different end consumers have different
demands which results in like different
places in your architecture where you
merge multiple streams into a separate
stream and then like how do you maintain
oversight and how do you yeah do you
have any ideas on that like what are
good our software engineering practices
for maintaining a streaming system ah
yeah so I mean probably the best advice
it's going to be really obvious one
which is try and keep it simple that's
actually probably the one that lets gets
doesn't necessarily get followed that
well um like there is significant
complexity in this there is the ability
you know I'm not going to do that saying
about power and stuff but obviously you
have a lot of rope here which you could
really hang yourself with if you wanted
to that's kind of the nice thing about
having a sort of appliance for you just
ask you to do a job but it does it in it
returns is that you have really not that
many options that you have as much
lateral room to maneuver so yeah there's
there's a lot of there's a lot of
opportunity to kind of get this wrong so
I think that the cool thing is really
just it is to try and keep these things
simple don't for example if you can
don't use a multi-stage topology which
you can do in a lot of a lot of use
cases don't require a multi-stage
topology that's actually a lot simpler
you can insulate case dreams you don't
have to so you have the option of
basically chaining together different
operations and you can choose whether or
not those go out of the process space or
not so I said they hear back into Kafka
or not and it's generally simpler if you
can just create sort of bound your
problems so that it is either one note
or just one set of nodes that scale
horizontally that kind of keeps things
simple and other than that
actually the reality there's a reality
which is that any database when you
build something thats a good database
you actually got all the problems that
then will result from having a database
so anything that you can imagine a DBA
doing you will end up having to do in
this system in this system you build
that's actually the reality of it so
things like you will end up worrying
about schemers and the schema migration
and what happens a date it gets old and
how do I get a snapshot of data into my
development environment so I can get a
review producible result and it's all
those kind of things so yeah these
things are definitely actually the
sympathy to the best thing to just to
keep it simple where the best advice
I've got ah yes hey and what tools are
there available for managing and
optimizing a kind of streaming framework
is there anything available really for
actually just managing this distributed
system for you and coming up with say an
optimal setup not that I know of no
certainly isn't anything for k streams
might be might be something in well yeah
maybe for storm no it's a good idea that
maybe should build one I think that
sounds like a very useful tool at the
moment this stuff's pretty raw it's the
truth what real world uses is the input
to you mean hinted it analytics for
orders but where is it being used in the
real world and are there any places
where it's working really well or really
badly ah if you can talk about the
lesson so k streams itself is actually
only about six six months old a bit lock
build on that so although there's a lot
of activity there's actually prob ly
about as much activity on the user group
about k streams as there is about Kafka
and catholics got a pretty big user base
they're the most of the use cases are
actually relatively simple so you see a
lot of people just doing a lot of people
kind of dipping their toes into this
with simple enrichment eggs exercises so
i guess just like that just that idea of
having
blending I'm a stream in a table and
actually for it's worth there's another
features coming in which is really
important for that that type of use case
which is more of a sort of business a
use case which is to be able to do a
multi joint at the moment you know it
only ever join we're going to talk about
this but you can only ever join two
streams the reason for that is that you
have to share the streams with the same
key so you actually end up in a one of
the things that you end up doing is
rekeying data set so that you can get
them on the right key for the joint
because each partition has to be the
join key for the partition the
partitions have to be co-located so but
the dinner there's a there's a kind of
more general framework for doing that
which is about replicating tables across
different stream processing engines
which is coming out shortly but you're
most of the use cases mostly the use
cases are rather that that I've seen
either that or they're more sort of
classical streaming systems so that's so
analysis of click streams like normally
things which results in aggregate values
so things that end up on on like a
dashboard so this is my conversion rate
for sales conversion rate for this
particular product that kind of thing
and yeah so stressful relatively new but
stream exactly been around a while well
writer I think we're out of time so we
loved it thank you been</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>