<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Matthew Sackman - GoshawkDB: Programming with Persistent Distributed Objects | Coder Coacher - Coaching Coders</title><meta content="Matthew Sackman - GoshawkDB: Programming with Persistent Distributed Objects - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Matthew Sackman - GoshawkDB: Programming with Persistent Distributed Objects</b></h2><h5 class="post__date">2016-11-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/PPZhksQdNBc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so first Who am I I was one of the
main engineers and designers of rabbitmq
so if you're currently suffering from
RabbitMQ particularly to do with high
availability or clustering or mirrored
cues I apologize that's probably my
fault but lessons have been learned so I
no longer work on RabbitMQ so I'm
working on a product called goshawk DB
it's a relatively young database it's
been in development a couple of years
this talk is about how goshawk compares
to some other products on the market
some of the algorithms that goshawk uses
and how and when you might choose to use
goshawk the first release of goshawk not
point one happened just before Christmas
last year not point to is about four or
five months ago and north point three is
going to come out in the next couple of
weeks but first I need some audience
participation so could you raise your
hands if if someone comes up to you and
asks you questions about a particular
database would you be confident in your
arms ins quickly about the features and
semantics so first of all MySQL or Maria
DB who's particularly confident about
that okay we've got one what about if
it's about the spider storage engine or
galera clustering well no one okay
postgresql 100 cave half-dozen brilliant
MongoDB to excellent Cassandra couple
okay you know a lot okay fine so in this
first part of the talk talks in two
parts the first part of talk is going to
be talking about different databases and
how you can try and compare them and so
I want to put it in the situation that
we are starting a new application and we
think we're going to need to use a
database so how do we select that
database well we might start by coming
up with some sort of rook features
requirements list so we might say we
want it to be distributed because
everyone
distributed things these days but
immediately we have a problem what does
it actually mean does it mean okay we've
got a cluster of nodes that make up this
database can i connect to any of them
and submit any query to any database or
do I only have to connect to a
particular primary know if that primary
node fails how do I discover the new
primary node we've probably wanted to be
thought colorant so that we can continue
working in the face of failures so that
means that data is going to have to be
replicated between different nodes how
many nodes how many replicas do we want
so that's another question to answer we
might want automatic sharding as well so
by this I mean that sure data is
replicated but we might not want every
single node to know everything because
if we do then that might then mean that
as the cluster gets bigger and a bigger
performance is actually going to drop
off because we're going to have to spend
more time distributing data about do we
want it to be transactional we don't
always need it to be transactional but
transactions are a really powerful
feature they compose by nesting and they
often make the code that we write on our
application a little bit simpler but as
I say we may not always need them most
importantly we want it to be intuitive
we want to have a clear model in our
mind as to how this database is going to
behave particularly during failures
because it's when everything is on fire
and there are lots of failures going on
that we really need to trust the
database to do what it claims it's going
to be able to do and obviously we needed
to be fast enough as well now we're just
starting out an application we may not
have exact performance requirements but
we should have a few orders of magnitude
accuracy so we should be able to say are
we doing a thousand operations a second
are we doing a million operations a
second and obviously that's going to
have an impact on our choice of a
database so I'm now going to talk about
some of those databases i mentioned
earlier three things I'm not an expert
in any of those databases what I'm
presenting here is I spent about half a
day reading documentation on each one so
if you're sitting there you know I'm
wrong please stop me and correct me i
have no intention of spreading
misinformation number two please don't
interpret anything that I'm saying here
as criticism all of these databases are
really really popular and often for
really good reasons they clearly meet
the requirements of
lot of people and finally I'm going to
be introducing a lot of terminology here
without explaining it and that's sort of
the point the point is that in order to
understand these databases we need to
understand a lot of terminology we need
to know a lot about how these databases
actually work so first full morir DB is
it distributed yes it does a primary
secondary is designed by default you can
use this galeras stuff which sits on top
and that gets you full multimaster so
that means that you can connect any node
within the cluster and submit any query
is it tolerant yes and galera various
reason has to be a CP system which comes
from cap theorem so there is a majority
of nodes and provided you're fully
connected with that majority of nodes
you should be able to submit queries and
it should continue working if a node
fails and then joins back in apparently
will automatically resync the
documentation suggests that might be a
slightly expensive thing I don't know if
not actually tested it can it do
automatic sharding ish galera itself
can't but there's this other thing
called the spider storage engine which
apparently can and you can put that on
top of galera is it transactional yes
but with glare you only get weak
isolation levels only so the maximum
isolation level you can get is
repeatable read so is it intuitive it's
a bit complicated and again please don't
take any of this as criticism there is a
lot of documentation about limitations
and that's really really great it's
really important that this documentation
exists and that is completely illegible
but what I worry about with stuff like
this is I would love a tool which can
verify when I check in code that my code
isn't violating any of these constraints
because I want confidence that my coach
going to be the right thing so it's
really important to have tooling around
this sort of thing we can also start to
build up a list of extra terminology
that we need to understand so we've got
CP we've got isolation levels and we've
got this thing about how do we actually
validate that our code is is safe to run
in a cluster moving on MongoDB is it
distributed yes it's definitely a
primary secondary design the
documentation I may have misread verte
this but it sort of suggested to me that
you always have to connect to the
primary even if you're just doing reads
I might be wrong on that is it
fault-tolerant yeah
again it's a CP system so there's this
fully connected majority of nodes thing
does it do automatic sharding yes this
is built in within Mongo you can do
sharding eyes on the collection level
and there are several different
strategies available is it transactional
no absolutely not long guy doesn't try
to do any transactions is it intuitive
well as with all of these systems
there's a bunch of terminology you've
got to learn got to learn the api's and
stuff so there's a thing called right
concerns and read concerns so that's
when you submit an operation it's the
number of nodes that have to essentially
duplicate the operation and agree on the
outcome again there is a bunch of
documentation really important this
exists about you know how to use this
stuff safely it would be great to have
some tooling around this okay and then
we can add to our list of stuff that we
need to figure out so we've got right
componentry concern we've got re
preference so that is when you submit a
read query you might want to indicate
that you don't worry you don't you don't
have any worries about this query being
performed on the primary you're
perfectly happy for it to be performed
on a secondary node moving on Cassandra
is it distributed yes completely you can
connect to any node and submit any query
to any node is it fault-tolerant yes
completely differently there's a
replication factor which is a number of
copies of each piece of data there's a
thing called hinted handoff which means
that if you have got lots of failures
going on so that your replication factor
can't be made but you're right concern
is lower than your replication factor
you can still make progress because the
hinted handle scores on the local node
the handoff if you've got to clients
writing to the same value at the same
time how do you decide which one wins
well in Cassandra it's lww which is last
right wing's the problem with that
though is that the last is defined by
time stamp which I believe actually
comes from the client and the time stamp
is just a pure physical clock so it's
your ntp time so that sort of doesn't
work because clocks drift so it's not
last right wins it's some reasonably
recent right
wins does it do automatic shouting yes
Cassandra does it based on consistent
hash ring that sort of has its own
problems you don't actually get uniform
distribution with a consistent hashing
don't have time to go into that here is
it transactional know it does support
something called lightweight
transactions which allows you to do a
bunch of operations on the same object
and make those things atomic and
isolated there are some question marks
about how that's currently implemented
but I have been told that it's currently
being reimplemented so I'm sure it's
going to get better in the future is it
intuitive it's very flexible so you can
go all the way from an AP system where
it doesn't matter how many things are on
fire you should still be able to get it
to do some work for you right up to a
system where it will give you lots and
lots of guarantees so again we can we
can add more stuff to our list of
terminology so we now have CP and APC
ApS that's the cap theorem we have last
right wins we have time stamps and
clocks we've got physical wall clock
time we might also want to start
thinking about hybrid logical clocks and
ways of solving problems with clock skew
we have consistent hashing there are
lots of different ways of implementing
that some ways a better than others
we've got pack sauce which is a
consensus algorithm we might also want
to think about other consensus
algorithms there are lots of those so
let's have a quick look at a couple of
these cap theorem I'm really not going
to talk about at all in this talk for
three reasons one I don't have time to
I'm certain there are other people
giving talks here who are going to cover
that very very well and three I don't
know if you think it's a particularly
great way to compare different databases
I think there are better ways to compare
databases and so let's have a quick look
at isolation levels instead and so here
is a chart of isolation levels we've got
some words and they're in a tree well a
graph like structure and so the the
simplest thing and the most intuitive
thing is a thing right at the top strong
serializable and so a database that
implements strong serializable has to
behave as if it is performing one
transaction at a time it runs a
transaction right the way through to
completion before starting on the next
transaction now if you're implementing
that you
obviously don't have to implement it
that way you can apply as many
optimizations as you want but the
behavior has to be equivalent to one
which only does one thing at a time so
over here we have a serializable we have
repeatedly thing from a galera isolation
which is the default isolation level of
an awful lot of databases so let's have
a quick look at snapshot isolation oh
and basically as you go down this chart
more and more weird stuff happens and
it's it's inherited so is a bit weaker
and strong serializable and the
weirdness that it introduces will snack
for inflation has the same weirdness
plus a little bit more so let's have a
look at snapshot isolation this nice one
sentence definition its guarantee that
all the reads made in a transaction will
see a consistent snapshot of the
database and the transaction itself will
successfully commit only if no updates
that it has made conflict with any
concurrent updates so a transaction is
going to start we take a snapshot of the
database all the reads are run against
that snapshot and we keep a record of
all the rights that the transaction has
made and then when the transaction
commits we compare those rights to any
other rights that are going on at the
same time and we see do these things
conflict if you are unlucky enough to be
running oracle then you can put a record
into serializable mode and you don't get
serializable you get snapshot isolation
instead but in general just don't use
oral okay so I mean this is a great
little one line definition you specify
can look at that you can probably go and
implement it it doesn't tell us anything
about what can go wrong so let's have an
example of what can go wrong so here we
are going to have two clients they're
both going to connect to the database at
the same time they are going to submit
transactions to the database at the same
time t1 and t2 and this database only
contains two bits of state x and y so
under serializable the database is going
to have to choose to run one transaction
and then the other so we can start with
t1 so is X equal to 0 yes it is okay so
we can do our assignment t1 can then
commit that's all fine then t2 will
start is y equal to 0 no it's not
because we've just modified it
so t2 commits but it doesn't do any
further work so we're left with X is
zero and wise one and obviously under
serializable these transactions were
submitted concurrently so the database
could choose view them the opposite way
round and we get the exact opposite
result no problem and the snapshot
isolation there's a third thing that can
happen so snap isolation can do these
two things and most of the time it will
but under extreme load it'll do a third
thing which is they will happen
concurrently so we follow our rules the
two transactions start we take snapshots
of the database so we can then
concurrently test is x equals zero yes
it is is y equal to 0 yes no problem
they can both do their assignments and
then they'll both try and commit so the
rule is we compare the commits are the
rights of one transaction with the right
of the other transaction and see whether
they conflict and here they don't
because they're actually writing two
different objects so we get this really
weird outcome of x1 and y1 and so this
is an anomaly as you go down that chart
you get more and more anomalies this
particular anomaly is called right ski
and and this is super surprising right I
mean how many of us have ever tried to
deal with this sort of issue in in
production yeah have we ever written
code that tries to detect this and undo
undoes this this sort of thing is going
to happen when half your cluster has
failed you've got more users than you
ever estimated and suddenly you've got
weird corruption in your database that
you may not even be able to detect the
stuffs really weird so moving up from
snapshot isolation the one above that
was serializable and one above that was
strong serializable which is also often
called strict serializable so what's the
difference between those two well strong
serializable must obey causality whereas
serializable doesn't need to UM and this
this is really surprising so a simpler
example here we just have one client
which is submitting to transactions one
after another and we have one piece of
State so the client is going to submit
t1 and then t2 and most of the time the
server is going to run them t1 t2 but
it's actually legal if certain other
conditions are met for the server to UM
ct2 and to actually go yeah I'm not
going to do that I'm going to pretend it
was done a long time in the past
of this database because there may have
actually been a safe point previously in
the history of the database where it
would have been safe to insert t2 so the
causality here is being violated the the
client definitely has a causal
relationship between t1 and t2 t2 must
have happened after to you want the
server is actually free to violate that
so the difference between strict
serializability and strong
serializability is is really really
powerful and and as I say these things
are inherited so if we look at snapshot
isolation again will note that there's
actually no requirement as to when the
snapshot was taken it doesn't have to be
the most recent state of the database it
can be some historical state yeah which
is what I said that so then the only
thing that I'm going to say about camp
is that we can overlay a cap on top of
this and so we have proofs of this and
so all it's really saying is if you want
to achieve any of the isolation levels
which are in the top half you have to be
in a CP system and that's really what
i'm going to say that so finally we get
to go to hook DB and obviously it's
going to take all the boxes so is it
distributed yes you can connect to any
note and do anything from anywhere is it
fault-tolerant yes in the configuration
you specify primate called f which is
your failure parameters so this is the
number of failures you want to be able
to withstand and goes we'll make sure
there are enough replicas of every piece
of data so that you will still be able
to make progress so long as there are no
more than F failures does it do
automatic sharding yes completely
transparently as a client you never
never need to think about that is it
transactional yes and it only does
strong serializable and that's kind of
rare but yes you have no choice at all
that that is all you get so is it
intuitive hopefully strong serializable
I think is the the simplest and easiest
isolation level to understand goshawk
it's only two years old so it only has
currently a fairly small API and
hopefully clear documentation I sort of
have a theory that products get popular
and they have more features and then
they get more popular and then they add
more features and they reach a stage
where no one understands what they do or
how they do it and some of these other
products may be further down that path
gospel definitely isn't and I'm trying
to resist adding unnecessary features
just yet so the conclusions of part one
understanding databases is hard there's
a lot of terminology which makes up
comparing them very very difficult in my
experience it's very common that your
requirements will grow over time so
you'll start on an application and
you'll think where we don't need
transactions we don't need strong
isolation levels let's just use
something simple and that will be true
for month one month to month three but
six months a year down the line you'll
suddenly be implementing feature and
you'll think oh I really really wish
that I had transactions or I wish that I
had you know stronger isolation level or
something like that so I would advise
and again obviously I'm going to say
this had your bets go with something
that gives you very very strong
guarantees and simple semantics to start
with provided that it's fast enough now
obviously the more work and the more
guarantees that a database offers you
the more work it has to do per
transaction so it is going to be slower
now you know you can optimize and do a
good job in implementation but
ultimately something that offers you
very very strong guarantees is not going
to be suitable if you're doing some you
know really high-volume clickstream type
stuff so you know horses for courses and
then as we've heard earlier actually
with the cockroach tool and that scale
problems stop being rare so we've
learned this from google and Netflix and
Facebook and Amazon you know you're
writing some code and there's a feeling
in the back of your hand you dig into it
and you discover actually yeah there is
a race condition between these two
different things but I reckon I'm going
to be able to get away with it this this
sort of events these aren't going to
occur in practice in production well
actually they will stuff will be on fire
you'll have more users than you expect
and these sorts of things will occur
these raids conditions will occur and
actually detecting that these race
conditions occur and you've now got
corruption in your database can be an
absolute light now okay so part two
api's and so most databases have some
sort of query language and obviously SQL
is the standard query language it's been
around for a long time and for a lot of
use cases it is absolutely the right
choice but it's not always the right
choice over the last 10 years we've seen
a move away from SQL and towards know
SQL but really that's for just replacing
the s there
is often still a query language of sorts
goshawk is about as far away from query
language as possible so goshawk doesn't
have tables or rows it stores objects so
what does the API look like well when a
client connects to goshawk it gets told
about a root object and so from there it
can inspect the root object you can see
whether it's got pointers to other
objects and as it discovers more
pointers and more objects it can
navigate over the object graph and you
can have an object graph of any shape so
we've got some aliasing here we've got
two objects pointing at the same object
you can have cycles have bigger cycles
there's absolutely no restriction here
so why if I chose him to do this well in
our programming languages we're really
good at dealing with objects this is how
we model the world so why not just make
some of those objects persistent why go
through this whole rigmarole of
translating our objects and all of our
state over into tables and rose and
stuff when a lot of the time we don't
need that now you know sometimes we do
need that but not always and another way
to think about it is imagine a perfect
world where our computers of infinite
round got more than my cpu and our
hardware never fails and our
applications never have bugs in them how
would we actually write programs and
manage data would we use a database at
all and I think for a lot of cases we
wouldn't and because we have all the
necessary data structures to manage
large collections you know dealing with
a collection of a million objects in a
hashmap is is straightforward if
everything's in RAM and so I think a lot
of time in this scenario we wouldn't use
a database so it then raises the
question what are we actually using
databases for so we use them for
persistence we use them to share data
between different clients maybe we've
got lots of copies of the same client
for performance maybe we've got
different clients doing different jobs
but basically we're using the database
to regulate access to shared state so if
that's all we need it for then again do
we actually need tables and rows and
columns and something like SQL or is it
much more intuitive to have just
persistent objects and an object graph
but you know it depends on what you're
using it for okay and if we're doing
that that means we can get rid of object
ready for mappings which I've never
liked so they tend they do weird things
they tend to do one table per class or
prototype which introduces unnecessary
contention you have objects which have
nothing in common apart from there the
same type and yet they're shoved in the
same table and so if you know the tables
got a primary key then on inserts you
then have contention for that then it
introduces attention as to who actually
writes the SQL because at all sometimes
producing inefficient SQL because
they're trying to work with lots and
lots of different databases they
sometimes do strange things to your
schema but generally it's another thing
to go wrong it's another layer of
complexity more things to debug more
things to understand if we don't need it
let's let's not use it okay so I'm now
going to briefly talk about some of the
algorithms that goshawk uses now
distributed databases especially ones
that offer strong guarantees the most
complicated thing that they have to do
is for every pair of transactions any
nodes that are involved in any pair of
transactions have to agree on the order
of those transactions and that's the
most important thing that is the subject
of a completely different talk and I can
talk for hours about that sort of stuff
because it's very very complicated so
I'm not going to talk about that in this
book I'm going to talk about some of the
other algorithms that we use so goshawk
uses mvcc which just stands for multi
version concurrency control lots and
lots of databases do this it's really
just a way of versioning your objects
it's no more complicated than that so
here we have a rob jet graph every
object has a unique identifier so this
is exactly the same as in memory you
can't have two objects at the same
address so it's completely analogous
there so we start by giving everything a
unique ID and then objects also have a
version number and so we might be able
to compare two different versions of the
same object now in goshawk the only
comparison that we can make is one of
equality or inequality we can't order
things based on their version number
because as I say ordering is very
complicated thing and that's handled
separately but what we can say is that
objects which share the same version
number must have been written by the
same transaction so in the middle here
we have F G and I all of which are at
version 5 that means there was a
transaction 5 which wrote
fgn I so we can also look at this from a
timeline point of view so this is from
the server's perspective we've got five
objects here a through E are they all
start at version 0 so the server is
going to receive a question from the
client and so the client says okay I've
just run transaction three transaction
three read a at version 0 it read be at
version 0 and it wants to write to see
and the client is saying is this okay it
is still consistent with the state of
the database well is it yeah i mean a is
that 0 beers at 0 so that's fine so this
can commit and and so what happens is a
read doesn't modify the state of
anything so a and b state version 0 c
however moves to version 3 and so the
version number that it takes is simply
the idea of the transaction and so
that's one of the reasons why we can't
actually order things by version numbers
then another transaction arise
transaction to this reads be at version
0 and read see at versions 3 so this is
very interesting we immediately have a
dependency here to must have come after
three because to read something that
three rights so can this commit yes
absolutely no problem and again the
objects that were written d and e they
change their version number to take the
ID of the transaction that wrote to them
now we receive a problematic transaction
so this is transaction 7 it reads be at
zero and it reads d at 0 so we can try
and draw this in like this this crossing
line this immediately is a signal that
something's wrong now we might actually
have some other information we might
have some causal information we might
know that 7 is definitely happening
concurrently with two and three so we
might attempt to reorder it but actually
in this case even if we try and move all
of this stuff back back in front of
three that would actually violate some
of the dependences of three so we have
no choice we have to reject this
transaction and what will actually
happen is the server will say to the
client no this can't commit the reason
why can't commit is because you've tried
to read d at 0 but D has changed a
version two so here is the new value of
D at version to go away rerun the
transaction and come back to me and then
finally we received transaction for
which we see it three
and right to beat India so this is just
showing that you have a transaction that
both reads and writes to the same object
okay so the question that is being asked
here of the server by the client is
quite different to how a lot of
traditional databases communicate from
client to server so if you look at the
life cycle of a transaction on it on a
traditional database it would look
something like this the the transaction
will start the client will ask question
try and fetch some data from the server
and get a response itals request more
data it might make some sort of
modification finally the transactional
end will try and commit and I hope you
get some acknowledgement now we've got a
lot of round trips to the server here so
this can hurt performance and so one of
the ways that for example SQL has tried
to deal with this over the years is it
since it's implemented more and more
commands so if you think about something
like up third which is now reasonably
widely supported it's a combination of a
select followed by a conditional insert
and yes it means that we write slightly
less code on the client but we have to
learn more stuff but it importantly
reduces the number of round trips to the
service it will go a little bit faster
and in the same breath we can also talk
about stored procedures which is again
trying to reduce the number of round
trips make stuff go faster get the code
into the database itself goss Hawks
designs very very different so in
goshawk we assume that the client
contains all the data that you need for
the transaction we just assume that so
the transaction runs to completion on
the client only no communication at all
with the server assuming that the cash
does contain everything and then right
at the end we just need a single round
trip to the server saying this is what
happened is this okay and obviously the
the server can turn around say no it's
not okay because your cash is out of
date here are some updates go away and
run the transaction again so this
changes the question that the client is
asking of the server from something like
what is the value of x into are these
reason rights actually creates as well
are they consistent with the current
state of the of the databases it's safe
to apply this and so if we go back to
this diagram well this explains the
nature of these questions that are being
asked here this is the final commit
it's a record of what happened in the
transaction cameras be safely committed
but I'm sure a lot of alarm bells are
going off in your heads because
obviously it's clearly insane to ship
all the objects from the server down to
the client because they're going to be
millions of them so in goshawk objects
are loaded on demand transparently
behind the scenes as you navigate that
object graph so you only bring down what
you actually need and and we have in
other programming languages and data
structures which are really good at
dealing with big large numbers of beta
we have hash maps and see like with with
a hashmap you have a million objects in
it if you want to do a lookup in it you
only need to access about 10 of them and
it's we have exactly the same data
structures in God Hall so you don't need
to bring down everything and so you know
there is there is no problem here and if
we think about it in a normal database
e-way and it's exactly the same is just
adding in index because we all know that
a linear scan over a huge table is going
to be very expensive so we reduce the
number of objects that we need to access
and so we have exactly the same thing
here okay so I'm now just to finish with
i'm just going to give a quick example
of how we might design something to work
with goshawk and really all i'm going to
be demonstrating is there a lot of
object-oriented design lectures that I
skipped at university and so we can have
a customer record and that will contain
a name and some email details and it
will contain its own little collection
of orders and this is exactly how we'd
model it in in Java or you know any
other language we wouldn't shove all of
the orders together from every cluster
every customer in one giant great table
so we have our orders and they have
items in them and again it will just
have a little collection and so we get
we get the encapsulation that we're
always used to and finally yes we might
want to track the state of order so we
might want to have a collection of
incomplete orders as that we can keep
handled on that and you know we get we
get pointer sharing so if two orders
order the same ID well we don't have a
copy of the item we just get function
which yes
you can do in SQL and do joins and stuff
like that but yeah you end up with
something where your data in your
database actually properly reflects the
state of your objects the state of your
objects in your programming language so
the actual API for accessing an object
within a transaction once you fetch an
object it is just kind of get the bytes
array out of it or write the bytes array
into it and that is really it so you're
free to choose any serialization format
you want the object is completely opaque
to goshawk it really doesn't care and
because hook allows you to wrap all of
these operations up into transactions
transactions can be nested so that means
you can write nice composable api's
we've got libraries which can be easily
reused and it's really quite a natural
way to work and in many ways if any of
you familiar with a software
transactional memory it's a lot of the
same ideas from that as well okay so in
conclusion databases have a habit of
going wrong once everything else is on
fire and it's not really going wrong
it's just most of the time they behave
in a way that we think is intuitive and
only when they're under a lot of
pressure so for example you can write so
pests you can write benchmarks and
things like that which will demonstrate
what actually happens under snapshot
isolation and it's only under those
stressful conditions that they will
actually demonstrate their real behavior
and it's documented but it's you have to
learn a lot in order to understand what
those terms actually the easier it is to
understand semantics of the database the
less you'll be surprised by it so that
is why i picked strong serializability
it is the simplest thing to understand
in my opinion we don't always need
tables or query languages yes there are
lots of use cases where you do and but
we don't always need it and a lot of us
I don't think are dealing with millions
and millions of events per second and I
think there's a slight skewing from the
industry when you know big companies
produce very very impressive results
don't get me wrong but are those
solutions really applicable for all of
us and so yeah we can get away with
something that
hopefully is a little bit more intuitive
thank you very much I seem to have lost
Chris but yes questions yep so in goes
there is a collections library which is
just something a bunch of code that is
written on top of the standard like the
standard clients there's a client in
Java and a client in go and one of the
collections that I've implemented is a
linear hash map so you would just use
that and so your keys the API expects it
to be a byte array and the values are
just a pointer to an object and so you
would just use your string and just get
the bytes of that and that would be your
key in your collection so it's not that
the the database doesn't present itself
as a key value store it's that you are
essentially choosing to use it in that
way does that the implementation if
you're so every bucket will be an object
and then the essentially the key value
pairs within each bucket our pointers so
I've implemented some so for example i
implemented a skip list where every
single key value pair was a number of
objects and there are performance issues
there so if you end up with transactions
of thousands of objects you are actually
paying CPU cost for every single object
within a transaction so I've
deliberately tried to select data
structures which are a little bit more
efficient than that but equally you want
to avoid essentially a single point of
contention so there is balancing act
freedom anyone else
yep
you will always get the latest version
of some definition of latest you know I
mean ultimately we are dealing with
relativity here you know clocks go it
computers go difference between or the
restaurant now if your transaction runs
to completion and your cash is hot you
can get right to the end of the
transaction and you can say abort and so
within that transaction you have read
values you've read what's cashed you're
choosing not commit it's your choosing
if it's a read-only transaction this is
safe you're choosing to say I don't
actually mind if my cash is out of date
so you can control that you're you're
essentially skipping the validation
right at the end of the transaction
yep yep
so in any system which is strongly
serializable you can't do any side
effects within a transaction so you all
you shouldn't write so it's only when
the transaction has finished because the
transaction might have to restart
several times yes yeah so if ok
brilliant anyone else know ok thank you
very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>