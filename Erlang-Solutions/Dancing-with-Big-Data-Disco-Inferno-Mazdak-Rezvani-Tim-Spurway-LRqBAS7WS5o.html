<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Dancing with Big Data: Disco + Inferno: Mazdak Rezvani, Tim Spurway | Coder Coacher - Coaching Coders</title><meta content="Dancing with Big Data: Disco + Inferno: Mazdak Rezvani, Tim Spurway - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Dancing with Big Data: Disco + Inferno: Mazdak Rezvani, Tim Spurway</b></h2><h5 class="post__date">2012-06-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/LRqBAS7WS5o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">as long as we don't get booed off for
doing some Python coding in a rank
conference I think we're going to be
okay so let me let me get started let's
first start talking about disco which is
the the foundation on which we built our
little framework called inferno this is
a really great project started nokia
life in fact some of the guys who
created and I'm working on it are here
and probably doing a talk later today so
I encourage you to really get into that
but this go you know it's in Erlang the
foundations are in Erlang and the sort
of the runtime is in Python or really
you can write in any language you like
but the default package has shifted it
is in Python it's uh there's no java so
you know so why didn't pick disco to do
our big big data work with and i'll talk
about what we do in just a second but
really there's one main reason no xml i
mean i don't know how many guys have set
up a dupe or sort of similar projects
but you really have to you know thick
through hours of xml modifications
before the thing is up and running and
we wanted we wanted something different
really you know when we start with this
go we were a small start-up you just
wanted to get something done so it gives
you the simplicity of Earl and clusters
I can't remember actually writing any
code to tell the notes to communicate to
each other in editing any XML files to
do that it just kind of works it's got a
really cool tag bag tag based
distributed file system so you're not
dealing with directory structures and so
on what you're dealing with your tags so
you can tag the same global data
different ways it has some really nice
properties that I'll talk about in a
second as well and you know really
important for us minimal DevOps effort
you know again a small start-up you
don't want to have three people just
maintaining your cluster all the time
both dural encode and the python code
that should be discourse small efficient
and we
makes it makes makes it really great for
debugging and finding out what's going
on and when you're running your jobs
it's got a really almost non-existent
footprint so what's inferno we were
using this go from the very beginning of
our companies sort of adventuring to big
data and we start developing certain
patterns of usage and more and more
these patterns start evolving into a
little mini framework that you know
earlier this year we decided should
become an open source project that
everybody else can take advantage of we
like to see disco really grow and really
take it sort of rightful place as one of
the best MapReduce platforms out there
and we thought that by contributing a
little benefit to the community might
invite at least help in that regard in
front of us all in Python so sorry
chango let me talk a little about chango
we're going to act tech company we
created the technology called search
retargeting which basically works like
Adwords but in the display advertising
world that so-called banner ad world
which basically allows allows us to
target users that have searched for
something in the past so if you searched
for say you know iphone and you know
Apple is one of our advertisers which
they're not we would then be able to
match you on the real time at exchanges
and serve you an ad that that was
relevant to your search the technology
that that basically makes it all happen
is called real-time bidding it allows
many different ad networks have now put
up their ad inventory up for real-time
bidding essentially guys like Google App
Nexus admin so on and so on put their ad
inventory offer up for an auction and we
have to be participating at auction the
auctions are super super fast paced so
we're talking about hundreds of
thousands of requests per second and
every one of those requests
results in essentially a liner lock
files about something about that ad now
if you happen to bid on that ad we would
log that again if we happen to serve an
ad we will log that again if your user
clicks on an ad so on and so forth so
every action starting with the very
first request for for a bit on that ad
is logged and processed by our system
that amounts to about 10 billion records
a date today although that number really
goes up and down and it's on the new
products we're about to build that
number might even double briefly also
you know even though the code base for
inferno is in its in Python we have a
couple of major Erlang projects that
sort of power the chango's
infrastructure one of them is couchbase
which i believe again they're at this
conference and talking about their
product a really cool product there are
real-time bidding platform users
couchbase we were you know roughly
around 200,000 requests per second or
are sort of pumped through couchbase a
very very very cool project and disco of
course we have currently on about 24
notes about two terabytes in each node
not a massive cluster but amazingly it
gets a lot done so what is in front of
you in front of us really a sort of a
DSL for querying your log files I said
our system is really performance
sensitive so we never log to a
persistent data store other than
basically log files and what what in
front of allows us to do is write rules
that go through these log files really
fast and sort of give a summary data
about it it also allows us to do
automation around this reporting so we
have about 40 different sort of database
tables that contains summary data of all
the data we have and these fully tape
tables are populated on a regular basis
by jobs that run automatically on
inferno on disco using inferno you know
one of the benefits is for us that
instead of how you know buying a massive
data store and paying you know a massive
licensing fees we were able to summarize
billions of records into thousands of
rows and insert it into
our data warehouse interestingly because
of the interesting properties of disco
and the way it sort of launches jobs on
undistributed knows we've actually been
using it as kind of a distributed
computing platform doing some of the
some of the stuff we do requires a lot
of parallel computing and without
actually doing any real MapReduce and we
basically been doing that as well now
the thing we do with logs is uh is a
little bit different maybe than your
average logging but uh but you know in
front of works with both your standard
sort of CSV type logging we actually
happen to log everything in a structured
format so each line of our log is just
like a valid Jason packet that has some
nice properties as you will see in Tim's
demo in a few minutes but if even if you
sort of flog just tap separate or comma
separated lines which is data you could
definitely use inferno the tools also we
would like to replay our locks often
that's either to reprocess data or to
try to simulate events and see if a new
algorithm improves on the existing on
the existing one and the way we
accomplish that is all for a lot lines
of habit have a time stamp on them and
each each time we push our logs to DD FS
we tag them with a date as well and that
has very nice property that you're going
to see in a second now this Co has this
really cool concept of chunking you know
our large sort of source data files it
basically splits them up into smaller
parts compresses them and puts them on
various nodes on the cluster by putting
them on different nodes in the cluster
it allows the jobs that are running to
operate on them imperil so here's an
example of a line of our logs expanded
out and that's literally one of the
lines somebody searching for five signs
of stroke but
let me show you a let me show you a sort
of a quick demo of what what our sort of
cluster looks like and what I all right
so if I can make this happen so this is
sort of your standard disco screened
that this is actually our life
production cluster so the yellow means
stuff is running so you know these
things will start becoming very clear
after Tim shows you the demo but these
are the infernal jobs that are currently
active in the disco cluster the green
means that just finished successfully
yellow means they're in progress and
Redden's their fail for some reason so
yeah so the 24 nodes are basically kind
of or 23 notes what happened to 24th
note I don't know ok I going at it right
now alright so back to my back here
alright so what is they what does the
query dsl look like the query slk has
three major components the topmost level
is a rule a rule basically describes
what you want to do with your data the
rule has key sets and the key sets have
parts and I'll get to them in a minute
the two types of rules that you can
specify Inferno ders their their
automatic rules basically what I talked
about before things that you can
basically run on a on a schedule they go
through logs and basically do something
with the data and they do it on a
regular basis you can also run the same
rules or different rules manually from
the command line to just make your life
a lot easier you can specify the data
source where data is coming from DDF s
is normally where the data comes from
but for a rule we've extended it to
support as 3 for instance so you could
actually pull you a lot from s3 or HTTP
or whatever and the rules also
optionally you can specify a date range
so you could basically say I want to
operate on files from you know three
days ago until today or something and
and go from there rules also allow you
to specify processors these are pre and
post processors allow you to sort of
massage your data before you get them
into the
into the MapReduce job and I promise you
all this is going to become a lot more
clear once Tim gets get it does this
demo I'm actually going to run through
this a lot quickly just so we have time
for the for the more interesting part of
the talk and transformations also so it
allows you to transform your data you
know if your data comes in as a CSV
everything is string sometimes you want
operate on integers or floats or
whatever and allows you to do that so
key sets are basically allow you to
specify the sort of the map and the
reduced part and there's at least one
key set Peru but you can have multiple
the reason you would have multiple key
sizes so you could operate on the same
log files and extract different sets of
information from them instead of going
over to same file over and over again in
the same run you will look at the file
from different angles and extract
different pieces of information from it
so yeah the parts would basically be the
key parts which sort of the map phase
and then the value parts matures
descriptive reduce so as an example if I
wanted to count all the clicks on an ad
on a particular site the keys would be
the ad ID and the site ID and the values
would be you know the count magic
function oh and you'll see that in the
in the demo so here's an example of an
inferno rule this is actually a more
complicated example it needs to be some
of these things are automatic now but
again that's the best scene in a demo
some of the things that you can do at
the key set levels was filled transforms
as I said you could basically can
transform one feel from one type to the
other you can in fact create new fields
as you go based on sort of existence or
not into other fields so if you're
looking at a JSON block for instance and
you've upgraded the version of the log
file in the previous version there was a
field missing if that field exists in
the new version you'll act on it
otherwise you can basically create
ignore or create basically that the new
field from scratch or use some previous
data to calculate it you can do selects
as in filter out things that you don't
want to operate on as well and you can
change these selects these filters
together it's sort of model right after
the weight disco does its own input
streams and so on so you can chain a
whole bunch of selectors and generator
together to do some really cool things
you can do post-processing one of the
things that we do a lot of chango is we
use the post processing function to to
take the summary data that's generated
by the MapReduce and actually inserted
into a database it's sort of unique way
of dealing with it I think but you can
basically do anything you want the
post-processor fracture you're dealing
with a the results of the gue MapReduce
job you can have it sorted or not sort
of if and sort of go from there and you
know if we've predefined some input
streams to how to help to help out
especially with the JSON stuff which is
our standard login mechanism archiving
is a really one of the things that we
think is the best feature inferno let me
give an example as I said we we have
logs that are basically so let's take
our ad server it's constantly logging
right it's never non stuff every 15
minutes we take the ad server logs from
all the different servers and we push
them to one single d DFS tag on disco
you know 15 minutes later you're pushing
again to the same tag so we keep on
pushing these blobs to the same tag and
the archiving mechanism allows us to
only operate on tags that haven't
already been operated on so if you have
a constant sort of a stream of data
coming in the best Locker me allows you
to basically look at the blobs that are
not haven't been processed and processed
them so to avoid duplication archiving
also basically uses the tags that are
the dates are impotent embedded in the
tags to write back to the correct
process tag as well as if you want to do
any reprocessing your data it also
allows you to do that and in general you
know it's the mechanism that allows us
to do sort of schedule jobs or at this
point I think it's time to see this in
action as opposed to the abstract stuff
that I've been talking about so I'll let
Tim take over and go from there you want
to take mine you
you
hey folks I think what I might trying to
do is sit this way so I can keep one eye
on the screen and when I on y'all yeah I
promise I won't get into debugging which
when we did yeah the run yesterday would
happen alright so what I wanted to do is
give you a feel for how we use inferno
with disco and so instead of using our
log files I just went out and grabbed
some US census data which is fascinating
and so we're going to take we're going
to take a look at it a with that okay so
what I'm going to do this guy over here
ah font size yeah well let's just take a
look at the date and we'll see if we
need phone sighs because I don't know if
I'm gonna have a lot of room here so
here's a sample of the CSV file this
this is a employment data for the u.s.
u.s. census so it gives you a breakdown
by state and by industry by sector and
and so what we're going to do is we're
just going to write a couple of rules
and hopefully you guys will get a feel
for what you can do with Inferno a
couple of features of this you've got
you know it's a CSV file we're talking
about JSON we can do JSON csb we have
you know all of the input streams to
deal with that the data will give
there's a coat there's a bunch of codes
here for enterprise sighs and then
there's descriptions over here of them
so there's like co2 means 0 through 4
and so
and i think i have this up here so
there's some codes that tell us how big
enterprises are and so what we can do is
split this out by you know we can ask
questions about what states have what
size enterprises and how many employees
what's our payroll and that kind of
stuff so what I'm going to do is I'm
going to just dig right in and start out
with a an empty file the way that we've
done this just to make it you know easy
we could do a lot of introspection and
stuff on on the code to figure out where
your rules are but we've just decided on
a convention you have a variable named
rules which is a list of rules and then
you point inferno at that file and say
execute this rule and we'll see how that
goes so what you do is you create an
infernal rule and you give it a name and
so on so you just fill in a few things
one of the one of the things my
autocorrect here one of the things that
you tell it is what where you want the
data to come from so this would be what
tags indeed EFS do we want to pull out
and that's a list so you can specify
multiple and at this point I'll give you
a little whoops we'll go and take a look
at the tags that we've got indeed EFS
and I'll give you a quick
this is by convention you have indeed
EFS and I probably the guys in the that
are going to be talking about disco I
don't want to steal a thunder or
anything but just the naming convention
is usually colon separated and so we're
going to here's the sample that I
uploaded into my cluster and so and so
on I've actually got a I've got all of
this stuff written over here and what
I'm going to do is copy and paste it the
one rule that that we're going to look
at first and i'll just copy it over so
that it's a little bit we can move along
a little bit quicker okay so we're going
to look for the largest employing states
in the US there's our source tags is
that you know tag and DDF s we have this
thing called a parts preprocess and what
this takes is a list of functions and
this is a pipeline if you will of
functions that the data will run through
in the map phase allows you to do three
things it allows you to add more data to
your parts that is your inbound data
each line it allows you to select data
and it allows you to generate data and
you can as you specify a pipeline of
them the data will run through it and
you can do really useful things so my ID
here is telling me I have to supply a
function called filter totals and so
let's jump back here and grab that
function and we'll talk about it
typically what you're doing in
preprocessors is is you know dealing
with the actual data so you're looking
at your data definitions and you're
saying you know if the parts contains
the actual one line of data coming in
from the file its organized it into a
nice dictionary for you to access that
dictionary is built using this which I
took from the CSV file here saying these
are the expected fields in the in the
CSV so what i can do is i can say parts
past necks is not equal to this next is
this code this industry code and where
my enterprise size is not equal to one
in the sample we can see that there's an
enterprise size the one is is the total
now the interesting thing about the
damage so you get the feeling that okay
so we're going to select this is a this
is a filter or a selector we're saying
here is where yielding the parts of it
meets this condition by yielding it
allows it to go on to the next function
in the chain and you can chain all this
stuff together and you could also choose
to yield multiple things for each line
and what that would allow you to do is
say take the case where you're you know
splitting text off into constituent
words or sub phrases and generating more
data so it's pretty flexible mechanism
and very simple field transforms are
very similar to parts processors but
what they do is they just call a single
function with the value for that
particular column if you will so because
we're dealing with the csv file there's
no unlike Jason where you'd have type
information this you don't have any type
information we got to turn it into an in
because we're going to sum this thing
so I'll just go in and grab that
function as well here's another typical
thing you'll be doing in inferno or in
any MapReduce thing it you know if you
have bad data you don't want it to kill
your whole job or whatever so we're just
going to by convention will have
inaccurate data rather than a dead job
this csv fields are kind of explained
this already this is how do i interpret
if you leave this out what it'll do is
it'll number your columns of your data
in your csv if you've got a json file
its self tagging so you get to use those
right away down in the bottom here we
have the key parts and value parts these
are going to be in the map the key parts
you know that's our key so give me a the
sum of all employees for each state okay
so we've got a rule written and we can
go and run it after you've installed
inferno on your system you just run it
by saying inferno the cool thing about
this is that if you've got multiple
clusters you can specify which disco
master you want this stuff to run on
right from the command line so my
machines called Tim go but if you wanted
to do development and run it on the
production cluster you just have to
change this to point to the production
cluster minus I is interactive mode this
is the name of our rule so the
conventions are dot two separate so demo
was the name of the Python file the rule
within the Python file was largest
employing states on the other thing we
have to tell inferno is where to look
for where to look for this code this
rule and for reasons that we can't
reveal it's the minus y option oh and I
think that should do it so this says run
that rule that's in that demo file on
Tim go my cluster and find the files in
this directory will let that chew away
for a little while you can see here
you've got some basic logging starting
the job gives you your disco job ID and
it tells you how many blobs it's chewing
away on this is about a million lines of
data the employment data for the Census
and it's from my believe 2009 so we can
go see it running in disco that's a
production disco this is tim go you see
i ran a whole bunch of stuff before and
killed them so if you you know dial in
on it you'll see MapReduce has already
done it's it's busy reducing here I've
only got you know I've assigned five
processing units obviously if you had a
cluster machines it would be running it
across you know all of them we might
even be able to finish this before going
on to the next rule okay so here's our
output you know pipe it into a file put
it in Excel make a pie chart I don't
know that's the basic idea okay so what
I'd like to do now is just take a look
at slightly more where's my moves
slightly more you know just elaborate on
this on
this rule a little bit so i'll go over
here we've got right so write another
one here and let's pace that one in here
i'll comment this guy out indent it like
50 mics
so what I'm going to do here and and
this is for interactive type what-if
analysis what you'll typically be doing
is you'll you'll be doing a general type
of query across you know a constrained
set of keys with a lot of summary data
and then what we can do with Inferno is
pass in parameters on the command line
that get plugged into the preprocessor
function so you can say here we're going
to be looking at these nakes codes or
nicks descriptions these are the the
sectors so manufacturing or fishing or
whatever and then you'll also be able to
slice and dice based on the size of the
of the corporation so this is going to
be looking at the professions and sizes
ein you'll be getting all of the
summable data one of the things that I
kind of failed to mention is that for
our values the default operation on all
value parts is just to sum them this
might be a little you know naive but we
were finding that that was that covers
off ninety percent of the cases that
we're doing and I suspect that's the
same in most MapReduce kind of
applications of course everything is
over over I dab'll here if you don't
like the the map or reduce functions in
inferno you can specify your own map and
reduce functions right here as well so
okay so let's take a look at this I've
added another two functions here and
what we typically find you'll have keys
and you'll have parts preprocessors that
select for those keys let me just go and
copy the functions will take a closer
look at them and so this is select the
enterprise size and select the industry
or sector
to get coffee let's go put those guys up
here and just take a closer look okay so
basically you've got the same thing we
saw in the last function it's a type of
conditional yielding so it's a filter
but it has an additional test here on
this params soap rims are parameters
that are passed in data that you can use
usually metadata about your rule or
whatever they get passed to all the jobs
that are running but in all phases so
the maps and the reduces all get these
parameter size in if its present then do
this test if it's not present just yield
it's basically an optional parameter so
this allows me to say I only want to
look at companies that are less than 500
or companies that are less than 20 and
greater than 500 and so on same naxx is
the same thing I'm interested except in
this case what we're saying because the
naxx code is a hierarchical code it's
like a six digit code but it's prefix
like 33 for example is manufacturing and
then all the three three stars will be
like all these different will be all the
different types of manufacturing so what
i can do is i can i can say at any level
give me you know all manufacturing or
you know a very particular sector so
with just these two you know
standard-looking functions and this rule
I've built a fairly robust and useful
rule that I can launch from the command
line the only question is how do I get
the data in so if I want to specify the
entreprise size and so on and I'll show
that in the command line so let's clear
this data I'll start for my last rule
and so the parameters will pretty much
be the same obviously the rule name is
different it's summary by profession
still going to do it here to pass in
custom data I use the capital P and I
say or the name of those things honor
sighs in and remember this honor sighs
is this special code so let's let's take
a look at enterprises that are less than
20 employees so I would do that by
specifying honor sighs in 05 and if you
wanted to do other ones if you wanted to
do less than this and greater than 500
you could go on but let's just look at
this one and I also wanted to look at
nakes equals so I'm saying minus P makes
equals let's say 33 I know that's
manufacturing well didn't die so that's
a good sign again we can see our summary
by profession grinding away let's go
look at our code again for sec yeah
there's there's a lot of there's a lot
more to inferno than these simple ones
you can specify the way it handles date
so in the way we run stuff in production
everything has a date tag and a lot of
the operations in inferno will you'll be
able to specify you know where the day
tag is the date is tagged inside the
tags and you'll be able to specify
ranges and so on so you can end the way
it typically works is because we always
work backwards we say let's start with
yesterday's click logs and work back 30
days or last weeks and work back 10 days
from there so from the command line you
can very quickly if you've got canned
rules and typically what we're doing is
we're building like libraries of these
rules and then doing analysis as you
know business requires either on the
command line or in an automated way and
send out reports and so on so as you can
see here's all the here's our results
came back and even not in a single
machine I mean you can see it's pretty
snappy to go through you know million
odd records obviously it's like that on
real production Hardware in a real
cluster so you know this is the forging
and stamping and so on
there's some there's some funky
professions in here too which are kind
of interesting let's see if we look at
the next codes hunting and trapping
there's uh there's fishing but there's a
there's also fin fish fishing didn't
even wasn't even aware of what that is I
mean you can go in and dig in on this
stuff and and you know take a look at it
I mean I could go on with more rules but
I mean I think that that kind of gives
you a flavor for what Inferno is and
what you can do with it sure I just hit
play right space bar oh yeah that's
right just go Inferno with Steve Jimmy
Ellis the guy who wrote it just uh yeah
time last week so this presentation is
dedicated to to Jimmy I guess wool and
that's our that's our info
any questions on that where do we run it
we have dedicated cluster at a data
center but you know you could run it in
the cloud yeah we use dedicated hardware
it's just we find it a lot more reliable
and so on for us and we don't have you
know our requirements don't change they
just kind of keep getting bigger so
that's how we do it yeah yeah it's a
it's a fair question the question is
could we do what we're doing with this
go in in the database the reality is
we've got so much data and add different
intervals that we find that it can be
you know the summaries that we can get
using disco can shrimp shrink our data
size by like you know magnitudes orders
of magnitude so we find it you know
really useful to do disco upfront for
that for data that is bound for the
database anyway it makes everything
faster makes everything run better and
it doesn't it doesn't cost us a lot to
do that compression up front now the
other thing is is operating on really
big sets of raw data we're producing raw
data anyway because we've got all of
these processes that are generating log
files and being able to do operations
directly on them that might be quite
expensive machine learning algorithms
and so on
we don't have to burden our production
database where people are expecting that
to be responsive for dashboards and you
is so that's kind of we we find a role
for both both things but the lines are
blurring thank you Mike for say
certainly so yeah I mean certainly for
so far logs a database works just fine
in fact do you know have a commercial
data warehouse in production as well
where now that our summary data is grown
so big that even for the summary data we
needed a really robust database so we
can put say our impression logs in there
like the ad server logs but our
real-time bidding logs where we're
talking about over 10 billion just
real-time bidding logs just just no way
we're going to be able to fit them in a
database and without paying just massive
maxima and it won't be able to really do
things like data transformations real
you know real time operations like
machine learning algorithms that we run
well on disco and one of the things i
mentioned earlier was that we do
distributed computing quote-unquote with
the disco cluster and one of those
operations is machine learning so we go
through these logs and mostly operating
the map phase really be just in the map
phase which is distributed across the
cluster we do certain things to the logs
and and sometimes there's even there's
not even a reduced phase so all the real
sort of computation happens in that
phase and it's a but it's in parallel
and across all the nodes and so on
right why not the dupe is the question
if you know again I think I answered a
question earlier on the talk a dupe
takes manpower to set up and operate it
really it's not that I've we have you
know we didn't try but as a start-up we
wanted something that just works and it
just is really fast and you know disco
gay was dead you know it again the fact
that the Erlang sort of sub system
automatically takes care of all the
networking in the clustering the fact
that the jobs are super lightweight and
they were in Python which happened to be
the language we were using for our
front-end anyway or back in anyways
which was really great but you know
should you have a bunch of java jar
files sitting around and want to run
them on disco you can still use the the
new version of disco supports workers
that you can basically run it in any
language you can run jobs in Erlang if
you wanted to for and Oro camel or you
know any other sort of language you
desire the other big so the devops f
effort was a number one reason number
two reason it was just we didn't like
doing that so no files and so on and
then disco just made sense because he
was so like wait took one day to set up
honestly and now that we know how to set
it up it takes about 25 minutes to set
up so yeah yes please play yeah yes
constantly yes absolutely so the
question is you are the rules or just
you know do you have to run the rules
man you all the time where the system
automatically take care of running the
rules if I understand collect correct no
its run time so it's okay so the
question was the run time as a compiled
code and compile no it's not compiled
it's basically just good old-fashioned
Python it it uses the Python interpreter
and now Python does create these pipes
PI C files which is really just a byte
code but there's nothing inherently come
you know compiled about it that it
starts an interpreter for every single
job every joy every job has got a new
interpreter and the interpreter
basically starts ice cream the jobs are
so lightweight I mean honestly if you
look at the big you know the whole disco
code base and add on top of it the
inferno sort of bits you're talking
about you know a few hundred lines of
Python that are actually mostly I oh so
if you really wanted to speed certain
portions of it up you could use C
modules to do that and the only caveat
is that you have to get your sea modules
loading all in your cluster everywhere
so when the Python interpreter loads it
has they see modules to go with it but
our our system basically just runs by
uncle it gets when you run it on when
you start the job it basically pickles
uses the parts on Python marshaling
system called pickle the entire sort of
environment and then ships they pickled
code across the entire disco cluster and
starts running it it's kind of a funny
see funny thing but it's really it works
really well yes
the interface for you know what that's a
better question for that some of the
disco folks but what I jared's right
here when you want to try to take this
question is using ports you said okay
just for the record so it goes into the
video using force yeah
so the question is the the sample your
data sometimes work better than just
running through the entire thing Oh as
representative okay well that's actually
pretty domain specific thing if the data
that we have when we're doing our
machine learning type of work we look at
you know clicks and conversions as our
sort of signals for what we want to
optimize for and frankly those things
are much more sporadic than the massive
pipe of incoming bit request so if we
don't run through our entire data set
we're actually going to not be doing
ourselves any favors because we probably
miss a lot of signals but if you if you
happen to have a lot of signals in your
code then you can use a small portion of
an adjuster do your cast with it now one
of the things that I personally
discovered recently was that we were you
know there's a glow neat Python packages
that are sort of designed around doing
machine machine learning that I didn't
know existed and so one of the things
we're just experimenting with is instead
of because we rolled our own code which
we do machine learning is to use these
packages which are basically in C and a
lot faster to see if we can basically
run those on the disco cluster and see
what they come back with but our
particular data said it doesn't really
respond well to sampling
yeah that's a good point actually some
so the date Rangers as Tim was saying if
you know you guys didn't hear that it
was sort of an actual sample that we use
sometimes it turns out if you can use
any bit data in our case going more than
five days results in so much data that
is just you're just going to basically
sort of destroy the cluster just going
to be sitting there for hours and so if
you take only like three or four days of
bit data and combined with maybe a few
days or weeks of conversion data that
actually ends up giving you some pretty
nice results any other sort of questions
comments alright great well thank you
very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>