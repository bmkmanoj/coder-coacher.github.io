<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Jamie Allen - Beyond Eventual Consistency - Code Mesh 2017 | Coder Coacher - Coaching Coders</title><meta content="Jamie Allen - Beyond Eventual Consistency - Code Mesh 2017 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Jamie Allen - Beyond Eventual Consistency - Code Mesh 2017</b></h2><h5 class="post__date">2017-12-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/MDPU7_NGNKk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Jamie Allen I am the director of
engineering at Starbucks and we're
working on a project called unified
commerce platform and what this really
is for us is the complete reimagining of
our entire rewards and mobile order and
pay systems so one thing to know about
Starbucks historically it's not really a
technology shop its place where we
leveraged a lot of packaged systems to
be able to implement this kind of
functionality in the past and a lot of
those systems have scaling limits and
they have a limited capability for us to
be able to implement new features so if
you think about this from Starbucks
perspective if we buy a package from
someone and then we say we want to do
something really cool that will enable
new ways to engage with our customers
well the only way to do that is to
tailor these packages and then all of
our competitors get it as well so it's
not exactly the sort of business value
we want to unlock for everybody else
so the unified commerce platform is
really starbuck saying we need to be
much more of a technology company and
they made some decisions before I showed
up such as we were gonna leverage Scala
and akka and Cassandra and Kafka and
really use this reactive stack that you
hear so much about these days and that's
actually why they hired me because I
used to work at type safe light Bend now
helping people as the head of global you
know consulting and training helping
people adopt this and really helping
companies make the transition to these
kinds of approaches the great thing for
me is now I get to actually put stuff
into production when you work at a
vendor you don't get to do that so much
or at least you may help somebody do it
but it's not your baby right it's not
the thing that you help build and I'm
super excited to say that for the very
first time we are in production in Japan
with the new system has added 2 million
new rewards customers just in that
market and for me this is fantastic and
mainly because if you look at the u.s.
which is our largest market we have
roughly 50 million Rewards customers
of which I would say roughly 16 million
are active people who are regularly
interacting with us on a daily or weekly
basis so a lot of people will create a
rewards account and then you know they
want to get the free birthday coffee or
something like that and then they won't
come back or maybe you even had somebody
who once tried to create roughly 280
different accounts for a free birthday
coffee and at that point you know we
kind of said come on you know we don't
mind that people want to get the free
coffee we give away a fair amount of
free coffee actually but at the same
time let's be fair to each other right
and that's actually when things I love
most about working at Starbucks is it's
a company that does care deeply not just
about their customers but about the
world making a positive impact so it was
really interesting I was just sitting in
David Nolan's talk prior to this so he
was talking about the atomic and he was
talking about how you know we want to
really limit the amount of complexity in
our in our code and for me one of the
things I really want to make sure we do
is not just go out and you know whenever
you sit there and say hey we're gonna
use Scala we're gonna use akka we're
gonna use all these really fantastic
technologies I want people to use from
sanely I don't want people saying
alright well we're gonna write all of
these incredibly stateful services
they're gonna have clustering us trying
to figure out how we'll deal with
whenever you know sharded node goes down
and how we're gonna re you know bring
that back up with all the data that was
on the node that went down and all that
kind of craziness we aren't high
frequency trading when you look at
Starbucks it's a company that has 91
million transactions a week in North
America and I mean that's sizable scale
it's not it's not high frequency trading
it's not dealing with the in the
financial markets or anything like that
we don't have to write something that
has to be the most complex approach to
leverage the most highly optimized
solution so I prefer for my team to
think about how we can leverage tools to
deal with tough problems rather than
writing that in code that way we keep
things as simple as possible make the
system more maintainable and whenever it
comes to dealing with consensus and
really difficult problems we leverage
tools that have done it really well over
time so
I looked at Cassandra I was actually the
database that we were given at Starbucks
when we're building this platform we
decided that we were going to do
something that's also very difficult
we're gonna build a pass over AWS of our
own and the pass is a beautiful thing
there's another team dedicated to this
and what they do is you know
infrastructure is code so that we can
rebuild an environment or deploy into a
new market really quickly but also
security by design and security by
design means that you're thinking about
security as you're building the system
you're not saying at the end oh and we
now need to encrypt a few things right
you need to have this if you really want
to be PCI compliant today I don't know
how many people have to think about PCI
compliance but anytime you're dealing
with any kind of personally identifiable
information or you know some sort of
credit card data or something like that
this is the kind of rules you have to
follow in order to make sure that your
system is not going to be compromised
Oh wasp stock - type two in the u.s.
anybody who wants to deal with us has to
have a well-filled out ca IQ which is
we're showing that you have taken the
time to you know actually adopt security
principles in advance and we have
Cassandra on this pass it's given to us
as infrastructure-as-a-service
we know that this is always going to be
available to us any time our team that
built the pass deploys into a market but
it's the one database I get and for
those reviews Cassandra there are
certain semantics to it which don't fit
every use case one of the things I did
whenever I was at type safe and light
been was work with a large consumer
electronics company located in Cupertino
and you know actually they're very
public about the fact they have
extremely large Cassandra footprints
Apple right at last I'd heard several
years ago they had 75,000 nodes and
that's not a single cluster but multiple
clusters I'm sure and the reality was
they actually had two of them it wasn't
just one 75,000 node one
Cassandra does amazing things at scale
but it's also a dynamo store and it's
going to have much more eventual
consistency semantics and you can sit
around and try and play with the various
consistency levels replication factors
and quorums that you're going to use
but the end of the day you're really
trying to take the database away from
what it does extremely well and my use
cases aren't always going to align with
those kinds of semantics so when we
looked at Cassander and said well okay
if a customer's coming in and we know
that they're going to do in a cruel
event where they are buying a coffee and
they're going to get stars from you know
this coffee and be able to redeem them
at some point the accrual is something
that I don't know when I'm actually
going to receive it's not an event that
comes from our point of sale it goes
through a credit card processor and that
credit card processor could actually
send it to me anytime within 24 hours so
when you see you know a rewards program
and you don't see your stars or you know
your mileage points or whatever right
away and a lot of cases it has nothing
to do with the system design of the
company that produced it has everything
to do with whoever is actually doing the
payment processing for them however a
Redemption event is something that has
to be done fairly quickly and when our
debt redemption event occurs there are
semantics that mean that eventual
consistency just doesn't work really
well for us think about in terms of the
void of a transaction right whenever
that occurs and it's a Redemption a void
of a Redemption transaction and then
being able to buy another coffee with
the Stars that they had those are all
things that don't loan themselves
particularly well to an eventually
consistent store so Cassandra doesn't
really solve all of the business
problems that I have to deal with and
one of the things we started doing
saying what would well when we think
about those sort of databases we have to
break this down to the most elemental
you know concepts and stuff like that
and I'm sure the majority of the people
in the room have heard of the cap
theorem right the idea that you're
making a trends a choice between
consistency and availability in the face
of a network partition and it really
defined the way we thought about
databases for like the last 10 years
because we knew that to increase the
scale we had to have more nodes but the
more we have more nodes unless we can
guarantee that we're going to have a
consistent read this is really the basis
of how those cap theorem defined dynamo
stores would work you would write to a
certain number of no
you wouldn't say all of them because if
you were saying that then you're asking
for a fully consistent store but in
doing so you're limiting your
availability when we want availability
and we want to have systems that are
going to be resilient and staying up all
the time you know we we have to take
into consideration that we're gonna have
eventual consistency just because of
physics you know the nature of I'm going
to write to several of the nodes and I
might read from one of them that has not
yet been updated so how do I deal with
that and then the question becomes while
that define the way we thought of the
last ten years our new theory is
starting to disprove the cap theorem and
even more importantly are we optimizing
for the wrong case so I'm going to talk
a little bit about later on how maybe
when we thought about AP the idea of
availability in the face of a partition
what we really optimizing in the wrong
direction so has anybody ever had a
database that never went down
anybody know so what does ap really mean
if we're sitting down and saying that
we're gonna have a database that has
nine nines of uptime and this is like
the classic airline you know uptime
theoretical guarantee right that you
could have theoretically nine nines of
uptime it's amazing to think about what
is that Francesca that's on the order of
three seconds in a year fantastic who
doesn't want that right we don't get it
and if we don't have that what is the
point what is the point of saying that
we have an AP system now Daniel Abadi
has anybody ever heard of this fellow he
is a researcher now at the University of
Maryland he was previously at the Yale
University in the United States he said
that we're not really thinking about
this the right way and instead he
decided to frame the terms around
something he called pack elk and this is
where he's saying that if there is a
partition well then we can start making
trade-offs around availability and
consistency at that point alone
otherwise the real trade-off we're
making is between latency and
consistency and we can have consistent
semantics inside of our programs and
inside the database so long as we're
accepting the fact that database parts
our network partitions are rare
now I actually had the pleasure meeting
Daniel Abadi last I think a year and a
half ago in China at this massive event
held by Huawei and it was incredible
it's like this you know there was a in
storm data bricks there there were
people talking about flames there was
this great combination of the
researchers and you know academia and
you know enterprise and Daniel was there
talking about a distributed file system
and he was basing it on something called
the Calvin paper that he had written
where he was talking about pack help and
Daniel mentioned you know network
partitions are rare and that set off
bells in my head because how many people
feel that network partitions are rare
now think about them in terms of how
many operations you're actually
performing and how many are affected by
a network partition and this is where
Daniel was going with it he said we're
optimizing toward this availability
consideration when it's really a small
percentage of our overall transactions
that are affected by that and because I
didn't understand what he meant by rare
I actually argued with him not like you
know bitterly or something like that but
it it was one of those cases where I was
like they're not rare I worked at
juniper Networks for like a year and a
half they happen all the time and he was
saying no they're rare and once I
started understanding what he meant by
rare which was a good year later hmm
yeah it suddenly went oh you know that's
that's a really big change in the way
they I'm thinking about how I'm storing
data so I sent him an apology email and
said I'm really sorry about that and he
was very very kind about you said that
you know Calvin is the sort of thing
that people don't get it first and then
later on they really start to grasp so
if we talk about just data management
taxonomy so we get a feel for the sorts
of things we care deeply about I'm gonna
leverage a little bit of Kyle Kingsbury
the person who created the Jepsen series
and the way he tends to think about this
because it's definitions are quite good
first of all we have to make sure that
we understand the difference between
consistency and acid properties versus
Sisson see in the cap theorem and when
we talk about it in terms of the acid
properties we're talking about you know
the transition of a database in a
transaction from one valid state to
another it has nothing to do with
distribution across multiple nodes it's
merely the idea that consistency means
that a transaction occurs and you go
from one valid state to another
whereas when we're talking about the cap
theorem we are talking about consistency
in terms of am I getting the same data
whenever I read from different nodes
right serializability
can I have guaranteed right order now
there are no necessarily like no claims
as to what that order might be you can
define that or the database could define
it but either way to have
serializability you have to have a
guaranteed order of some kind and
linearize ability they're all nodes
going to have that order more
importantly if I have linearizable
consistency no matter where I read from
it am I always going to get the same
value in a distributed context so these
are all things that we wanted to make
sure we had if we had to implement a
system that had to have consistent
semantics but still have availability
you in case you know nodes or an
availability zone or even a whole you
know region went down how are we gonna
deal with that and then you get into the
whole optimistic versus pessimistic
concurrency you know locking inside of
databases they all have very similar
kinds of concerns around them when
you're talking about optimistic you're
talking very much about compare and swap
and that's great it sounds fantastic
until you get into a right heavy context
if you're doing tremendous numbers of
writes those compare-and-swap type
semantics are gonna be more difficult
because the values changing all the time
one of the things we came across in you
know Juniper Networks back in 2012 was
we really wanted to use software
transactional memory it sounded
incredible until we got to the point we
were thinking about a 20 million item
double length list that was changing
thousands of times a second now there's
no determinism around how
compare-and-swap starts to fall over and
no determinism therefore about how
software transactional memory will fall
over at some point you will
non-deterministically begin to see the
inability to commit a transaction and
that's one of the difficult things about
it you see Hardware transactional memory
now trying to replace this I see very
smart people such as guilt an a as huge
fans where Hardware transactional memory
is going and Margo had that tremendous
talk this morning about how they want to
be able to leverage those semantics
inside of what she's doing and hopefully
it will work but I've also seen Intel
pull it back once or twice you know and
disable those instructions inside of
chips that they've released so I'm not
entirely sure that it's 100% ready I
don't know I don't know anybody actually
leveraging it is anybody leveraging
Hardware transactional memory yet say by
leveraging software transactional memory
you might be an ask for closure or
something like that right and it's so
long as like I said you don't have
overwhelming amounts of Rights taking
place it can work great pessimistic on
the other hand is the idea that I'm
gonna lock something I'm gonna do my
operations I'm gonna commit nen I'm
gonna unlock and that sounds you know
great but it comes to his own cost
however you can so long as in a
distributed context you don't have nodes
going down or in the inability to reach
them and then worry about how you're
gonna deal with the lock that it's
holding you know how are you gonna
compensate for that and you've got you
know really great people working in that
space and solving that problem for me so
I don't have to this is why I want
databases this is why I want Kafka so
the solutions that are out there that
are worth considering at least in this
new space are largely available across
these cloud providers I'm gonna go
through them sort of one by one and how
we personally evaluated them and what we
were thinking
Amazon Arora how many people have used
Amazon Arora so far a couple it's MySQL
in the cloud which is great actually to
a certain degree because when you think
about it it means that you get the
ability to almost plug-and-play your
application directly into it it's not a
completely different way of interacting
with the database than you were before
and the SLA around Aurora is actually
pretty significant nine nines of uptime
being guaranteed in a MySQL
system I'm not sure I could get nine
nines of uptime myself trying to run it
that's not easy to do the compatibility
with the existing solutions like I was
saying but the one thing about whenever
you think about these kinds of systems
based on a master and replica type of
architecture is that you're still always
bound to what the master is doing and in
that particular case what if you know
the size of the box that it's running on
isn't big enough and you need to swap
out for one that's bigger well then you
have to take the master down and that
does imply downtime to swap it out for
the new one and then you have quorums
four reads writes across you know
available availability zones which is
fantastic it's multi data center your
performance limit is to the bound of the
masters you know box it's running on and
this is really giving you pessimistic
locking so you have throughput at the
cost of latency how much time are you
going to wait for somebody to unlock
what you're trying to also update
vertical scalability no sharding that's
difficult what if your data set is
larger than the master what if you need
to be able to scale across multiple
nodes and then you really can
so Aurora is actually something we are
considering for our architecture because
if we're gonna use Cassandra for the
majority of the cases of all of our
microservice architecture you know they
each have their own dedicated database
space that they're using but we can
limit our points the view of our points
to you know a MySQL in the cloud
instance it might work really well the
question is whether or not the business
is willing to give up the idea that
we're gonna have downtime for
maintenance whenever we have to switch
from one master to another and that
there is a finite limit to the amount of
you know records that we could store in
it since it's only points it may not be
quite such a big deal it's not all the
data that we're trying to put in there
but we're still evaluating it from that
perspective
Microsoft Kosmos anybody ever used that
it's used to be document DB so it you
know it's not a database it's totally
brand-new it is being marketed in a
completely different way however and
it's decorative
consistency this is the idea that you
get to sort of choose the level
consistency you want to pay but at a
global scale and this is where we are
seeing people starting to let go of the
idea that the cap theorem was the Bible
you know Eric Brewer himself back in
2014 started making noise through info
cue and other channels about how cap was
really not entirely a binary choice and
when he was talking about it I thought
he merely meant that whenever we were
looking at Cassandra that if we tuned
our settings around you know the
replicas and the in the quorums and the
event and the consistency level that we
were going to have a you know a tunable
line between consistency and
availability but that wasn't it at all
it's really where he was going next
Azure does have acid style transactional
applications to it and like I said it
evolved from document DB if you like
having that enriched kind of view of
data that's based on a document of
here's all the fields inside of it and
also with the value for it it's very
much like a Mongo store and Mongo is an
interesting case in and of itself
because you look at how it recently
passed the Jepsen series back in I think
February and this is a really big step
for Mongo because replication and Mongo
wasn't exactly well respected losing
rights was something that people just
kind of accept is the cost of doing
business but if you were using Mongo as
a cash to hold just data temporarily
while you were doing operations it could
work pretty well in that context that
said now that it's passed the Jepsen
series is now reliably passed and what
we would probably consider the most
authoritative test suite in the industry
for replication at least publicly
accepted there may be others that are
not like available for all of us to use
and that does not however take into
account sharding and it also does not
take into account performance so when
you think about Jepson don't
automatically associate those properties
with those tests the declaratory model
is nice you don't get uniqueness or
foreign key constraints which you know
isn't a typical I think of document
stores you also don't get distributed
transactions how many people would do
distributed transactions now all right
one do you do sagas sir
do you do the saga pattern or anything
like that or using like XA or something
back in the day or oh you're using
airline okay and you amnesia or okay
when it comes to distributed
transactions I mean we all know that
there are costs associated with them and
historically speaking I wouldn't say
that many platforms have had a lot of
success with them err Lang is a platform
that is unique in those four kinds of
regards but when you talk about
distributed transactions ordinarily
this is a sort of thing where you have
to implement a saga pattern and by that
I mean export with the transaction the
way to roll it back to the store itself
because what if they can't talk to the
rest of the world and it can't say am I
in consensus with the other place that
had to be updated at the same time I
need to then pull it back if you're
looking for a really great talk about
sagas I recommend you check out Katie
McCaffrey stock she formerly worked at
Twitter but she's just recently joined
Microsoft Research in Seattle I'm very
excited to have her in our town again so
really excellent talking I'm sure if you
just look up Katie McCaffrey and and
Sagas you'll find it now we're getting
to the interesting part where we start
talking about the really crazy news
stuff and that's like Google Cloud
spanner anybody read the spanner paper a
few people Martin I'm not surprised
Martin Clubman from from conflict
so spanner is the idea of global
consistency as well but with a
completely different view than where
cosmos was trying to take it and you
have globally distributed consistent
data Eric Brewer the writer of the cap
theorem is not working directly on
spanner but as he says he's a big fan of
it in this particular case it's only
available on GCP and obviously Aurora is
only available on AWS Microsoft cosmos
is only available on as you are these
are limitations for me like I can't
deploy to GCP right now our past is over
AWS we're not going to be cloud portable
is not a requirement or a concern of
ours but that does mean that I don't get
just a cosmos or spanner but I really
like it it would be so cool one of the
things about spanner in particular and
unfortunately this part is proprietary
is the concept of true time we're using
Google's wonderfully fast network they
can sit down and figure out order at the
read node at the leaf how to order all
the transactions have been committed
across all the nodes on a global basis
now
I also don't have a use case where I
personally have to worry about global
consistency I do care about it within a
region because of things such as you
know global data protection regulations
GDP are I cannot be deploying a view of
United States data in the EU
Japan's data has to be separate from you
know other countries China I mean I
don't even know what kind of access I'm
gonna have for that sort of thing so for
me in particular this isn't this isn't a
concern at the same time having within a
region within the United States within
Canada within Japan within you within UK
global consistency in a distributed
context would be fabulous because then I
don't have to think about that and the
race conditions that might result inside
of my application now mind you when we
talk about these sort of things and also
ordering one of the tricky things about
this is it makes no guarantees about hey
the order of when the transactions
arrived
it can only disambiguate based upon what
came into it now when it comes to
spanner the tricky thing is you have to
use their way of talking to it and they
do have a new SQL kind of syntax but
like CQ oh it's not a one-to-one match
to you know the the ansi SQL that people
might be familiar with that said how
many people got to work with the
database it said they were ansi
compliant on SQL anybody 100% i mean not
everybody really is at some point you're
dealing with some sort of I don't know
dialect I use this pessimistic locking
which it gets away with again because
they've got a very fast network behind
it that the majority of us don't get
access to wouldn't that be great five
nines of availability
with strong consistency and by strong
I'm saying that whenever I read I am
guaranteed to get the value that I
expected no matter what note I wrote to
that's fantastic
and spanner and cosmos are also offering
the same basic SLA surround throughput
and latency and consistency which is
really really amazing I mean they're
there they are bringing us in by
offering levels of service that we might
not be able to even achieve on our own
now the tricky thing about those sorts
of solutions as I alluded to earlier is
whether or not they're going to be
secure and then there's also the cost of
how local they are to my application I'm
going to have transit time trying to
connect to their databases wherever
they're located or they can be really
close to my app or not and I really
don't have guarantees about that nor do
I necessarily want to frame my
application around those sorts of
concerns I want that kind of portability
in that regard I don't want to be
sitting around trying to optimize around
the locality of the database so you
start looking at things that you might
be able to install on your pass yourself
and you come to things such as cockroach
DB is anybody played with cockroach DB
yet this is okay one person there this
is basically saying that we can have an
open source spanner which you know
sounds really exciting to me
I want a database that maybe I can
control in my own context and have all
the control of the data and not worry
about the security but because cockroach
DB can't use true time it's kind of left
in a lurch and Chris and I Chris Michael
John and I were talking about this in
Seattle just a couple months ago and he
had this really great explanation I'm
not going to try and say verbatim but
the basis of the time mechanism inside
of cockroach TV is NTP and there's a
certain amount of drift that occurs
across time across every one of the
boxes that this database is going to be
installed on and that means that the
database in order to figure out how to
manage that consistent view and order
has to also figure out how to deal with
drift and that means you have to pay a
latency cost at least at the current
I'm not sure how much further cockroach
DB has gotten at least things that they
publicly published around how much
they're progressing in this regard but
that I think the general rule of thumb
is it's roughly on the order of 200
milliseconds and those are expensive
costs to pay for a database read or a
database update so it did pass the
Jepsen tests again this is the you know
closest thing we have to an industry
standard around the consistency of a
distributed database that's fantastic
but because of the costs of the latency
this is something we didn't want to
consider now we get to follow DB and now
I want to talk about how this relies
relates back to you know PAC L and Dan
your body fauna is based on the Calvin
paper and the Calvin paper is basically
saying that I'm going to write all my
transactions into a distributed log and
you know just for reference think of it
in terms of if it works Kafka right
we're all used to the idea of putting
something into Kafka and when it goes
into the log at that point I'm going to
pay the cost to figure out what came in
in what order and then I'm going to push
it out to all the read nodes so by the
time it reaches a read node is already
guaranteed to be ordered and you know
consistent assuming though those nodes
go down now if the nodes do go down it's
falling into the pack out realm we're
saying now we will pay the cost of what
we do and availability versus
consistency but they're in essence
saying that the nodes that go away are
not part of the ring anymore and you're
not going to be making requests from
them
it is transactional with single phase
commit across thousands of Records and
indices and then with an optimistic
locking approach instead of estimate
stick which is where you see spanner
going I can't say that we've deployed
this into production but we have been
doing a pretty heavyweight POC and we
have written on order of a hundred
million transactions to this database so
far and not seeing a single consistency
issue despite taking down notes which is
pretty impressive
it's serialized and it uses RAF to you
know manage the order of the updates
it's linearly consistent
it's got hierarchical row-level
authorization for tenants which is
actually a really cool thing for us
because if you start looking at
Starbucks's business model you go into
Starbucks and you actually don't know
who owns it we sold all of our stores in
Germany last year in the UK it's largely
franchises which means you don't have
large organizations running it but maybe
if the one group of people owns a couple
stores and a lot of that has to do with
the fact that in a lot of respects we're
also a real estate company like we have
to manage real estate and in the UK it's
not a viable proposition for us as far
as a business model goes in Korea were
joint venture in China we were a joint
venture now they're all company owned in
the u.s. we have various licensees such
as stores like Target and various
supermarkets and stuff like that and all
this layers into a complexity of a
business model it makes it really
difficult to say are we not going to
have to manage tenancy inside of our
database such that we say this is
target's data this is amorous data for
Germany we need to be able to think
about tenancy in a completely different
way it supports multi multiple indices
which is great for reads you don't have
to come up with I mean it does anybody
here dude like CQRS the idea of command
query responsibility segregation where
you're saying I'm gonna have a writable
store I'm gonna transition that data
into some sort of readable views and
those readable views end up being a
non-relational real quick access kind of
thing based on some key can I get a
whole bunch of values and in the case of
fun I really don't have to do that
because I can use multiple indices for
my queries
it supports temporality which is also
good for audit and rollback David Nolan
which is talking about this as far as
you know des Tomic goes being able to
say that I want to get the data as it
stood as of this time and it can do that
and it has you know supports graphs and
I think part of this has to do with the
the fana team the team that built this
they really came out of Twitter and
their underlying philosophy for what
they were trying to achieve was we built
a whole bunch of specialized stores for
you know Twitter and you saw a couple of
them like flock DB which is a
raph database right and they said well
what if we could just build the perfect
one
now that automatically starts setting
off alarm bells in my head because there
should be no such thing as a database it
does everything great personally I don't
care too much about graphs I'm not
trying to figure out the social network
of the coffee drinkers at least yet it's
not a requirement anybody's actually an
put on me yet but that said it does have
these capabilities and you know if you
needed to leverage them and you could do
it within a single store but let's think
about some of the things that are kind
of a negative and one is that fana DB is
a closed source database and to be
honest with you this is really the trend
in open source it's unfortunate reality
that there just isn't much of a business
model around selling free stuff and I
know this from the time that I spent
four and a half years at typesafe light
been you end up spending a tremendous
amount of time coming up with products
that a lot of customers can't leverage
so for example Fawna DB does have a SAS
based solution that they're hosting
allowing me to theoretically just use
their database and then I've got to get
into all those questions around security
and how they're gonna manage my data and
make sure that they're following PCI DSS
and make sure that they understand sock
- type - how many startups whenever
they're like 20 people actually have
that capability or knowledge now through
a certain degree they do but it's still
not a risk and and all of the evaluation
we have to go through means and we would
only be deploying this in our own
footprint transactions are limited to a
single function and to get into that and
into the semantics is how you're doing
your queries and your updates it's with
their own query language now the very
much like what we saw with day Tomic you
can adapt to a query language because
typically they have very similar
semantics but it does have a training
overhead for your team and whenever
you're talking about a large enterprise
like Starbucks these are the sorts of
things that managers are gonna sit down
say cost cost cost cost it's a new
product it's growing maturity there's
one publicly disclosed you know user of
it right now and that's Nvidia I do know
of a really large financial institution
that is evaluating it much as we are
that's also in the United States and
because of this lack of existing users
of course that means that the case
studies for managers to feel comfortable
about making a choice like this are thin
and of course you're beholden to whether
or not Nvidia is willing to talk to you
about what it's like to actually
operationalize this thing it's not
verified by Jepson but in this case evan
weaver the CEO made the conscious choice
not to do that as a personal friend of
Keio Kingsbury in that particular case
he was saying yeah we agree that Jepsen
is pretty good series of tests but we
want to go further and test it deeper
and there aren't many people I would
trust to say that they understand what
further and deeper means than Kyle
Kingsbury's deaths and Jepsen
buteven is one of them and we did look
over the tests and run the tests
ourselves and we were generally
impressed but one of the bigger problems
here whenever you're talking about
adopting a new technology like this is
what happens if it doesn't exist anymore
Apple acquired foundation DB I think in
2014 or something like that and it was
an open source database and all the
sudden boom done no access to it what
are you gonna do don't know if you
installed it you don't get maintenance
updates anymore or anything like that
unless somebody picks it up and starts
running with it you see what happened at
bash oh I was a huge react fan I used
that at large cable companies in the US
and it worked extremely well for us and
making read views and stuff
unfortunately with bash ogon now you see
I think is it Lmax or Betfair one of the
two of them picked it up as a database
that they're now providing at least
financial support around as the
community reorganizes to support it and
then you see we think DB which pretty
much just kind of disappeared because
again business model or free software
it's very difficult but getting into
that business model around free software
whether Kassandra data stacks you know
it's obviously a big vendor for us and
and we're extremely impressed by
Cassandra I mean who wouldn't be it does
amazing things when used in the correct
context but they're more and more moving
away from the open source project
because of how hard it is to sell
something that is free and at this point
their contributions the open source
Apache project of
they've really dwindled to a trickle if
anything at all some of this has to do
with the business model question other
parts of it have to do with the politics
of being part of the Apache foundation
and whenever you control the technology
you have to be able to give up that
control to be as part of that foundation
and and those dynamics aren't always
positive for the companies who are
trying to make money and support this is
a long-term endeavor so cassandra is
actually starting now to talk about
their own close source solutions that
move away from the Dynamo paper and
while I was supposed to have a meeting
with them to discuss their roadmap
before this unfortunately I didn't we
are gonna meet probably in the next two
weeks either the 17th or the 20th and
after that I'll have a much better feel
for what it is they're going to be doing
in order to support different semantics
it's around you know Sandra but until
then I'm sorry this slide is
unfortunately blank nonetheless
open-source is a really tricky place to
be as far as selling things and what you
see vici is doing they're investing in
these companies and telling them they
have to be product companies you can
have consulting and services around them
but they have to be limited to 30% of
your overall revenue and 70% has to be
generated from product and the reason
for that has to do with valuations which
is the number one thing a VC will care
about am I going to have a valuation of
a services company which is on the order
of 1 X to 1.5 X or am I going to have a
valuation of a product based company
which they see is as high as 10 X so
obviously there's a big incentive for
them to have you pushed toward a non
services model and that's actually
restrictive if you ask me toward the
adoption of so many of these great
technologies because there's less people
out there to help you do it every single
one of the companies I work with said I
just want somebody around who can make
sure we're doing the right thing and
unfortunately if you were being guided
by a VC you're not allowed to provide
that service and instead you've got to
build a whole you know ecosystem of
other providers who can do that for you
so this is a tight space that you see so
many of these companies in so that's
really my talk does anybody have any
questions
just to clear up the comment that you
made it's it's actually better 365 and
purchased butters IP okay and the
intention is to as well as support the
community we're building a whole team to
take the development forward along with
several other organizations including
the NHS which you know this is actually
a really great thing for open source
where you see a company almost adopt an
open-source project and say we're going
to be the financial and community
supporters for this right it doesn't
mean that you as a user are going to get
the features or fixes that you want
because let's face it
Betfair being the sponsor you know they
get three say they get to make the call
and that's fine I mean you're still
getting access to an amazing tool that
said let's look at say Neddie Neddie
right now is largely kind of controlled
by Apple because the primary person who
runs that project Norman Marr works for
Apple so Apple kind of gets first dig at
what they want inside their now mind you
we all benefit from the things that
Apple puts in their write the things
that Norman is doing to support Apple go
into the open source project and that's
to our benefit just you know there there
are caveats so but thank you to Betfair
for doing that it's 56 form of it I'm
sorry
it's bet365 bet365 I'm so sorry I'm
totally mentioning the wrong company
it's bet365 is doing it my apologies hi
with respect to Cassandra have you've
been looking in to ask allah to be as a
alternative Sylla I don't know yeah so
Scylla DB is supposed to be the idea of
Cassandra but it faster right which does
affect the latency toward eventual
consistency right which is great except
it's still a dynamo based database right
and it still has the semantics of
eventual consistency at the X you know
trying to give you availability at the
expense of consistency to some degree
and
if there was a deterministic way to say
that I was guaranteed to have an update
within so much time then fantastic but
nobody's willing to make those
guarantees even at the increased
performance of Scylla now mind you if
I'm writing an analytics platform or
something like that and I need to have
quicker access faster consistency or at
least gossiping and stuff like that
Scylla is definitely a platform to
consider but again it's not the problem
we're solving and in this particular
case for Starbucks Cassandra was
something they'd been selected even
before I showed up and it was the
database we get period so yeah I just
had a question about so I mean the
funnel story is nice right Kelvin is
nice but the question I have is if if
you build assuming this Geo scale single
like you know strongly consistent view
whatever strict serialize ability what
happens when you reach the max
throughput like where do you go from
there yeah what's the operational story
like because then you probably have to
decompose things in the services right
and so maybe why not start from that way
and build up well I mean what's uh what
are your thoughts on that okay so you're
asking first if you know what do we do
whenever fauna reaches its maximum
theoretical throughput and stuff like
that and again because we're limiting it
to a single storage of one kind of value
the points which are associated to some
member or membership ID we don't in our
testing we have not run into that limit
and our testing has been significantly
greater than our actual throughput and
you know it hasn't been a concern yet if
we ran into it what would we do I don't
know I mean we'd have to start thinking
about how to reorganize our actual
system in order to take into account the
limits of the database but I don't want
to do that kind of complexity now the
scale like I said is on a relative scale
not that great I mean 91 million
financial transactions a week it's
nothing to sneeze at in one market and
it is a company that has a 30 billion
dollar revenue every year
and the market cap of almost 90 billion
but at the same time it's not high
frequency trading right so I'm lucky
that way
why is it I know all these people hi
Jamie hi Chris
so if I'm remembering the Google spanner
paper correctly I think with true time
you have the same kind of problem that
cockroach DB has where you've got like a
minimum bound on your latency right it's
true but because kind of numbers are we
talking about that what what is that
bound for say Google Cloud spanner what
is the actual number yeah I don't
remember but significantly lower mainly
because of their speed of their network
so and also think about in terms of how
they're doing this they're doing this at
the leaves right the disambiguation it's
happening at the Reed nodes themselves
with pessimistic locking and the only
reason they can get away with that is
because of their network but you know I
didn't remember the actual number in the
paper myself okay but it's a reasonable
number you could actually certainly much
less than the 200 milliseconds that we
were thinking of in terms of you know
cockroach okay good any further
no it's the host directly at the end
thank you very much thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>