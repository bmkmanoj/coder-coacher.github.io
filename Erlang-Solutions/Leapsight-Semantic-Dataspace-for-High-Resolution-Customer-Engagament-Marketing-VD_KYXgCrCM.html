<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Leapsight Semantic Dataspace - for High Resolution Customer Engagament Marketing | Coder Coacher - Coaching Coders</title><meta content="Leapsight Semantic Dataspace - for High Resolution Customer Engagament Marketing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Leapsight Semantic Dataspace - for High Resolution Customer Engagament Marketing</b></h2><h5 class="post__date">2014-06-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/VD_KYXgCrCM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello and welcome to the airline
solutions monthly webinar my name is
Laden military chat and the solutions
director here at airline solutions
today's webinar represents a
continuation of a series of webinars we
have been running across topics of
interest in world overlying and dealing
with solutions based on the airline
programming language today we will talk
about the leap side semantic data space
and dicks this next-generation database
offers a bleeding edge approach to
marketing concepts tools and campaigns
that are truly tailored to each
individual in audiences that can offer
number millions and all of this will be
based and is based on an airline
framework offering the massive
scalability and the extreme fault
tolerance and concurrency needed to
deliver such performance as with any
live event please excuse any technical
issues that we may encounter today now
to start by telling you a bit about
airline solutions we are a products and
services orientated company completely
devoted to the airline programming
language since our founding in 1998 we
have worked with organisations and
individuals using airline helping evolve
the language and supporting people and
businesses using it today we have about
80 people across our offices in london
stock on crack hog budapest and most
recently seattle and working on projects
across the globe we are very keen on
creating value and competitive advantage
for our customers across industries and
truly unique features and
characteristics of airline gazzard
programming language as you might guess
we are ambitious in development of
airline based products and we work to
create lasting partnerships with
outlines the products and solutions we
market include Mongoose IM which is a
massively scalable messaging platform
the react distributed data store and
other solutions applicable across
sectors and problem areas where aligned
as a language makes sense I am
particularly pleased to say our speaker
today is Alejandro romulo the co-founder
and CEO of websites and the creator of
website semantic data space prior to
leave side Alejandro was the chief
technical officer
at british american tobacco and then
chose to pursue his ambition of
delivering the next generation airline
based data store as foundation of
individually tailored marketing efforts
please allow me to finish by saying you
are welcome to post questions throughout
the duration of the webinar by using the
chat facility our speaker Alejandro will
answer as many questions as time allows
at the end the webinar if any questions
do go unanswered you are welcome to
raise them by an email using the
following address that's webinar at
laing hyphen solutions com if you're
interested in learning more about
websites semantic data space or airline
will just wish to establish whether they
could be a solution for the challenges
your own business may be facing then by
all means please feel free to contact
myself directly my email address will be
displayed in one of the final slides of
the presentation we will share with you
the same goes for any other questions
you may have feel free to contact us I
would now like to hand over to Alejandro
roam all over we'll be glad to start us
off I hello everybody thank you very
much first of all in London and hold
Erlang solutions team to for having me
here inviting me and thank you all guys
for and and girls for being here so
we're going to try and rush through the
first number of slides we don't want to
speak a lot about who we are but just a
brief firm as a context to let you know
why are we doing what we're doing and
then go through the challenges that we
face in the last 10 to 15 years as a
team developing front office solutions
for consumer goods and and how we
decided to take on those challenges in
terms of developing some technologies
that we're now presenting so a little
bit more about myself as in love and
said I I had like 10 in the last 10
years I spent most of my time working
for bridge American Tobacco and before
that I work for other consumer goods
companies as well I being architecting
marketing technology solutions since 97
we specialize as a team as well as in
consumer engagement trend marketing
sales and distribution
I'm saying technical point of view we
arrived at airline early you know early
in 2010 i would say before that we were
pretty much concentrated object oriented
technologies I've been doing or object
orientation since the days of open step
we've been watching them the next step
platform so who we are so we are we
define ourselves as architects of new
solutions and technologies to help you
dig more accurately and more deeply into
consumers world to engage and create a
nurturer long-standing long-lasting
relationship with your brands and we're
going to expand into this topic and and
and understand how this differs from the
typical CRM processes that we thought
you might know and you have to say about
leap side that we started operations in
2011 here in the UK and in Argentina we
have a a strong background in
architecture and implementation of
bespoke scalable high availability of
solutions for marketing and we typically
engage with customers doing you know
from strategy and architecture
consulting all the way down to
developing this platform in particular
through our last 10 to 15 years in
bridge American Tobacco we envisioned
and design and implemented a platform
that has no equal in the consumer goods
environment so basically since 2001
we've been engaging consumers on
one-to-one basis and tailoring the
office to those consumers in real time
through multiple touch points that is
brand website interactive kiosks
integration with fossil services and SMS
and email the the other consumer goods
companies are still not jumping into
these and basically we needed to go into
this environment because we were losing
all communication channels with
consumers due to advertising
restrictions and therefore we decided to
go and implement this one to one
solution now but by the time we we did
these obviously we started using the the
tools that we had at hand although we
started innovating in that in some of
the areas we used java application
servers object relational mapping
we spoke real engines so on so forth and
what we realize what we started with
Lipsyte is that we couldn't go this
route again I mean starting to compile
all these different technologies and try
to work them to make them work together
is difficult enough and if we if we then
need to to be very you know flexible and
agile in order to anticipate what the
consumer needs and in order to satisfy
their needs it is very difficult to do
that in that you know particular
environment so what we decided to do is
to create our own platform and that's
what we're going to try and cover as
soon as possible so a little bit off in
the history of this say over the last 60
years marketing has been evolving
towards a higher resolution or fine
grain approaches so from the lowest
resolution that it was epitomised but
Henry force phrase you can have any
color of Model T as you as long as it's
black to the latest customer
relationship management technologies and
enabling this kind of marketing now
despite this consumer oriented marketing
has always maintained a low resolution
push approach that regards consumers
specific targets of mass marketing
campaigns now even state of the art
medium resolution methods such as a CRM
customer relation management are still
rooted in this in this bush approach so
basically we all know all these tools
existing today and basically what they
do is the organizing campaigns around
pushing content in email SMS or other or
personalization of content and website
but there is no real understanding of
what the consumer wants or needs and
there is no real development of a
relationship between a brand and a
consumer now this push marketing
unfortunate doesn't work anymore the
increasing number of channels enabled by
technology and technology revolution the
last decade has produced a fragmentation
in media which eroded the effectiveness
and efficiency of these approach now
also in the last decade internet
globalization mobile and social media
have had a profound impact on the
behavior
of consumers and these forces have
generated our bundles of options and
ease of access to information which
empowers these consumers to exercise and
present levels of choice so it's very
difficult to certify this consumers
nowadays and in trying to do that
consumer market ears are exaggerating
claims about the performers and
differentiation and that leads to low
trusting company generated advertising
and more and more consumers trusted
peers as opposed to the bronze in terms
of what brands and what what it means to
them and word of mouth has become the
most trusted form of advertising so we
had this problem before in a different
way when we were working in the tobacco
industry basically media disappeared
because we couldn't use it so we realize
we needed to use another means of
engaging consumers and we started using
a pull approach so basically instead of
pushing content to the consumers we'd if
we design a number of mechanics that we
will use to get the consumer going or
coming to the brand and and apart from
that the other key component is a mass
customization of that brand experience
so we need to understand who this
consumer ace and go down to the
individual consumer level and try to
figure out what is it that this consumer
wants now that is what we call high
resolution consumer engagement and we've
been doing that for more than 10 years
and we decided to create a company to
explore these opportunity and help other
companies overcome the problems that the
or similar problems and also tap into
these opportunity now this kind of
approach doesn't count without
challenges so we're talking about a
real-time environment high concurrency
basically you're talking about creating
a platform and a capability similar to
the ones that Facebook amason and the
logs are using to engage each one of us
now traditional enterprise technology is
not fit for purpose you cannot use
station or databases and traditional
application service to do the
and that is what Google Facebook and the
likes have already found out like 10
years ago but what is most interesting
about all this is that the future of
marketing therefore is in distributed
concurrent and dynamic and intelligent
systems and marketing therefore is
becoming the most computation a
significant processing the enterprise so
let's see how we are trying to solve
this problem today I mean if you go into
if you work in a cosmetics company this
is probably your typical scenario your
typical architecture so basically you
create new touch points to engage with
the consumer point of sales applications
mobile applications web call center you
also use Facebook Twitter and social
media to engage with them but through
all this but what basically is happening
is that you have the version
capabilities all these touch points
probably all the structures are using
different technologies you want you know
that the application the mobile
application is using a PHP backhand and
the web is using a Java backhand and
different databases the data elements of
the data models are not compatible so I
don't have a holistic view of the
consumer through all the touch points
and I have different mechanics and
different rules implemented in all these
Dutchmen and therefore my capabilities
are not the same what this generates is
a generic and irrelevant and
inconsistent brand experience and
because trust is the most important
thing when it comes to to a relationship
with the consumer and because having an
inconsistent experiencing you know it is
going to affect trust this is a key
component of of the of the consumer
engagement strategy so how can we solve
this so 10 or 20 or 12 years ago we
solve this and ability and basically
what we did is a platform that will
handle all communications through all
touch points and basically if you do
that you have a single point of access
for all the consumer information and you
have a single point of access also to
all the mechanics and all the
capabilities and that Lily resolve
obviously a customized if you're if
you're intelligent enough and relevant
and consistent experience to that
consumer
today we're going to concentrate on just
one element of these and that is what we
call the sure knowledge space and this
is not only about data but what we
realize is that we needed more than data
we need to store the rules or we need to
have a more knowledge management
approach to to marketing now this is all
lazy we've done this before so when we
set to do these we started with you know
with the same ideas on the same patterns
and the same designs that we did 10
years ago but then we realize a number
of things the first one is that today
the web is the database this idea of
creating your own database is obsolete
right so if we are going to engage with
consumers we need to understand where
these consumers are and what is the
information this consumers are living
where is the digital footprint of this
consumers they are living in Facebook
and Twitter YouTube and all these
different websites so our word data
management capability needs to be able
to ingest these data from different
sources sources that we don't control
and and be able to amalgamate all of
that Andreessen without without data now
the the extreme challenge that this
basis is that you know before in typical
Enterprise IT what you will have is in
an ETL process and extract and form a
load process or an elt and you will get
all this data from different sources you
will transform that in a batch process
and then you will store that in a data
warehouse now the challenges as you will
say this doesn't work anymore because of
the number of sources that we have
because of the frequency of the data and
because of the volumes of the day as
well another two elements I'm going to
refer to that we think that we we learn
in the past 15 years so how can we help
can we cook with 81 fortunately in order
to do that there are some guys are
already been working heavily on that and
the first thing that we need to consider
is we need to delegate this type of work
to the machine if if we follow the
normal marketing route of you know
putting people in the call center and
sending people in the in responding
emails and trying to respond to social
media
wouldn't work so we need to be able to
automate these using machines now
several companies have been producing a
number of innovations throughout the
last 10 years and if you are an airline
user you probably heard about most of
these in most of the reason
presentations and if you are a react a
bath she'll react key value store user
or a Cassandra user or a MongoDB user
you will know all of these concerns as
well so basically what we turkeys we
started working with these approach
trying to merge what we did before with
this new world of technology now just
before we go into into our technology
development a few a few more concepts to
to to paint the picture of that other or
to complete that context so everybody
knows about big data and big theta is
defined today as a combination of any
one of those things right the volume of
information the velocity of that
information coming into our systems and
the variety of the information from
different sources different syntaxes
different schemas now while doing
consumer engagement we learn to other
elements and there are key in being able
to understand the consumer and engage
properly with them the fourth element is
the veracity because of these different
data sources we need to understand the
data is incomplete is incorrect and is
ambiguous now systems traditional
systems don't know how to deal with this
a relational database is a clothes on a
closed world assumption in which
everything is true or false and whatever
is not in the database is false or
whatever is there is true there are no
degrees of completeness or correctness
and this is something that we need to
fix if we want to deal with the world in
the way it is and the the fifth element
is validity now the database in normal
that it will say this is valid and this
is a valid forever now we understood
that this is not correct and certain
changes and in the consumer
specifically life the lifestyle changes
or life cycle changes affect the way you
need to care about you need to operate
and therefore you need to be able to
recognize when a fact about a consumer
is valid right what is a period or
interval in time in which these fact is
valid so just to clarify that little bit
more imagine that you have this person
is called Mary and you gather
information from mary from different
sources so I'm covering i have already
store information my database i gather
information from facebook twitter google
and i find a number of issues there so
the first thing is that according to our
database mary m is married right but
according to her profile in facebook
which was updated one week ago she's not
right so what shall we do now it depends
on who we are what is the product that
we're trying to to to serve and
communicate other brands try try to try
to communicate but in might be the case
that the attribute is very important to
us and it might have legal implications
so it is very important to us to be able
to isolate this too fat and be able to
associate metadata to these facts and
and and when i'm going to recent about
the facts be able to use whatever
information whatever fact is right for
the occasion right so basically what we
need to add to this is a provenance
capability so i need to separate these
elements in different namespaces so that
i can flag and say well this is data
that I brought from facebook this is
data that I store or or mary has gone
through some kind of registration
website or in a touch point and she left
that data with us but also i need to
associate what is the degree of veracity
or accuracy or probability so i'm going
to say now that you know because you
know we've had a long religion with mary
and we know she's been married for a
long time we're gonna say that we trust
our data a lot so that's hundred percent
of rasa tea now facebook you know we
don't
really we need to we need to try that
out in the case of Mary we're not so
sure when therefore we're going to flag
this with the seventy percent apart from
that I need to say at least that these
record is valid scenes this moment in
time we're going to see later on that
there are other ways in which we can do
that and more information that we could
actually store and just a complete lease
picture so fine so so we have this data
complexity and big theta is one element
we can take the five element that we
decided that we shown before and we have
a complete big data framework that we
can work with and so fantastic so we
have now the means are fun the
understanding to be able to gather all
this information now there are two other
elements that we mentioned before that
come together and create the perfect
storm and those are the business logic
complexity remember we're moving away
from Porsche and into pool and we need
to develop new kind of mechanics to
engage with consumers and those
mechanics can be loyalty programs
ruffles instant wins but then the key
aspect or the key congregation of these
is that basically i am multiplying my
business logic complexity both from the
management of the business logic in in
hands of the marketeers but also from
the development environment for the
developers and apart from that we have
the real time and always own
capabilities right so the listing these
consumers one nowadays because they're
not used to Amazon and Facebook and all
these different platforms they don't
want a you know they don't want to be
interrupted in their operations or the
way when they are engaging with the
brand so basically what you need is all
these touch points up and running all
the time now big data is not solving
that issue a big deal is just a
fantastic way of generating insights but
what is lacking in marketing is the
capability to react in real time so
the typical issue that we have in
marketing is not big data everybody's
jumping into big big data wagon
investing me big data but the issue is
most of the marketeers have no means of
executing that or they insert that
generated with with with with that and
therefore we decided to concentrate on
this aspect which is what we solved 15
years ago in a different way from with
different technologies and what we learn
is there are two main loops and this is
an oversimplification of our blueprint
but basically there are two main loops
we need to completely automate the
interaction with the consumers but every
time we interact with them we need to
feed our knowledge base so that we can
start learning and adapt and and that's
the second loop that we call the
learning loop and in order for these
loops to be working together end-to-end
to be able to do this and to react in
real time we need to share these Noli
space between them and this is
completely different to what crm
technologies have been doing the last 20
years you know CRM talks about
operational CRM and analytics crm and
you have the operational data store and
you have the data warehouse and then you
have processes that move data from
dinner warehouse in a batch way into the
transexual one that we proven that
doesn't work anymore right so what we
needed to do what we did at that point
in time is basically generate data miles
on the fly using an off-the-shelf bi
solution but what we decided to do this
time is because of all the elements at
we mentioned before is to create a new
kind of platform that can enable this
architecture blueprint naturally and and
for that what we need is basically this
automation group to respond to events as
they occur feed that not using the
knowledge base to do that that what it
means is I need to use the data and the
rules they already learned to be able to
automate that interaction with a
consumer I'm gathering those events as
well and i'm going to use big data to in
the learning loop to generate more
insights and i'm going to feed that
into the knowledge space with more data
more facts some of them summarize but
also with note with new rules so we form
a kind of continuum that orchestrates
all this to these tulips to be able to
satisfy the needs of the consumer now
which are the requirements for these
things say basically we're talking about
dynamic data model because we are
engaging with consumers and we done an
inky most important when we start doing
this we don't know exactly what
information what facts we need to store
about the consumer so we need to be able
to change that very quickly now if we
went for an architecture that usually
say traditional relational database +
object relational mapping + middle were
every time we change the data model we
need to change all the different stacks
and that is too slow for marketeer by
the time we finished with all of the ec2
it took us six months and the
opportunity for the market here
disappeared the second element is
relationships we leave now in the world
of highly connected data this is not
only because of social media become
because of the important of
word-of-mouth and importance of being
able to map who are the friends of these
consumers and wire is the influence this
consumer has their own network but also
increasing we have devices that connect
with our devices on with people so we
need these new graph capabilities this
capability to relate everything to
everything and we also need this data
layer to be programmable ideally
functional relational and logic right
because it's the right way is a
declarative right way to do that in
elysian we need low latency and we can
achieve that by eliminating that
impedance mismatch with that we talked
before the razor database is the option
region mapping the rules all of those
require bridges to be build between them
to be able to work together and that
adds latency and finally we need to deal
with the 5v so with the ambiguity with
the uncertainty with a validity plus the
availability and and and and and of the
of the of the touch points so basically
because of this we created the lips I
semantic data space or
energy for short and by the time with
these we thought really this was like
marketing on acid so we've been dreaming
about this platform for the last 10
years and we couldn't do it in you know
in the corporate environment that we
left that corporate environmental to
realize our dream to build to build this
and so what is LSD so and this is a
distributed scalable falter ended active
semantic data space now I'm going to go
into each one of the elements i'm going
to start digging into it and then try
and do some riff demos to show you what
we're talking about so as masterless and
it's Joe nothing so basically you create
a cluster of notes and those nodes have
no master the peer-to-peer that
replicate their use of puter p
synchronization and we in this in in the
cause of allah see we adopted the
eventual consistency model and this is
is very interesting because we we
started working with these when when all
this idea of eventual consistent was
being challenged as well and to us was
really important because in our world
what we want to do is to be able to
create a knowledge-based / consumer
right I'm not interested in processes at
the beginning we're not interested in
processes that span multiple consumers
those are processes that get that
typically a batch typically you want use
transactions to do those because they
are too heavy anyway if you try to do
those using the best transaction
technology out there it wouldn't work so
you know flagging all these millions of
consumers in batch you know is something
that you do you don't use transactions
at all now
to engage with the consumer what I want
is actually to create a virtual database
for that consumer alone right so my
requirement on the strong consistency
and transactions is relaxed already plus
I want this to be up and running hundred
percent of the time so I don't want the
consumer to miss everything anything and
therefore what we want is is at least
you have the capability to do eventual
consistency and in the future maybe go
into stronger models in scalable I can
scale this cluster by adding new new
nodes in the similar way to what other
no secure database will and we you know
obviously we can just come OT hardware
now
the other item is a faltar us and that's
why one one of the main reasons why we
selected our length to do this so what
we went to the newest continues
availability no single point of failure
and we use Hinton hander of an anti
entropy mechanisms to do self-healing
now this might ring a bell if you're an
airline user or react user and basically
it's because we are using react core
which is an open source Erlang library
that helps you build this cup systems
using the dynamo architecture that was
defined and described by amazon and they
use it for the design of the shopping
cart and other services in the inside
amazon disturb running finally as
opposed to all the other databases that
you might you might have seen this is a
deductive database so basically we
create we can create rules and query
those rules and using those square those
rules and we're going to generate new
data for you for that we creatively blog
which is the main specific language is
based on data log which is a declarative
query and logic programming language
it's compatible with the relational
algebra and as you will say is is even
more powerful than cql if you provide
some extensions to that to that language
into the semantic aspect what we decided
to do is to base our information model
on the resource description framework
which is the technology behind the
Semantic Web and basically these is a
knowledge representation language but is
also directed label graph and what we
were talking about relationships before
because we want to relate to everything
to everything these was right model plus
if you want to bring data from sources
like facebook facebook already supports
rdf so that simplifies a lot the inter
interaction and integration of data from
these sources
finally in terms of the data space so
this is a database of facts and
relationships if you if you come from
the relational database model this is a
relational database in which we deal
only with a universal relation that is
there are no any relationships or
relations in this database is a single
way atomic fact that we can that we can
store and we're going to show you that
in a minute and is also multi graph
database so let's jump into a little bit
more of the details of how how we do
these we're going to start talking about
the information model and then the
stupid scalable and fault Turin's
aspects of data space and what we're
going to go into the didactic and
semantic elements of it so the
information model if you are familiar
with the with the enterprise and entity
attribute value or model or the
resources scription framework I can talk
about any thing and Express facts about
everything just by using a triple right
at uppal with three elements subject
predicate and object in this case
because we're adopting the RTF
specification those subject predicate
object have different types so probably
this one is the most secular slides in
the presentation but basically there are
certain there is a type system that you
can use and basically the subject East
says a URI is it was an animus resource
I'm going to give you some examples
right now that allows you to identify
something or somebody it predicate is
another year I that is going to show you
is going to talk about the properties or
the attributes of that entity and the
object can be another subject or can be
a value so I'm going to try and now go
into
plz environment
to show you some of the an example of
how we can use the LSD environment to to
do this
alright
fine so hopefully you guys you got all
see that I'm going to I've just launched
a node and ellezi node I mean in in a
Erlin console and what I'm going to show
you right now is just a limit of how we
deal with those resources so let's say
that I want to talk about leap site so
what I'm going to do is I'm going to say
I'm going to create a resource that is
website and that creates a structure
their structure that we are going to use
internally i'm going to say also that
i'm going to use some elements from the
RTF specification going to say there is
a property called type now that's an
internal properties that is a built-in
property and therefore is going to be
represented by an erlang atom that is in
order to say because we're going to use
this property a lot because it's part of
the body are going to be going to
simplify that by using atoms I can also
create literals as we said before so in
this case a number is
integer that integer is going to be
representing internally as an airline
integer in the case I wrap that with
strings that's going to work again
because I'm forcing it to too literal if
you're coming from airline and you want
to represent appear as well the peat is
going to be representing the same way
now if you take a date for example that
is the way we were we want to represent
the date and just to comply with the rdf
specification if i write an integer and
i specified that is it is an integer i'm
going to get that integer again but if I
go and say I'm going to use the exes II
the XML data types in as opposed to
integer I'm going to flag that with me
the data so that is a way we create in
principle all these elements that we
were talking about in the presentation
so we're going to go back to the
presentation
so there are more elements that we need
to add in order to to create a statement
and to statement is a fact about about
about an entity so what we define before
is that we wanted to respect or isolate
the data from different sources so for
that we need a fourth element so this is
beyond the results description framework
and but also if we want to
gather information about the certainty
we need to add another element which is
a certainty degree and finally what we
want to also store is when these
information was recorded right so in
order to do this we're going to use what
we call transaction ad this is a this is
a lexicographically k order ID that
we're going to use in several ways
throughout throughout the throughout the
LSD and we're going to see how so let's
say that we want to represent that
information that we've spoken before so
we're going to assign a URI to marry
let's say is the ID hundred twenty-three
we're going to define a predicate that
is called the foo first name and we can
assign the object marry the string merry
as a value and we're going to we're
going to store that in the consumer
profile with a hundred percent degree
and it mean to go that social is going
to be assigned and I can do the same for
all this information that I help from
Mary she's married according to us in
the same graph but according to facebook
I have more information so i have a
friend from marie that is the ID hundred
2424 itself is being repeated described
as well with the first name and Kate so
another way of talking about these is by
drawing a graph say and this is exactly
why we've chosen these representation so
in this way what I have is a homogeneous
way of representing any relationship so
relations between people on brand and
things are have the same shape us
descriptions of those elements so
basically what i'm doing here is saying
the same that i'm doing before now for
each one of those statements i'm going
to add the degree in the transaction ID
but i'm gonna scoop i'm going to just
show you that in in the case of that
middle statement i'm not going to do
that for the rest but also i'm going to
flag that this is what happens so what I
what happens I'm describing Mary in
different graphs
as you will see I can then go and query
those different graphs and I can merge
the information from those different
graphs as well so how these all works is
basically we store all those statements
in graphs those graphs leave in logical
spaces and those logical spaces leave in
Terrence's so all of this is logical the
only thing that we're going to store is
a statement out and out of the statement
we're going to get all the structure for
us
not what we can do with with this kind
of API is at this level is we can write
remove read or this data now we have
four different elements for look for for
for dealing with writing and removing so
writing a statement basic what we write
a statement we are actually not writing
the stem itself we as we're going to see
that in another sliding that in a more
technical definition of it we're writing
actually indices that will allow us to
query the data in any way we like and in
basic configuration we can store up to
six synthesis and these are indices that
are covering in this is so they stole
the data they're not pointers to the
original data they actually store the
data and they provide different
collation orders so in the case of you
know the 16th is the first the first one
for example will store the graph
followed by the subject follow blow the
Prairie hopeful followed by the object
that allows you to for example query the
the graph without with wildcards for
subject breagan object or query the
graph with the subject and why it comes
for pretty good offices so and so forth
now every time we'd right we're
appending the data we're not removing
this is not an updating in place so we
never delete and we never update data
the only thing we do is we append data
the remove is actually a right with the
degree of 0 so when you write a
statement you tipping rights in with the
degree that is between 0 1 meaning syrup
you know more than one third on sue
person and hundred percent but if you
ride a stem with the degree of zero it
means is a Thompson you're deleting
logically deleting that statement now
the right is it important for the same
combination of SP OG within the same
transaction ID so if i want to write two
things with the same SPO G&amp;amp;D actually
that won't work so only one would be
written which is what we want because
stating to think one thing several times
doesn't change doesn't change the fact
and in the same way if you deal with
with the Avenger precision react kv and
in Cassandra and other systems we do
here so we allow you to define how many
replicas are you going to have for this
cluster let's say we have three replicas
and then we're going to every time you
do a right you can app you're going to
configure how many how many acknowledges
you want from the three replicates do
one one two or three those
configurations are on a per operation
basic so every time I do it right remove
or an async right and acing remove I'm
going to get that the case of the async
operations basically I am getting back
future I can carry on doing what I what
I want to do and then I can check
whether the result has been received so
a typical right will will happen like
that I need to have a client I'm going
to using the Erlang as a client here so
i'm going to write a number of
statements in this case and we're going
to write two statements that i actually
equal in terms of the SPO and g and
because of that the first one is not
going to be written because the first
one was the only difference between the
two is the degree a basic I'm going I'm
going to write only the the second one
in the case of reeds basically the way
we read in this API is by using pattern
matching so
basically i'm going to create a a
statement with wildcards i'm going to
use that to read the data from the store
in the sub cases right i can do an async
read as well and then what i have is to
take a take is a read that actually
removes the statement from the store
right so every time every time I take
what I'm doing is it is a DA remove and
read all of these api is returned and
operate on statements there are no
projections as you get as you can do
with with cql we're going to do that
using another API and then he threw what
does is that includes all the versions
that that you can get from that resource
so every time you read you're going to
read information from the space if that
was deleted the information will be Neil
but if I want the history of what
happened to that statement I can use
history to do that
so how I create a pattern basically I
use the API to create a pattern that use
workers and variables and constants in
this case what I'm saying is for example
I want to bring all the subjects whose
predicate age is more or less than 20 21
so basically I'm trying to find out in
this case consumers are less than 20 21
years old but also i'm going to use at
least 0.5 or fifty percent degree
certainty in this case
so let's do
now a demo of how we can do this here
I'm going to resort to some
data that i have here so the first thing
I'm going to do is I'm going to create
the client
you
right so the other thing I'm going to do
is I'm going to create a number of
statements i'm going to set there is a
graph G I'm go the way we store graph is
using a particular you are I I'm going
to make this a little bit bigger
we use a particular URI that is using
LSD as a as a as in as a as a specifier
then we use the the space name on the
graph name so basically this means store
energy in my in my tenancy which in this
case is Lipsyte i'm going to create a
space called ESL and i'm going to create
a graph call g then using airline and
going to define for resources Joe
Armstrong might really enjoy reading
class with some these are the guys who
created our own I'm going to create the
resources I'm sorry should that is a so
for I do that
what I need to do is I am using at this
point in time
I am using a perfect smack so I don't
want to write all the time all this long
you are I so what I'm going to use is a
way of getting these your eyes short
away so basically i created a prefix
mapping right in that prefix mapping i
created a demo
graphics now i'm going to know i'm going
to find again my resources
I am going to say that p m equals p.m. 0
sorry p m equals p.m. 0
fine now I'm going to
sources as you will see the previous
resources were using a short
determination basic i'm using short
Curie's or short short you are ice say
now demo is going to be expanded into a
full year I here by using the prefix
mapping in here now on what I'm going to
do is I'm going to create a statement so
the way i create statement is by using
the API I could extend them and say well
the subject in this case each one of the
Ganga for i'm going to create extinct
for each one of them saying that they
created airline right and i'm going to
write that in this graph G and I'm going
to say that that I know that for sure is
hundred percent true so what I get back
is a number of statements now these
things are not storing data in the data
space yet because I have an issue right
so these are in memory so what I have
now is there is a list of the statements
for each one of the airline creators
saying that they created early all say
i'm going to create another resource i'm
going to create a resource name
Francesco Cesare any the founder of our
lung solutions and now i'm going to say
that obviously Francesca knows of these
guys right so I'm going to create now
the series of statements saying
Francesca notes all the subjects in the
in the Ganga for right so what I get is
Lisa the stem is saying Francisco knows
all this guy's now I'm going to do a
right so I have statements to and
statement ones variables mapping all
these statements i'm going to write the
statements one sorry
thus client 0 I used before so I'm going
to use client if you're not familiar
with our lang and airline doesn't have
its a single single variable assignment
and therefore i'm messing around with
with that at this point perfect so we're
going to do that again that's it so we
just created
in this in this case in the four sevens
and we've written that three times into
into the store now we're going to write
the other statement saying that
franchise because they only know these
guys and we we have we have all that now
now what I can do now is create a
pattern and I'm going to say that I want
to know who knows
who in the graph G and we had before
I can actually do these and says LSD
easy LSD
cell demo
and I'm not interested in the degree so
any degree will make it
right so those have been expanded
because I have the prefix mapping
installed now I'm going to do a read so
I'm going to use a client i'm going to
say read client and I'm going to use be
as a pattern to read the data now what I
get back is again the information that I
store before in this case Francisco
knows knows all these guys now I can
actually say carry on with it with the
pattern that again I can do any party
and I like so I'm going to just copy
this and I'm going to start changing
some of those queries so instead of
doing this with a verbal I'm going to do
it in line I'm going to create a part in
I'm going to say well bring me
everything about this or anything that
is store in this graph so I'm going to
get not only that Franciscan of this
guy's but also that each one of the guys
are the creators of airline right so
basically what i can do with the pattern
is you know much anywhere i like all
this information i can i can actually
for example say Francesco Cesare any if
I 12 just bring the formation of our
Francesco i'm going to say demo
Francesco Cicilline
right and i can actually
all in any graph in this case Francesca
is only one graph so I'm going to get
only the information from Francesca in
that graph
right so that is how I read and write
information in in the space I can
actually go it'll be further and use
some some patterns but some conditions
in patterns but I'm going to skip that
and go back to a more interesting way of
looking at that so going back to the
presentation
we're going to go a little bit into the
technical architecture of this system so
basically this is the way it is
implemented we're using react or as the
distribution framework and we're using
the react or patterns which means that
what you have is basically if you look
at the boxes below we have a vinodh
master that is orchestrating the work
with different vinos now this is a very
clever design did by amazon and a very
clever implementation built by bosch
show on turf 'verily so basically what
it means is that as all these nodes my
fail I need another level of indirection
I need to be able to store data in
virtual nodes and those virtual nodes
can move from node to node in order to
satisfy the requirements for higher
availability now what we're going to do
is we're going to we're going to split
the all the cluster in in a ring in a
consistent hashing ring and that is all
done by react core and we're going to
store all this information i'm going to
show you how we store this information
in the next in the next slide so
basically you interact with these
through a client it can be an HTTP or
protocol buffers and we interact with it
with the API that which are seen now
we've just seen the data space API i'm
not going to show you the object or the
graph api which are just elaborations
top of the data space API but i'm going
to show you something that we call lee
blog in the number of slides but before
doing that
just wanted to show you how these works
in a bit more detail so basically I can
scale out this platform by adding nodes
this is exactly the same in the same way
that riac kv works because react kv is
using react core now every one of these
nodes is going to store their in main
memory we have a number of back-end
storage engines the main one is ETS but
we are experimenting with another set of
options that will probably bring the
consumptive memory consumption down
we're using a term dictionary
compression so in every one of those of
you notes we are sharing those your eyes
lengthy you know those length of your
eyes and those elements so we're
compressing the store by sharing all
those elements in a dictionary and we
assign an ID to each one of the
dictionaries and therefore what we
storing mean this is our are pointers to
that dictionary we use command logging
to persist every time you write so every
time you write we're logging the
transaction not not the data so in the
case of a failure we can go back app
read that common logging and reapply
difference actions reusing snapshots to
disks and increasingly moving into
snapshots to leveldb because what we're
planning to do is to add this space
graphs with level to be and history disk
archiving with level 3 so basically all
the data in disk following these will be
in level to be and will be queer evil in
level to be as well but obviously the
performance that we're trying to get is
because because what we're trying to do
is read them with the data as you will
see in a couple slides we need the hype
of the the rain memory plus what we
learned so far is that in our previous
environment we needed to cash at least
seventy percent of the data so what we
design it is let's go for a min memory
technology this is the right way to to
go in the future so let's optimize for
that environment first now how we store
all this data the stamens are store in
covering in this as we mentioned before
we distribute these using consistency
hashing so I'm going to show you how we
do that so every time you write one of
these statements
we actually actually generating between
three and six covering indices and those
in this is I going to be stored in n
number of copies so if we say that by
default we're storing these in three so
I want a factor of replication
replication factor of 3 i'm going to end
up writing this into at least 3 v notes
/ index right so this this means that
basically we're reaching 16 different
partitions if i am using the six
covering indices so in other two to be
able to do this efficiently the best way
to do that is to limit operations write
operations prograf now we don't stop you
from writing to multiple graphs but that
one might mean that your inundating all
this all these vinos at the same time
right so we're going to come up with
another option that is basically using a
stronger transaction approach and will
limit that to three participants in
that's the three vinos in that in that
operation so you have the options to if
you're right by graph so you're reaching
typically if you fewer number of nodes
if you run to right to multiple graphs
in the same operation then you're
reaching multiple nodes and that might
be may be an issue so you want to take
care of that in terms of anti entropy so
what happens when I write and one
replica I didn't receive the the copy
and then when I read while depending on
the on the configuration of the of the
consistency that I want if I say that I
want three copies are the three copies
that I should have stored then then
what's happening in those in this case
is every every sick every query that I
issue to this video's is going to be
signed digitally using a miracle tree to
do that and we're not signing the
content of the result we're actually
signing everything that was required to
create the result so in the case of more
complex queries and in the case of
queries that are involved multiple
versions of a of a of a of a statement
or include
in Thompson's basically we're including
all of that to make sure that when we
say that these are exactly same replicas
are they are at all levels not at the
level of a result only but of the level
of the the base data storing that in the
V node and in the case of a conflict
then what we can do is we can issue a
writ repair by replaying blame back that
transaction ID from from the replica
knows from this particular note that was
music data and we are enhancing this by
using active and gentlemen the same way
that react TV is using it now so finally
we go into the more interesting aspect
which is the deducting semantic data
space now hopefully if we have enough
time we can do we can do a demo by this
but basically this is the real reason
why we did all this so in order to be
able to do that we define something we
call lee blog now lee blog is a language
that is based on data log now a quick
overview of they log in to slice so data
log is a query language is actually a
logic programming and query language and
this is subset of prologue which is a
logic you know is the logic programming
language now um for those interested in
these technical aspects so prologue is a
Turing complete program language data
log is not so you cannot use data log as
a general-purpose programming language
so therefore for us is going to be a
domain-specific language that we're
going to use to define business rules so
the logic that we want to operate and
the data and the queries that we're
going to use against that data and rules
and dana log is one of the most
researched non commercialized
technologies in the world probably i
mean most of the innovations in c ql
technology is actually deriving from
their log but the vendors moved away
from data log in the early 80s because
they thought you know secret cql was was
was all that we although we need it now
for those of you very technical you will
know that c ql by definition doesn't
allow
ocean now db2 is one of the IBM db2 is
one of the databases that and I think
the Microsoft cql also implement
recursion that is based on data log now
several extensions exist for route 44
data log which include negation our
arithmetic comparisons function symbols
and a greater we're going to see how
others works now if we aggregate if we
take the log and negation we can get a
language that is more expressive than
cql now this is how they log typically
looks like i'm going to go rapidly into
each one of the elements so what we have
here is what we call a rule the rule has
a head or a goal it has an implication
symbol and then it has a body so
basically this rule says if there is
exists ate apple or relation in there in
somewhere father XY that means that
either in this case of why is the father
of X then it means that
why is the ancestor of X right so too
girly be more detail ancestor father are
what we call predicates right and you
can think of those as the name of the
table in database in the religion
database so what I'm saying here is that
if you find somewhere either by another
role or in the database a relation ie a
row in a table father for x and y then I
derive the fact that why is the ancestor
of X now in order to be able to
calculate and sister neither another
rule and I'm going to use a second rule
that says that if Y is a father of X and
set is the finances of Y then said is
also the ancestor of X in this case I'm
using it what is called a conjunction so
I have two atoms or two soup goals
separately by a comma the comma means an
end so basically what I'm saying is if
there is a father XY and an ancestor
why's that then the reason is hysterics
sad and then I'm going to do an assert
I'm gonna say it is true that jon is a
father of tom and then i'm going to ask
a query and i'm gonna say who who is
Tom's father and the answer is going to
be John but is going to be John sorry
who's the ancestor is going to be John
because John is Tom's father and because
of the rule 1 and Row 2
John is also the sister of Tom right so
that is the way you work with
data log and what we call the rule the
first two rules assertion in the query
we call that a program and and and
that's what we're going to use in lib
log to define the logic so what we what
we what we thought about these we
thought you know this is the right
technology to use a marketing marketing
i need to change data constantly so the
data space allows me to do that i can
create statements facts without actually
defining what a table is without
defining columns and i just write
whatever I want in the data space those
are flat tapal so there is no way i can
i can i can i can make a mistake there
everything is atomic so i'm going to
store all these facts there but then i
need to be able to also move very
quickly in terms of marketing and define
all this business logic and the computer
logic that want to use and what about
testing all this different business
logic in real not in real time and i
need to change that business very
quickly if every time i want to change
the logic i need to go on and higher
code something in java and compile it
it's going to take a while so what we
wanted to do is to start using logic to
be able to express those current rules
now before when we built this platform
we use our own bespoke rule engine that
will build in other to overcome the
limitations of the compile cycle but
nowadays with if we obviously defined
and we found that that day law was the
best approach for this so what is italy
block then so live blog as a city that
is a database language we adapted it to
work with rdf unless you can make
deductions based on the rules expressing
in lib lock we use a top-down set
oriented evaluation we provide built-in
predicates and we're starting to provide
now function symbols as well but those
are built-in functions so we created
some functions to manipulate some of the
resources we still don't allow you to
define your own functions you can do
that outside in whatever language are
you using to consume the results and we
haven't got negation and aggregates but
we're planning to
that so an aggregate will be for example
counting all the results or adding or
calculating averages or mins on or a
result so in terms of
the Leigh blocked syntax is very similar
to deadlock so basically what we do is
in this case is say well because we're
restoring these facts we you know the
the only the only predicate that can
exist in the database is the statement
predicate and because we don't want you
to rise statement you know constantly
that is implicit so if you write a tap
hole with four elements in this case I'm
not going to cover how we deal with
degree but we deal with agree as well
that's one of the syntax but I didn't
want to complicate the presentation so
basically what we do is hearsay well
there's gonna it's gonna exist the
ancestor y of X in the graph G if there
exists statement of the shape of X foo
father oh why in the graph G right and
then I can actually query for that
ancestor but I can also generate
statements are out of the rules which
I'm not showing in these guys so let me
show you how we deal with queries then
let's say that we want to do the same
query that we did before I'm going to
simplify this i'm going to say i'm going
to use another API so instead of using
read now i'm going to use select right
and i'm going to say
i'm going to call it the select function
i'm going to say i want to select
everybody i'm going to send this the
same the same double ID before very
simple i'm going to just grab all the
data from it from it from a graph so i'm
going to say i'm going to write the data
log in as a string now the Select
function will pass the string we
validate that they lock that Lee blog
syntax and will execute the query so
we're going to say I'm going to bring
all the espys and O's in the graph and
the Z
ESL demo right now what I get in this
case is I'm not getting statements what
I'm getting is a projection right what
I'm getting is because i haven't say
what the project journeys and i'm going
to show you how you can do that by
definition this query is going to
project in these three variables SBN oh
right I'm going to have the variables
here so the head of these results said
says the variables projected RDS piano
and the values associated with s piano
are each one of these so again all the
rows right now let's say that I wanted
just to know who is in this graph right
who are the objects in all the subjects
in this graph I can actually say these I
can come here and say I just want to
project out
I'm going to use the same syntax that
use for rules and now I'm just kidding
i'm going to do that again i'm just
getting the binding for the s variable
right I cannot honestly do SN oh and I'm
going to complicate these and say well
let's say I want to know who knows who
so I'm going to say demo nose and I'm
going to just get that Francesca knows
Mike Joe clays and Robert right so these
are very very simple and best way to to
query but i'm not getting stamens the
reason why i'm not going to statements
is a because i'm projecting out and and
and the second reason is i might be
deriving as it as we will try and see if
you have enough time we might be driving
beta from rules and therefore if you
wanted to write their different rules I
don't want to create stemmons unless i
explicitly want to do that in which
because i need to use another API so let
me see if I can bring that app and say
okay I'm gonna create a
rule I thought I had it here yeah i'm
going to create a rule here
let's see if we can if we can use
another
I'm going to you i'm going to show you
something else which is the a
preliminary whip fly that we have so
let's say that i'm gonna do the same
query here
you
oh no no no I'm doing something wrong
another server
active sorry guys I'm going to go back
to this one so i'm going to write a rule
that says
the following see if it works easy win
everything everything collapse so I'm
going to write a rule one that says
this right so what I'm going to say here
is if there is at Apple in a statement
in any graph that says that a subject
i'm going to use s here is a creator
you
let's complicate that right so what I'm
going to say here is that if if there is
a subject who is a creator of something
right and there is another subject there
is a creator of the same thing and this
suck those two subjects are not the same
subject then these two subjects are
colleagues right so i'm going to write
that rule
and I'm going to apply that here sorry
I knew this would happen
you
you
so we're going to use that prefix here
you
you
good
you
you
i won
you
you
right here with my environment guys
sorry about that
you
let's try
and do that let me try and fix that and
see if you make it work now he's not
working so I have something funny with
my environment here those not working so
you
so back to back to orientation so
basically
you
sorry
so basically what we want to do with
this with these generate new facts that
are the other are of rules and the facts
already have in place
the challenge the chance that we have
with with with RDF is obviously that
the statement is I can I can I can
derive statements so I can stall same as
I can't arrest seven so dead log
actually doesn't allow you to do that so
you know the two that we need to
overcome that unfortunately I have a
broken environment i cannot show you how
it works
so that's it for
NZ and basically we are starting to work
with a number of customers in the space
of defining this kind of environment for
marketing and we obviously we are early
on in the implementation off of the
easiest product and we're looking for
people like you to to become early
adopters and working with airline
solutions as well what we sell us to to
be able to reach the market with these
we're very flexible in terms of the no
license in supporting and consulting and
we are going to be offering a community
edition soon we're going to start
doing that very limited because we have
obviously we starting up with these we
have limited resources and and basically
that's it I mean we we are we
interested in understanding how do you
see these in your environment we we
built this for for a marketing
environment in which we want to reason
about every consumer 11 ones you know
one consumer at a time we don't we don't
know we do not intend this solution to
be a solution to compete with
high-performance query engines or
high-performance databases like all TV
which they have a niche and which
require probably two to know in
anticipation what is a query that you
want to do so what we want to do with
these is basically be able to be very
flexible in the way we store data and be
able to be very flexible the way we we
we collect the data as well and we use
the rules to generate this excellent
well armed I'm sure that everyone who
has participated in the webinar will
join me in thanking yourself Alejandra
for a very inspiring talk on the website
semantic data space technology we have
crossed over the time that we had
available for the webinar but to be fair
to our audience in to be many questions
we've received first of all I have to
say that because we've gone over the
time we won't be able to answer all the
questions but we should try and at least
answer a couple of them and as we said
previously my details and our details
are from this last slide you're welcome
to send us emails and ask additional
questions and we'll obviously answer
questions that have arrived that we
can't answer now late at a later time
and by email but to to honor the
questions that we've received at least a
few of them I'll home drew one of the
questions that has sort of come forward
and you've answered many of these for
your presentation but a bit of a broader
question in a sense of how do you see
website semantic data space developing
and evolving going forward what what is
it missing at this point that you sort
of wish to add to it in the future how
do you see the technology progressing in
time ahead so basically what I am
there are a number of elements the first
element is optimization of very large
rule sets so at the moment we have a
number of optimizations implemented but
again we are focusing on reasoning one
consumer at a time when you want to
reason with with much more data and and
you know multi you know more complicated
queries we need to think about more
optimizations so we have two ways of
optimizing thanks to our land 11
optimization is paralyzing even more the
queries so obviously I haven't shown you
and have this broken environment but I
haven't shown you these in terms of
performance but we can deal with normal
queries simple queries in milliseconds a
single retrieve of thousands of stamens
is about three milliseconds riesling
goes up to 20 30 milliseconds but if you
want to do a very very complex square
with a lot of rules then it gets it gets
complicated so one element is to
optimize that and the second one is in
terms of providing more element or more
options in terms of consistency work so
we adopted the eventual consistency but
we know that we need to to go into
stronger consistency elements for for
some of the use cases fortunately we
have react or as a base and because
react or to the 0 is just about to be
released we're going to get more
elements in terms of that so commutative
replicated data types or stronger
consistency options that we might be
able to use out of the box and that is I
guess a second the second key element I
probably the third element is improve as
I said I think that in the last slide
and sorry because we ran out of time and
in the in the in the last I was saying
one of the elements was enhancing also
durability say more performance in terms
of the checkpointing of data recovery of
data Thank You alejandro another
question that has come through and one
that I think is quite interesting in
terms of female viability of LSD
could you tell us a bit at how does LSD
perform in terms of OPEX and capex in a
large for example fmcg environment when
you know but you're supposed to be
addressing millions of consumers what's
the cost of setting it up what's the
cost of running you how do you see this
from from that kind of business
perspective okay so basically we are we
are in it we have a number of different
business models say we build this
technology to be part of the an asset
that we developed to define as you know
several solutions as a services one so
so one of the things that we bring into
market is not just letting this
technology this is ontology Wars our
brain child on what we wanted to use to
be able to build the because we're
different platforms that that we're
offering so if we were building a
consumer engagement platform for you
then the lse will be part of that
solution embedded in and typically our
business plan and the reason why we
created Lipsyte is based on our
experience in doing this with millions
of consumers and we think that we can
get back to a number of something
between one and two dollars per consumer
per year for all the platform software
infrastructure running including lolsz
running inside right so so we're
tackling these from a business case
point of view not from a technology
selling point of view so from a business
point of view we think that we can
enable a platform that will cost to
operate to build you know to run
including all the licenses of our sofa
and other software that we might need
plus the infrastructure we can make it
to run in between Wanderers and two
dollars per consumer / yet and now in
the typical consumer goods company that
means you know that you will pay that
with 10 10 transaction so if you are a
color program a coca-cola or Pepsi or a
Unilever you know or a tobacco company
you might cover that cost by selling six
units right and any more if the if the
whole operation if the the whole
consumed aggressive strategy is a one
that generates that then there is basic
for free and you know that to be able to
do that is the reason why we came to
Erlang and to and to react or and to
technology that we're developing because
you cannot do with traded you know
traditional enterprise technology that
wouldn't be affordable Thank You
alejandro again many apologies we've
received so many questions during your
talk and will only be able to honor a
few of them due to the time that is
expired the one thing I would like to
ask you is them in terms of you know the
rules you've set in the artificial
intelligence in place how would you say
relevant and accurate typically are the
conclusions that the system creates
about any individual consumer given thee
just given the complexity and dynamics
of data you've addressed this party
during your talk has this ever been
tested and verified you know can you
sort of guarantee or give us at least a
sense of how accurate a typical
conclusion is when you're dealing with
you know millions of consumers and
obviously creating these Taylor
campaigns okay so basically um remember
that when you when you deal and based on
our design a bit of our experience yet
the way to tackle these is by recently a
consumer at a time so I need to prepare
the knowledge base I I might decide to
use you know machine learning and data
mining the stuff that we did before to
analyze the data in a bulk you know I'm
going to analyze all this data I'm going
to generate facts about these consumers
but then I'm going to generate rules
that will operate on these facts now
when I'm going to engage a consumer and
the order goes missing gauging ass and
I'm having an interaction through a web
or a mobile application or whatever what
is it going to happen is I'm going to
recent only with the data from that
consumer so what I'm going to do is I'm
going to create from another one point
of view when i'm going to create is it
is a process that represents that
consumer i'm going to i'm going to start
consuming the data and applying rules
that consumer now one thing that i
didn't cover which is interesting is
that i can use a complete different rule
set / consumer so on the way we design
these is so that
I can not only change this logic by
changing data but I can change this
logic by changing the logic so basically
it means i can write AC a single
application / consumer i can write what
how do i do that i do an application
that rise applications for me right so i
do an application that based on machine
learning or based on big data with
whatever nice is you want it will write
not only data but will write rules now
the rules are there just arriving what
is the in the store so it's very you
know this framework this logic firm has
been proven now the element that i
haven't shown you which is very early
days is the how do we use the degree the
probability degree to drive knowledge
and for that we need to we need to go
into stuff like basin networks that are
a little bit more complex so it is not
as simple as a multiplication of a
probability of multiple of a join in
order to derive that so in terms of the
correctness you know if you know if you
write the rules on the right the rules
are right you know you will always
derive the same results on the same
given the same data under some rules it
will always the right the same data
assuming that you're not messing around
with degrees of probability if you're
missing around with that then it will
depend on the algorithm that you use all
that we use to calculate that the
compound degree of something right and
for that I'm many options there are
simple probabilistic multiplication but
then you have by a bayesian networks
approach and markov chain approach which
we haven't implemented yet Thank You
alejandro we have Robert asking a
question how does leap site semantic
data space deal with various privacy
protection and you know rules in place
and how how do you deal with those types
of constraints in collecting your data
and using it okay so basically the data
space itself doesn't apart from the
separation of tenancy spaces and and
graphs doesn't give you much I mean
something that you need to design so
remember that every time you write data
you write it in a in a logical address
that logical address is a hierarchical
address but actually you can create
paths so basically every time I write a
statement i'm going to write that in my
tenancy logical space so there is no way
i can read other tenants data so that's
the first barrier the second barrier is
i can write that into a specific space
and what we're adding now especially
because again because of react or they
did at four they did this for us they're
using role-based access control as part
of react so we're going to inherit that
we're going to allow you to do a role
based access control in obviously in
spaces and graphs so I will be able to
say this these user cannot use this
graph the user going to use or these
classes only these the other thing that
we planning to do is use pattern
matching to limit what a user can see so
for example if you don't want somebody
to see the information about a person
consumers sex or gender or religion what
you can do is that you can ban certain
patterns so for example what we did
before is you know could marketeers can
see the data but they can never see the
name of the person so they can see
anonymous data because there is no no
reason why a comes in market brand
marketing brand manager would like to
know the names of you know 10 or 50
million consumers right so we will
typically off escape that so we can do
that with button matching we can say if
there is a pattern that is trying to
match this particular property that is
going to be projected out because this
consumer of this brand marketeer cannot
cannot see that information now if I'm
illegal in legal department and I need
to get access to the name and address of
the specific person I might be able to
do that because I have the the the the
rules robot draw basics to control that
allowed me to that now apart from that
we are there is nothing there is nothing
else in terms of for example regional
that weekend that we can impose is
something though we can work with you to
try to understand and see if we can if
we can find a solution for that so for
example if we have a multi data center
hosting of the of the data space and
want to replicate between data centers
and then you want to eliminate certain
data from one replica because you don't
want to share with that you want to
replicate the yes but you don't want the
European nationals data to be in the US
replica you can you can we can probably
come up with it with the solution for
that Thank You alejandro that makes
perfect sense now again apologies for
everyone attending we can only
accommodate two further questions and
the first question I would ask you is
from Louise Louise is asking in terms of
available channels that you're using to
sort of reap and you know harvest
information would you say that website
semantic data space is negatively
dependent on any one of them in a sense
of any one of them disappearing
influencing your results out comes into
the information you're collecting the
conclusions you come to know I mean the
whole idea of the data spacer the whole
design was all of all of these sources
will appear and disappear so if you ask
me I think Facebook will disappear and
another Facebook will appear or
replacement will appear I mean even you
know if you have children I have three
children there they're starting to stop
using Facebook right and that's why
Facebook starting you know going after
snapchat and all these different
environments because you know youngsters
I going out of Facebook you know all
these youngsters don't want to be in
phase with their mothers and that's you
can actually see what they're doing so
basically it is a very dynamic
environment so we are agnostic so there
is nothing in our design that precludes
or includes Facebook as a special kind
of thing I mean the only thing there
isn't what I mentioned phase we can
trigger is because obviously they're the
major platforms are there and in the
case of Facebook they're ready adopted
RDF which basically means I don't need
to do any transformation of data when I
bring data from facebook and
and we are kind of betting that others
will follow suit and adopt RDF as well
but in terms of dependency now there is
no dependency in terms of obviously you
will depend on your domain which kind of
which kind of data and it will depend on
your consumers which kind of data is
more relevant or not for some consumer
products you know maybe the behavior of
a consumer in favor is more important
that what the behavior is another social
network but you know that is we are
agnostic to to that Thank You alejandro
I think that that clarifies that that
particular Asian finally the last
question from Dmitry Dmitry is asking
what if any is the scalability
limitation on the website data space
platform oh I mean obviously we are not
claiming this is a you mean there are
limitations as in with any any
technology and obviously in the early
days we're going to find you know more
limitations that we will need to work
with with the early adopters to to work
in a solution for that i would say that
first limitation is obviously because it
is a new memory technology in principle
so obviously there are limitations to
what you can store in memory as if a and
depending on the budget that you have
what you can store in memory a purse or
what you can store in disk so that's why
what we are planning to do is all the
way we want to design these for to be
deployed is all the data that you will
collect about the interactions of the
consumer will not end up in the data
space that will go into disk now that
might be our own solution as part of the
space so this you know space this based
solution that you can query using the
same language using the same
architecture or you might want to do
that using Hadoop or or or or similar
technology for example disco in early
now when you apply that goes into the
learning loop then you apply all all
your magic and out of that will come out
you know summary facts and also new
rules and those are the things that I'm
going to use to make decisions on line
right so what I want to store in the
space is the day
that I need the active set that I need
to be able to make decisions on line
right so the first limitation is that
one the second limitation comes to the
number of thing this isn't and the white
the way you want to operate with it so
if you want to be able to say bring me
old information from for this for
alejandro in all the graphs that
Alejandra store in then I need to store
six indices the ones I'm show right and
that means six copies of the data so
more data in memory now if I only go
into a parade by graph let's say that
will never say get me all the data in
all the graphs for a handler but i will
say give me the data for Alejandro's
data from facebook or give me the other
hand rose data from the profile one or
give me the handles data from the
treated data then i can only use between
three and four indices which obviously
is cheaper to maintain in in memory and
finally some of these indices we will
have a downed rice a when I store the
data for a particular graph in the graph
index I'm storing all the data for that
graph in 13 node and that is in one
airline node so I need to I need to be
able to understand how I design how I
write my data you know how I partition
my data and graphs to be able to avoid
an overflow of a graph of a vinodh now
one of the ideas that we have is to be
able to handle that overflow by writing
into disk but that will affect obviously
the performance of the right and the
performance of the reed following that
right right but those I would say are
the the initial limitations that we were
thinking of Thank You alejandro to close
up the question session as we really run
out run out of time I'm sure that
everyone will join me in thanking before
what has been a fascinating talk on the
website semantic data space many thanks
to all of you who have joined us for the
webinar as well please join us again for
our next monthly webinar and following
today
we'll be sending you a short survey just
to make sure we capture your feedback
off today's webinar please also note
that the recording of the webinar and
the presentation that we share today
will also become available for you to
collect on our airline solutions
corporate website at airline hiking
solutions com thank you all once again
please send us any further questions you
might have and we look forward to seeing
you on our next webinar</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>