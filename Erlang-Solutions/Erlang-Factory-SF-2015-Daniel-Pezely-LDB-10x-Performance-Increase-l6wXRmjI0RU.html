<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Erlang Factory SF 2015 - Daniel Pezely - LDB 10x Performance Increase | Coder Coacher - Coaching Coders</title><meta content="Erlang Factory SF 2015 - Daniel Pezely - LDB 10x Performance Increase - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Erlang Factory SF 2015 - Daniel Pezely - LDB 10x Performance Increase</b></h2><h5 class="post__date">2015-03-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/l6wXRmjI0RU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so yes my name is Daniel Paisley I'm
with splunk the title this talk is it's
about ldb the Lethe database some of you
who may have been at prior or lying
factory conferences in San Francisco the
past two years would have heard from
some of the bug sense founders bug since
has been acquired by splunk I inherited
the coat and I'll get into some of these
details so what I'm going to talk about
is we had a linkedin module written and
see that implemented a lisp dialect of
scheme and we found that by rewriting
what we were using what our intention
for that feature was by rewriting it as
a pure Erlang module we had a 10x
performance increase after about four
months we've actually gone up from there
but I'll get into that so this is a
classic far side comic strip for those
who may not be able to see there are two
aliens one has a couple of humans in a
jar and the other is saying remember
this time punch holes in the lid and
that's kind of the theme of this talk
it's about the details remember to punch
holes in the lid so just how I'm going
to present this so I'll move somewhat
quickly because I know we're kind of
compressed for time but also one of the
benefits of being one of the last
speakers are in the last time slide of a
conference I get the benefit of everyone
who's presented before me so the session
just that was in this room before on
debugging complex systems all of that
apply so hopefully you got that if you
didn't if you weren't here for that
session you can get it in the archives
once that comes out so if this were a
three-act play the middle Act would be
the bottleneck the performance and all
those details and there would be an
epilogue of for those who want to take
it to 100x beyond the initial 10x I'll
touch on some of those but that'll be at
a high level so first some background so
I mentioned the name bug sense for those
of you who aren't familiar with the name
bug sense is a company that was founded
in Greece about two years ago and
chances are good that you have their sdk
on your mobile phone on your tablet and
so on the essence of the company is to
harvest crash
reports in addition to that some
features that have been added over the
past year have been more visitor
analytics traditional metrics you might
be familiar with from you know say
omniture which is now adobe so the way
we harvest that part that the metrics
goes into the least Lethe database lbb
so i may use the names interchangeably
the important thing about our system is
it's a stream processor now not everyone
knows what a stream processor probably
half of the people at an airline
conference do understand what it is but
for those who don't I'd like to make the
comparison between data flow versus
workflow no not data flow there's a
hardware architecture and software
architecture that uses that name also
but in one in contrast to the other data
flow versus workflow with data flow or
with a stream processor the idea is to
move through the pipeline as efficiently
as possible and every decision that you
make is how can you move the data one
step further down the pipeline secondary
to that is any transformation that you
would make on the data so that's what we
mean by a stream processor so we take
data from millions of devices actually
hundreds of millions of devices and bug
senses been considered in Erlang success
story for the past few years and some of
the earlier articles is noted down here
I won't get too much into historically
what bug sense has done or what the
Lethe database was years ago there are
good quality articles high scalability
has an article if you just search Erlang
or Erlang Lisbon see if there are too
many Erlang articles you're sure to find
John's guest post on there a little bit
about my background there's a little
blurb you know my bio which was included
when you found this talk but one thing
of my early experience when I was an
undergraduate in 1990 I was working in
virtual reality at that time and one of
the things that influenced me was when
you're trying to build a really complex
system like that when you're trying to
model reality well ever
anything else seems to be a subset when
you're building for all that complexity
when you're designing for that it it
didn't work I mean obviously you know
I'm not known for virtual reality so you
know we had a waste ago we had to invent
things that didn't exist yet but that's
a story for another time but through
that we live our lives we view through
the lens of our experience and I'm sure
you've heard various quotes similar to
that so this was no different when I
inherited this project part of it to me
was there were some obvious issues some
obvious places where there was room for
improvement and then there were other
areas that for many people would have
seemed to be the first place to start so
anyway I'll get into some of those
different different elements but I want
to point out this last line on here I'm
new to Erlang so that's also another
component to this talk someone who's new
to Erlang but an experienced developer
and experienced software engineer coming
in picking up Erlang so it's already
familiar with functional programming
from common lisp I've been doing high
performance Common Lisp for six years
prior to splunk so part of this is also
to include the learning curve that I
went through so as the title gave it
away we had a 10x performance increase
but what were the numbers so we were at
200 requests per second now we're at
2,000 actually the 2,000 was about the
four or five month mark and since then
we've gone up since then I think we're
probably at about 20-25 increase so the
hardware that we're running on were on
the Haswell servers at Amazon we get
several billion requests so single digit
billions but still many billions per day
the service that we use have 30 gigs of
ram and that one server may have upwards
of 30,000 customers now of course
they're not all active at one time we
also have machines a server instance
that would be dedicated for one customer
but that may give you some idea of what
these numbers mean when we say 200 per
second 200 requests per second versus
two thousand requests per second
so the use cases so as I mentioned basic
analytics and advanced analytics similar
to visit or journey data that you may be
familiar with if you say Google
Analytics and I mentioned some of the
others earlier we have other components
that deal with other the other elements
of data that we harvest so I mentioned
crash reporting so if you're familiar
with developing for iOS or Android one
of the techniques the developers will
use there is to strip the symbols out of
their executable this way competitors
can't see what they're doing but it
makes it difficult to see your own crash
report so that step 2 remate though that
symbol information is called symbolic
ation so we have other systems that do
that and hopefully at an upcoming Erlang
factory the other guys will present
those systems as well so our operational
criteria is very similar to what someone
would have in an early stage startup
when bug sense was acquired by splunk it
was basically nothing changed in that I
mean they moved from their early service
provider to AWS added a few more server
instances but not a whole lot changed so
we we still operate with the same model
but it's also important to point out our
traffic is steady around the world we
have a lot of big customers in Japan and
Europe the United States and due to that
if we had a queue for all this data we
would never be able to get caught up so
it's just the one of the realities that
we have other people have Peaks they
have lolz so that's not a criteria for
them but it is important to us to
eliminate cues so I already mentioned
that we focus on data motion which again
is another way if you read the academic
literature what I talked about data flow
some people refer to it as data motion
and again it's going back to work we
have a stream processor so it's also
important that when we receive data from
a mobile device that we respond within
milliseconds
single digit milliseconds and you know
telecommunications also have this hard
requirement ad networks will also have
this requirement the nature of the data
that we have it's a lot of counters
probabilistic counters and linear
counters so for probabilistic counters
again on high scalability com there's an
excellent article on how to count a
billion unique items so I highly
recommend that for anyone who's
interested in that topic and i'll touch
more on that in a bit so the language is
that we have so Erlang Lisbon see as I
mentioned earlier okay we're at an
erlang conference so obviously you
understand why we're using Erlang and
why it was chosen early on our top level
code was in Erlang and again now with
the pure rewrite it's all in Erlang but
it's also important to point that we
have a long live port so some of you may
be exploring do you want to have a
LinkedIn module and you're just learning
about ports so I'm not going to get into
those details the Erlang documentation
and other tutorials are sufficient to
guide you through that path I just
wanted to give you some additional
context so that's the essence of you
know why I'm presenting today not to be
a tutorial on how to build a port so but
also I didn't build a port I inherited
the code and part of it was to find the
problem and resolve it so these ports
are long-lived now many people will have
a port command they'll have a LinkedIn
module and they're just computing some
value you know the function returns in
milliseconds and you probably won't have
any of the issues that we ran into for
us we were also using this long-lived
port for for our data storage so so okay
so the next two piece is the Lisp and
see so the list dialect was scheme it
was an r4 dialect for those that are
interested the reason it was chosen this
is a very malleable language now as a
short aside Joe Armstrong gives an
excellent presentation on how malleable
early
is and it is malleable but the example
that he used when I saw him give this
presentation was about implementing
crypto types of things so he had an
implementation of the RSA algorithm and
the top level code reads like a spec
which is a wonderful thing but sometimes
you're not even at that level so an
early stage startup needs something
that's malleable because you don't know
what your problem space is let alone
what your solution space will look like
so it was the right decision at the time
and we'll revisit that notion in a
moment but unto the sea part so it's
also important that the implementation
of Lisp that we had it was essentially
an interpreter but calling it an
interpreter is almost too generous it's
parsed like in the sense of tickle if
any of the interview are familiar with
that so it's a very I don't want to say
a week implementation but it is what it
is but it wasn't a robust lift system
like you would find with sbcl and common
lisp so we also implemented all of our
counters like I mentioned and all of our
storage so our hash tables were
implemented in C and that gets to the
root of the problem so the nature of the
day that we're storing so key value
pairs sets which are essentially arrays
and the probabilistic counter so hyper
log log or hll as it's sometimes
referred to so with the earlier
architecture everything ran in one
process space we had one Erlang process
per customer and we could handle this
based upon the messages that were coming
in there was a customer ID associated
the process would exit when idle which
many people you know is the common
pattern you'll see an airline code also
regarding one of the principles of ER
line let it crash well we would also
allow that but at the operating system
level we would let it crash hard so I
mentioned that there were some flaws
with or not some flaws but some
limitations to the sea based
implementation of Lisp we had no garbage
collection so memory would grow and grow
and grow and grow so eventually the
Linux out of memory killer would do away
with it but fortunately it would come
back beam the virtual machine would come
back within milliseconds so for our
operational characteristics are
operational needs that was sufficient
well I should say not ours because these
were the decisions made before I was
involved so it worked for an early stage
startup it worked to help them get
acquired and the operation it's still in
operation today that early system is
still still in production right now so
one other thing that's critical to point
out is this last line high-traffic
customers were throttled so I say maybe
throttle but actually they were throttle
all of them were throttled so we're in a
statistical modeling world so the proper
way of phrasing it there is to say that
we were doing subsampling now sometimes
you want to be able to do this for a
brownout type of notion and for those
who may not be familiar with that term
blackout versus brown out most people
associate this with the electric company
and you know electrical storm comes in
you have a blackout everyone's without
power but in the summertime the air
conditioning is running too many people
are drawing too much power the power
company the electric company may
intentionally force what looks like a
blackout to all of us but it's
intentional so they call that a brownout
so sometimes you may want that
capability but the point here is we had
to use this capability because again we
only had could only handle 200 requests
per second on one server instance so as
I mentioned room for improvement so when
you talk about an erlang server to
someone who has just a vague notion of
Erlang their mental image is likely to
be a system that can handle thousands or
tens of thousands of requests per second
200 is kind of pitiful to be honest so
we knew there was room for improvement
there but we also had a revelation and
this is one of the points that I said
we'd revisit the early-stage startup has
certain needs but two years on and after
acquisition those needs were not the
same
so we no longer needed something as
malleable as Lisp so we revisited that
and that's an important thing for an
organization as it matures to realize
okay we need to revisit the original
criteria and maybe throw some of them
out so some of the items out so we love
lists but like I said prior to splunk
common list was my bread and butter was
my server language and I write a little
bit of JavaScript for the front end work
but I mean I wrote code for the decision
maker at an ad network and Common Lisp a
high-performance recommender system and
so on so so you know the geek in me I'm
sure we have a lot of language geeks in
here so you know as a language geek you
know I didn't want to let it go but it
was the right business decision to do so
so even though so we had the premise
that even though there were no
limitations my own experience pointed to
there was probably a problem on misuse
of compare-and-swap primitives so the
emphasis there should be on the misuse
so and I'll speak more about compare and
swap in a moment so we needed to make
sure that it was what we thought it was
now as I mentioned the preceding talk on
how did you bug complex systems all of
that applies so I highly recommend you
know using all the tools that the
previous fellow recommended but in our
case like I said call it intuition call
it experience call it dumb luck but
anyway we started with the experiment so
we didn't use dtrace as trace or
anything like that so in this case the
experiment was let's eliminate the list
proportion let's eliminate the C code
that implements lisps that's where there
was a lot of complexity only have cheap
counters I should also mention we have
the ability in our server farm with
another Erlang application to mirror
production traffic so this is a
production environment with production
traffic but we don't need to rely on the
results of that traffic passing through
our server so
we're very fortunate in that regard that
we have that capability so by forking
our own code we had the LinkedIn module
that would return within a few
milliseconds now it's also important for
those who are just exploring just
entering the whole linkedin module
business this notion of reductions so
the Erlang scheduler will let me back up
a second it's best to think of the way
an erlang process works you have the
functions that are executing but it
takes some amount of time so that unit
of time for which to measure the
execution time is a reduction or
reduction counting now I'm glossing over
the details but I just want to highlight
for those that are new to Erlang
linkedin modules and ports learn about
reductions make sure you understand it
because that becomes very important so
again back to our experiment we made
sure that the experimental branch would
return within a few reductions so what
we observed was only a marginal increase
in performance so that led to a
confirmation that okay if we saw a huge
increase well the huge increase then
would have implied that it was the sea
implementation of lists it was a
marginal increase so that code probably
was not to blame so this reinforces the
notion that it's probably something else
that was already in that was still in
the code path so we went with my notion
of it's probably the so called atomic
operations and i'll explain why i'm
saying so called and putting atomic in
quotes so so our course of action was to
rewrite and pure Erlang so incidentally
this conference last year John la hoja
honest so he's from Greece his nickname
is john romano romero sorry john romero
so if you want to find him on twitter
you'll have to find it by that name he
gave a talk here last year presenting
the old architecture and after giving
his talk he went to a coffee shop down
the street and started writing the pure
Erlang for
so just a little side story so another
little detour how compare-and-swap can
be harmful and I'm not going to get into
all the details as you can see there are
little footnotes here so I encourage
people to download the slide deck after
the conference but what I want to point
out are the things that typically will
trip people up and the middle section of
the slide beware lock-free is not
necessarily weight free so there are
multiple definitions of atomic
compare-and-swap is atomic in a sense of
a transaction as with a database all or
nothing it is atomic in that it's
indivisible there's one line of assembly
however it's not atomic in that it does
not occur instantaneously so depending
on the hardware that you have that one
line of assembly could take anywhere
from a hundred CPU cycles to several
hundred CPU cycles now I'm not an expert
on the different architectures and where
the performance penalties are where but
follow the references and they get into
all the gory details so this goes back
to the comic at the beginning remember
to punch holes in the lid if you're
going down this path down this rabbit
hole of a long-lived linkedin module a
long-lived port you need to be aware of
these things so again we were using the
C for the C code for our data storage
layer so it was long-lived we also
needed to have either some kind of
locking mechanism or some kind of
compare-and-swap type of mechanism so
these were the issues so these are the
issues that I wanted to call out so I
also want to emphasize the first
sentence that's on here compare-and-swap
as a simple replacement for locks is
worse than naive because a naive
approach is uninfluenced by locking
methodology so you almost would be
better getting a student who knows
nothing about locks and say okay read
this material about compare and swap
up and go write an algorithm for us
you'll probably do better than taking
someone who's been in the business for a
while and you know my brain is all
muddled by all these different
approaches and I have made all these
mistakes so I'm not creative criticizing
the people who wrote the code before me
you know I've been there done that so
back to our specific problem so as I
mentioned we had we had a few different
places to look for where our performance
bottleneck was so the theme here is
strive for 10 X or more performance
gains there are lots of things that we
could have done we could have
implemented a proper compiler we could
have implemented garbage collection and
all these other elements for the the
Lisp system and again I mentioned I'm a
language geek I would have loved to
build those things but it wasn't
practical because any one of those would
have only given us maybe to 25 x
performance increase it's not enough so
a few years ago you would have here
heard people say well you want to strive
for 10 x because otherwise you're
wasting time compared to moore's law but
for those of you who were in the talk by
the guy with the parallel ax in his
pocket he'll point out that Moore's Law
is essentially dead from this point
going forward so you know I like what
they're doing so I thought I'd change it
to instead of Moore's Law let's just
call it parallelize law I don't know I'm
sure what the industry will come up with
some kind of phrase for what's next so
and for those that aren't familiar with
parallel ax and the company at they're
working on a thousand core system but he
couldn't get into too many details
because they're working on that now so
it is becoming a reality and he said
this calendar year so yeah so you know
there are all these other approaches
that people can take to punt on the
other low-hanging fruit so the classic
ones load balancers and so on and as far
as load balancers I always take the
approach of begin without having a load
balancer you can always add it you that
gives you that extra Headroom but it's
easy enough
to do the routing through software and
I'm talking to an erlang crowd you guys
get it so all right enough said there so
the revised architecture the top section
is the same as before one process so
everything in one process space on a
server instance one Erlang process per
customer we initially use them neva when
I mentioned that John a year ago went to
a coffee shop and started working on the
pure erling version that virgin used
amnesia we knew we would have to replace
it because we know that amnesia will
only scale to a certain point now also
by mentioning that we have everything in
one process space we are not using the
distributed aspects of Erlang we're
using Erlang for the parallel the on the
same server concurrency so anyway the
initial process began with amnesia but
we eventually moved to ETS so those who
aren't familiar amnesia itself is
implemented with ETS and debts at some
debts so the current version that we
have does have ETS and that's the one
that has a bit more than 10x performance
increase but when i submitted the talk
we weren't quite there yet we hadn't
measured it in a production environment
yet so i wasn't comfortable citing those
numbers so some other things to point
out so we have these three forms of data
that we store as i mentioned before key
value pairs sets which are essentially
arrays the way we store it and the hyper
log log the probabilistic counters so we
have those three structures and we store
it for each day of retention so say if
we store seven days of retention we may
have an eighth day because we do a
modulo table naming scheme so it wraps
around but we also have to multiply this
by the number of customers so again if
we have 30,000 customers on a box that's
a lot of tables so
so we haven't stress tested so I should
point out we have not done full loading
capacity testing with that many of
customers but we have done the loading
capacity testing with a very high
traffic customer several customers
actually and again no customer data was
harmed in those exercises usual
disclaimers so previously we were not
using Jen server or we weren't using
many OTP things but now we are so one
thing to point out the last line of this
slide we now can accept all data there's
no throttling no subsampling we have the
capability to do that on a node that
sits in front of the leafy database and
we can do it there if we need to have
some kind of brownout situation you know
Oh overcapacity denial of service Oh an
interesting thing about denial of
service so it's in the the abstract of
the talk that our traffic coming into
our network appears as a distributed
denial of service attack and when bug
sense originally tried to use Amazon
they were shut down in the beginning
because Amazon thought they were under
attack because all these hundreds of
millions of mobile devices sending very
small messages looks like a distributed
denial of service attack so anyway but
we we've worked it out we're at amazon
now and we accept all data no throttling
so consider that a huge success so now I
want to touch on some of the things
about the learning curve my learning
curve and what worked what didn't work
so what went right what went wrong what
could be better so functional
programming I think is a huge part to
explain how we were able to get so much
traction so quickly so the emphasis
being on no side effects so being new to
the language I would write a lot of test
code and I'm sure all of you have been
there too whether it's the hello world's
type of program or anything beyond that
well write it as a test case and leave
it as a test case I mean
there are some really trivial test cases
that we have just because i needed to go
back and refer to it for my own sanity
at times so i can't stress it enough and
with Erlang you have a unit e unit is a
wonderful tool for those who aren't
using it i highly recommend looking into
it but it also includes a code coverage
tool so we reach eighty five percent
code coverage was so we actually it was
me the code base was dumped entirely on
my lap so in four months we were at
eighty-five percent code coverage to get
to ninety percent took a bit more effort
and we hover we peaked at about ninety
three percent added a few more features
and it kind of came down again so test
early test often enough probably said
about that what I also want to point out
something that went wrong when we had
the amnesia system the Amiga base system
it would die after 12 hours so there's a
mailbox that's associated with amnesia
that under this very high traffic
scenario when we were doing our load of
capacity testing I mean this machine was
just being hammered it would explode
after about 12 hours the Linux out of
memory killer would step in and do its
thing so we knew that was going to
happen we put it into that environment
because we wanted to see how quickly it
would happen so the amnesia to ETS
migration we hit 10 I don't know if it's
really an architectural issue but there
were some features of amnesia that we
had to reintroduce and I kind of felt
like well you know should I have instead
started by forking amnesia no don't do
it there's a lot of complexity in there
it was easier to just write the few
functions that we needed like wait for
tables and things like that we did have
to introduce one mailbox for the hls for
the probabilistic counters because it's
a read-modify-write workflow so we
needed to introduce a synchronization
mechanism otherwise with ets the process
the Erlang process will block if it
can't write because another process is
writing
that model works just fine for our needs
so some of the things that could be
better we're not yet doing Erlang
releases for this project but as of
probably a month or so ago the other
guys are starting to go down that path
so we'll probably incorporate that
sometime over the summer for here so
pains and pitfalls and so on good
occasionally hear from my desk why Joe
why so I had a copy of joe armstrong's
book on my desk and he kind of became
you know the scapegoat if you will
metaphorically so but the end of the day
it's there's a learning curve and humans
tend to blame their tools so you know no
exception here so again it goes back to
the cartoon at the beginning remember to
punch holes in the lid so you know there
a lot of these things seem to be paying
points at first but maybe was on the
earlier slide where I mentioned it one
of the things that i would encourage
people to do especially if you're
learning Erlang rather than use google
to search to find the documentation that
you're looking for you know like i owe
format you know to look for the
particular zor whatever learn to
navigate the document tree it will save
you grief in the long run because along
the way you're going to trip over things
that you didn't know we're there and
anyway I've done that I've made that a
practice for when I was learning Python
in 99 Common Lisp again in 2005 and so
on so i highly recommend taking that
approach but you know there are elements
of the language that are not obvious
that are not intuitive so for instance
just this week on the Erlang questions
mailing list there was a question about
hidden behavior pertaining to the exit
message and Jen server so you know and
it turns out Jen server has the
terminate function that gets called
automatically when the exit message
comes so you know learning curve ok
enough said so just to
start to wrap things up so beyond that
10x performance increase if you want to
get to the next level you want the 100 x
the 1000 x and so on the classic
approach is focus on reducing contention
so reduce contention for resources so
keep in mind that many unix system calls
and their equivalent on windows if
you're on that platform and so on many
of the unix system calls will cause the
beam will cause your unix process will
cause the beam virtual machine to be
blocked by the operating system and a
lot of people i mean if i were to ask
you about that of course you know that
but when you're writing your code you
don't always think about it and
sometimes you're calling something in an
erlang module and you don't realize
underneath it's actually making a UNIX
system call so there's those sorts of
things so what I wanted to call out with
this slide are just some of the basic
things to look for now keep in mind that
when you're doing this you're
essentially doing things that Erlang
does for you that the beam virtual
machine does for you so then it's sort
of turning this talk upside down because
you're going to be writing a linkedin
module probably writing in C or C++ and
in order to deal with some of these
things so anyway just to quickly go
through these things to avoid memory
allocation or memory construction in the
list world that's referred to as non
cons and cons being short for
construction I don't know if that term
is used elsewhere but some of the
literature will refer to that because
again Lisp is over 50 years old so
there's a lot of analysis on these
techniques in the list world and I know
a lot of people only touched list maybe
in college and thought oh it's the slow
language but now actually it is it can
be very high performance I know Erlang
conference enough about less so so one
of the things about this second bullet
point when you're striving to avoid
memory allocation when you're starting
from a language like
erlang any of these virtual
machine-based dynamic languages you're
initially allocating memory in a sandbox
so don't be afraid to populate a list to
generate a tuple and so on because that
structure is probably only going to be
short-lived the garbage collector will
never see it it will never get promoted
to you know the larger memory store so
this a lot of these are double-edged
swords so be very careful when you're
going down these paths so i called them
out so you know what to look for you
know what to think about but again tread
carefully if you're going down these
paths so one of the things a whole class
of problems that you'll hear from the
c10 kc-10 em so see 10k was concurrent
10,000 connections and see 10 m
concurrent 10 million connections the
way to get there on commodity hardware
is to move your IO stack into user land
again going back to this first bullet
point you want to avoid unix system
calls that would cause the operating
system to block your process well if you
don't have to go into the kernel with
the system call you're not blocked so
one of the ways is to move the tcp/ip
stack into user land so the one that is
probably the most promising on this list
is is freebsd they have the lib you inet
so it's not you I it's you I net user
land I net so in they give a talk in
2014 and at that time they had about
seventy percent of the performance of
freebsd s- stack had but it's important
to note it's the same code the freebsd
runs the difference is some data
structures that live in the kernel have
been moved to user land so they sell
some work to do and some of the links
one goes to their github page the other
one goes to their presentation but if
that's not an option you're on Linux you
know you can't do freebsd at a prior
Erlang factorytalk actually two prior
Erlang factory talks there were two
different implementations of moving a
tcp/ip stack into Erlang and should that
fail you going away back to the 1990s
mellon university in standard ml and you
know there's standard ml you can if you
squint the right way kind of looks like
Erlang maybe so you may be able to port
from standard ml but you would also need
to update because of you know changes
and optimizations and lessons learned
about tcp/ip stacks so with that here's
a list of references again you'll be
able to download all this so I will open
it up to questions if anyone has thank
you anybody yes sir I can I should have
known if I'd have seen you standing
there so um you said that
compare-and-swap instructions caused the
major slowdown have you considered some
sorts of threadlocal structures that he
would update without ever locking the
bus so yes we thought about that and
like I said half a mrs. or the language
geek the other half would love to go
down that path it just wasn't our core
business to do those sorts of things it
was so if we still ran into problems
with the pure Erlang version as an
experiment so back before the ETS
version we would have pursued that if we
needed the performance but it becomes
trade-offs at that point so for an early
stage startup that has to keep their
operations cost lean yes if we ran into
problems with Erlang we would would have
pursued that assuming I would have been
recruited by the startup but once Splunk
acquired them so splunk had its IPO a
few years ago they have their year-end
reporting recently good numbers so it
isn't cost effective in that sense
because it'd be easier to just scale
back and say okay fewer customers per
machine just throw more machines throw
money at the problem so companies once
they reach a certain point
that's the appropriate business decision
for them but again as the language geek
and so on yes I would love to do do
those sorts of things but it just one we
didn't need to Erlang and ETS performed
well enough for us that we didn't need
to so we stopped there the questions I
would like yes
you had on your you had on your slide
but you never actually mentioned it why
not LFE oh so yeah at Erlang factory san
francisco last year Duncan gave a talk
Robert was in the audience the three of
us were chatting after his talk so the
one operator that I pointed out yeah the
module colon function it came about from
that conversation I mean Robert was
considering it he had feedback before so
it I saw it as a wart at the time
because just the colon operator so for
those who didn't know you would have an
open parenthesis a colon space the
module name space the function name it
seemed kind of weird to me at the time
so they did fix that I forget what the
code was committed and merged it was
probably about a month later memory
serves so the main reason so again being
a lisp guy I would have loved to the
reality was we have other airline code
bases and it made sense to keep
everything in native Erlang that said
one of our guys is now playing with the
lexer so go figure so yeah that's the
answer at the time I do recommend list
flavored Erlang so those who are
considering it I do recommend it it's it
seems to be robust I don't know what the
version is now I mean i watch the
mailing list but I don't use it day to
day but you know I mean same with the
liq sir you know it you're using the
same common back end so there's no
reason not to use it as far as you know
like JRuby there in the early days there
were pain points there or J Python
before it was Jeff on you know pain
points there you don't have to worry
about those issues so yeah let's play
werder lying i would recommend for those
who are interested other questions
all right thank you very much to thank</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>