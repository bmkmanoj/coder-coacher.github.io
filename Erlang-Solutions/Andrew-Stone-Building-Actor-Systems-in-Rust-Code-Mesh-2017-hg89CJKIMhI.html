<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Andrew Stone - Building Actor Systems in Rust - Code Mesh 2017 | Coder Coacher - Coaching Coders</title><meta content="Andrew Stone - Building Actor Systems in Rust - Code Mesh 2017 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Andrew Stone - Building Actor Systems in Rust - Code Mesh 2017</b></h2><h5 class="post__date">2017-12-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/hg89CJKIMhI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">faced by logging in to the admin server
on any node and appear once again appear
in a consensus group is managed
separately from the node it's running on
there are two disjoint things so you can
join cluster nodes together and spawn
consensus groups peers peers in a
consensus group separately from that so
Herot itself uses view stamped
replication for consensus it's similar
to raft in a lot of ways it's quorum
based it has a replicated log and it
uses replicated state machines as the
backend so one distinguishing feature of
the you stamped replication is that it
can operate entirely in memory the
reason it can do this is that in order
to maintain its current place in the
protocol so that it doesn't forget like
leaders that it's voted for it can
actually recover from a majority of
peers when it comes back online it
learns of the state of the system rather
than recording it to disk so this
provides it like I said to operate
entirely in memory the Herot the Herot
back-end is a hierarchical tree and this
is a tree per consensus group so each
namespace acts like its own independent
database which can be sharded which you
know where each consensus group is an
individual shard so it has a few unique
features one is that the tree itself is
versioned so you can actually do a
compare and swap on an individual node
in the tree this is because when you
update a leaf node it actually
increments the version of every parent
node simultaneously and this allows just
a single atomic operation to see if
anything's changed underneath a subset
of the tree additionally it has typed
the leaf nodes in a in a manner similar
to Redis so each leaf node has its own
data type
I guess most useful and similar to other
systems is
the blob which is just a good old binary
blob of data that you can put and get
additionally there's like there are
queue and set data types and plants at a
counter and other types but these allow
doing sending specific operations over
the consensus protocol which will do
things like implement a set Union
between two different nodes in the tree
one other one other cool feature is that
since the tree is persistent in the
functional data structure since meaning
it uses structural sharing you can
actually snapshot the tree in a
different thread from the main protocol
so you don't have to block the protocol
in order to snapshot the tree to disk
you can just send a reference to the
tree to a different thread and snapshot
it so here's an example of what the view
stamped replication proto protocol looks
like in the normal case operating inside
Herot so you can see that a client
request comes in to the primary the
primary broadcasts prepare statement to
the followers which reply with prepare
okay so once the primary is heard from
one of these followers it can go ahead
and commit the operation to the log and
then update its back-end state machine
which could be just you know pushing a
value onto a cue leaf node and then
it'll reply to the client so one thing
to note about this path diagram is that
it shows a bunch of processes that are
communicating via asynchronous message
passing this is because when you look at
the literature of distributed systems
most algorithms are described as
processes interacting via asynchronous
message passing and this pretty much is
identical to the actor model so actors
lend themselves straightforwardly to
implementing distributed algorithms so
in an actor system each process is
addressable by name and each process
maintains internal state that may mutate
when a message is received once again
there's no shared state between
processes so you can't share you can
only process can only interact by
sending messages so as I mentioned
systems distributed algorithms
are well reflected by actors and
therefore we use our actors to represent
the peers in our consensus group inside
Herot actor systems provide a very clear
way to reason about the algorithm
without worrying about underlying
implementation issues such as like TCP
sockets and all that stuff so in order
to implement our algorithm inherit I
built an actor system called rabble so
rabble just provides the actors freshens
for implementing distributed algorithms
so when you're as I mentioned we don't
really want to worry about incidental
details like network transport and
connection manners connection management
and cluster membership we kind of want
to just write our algorithm algorithms
are hard enough to write without
worrying about those things we just want
to deal with named processes and message
sending and kind of figure out ensure
that our patterns are our protocol is
correct so rabbil allows you to do that
it takes care of the underlying
communication so that you can just treat
your algorithm as a group of interacting
processes so rabble the the few things
that rabble does is it maintains a
dynamic cluster membership provides for
TCP transport and message routing so
rabbil does maintain a full mesh of
connection connections between nodes and
this is similar to distributor Erlang
rabbil itself is designed for
testability so each process and rabbil
doesn't have its own thread of control
processes are reactive only meaning that
they only respond to messages they don't
generate them on their own
so they respond to a message and then
they will either send mutate their state
and may send outgoing messages this
allows lightweight processes and Ravel
to be tested in isolation of each other
and this itself can lend itself to
deterministic test runs which we'll talk
about a bit later so rabbal is a work in
progress
you know it's used inherit but so far I
am the only customer and it needs more
instrumentation there are some counters
and histograms but I'd like to add more
just some general cleanup of other parts
of the codebase you know as can accrue
so that's just a word of warning or a
word of warning
but rebel does tend to work well for my
use case and I'm pretty happy with it so
far so another thing that I need to do
is make it scale beyond tens of nodes so
rabble itself currently uses full
membership connections between nodes but
I'd like to add in a partial membership
system so we could scale to thousands of
nodes and layer your consensus your
consensus groups over those thousands of
nodes maybe hundreds or thousands of
consensus groups running on thousands of
nodes managed from a single access point
I think that'd be pretty cool and useful
for managing large-scale data center
projects lastly to reiterate rabbil does
provide a clean way to implement view
stamped replication inherit primarily
because it allows you to extract out
your algorithm in terms of actors
without having to worry about underlying
transport and membership issues when we
talk about actor systems the first thing
we have to think about is how do we name
actors so each process in Rabel is given
a pig that has three fields so it has a
name a group and a node ID so the group
is just a way of organising processes
that share a commonality such as peers
in a specific consensus group but we can
pretty much ignore it for now so once we
can name processes we want to send them
messages so the structure of a rabble
message is Ana knew mom type and rust so
all actors share the same exact type you
can't have more than one type in in
rabble in a rabble cluster however since
it's a sum type each
each each message can have an individual
variant so it can be patterned matched
on and different processes can only
respond to the parts of the message to
the variance of the message that they
care about so we can see here that we
have two real two separate sections of
the variants there's one variant that
users use to define their type and this
can also be a nested entomb and then
there's a bunch of rabble built-ins
so the built-ins are just like specific
messages that you use to interact with
the rabble system there's no special
like built-in functionality to rabbil
where processes can just call functions
they can only communicate by sending
messages not only between each other but
also with rabble itself so they start
timers and cancel timers by sending
messages so you can see here that those
are specific grab hold data types or
variants in the rebel message I should
say so I just want to make a quick note
about why I implemented things this way
I actually implemented dynamic types
using any as well but with the
serialization issues like figuring out
you'd have to tag each message in order
to figure out how to serialize it on the
wire when you receive it and then
populate it into a type it it turned out
to be a quite quite a headache in terms
of implementation and also the actual
use use the usage code was quite on
organ ohmic and since rust is a
statically typed language just sharing a
single type kind of made things pretty
simple and straightforward kind of in a
in a rust idiomatic way so in order to
route messages throughout the system
they're encapsulated in envelopes and
envelopes have four fields
there's the addressee the sender the
message itself and a correlation ID
perhaps the only thing that's not
obvious here is the correlation ID you
could always respond to a sender by
looking at the from field however if
your message takes multiple hops
throughout a system and it needs to go
back to the original sender that can be
tracked in a correlate
and ID correlation IDs can also track
like connections in an API server things
like that there's there's a few
different fields besides just the
sending pit I'm not gonna get into
detail just keep in mind that it allows
you to kind of respond to the call to
the original sender so we've talked
about messages we've talked about
envelopes and we talked about naming but
what do our processes look like in
rabble so processes implement a trait
and a trait and rust is an abstract
interface that describes a specific
behavior a type has in the form of
methods so all processes need to
implement at least the the handle call
back so that they can be executed by the
runtime so the handle method is
interesting and then it has no external
side effects so state can be mutated
internal for the actors so say when a
message is received the handle callback
is called with that message along with
the sender and correlation ID and all
that happens is the internal state of
that actor itself can be mutated but
messages themselves aren't sent directly
as I mentioned earlier there's no
special function calls that processes
can call to interact with the runtime so
to keep things to enable easier
deterministic testing instead of sending
directly messages actually return or
outgoing envelopes in response to a
message are actually pushed on to this
output vector and that allows the
runtime to pull them off after the
function call returns and route them
appropriately
this makes things easy for testing
because you don't have to mock out
communicating processes you can actually
just call the handle function of the
process under test directly and verify
the messages in the output Veck and the
internal state matches what you expect
so one other thing to note is that
there's this extra parameter to pass in
we pass in the mutable vector however we
could just return avec I mean that would
seem to be the more straightforward
choice but this is done just simply for
a minor performance win
if we pass in a reusable vector then we
don't have to do an allocation every
time a message is received so here we
see a diagram of a simple echo server
there's a p1 as a client sending to p2
and it's just sending an envelope you
know from p2 to p1 or from p1 to p2 and
it has a user message and the specific
user message type in this system is
hello not very useful and then there's a
correlation ID and I ignored most of the
few of the fields and I just show that
it's a sum p1 since it's optional some
denotes that the field is actually
filled in so the correlation ID says the
original sender is p1 it gets sent to P
2 and P 2 responds just by flipping the
two in from fields essentially and
responding with the same message and
correlation ID so that's what the
diagram looks like and here's how you
would implement that echo server as a
lightweight process in rebel so all you
can see all you have to do is implement
the handle function here so it's kind of
a contrived example in order to show
pattern matching so you can see bross
uses the match keyword to implement
pattern matching so we match on the
receive message and we ensure that it's
of our user variant and that it contains
solo and then we take the sender and
populate it into the two field and we
fill in our own internal internal pid'
for p2 in the from field and then we
recreate the message by just creating a
user message with hello in it and then
we create the envelope with two from'
message and the original correlation ID
and then we push that onto the output
back so pretty straightforward all right
so one thing is that since rabbil
processes are purely reactive they don't
have a threat of control there needs to
be something that can locate processes
when envelopes are sent throughout the
system and then execute them and this is
the job of the executor the executor it
runs in a single thread it's it's pretty
much the most naive way of doing this
possible each process doesn't have its
own mailbox all envelopes received by
the executor are processed in FIFO order
so you'll see there's a single thread
and it's pulling envelopes off of Rosse
channel and it looks at the sender or
looks at the destination the to field in
the envelope it looks it up in a hash
map which Maps the pid' to the trade
object which is a process and then it
takes a reference to that process it
calls the handle function with the
fields from the D structured envelope
and then it takes the output that goes
into that output back and takes all the
local envelopes and pops them back on
the executor channel so they can be
processed in FIFO order so not shown
here is that envelopes that are destined
for remote nodes are sent forwarded to
something called the cluster server
which will route them to other nodes
will discuss the cluster server in a bit
so the cool thing about the executor
being really simple is that it lends
itself easily to being reimplemented as
a deterministic scheduler for testing so
you can do property based testing where
your generator is simply a list of
messages and in order to do this there's
really five steps you construct a group
of processes in an initial State you
generate a schedule of test messages so
I use quick check for this or else quit
check works pretty well then you call
the handle function you have our test
scheduler call the handle method of the
process with the test message you
collect the output messages which are
envelopes really from the handle method
and then you schedule these so you can
schedule these in a multitude away you
can actually run through all the output
messages before you take the next
test message from the generator and you
can like send them to their processes
and then basically collect all the
output messages from an initial test
message so basically exhaust that single
protocol step before you take the until
there's no more messages left and then
you take the next test message off the
generator alternatively you could inter
leave those messages with the test
messages to ensure that different
protocol steps can interact can interact
safely when interleaved so that's like
your single threaded verse verse
concurrent testing model so the neat
thing about driving property based tests
via messages is that since timeouts are
just messages you can actually trigger
protocol state changes just by adding
test message time-out messages to your
test generator so since we're using V
stamp replication as our protocol we
want to be able to test the different
state transitions we want to test that
we want to trigger a leader election on
demand we also want to do things like
trigger commit messages to be sent so if
we set our election timeouts to zero
every time a timeout message is sent
from the test generator to a follower
it'll also actually cause a leader
election to occur and the follower that
receives the timeout message will send a
bunch of will broadcast some start B
change messages similarly we could send
a timeout message with it with the
keepalive timeout of zero to the primary
in the system and it will actually force
a commit message to be sent that's just
how the V stamp replication protocol
works we can also drop and delay and
interleave message outputs from
different replicas in order to further
explore explore the scheduling state
space
so we've seen how we generate tests and
and how we can kind of make them trigger
our phase changes but we also want to
know when things are incorrect so we
need some sort of assertions so we can
treat each handle call as kind of a step
in the test and before we call a handle
we call the handle function with a
message we can actually check a
precondition and afterwards we can check
a post condition but more importantly
since we actually step through these
messages individually we can stop the
world and check all the replicas and
ensure that global state invariants hold
so an example of a global stadium
variant is something like we can't have
two leaders in this with the same epic
and view number if we see that we know
that we violated an invariant and our
test fails so we can take these tailing
failing test runs since they're just a
list of messages save them to a file and
then run them as a regression suite so
this is pretty handy just to ensure that
you don't reintroduce the same bug if
you add some performance improvements or
something like that one other cool thing
you can do since messages are test
generators and you can kind of step
through the messages is since you treat
a message receipt as one step in a test
schedule you can build an interactive
debugger a reversible interactive
debugger actually where you run through
all the schedule of messages and record
the state at each step of all the
processes and then you can when an error
occurs you can actually step backward to
the previous state and say see what has
actually changed to induce the invariant
to fail so you just start in your
initial state that you would in any
normal test and then you play the
message messages as received at each
replica and diff the the last state and
the current state of the replica that's
causing the problem and I actually did
build this at one point into Herot and
it worked pretty well it did allow me to
tease out one bug without having to like
look at super long traces
of messages but unfortunately I couldn't
keep development pace with it with the
rest of the project because it was I am
a one-man team up until fairly recently
so I dropped it but really what I'd like
to do is take this type of debugger and
build a more general one that isn't just
used for vrr and can be generalized and
included in rabble so that users can
build their own systems with their own
debugger so far all we've talked about
our lightweight processes on local on
local nodes and how the executor runs
them but there's also something called
the cluster server and the cluster
server is just a simple non-blocking
TCP server that runs in a single thread
and it has a connection to every other
node in the cluster so messages are
forwarded from the executor to the
cluster server when they need to be
routed to other nodes in the system
grabble cluster membership is dynamic so
you don't need to fix your cluster size
right up right at startup you can add
and remove nodes concurrently if you so
desire I don't know why you do it
concurrently but it is safe because we
maintain membership in an observer
moveset which is a CR DT so if you were
to concurrently add and remove the same
node from different parts of your
cluster when the nodes gossip that
information around they'd converge to
the same state and because we use in add
one set it'll just ensure that the node
isn't removed we just want to ensure
that it's deterministic you can make the
opposite choice and say if there's
concurrent adds and removes remove the
node it doesn't really matter
other than we don't want to wind up in
the state where we have some nodes that
think there's only five nodes in the
cluster and then other nodes think
there's four
now all that we've discussed so far are
lightweight processes and lightweight
processes are reactive we've mentioned
that the executor runs them in a single
thread that's it's useful for writing
distributed algorithms but it's not
useful for building real systems because
sometimes you have to interact with
things like a disk or you know you want
to run an expensive calculation and for
that you can't block all other processes
indefinitely while you wait for those
computations or disk read or write to
finish so we have a second type of actor
here that is called the service and a
service runs in a single thread and it
can it has its own pit and it registers
a channel with the executor so that way
when messages messages can be sent
between services and processes in order
to do things like log to disk so a bunch
of let's say for instance in heret we
want to add a right ahead log to the vrr
protocol which doesn't currently exist
but if we want to log every operation to
disk we need that so we could for
instance have multiple consensus groups
running on the same node and we could
send each commit to or rather each write
each each new operation to the right
ahead log in the disk manager service
and the disk manager can log them all to
a shared right ahead log and then when
it completes it can just send messages
to the requesting process so that way we
can run our processes for our algorithm
concurrently and worry about managing
heavyweight things in a separate service
so the main reason I did this was
because I'm not a scheduler expert and
it may not be the most elegant
architecture but it works for my use
case it was quite pragmatic and it
really did simplify the implementation I
feel like it may also simplify debugging
although that's just a gut feeling but
we'll see how it goes I guess in the
future if we ever put things into
production so I have a few takeaways
here I learned a lot
building rabble and Herrick I've learned
that not everything I built is perfect
and nor should it be at least to start
so with that let me get into some of the
key things that I've learned so one
thing I learned people some people in
the audience may say why did you use a
single message type for your for all the
all the actors in the cluster and
honestly the reason is because it just
kind of fit it's all I needed at the
time I didn't need an OpenType I was
building one system to run on top of it
and every time I tried it to make tried
to make the type deterministic which
happened about three times over the past
year so it ended up being horribly ugly
and tripling the size of the codebase so
I just stuck to in a new so yeah it was
a pragmatic choice to just get something
working and I actually think it was good
enough another takeaway is that writing
good schedulers is hard I've read a
little bit of scheduling literature
I actually went ahead and started to
implement a distributed scheduler for
the or multi-core scheduler for the
process where the scheduler scheduler
per thread or scheduler thread per core
with work-stealing and parallelism and I
you know I took some time I looked at
the beam code which is hideous by the
way I also looked at the Pony scheduler
which is much nicer really small so I
guess take what you will away from those
anecdotes and got 95% of the way there
implemented and then I realized that for
my purposes this is actually probably
gonna make my system way slower than it
would be just running everything in a
single thread since I don't expect to
have like 10,000 consensus groups
running on a single node so yeah there's
a downside you kind of have to figure
out where you're gonna split your work
right you have to ensure that you move
your heavyweight workloads off to a
separate service thread which
theoretically could also be a thread
pool I don't think it'd be that hard to
adapt a service to be a thread pool
rather than a single thread but yeah
what we have is good enough and it
allowed me to make progress another one
is that design your systems to make
deterministic testing easier so this
isn't always possible but for the view
stamped replication
protocol since everything operates in
memory typically although we are adding
a right ahead log you can actually make
everything deterministic just by sending
messages and replaying those messages
and if you can do things like this you
can when you no test shrinking occurs in
your property based testing you can
ensure that you're not you can ensure
that you can read you can reuse the same
failing tests over and over and and not
not be not now have the possibility of
trying to catch the same error over and
over like you you should be able to
repeat repeatedly catch that same error
I don't know deterministic testing has
made things a lot easier than some of
the experiences I've had working with
other actor based systems I wouldn't
necessarily say that rabble is better I
would say actually it's probably
strictly worse than things like our line
and maybe even akha but there were
practical constraints that led me to
building it and deterministic testing is
one advantage that rabal has I think
over other systems because it does place
envelopes in the in an output vector
rather than allowing like side affecting
sends from within processes during
property based testing so lastly all
those all those takeaways have a bunch
of thing have a few things in common
they're all compromises and they're all
kind of pragmatic decisions for me
working as a one-person team and they
allowed me to build a good enough
solution that works for my needs and
that works now without just taking
forever to implement so don't attempt to
build the perfect system focus on what
you need and refine it over time so
here's some links to the code there's
rabbil itself there's the observed
remove set and the observer move set
itself is what's used for membership and
rebel and then there's Herot which is
under VMware's github account and i'd
like to thank a couple people so Tom
santero who works at post minutes he
really helped me a lot he let me give
this talk to him a couple of times the
original version was much different and
he
basically had me redo the entire thing
so I really appreciate all his help but
note that any horrors that you may have
seen or omissions or just terrible
terrible talking nonsense is all mine
I'd also like to thank Justin she/he
because he hired me to work on her at
VMware and he's been a really good
mentor over the years and lastly I'd
like to thank VMware for paying me to
work on this stuff and letting me do it
in rust at least up until now thank you
it sounds like you had some very
particular kind of use case and design
criteria you could you could saying that
was good for what we're doing but I
didn't kind of get a clear picture of
what at what that underlying use case
was oh the underlying use case was what
I talked about at the beginning was a
Herrick so I want rabble's the actor
system underpinning Herot which is a
coordination a coordination system and
kV store
anybody else should we do a jig I don't
know all right I'll ask a question can
you talk about can you say something
about why you chose view stamped
replication specifically so there's
there's two answers that one is that we
actually have some diskless use cases at
VMware and raft and paxos and all the
other consensus protocols require
logging to disk and that was pretty much
a no-no out of the gate we wanted
something that could operate entirely in
memory there is a downside to this
though the big downside is that if you
lose a majority of your peers you can't
recover ever so your system is dead in
the water this is a reason that if you
do have disks you probably want to add a
write head log because this is usually
important data that using a consensus
system for although sometimes those
operations are short-lived so in a disco
scenario you know you can trust that
over you know a 10-minute period where
that data is valid you won't lose you
know a majority of your peers the other
reason is that there's a million raft
implementations and I've actually
implemented one before in Erlang and
Justin just wanted to use vrr because he
loves it
I don't really know rust so I was I'm
asking about the benefits of using rust
oh sure
so it's funny funny you asked so
originally the first ten minutes of my
presentation where as showing the same
small function over and over and how the
compiler won't let you run it won't let
you build it rust is pretty cool and
pretty original and that it's a systems
language and it will ensure no data
races at compile time so if you have
immutable if you have an alias variable
among multiple threads um say a
reference shared among multiple threads
well you can't in Ross because it won't
even let you compile that what you have
to do is actually wrap it in a mutex and
to ensure that it's safe to share across
threads and wrap that mutex and an
atomic reference counted pointer so that
way the value doesn't go out of scope
and become Delocated when it's used in
another thread so Russ can guarantee all
these safety things at runtime that lead
to horrific vulnerabilities and bugs and
C n C++ programs and sometimes those
bugs aren't just crashing there there
are things like corrupting your memory
state so that your program doesn't
actually work at all but it but it keeps
running and everybody thinks it's
working and the world burns and Ross
prevents that so and it's fast
okay let's thank the speaker again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>