<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Simon Marlow - Haxl A Big Hammer for Concurrency - Code Mesh 2017 | Coder Coacher - Coaching Coders</title><meta content="Simon Marlow - Haxl A Big Hammer for Concurrency - Code Mesh 2017 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Simon Marlow - Haxl A Big Hammer for Concurrency - Code Mesh 2017</b></h2><h5 class="post__date">2017-12-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/bvgUTS0pX1k" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so my talk is about axle
so tell me set the scene just to start
that we're going to talk about IO today
mainly so you've heard lots of things
about beautiful functional programming
during this conference so far but sooner
or later we have to write programs that
interact with the real world and that
means doing some kind of i/o but I know
it gets really messy and it has a bunch
of problems so some of these problems
first of all it tends to be quite slow
for example fetching data from a remote
server is much slower than fetching data
from local memory there are lots of ways
in which IO tends to be slower than
doing local things and that means that
we don't want our programs to be waiting
around for the IO to finish so we try
and overlap the IO as much as possible
or maybe we try and batch it together
and do it in a single batch but in some
way we need to do some kind of
concurrency to avoid our IO
taking too long the second problem is
that I Oh tends to be hard to test
that's because if you do IO again you
might not get the same results that you
got the first time the IO is interacting
with the real world the real world
changes underneath your feet the results
that you get will be different because
the world has changed so that means
constructing a repeatable test case can
be quite challenging the third problem
is that it's hard to debug when
something goes wrong that's because the
conditions that existed at the time when
you did the IO might not exist again you
might not be able to reproduce exactly
what happened when the IO failed so that
makes it hard to debug exactly what
happened maybe you're trying to debug
something that happened on a customer's
machine or happened in in production a
while ago and the same thing doesn't go
wrong in the same way anymore so these
problems might not be seem related and
in fact it was quite a surprise to me
that they were actually related because
I was working on a solution to the first
problem and the solution I ended up with
happened to help with the secondary
problems so this is kind of
prai's to me hopefully you'll find it
interesting but maybe you're thinking
Simon what are you talking about these
are all trivial things I I do these
these things all the time right I know
how to do concurrency I know how to test
my code I know how to debug problems
when they happen and that absolutely
right you're right for example every
language has some way of doing
concurrency so here are a few examples
of languages that you might have used in
Python we've got asynchronous i/o
JavaScript has an asynchronous thing
Haskell we've got lightweight threads
and various abstractions around those
C++ has you know various different
things including futures as I've it here
so these are all good ways of
overlapping i/o in particular the
asynchronous sort of async/await style
api is that we see quite a lot these
days
very good ways of overlapping i/o
without going all the way to having
heavyweight threads so these are very
very popular and that's all very well
but there's a bunch of problems here so
first of all I have to remember to do it
so I've got to go and look at my program
and notice the things that I can overlap
safely and I've got to write something
explicitly my program that does that and
even if you do it there are ways that
you can still go wrong so if you await
too early for a result before you
actually need it then you're limiting
the amount of concurrency that you can
actually exploit in the program and
nothing's gonna actually go wrong if you
do this you know you're just going to
get a program that's slightly less
efficient than it could be and then
someone comes along later on to refactor
your code and they change the
dependencies between things and now the
things you can do concurrently before
can't be done concurrently anymore and
you know maybe the there are opposite
things where you can do more concurrency
than you could have done before so you
have to go and fix all those awaits and
we refactor everything and when you've
done that concurrency is is cluttering
the code so it's still it's obscuring
the functionality that you were trying
to express clearly with the code you're
writing and in fact even doing the
refactoring is just harder because
you've got this extra structure you've
got this extra grouping and the
dependencies of obscure
or making it hard to refactor the code
purely due to functionality but I'm sure
you're probably thinking well this is
here for a reason we have to do explicit
concurrency because there are these
things called side effects and if I was
to just overlap everything arbitrarily
things would go wrong because now my
side effects happened in a
non-deterministic order and I can't tell
what's going on in my program anymore
and yes that that's absolutely true but
what about the parts of my program where
there are no side effects well where I'm
not very sensitive to the ordering in
which things happen and in fact that
happens quite a lot so in our programs
we often do things where we're gathering
some data
maybe we're fetching data from from a
remote database or other services that
we're making some decisions based on
that data then maybe we fetch some more
data and we makes more decisions and
finally we take some action so this is a
common sort of pattern that you see and
one of the places that this very
commonly happens is for example in
generating a web page so the user makes
a request to the web server the web
server goes and consults various
databases other services caching
services whatever and then it generates
some HTML and sends that back to the
user so this is all you know fairly
side-effect free and I'm not going to
tell you absolutely side-effect free
because technically speaking fetching
data from a database is a side effect
because the database might actually
change right but usually you don't
really mind about that if you really do
mind about that then you probably should
be using a database where you can fetch
a consistent snapshot and then make
multiple queries against that snapshot
but most of us don't use that kind of
database we're just happy to make
multiple queries and assume that
everything is fine so I'm going to I'm
going to do two things I'm going to
ignore the problem I'm also going to try
and reduce the problem as much as much
as possible by caching things so I'm
going to cache requests so that if I
make the same request again I'll get the
same results and I'm not going to be
affected by data changing
so the main claim I want to make here
about concurrency is that in the parts
of your program where you're insensitive
to things running in a different order
when there are essentially no side
effects that you're worried about then
concurrency is a better default to be
working with and I'll give you some
illustrations as to how that might work
but before I do that let's go back to
the other problems that I mentioned so
how do we test our i/o code how do you
test your ion code let me make you but I
I often just try it a couple of times
and if it looks like it's working then
that's that's pretty good and maybe when
I'm writing my writing my code and
sending out for review maybe the test
plan on the right is that it works once
on my machine on that and that's about
the best that I can do the best you can
do how can we actually make a reliable
test given that the world is changing
underneath our i/o well one thing that
you can do is to have a special part of
the world that you reserve just for
testing maybe a little part that you
don't worry too much about and this is
all very well but there's a few things
that we have to rely on for this to work
so first of all we got to rely on the
fact that this little bit of the world
that we're using for testing is
representative enough of the rest of the
world so that our test is actually
testing something that we care about
secondly we have to make sure that the
world is actually working because we're
using the real world albeit a small part
of it to test that code and the third
thing we have to worry about is that the
test might not just touch this small
part of the world that we've reserved a
testing it might accidentally go and
touch another part of the world and that
would be bad so this is not an ideal
solution so maybe we could have some
kind of fake world we could have an
alternative world that we're going to
use just for testing our code on this
has to pretend to be exactly like the
real world and this is what we often
call a mark API
a substitute for the side affecting
parts of our program or the parts that
interact with the world and the mark
will just replace our API with one that
that's going to return results from an
internal database or something instead
of talking to the real world but somehow
we have to populate our NOC API with
some test item we have to get some test
data from somewhere so you can write it
manually that tends to be quite hard
that's quite a lot of work involved in
doing that sometimes what we try and do
is to record the interaction with the
real world when we run the test for real
and you know capture that recording and
place that in our mark API so that the
mock API behaves exactly like the real
world did at the time when we recorded
the data and that's fine but we have to
build up lots of infrastructure to make
that work and it tends to be different
depending on the different kinds of i/o
that we're doing so you might have to
solve this problem multiple times and
this is often what we do when we really
care about it you know if you if you're
writing a system that needs to be
heavily tested that needs to be working
we're willing to go to quite a lot a lot
of effort to make sure he's tested well
so we might build a mock API but I
wanted to be easier than that
I want my mock api's just to exist just
to to exist a very little effort so what
about debugging let's go back to this
problem about debugging what happened
when something went wrong in production
and now I've got to figure out how this
thing went wrong and what the problem
was and I can't just run it again
because now the world has changed and it
won't behave in the same way so one
thing we often do to address this
problem is to do lots and lots of
logging we we write out data perhaps in
production perhaps to a log on the
customers machine whatever in some way
that we can get this logging data back
after the failure has happened and
inspect the log to try and find out what
happened well that's not a great
solution to the problem either again you
have to remember to do it so you've got
to remember to put logging in the right
places in your code you
to anticipate the kind of things that
might fail you got to log all the stuff
that will let you inspect it later on
and diagnose it and often you might not
log enough stuff you might forget to log
something and then when the failure
happens you inspect the log there's not
enough in there to diagnose what the
problem was you've got to go back add
more logging hope the failure happens
again and then hopefully you'll be able
to diagnose it the second time and
hopefully you don't have to do that too
many times in order to diagnose the
problem and then finally all this
logging actually trusses the code and
you've got to make sure that everything
is printable and you know you got all
these logging statements interspersed
with the actual functionality so this is
not an ideal solution what I want is to
somehow just let me reproduce exactly
what happened when the finally it
happens I want to get the failure and I
want to get enough extra metadata
somehow that will let me reproduce
exactly what happened and to diagnose it
so let me digress just a little bit and
talk about big hammers so the idea of a
big hammer is some kind of technology or
library or abstraction that solves an
important problem one or more problems
sometimes and it really solves these
problems are good and these big hammers
tend to have a common characteristic
which is that you have to go to some
effort to adopt it okay so you maybe
have to write some boilerplate or you
maybe have to change your workflow or in
some way you have to you know make a
conscious decision and and put some
effort into actually adopting the
technology so let me give you an example
of the kind of things that I'm talking
about distributed source control is what
we would call a big hammer I think I
come from a time before we had
distributed source control where we used
to email patches to each other on
mailing lists and we used to you know
manually patch our local copies of the
source code and it was very difficult to
synchronize with other people working on
on their own branches
nowadays we take that very much for
granted that we can look at other
people's branches we can merge those
branches we can we can exchange changes
very very easily so that's really
revolutionized revolutionize the way
that we do distributed development
collaborative development
another thing is garbage collection so
garbage collection addresses the problem
of memory management it makes the
problem of many memory management from
the programmers point of view almost go
away well maybe it gives it to the jvm
team at your company but from the
programmers point of view they don't
have to worry about memory management so
that really helps the productivity of
your developers language-independent RPC
so this is something that is very very
useful if you're writing code in a
heterogeneous environment where you want
to have different components of your
system written in different languages so
you want to use the language that's the
best tool for the job so you have
different components are in different
languages but they want to talk about a
common data type or they want to talk to
a common service so you write the
specification for that service there's a
language independent way and then you
can compile that specification into
serializers and deserialize Azure human
services clients and servers for that
service so they just let you choose the
right tool for the job in different
parts of your system and that's a really
good generalization so where I'm going
with this is that the the framework that
I'm talking about axel has the
characteristics of a big hammer I don't
to be too presumptuous and say that it's
you know on the same level as garbage
collection or source control but axel
has some of the same characteristics
it's going to solve the problem namely
concurrency and also help with testing
and debugging and you're going to have
to go to some effort in order to use it
so axel is a haskell library and it
provides an abstraction over concurrent
i/o it also works in conjunction with
something that we implemented in the
compiler in the haskell compiler called
the applicative do extension
so I'll talk a bit about what that is
shortly let me give you an example so
the example I've chosen is a script that
you might write in a kind of DevOps type
setting so the script is going to the
idea is that we have software installed
on a bunch of machines somewhere and we
want to update that software to the
latest version and the machines running
the software might not all be running
the same version of the software so
we're going to have to interrogate each
of the machines on the network to find
out which version of the software it has
and then for those machines which are
running an outdated version of the
software we're going to update those
machines so we'll need a few operations
to do this and this is the API that I'm
going to be working with so get latest
version is an operation that returns a
version so you can see the type on the
right here this is a Haskell type
signature with the type on the right and
type says it's a hack so computation
actually is the framework that we're
using and when you see a type that has
hacked something like that
it means that this computation may do
some data fetch you know it may do some
IO underneath it may fail but in the end
it's going to return a version if it
returns the second operation is get
hosts so this is going to interrogate
some state somewhere and return the list
of hosts that we have on the network get
installed version is a function that
takes a host and it's a computation that
returns a version so it's going to
interrogate that host to find out which
version of the software it has available
and return that version update to is an
operation that takes two arguments it
takes a version to update two it takes a
host and watch which to apply the
updates and it returns no arguments this
one is actually a side-effect it's
really going to update the software on
that machine so here's what a script
looks like so it's a do expression in
Haskell and do introduced is a list of
statements so the first statement is
going to get the latest version just
call that operation get latest version
and bind that to the variable latest
next we're going to get the hosts and
bind that to the variable
hosts next we're going to interrogate
each of those hosts to find out which
installed version of the software it has
and to do that we'll use the mapping
function from Haskell so map M takes a
function which is a computation and it
applies that function to all of the
elements of the list in the second
argument so it's calling get install
version for each of the elements of the
list hosts that we got on the second
statement and it returns a list of the
installed versions for those hosts so
next we have to construct a list of all
the hosts that we need to update and
that list is going to contain all of the
hosts for which the version installed on
that machine is less than the latest
version so this is a list comprehension
in Haskell it's a list of H such that we
get pairs of H and V by zipping the two
lists host and installed and if V is
less than the latest then we will put H
in the result list finally now that we
have our list of hosts to update we can
call map M the underscore here means
that we throw away the result we don't
care about the result and we're applying
the update to function applied to the
latest version to each of those hosts
that we got from this list comprehension
okay so this is just really a
straightforward sequence of operations
just a really a recipe for what it means
to update all these hosts to the latest
version but clearly some parts of this
clip could run in power though which
parts let's look at the dependency graph
and what's going on here so from the
start we can do both
get hosts and get latest version because
there was no dependencies between those
two statements so we could definitely do
those two things in parallel
next we could also do all of the get
installed version operations in parallel
they were part of a map we were mapping
that operation over all the hosts so the
map is completely independent and we
could do all of those in parallel for
sure finally when we have the list of
hosts to update
and we had the latest version that we
need to update to we can do all of the
do up update operations in parallel -
and the nice thing is that if you run
this script as is with hacks along with
this compiler extension that I mentioned
axel will actually execute it according
to this dependency graph so it will
exploit all the parallelism that's
that's inherent in the dependencies
between the statements so how does that
work well data dependencies determine
the ordering the compiler is going to
analyze the dependencies between those
statements and use that to determine
which order we do things in and which
things can be done concurrently we
didn't have to write any explicit
concurrency constructs so the programmer
doesn't have to write any async await
future any of that sort of stuff the
concurrency is just inherent in the
dependencies and we just wrote the
obvious code so you might use this for
simple scripts like this where you
wouldn't probably go to a lot of effort
to make sure things were optimally
concurrent if you can get concurrency
for free why wouldn't you do it so I'll
draw a conclusion draw a comparison
between hacksaws and garbage collection
is so garbage collection is abstracting
the way from memory management and in a
similar way hacksaw is abstracting away
from concurrency and neither of these
things give you the optimal solution so
if you're prepared to do your own memory
management right if you writing C++ you
can do a very finely tuned job of memory
management and and do better than the
garbage collector would be able to do
but then if you use the garbage
collector you can get 80 or 90% of the
way there for no effort and hacks always
a similar similar kind of situation
where if you're prepared to do the
scheduling yourself you can probably do
a better job than axel would but hacksaw
is going to get most of the way there
without you having to do anything so to
put it in a slightly different way we
just change the default so instead of
things running strictly sequentially
things are going to run concurrent
concurrently where where they were
possible where the dependencies allow it
and this makes a lot of sense in the
parts of your program where concurrent
is the right default so those parts
where you're insensitive to the
reordering of things and you don't mind
if things get happened things happening
concurrently so how does it actually
work well the main problem that we have
to address here is that data
dependencies are not first-class things
in the language that you can talk about
in a library so I can't I can't write a
library that talks about the
dependencies between two statements in
the do expression I'm going to need some
kind of compiler support in order to be
able to do that but we don't to build
hacks or we don't want to build anything
specific about the way that we do
concurrency into the compiler so we had
to search for a more general solution to
this problem so here's an example going
back to my example earlier and these two
statements get latest version and get
hosts and these two things are
completely independent but we can only
know that by looking at the names of the
variables and seeing where those
variables occur in the statements so the
compiler has to get involved here but
the way the compiler deals with this
expression is the treats it as syntactic
sugar if you go look in the haskell
report it tells you what the definition
of the do expression is in terms of
syntactic sugar and it expands literally
into this expression here where we're
using the monad bind operator it's
greater than greater than equal to
operator to combine the statements so at
the top here we've got get latest
version and that's the left-hand
argument to bind and the right-hand
argument is a function from the variable
latest which gets bound to the result of
get latest version and then we do get
host and that gets bound and so forth so
this is how monads work every monad
provides a different implementation for
this operator and that's how the Duke do
notation works with different monads and
provides different behavior depending on
the monad you using but we've already
lost at this point
if we allow that to be the D sugaring of
our du expression and that's because the
monad bind operator
from its type we can tell that it's
already sequential because the first
argument to bind is a computation that
returns an A and the a that we get back
from this computation is passed to the
function that's the second argument to
bind so interestingly this almost this
exact same slide appeared in the talk
yesterday in in Daniels talked about IO
and Scala and he was pointing out that
this is a good thing because IO enforces
sequentiality and here I'm saying that
this is a bad thing because IO enforces
sequentiality which is bad if what you
want is concurrency and but the so the
main point here is that sequentiality is
is a fact of life of monads it's
embodied in the type of the bind
operator so this is really the
sequential combination operator and once
we've done this teacher going
it can only be sequential we can't go
back from here and in for any kind of
concurrency at all so we need to use a
slightly different abstraction
unfortunately Haskell provides this
abstraction called applicative so
applicative has an operator that we call
a plus and finder splat this angle
brackets star operator and it combines
things in a way that can be concurrent
given a suitable implementation and axel
is going to provide that implementation
so why is this concurrent or why is it
able to be concurrent it's because the
type of the app operator takes two
computations but they have no
dependencies between them the first
computation that it takes returns a
function from A to B and the second
computation returns an A and finally
after both of these computations have
taken place we apply the function that
we got from the first argument to the
argument that we got from the second
argument and finally we get a B so the
difference here between monad an
applicative is that to run the second
computation of the monad we needed the
value from
argument whereas with applicative we can
run the two computations and then we
combine their results and the two
computations can run concurrently and
this is all very well but I don't want
to write this stuff by hand okay it's a
little bit complicated the operators
have perhaps a bit of school but I think
that's not the real problem the real
problem is that I've had to do something
explicit I have had to look at my
program and figure out what I can do
concurrently and you know group those
things together using applicative and
that's exactly what I didn't want to do
I wanted this to happen automatically so
this is where the compiler comes in and
the extension that we added to the
compiler called applicative do so
applicative do takes one of these do
expressions and it analyzes the
dependencies between the statements and
it instead of doing that simple dish
agreeing that we saw earlier on where we
just use the monad bind operator it will
use the applicative operator where it
can and that's not necessarily
everywhere because in places where we
have real dependencies between
statements we will have to use bind but
in the places where we have no
dependencies we can use applicative
operator and this will enable whatever
is implementing these operators to
behave in a concurrent way and in
particular the axle library is going to
implement this operator the app operator
to provide concurrency maybe you're
wondering how exactly does it do that
well I'm not going to go into details
about how the implementation actually
works but what it looks like from the
users point of view the user of the axle
library is that you get to see a whole
bunch of i/o that you can execute
concurrently so your job as somebody
who's using the axle library is to
implement this function called fetch and
fetch gets passed by axle a list of
things that you can do concurrently and
in this case I probably have some
operations like get latest version and
get hosts in fact we'll get past these
two operations and it's the job of fetch
here the job of your implementation of
fetch
you returned the results of those
operations and it can implement that in
whatever way you deem appropriate
you could fork threads or you could use
asynchrony underneath you can use
whatever method you want to maybe you
batch these things together and send
them off to a single server or whatever
but what axle is doing for you is it's
telling you which things you can do
concurrently by executing the program if
you think about this in a kind of a
dependency graph way the the computation
that you're running evolves into a tree
like structure where the branches in
this tree are created by the applicative
operator every time we use applicative
we can go down to branches and if you
explore this computation tree you will
get to a whole set of of i/o operations
the yellow blobs that i've used in this
diagram here so hacks 'el explores the
tree it finds all the things that it can
do concurrently the i/o operations and
it performs them all at the same time
and what we call a round so round one is
already performed all these yellow blobs
here and then when those results are
available then they unblock further
parts of the computation and we can
explore more of the tree and then
perhaps we'll find some more i/o and
this process repeats if we do a round of
i/o we did some more computation and we
keep on doing this until the whole thing
is finished and we have the answer at
the end so taking away the tree it looks
like this we do computation we do some
IO really some more computation do some
more i/o and so forth but one problem
with doing it this way is what happens
when one of our i/o items ends up to be
very expensive much more expensive than
the others so the difference between
this and this is that we're now doing
the computation much later a second
round of computation much later than we
would have done if they were all the
same size so this is not making the best
use of the resources that we have
because we're waiting for just one thing
to finish and perhaps we could have done
some more computation that depended only
on these bits of the i/o
so that's one limitation the way that
we've done this but this limitation is
kind of inherent sometimes in the code
that we write so I want to go back to
this example that I had earlier on with
this script and it turns out that this
is not the best way I could have written
it because what's happened here is that
I'm mapping the get installed version
over this list of hosts so I have to get
back this full list of installed things
before I can go onto the next step and
do the updates and perhaps a better way
that I could have written it is like
this where I've here made a local
function called per host this takes a
host as an argument it does the get
installed version call for that host and
then it makes the decision about whether
to do the update too and finally it does
the update to if necessary so here I'm
I'm kind of refactoring the way that we
were doing the operations which gives
rise to a slightly different graph
instead of this dependency graph now it
will look something like this
so now I have for each of the hosts I'm
calling get installed version and the
result of that is going down into the
new update and I'm not going to do do
update for all of the hosts some of them
don't need updating but only the ones
that need updating will get the final do
update operation and the only thing to
notice here is that the get latest
version operation the result of that
gets sent down to each of the do update
operations down here because so I need
the latest version here in order to
decide whether to do the update to so
it's pulling that apart a little bit I
now get all of these concurrent sub
graphs for each of the separate hosts
and this would work out quite nicely if
maybe one of the hosts took a little
while to respond and this get installed
version takes a bit longer than the
others
and now maybe I can do some of these do
updates earlier so you can't do that
with axel as I described it up to now
we're we're doing these individual
rounds but we've been working on a new
version of axel that we call hax all too
and this drops the requirement that all
the concurrent IO has to happen in the
single round so now the io can be
arbitrarily overlapped and it could be
overlapped with computation as well so
it just changes the the requirements so
that you don't have to finish all of the
IO before you can go on and do the next
lots of computation and this means that
we have to have a new contract with the
IO provider so before I talked about
this fetch call that has to take to all
of the i/o you can do and then returns
all the results and now we have to have
a slightly different contract so instead
of just passing all the i/o to the fetch
function we're passing a set of pairs
where the left-hand component of the
pair is the IO IO that we want to do in
the right-hand component is a variable
into which we can send the result so
it's the job of the IO provider here to
perform the IO perhaps in the background
and then later on you know fill in this
this result var with the result and this
will have the effect of unblocking
whatever computation was waiting for
that result so this gives rise to one
slightly interesting trade-off because
so before when I ran all the IO or
rather when I when I wait for all the IO
I can maximally batch all that IO
together so I can look at all the places
where I'm sending IO to the same place
and I have the opportunity to batch all
that into a single request if that's
beneficial but now I've got a Oh
happening at different times possibly
overlapped in arbitrary ways and I
probably don't want to wait for all the
IO to arrive before I send it off so I'm
going this trade-off between how much
batching I can possibly exploit against
latency because if I decide not to wait
for all the IO
then I perhaps get more light and lower
latency but I get less batching and so
hax until actually trade off this these
two opportunities when we deployed this
in production at our service at Facebook
we actually saw a latency reductions so
some of the requests that we were doing
had dependencies that allowed hexyl to
do - to do a better scheduling of the
i/o and actually reduce the latency so
the batching trade trade-off turn out to
be in our favor axle - right now is
available on github actually not quite
available on hackage I haven't released
it on package yet but it will be there
very shortly so just to recap what we
what we saw here we we had this example
where there was lots of inherent
parallelism in the code but the the code
didn't expose all of the parallelism in
order to squeeze out the last bits of
parallelism in order to be able to take
advantage of the the maximum amount of
overlapping we had to refactor the code
slightly so this is kind of reinforcing
what I said about it's not it's not
going to give you the optimal solution
so axle isn't going to exploit all of
the possible concurrency that you have
but it's going to get you most of the
way there 80 or 90 percent perhaps and
then probably you can go back and
profile or analyze your code and find
the places where you can squeeze out a
little bit more so I talked to the
beginning of the talk about testing and
debugging so let's go back and talk
about those again so in order to use
hacks or for your i/o you have to write
a little bit of boilerplate for each of
the different kinds of i/o you have to
write a data type so here's an example
of a data type that you might write for
an HTTP request so this is how you
introduce a datatype in the haskell HTTP
is the constructor this is the argument
the request and it's annotated with the
type of its result so it's an HTTP
request that returns a text result and
then when you've written this data type
you have to add some boilerplate
instances this lets
axl do it stuff and you have to
implement the fetch function which tells
axel how to go ahead and just an execute
your request and return the result so
now what we've done is made all of our
i/o into data and data is very malleable
you can do lots of things with data this
is going to give us some nice benefits
so hacks will stores all of the i/o
requests and results into an internal
cache that means that when you request
the same thing again if you do the same
io again it compares what you're trying
to do because it's data against what's
already in the cache and it will give
you back the results straight away and
that's very good obviously for
performance it's also good for
correctness if the data changed
underneath you if the database is
changing behind you then you won't
notice the changes until you run a new
request because this cache is going to
be per request only it also has some
nice benefits of modularity which might
be a little bit of a surprise so how
does this help with modularity well in
the absence of caching if you are
fetching the same thing in multiple
parts of your program what you would
probably want to do is to find those
parts of the program where you're
fetching the same data and extract that
higher up into your program it affects
the results just once and then pass
those results around to the different
parts of your program but if you have
caching you just don't have to do that
so if you have caching you can just
fetch data wherever you like
or just do i/o wherever you like and not
worry too much about doing the same
thing in multiple places because it's
going to be automatically cached so this
means these two components which would
have had to be coupled if I had no
caching can now be completely decoupled
so the cache records all of the I own
and the i/o is the only thing that's
actually non-deterministic in the
program everything else is deterministic
so given the live Castelli i/o if I go
and run the same program again I'm
guaranteed to get exactly the same
answer all right
and we're going to ensure that you do
this for all the i/o and the Haskell
type system lets us prevent you from
doing any nefarious IO in your program
so once you've done this for all your IO
we can go ahead and print out the cache
so hacks will provides this operation
called dump cache as Haskell so we can
print out the cash as a Haskell program
and now if you run this Haskell program
the effect of running this program is to
populate the cache with the same
requests and responses that we saw the
first time so perhaps you can see where
I'm going with this if we've populated
the cache with the same requests and
responses that we have the first time
and then we run the code again we're
guaranteed to get the same results which
is basically a unit test so to make a
unit test you've run the code capture
the cache but the cache in the file
commit the file to your repository
include it in your test code and you got
a unit test so that just makes it really
really easy to record and replay data to
make unit tests and this abstraction
that we have for populating the cache
which I skipped past
it's called cache request so cache
request takes the data item representing
the request it takes the result and it
inserts that item into the cache so this
is the the basis that you can also use
for writing synthetic test data because
you can write functions that create
requests and responses and set those
into the cache and now I can create any
kind of synthetic test data that I need
to run my test and it also gives me a
handle on this debugging problem that I
talked about too because now when
something goes wrong if I persist the
cache somehow I have to store the cache
I can log it perhaps and but the
programmer has to do nothing specific to
any particular kind of failure as long
as they using this framework they can
persist the cache and now when the
failure happens they have to look at the
cache corresponding to that failure and
they can rerun the code so you need to
know exactly what code was running
obviously you need to know the input and
you need to have the cache
now you can reproduce exactly what
happened
during that failure and you can see if
one of your databases returns an area
you can see what that area was and then
you can see what happened in your code
as a response to that failure so perhaps
you're wondering why we made axel well
there's a blog post you can go look at
this is a few years old now but actually
it's being used at Facebook at scale to
fight various kinds of abuse we have a
rule engine system called Sigma and
sigmaz job is to classify lots of
different actions so when you make an
action on Facebook like you post a
message or you go and click like on
something or you you know you post a
status update something like that each
of these actions has to be classified
according to whether it represents some
kind of abusive behavior and I mentioned
here spam spam is one of the main kinds
of abuse but there's lots of other kinds
of abuse as well like people posting
links to websites that have malware on
them to try and fish your credentials
all these kinds of things and so we have
a system that tries to classify all of
these actions and the system and the
rules are implemented in Haskell using
the axel framework and hacks all means
that we can have people writing these
rules without worrying about concurrency
the rules have to run really fast
because they're fetching lots of data
from different services in the internal
network but the authors don't have to
worry about making sure that the data
they're fetching is fetch concurrently
and they get lots of benefits like
caching and memorization
so the performance aspects are more or
less handled behind the scenes by the
axel framework and using Haskell means
that we can move very quickly we can
implement new rules very fast because we
have the type checker making sure that
we're not doing arbitrary i/o we're not
interfering with other rules that
already exist in the system we can add
new rules without the worry that they
might interfere and that lets us add new
rules very quickly and safely just some
quick stats our services serving over a
million requests a second we have
thousands of machines across multiple
different data centers around the world
hundreds of thousands of lines of code
of Haskell and this is changed at a very
rapid pace we have dozens of people
writing code and checking into the
codebase hundreds of changes per day and
each of these changes gets deployed
almost immediately to the production
system actually is actually open source
github you can see it and as I mentioned
the new version of hacks all that we
were working on with a new scheduler is
visible on get up at the moment so the
idea is behind hacks will have become
quite popular and so popular in fact
that there's a whole bunch of different
clones of hacks all in different
languages so if you'd like Scala or you
like pure script or closure
even if you like Haskell there's a
choice of different implementations of
hacks them so you can go ahead and look
at these mostly these implement the idea
behind the applicator instance in hacks
form not necessarily the the compiler
extension that we added the applicative
do extension so that's something that
you really need hacks all to get well
Hospital to get so just to summarize
what I talked about you have to write a
little bit bit of boilerplate for your
IO you have to make your IR into a
datatype and you have to write a
function that implements the IO for that
datatype but in return you get quite a
lot automatic concurrency caching ways
to do nice testability in debug ability
by virtue of the fact that you've just
written this bit of boilerplate around
your IO
so this might be because I'm an HTTP
nerd but how do you invalidate the cache
well the cache is per request by default
so there's actually a run axle function
which takes a hacksaw and the cache
leaves for the duration of that so in
our system where we're running quite
short lived requests and the request
throws the cache away at the end of the
request so we don't worry too much about
the cache blowing up but this is a
question I get asked often so if you
were running axel in in a situation
where you had much longer lived requests
or you wanted to have a computation that
just lasted much longer then you would
probably have to think about when do I
want to flush this cache you know what
things I want to remove from it and it
depends very much and how often you
expect the the data to change underneath
you how often do you expect the database
to change how long do you want to catch
results for and that sort of thing
so on that then where does the
responsibility lie if you wanted to do
that is that in the fetch implementation
or is it it's somewhere else that's
actually in the it's at a higher level
in in whatever in Vokes hacks all so you
have a you define your axle computation
and then you call a function called run
axle to run it so it's in the level
where you're calling run hacks well you
get to choose where to start a new cache
or not all right it's now time for lunch
so I suggest that you if you have any
more questions you grab Simon outside</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>