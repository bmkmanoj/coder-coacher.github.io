<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Build Your Own Elixir - Louis Pilfold | Coder Coacher - Coaching Coders</title><meta content="Build Your Own Elixir - Louis Pilfold - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Build Your Own Elixir - Louis Pilfold</b></h2><h5 class="post__date">2016-10-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/IONWi9hayEA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so hi I'm Luke your fault and I started
writing a literature lying a couple
years ago and through various pet
projects i went from knowing nothing
about compilers to somehow having a
little language on the beam so I learned
about tokenization passing abstract
syntax trees and code generation and
what this can look like in the context
of erlangen elixir so I'd like to share
this journey with you and also share
some of the things I've learnt along the
way so when I discovered elixir I was
professionally writing Ruby back in the
Ruby world we have sorry I developed a
soft spot for static analysis programs
one of which was a style into quadruple
comp which is a program that inspect
your code finds stylistic areas and
common mistakes and alerts you to them
so I find that handy for keeping a code
base and I seek assistance and also
argue avoiding arguments about
indentation so being a young language
Alexa didn't have an equivalent tool so
I thought I'd have a stab at making one
myself so lintas or effectively
functions that take some source code and
then return a number of errors to the
user so I didn't know exactly how they
worked so i looked at the source for
rubra cop and another javascript linter
called Jesse yes and I discovered that
when you look a little closer they look
something like this they take some
source code they convert them to one or
more different data structures and then
analyze those forms in order to find any
errors so these errors which Sharra's
patterns in the data that we can match
against so the first data structure that
we can create from source code is a list
of tokens tokens being the smallest
elemental parts of the source code with
type wow you could still hear me that's
great and the smallest element of parts
of the source can we type in basic
building blocks of code so for example
if we took the English sentence hello
world and tokenize it we might end up
with something like this we have the we
have five tokens the first one is a word
with a value of hello the second one is
a comment the third is a space the
fourth is a word
the value of world and lastly we have an
exclamation mark so we can do the same
for Alexa code so here's a snippet in
which I pipe the number one into a
function called add and a function also
takes an additional argument of two I
said that's a syntax error when never
mind when tokenizing it becomes a list
of for tokens the number token with the
value of one and arrow up token which is
the pipe operator and identify token
with the value of AD and the number two
so in elector terms this data would look
like this each token is at uppal where
the first element is the name of the
token type as an atom and the last
element is the value of the token and in
the middle there is some metadata which
I've admitted so in the Ruby and drop
Scott winters that I looked at the
tokenization process was quite large and
complicated as they had to implement the
tokenizer from scratch luckily this
isn't the case in elixir the Alexa
compile is written in our line and thus
we can call it from Alexa and the
modules are available in the standard
library so Alexa tokenization are as
simple as calling the tokenized function
from the Elector tokenizer module so
getting the tokens is easy how might we
use them in a linter so one simple thing
alinta for elixir might want to do is
forbid the use of semicolon to separate
expressions each expression should
instead be separated by new line which
is a much more common an idiomatic so to
make the Lin to detect violations of
this rule and I first to find a function
called semicolons that returns true if
passed a semicolon token and returns
false if passed anything else and it
attacks the semicolon token by pattern
matching on the first element of the
token tuple which in this case is the
semicolon character has an atom so now
with this function I can use a num any
to iterate over the list of given tokens
and return an error if any semicolon
tokens are found so tokens are simple
and easy and be quite limited in order
to do more meaningful analysis you need
another form to represent the source
code then
form is an abstract syntax tree so while
tokens were the linear sequence of the
elemental components of source code and
abstract syntax tree is the
representation of the syntactic
structure of source code so here's an
example on the first line is some code
in which the function at is called to
the arguments one or two and below this
is the tree this expression would form
so the root node is a call to the
function add and the call has to leaf
node children the first is the number
one and the second is the number two and
here's a slightly more complex example
and at the root of the tree there's a
call to the sin function which has two
arguments and thus two children the
first the first is a cool to the zero
arity function itself and the second is
a tuple so the two has three elements
and thus the notice through children
they are the Atem compare a function
call and the number one and the function
call is to the plus operator and it has
two children each of them the number two
so normally one would get the tree by
first tokenizing the coat and then
passing the tokens in order to construct
the tree but an elixir there's an easy
method too thanks to the macro system so
when the quote special form is called
and an expression it returns the elixir
abstract syntax tree for you to inspect
or manipulates so Alexis ESC is a quite
concise and simple everything that
everything that's not a literal in the
st is a three-item triple where the
first element is the name of the
function or the constructor and the
second item is a metadata and the third
is the children in a list so here we
have the add function again only this
time the quote special form is being
used to get the get the tree and at the
root is a function course so it's a
three-item super the first element is
the atom and as that's the name of the
function and the third element is the
notes children which are the numbers one
or two in a list so now I've learnt a
little battle ast and how to obtain it I
can use it in the linter so say I want
to forbid the use of the unless macro
with an else Clause as I think it'll be
better written as an if in order to
prevent the double negative so with an
ASC I can enforce this by walking across
the tree it's like
come to the offending pattern and the
inferred the offending pattern in this
case is a node with the Atem unless in
the first position making it cool to the
unless macro and then an else block in
the second child of that node so
traversing the tree is actually quite
easy thanks to the macro pre-walk
function provided to us which takes an
ast and accumulator and a cool back that
receive each node in turn so I have this
check unless come back with two clauses
the first one pattern matches against
the offending nodes and returns the Atem
error in place the accumulator and the
other is just a catch-all of all other
nodes all of the patterns except for the
offending one are valid so we just
returned the accumulator and so with
that I have the beginnings of a working
linter on my versa learnt about
tokenization and abstract syntax trees
and a little about passing in general so
through the macro system and its
associated helper functions elixir has
made this task much easier and that
maybe it would have been a fair trading
JavaScript or Ruby or elsewhere all
those left is to do some plumbing to
present the errors to the user and then
maybe write a few more rules and after
writing a few more rules and then
experimented macros myself i soldered
feel pretty comfortable working with
syntax trees and give me the first part
of that compiler puzzle so sometime
later I found myself writing a simple
web app in elixir nothing exciting it
just rendered some hates your pages to
the user and they gave them some forms
which they can use to record data into a
database so while while I was writing
the HTML views I found myself getting a
little fed up of the default templating
language eax it's fast and easy to use
but I still have to write HTML which I
don't think is the most fun thing to do
so it has lots of superfluous angle
brackets and the rabo syntax for closing
tags and I have to manually escape
certain characters and I just much
rather not do this and I want to type
something much more concise and when
working with Ruby or Java scripts or
Haskell I can do this so there's a
templating system for Ruby equalled slim
and another for java called pug which
allow me to write HTML like this all the
superfluous syntax is gone and the
delimiter is happy replace of
indentation
granted it's not everyone's cup of tea
and but I've become rather used to it
and as a result I found myself wanting
something similar in elixir I'm with my
newfound knowledge of tokenization and
passing I decided to have a go at trying
to make the same thing for Alexa so the
templating library is effectively a
function that takes a template of
alternative HTR syntax and returns a
function the given data produces a
string of HTML in order to know what HT
mozza generates I need to yet again do
some analysis on some source code so
like with the Linzer I need to generate
an abstract syntax tree which I can then
expect and do things with so unlike the
linter I don't have a prebuilt function
for getting tokens from my HTML
templates so I have to build my own
tokenizer after a little digging I
discovered that the Erlang standard
library contains a module called Lex
which offers a DSL for creating to
organizers and this is also the same
module that the LFE language users users
for tokenization which is a pretty good
endorsement in my books I should also
say it's probably a little over the top
to do this here as I could probably
easily pass a string using pattern
matching but this seemed like a good
excuse to something new and fun and
potentially useful so one line one line
is one element in my syntax so I'm just
going to split all new lines and trim
the annotation leave me with just the
element that I'm interested in passing
so looking at these elements I can see a
few token types there is one for names
which are element names class names ID
names so on and there are 14 dots and
hashes which I use to denote classes and
ID's respectively the syntax for what
tributes includes open franz clothes
brands and then a little equal sign to
get between attribute name of the value
and there are strings which are series
of characters surrounded by double
quotes so lastly there's white space
tokens and word tokens and were tokens
are any non white space character and
that NE devas don't match so now i can
teach legs what my tokens are and start
building the tokenizer so elects module
is a file that contains some airline
like code and has the file extension
exile
within it within it has three sections
definitions in which the author uses
regular expressions to define a pattern
for each type of token and the rules
section in which the author declares a
data structure that is to be formed for
each token and then lastly the airline
code section which contains helper
functions which you might use in the
other two sections so here's my next
definition section containing all of my
various token patterns and names are
capitalized and go in the left hand side
of the match operator and patterns go on
the right so it has a simple lateral
literal patterns for dots hash EQ open p
and close p and after that we have more
complex patterns such as string name
whitespace of word so regular
expressions can be a little hard to read
so I'm just going to quickly go over
them here so the string pattern is a
pair of double quotes with zero or more
characters between them where the
characters are any number any non double
quick character or any character
preceded by an escape in / so this is
how with red x's and we can get support
for escaped escaped quotes inside shrink
bodies and the name is any letter
followed by any mix of letters numbers
numbers underscores or dashes and white
space is just Sint spaces or tabs and
then a word is anything else hmm so
after the definition section comes the
rules section in wit which is a mapping
between patterns and a token data
structure so the syntax for a rule is
the name of the definition the curly
braces on the left and then an
extraction to pour on the right and a
colon in the middle and because this is
earl eying everything ends with a full
stop so the first element in the tuple
is the Adam token which is a structured
to output a token for when this pattern
matches the second second item is a data
structure but we want to form here I'm
always forming to item two pools with a
token name as an atom in the first
position and then the match characters
in the second position which I access
through the magic variable token Charles
and so some of these definitions overlap
and will match the same text for example
name and word so for in order for the
tok
that always produce the same results the
order in which the patterns are used to
test matters so if leaks rules at exited
from top to bottom and the first one
that matches is used much like a case
expression so if i place this file in
the source directory of an elected
project mix will compile this into an
erlang module for us which exposes a
string function and this function takes
HR list of code and returns a list of
tokens and because i used to item to
pulse with an atom as the first elements
and i get back in alexa keyword list so
here I am I'm tokenizing a line of code
and I get back a name token with a value
of div a white space token a name token
with valid hello another white space
token and inane checking with a value of
world cool so at first they seem like
enough but when tokenizing another line
I discovered a problem so when I receive
a string token the value of the string
is as written in the source code when I
actually want the value of the straight
see it has the quotes around it and so
to resolve this i can make use of the
final section of the league's module so
here I've I've dated and the token
topple two for the string to call a
function called string on the token
Charles before inserting it into the
tuple so the definition this helper
function goes in the ER line code
section and it simply gets rid of the
quotes by dropping the first and last
characters using the drop last and tail
functions so now i get the value i want
for string tokens and later i may also
want to add additional helper functions
for passing numbers escaping handling
escaped characters and so on and with
the tokenizer done i can move on to
building and AST from the tokens and in
the same way that our line supplier too
tall for tokenization it also supplies a
tool for passing which is the yak module
so like Lex it's used by writing a
module of a specific syntax and file
extension which is then good party to an
online module and this module contains a
grammar which is a set of rules that
describe the syntax of a language so the
final consists of five main sections
which are non terminals terminals the
root symbol and a set of grammar rules
and another airline code section which
is used for helper functions so
terminals are the most basic symbols
that are recognized by the grammar they
cannot be broken down into smaller parts
in this case they are the token types
which my tokenizer can create so non
terminals and our higher level symbols
which are formed by composing together
terminals of a non terminals or a mix of
both and the root symbol is the highest
level non terminal in the tree that
composes all the other ones and lastly
grammar rules are definitions about what
symbols compose other symbols and in
what context they do this so let's look
at some of my non terminals an example
non-terminal in my grammar would be a
class lecture so in the first line first
code example there is a h1 element with
the class of jumbo token eyeses into
three terminals which are the name h1a
dots and the name jumbo so in this
context the dots terminal followed by
the name terminal can be composed
together to form a class non-terminal so
a class is a dot and then a name and
likewise an ID is a hash and then a name
so elements can have many last letters
on them so we need a classes non
terminal and classes are a single class
or many phalluses and a repeating symbol
like this is defined recursively so here
classes can be a class or alternatively
they can be a class followed by classes
so names is the head of an element in my
shorthand syntax so it can be a name or
a name then ID and then the name of
class is or name them ideal in classes
or just classes or just an ID or an
ideal in classes and and the these
declarative rules continue I've got to
define a rule for attributes many
attributes and pieces of text content
which is formed from composing many
pieces of text on white space together
and so on until I've covered every
symbol all the way up to the root symbol
and here my root symbol is an element so
an element is a set of names or its
names followed by tributes or names
attributes and their content or just
names linked content
and so now that I have and definitions
for all the different symbols from the
lowest term laws to the root symbol non
terminal and yet has enough information
to be able to pass a single elements
from a list of tokens and the only thing
that's left to do for it to be able to
actually build a tree that we can use is
to instruct it how to build a data
structure for each node for each
non-terminal so in the mini st i want
the class token to be represented by
string with the same value as the token
so to achieve this I need to be able to
refer to the triple which makes up the
name token so that I can extract the
string value from it so helpfully yet
assigns pseudo revels in the form of
atoms for each symbol used in a
non-terminal definition so if a class is
a dot than a name Adam dollar one refers
to the doctor gun and Adam dollar 2
refers to the name token the string I
want is the second element in the tuple
so I call the element function on dollar
to in order to get it and the code is
placed after conan after all and but
before full stop for each definition so
now that I've defined a data structure
for class I can do the same thing for ID
and it also just pulls the string out of
the name token and some of my some of
the nodes in my ASC will be represented
with a list so one such example is the
classes symbol which is one or more
class symbol so for the base case of
just one class I can just wrap it in a
list and for the case of a class
followed by classes I could just
prepared the class to the value of
classes and this will unfold recursively
until we have just one class which is
the base case at the top so other
symbols are more complex than just a
simple value or a list and the name
civil consists of a combination of an
element type such as a div or body an ID
and then one or more classes so we could
we could represent this with a throw
some tuple but then it's really hard to
remember which field is which were two
balls so instead I've opted to use an
airline record like Alexis trucks each
field in a
record definition gets a name and a
default value and here I have the names
record and it has a default value for
the type field which is diff the default
value for the ID field is the atom nil
and then the default value for the
classes field is an empty list so with
this record and I can use a nice syntax
for setting and getting named values on
a complex node so now that I can build
simple values or complex values and
collections of values I can work my way
all the way up to the root symbol which
is the element so in element here is a
record we have a type and ID some
classes attributes and lastly content so
for each definition I can use the dollar
variables to set to the various fields
from the child nodes and and then with
that all the definitions are in place I
can place this file in the source
directory of an elixir project and their
mix will compile it to an online module
for me so I can now turn source code
into tokens and tokens into an AST the
next step is turning an AST into a
function which produces HTML so it
wasn't entirely sure I have to do this
so I'm I had a look at the source code
for IX on get up and I discovered that
the the code leverages some of elixirs
meta programming features in a way which
seemed pretty clever but also quite easy
to imitate so yet again like with
tokenization Alexis done some of the
work for me so here's an ax templates
and the function that the es compiler
would construct from that templates the
function simply concatenates each part
of the template together so let's look
at how this is done so the exs parser
splits the template into text and
paragraphs the template in the first
code snippet would be split into the
text hello and then an expression
consisting of the variable name so this
list is then turned into an expression
which can be used the body of a function
so here's a compile function which
construct some expressions it first
reduces that reduces over that list with
the cogent function and uses an empty
string as the starting value
and the co gian function has two clauses
and the first one is for text elements
which it can catenate onto the buffer
and doing so inside a quote block like
this results in the the tree being
returned rather than the expression
being evaluated so the other clauses for
expressions and it works exactly the
same way as the previous course except
equals code string to quoted on the
value in order to turn it from a string
of Alexa code into an expression which
is part of the AST so here the compile
function is called on the list from
before and it's a little hard to read
these expressions so at the bottom I'm
converting a package of string with the
macro to string function and then on top
there is the function from before on the
bottom is the string that it was just
returned and they're pretty much
identical so how would this work for the
HTML templating librarian so the parser
is used to generate an elixir data
structure we've all the required
information for the templates and from
the data structure a list of fragments
of HTML and text and expressions are
formed and this list is put through the
same kapaun function which results in an
elixir EST being built with the string
values injected into it and then that's
all that's left is to turn it into a
function so if i want the ASC i've
generated to be the body of a function
how do i do that well it turns out that
def at seen before was not a keyword
when instead of macro and as a macro I
can use the unquote a special form to
inject an ast back into the code as an
expression a compile-time so to build
the function m a template is tokenized
past compiled and then unquoted into the
block of a function so now I have the
beginnings of a HTML templating likely
that i can use in a real app except it's
it's still missing a few things I
probably want to add a looping construct
so I can iterate over collections and I
probably want conditional expressions so
I can have more dynamic more dynamic
templates and I would add these by
adapting my tokenizer parser to output a
new type of node for each construct and
then in the code in function I can
define a new clause which construction
you ast that gives the behavior that I
desire
so it's about this point I sort of
realized that we've relatively if little
effort I had created a program that
takes some source code passes it and
then generate some code which is
executed by the virtual machine the
outputs of my program is a set of
functions without really knowing
anything about compilers I've sort of if
you squint written a micro language for
the beam and getting this far was easy
thanks to the excellent tools that
supplied by the Erlang ecosystem so I
thought presumably it wouldn't be that
much harder to create an entirely new
language using the same tools and as a
language nerd this really excited me so
I jumped right in an abandoned give a
project I will finish it so here's a
language it's good gleam and I believe
that in order for a language to be
worthwhile ease have a really clear idea
of the problem that is trying to solve
and the problem that gleam is trying to
solve is there's just not enough curly
braces on the beam after all how are we
going to convince the JavaScript
community to join us so the first step
in solving this problem is to make a
tokenizer so I've got a few definitions
I've got to float and an inter for
numbers I've got a string and identify
an atom and at'em of quotes and lastly
white space and the first pass of rules
are the key words de limiters
punctuation and tokens which are all
very straightforward so after that comes
the more complex tokens that use
patterns from the definition section so
the value of each of these tokens comes
from calling a helper function on match
characters so the Interphone shin
converts the value to an integer the
float function conversed with floats and
for an identifier and an atom I convert
to an atom and for strings and I get the
contents by dropping the quotes as
before so these functions are all
defined in the airline code section also
notes how the the end pattern and the
float pattern are both being used to
construct the same token so Lex allows
multiple rules can start the same token
so we can have some variations in syntax
like this so lastly there's the rule for
white space
instead of constructing a token i use
the skip token adam in order to signify
that text matching this pattern is to be
discarded whitespace has no meaning in
gleam so it can be safely ignored and
that's the basic tokenizer done later I
probably want to extend it with
mathematical operators and the pipe
operator and move some syntax on maps
and but for now I can just move on to
the Yak passer so here is a Glee module
called stack it's made up of multiple
statements and the module declaration at
the top and each of the function
definitions our module statements so the
root symbol of the grammar is a single
module and a module is defined as a
series of statements and statements are
defined as one or more statement and a
statement to see from a module
declaration or a function so module
declarations are simple they always have
the module keyword followed by an
identifier and both of these are tokens
making them terminal symbols and leaf
nodes in the AST so to make working
we've nodes a little easier I've used
Erlang records for each one the module
declaration has a name field in which I
place the value of the identifier moving
on to the other type of statements a
function is even the public or private
keyword followed by an identifier and
then a function block and the resulting
function record contains the function
publicity and the name and the clauses
which are taken from the function block
and a function block is a pair of curly
braces around one or more function
clauses and later I would imagine
there'll be other function block
contents for example docstrings or maybe
unit tests and which is why I've use a
record for this block as using a record
makes things very easy to extend as I
can just add another field and next to
function loss it's the DEF keyword
followed by M and arguments to pool and
a clause block and again I'm
constructing a record this one has
fields for aritee and arguments and the
body and this continues for every part
of symbol in toilet I've defined a rule
for every node in a syntax tree so
there's all the nodes and the currently
make up the name AST each one is record
this makes it easy to extract the values
from them but in addition to that it
also before provides a handy method for
pattern matching on each node which will
be quite useful later so the next part
of the simple compiler is to convert the
gleam abstract syntax tree into a format
that can be fed into the virtual machine
so the templating language and this
format was elixir AST which again would
work here and however there are also
other options since we've come all this
way just using the OTP standard library
and let's explore one of the
alternatives which is caught a line so
quarterly is an intermediary language
used by the Erlang compiler so when
regular airline code is compiled it is
converted attic or I'll encode before
being optimized and converted is the
bike co to the virtual machine actually
runs so if I target caught Erlang rather
than trying to generate by code myself
and gleam will hopefully benefit from
all the same compiler optimizations
Erlang and hopefully be as fast and so
quarter lang has a texture
representation which is quite pretty
which you can see here and these code
snippets are equivalent the first one
it's Erlang and the second one is cooler
like and which as you can see is much
more verbose and explicit so you have to
you have to write guards and function
application requires and apply keyword
and so on but in addition to the textual
form it also has an abstract syntax tree
consisting of regular airline data
structures primarily records which is
great because if it's just regular line
data I can just construct it construct
it manually well not quite because the
official documentation states that core
Erlang ast is subject to change at any
time without notice it may be that when
the next OTP arrives and the quarter
Langtry looks completely different and I
have to rewrite this section of the
compiler from scratch so if the asd
format is not spit if the SD format is
not formally specified how can it be
generated so the answer is use the CL
module which exposes functions for the
composing and do
posing of this AST so the decomposition
functions could possibly be useful for
reflection in a similar fashion to how
the alexa lintel worked but for the job
at hand i'm interested in the functions
used for composing the tree so here we
see MVC Adam constructor be used it's a
function which takes an atom and
returned the core Erlang node the
represent an atom what exactly this
looks like doesn't matter it may change
in a later version of OTP the only thing
that matters is that this funk that we
trust that this function will always
return the correct no data structure
whatever that may be so there are
similar functions for all the other
possible types of nodes with these I
convert a gleam a SC into core Erlang
AST by traversing the tree and calling
the appropriate constructor for that
note this is where the gleam passes use
of records comes in handy if i can
define a function which takes a gleam
note and then returns a quarterly node
by defining a function for each record
i'll be able to cover my entire tree so
for example here's the clause for the
string record and thus the string note
it just calls the ceasing see strain
constructor on the value from the string
record next we've got clauses for
numbers so the gleam ast doesn't match
up perfectly with the coral angwin so i
have to use a guard it to differentiate
between int's and flits and after that
is just a matter of calling the correct
constructor for each type so all the
nodes we've seen so far have been leaf
nodes and they've been very simple to
convert to court Erlang while branch
nodes are a little more complex so
branch nodes other nodes that have
children and they consist of multiple
nodes joined into one so for example a
list or a tuple will have elements and a
function will have a name and also
function clauses and converting these
nodes is more complex as not only do
they themselves need to be converted but
also their children need to be converted
as well so here's a tip or node and it's
children are the elements of the tuple
this one contains the atom okay and the
string hello so the children would be
the atom node and string node
in the gleam tree this this to pool is a
record with an element's property which
is a list containing the atom and the
string and the clause that handles
converting converting tuples first has
to map the co gian function over all of
the children in order to convert them
into our lang and then once they're
converted the sea tuple function is
called on the new elements in order to
create a quarter like tuple so am i gon
back here the children living yes so
here the the children were leaf notes
they were string and an atom but what if
the children were also branch notes what
if one of the children was a tuple and
that to book contained a list and so on
so each clause that handles a branch
node has to cool the cogent function and
each of its children and because of this
the cogent function when cooled on the
root node of the tree and that nodes
children we transformed and that nice
children that most children we
transformed recursively into all the
nodes children and their children so on
and the entire tree is converted into
quarreling so jumping from the bottom of
the tree to the top here is the record
module it is considerably more complex
in the previous loads the CR constructor
takes three arguments the module name a
list of functions to exports and a list
of module functions themselves so and of
course each one of these needs to be
given in the core airline form so the
name is easy it's just an atom so we
call the see Adam constructor on the
name for forming the list of exports
I've created a gin export function which
filters the list or four functions that
are public and the construct a quarter
line exports for each one and lastly
functions are transformed into quarter
line by mapping in a cogent function
over them which in turn recursively
transforms them and their children and
the rest of the trick
so I found that with more complex nodes
it's not immediately obvious how to use
the constructor function and if you're
going to play with cl I really recommend
downloading a copy of the OTP source
code and reading the documentation
inside the module itself and also
playing close attention to the type
specifications which can be very
informative when I still had problems I
found a good trick whilst you turn the
turn to the source code of another
language that uses this module and see
how they use it good examples of
languages that use it would be l fe m l
fe and jock star which were quite good
to study so right with a co join
function clause defined for each node in
the AST I have all the basic parts of a
beam language compiler the only thing
that's left is to stick them all
together so here i have a gleam compiler
module which defines a load file
function and the function takes a a path
to a gleam piece of gleam source code
and reads the file and then converts
into a nanite string the string is then
fed into the tokenizer which breaks the
source code down into a list of its
atomic parts such as strings numbers and
punctuation and the tokens and then fed
into a parser which constructs an
abstract syntax tree and this can this
tree contains all the syntactic
information about the code so lists
contain elements and functions have
clauses and so on the tree is then fed
into a cogent function which converts it
into a format which can be loaded by the
beam here that format is caught erlang
which is the intermediary language which
the Erlang car parts to and then the
court i er Lang is compiled into beam
bytecode using our landscape Isle forms
function and the loaded into the virtual
machine using the code load binary
function so here's a here's a Glee
module and if i call the load file
functions the file will be read
tokenized past transformed into co I
Lang and then loaded onto the beam and
then the clean function called just like
any other function and with that there's
a 15th language running on the bin
so I was ecstatic when I first compiled
my first module and it just said hello
world and it was such a small thing but
it was such a fun journey and an
unexpectedly easy and smooth on to so
with Erlang an elixir we are supplied of
a large range of excellent tools that
can be used individually or all together
so Lex and yet give us an easy out of
the box way to do tokenization and
passing of code and with Alexa macros
and quarterly we have a friendly way to
generate code that can be loaded into
the virtual machine coupled with Alexa
an airline excellent pattern matching
and data handling these tasks become
quite straightforward so I think it
would be really exciting to see more
projects that make use of what the
ecosystem gives us here I'd love to see
more static analysis tools I'd like to
see a code formatter in the foot and the
form or in the style of go format or elm
format I think it ought to be really
interesting to see a language with a
powerful ml style type system on the
beam and some work has been done here
with the MLF II project and be really
great to see that mature and develop
into something interesting and I've
really enjoyed working in this space and
if you think you could also find it fun
I encourage you to try and build your
own Alexa and this plane to your space
for more exciting projects on the bin
thank
hello yeah I just want to make a comment
yes implementing languages is fun right
it's definitely and it can be very
useful just that I agree yes let's take
a hand up if you want to ask a question
I'm coming excellent talk super dense
thanks that's very cool did you test
drive that while you were building the
language that's one of things I find
it's really neat about elixirs that you
can look at the test driving of the
language development as in did i do TDD
you a nice booting the language right on
yes yeah every single bit of it I find
it's much faster to test drive anything
because if I have a piece of code that
checks if my code works then I don't
have to do it myself and i'm like
ecstatically lazy and so yeah do it i'm
also written a talk would mix test watch
which will automatically rerun your ex
unit tests any time you save the file so
that helps a lot of why i find any other
questions does anyone else written there
other their own language other than
Robert you can't answer that you have do
you have a Joe inside and so I have a
small attribution in game Louie to build
his own language cuz I beat him to it
thanks but I didn't use the beam i
wussed out and tried to use I can't even
remember the name of it now it's what
they use for c and but yeah it is it's a
tremendous amount of fun it's actually a
lot harder than lumex out and a lot of
it is trying to come up with a grammar
for your language that is nice to play
with
and accurate something a computer can
actually pass correctly so a lot goes
into the language design which is why
copying other languages is the easier
way of doing it yeah and that's why I
sort of implied my language is
completely useless and I focused only on
the compiler because that was relatively
straightforward i found that Lionel
later gave us so much there and the real
hard bit is designing a language which
is anyway useful interesting which I've
got huge amounts of respect for everyone
who's worked on like a lying and l fe
and Alexa because they're all such rich
languages yeah I'm just curious how long
did it take to you to go through this
like journey this process of like
researching and getting something done
and so I I started writing the linter
almost immediately when I started
learning Alexa and it was awful to start
with and I just found that through
various little projects I just
accidentally picked up little bits of
knowledge about compilers so there was
never a point when I went I'm gonna
write a language now and I got there it
was as I said like I I got the the HTML
template in library to work and I went
oh wait I've just generated a function
that's really strange ah maybe I could
like do that little bit extra and
generates modules and functions then I
have a language I but then once I
started looking at quarter language
stuff maybe it was I don't know maybe a
month of evenings mostly because i
didn't really know where i supposed to
look to find the information about how
the how this stuff works so hopefully
with this talk and maybe some other
resources we can make it a bit easier to
get into making languages because it's
really fun thanks
so I've written a few languages but I
mainly get involved in writing dsl's do
you actually have seen many people
writing domain-specific languages
basically you don't have to write a very
rich language you can write something
that's quite very simple I my idea with
it dsl's is a fact I want just how right
the simplest way I want to express what
i want to express then they'll just
write a DSL for it so then I've got a
language then that does that thing so
just wonder if you come across many
other people MDS Niles so dear cells are
very very popular in the Ruby walls
you're going to use that all their like
just completely insane runtime
metaprogramming in order to like build
quite powerful dsl's and i think because
of some of the Ruby influence and the
fact we've got this really powerful
macro system there definitely is a bunch
of dsl's use an elixir which are cool
and fun but I think I'm often you can
represent this stuff in data quite well
I find and times in which i think a DSL
might be really useful so I'm really
interested in like the use of code for
performance I really want to see people
trying to like make musical performance
is an art with code and when I recently
saw a chap write a parser for like a
very succinct music notation using elm
so he could like type is cool little dsl
when it make a song life I thought
that's really exciting use of it but in
sort of more big production any things I
won't ask myself can I just write some
data I get away with just using data
here yeah so the dsl represents date
often yeah so the second question I had
was I the pauses some deposit i right
I'll pausing gigabyte files so did you
come across any any thing for doing like
stream-based parsing so basically you
don't have to necessarily get everything
into memory and then go from em remember
and I don't know if they're any tools
for that and exists already in the
Erlang world and everything that I've
passed its been things I've written
myself and i'm very lazy so they've
always been like five lines tops right I
don't write this stuff it just comes to
me
cool thanks very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>