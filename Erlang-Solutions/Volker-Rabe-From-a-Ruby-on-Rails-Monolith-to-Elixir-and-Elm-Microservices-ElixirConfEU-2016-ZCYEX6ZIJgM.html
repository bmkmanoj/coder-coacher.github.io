<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Volker Rabe - From a Ruby on Rails Monolith to Elixir and Elm Microservices (ElixirConfEU 2016) | Coder Coacher - Coaching Coders</title><meta content="Volker Rabe - From a Ruby on Rails Monolith to Elixir and Elm Microservices (ElixirConfEU 2016) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Volker Rabe - From a Ruby on Rails Monolith to Elixir and Elm Microservices (ElixirConfEU 2016)</b></h2><h5 class="post__date">2016-05-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ZCYEX6ZIJgM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">cheese and so I would assume that at
least if you've got you guys have
experience with Ruby on Rails and
monoliths or something similar
okay raise your hands
oh yeah okay so you probably all face
the moment where you couldn't scale
anymore in terms of you know not in
terms of power but in terms of features
and complexity of your code so we when
as we joined when we adjourned out minor
to me there was Ruby on Rails up and was
kind of working prototype which was sold
to the first customers later on after
some improvements but it was pretty
clear from the beginning we need to
improve the system in order to scale the
business and I want to take you there to
this journey which is much more I'm
afraid an architectural and design talk
less a coding talk but at least I want
to explain in the end so what why did we
choose elixir and why Elm and how does
it fit together with micro servers and a
bit of infrastructure automation so
Miami to me provides a software service
which is called reportin and it's
tracking adverse events so if you have
you know headache and you take aspirin
and you get really bad stomach then you
can report this adverse event in an
online questionnaire so this is probably
one of the first times that epic events
can be reported in a systematical way
with small start-up in Soho good anymore
Soho actually hope
but and we have about 28 staff members
like seven developers we started with
two customers of Pfizer and Pfizer and
elegant and it worked quite well I mean
we could provide all the features they
wanted we customized it but at a certain
point we were running into troubles and
so this is that we're running on nothing
really you know awkward and limiting
factor or the restriction we have we
have to provide GX p compliance because
when you work with pharmaceutical you
always need to you know host your
environment in controlled environments
and have kind of documentation of the
changes you did so the mission we were
starting almost a year ago was scared
the business without scaling linear the
team and that's kind of problem for us
because you know like I said the
application works well but we have some
fundamental challenges with business and
functional requirements that makes it
hard for us to scale in terms of number
of clients number of features number of
products but we get there so what's
actually wrong with the monolith we all
started perhaps with the monolith and
actually it works quite well right it's
easy to install you can you know hosted
somewhere you understand you know the
whole process flow and communication
flow
and you can debug it easily well at a
certain point if you add more and more
functionality that leads to an entropy
of the system and well in terms of
climate change you may understand this
better at a certain level of new
quantity we getting into a new level of
quality so if like for example the
average temperature just you know rises
by 1% we might not have any poles and 1%
doesn't or 1 degree doesn't sound that
much but it's just the tipping point and
at that stage it becomes critical
mission critical at least for human
mankind so and we have the same kind of
in software because entropy leads to
complexity in the code and then it
becomes harder to and harder to maintain
the functionality and extended with new
functionality so the case in was my med
Samir's
we have a grown a platform so we have
ahead of us a couple of number new
clients and new products coming up and
they're all kind of diverging so they
won't have the same functionality they
are highly customized customized and
they have probably different versions
they run on two and didn't mention that
although we have a software as a service
the problem is it's hosted per client so
that means that we have for each client
order environments up and running and
they probably run all in different
versions with two customers two versions
no problem if we get to a bigger number
we'll get harder and
so in our case the new quantity our
means instead of one product and two
customers
two versions we might end up by 2020
this is a bit hope we have up to four
products 20 customers and my maybe 20
versions maybe only 10 but I'm trying to
accelerate the example of that here just
to make the point here so if you have up
to four different products multiplied
with 20 customers you end up probably up
to 80 different versions then given that
we have like at least four environments
perk line that runs a number on host
which is hardly to take just infer step
because 320 we're doing half managing
like I don't know 20 environments that's
a different scale that's definitely
different quality the result of this is
the platform complexity grows
exponential kind of purple version and
customer and that means in the end we
have decreasing productivity
extensibility and maintainability of the
platform and the outcome is an importing
kind of time to market and stability and
exploding costs so how should we deal
with this
well there in general may be to quite
extreme options one is the turtle tactic
did you know actually the turtles lay
their eggs like at
thirty meters away from the sea and they
do it every time so it's kind of genetic
thing if there's flood whatever they
still do it they can't change their
tactic they just use the behavior they
got inherited with their genes there's a
second option I call it reading tactic
they actually quite clever so they find
always kind of tools to get along with
the situation and find a solution for it
well I would say let's do the second one
and if we do so so if we want to be
smart about how we deal with this new
quantity and quality we actually have to
find some solutions for some of the
business problem ever some of the
problems which are generated by the
business oneness and like I've said we
have Taylor writes and products per
customer so for one and two again that's
fine but then we have to you think about
all these questionnaires and we have
they have to map a data structure if
they diverge and we have like 20 of them
that's going to be a problem at the
moment is really pity to say but the
consultants who set up the platform in
first place they use I don't know if you
guys know sell it's kind of a gem and
Ruby on Rails for component usage it's
getting really out of control if you
leave your logic inside these little
components so we have now reusable UI
component
which are spread all over the
questionnaire and actually we have like
80 of them and the logic how the whole
questionnaire are set up are distributed
in this UI components so that doesn't
scale very well we thought about we can
come up with a different solution and
put all this complexity into a graph
database so that we can fully generate
the UI and test it automatically and
yeah have some validation checks as well
the next problem we have you know is
fast beta because every user fills out
the form quite differently and depending
of what he has answered in one question
he might get different triggers for
follow-up questions so the data model is
really not really stable and robust and
at the moment it looks like this it's a
JSON blob in Postgres database and this
is really hard to deal with so we
thought given that we use or ent B which
has not only the option to structure
graph data but also document data we can
actually store the data in a way that's
connected in this case we just have a
central report with different report
sections they all link together and
because these kind of joins are very
cheap and fast in Orion to be we can
query them in nanoseconds so that
provides a is also the possibility to
run analytics on them real-time
analytics at the moment you know try to
query JSON blobs over a different
database entries that's quite hard
so another problem we have is since all
the logic is kind of for the good part
of the logic is inside the UI components
exchanging the UI by a different UI or
providing well a kind of API for a
mobile is really hard for us at the
moment and this is also a problem that
the back-end and front-end are tightly
coupled to oh you can't exchange one of
them without you know breaking things
apart on the other side the idea we had
was to say okay we can use WebSockets
decouple entirely front-end and back-end
and then we can say you define a kind of
API and they can talk to each other just
using this API and then it doesn't
matter if it's mobile device or browser
we can render or we can generate
different your eyes just with the data
we get from the backend so the products
we have they're quite similar but then
they vary in certain bits like you know
we have general adverse events kind of
product but then one which is specific
for clinical trials and they have very
advanced kind of requirements so the
data model can change within one study
for example so we don't actually want to
link them together but if we sent up a
new product we can't just say we want to
use authorization the questionnaire part
and we you
in the new product this is something we
wanted to provide in the new platform
and the type coupling between the
modules make it all so hard to reflect
our bits and pieces so if we change for
example just a data model a bit that
will cause us a lot of problems the
alternative we just found and this comes
with a kind of throw bed but is
micro-service because it leads to a very
self-contained
service approach where you can say each
service has its own you know database
model its own database schema if you
need to scale you can provide a separate
database server as well and you can
deploy them individually of course there
are dependencies between the servers but
in the end if you don't have breaking
changes you can deploy your update in
your servers without touching the rest
of the code and that's actually a big
deal for us because we have not only
user acceptance tests from our clients
before we release something it needs to
be tested and documented internally as
well so we have formal testing and
manual testing and that has to either
provide all these documents which kind
of use cases we tested and that becomes
really unbilled ii if you have to do the
full regression test with every single
change another one is regarding this at
the moment
we have a strict separation between ops
and depths but if you want to bring in
more customers and set up more new
components that gets harder as well
because we use an approach which is you
know infrastructure as code you always
deploy updates on a server and change
just the configuration on top of it
sometimes you don't really know what is
the latest change on one instance so
dealing with a lot of customers and
different versions becomes kind of a
risk since we use or want to use micro
service the container approach provides
an alternative where we can have more
infrastructure as state and I can say we
have an image and we created a container
and at once on integration we know it
will work on staging as well and we can
deploy it without risk so if you imagine
we had all these different customers and
all these different versions and
products even for deployment the
handling on the infrastructure that
becomes a difficult job and I think
something like autumn ization and also a
very strict process can help to get a
provide a better throughput at the
moment and we do a lot of stuff like you
know on demand and we have to trigger
some jobs and provide that one demo
service running right at time for our
sales guys what we want to do is
and kind of orchestration of all the
containers so you don't have to worry
manually in which kind of configuration
setup this certain environment runs we
can just you know promote the changes
with it on integration because we tested
them as developers to staging and then
our B is and autoeroticism can check the
latest changes and the good part about
this and so each little boxes like Becca
and content and these can be deployed
individually so we have less risk of
change so how does it all work together
we had some good ideas maybe but how
does it all work together we had a
vision and this vision like you know all
visions are very long-term and it's not
like that we're any near there yet but
the vision might can help us to you know
move towards this goal so internally
technically speaking we call it
preceptories
and it's all about you know data
transformation propagation
parallelization in real time so we want
to know if a user enters his efforts
event who has similar things reported
and can maybe see if there's any sign of
a pending me in North America for
example yeah so preceptories comes from
perceptron artificial neural network and
you want to actually kind of qualify the
data you get as an
put and then make decisions on it after
you you have better qualified data
actually yeah and percept ryx is like
many of these perceptions connected in
the metrics but like I said this is just
a big idea but I have helped us to
design the kind of architecture the
architecture looks like this and it's
probably not very well common for elixir
or Erlang kind of setup so we say don't
worry I will get into each part we said
we want to split up everything
individually so that we can deploy it
individually and have one central broker
which takes the concern the technical
concerns of the WebSocket handling and
this looks like this so a prong we have
to hope the lender but behind that we
use actually Phoenix to handle all the
WebSocket connections and in this case
Phoenix is quite dumb it doesn't do
anything else then you know forwarding
the messages we get from the WebSockets
the good part is we can scale it and it
doesn't you know it doesn't have any
business logic inside it so if it
crashed one process and this is the way
we try to forward the messages so we use
RabbitMQ with Phoenix to propagate
messages from the WebSockets to the
backends why do we do this I mean life
could be easier just using elixir
process but once we start using an
elixir process and the backend service
talk directly to each
they're not independent anymore so we
can't say the next service will be
implemented in Golan or something else
just because it's for this certain case
the better to also the risk of you know
why you refactor your code inside the
service and you break something it's
bigger in terms of it's not that obvious
you don't have this API where you can
see what can break and what can't break
and for data synchronization between the
service because each service has a
separate database and holds only the
data or writes only the data it's
responsible of like you know
authentication is responsible for the
user data and data capture so the micro
service who takes all the data from you
know the online questionnaire is
responsible for report but what if I
want to use user data in data capture
well if you use the same database like
authorization then it's not independent
anymore you can't deploy it without
having dependencies on schema changes in
authorization so that's why we want to
just replicate the data and take only
the information we need for data capture
because we don't want to see when he was
logged in last time or we don't need
single sign-on kind of permissions yeah
and these are the services we have at
the moment I have to say we just have
provided a demo version for a sales
pitch yet so we're not into production
this is the goal for for the end of the
year to provide a solution that we can
host into production but the idea is to
have you know per concern and this is
authorization integration like system
integration and data capture separated
isolated service for back-end and
front-end so and then back-end in the
elixir and the front end in elm so the
approach why why do we actually want to
use messages to communicate between
these service well it's all about
dependencies and so one option we would
have would be use using rest and using
synchronous codes but then if you see
the picture on the left hand side you
end up with various kind of dependencies
between the service and once you you
know have more than three service let's
say 10 this becomes even more
troublesome because if you take down one
the other ones might not work especially
because your I have a dissolute system
you have to ensure that you provide
solutions for it for torrents not only
within the service where elixir course
helps but between the service and our
solution we have chosen the event stream
provides a possibility where you have a
broker in the middle and this broker in
this case is Phoenix you get every
WebSocket connections
and you know pass the message is through
so we only have to ensure that Phoenix
is up and running and we can cluster it
and scale it like we need to on on the
other side it's the same with RabbitMQ
and these kind of systems are made for
scaling and for tolerance yes and pretty
much covered this I think well actually
why Alex a back-end service um first of
all it's functional this has some
implications especially if you have been
distributed system you need to scale it
and the functional approach is much
easier to handle especially if you think
about immutable state when you have to
deal with concurrency then it also helps
to you know don't want into any conflict
pattern matching and recursion is very
helpful for an approach which is mainly
event-driven or message driven and where
you have to pass data and transform data
and you want to do it probably in a big
scale also from you know querying the
database getting the data or providing
it in a shape that it can be read from
the UI and protocol to metal programming
and they helped us so first of all
protocols are always good to ensure that
we have different kind of options that
we can say okay in this case for example
for the integer we have a different
implementation of the protocol and this
is really handy especially if you
combine it for a test case we have use
metaprogramming
extend the usual test case for our
scenario for an integration test and
it's so powerful with a few lines of
code you can extend the original test
case and provide the functionality you
need for all your integration tests
concurrent programming I think with
elixir is kind of made simple in a way
that as you know actually what is the
stage and you can hammer it with the
patterns you get from OTB I think even
non-functional language was much harder
and since we have a distributed system
with supposed to be event and message
driven this is actually quite helpful as
well also we can scale as many poses as
we need for the service we run like data
capture well our clients I don't know
your clients but it's the same they
don't like downtime and they don't like
any you know failures so in our case is
very critical as well and a
fault-tolerant system helps really to
provide a reliable service where we can
say okay we're gonna deploy it even on
Sunday because the customer can only
take care of their integration system on
Sunday but we don't have to be you're
worried about the deployment because we
know you know the code itself is
volatile and if even if one process
crashed because we have a problem the
number one picks it up and the
interesting part is that actually you
know kubernetes for example provides
helpful approaches to provide the folder
runs
of service if they crash so they can was
a health shake you just have to provide
a health check API um for your service
and can bring up your elixir service
again so this is actually the summary I
think in the end what we gain hopefully
is high productivity and low maintenance
code so whatever people implemented I've
talked to they always said you know we
installed it and then we totally forgot
about the service just running the
another thing is you know data
transformation is for us one of the keys
and with functional programming approach
that's quite easy and the most important
bit of course scalable and for torrents
applications well phoenix helps us with
its kind of architecture for you know
the WebSocket approach because it's very
performant and scalable and we don't
have to worry that you know if even one
force crashes and the whole system will
fail so why actually Phoenix there are
several reasons I mean one is you know
we can provide authentication with it we
can use pattern matching and the beauty
of Alexia but also I mean plaque is a
really cool concept it's very easy just
functions in you can extend your
controller and your routers in the way
you need to and it's not really hard the
thing we haven't actually considered as
to you
use Phoenix pops up with Redis just
because as far as I know at the moment
there is a client for Ruby on great so
you could community communicate by a pop
top with rails using a gem for Phoenix
pops up and using weather Redis but are
we decided to use RabbitMQ because we
have just had the transactional messages
for the data synchronization already and
rabbitmq but maybe this is something
which will you know expand in the future
so there will be about clients as well
and other use cases in the end we get a
very high available dumpin secure broker
and its central instance for
cost-cutting concerns and when I sing
cross-cutting then I mean more
non-functional like logging and all this
stuff and that's really helpful in a
distributed application it really helps
you to see just in the log file so what
kind of message got in from the browser
from the UI and how the you know
different micro service responded on
what was the result
that cup well yeah so I think the nice
part about is and I don't know who of
you is familiar with it but I if you use
more a request kind of approach with
micro service there's kind of pattern
called a gateway API so the the client
connects through the Gateway API to the
micro service
this helps to exchange services and so
on but actually using a more message and
event
driving approach you know this is kind
of performance because first of all
websites they just need to UM get a
little TCP connection once and then you
I have you know permanent connection
very fast communication and also um
because actually the sender doesn't know
who's sending the message to you can
change the receiver of the message for
example if data capture is not you know
it's not responsible anymore to get the
data for reports then it can be you just
picked up by another service and you
don't have to change any configuration
or pointing in your API gateway so I
think that's for us a solution that
works quite well so RabbitMQ I think I
don't want to talk about this too much
because everyone knows it and it it's
really won't therefore wireless very
reliable performance and provides really
nice solutions because you can pick up
any kind of patterns you know not only
pops up but also RCP so we do direct our
CP codes by a rabbit m2 as well and that
works yeah and then yeah you can use if
necessary transactional handling and it
provides a kind of persistence of
secures this is kind of critical
especially if you want to you know
synchronize data between service and of
course it has a management interface
that makes the developer happy
yeah it's highly
available resilient and reliable so we
don't have to worry about it once its
clustered it's fine
and and we can use this for both kind of
use cases so for the inter process
communication between the micro service
but also which I know they don't really
have to be you know transactional that
will be a big overhead but we can use
transactional handling and persistence
for the data synchronization right so in
the end it helps us to keep actually
their back-end servers with an yeah
technology agnostic approach so if you
want if we want to provide a new
back-end service in a different language
and it doesn't matter it doesn't have to
you know use Phoenix or whatever it just
needs to have a client for rabbitmq and
this is probably widespread over all
languages yeah so elm this is kind of
interesting since they changed yesterday
or they have announced to change the API
I haven't changed my slides that much
for it but well anyway so why did we
came up with Elm actually well the last
kind of a JavaScript Interop framework
we tried was we have chairs and it works
there were no complaints but once you
use NPM for you know dependency
management and you have continuous
integration and both with you know
containers that gets really tricky
because sometimes just modules disappear
p.m. it runs forever and if it's quite
often so we had just the opportunity to
have a look around or what kind of
alternatives are there and M was for us
kind of nice because not only it's
functional and it when does excel
compiled actually JavaScript HTML and
CSS so you get compiler checks it's
static type you most of the time if it
compiles
you know it will one so you don't have
to check actually in your you know
JavaScript console in the browser if
there is any error and have to debug it
you can do it upfront and yeah this is
not a real end session I'm afraid what
so the things we liked about Elm and
what we picked it up is its functional
aesthetic type its provides stateless
immutable data and you know you can all
use all this pet imagine recursion which
is very nice for us because when you
render the UI from the graph and then we
have to do transformation over the graph
result and then also it provides an
architecture even if it changed snow
which is kind of straightforward so just
a hint if you want to use my own you
probably you have to keep in mind that
they will have breaking changes and it's
not gonna be so part of reactive
programming anymore but actually it's
getting easier it gets more close to
elixir and down
early so we'll be more like event
handling in general and just a few
renaming cool powers the time-traveling
debugger because the you know it's on
static type and you can record all the
events in time you can replay them as
well and this is kind of nice because
you can see actually what is going wrong
or what behaves in what way at certain
stage and a very important was the
semantic versioning of the package
manager so now since we replaced almost
replaced we have just we don't have any
trouble together with continuous
integration the build is much faster we
don't get so many dependencies and it
just works once it compiles it just
works and again that means high
productivity and low maintenance code
for a small company that's really
relevant especially if you want to scale
and efficient data transformation that
makes it easy for us and the turned out
it's even a nice language to pick up for
back-end developers so before yeah we
had a lot of back-end developers picking
up reactors as well but only to certain
extent and then we said ok we trust you
guys and you take over and but with elm
it's now like we haven't come
understanding and we have a similar
approach on the your iPod like on the
backend part and it just suits well with
the event-driven architecture why
I think we can skip that because it's a
long list and we are running late
already so and if you have questions
than how well yep stay a mic oh okay I'm
sorry mate but I just catch up with me
later
yeah sorry for the daylight guys and and
I hope you're not too disappointed it
wasn't less code centric talk but um
maybe it gives you some inspiration for
the problems you have to deal with to
think a bit broader and to think how you
can use maybe even elixir in a different
approach thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>