<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Chaos at Netflix - Casey Rosenthal | Coder Coacher - Coaching Coders</title><meta content="Chaos at Netflix - Casey Rosenthal - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Chaos at Netflix - Casey Rosenthal</b></h2><h5 class="post__date">2017-03-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_ploxRVSlN0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so my name is Casey I work at a company
called Netflix how to describe what they
do so Netflix is in is a subscription
business ok so it's kind of like a
library so like a library you get a
subscription you take your card and you
can like get out whatever book you want
when you want you can read books out of
order
you're not confronted by advertisements
that sort of thing so it's similar to a
library well except we hire people to
act out the stories so it's like a
library but so basically Netflix is
value prop is that we save you from the
burden of reading but like a library
it's very popular so it's so popular
that Netflix accounts for Netflix
traffic download traffic accounts for
over 30% of the bits on the internet
right now which by our calculation is is
a lot and most of that data is coming
out of the data plane we have the
world's largest CDN by far most of that
data is coming out of the CDN but you
know we're a functional program
programmers many of us here so
philosophically we're Hobbesian we
understand that state makes us civilized
but we want to keep it as small and far
away from us as possible so let's not
talk about the data plane instead we'll
talk entirely about the control plane
which runs in AWS in three regions us
west us east and EU west and the control
plane is a micro service micro services
mean a lot of things to a lot of
different things to different people
different companies so I'm going to
explain what a micro service is from
Netflix is point of view and then we can
draw analogies to airline and other
systems so here I have a diagram to
scale of Netflix's micro service
architecture we have three for
simplicity sake I just renamed them A
through G but an example of micro
service might be contained
just information about customers or
another micro service might afford
signup functionality or have data about
movies or things like that and we
support thousands of devices so we've
never really deprecated a device going
back to like blu-ray players and from
like 2007 so it's big so a request comes
in from a device and in this case that's
hitting service D which might be our
reverse proxy and of course in a micro
service one service isn't going to be
able to have all the context to respond
to that request so service D will reach
out to service C and F to grab pieces of
information like oh let me find out
about this particular customer or this
particular movie that they're looking at
and C will reach out to a and F will
reach out to B and G for additional
information and a will reach out to B
which G also reaches out to an F reaches
out to e so you C can see very quickly
we get this like the color isn't
important but it's like green spaghetti
of message passing between the different
micro services and once they've all
responded to each other or timed out the
request collate and we returned their
responses collate rather and we emit a
response back to the device except I
lied we don't have seven we actually
have several hundred here are a few the
top ones in in our streaming path and in
this visualization you can see the
circles or micro-services here is
traffic coming in from the internet and
hitting our reverse proxy and then kind
of propagating through our microsystem
architecture so it's a little bit more
complicated than those seven but the
problem remains that we have all of that
complicated communication and message
passing between them now in a former
life I was a consultant engineer and
architect and so I was often brought in
to fix engineering problems and what I
would typically see is that engineering
teams just generalizing across the
industry new projects long existing
projects teams will tend to optimize for
one of three things the performance of
the application its availability or its
fault tolerance I can get into the
difference between availability and
fault tolerance later if anybody's
interested but I'll assume that this
audience kind of understands the
difference between those two and a good
more experienced team tends to be able
to optimize for all three of these
variables simultaneously at Netflix we
explicitly had a fourth goal feature
velocity so I'm sure you've heard you
know agile lean you know there's a bunch
of process and management guided
designed to improve feature velocity but
what I'm talking about here are
architectural and engineering decisions
that optimize for feature velocity and
so as engineers at Netflix what we try
to do is yes optimize for the bottom
three but also for for this fourth
variable feature velocity and that's one
of the reasons why we chose a
micro-service architecture because
having a large product and dividing
functionality up this way we basically
don't have one large engineering team at
Netflix it's easier to think of of
Netflix the engineering org as being
comprised of like a hundred or so
smaller engineering teams of like five
to seven engineers each and that way
each one of those engineering teams can
deploy on their own cycle they can you
know assume their own best practices for
those those four variables which might
differ between different functionality
and ultimately that architecture allows
the whole company to engineer faster
slight tangent because we're here and
because I know a lot of you are
interested in elixir increasing in
popularity and evangelizing for this
language I also am very interested in
air lang and elixir getting more market
traction
so I'll point out that rails and I would
love to see a lick sir have some of the
success that the rails community had
rails was successful because it
optimized for feature velocity okay the
the convention based architecture of
rails allows any rails engineer to
pretty much move to any other rails
project and within a day pretty much
know how to navigate the application
that is the key to that was the key to
Rails adoption but you notice it didn't
optimize for any of these other three
things which is interesting so so if we
look to elixir projects like Phoenix
yeah we want to optimize for one of
these things but just copying rails
isn't isn't gonna do it that's that
that's kind of already been done we
would need another breakthrough and
certainly with the airline VM underneath
we can optimize for things like
performance fault tolerance and
availability but we also need another
leap in feature velocity above and
beyond convention based architecture as
a tangent so let's get back to micro
services so that's what Netflix means by
a micro service architecture I hope it's
transparent how you can analogize
between that and an airline application
or pretty much any other distributed
system there's a problem with this which
is that all of that messy spaghetti
communication between the different
services is too complicated for a human
to hold in their head so at Netflix we
don't have chief architects we don't
have architectural boards we don't have
principal architects because typically
that role is reserved for people who
kind of know how all of the parts fit
together and we've just done away with
that and said you know what human can't
do that so we're not going to rely on
that so we lose the ability to
interrogate a human and say hey if I had
this new feature is it going to work and
for them on high to say oh yeah that
kind of or no you really haven't
considered this other thing over there
we don't have
that and and that's that's okay but it
gives us less confidence in the
distributed system so let me highlight a
particular problem in a distributed
system and I'll do it with a game called
the beer game anybody here like beer or
my barking up the wrong tree okay so a
few of you so the beer game has three
players customers who like beer
retailers who sell beer and brewers who
create beer and typically this game is
played in MBA courses so if you plan on
getting an MBA and you take a systems
thinking class I'm ruining the class for
you
don't bother typically how it's played
the the classes divided up into three
groups corresponding to the three
customers and the object is to optimize
for customer happiness profit for the
retailer and profit for the brewer and
they're sent into different rooms and
they're only allowed to communicate to
each other by passing slips of paper
back and forth that indicate intent to
purchase or goods or money exchanged so
I'm going to take you through a couple
like facilitated rounds and I'm
condensing this but you know just to
make a point numbers aren't too
important but I'll put them up there in
case anybody wants to follow along so
here in let's say week 0 the customer
demand for this kind of beer we need a
name we're in San Francisco so we'll
call it start-up beer but without the
last you because you know startup beer
so it's a it's a relatively unknown kind
of beer although everyone secretly wants
it and so demand is for one case it this
week customers purchased one case of
retailers - in stocks they're left with
one they order one the brewer has eight
ready for sales bottling one so we've
got a system in steady state right
demand for one case production of one
case we need a hero
yeah one each yes we don't have to worry
about competition this is a far simpler
game back to our hero we'll call him
Earl we have a an engineering mouse and
he just he just knows he's on the verge
of an engineering breakthrough and he
just has to like sort out some thoughts
and get this idea out and then he knows
like you know stardom and fame are our
will be his but he just needs like he's
looking for a muse to help him work this
idea out and of course he could he's a
musician obviously because he has a
guitar so like any good musician he says
you know what I just need to find my
inspiration in the bottom of a beer mug
so he goes to the retailer with an
intent to buy three cases of beer he can
only buy two because a retailer only has
two I put a negative two there to
indicate a loss of potential sales the
retailer orders for to stay ahead of
this of the brewer has eight they start
bottling five to stay ahead of that and
that weekend Earl writes out a manifesto
and it's brilliant right it's going to
change how we all do our jobs he comes
up with concept called ops dev and it's
brilliant because it takes the best
practices of of one practice and applies
them to another and that always works
right so the the manifesto is is great
you know we'll treat developers you know
the way you know we've learned to
optimize for things and operations so
we'll treat them you know hot-swappable
and we'll shoot for engineers who aren't
aren't coming up with original ideas
because we want immutable engineering
right so we want boring engineers who
are easy to replace if they're too
original they might give a talk at a
conference and then somebody might hire
them away and then like the you know how
you hire some you know somebody new to
come in and do the job and they look at
this you know
very creative solution and then they
can't figure it out that's just bad so
again from an Operations standpoint we
want
boring boring engineers and that way we
can just focus on their productivity we
can give them like hand exercises that
they can type faster and increase their
throughput and in his TL DR he credits
startup beer for this brilliant idea so
you post it like you know number one in
hacker news obviously and Twitter blows
up and so the next week demand for
startup beer from you know other people
who want you know this inspiration
skyrockets to sixty four core so they
can only purchase five because the
retailer only had five in stock so
there's a missing demand there of 59 the
retailer again wants to stay ahead of us
so they are 65 from their Brewer who
opens up a new distillery and starts
bottling 250 bottles to stay ahead of it
but then that weekend something happens
are we need an antagonist I'll call him
Joe writes a scathing rebuttal and this
is it this is another drug just complete
coincidence
scathing rebuttal on on Twitter and it's
but I'll read it in its entirety it's
dude no and you know the internet
explodes and you know Twitter falls down
from all the retweets because they can't
handle it because you know they chose
casandra's are back in and and and and
and Earls Earl is crushed like you know
his own personal stock you know bottoms
out hits an all-time low you know Earl
chained you know the Earl coin so you
know it's it's just it's just crushing
for him and the next week the only one
who wants to buy startup beer is Earl
himself so the demand is three he wants
to drown of stars essentially he
purchases a three easily enough to
retailers left with eight the brewer
finally gets those two hundred fifty
bottles in and then the following week
we go back to steady state demand of one
case per week which you know let's just
jump to it leaves us with this result
the retailer has 62 cases in stock
missed the opportunity of selling 61
cases the brewer has a hundred ninety
six cases that they're never gonna be
able to sell in an extra distillery WTF
are they going to do with that now
typically this this is obviously
condensed usually this process takes
like a day to play out in three teams
but almost always this is the result of
the game which is why professors like it
because it's predictable or because it
makes their students miserable one of
those but but I don't want to leave you
miserable so let's look at this cute
picture of Earl the mouse instead so if
we assume that the point of the game
isn't to make people feel bad then what
is the point well the the actual put so
specifically the point of this is to
teach in effect in systems thinking
called the whiplash effect where or the
bullwhip effect sorry we're a small
perturbation and a small change in the
input of a system creates a very large
swing in output but more generally all
three teams are smart they didn't do
anything wrong
and yet we ended up with this result
that is highly undesirable and that's
where there's a direct analogy to
distributed systems and micro service
architectures each one of these micro
services can be provably correct right
or more reasonably have like full unit
tests full integration tested like may
work but the interaction between them
exhibits a behavior that is undesirable
I can give you another example that's
maybe a little more realistic than a
musician Mouse Netflix has a show called
house of cards
has anybody seen house of cards Oh a lot
of you cool of course I'm referring to
the American version but but yeah so
that you and that right so I'm glad a
lot of you have seen that right there is
the brilliance of Netflix they turned
something that is so boring nobody would
stay up late worrying about at
presidential politics and they actually
like meet at dramatic but in a former
life I had a I had a job where I had to
commute semi-regularly between
Philadelphia and Washington DC which is
the perfect way to watch house of cards
like on a train going into DC because
the story mostly takes place in DC and I
think it's in the second season there's
like a rather dramatic scene that takes
place in in a subway and so there I am
on the train I got my laptop like set up
on on my lap and my coffee and there's a
scene and I'm surprised and I'm like
whoa and like I spilled some coffee on
my laptop and so like any user I start
hitting refresh right like I must have
hit like a thousand times and of course
like Amtrak I think they use cell towers
for signals I'm not entirely sure so
there's a network partitions like all
these requests back
and I don't know a Q and chrome
somewhere I have no idea and and
eventually like all of these requests go
over and hit Netflix in the control
plane and the response I get back like
is a is a new page for Netflix that
doesn't look familiar to me so what
happened on the Netflix side well one of
these micro services might be a service
that personalizes the experience for
users right so it has recommendations
based on your viewing history that are
specific to you and another micro
service might be might track information
about a particular user so a good idea
if you've got a Microsoft at tracks user
behavior is that you don't have all of
the data for all of the users because we
have a lot on one machine instead you
can use like a consistent hash to make
sure that this the the requests are
distributed around a fleet of servers
and for any particular user it hashes
them and always sends those requests to
the same service the same node good idea
in at Netflix the engineers are also
responsible for the operation of their
own micro service so they have rules
like keep the service properly sized so
a cluster might have auto scaling rules
where if I don't know say CPU load goes
below a level it'll shrink and if the
load goes above a lot of level the the
cluster will expand and will launch new
instances of that node and other things
like fallbacks between different
services if it can't connect to a
particular service it will have a
logical to fall back so if I can't find
your specific recommendations then I
might just show you the most popular
things that we have right now which I
don't know Zootopia and Luke Cage I
don't know what like normal people watch
but whatever like the most popular
things are we might just show those
or display those as potential choices
and then other things like if I can't
connect to a database then maybe I just
serve the answer from local cache so the
database being our canonical storage
that has the most up-to-date information
if I can't hit that for some reason then
whatever the last request
I served I'll just return that so all of
a sudden this user Microsoft in system
hash and each time it makes a lookup it
takes a certain amount of time with the
database so the database starts to not
respond as quickly as it could
so the that machine starts serving its
response out of cache right great
fallback makes sense
of course serving something out of cache
as opposed to hitting the database uses
less computational power and other
resources so that servers CPU load goes
down so we have that scaling rule that
wants to be you know cognizant of our
cloud spend and so it looks and says oh
average CPU load has gone down I'll
terminate an instance to be right sighs
I'll pick that one because it has the
lowest so that one gets turned off and
request that the the responsibility for
those users gets distributed to the rest
of the cluster the personalization
service which was depending on response
for that now sees they can't get a
response because it's bitties of busy
reorganizing those customers so it
returns to this default of like zootopia
which is like not something that I
particularly am interested in watching
so because it's different right do what
any user does I start hitting refresh
again another thousand times partition
it comes through again and it hits the
new server that now has responsibility
for me as a user and the same thing
happens it switches to serving out of
cache because it can't handle the the
database connection for that node gets
overwhelmed and so the average CPU load
goes down to the server shrinks of
personalization starts returning this
default thing but there's multiple users
involved on these different servers so
now a lot of users are seeing this
default
Paige that is not what they're used to
so what do they do yes so they start
they start hitting refresh now we have a
user induced retries storm and you know
all of the nodes in the subscriber in
the in the user microsoft to switching
from cache and so the cluster shrinks
dramatically until there's only one node
left and then that falls over because
they can't handle the load
and the entire Netflix service falls
down because there was a very surprising
scene for me in the subway in house of
cards while I was on the train now this
did not actually happen maybe but it
illustrates the problem and so so far
you'd be entirely correct if you said
you know we're drowning here and you're
just describing the water so what do we
do about it so that's where chaos
engineering comes in you might be
familiar with to chaos applications at
Netflix if you follow our tech blog when
it's called chaos monkey another is
called chaos calm actually I don't know
does anybody heard of any either of
these oh well are you cool
so chaos monkey roughly chaos monkey
will sudo randomly pick an instance
running in our cloud deployment and turn
it off I know it didn't take long to
write this
feature velocity remember so this is
actually this was actually really really
useful to us as we move from the data
center into the cloud we were faced with
a new issue which is that whereas things
sometimes go down in the data center
sure that's true we were guaranteed that
it would happen much more often at our
scale in the cloud instances would just
disappear and you know with I don't know
hundreds of engineers and without like
an architect or we just don't have a
mechanism at at Netflix for somebody to
dictate to engineers
that's not how Netflix works we engineer
solutions bottom-up so there's nobody
who can say hey we can't go down any
more from a server disappearing so all
of you have to do X you all have to have
you know N equals three redundancy or
these fall backs or you know say there's
a dozen best practices we know what they
are there's just no mechanism for us to
tell everybody what to do so instead the
brilliance of chaos monkey is that it
took that pain of being in the cloud and
brought it to the forefront for every
engineer in there the course of their
everyday work so we don't just run this
on in production we run it run it in our
test environments and our staging
environments so that as you're
developing your application your service
during business hours some of those
instances are just going to blip out
they're gonna go away so in your first
couple weeks you're gonna have to
overcome that event you're gonna have to
engineer in some redundancy or some way
some fallback to handle that and we
don't care how you do it
as long as you have that resiliency and
this was extremely effective so in the
last four and a half years we have not
had an outage from a server disappearing
because we brought that pain forward and
made it part of the experience of all of
the engineers on a regular basis and now
it's just part of our culture we don't
have to worry about it we thought that's
great at a small scale so let's do it
on a big scale so chaos Kong where cast
monkey turns off a VM KS Kong turns off
a region for whatever reason AWS won't
allow us to turn off an actual region
yeah something about having other
customers there so we simulate it but we
simulate it effectively enough and we do
this often enough that again all of our
engineers have had to build their
applications to be active active and
multi region and this also works so this
is from just a few weeks ago one of my
favorite headlines because of the
metaphor when s3 went down on in us east
Netflix did not it did not because we
were able to move traffic out of that
region to our other two regions and we
were able to do that because we were on
Cass Kong often enough that we just know
that that works so we thought okay these
two things are great Cass monkey and
chaos Kong they're very effective at
giving us confidence in these particular
failures so how do we do chaos
engineering like more and better and at
that point so this was about two years
ago if I went around to the industry or
even people in the company and asked
them hey what's chaos engineering if
they had an answer usually the answer
was along the lines of oh that's when
you break stuff in production and that
sounds cool for a LinkedIn profile maybe
like I break stuff in production but I'm
sure all of us can imagine situations
where we break stuff in production and
that doesn't add any value to the
business so we need to something more
than that so we look to formalize chaos
engineering and we did we have like a
manifesto a she thing up at principles
of chaos org and the TLDR is that chaos
engineering is the discipline of
experimenting on a distributed system in
order to build confidence in that
system's capability to
withstand turbulent conditions in
production so turbulent conditions in
production we all know stuff happens in
production we want to design experiments
so that we can be confident that when
that stuff happens our system will be
resilient this is in contrast to testing
so I'm gonna I'm gonna call it classical
testing but in in in classical testing
you know you generally have some
condition you give it to a function or a
system and you assert an output and if
your systems are more complex then you
can kind of view this as an amalgamation
of functions chaos engineering is a form
of experimentation not testing so
strictly speaking testing does not
create new knowledge it gives you a
valence for a property you in a test
you're making an assertion and usually
it's binary you find out it's either
true or false but in an experiment you
you uncover new things interestingly
enough property based testing lies
somewhere in the middle and I haven't
like completely thought out how that
fits in but certainly experimentation
different than qualitatively different
than testing and so we have some
advanced principles for chaos
engineering and experiments that we set
up one is that we build a hypothesis
about the steady state behavior of a
system as engineers we all have a
temptation to want to know how something
works so our temptation and
investigation is to validate that the
model of the of the code matches our
mental model and a chaos engineering
we're saying that's not what we're
trying to do we want to verify that
something works we're less interested in
how it works okay so our hypothesis for
the experiment is most powerful if it
focuses on the steady-state behavior of
the system some KPI is some important
business metric for Netflix it's easy we
know
what the diurnal pattern looks like for
play starts video starts per second so
we have a KPI called SPS starts per
second that we have up on charts and
stuff that we can see like you know we
know it goes up a little bit before 6
o'clock Eastern time and it's just it's
a very smooth very nice nicely bounded
metric so we know what our steady state
behavior is very real world events so
obviously we don't want to introduce
variables that aren't effective we want
to look at things that actually happen
hardest going bad service disappearing
Network partitions high latency it's not
all failures good things sudden
increases in traffic things that aren't
good or bad just weird combinations of
things hitting at a time and you know
operational effects experiment in
production at Netflix this is easy
because for us to build a system that
replicates our production system and
test it would be too costly so we just
can't do it
but we think this is as valuable as a
general advance principle because an
experiment you're going to have things
affecting different systems in different
ways and if you're running chaos
experiments on not your production
system then you're building confidence
in not your production system which is
much less valuable
so really chaos engineering has to take
place in production and automate
experiments to run continuously this
isn't something that can effectively be
done manually so we put a lot of
emphasis on automating our
experimentation so these are our
advanced principles it resonated with
the community so we ran a chaos
Community Day last year in San Francisco
this past year in Seattle all of the
usual suspects Google Microsoft Yahoo
AWS etc many startups so the this
concept this formalization of chaos
engineering
only resonates if you're interested go
there sign up at the to the Google group
or come find me we'll have another one
this year at least one more this year so
let me illustrate the tools a little bit
more so again say we want to test
something similar to what I described
earlier we have a gateway a reverse
proxy an API layer and then a
personalization service what we can do
is as a request come in to the gateway
we can randomly select our pseudo
randomly select some percent of requests
to participate in an experiment and as
those requests populate fan out through
the system in this case I've designated
one of those requests as red to indicate
that's part of the experiment we have
injection points between in the client
libraries between different services and
at those injection points we can say
based on the context in the request
header whether or not we take action so
in this case we'll allow that request to
pass through except when it tries to go
from API to the personalization service
we'll just we'll cancel it well they and
Co will time it out or to return an
error or whatever and then we'll see hey
does that have an effect on SPS so this
is a graph of our starts per second and
our first tool fit failure injection
toolkit did this the problem is we start
an experiment right there where we did
this to 10% of traffic and you can kind
of see something there but you know I
don't know is it statistically
significant so there were a couple
problems with that approach one is that
people might come across that error hit
retry and as they come through the
second time they're not part of the
experiment so it just works we can fix
that by making making making that sticky
so that those users are always forced
into that experiment and so we came up
with a couple other things that were
allow us to have a better empirical set
up a barek better automated experimental
setup we came up with what we call chap
the chaos automation platform and what
chap does is it will take a service that
we want to study in this case service C
and it'll spin up two nodes identical to
the rest
the service a control and an
experimental group you can see they're
deployed in the cloud and will apply the
conditioner event to the experimental
group and then we'll just compare those
requests as they come out of the system
at the system boundary so it looks more
like this
we can select a smaller amount 1% of
requests for the control and 1x percent
for the experiment and as they travel
through that injection point says ok
most of the traffic's going a regular
API 1% is going to API control and 1% is
going a API experiment and then as those
requests propagate the API experimental
group fails out its response to
personalization here you can see the SPS
for those two groups the colors again
don't matter but one of these lines is
red one is blue roughly the SPS looks
the same which is great that gives us
confidence that having this failure
where API cannot talk to personalization
does not affect SPS here's another way
of looking at it on the left are the
successful responses the blue line and
shown in the left that you you have some
height to was the control group the red
line at the bottom is flat because we
failed all of them then we're successful
and on the right you have the fallback
successes so the fallback was triggered
for the the experimental group which has
some hate here and the blue line is flat
at the bottom because the control didn't
have any fall backs but CPU utilization
was much higher for the experimental
group so that worked and then finally we
have a chaos maturity model because
we're engineers so of course we overload
acronyms so CMM measures two axis
sophistication and adoption
sophistication is that here inserted
those advanced principles of chaos
engineering is that hypothesis about
steady state varying real-world effects
running in production other things like
can you automatically stop a chaos
experiment if something bad is happening
to those customers because that would be
bad and adoption
does the leadership in your organization
buy into it or is it as their
self-service tool our teams actually
using this adoption tends to be a little
bit more straightforward to measure and
if you put these two together then you
can have a nice quadrant where when we
talk to other companies we try to help
them figure out where does their chaos
fall into these quadrants so Cass
Comcast's monk you've been around for a
while they're both very sophisticated
and have high adoption at Netflix we're
just starting out with chap although in
the past quarter we've made some
significant improvements to its
sophistication so now we have to work on
adoption and in five minutes left so in
the five minutes that I have left I want
to touch on another idea so that that's
that's basically what I'm gonna tell you
about chaos our engineering we want to
touch on another idea because I think
this is important for distributed
systems engineering and that's intuition
engineering we just make up these new
disciplines and software at Netflix we
just do it all day so internet intuition
engineering we have an application
called visceral but when we are when
we're running a traffic failover when
removing all those users out of one
region to another
we've got great metrics so we can put up
like 48 charts and we can see all of the
relevant things changing through time
and we can dig through those to know
oh is it safe to move users over problem
is takes long for a human to be able to
dig through all those charts so what
intuition engineering does is it focuses
on creating user experiences that in an
instant give you a sense of the state of
the system I have an absurd metaphor for
this but it works so anybody here ever
on call okay so it's okay cool so some
of you can relate so imagine we meet a
soup and we covered it in electrodes and
the placement of the electrodes
corresponds to different microservices
and if that micro service goes down then
you get zapped there
now when you're on call you have the
privilege of wearing this suit
internally at Netflix we call this
affectionately the Payne suit and the
hypothesis is that after wearing the
Payne suit for a very short period of
time you would just have a sense of the
co system and somebody could ask you hey
how's Netflix doing and you'd be like
yeah it's pretty good or you wake up at
3:00 in the morning your shoulder hurts
and you're like huh subscribers down
again dammit
now we haven't built the Payne suit
yet but we did we did create a visual
analogue and that is visceral so the
dots moving represent requests the big
circle in the middle is the Internet
those are our three regions so these are
request responses going out to the three
regions blue dots are successful
requests occasionally see a red one
which is an arrow occasionally an orange
one wishes to fall back and in about as
long as I've been talking about this
your brain has already kind of sort of
figured out what normal looks like for
the volume of dots the speed the color
the motion and so at a glance we can
look at this and know oh yeah everything
looks normal and this has proved to be
invaluable to us as we manage
distributed systems and I have a lot
more to say about that but that'll have
to wait for a different venue
so I don't think we have any time left I
am just an engineering manager I don't
actually do work so these are the people
who do the real work
I work at Netflix I'll be here for most
the rest of the conference
so cool thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>