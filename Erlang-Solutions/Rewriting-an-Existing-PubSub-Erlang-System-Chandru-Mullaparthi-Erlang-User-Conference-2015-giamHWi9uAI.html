<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>(...) Rewriting an Existing PubSub Erlang System - Chandru Mullaparthi - Erlang User Conference 2015 | Coder Coacher - Coaching Coders</title><meta content="(...) Rewriting an Existing PubSub Erlang System - Chandru Mullaparthi - Erlang User Conference 2015 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>(...) Rewriting an Existing PubSub Erlang System - Chandru Mullaparthi - Erlang User Conference 2015</b></h2><h5 class="post__date">2015-07-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/giamHWi9uAI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone my name is Chandra I'm
Software Architect at 365
C I'm gonna talk about a few lessons
learnt while rewriting a bub subsystem
in Erlang
it was previously written in our line
but we wrote it again because you know
cord accumulates Kroft over time and so
firstly a little bit about battery 6
fine case some of you missed it
yesterday when my colleague Mike talked
about it
we were founded in 2000 it's a privately
held company located in stoke-on-trent
in the UK it's the world's largest
online sports betting company over 19
million customers and it's a very
technology focused company they the
secret for success really is their
success in harnessing new technology as
it becomes available and see yeah
battery swag really likes Erlang it
completely believes in the whole
redundancy model so we have actually two
principles of we're architects and I'm
the good-looking one so a bit of a
history of hairline news within the
company I've joined recently but basic
so I started experimenting with our line
in about 2011 so far there have been
three major systems written and in
Erlang and there's a fourth one in
progress the code base as you'd expect
is of varied quality because when you
first pick it up traditionally it's been
Java and.net with lots of Microsoft
sequel server and so some of the first
code bases look what you'd expect
someone coming from a Java background an
object-oriented background writing ed
line looks like so I'm gonna talk about
what we call the publisher internally
its valley on the face of it is a fairly
simple system there's a bunch of
databases which are data sources within
the business this is some called a data
pump which basically collects all this
data and pushes it through the publisher
onto well all the customers using
various devices I'll just quickly show
you what it's doing so typically if you
go to the home page and let's say if you
look at in play you if you look closely
you'll see some of these figures
changing or you see that right top right
it's sort of showing you how the game is
progressing pretty much every change you
see on this website is this change being
broadcast to every connected client in
real time so as you can imagine them in
with with about two million customers
concurrently connected at peak times
there's a lot of traffic going out and
that's what the publisher handles so why
rewrite hopefully this Lydell explained
to you this is showing the internal
process structure before we did the
rewrite this is the data pump this is a
socket handling process which which is
this is a single socket through which
all the data comes and every oval you
see here is a process so so it goes
through decode stage one and then it
went through a decayed stage two and
then it went through a decode stage
three and then it went through a stage
four you can imagine how many will fit
in this slide right segment is five and
then it went to another decode stage six
and then it went to a pool of processes
again
the number of processes selected was a
fairly finger in the air number as in
okay let's choose about 400 processes
and see if that deals with the load and
that forms of work to another pool of
post processes which then send it off to
the client
and that's the simple version of it if
you try to translate this into cold it
can get a bit and but the biggest
problem is it's a complicated process
structure so every time you wanted to
make a change in it it sort of gets
harder and there were message queue
hotspots because between each of these
processes we were using our like message
passing so if any one of the decals is a
slows down and the previous one is
slightly faster you you get these
message hotspots while you know while it
sort of builds with it and even if
there's a message queue build-up it
catches up it can lead to instability
which is sort of and the sort of
unpredictable in nature which is not
great so we decided to rewrite it from
the ground up so the first thing we did
was looked closely at the data basically
the data was shipping out clients is
essentially a very large tree and the
number of nodes in a tree is about 2.2
million nodes in the tree and but if you
look just below the root there about
130,000 top-level nodes and all these
are independent of each other so
actually there is no dependency between
them so the natural thing to do in
Erlang is obviously what we did was okay
you know the the data source was a
single socket you know there's no way we
could have changed any of that so you
have a single process to take matches of
a socket decoded update global State in
ETS but then we modelled each of those
130-thousand top-level cross nodes as a
process so it looks a bit like this now
so the first statement was we had a
process taking messages of the socket
decoding it updating global State
because we need that and spawns 100
thirty thousand processes and then we
have one process participe connection
from the client I mean typically our
clients use TCP connections if it's a
flash client but if it's web-based plain
HTML clients then they use HTTP so we we
model it as one process per client so
that we get nice isolation because one
of the problems we had with the previous
implementation was that because we had
pools of processes dealing with pools of
customers if one of them was on a slow
connection it affected everyone behind
it whereas now you could have a process
per client and if one of them slowed
down that's fine because the rest of
them carry on so that the first item
actually sort of worked well but the
problem was message queue was building
up at source because that single process
which is taking messages of the socket
couldn't decode it fast enough do
everything that it was supposed to it
was it was heavily loaded it didn't
cause instability in the system because
you know you have active ones optioned
on the socket which means you won't get
overloaded but it just means you're
really slow and you're not keeping up so
we just slightly tweaked it in the
second attempt so we I mean I I did make
a lot of fun of the number of decoding
stages I had but I had to eat a bit of
humble pie and say okay fine we'll we'll
have one extra decoding stage in there
just to cope with so that's what it
looked like in the end so really one
process was taking off socket putting it
into an ETS table and then another
process which was reading it off as fast
as it could and then doing the rest of
it I mean we I constantly chose not to
use message passing between the two
again because the socket handling
process could read faster than the
decoding process could keep up and ETS
acts as a nice buffer in between you
know all that you use up is a bit of
memory but you don't get instability
because one of the biggest you know with
our line message passing is there's
really powerful but once you get a
process which messes cues building up it
sort of all goes haywire and the system
performance becomes unpredictable so um
so we have these data sets which we
capture during our normal operation and
we test with that so but you know this
tree size grows with time as we add more
fixtures which we trade on so it worked
well with about 1.9 billion nodes in the
tree but throughput and latency fell for
2.2 million and we couldn't quite
understand why that is because it's just
a few more processes it should have just
worked so then we started looking at
what was possibly the cause of it we
thought maybe it's you know garbage
collection slowing down the system so we
thought we'd investigate that and you
know garbage collection isn't one of
those things you really think about in
Erlang because it hardly ever is an
issue so we had so I had to sort of dig
into the manuals and ask him the mailing
lists to say you know so to try and
understand it and actually there was a
few surprises in there you know GC spur
process great you know that's no
surprise that's fine
and again regular GC of a process is
sort of generational so every year long
process has an old heap and a new heap
and data that survives at least one GC
is put on the old heap and finally when
there's no space space on the old heap
of full speed for full sweep is
performed but there but the surprising
thing was that the time taken to garbage
collect eats into the 2,000 reductions
available for process that came is a big
surprise for us I mean if you once you
know this and you think about it it
probably makes sense because you'd think
you could do it either way but it's
yeah you can understand why that is but
it came as a surprise and but the
implication of this is that if you have
a busy process you know a process in
your system which which you can't
paralyze if you like or you can't make
concurrent because you've got to handle
messages in sequence and it's and it
produces lots of garbage then it'll
quickly spiral out of control because
the more garbage produce it's got fewer
reductions to actually do its work
but actually it's a busy process there's
a lot of work coming in so you stuck in
this cash for energy situation where so
and yeah so once we knew about this
reduction count penalty that if you if
you actually do a longer GC then
actually you're being your process is
being penalized we thought okay let's
see where else you know how else does a
process incur reduction count penalties
so I mean the first one we knew already
this is what are sending a message to a
process which already has a large
message queue you're penalized for it
and that's a so the other thing we found
out was if you have a long GC then again
your reduction count is penalized so
when we started looking through the beam
cord it found out that actually your
reduction count is also affected by the
size of message you send to another node
so if you're using RPC say for example
and you're converting a big are you
sending a big message across at least
from the way I read it it seems like
your process is going to incur a penalty
and if you're doing a lot of that you
might see a process slowdown but with no
real with no obvious explanation that
it's happening and it also seems that
actually it's proportional to the size
of binaries which are create which your
process is creating again none of this
was really obvious or well known and
there's a lots of other places where
reduction counts are being adjusted and
I'm not really sure what exactly they're
being adjusted for because the manual
says a reduction roughly equates to a
call to AB if I think that's what it
says so it would be it would be it'll be
nice if we had some documentation which
says you know if you're doing this sort
of thing like the efficiency guide we
have in their line if it said you know
if you do this this is the kind of
penalty you're going to incur so that it
helps in the design of our processes to
say okay you know if that is incurring
penalty I'll design the system slightly
differently I mean I I do appreciate the
OTP teams point of view that actually is
an internal implementation detail he
shouldn't have to know about it and for
the vast majority of cases when you're
writing at length of actual you don't
have to think about this but when you're
sort of pushing for the maximum
performance you can get out of the
system it's helpful to know because it's
not just performance penalty because
some of these lead to instability in
your system as well so if you remember
the process structure what this one one
process is dedicated to reading messages
of the socket and there's another
process is dedicated to decoding
messages it's receiving so with I hadn't
used this process priority flag for a
long time and I thought I know what I'll
do I'll just make it a higher priority
process and that should automatically
give me more processing power and I can
keep up with the the firehose coming in
but actually priority height does not
really mean you've got more reductions
all that it means is that you will get
sheduled before the others and again
it's if the process has work to do which
translates to have you got any messages
in your queue so actually so you can't
really have process busy looping I mean
one of the ways to do busy looping is
you just say receive after you
ten milliseconds wake me up again but
actually that's the timeout there's no
message in your queue so it doesn't
really help I mean I start I'd point out
you know the process priority isn't
really what you think it is but what
would be nice to have is for us to be
able to specify that there are certain
processes in the system which are which
are going to be very busy because we
know they're going to be very busy and
can we have I mean currently we running
the these publishers typically on 24
core boxes I think pretty soon we'll
probably be running in them on 40 cover
boxes in which case I can quite easily
you say you know what I'll I've got two
critical processes which I need them
running all the time because they're
going to be busy it will be good to say
I actually don't impose the mm reduction
limit on it just dedicate a scheduler
thread because I have got 40 to spare
and a and this sort of ties in with what
with Costas and Magnus is presentation
yesterday where if you just you know if
we start having access to chips like the
Epiphany where there are you know tens
if not hundreds of course you know it's
cheap then just say well you know just
dedicate a few course to these process
of mine because I've got loads of them
but yeah that's currently not possible
so none of these things really explained
what was happening in our system it
didn't explain why our system was
slowing down when suddenly we had a bit
more data it's not a it's not a large
went from one point eight million nodes
in the tree to 2.2 when it wasn't lot so
it so we still had no clue but one thing
we noticed when we were running H top on
the box was on a fairly regular basis
pretty much all the beam threads would
go into an uninterruptible sleep state
it so it sort of shows up as a D in the
process state when you look at the
process listing and I wondered what that
was about
so we tried to see what was causing this
we're looking for processes which were
entering that state and along with our
beam process this particular one kept
showing up on a regular basis that again
I didn't know what to make of it so I
thought I'll search for what a huge page
D is and it turns out that yeah okay
huge page D implement something called
transplant huge pages on Linux and if
you look up the Red Hat documentation it
says it's a feature in 64-bit Red Hat
Linux it's enabled by default for all
applications and it's it's an
abstraction over what's called huge
weight support in Linux and again I
hadn't heard anything about any of this
PHP or so when digging the first thing
you notice is actually most database
mint does recommend that you turn this
off before you install the database
because it's not going to work that goes
for Oracle toketee be SAP HANA all of
these guys so we thought okay
so what a huge pages I mean yeah once
you look at it it's fairly obvious the
the typical size of a page in memory is
4 kilobytes but nowadays you'd service
with lots of ram 20 gig of memory
equates to 5 million pages and that many
entries in the page table and these
these entries are typically stored as in
in the layer 2 cache which has limited
memory so if you don't use these huge
pages suddenly you having to do this
store this information in main memory
which which slows it down but Red Hat
kernel allows page sizes of two Megan
one gig and they recommend basically got
if you've got memory main memory in the
order of gigabytes on the Box you use a
page size of 2 Meg or if you've got
terabytes of RAM then use page size of a
gig so
how does this transfer and huge page
work because using I'm in the red the
reason Red Hat they did this develop
this transplant huge base support was
because using huge page support requires
a lot of change to your application
itself you go to plugins API as it
provides and it's non-trivial so Red Hat
developed this THP so that most software
which runs would be able to benefit from
it and according to their website it
says it aims to make it just happen I
think it also does some memory
defragmentation when you when you start
hitting high memory use and actually
which kind of tied up with the fact that
we observed it when the memory usage of
beam exceeded about 20 gig on a box with
32 gig of ram so well we we did what
everyone was recommending and turned
this thing off and performance instantly
improved exactly to how we expect it to
I mean I haven't really investigated in
quite a lot of detail exactly how it
affect beam but all I know is that
actually disabling this made our
rattling system work as we expected it
to so just for record if you you know if
any one of you has similar problems you
can just copy paste that which is what I
did from stack exchange so that's how
you can permanently disable it on your
box so yeah I mean it took her it took
quite a long time to to get to this it
probably took nearly a couple of weeks
to get to the bottom of why this was
happening I mean so now we're looking at
things like system tap to see actually
you know can we again this must be
something which most people suffer from
uh me when I went through the whole
troubleshooting process I felt after two
weeks when I finally solved this I
thought
someone else must have solve this
problem right how come I never heard
about it but if you're running some some
software or not honestly on a
general-purpose server there's really
only four things you are you're really
interacting with you're interacting with
the network at the desk is the main
memory and the CPU it should be easier
to monitor things like this from an
external point of view I think an
operating season like Solaris had DTrace
where it was quite easy but not easy but
it was very well instrumented for the
for detecting these kind of things but I
think it is possible on relax as well
down with system tab but again there's
not a lot of awareness about it when I
mentioned about the instability in the
first version of our system caused by
message passing I see this I see this
often in a lot of Erlang systems that
you know masa passing is great when you
first encounter it you can just right I
mean I think when I attended the first
training course for Francesco a long
time ago we had a database server
running in a bash I don't know a couple
of hours Hey look I can I can write a
database but but actually when you start
to scale it up and you want to use
message passing an anger what I found at
least is that unless you're really
careful you can easily kill yourself
with it because it's so powerful but
there's no way to look as a as a
receiver of a message there is no way to
there are no mechanisms to control it
any process can stick a message in your
mailbox and if that mailbox grows big
enough your processes about doomed so
what we did in if it you're just using
plain Erlang Gordon it's nothing fancy
we I wrote a mailbox process for every
process which it sort of gave what a
similar active
once semantics on a TCP socket and the
really surprising thing was actually
there's no improvement in performance
show but actually there was no
degradation in it which is which speaks
volumes about her lungs capability of
dealing with massive number of processes
so and and the way it was was that so
every worker process which wanted this
kind of active once mailbox what it
would do is it spawn a mailbox process
and store it spit in it yes and anyone
who wants to send it a message would
look up the test they will get the paid
and send it the message send the mailbox
process the message now the mailbox
process just accumulates in a queue data
structure in its state and every time
the work process actually said okay you
know I'm done with this give me the next
one it would just call similar to how
you do it on a TCP socket I I thought
the performance would suck well actually
it made no difference at all it just
it's used of a bit more CPU but actually
just carried on and this and this worker
process is one of those hundred and
thirty thousand processes which means I
actually had two hundred and sixty
thousand processes just dealing with
these nodes and not counting the process
per connection towards the client so so
the results of finally after we fixed
all the issues so this is this graph is
showing the latency of
big I mean latency is important to us
because any update say our traders are
doing you know like changing the odds
for a bet we wanted to get it out to the
client a squid clients as quickly as
possible so they're so they see those
changes so this graph is showing latency
for messages for a hundred thousand
lines connected at one times our
production route that is basically the
production load we were experiencing at
that time so and that's on a log scale
so you can see the vast majority of it
war are you know well within 100
millisecond range this the the regular
spikes you see are
we basically had log data for to our
windows we were replaying a to our
window of a busy Saturday I I come from
the telecoms world where you know you
you dimension your network in a way that
you can sort of gracefully deal with New
Year's Eve spikes and Christmas spikes
in bedding companies you got a Christmas
every Saturday because Saturday
afternoon is the time when all the
sports kicking off football is there and
it is good traffic just goes through the
roof so just spreading that out a bit
just to show you really what you know
more fine-grained view of that latency
that those spikes you see our base
because we we have a two-hour window of
data every two hours the the system
which is simulating the data source
drops its connection which means and
then a it's just an oddity of our
implementation that what it first does
is it when it first connects it it gives
a shared load of data and that again we
are able to read the data faster off the
socket then we can actually process it
and that's where you see these spikes of
latency because all this is accumulating
in the ETS table and the decoding
process is slowly catching up but at
least it doesn't cause instability in
the system because ETS is acting as a
buffer for the data and we don't have
processes building up message queues
tried the same test with under tank line
but four times production Lord and
actually the okay so you've got a longer
window where you know you see that big
10-second no more than 10 seconds
latency much more than 10 second ladies
over a period of several minutes because
it's trying to catch up or actually the
the system runs with very repeatable
pattern okay you know god is big spike
I'll slowly chug through it but then the
rest of it
again the latency is back to within 100
milliseconds so it just goes to show
that you know it doesn't kill the system
it goes slow but that's the best you can
do under under a massive spike that you
don't die but it just handle it
gracefully
so again bottlenecks within this within
the system like I showed in the graph
earlier the queues build up at very high
Lord but that's there really as a buffer
to absorb these these spikes their only
way really we I could think of to solve
that is by changing the protocol we have
between our data pump because it's kind
of it's not very efficient but that's
legacy reasons you know it's and at a
high load some of these processes
actually become quite hot in terms of
because they're actually doing quite a
lot they did they're handling all the
messages coming in they're maintaining a
data tree internally and there then you
know each of these client processes they
could all be subscribing to a single one
they're saying actually I'm interested
in that and you've got thousands of
these saying I want that so some of
these can become quite hard and but the
nature of the problem is we can't make
it anymore
we can't throw more processes at it we
have sort of putting some hacks but yeah
it's without changing our end-to-end
protocol it's sort of hard so yeah I
guess the more interesting is word to
describe some of the work in progress we
have so I'm working on something called
what I call a distributed data grid I
did it it came about because of request
from one of our development team because
basically he wanted I know if you're
familiar with something all infini span
I think it's a java world it's basically
a
it's a map data structure in memory but
it takes care of replicating it to a
basically n number of replicas I have a
long history of using amnesia and I
really like it but it has its issues its
back-end isn't great it doesn't scale up
very well and it it really doesn't do
deal well with partition networks so I'm
trying to build something which which is
amnesia alike but planning to use a you
know DB as a storage back-end yesterday
there was a talk about using leveldb I
think both are good candidates and yeah
and try to come and find a way to solve
this how to deal with partition networks
I think that the talk Bernard sorry I
got your name correctly mentioned
earlier about primer there was some idea
actually a lot of things I thought about
there was a lot of overlap between what
you're trying to do in primer and what I
have in mind with this so it'll be
interesting to see what all we can steal
us above bounded message queues again
this is a long term itch of mine I think
I back in 2003 or four on the mailing
list I said you know one of my systems
went out of control the process is
because the message queue build up can
we have an option where I can say this
process should have a maximum message
queue size of say a hundred or a
thousand and I got told off by all their
langu Roo saying no that's just bad
design go back and think about how
you're designing your stuff but I still
think it's a good idea I did thinking of
thinking a little bit about it it seems
fairly straightforward do even that you
know a passing an option and you know
there's a couple of bits you have to
modify and you know you should just be
able to kill the process if it exceeds
the maximum configured limit we can't
throw a message because that would
violate Lang semantics but kill
processes fair game right that's what
he's supposed to do in Erlang so that's
something we're looking at the other
thing we're looking at is yeah but if
you think about it if you if you look at
a system which which sort of becomes
unstable because it has built up a
message queue most often the only way to
recover out of that is either restart in
order kill the process so sorry yeah I
mean flush the queue okay how do you
flush the Goom the reason the queue is
built up is because the process is not
able to fast process it fast enough
so if you if by flush the queue you mean
throw away the messages well you come
it's the same thing do you kill the
process well it's still the same thing
you're still throwing away messages
other than taking traffic away from the
system so you route traffic away from it
so the process has a chance to catch up
but sometimes the message queue is built
up because the process is part of doing
work with that message is calling maybe
an external system or calling another
process if doing anything non-trivial
there is no easy way to you know make
that process recover
yeah yeah I mean that is true you could
do that but again sometimes you can't
have a pool of processes purely because
of the nature of the work we do you got
to do it sequentially like in our case
the messages which are coming out have
to be have to be held
processed sequentially before they can
be found out into that tree sometimes
there's that dependency I mean my point
about having an option like that is
sometimes you put a system into
production where you haven't foreseen
these message keeps building up and
typically what happens is if when such a
thing happens it's it takes a lot of
manual intervention to recover that
system back into a working state and it
dies a slow painful death because
actually you've got a lot of RAM on the
box of the message you can build up you
know with our before anyone has noticed
it
yeah
I agree I agree I mean which is what I
did
what we did in the end but but my my
reason for how one thinks is an option
is more to make it easier to detect it
and recover faster from it
you know sometimes systems are built
without thinking about that back
pressure or someone hasn't foreseen that
that kind of Lord can happen yeah but
that's that's a lot of extra yeah yeah I
mean I know you're right it is an option
but it just seems cleaner if the beam
process can't say you die if you you
know you're not allowed to exceed why
not why are you not using the ETS buffer
as a full FIFO queue so rather than
getting the message we'll just being
feed by themselves from the buffer I
just took it the queue in the queue
rather than trying to kill it if the
message queue is exit the max just put
everything in it yes meaty pit yes not
only one and then and then stop when the
limiting yeah I mean that and as you've
seen that that's what we do we use a TSS
pretty much message queue but not but
but before before sending message so my
point is to stop the conquerors to stop
sending message and resilience and using
worker just pulling from multiple yes
each time I'm sorry I don't maybe we can
discuss it later
but yeah I mean you know all the points
you've raised a valid but um the thing
is you know we we can be aware of all
these safeguards and workarounds and
things but sometimes system is going to
put
and where these things haven't been
total for someone hasn't foreseen it and
my view is that actually if a system is
going to die I'd much rather it dies
quickly so that when it records it
requires in a clean state what the worst
thing is something which just takes time
to you know a slow painful that it you
know I wouldn't wish it on anyone not my
processes deep packet inspection again
this is we were trying to understand the
kind of traffic patterns we are getting
into our network in real time because
that's quite useful to us again you know
obviously this can quite easily be done
and see you can do wire speed packet
processing all that but actually for
some of the things we want to understand
it's much easier to express that in in
Erlang code rather than having to write
it in C so we we're trying to our nd
team is looking at a way of taking the
firehose of traffic coming into our
network but then doing some deep packet
inspection on it and we'd love to do it
in that long but again it's we have to
see if we can keep up with those message
rates yeah I mean speed will be an issue
so yeah again message from my employer
as the Ken said early and come work with
us because doing lots of interesting
stuff with our line open for any
questions we have three minutes
yeah I mean I think
yeah exactly yeah yeah which is ya know
they said it would just work yeah yeah
but I think you know if that is if you
know we're gonna see more and more cells
with more and more memory I think it'd
probably be good if the beam could
actually natively use huge pages that
would be a huge boost
especially for yeah I mean especially if
you go if you're gonna you're gonna have
in-memory database which cache a lot of
data then it will be very useful
yeah that's the thing it's it's not at
all obvious that it's doing something
so sorryi saying that as of r16 the BBM
already right
okay right yeah I made it
it's good to hear that you know the
others have also observed this again it
was a that was the first time we had
encountered it okay good okay you can
continue the discussion right after
because we will miss the alongs thank
you thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>