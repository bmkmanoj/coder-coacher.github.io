<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Erlang Factory 2014 -- Eliminating Single Process Bottlenecks with ETS Concurrency Patterns | Coder Coacher - Coaching Coders</title><meta content="Erlang Factory 2014 -- Eliminating Single Process Bottlenecks with ETS Concurrency Patterns - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Erlang Factory 2014 -- Eliminating Single Process Bottlenecks with ETS Concurrency Patterns</b></h2><h5 class="post__date">2014-04-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/XrkY9WRY8p0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">what I'm trying to do is come up with a
library that encourages a certain style
of programming and OTP encourages a
different style of programming and this
will show some of the differences there
so in the discussion watch for patterns
because I want to emphasize the patterns
in an architecture and it's not so much
I'm not gonna be showing you code I'm
not gonna be showing you details you can
look it up in the manuals and and figure
it out for yourself you're gonna learn
more by coming up with a little idea and
trying it out but what I am going to
show you is the design components and
the considerations that make it
important to use ETS properly to get the
concurrency you're looking for so look
for cooperating sets of OTP components
and this is typical you know you'll have
a supervisor with a bunch of servers and
things like that that that's what I
talked about as a cooperating set of OTP
components but then also look for
patterns where I'm replacing a series of
OTP components with data that's
accessible from concurrent data
structures instead of in the layout the
way OTP pushes you and and here
everything is really related to doing
high-volume with multi-core because if
you're not on multi-core you're not
going to get the concurrency benefits as
much you'll still get it because the
Erlang first scheduler is very good and
very efficient but you're just
simulating concurrency at that point
when you have multi-core you actually
get real concurrency happening and and
if your environment doesn't have the
kind of hardware or the kind of problem
scale then maybe you won't see a
difference so the first thing Oh T P is
a library which encourages a style of
programming which is supposed to avoid
bugs be resilient and make things easier
for the for the programmer it says it's
a communicating sequential processes if
you know from ciear horror CSPs was
something big a couple decades ago
there's many processes
each process is independent and they all
have their own memory space stack
control they have their own garbage
collector that's what's different from
Java Java has multi-threaded garbage
collector erling there's a garbage
collector per process that's single
threaded and every process is single
threaded and so now you can have
concurrency with different processes but
they're easier to reason about because
you're just doing single threaded
programming within one process and then
you use processes to manage and control
how the concurrency works and how your
memory is being used and allocated
there's also a process dictionary which
is a key value store that's inside each
process that you can use although
everybody who has been doing or laying
an OTP for a long time says don't use
that but there are circumstances when
it's very useful to use and ETS is a way
to do the same thing that process
dictionary does in a safer although in
some ways not a safe way so we'll talk
about some of that and then there's a
message mailbox in a queue if you send a
message from one process to another
process that's the only real way of
communicating information and the
messages arrive first in first out in a
queue and so a process has a list of
messages sitting in there and it it
handles them one at a time so here's
some of the patterns that OTP encourages
supervisors everybody says they're great
you know your process goes down the
supervisor brings it back up you can
manage resiliency and fail fault
tolerance that way
but the pattern creates this supervisor
that's managing multiple children and
that that can become a single process
bottleneck now fortunately most of the
time supervisors don't do anything
except restart you don't send messages
to them and have them distribute it to
the children and things like that but if
you get ten thousand hundred thousand
workers under a single supervisor you
can actually have latency problems and
issues with restarts and things like
that so you know when you're talking
about highly concurrent and and very
active systems that becomes an issue the
other big concept is servers you got
Jen servers Jen FSMs and and Jen events
and things like that where you're
handling messages and then maybe
distributing an amount to other
processes so here's an example where
multiple workers are sending messages to
a single process which then distributes
in mouth maybe this is a router and it
filters based on type and then multiple
processes can contribute from many
sources and you can categorize and route
the information the the nice thing about
using Jen server like approach is that
you get automatic serialization there is
an ordering the order that the messages
arrive is the order that they'll be
processed each message is processed one
at a time they're independent
transactions so you get isolation
between data that's flowing through the
Jen server but this Jen server becomes a
bottleneck and if you've been around
you've probably heard of Jen server 2
which was a hack to get around the
problem when you have a really long
mailbox and you can't catch up once you
fall behind it's hard to catch up
because you're sequentially pulling one
message at a time Jen servitor would
dump all of it into the memory of the
process and then then you have to write
your own cueing and messaging and you
know it gets to be a nightmare but it
sped things up a little bit but you
still have this single process
bottleneck it's a good thing because it
encourages a style of programming that's
easy to understand easy to trace
easy to to eliminate bugs but it leads
to situations in volume where there's a
there's a bottleneck with servers do
organize the compensation with the
computation and they make it easier to
reason about things but multi-process
join is really just a wait you know when
when multiple processes are sending
messages and they're waiting for a reply
back nothing's happening and that's
where the bottleneck really kills you
especially if your message queue starts
to fill out and and those end up
limiting how much work you can do and
choke a heavy system and and and one of
the characteristics that you'll see and
I have
system is you'll start to get timeouts
the Gen server has a fixed five-second
timeout if the message queue is really
big and things aren't being handled and
you're waiting on another process to
send a reply before you can handle your
next message
you get a timeout and then it propagates
through your system so the other thing
about Erlang and and what Oh T P takes
advantage of is that all the data
structures all the all the algorithms
and libraries are single process data
structures if you use a dictionary or an
or dict or lists or Jen the trees G B
trees those are all single process in
the space of the process that's
executing data structures so you can't
get any concurrency out of those one
processes dealing with the memory
structure binaries are sort of the one
caveat to that if you have a binary
that's bigger than 32 bytes or 64 bytes
depends on your your architecture your
compile for then the binary data is
actually stored in a shared Heat and
there's just a reference in the process
space so now multiple processes can
actually point at the same binary and
you can get concurrency working on the
same data so if you're using let's say a
socket to receive data and you receive
it as a binary multiple processes can be
analyzing and looking at patterns in
that binary at the same time but that's
really a optimization of memory and not
a concurrency mechanism because if you
if you wanted to signal to the other
processes you'd somehow have to change
that binary and that's not really what
you do with miners and at that point as
soon as you change it you've got a
different binary and then the other
processes can't see it until they are
sent a message with that binary in it
and so you kill the the concurrency by
sending a message Erlang term storage
was a data structure it was brought
about to make it possible to handle
large amounts of data in a key value
type store just like the process
dictionary but to avoid garbage
collection because if you let's say you
had the
phonebook inside a single process in the
process dictionary you can get very fast
lookup but every once in a while the
garbage collector is going to run and it
has to walk through the entire
dictionary to see if any of those
references or things that could be
garbage collected or not and that pause
occasionally caused problems so when
they were originally doing telephone
routing and they wanted to do real-time
lookups they needed a way to look up
data in a large data structure that
doesn't get garbage collected because
it's static and it's not going to change
very often and then you can avoid those
pauses yeah one of the side benefits was
by putting the ETS data structure
outside the process it could then be
accessed by multiple processes and once
multi-core came around that turned to be
an advantage that ETS had that I don't
know if it was originally vision but it
turned out to be a nice big benefit of
ETS so ETS stands for Erlang term
storage it's a key value store of our
long term z' tuples are what go in and
it's the one truly concurrent cross
process data structure that's that's an
early it's part of OTP is part of the VM
you know it is built as an OTP component
as part of the OTP library so when I say
OTP encourages all these things that's
in the architectural language of here's
how you build a server and supervisors
and so on but ETS is a data structure
tool that you can use in other ways it
lives in the separate memory like I said
and that's allows other processes to
access it and it also avoids the garbage
collector of a single process running
through it it's really an in-memory data
structure that's implemented in C so
it's it's on par with bifs in terms of C
access to data structures and the owner
of an ETS table is the process that
creates it by default you can assign
that by giving it away but in general
this is a key point to remember because
if the owner of the process dies
the table will be removed automatically
right out from under you so you better
be careful what process creates the the
ETS table if you need that ETS table to
be around for other processes and this
that's where it gets tricky normally
when you're let's say you're dealing
with the process dictionary and some
data structures inside one process you
don't have to worry about any other
process and that's the OTP encouraging
way of writing things in a single
process but if you start to use
concurrency and things are shared now
you have to think about what does it
mean for it to be shared and what is the
lifetime of that data object because the
lifetime of that data object has to be
greater than the lifetime of the other
processes that access it and and the big
win is avoiding the garbage collector
it's really good for for really large
data sets because the garbage collector
won't visit it I was going to draw a
picture that showed locking of tables
and rows and things but it's just too
hard to draw that stuff all at the same
time so this is a basic conceptually
when an Erlang term storage is like each
row is a tuple you could make a record
and that's the most efficient usage of
of them where the record name and the
key field in the record and then your
other data in this case I have messages
with time stamps and sizes and then this
is the indexed field in a key value
store normally you have a key and then
you have a value the way ETS does it you
have a tuple which is the entire value
structure and the key is contained in
one of the fields of the tuple and so
that you identify which field is the
indexed field on that and you can make
that a compound field that can be a
tuple itself or another record or
anything like that and we'll get into
that later I'll talk about key
partitioning is an important way of
using that feature so there's there's
locking that's done by the VM in the C
code that manages the ets table data
memory and normally by default one
writer is allowed to go into an 80s
table and so that the table will be
locked somebody writes and then
then you're out again and it's it's
quick it's a very fast thing it's on the
order of microseconds to go into an 80s
table so it's not it's not that heavy if
you have lots of writers though you want
to use even if you have you know half a
dozen writers you want to use write
concurrency if you're doing a lot of
writing and you want them to access
concurrently and when you when you
enable the write concurrency flag you
should look you know after this
discussion if you're interested in ets
and using ets go to the documentation
and the ericsson that's one page just
read the whole thing it doesn't take
long it's 15-20 minutes to read through
it and think about it it tells you a lot
of little details about locking and so
on and it's really you can code directly
from that and and Jen up your own
example but when you do write
concurrency if you have multiple writers
it will do optimize the data structure
so that you can have concurrent access
from multi-core and get much better
performance and there's been you know
it's it's it's a significant improvement
it can be up to 10 times faster when
you're when you're using multi course so
having write concurrency is an important
thing and also not switching between
read and write like if every other
requests one's a read request then a
break write request with multiple
readers and writers you will get less
efficiency than if you had bursts of
writes and then bursts of reads and if
you have big blocks of those then you
want to use read concurrency and write
concurrency at the same time but in the
documentation on the arocs and built-in
documentation it it talks about that
stuff sure yeah
now it I'll get into very specifically
later
what's atomic what's isolated and what's
not because it depends on what you're
doing and reading and so on you can end
up doing non-atomic things but in
general when you're writing to ETS if
you write a tuple that's atomic its
isolated nothing else sees intermediate
results and and it completes and there
are
insert commands where you can insert
multiple objects and those are atomic
and isolated too so we use that to
advantage when we're doing concurrency
yeah it's the only way to make
concurrency work is to is to do atomic
write type operations you do get read
and write locks for each tuple so if you
have you know 16 cores and and you're
accessing 16 completely different
records you could do that completely
simultaneously and you get real benefits
on multi-core doing that when you do
access ETS though it's not residing in
your process space it's residing in the
ETS separate allocated process space by
the VM so if I access something my key
and say give me the record structure it
actually has to copy that record
structure into my process space so every
time you access for a read there is a
copy but it still is on the order of
microseconds for that sort of thing to
happen it's really fast if the data
structure is this huge tree of list
pointers then it's gonna have to copy
that stuff but if it's pointers to
binaries which are residing in the heap
then it just copies the pointers and you
get a top-level copy of the record
structure so it's very fast and
efficient for records being stored in
ETS and now that we're moving off of
Records to maps and we'll have to
rethink how we use ETS in that way but
there's caveats though when you use ETS
concurrency first of all you have to
know what are the atomic operations
because you can't read a value let's say
I have a counter or I'm keeping in the
ETS table and it's currently at 250 so I
read it and I said oh it's 250
let me just bump that so I'll make it
251 and I'll go store it meanwhile
another process read 250 and bumped it
already and somebody else read 251 and
bumped it and now I go right to 51 and
it was supposed to be 253 so you can't
read and then write that doesn't work in
concurrent access there are two atomic
operations that come with ETS update
counter and update element an update
counter does that bumping without
reading it so it will go into ETS and
internal the ETS it reads the value
increments it and then stores it and
then returns to you the newly stored
value that all is guaranteed to be
atomic and isolated inside the C code
watch out for read plus update is just
not concurrence safe anywhere you see
code that reads an ets table and then
even the very next line writes to it not
concurrent say if it's gonna fail one
day and if you have a high-volume system
it's gonna fail in about five minutes
it's not going to happen tomorrow if you
use fold select first next those are all
iterators they do read read read they're
doing read one record and then a little
bit later read another record and
anything can happen in between so those
are not safe and and always always try
to be aware of who owns the table there
are functions to manage ownership at the
table but they get complicated to use
they complicate your code and I haven't
found a real good benefit with them yet
so I use a simpler technique that I'll
describe later so here's some
concurrency strategies now I'm going to
be talking about architectural
considerations patterns to look for in
your data or in your system that you can
use but you have to think about how you
represent them in the data so that you
get the most benefit from using ATS and
these are this the the things I'm going
to talk about right here are
partitioning strategies so if I use
let's say I use two ETS tables I can
delete one table and that's a
partitioning strategy where I can manage
the memory and manage the objects very
easily but I also talked about
partitioning in other ways too so so
first of all having two tables or having
the owner of a table I can delete the
table or I can kill the owner both of
those will result in the entire table
getting deleted and that's a that's a
manual garbage collection technique if
you know you have a bunch of
data you can put it aside create a new
structure put some data in it and then
delete all the old data in one fell
swoop
splitting read and write this was a
trick that Fred Ebert used in discount
you have a ETS table that's only
writable and you have another ETS table
that's only readable and you can have
the same data in both of them now you
get full read concurrency full right
concurrency things slow down as soon as
you switch context between read and
write in his case the data that was
being read was not the data that was
being written because I see puzzled
looks like that doesn't work this is
about partitioning so you have to think
about your data space and think about
how to partition it in his case he was
limiting who could read the data how
many people could read it so he
incremented a counter in the right only
table if the counter was at zero and you
incremented it you got back 1 you're
good to go you can read it if you
increment it and got anything other than
1 you can't read that other table that's
cooperative partitioning that allows
concurrency without locking it's it's a
form of locking but it's like parallel
locking and if I was gonna allow 5
people to read simultaneously I could
any number less than 5 works as soon as
you get 6 you can't read but you have to
agree to follow the rules of the
architecture and not read that table if
I say you don't get the chance to read
it
so the but look for patterns in your
data that allow you to separate
read-only data from write data and try
and optimize that so that you can get
the benefit of the concurrency and the
best way possible as soon as you start
switching context you know I write 5
records and then I switch to reading and
I read 5 records it has to lock the
table and as soon as you go through a
lock that's a single point of contention
we got rid of the single process and now
we've got a single lock and you know in
the last two years
Erlang now has been the big bone of
contention that Earling now is
guaranteed to return a unique value
every single time and that means it has
to lock increment and
and if you have high concurrency where
your time stamping things that's the
bottleneck that lock and there's nothing
you can do about it except don't call
Erlang now or the VM eventually come up
with another way to guarantee that but
here we want to partition our data and
shardene as another technique people use
with with databases you might see this
with Redis or things like that where the
key range defines where data is stored
on which shard and that way you get less
contention and fewer hotspots because
you can partition it based on the
statistics in your data or whatever you
know values you can use for the ranges
and what this means is when you're
looking at the data you may want to be
very particular and discerning when you
choose what values to give your keys we
started out at one point just random uu
IDs for things well much later it's like
it would have been nice if the date
component even if it was obscured was
first so that we could get rid of a
week's worth of data just by looking at
prefixes or partitioning the key space
and eliminating things so partitioning
is a very important piece that you need
to consider early on because it not only
affects how concurrency works in your
system but also how you can manage data
and how you can speed up algorithms by
having implicit knowledge in your keys
that you don't have to run algorithms to
decode things and figure out another way
to partition which you don't see as
often is a cascading to create tree
structures so
shardene is just a flat list partitioned
and you know one dimension but you could
use a decision tree and have a series of
ets tables and at the top-level point to
which ets table on each level of your
data decision and so you could have the
key like a radix sort going down and
partitioning to a particular table and
then having a sub set of data that way
and then the other big one if you're
really going for concurrency is to use
the pid' as part of the key
so that you know only my PIN will access
that row and so I'll get complete
concurrency across all my data as pit as
part of it if your problem fits the
model of how you generate your processes
so now another concurrency strategy is
the data access if you use update
counter and update element on shared
Keys then you're gonna do atomic
operations and you get maximal
concurrency there but guaranteed safety
on the on the values that are in it and
if you were you know like Fred's Fred's
trick with discount where he was
reserving things by locking them and
incrementing by one sometimes you want
to know what the current value is but
you don't want it to change but you
don't want to switch from a write only
table to read and find out what the
value is
you can increment the value by zero and
it'll tell you what the result is after
you add nothing to the current value and
then you can stay in write mode and you
get full concurrency and you don't have
to do the context switch that's a big
trick there's so few atomic operations
and ets tables that you need every
little trick that's available to make it
work
another one is separating metadata from
data so you might be in the same ets
table and have a metadata record with
pointers to other data and then you can
have separate processes accessing the
data they may have to contend for the
metadata record but once they find out
where they can work then they're
independent in different partitions of
the data space so this is all kind of
abstract because we're talking about key
space and partitions within one data
structure that's a flat array of or
lists of elements so I tried to make
some pictures at some points but it's
some of it you have to actually go into
some of the key literature and play
around with your own projects to
understand what it means to partition a
key space in general and are laying the
easiest way to partition a key space is
to use a tuple for your key and then you
can put a series of fields that are in
order that
you want them to be and then it's pretty
clear how it's partitioned and I use
that technique with metadata
I'll just put tuple meta and then my
value and then in my data I'll have to
pull data and then the key value and
their partition that way public tables
when you make an ATS table though I
default it's protected that means the
owner can write to it everybody else can
read from it if you make it private only
the owner can read and write from it and
if you make it public anybody can read
from it and anybody can write to it so
if you want concurrency and you need to
write to the table you have one choice
public tables and now you have to use
cooperative architectural respect the
boundaries rules anybody can write to
the table if you don't follow the
architecture pattern you could just mess
up the data and nothing works so it's
really an important thing to have an
architectural policy and to make a
library that puts an API that enforces
an interface and tell people go through
the interface don't just access the ETS
table that said the other side of it is
monitoring and debugging and managing
your system it's very sweet to go into a
production system not touch anything and
peek into an ETS table that has scratch
data about what processes are doing and
what stage they're at and how much
progress they made how long they've been
running and when they started all that
stuff you can just access directly in an
ATS table that's public and not have to
touch any of your code and it gives a
lot of good insight into what's going on
and it can all be done concurrently by
by the running system so now if you want
to design an ETS solution that avoids
single process bottlenecks we're going
to talk about some of the patterns that
you use supervised processes so the big
thing is knowing who's the owner of the
ETS table the simplest way is to have
either soup
vizor although that's kind of not
recommended to do extra stuff in a
supervisor or a dedicated gen server
that owns the process and you want that
gen server to create the ets table
before anybody accesses it and you want
that gen server to live longer than
anybody else that does access it so you
would create the ETF's table then you
would spawn your workers and then you
would access the ETS table from there at
any time if the owner died and the ETS
table went away you would want all the
workers to go away otherwise you're
going to get crashes on those workers
because the ETS table is gone and it'll
return bad arg because the table doesn't
exist so don't ever just oh I need an
ETS table let me just put it in this
function put it in API library anybody
can call it because the first thing that
happens is somebody calls it in a
transient process and then it goes away
like a second later and it can't figure
out why your library doesn't work and
the code isn't running whenever they
start up a worker you have to know who's
the owner and guarantee that that owner
is there longer than everybody else and
yeah I've played around with the
inheritance thing it it works it's a way
to give away a table and take it back if
the owner the the typical thing is you
start up you create an ETS process and
you immediately give it away to somebody
else with the inheritance set so that if
they die they give it back to you and
that way the table doesn't die and
processes can die all around but you
have to write all this code that looks
at info messages and says oh you're
getting a table back you better take it
oh I better give it to somebody else
right away before I crash but it's a
bunch of code that doesn't really do
anything and you don't get much benefit
of it the only the only way I could see
inheritance working is if you spawned
multiple processes that each did like
some long computation and filled up some
ETS table and then they died and a
central server receives the ETS table as
a scratch pad and merges it with all the
other ones as they come in but
I haven't come up with a good pattern
for the inheritance thing but if you use
a supervisor and you do the Gen Server
trick and you make that supervisor rest
for one rest for one policy says if this
child dies everybody to the right of it
and the supervisor tree must die now and
then you start up again in order then
you'll get a good a good result so
here's here's the pattern you start with
a supervisor
that's a rest for one supervisor the
first thing it does is it creates the
gen server that's going to own the 80s
table in there NIT which it has to be a
synchronous init the supervisor and this
is then this and it's the ETS table
before the next worker gets built after
the ETS table is built and the
supervisor creates the other workers
then you build more supervisors your
system starts up you could do this in a
start phase although a start phase only
runs on startup so you might want
another mechanism if you've crashed and
you're now going through start phases
but it step five after the system is
stabilized in startup you send in a
signal to the workers okay start working
now everything's ready for you
and then they access the ETS table if if
if this guy dies and the ETS table goes
away the rest for one kills everybody
rebuilds in the same order and the
workers don't start until they get the
signal at Step five again you want that
consistent stable start process to make
sure that concurrency doesn't bite you
like a school of piranhas that's what
happens when you when you launch a bunch
of workers they immediately start
working as soon as they get time and if
you're not ready for them it's not going
to work the other technique that you
need to know about is using the atomic
operations
first there's table level operations and
in some of my patterns later I'll show
you this if you create a new 80s table
or you rename an 80s table that's the
top
so you could I've never done this but
you could just like create and destroy
ETS tables as a signal to somebody and
they could just check if the ETS table
exists and you could like send Morse
code across your concurrent system
sometimes you mean the lightest simplest
way to get things distribute it out and
that's highly concurrent way of doing it
insert and insert new atomically create
objects in the ATS table so I can say
make a thousand objects and insert all
of them in one operation and that's an
atomic operation all thousand will be
isolated and atomically created inside
the ETS table so one of the tricks is
you know building things to the side and
then saying here put all this in now and
then deleting is also atomic so you can
use that for garbage collection if you
partition your data or use a time basis
for rolling data from one table to
another and then just delete a table and
mass it's the quickest way to garbage
collect a whole bunch of data the other
quickest way is to kill a process that's
how you normally garbage collect manual
is you kill the process that's using all
the memory so if you use a atomic update
counter it only works if you've got a
table that's of type set or of ordered
set if you've got bags and things like
that duplicate bag it doesn't you can't
use this operation so and it will just
return bad argh if you try to do it on
the wrong type of table so so you have
to already decided that you want
asset-based key value store or an
ordered set and that atomic update
counter only works on an integer field
you don't have to increment by one you
can increment by any number including
negative numbers so you can think
increase or reduce the value but you
can't increment the key because it's
already indexed and it's in the data
static structure for fast access so you
can't increment that you'll get about
hour
if you do that - don't read from the ETS
table and then use update kind of like
it's no point the point of update
counter is to read an update and tell
you what the result is after it's
updated it won't tell you before
it'll only tell you after so you won't
know if it wrapped or anything like that
the update counter lets you do a warping
of indexes that's kind of what it's set
up for so it you can specify increment
by three but if it's more than 20 set it
to zero so you can do like bring buffers
and things like that very easily it
wraps around and you can do it and in a
negative way - when you subtract it
looks at the men and can wrap the value
and you can use that to your advantage
and do something really strange where
like if it goes over 20 just return
6,000 just so that you get a signal that
hey I've exceeded my max I better and
now it's set you know to this really
high number there are no conditionals
that like the only conditional is did it
go over the max or under the min you
can't say increment this value and if
it's greater than the column next to it
send the flag back or something you know
there's there's nothing like that so it
gets tricky to do anything really useful
with update counter besides having
arrays and pointers and counts and
things like that now update element is
also atomic update element lets you just
take a sledgehammer and smash whatever
values in there and put a new value in
and you can use use it to put in
multiple values you don't have to read
and set in fact you can't read and set
you have to know that this is the value
I want I don't care what value is there
now just kill that value but instead of
reading the entire object pulling out
one field changing it and then putting
everything back the way it was and
changing that one field which won't work
because somebody else changed all the
other columns in the meantime and then
you reset all their changes if you use
update element you can change one column
and other people will change other
columns
be able to get there too so what you
really need to do is use reserve write
and publish semantics so by using update
counter you can reserve some space use
it as a working area write data into it
and then publish it when you're done in
an atomic operation and that's really
how you get concurrency to work for you
I'm gonna speed up a little bit because
I'm running a little short there's a lot
of stuff in here
taketake the slides and and study them
later I want to get to the library that
has patterns in it key partitioning if
you use key partitioning properly that's
a good way to set aside the work area so
when you do reserve and then use that as
your working area that's in a key
partition space that won't be overridden
by other things then you can publish and
commit and if you do in the working area
create multiple objects then you can
grab them all with a long select because
they're in the working area but then
insert the entire set in one operation
with insert new into the publish area so
the atomic operation happens atomically
I mean the publish operation is atomic
but your working area can be more
involved so here's here's an example of
how you do it you you reserve space the
worker will say I need some space it
reserves it and then another worker
comes in and does some updates over time
and then when they're done they signal
to the first worker that it's complete
it queries the reserved area inserts it
with a single atomic operation into the
publish area and then you free up this
this working area space this is a single
ets table where the key space is
partitioned watch out for non atomic
operations I've said that multiple times
but multiple object deletes initializing
the table next and fold tab to list tab
to file anytime you see somebody read
and then write or write and it's it's
it's not concurrent so then I have a
library on github called epoxy or Lang
patterns of concurrency
it's OTP compatible we've been running
it at Tiger Tech since August it it
implements concurrency controls and and
mechanisms using ETS hides the
complexity it's it's complicated to make
it really work well with the concurrency
so here's an example of a problem you
have a firehose of data just a ton of
data coming at you we created initially
we created a ring buffer because we were
trying to record timing on thousands of
processes that are all running at the
same time and as soon as they and they
record they they record their timing in
the ring buffer it's highly concurrent
it's just a buffer that's done as an
array using update counter to reserve
space when you write to it a reserved
space and then publishes into the
reserved area I did metadata key space
partitioning so the key is meta and the
type of concurrency that you're running
and then in the data area that type of
concurrency has an array index and so
you can reserve spaces and write to them
that way and and the ETS table can
become a hotspot so I have an option so
that you can have dedicated 80s tables
as well as a centralized 80s table the
the buffer implementation uses this
reserved publish so here's an example of
how this works you you reserve a slot in
the array the reserved pointer gets
bumped and then the right pointer can be
used to write into that reserved area
meanwhile there's a separate read
pointer that's pointing to where you can
read from after you've written the data
the right pointer gets bumped and the
data is now reserved and written and can
be read by the reader now the tricky bit
in the library is that the reader is
concurrently reading a head and multiple
writers may have reserved and empty
holes could occur and theta can be
written after the holes so the reader
actually has to retry and yield
so if you get to the github repo you can
look inside ETS buffer and see how that
works it's kind of tricky but it's
commented and spelled out if you run it
for a long time it'll use big number at
array so it'll go on infinitely and get
slower with bigger gnomes and the next
thing that we had trouble with was
concurrency how many people actually
have a system where they decided oh this
can happen in the background I'll just
spawn a process and you do it and you do
it and eventually you find out oh my god
there's a million processes remaining
you need a way to cap that I call that
unbridled concurrency when you just spun
whenever you feel like it you can exceed
the CPU spikes can kill your VM these
are all kinds of problems and you end up
with single process bottlenecks when all
these things fail if you're using a
worker pool for that this is what it
should look like more controlled
carrying a big weight with all of them
at the same time bridled concurrency we
just have a max count in the 80s table
every time you try to spawn a process it
uses update counter and checks for the
type of process you're spawning whether
you're reaching your maximum concurrency
limit if it reaches that limit then we
run in line to give back pressure the
last object that's in the library is for
caching when you have expiration of many
many objects in a cache instead of
putting timers on every single object in
the cache we do generational caching so
we use two generations if you miss in
the newest generation it goes to the
older generation if it doesn't find it
there it goes to the database puts it in
the new generation and then when the
generation expires so here's an example
of getting something from a new
generation if it's not there it goes to
the old generation and migrates the data
into the new generation when it comes
time for a brand new generation to be
created it's empty and this old
generation is just deleted no timers
is deleted all at once and you can
expire based on clock frequency just
pulling it can expire based on how many
accesses there are to the generation or
a function that you supply and I'm out
of time so I wanted to talk about future
plans would want to put in
synchronization barriers so you can do
things like any some are also that it
will just happen concurrently and
generalizing something like discount
which is already in the concurrency
control mechanism we use something
similar higher-level patterns like
active task queues instead of worker
pools so that you can have dynamic
workers consuming from the ETS buffer
task queue and using a pipeline of
stages of ETS buffers for active queue
so you could have staged concurrent
processes for for completing it and I'm
looking for open source polls and
feedback pull requests and feedback of
concurrency patterns you're using that
you think should be generalized ets will
help you increase concurrency but you
have to understand how it works design
your keys and your access mechanisms use
this reserve right publish semantics the
atomic operations getting it rights
difficult use libraries that are
established and contribute what you
discover to those libraries because if
everybody builds it from scratch none of
it will work thanks
you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>