<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Lambda Days 2018 - Rafał Studnicki - Guaranteed SLAs with higher-order functions (...) | Coder Coacher - Coaching Coders</title><meta content="Lambda Days 2018 - Rafał Studnicki - Guaranteed SLAs with higher-order functions (...) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Lambda Days 2018 - Rafał Studnicki - Guaranteed SLAs with higher-order functions (...)</b></h2><h5 class="post__date">2018-03-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/u4KoU4ly6nM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thanks for coming I'm opposed to this
key I work a tunneling solutions as part
except and for almost two years now I've
been working I've been writing code at
Grindr mostly airing in elixir and well
I wanted to talk about some work we've
done quite recently that focuses on
meeting certain essays or actually
service level objectives to be honest
that we've done on some backing services
and well before I'll tell you how we
implemented it and if it even works if
it of this works let me just very
quickly tell you what was our motivation
and what problem we were trying to solve
but first let me ask you a question how
many of you know what Grindr is quite a
few people okay so if you don't know
what Grindr is it's well basically it's
a dating app the user base it's it's
more or less it's over slightly over
three million daily active users so
currently it's mobile mobile only so if
you want to check it out you can
download it from the App Store Play
Store but if you look at grinders back
in saggin set you would see that mostly
consists of Java services you don't need
to look at it now you'll see that it
consists mostly of the Java services
that handling that are handling clients
API calls but apart from that there are
some some other services and link some
more soft real-time things like chat or
an airline or presence in elixir and
because every single thing almost every
single thing that we do at back-end
initiated at the client and the client
is waiting for the response and at least
for me there is no more annoying thing
that this spinner spinning waiting for
the response in the app we want to make
sure that is used
that this response is generated very
quickly so that's why we focus on
observing this tail latency graphs all
the time and and trying to optimize that
and if maximum response time is below 50
milliseconds on our side that's great so
for example let's take a look at this
very very simplified diagram
architecture diagram so let's say we
have a user service user service that
handles some information about users it
has its own DB and it happens to be that
the present service is its consumer from
time to time it needs some information
about about users and so now imagine
this scenario when the user service
database user services database slows
down for any reason so that it cannot
process all these present service
requests quickly enough what can we do
then well the very simple approach the
very naive approach is to just put a
queue in front being it the user service
any queue all the requests when the
slowness disappears just process them
but the queues as any physical thing are
governed by some laws they are described
by the queueing theory but there is one
very simple law from the queueing theory
called utilization law it's very simple
it's very basic if you just divide
number at the rate of incoming requests
divided by the rate of requests in the
time you need that you can actually
handle it get the utilization right so
you you are getting 100 100 requests per
second you can handle 200 the
utilization rate is 0.5 and just think
the the load metric in the operating
system it needs to be the same unit it
doesn't matter I mean both need to be in
the same time unit so it
all right but yes so 0.5 is that we're
utilized at 50% right yeah yeah yeah
yeah so yeah as I said very simple yeah
no no right so let me just visualize
this very quickly if if we have
utilization rate 0.5 so on this graph
the purple line I mean on the x-axis
there is a two minute simulation x-axis
is time y-axis is a queue length we have
a single queue Purple Line is
utilization rate 0.5 so we can handle
twice as much twice as much request you
can see that this queue length is mostly
zero sometimes it's up to some number
but it's unloads very quickly the reason
for this to be higher than zero is that
it was sampled at different times now
let's take a look at this blue green
it's green on my screen line now the
utilization rate is 0.9 so we're coming
closer to 1 you still can see it's
stable it's slightly higher because we
do more stuff so the kulick queue length
is slightly higher but it's still stable
now if this utilization rate groves
below above 1 so if it's one point one
for example this will look like this so
the queue length will grow infinitely
with time and that's pretty intuitive
because we can queue more requests in a
time you need then we dequeue so with
time it grows so if we're unlucky and we
come to the system at the two-minute
mark we need to wait and until the
1200 requests before us will be handled
first and then our request will be
considered at all yeah yeah I mean it
was gnuplot not me
but yeah so sorry for this so yeah um
okay what was I saying
Yeah right
so from this we can learn that we cannot
really handle this overload situation
when this database slows down just we
have a queue it's an unbound queue just
to be clear well just because this when
someone comes later in each way it's a
lot a lot until a lot of requests in
front will we handle first so that's one
problem we need to solve handle the
overload somehow and the second problem
we want to solve is that this
percentiles I was talking about this
response time quality I was talking
about we need to keep it as low as
possible and as I said we don't well I
know should've I said that but the user
service from this previous example
cannot really influence this database it
doesn't control when it's slow or not it
also doesn't control doesn't influence
this consumer the present service so how
can we control this it sounds a little
bit unfair that we need to handle these
two things from this user service that
doesn't control that doesn't influence
almost anything but we can just go to
the field that's bread and butter is
just that controlling things that we
cannot influence and that field is
called control theory which is a branch
of mathematics that trusted
solve the control problem and the
control problem is given a dynamic
system which changes its output in time
what input should we apply to it so that
it generates a desired output the output
that we want and just to introduce some
more more terminology if you if you ever
see a design of control system it
consists of blocks like this that takes
some input and generate some output
looks like a function slightly kind of
not exactly because it also contains
some state in time so if we were to
implement it in elixir and also a plant
a plant is a term for the system we want
to control in the control theory just a
general name so if we were to implement
an interface that the components a
single component so a single block has
it has an init function that returns an
initial state and then there is a step
takes the initial state not initial
state the current state some input and
giant then generates new state and the
output okay why is it a step you might
ask because we'll we're in the digital
world so we cannot really input stuff
constantly
and we cannot generate the constant
output it continuous input and output we
just do it at some time intervals for
the rest of this talk assume 100
milliseconds so if we add another block
do this we can actually create a legit
control system we have a plant that we
control and we have a controller not
from MVC have a controller so it's it's
definition it's basically a component
that controls something so now the input
or system is the input to the controller
then controller takes it generate some
control action inputs to the planet plan
outputs some
and if you consider a real-world example
if you consider a real-world example of
that you can think of yourself driving a
car to work but we have a small caveat
you enter the car you start the engine
you close your eyes you drive to work
you arrive at your destination and then
you open your eyes so in this case input
is everything you know about the system
that you're going to control so all the
roads all the things that are happening
what may be on the road then then you
are the controller control actions are
your hands and legs controlling the car
and then plant is a car and then output
is the course the car took right by the
way I don't recommend checking this in
practice right if we just do one thing
why is it why is it the real-life
example real world example because in
this is something called feed-forward
loop feed forward control open-loop
control it basically needs to know the
input up front so in every single case
we pre configure something and then we
hope it will work this is example of
this another example is when you turn on
washing machine for a set program it
will run for 60 minutes no matter how
dirty the clothes are now going back to
the driving example if we just add one
more thing here and this is how in real
life driving a car looks like if we just
observe constantly what is the output so
what the course would the path would the
route the car is taking and we act
accordingly we will have a feedback loop
control so yeah this is what's happened
really because you don't close your eyes
you observe the road constantly if the
weather conditions conditions change
suddenly you react if there is there
what roadworks you react you take the
diversion so yeah
and and what what's the what's the thing
here now the input to the whole system
is is something else it's not everything
we know about the system it's a set
point so it's it's a desired value it
desired output of the system what we
want what do we want the system to be
output now the system the loop compares
this set point with the last output of
the system and then it generates an
error it as simple as subtracting them
and then this is the input to the
controller and then controller generates
control action right if you think about
some real-world examples of that
thermostat or any other air temperature
control cruise control in the car going
back to the car examples autopilot or
instrumented landing system in planes
and from the tech industry Amazon after
scaling that springs up more instances
if the latency goes up apples HTTP Live
Streaming that pushes the video with the
bitrate that's the best for your network
bandwidth ready scale fiction if you
ever use Redis if the memory if you're
low in memory or test-driven development
that's that's my favorite example ok one
second sorry cool now let's go back to
this unbound queue example and this is
something we'll try to if you remember
the problem in trying to solve is this
we want our service to one being
protected from overloads number two
being able to generate responses we've
given within a given service level
agreement or it's actually service level
objective because we don't pay any fines
directly to the customer when we miss
the deadline we just have
Wars user experience so that's why I
solo but SLA is more popular I guess so
right so now imagine this example from
with the unbound queue and we'll run
some other simulation for for the
baseline so now we have free queues
circles on the bottom there is a
dispatch on top and we generate 300
requests per second this part just sends
it to one of the queues doesn't really
matter if it's round robin or anything
else so roughly every queue gets 100
requests per second now what what what's
more every of the queues we can we can
choose what will be the service time of
a request in every single of the queues
so let me explain this now the
simulation is 60 seconds still x-axis
y-axis this time is the average response
time so this is the time between the the
request enters the system and and leaves
the system so this is the whole time
spent in the system as measured by us
and now there are four stages in this
simulation every 15 seconds and in the
first stage service time is 5
milliseconds so we have free workers
every of them can handle 200 requests
per second
so we can handle 600 requests per second
and we get 300 requests per second
coming in and the second part and you
can see actually it's also the scale
issue you don't really see the output
the purple output because it's so good
the response time is so good in the
second time in the second frame we
increase the service time by a factor of
two it's 10 milliseconds so it service
rate exactly matches the number of
incoming requests and you already see
the average response time starts to grow
because it's exactly the same as in
common
serve exactly the same as incoming great
so because of the dispatch overhead
where we actually serve is less than 300
and then in the first stage we increase
the service time by a factor of two
again now it's one service rate is 150
per second and you can see how how its
inflates the average response time and
then intersect in the fourth stage the
response time the service time goes back
again and response time is on the graph
so sorry service time is how we
configure the disk use so what I just
want you to remember from this slide
that their simulation takes 60 seconds
there are four stages it's good mostly
bad very bad good that's how what are
the properties of the underlying system
the system we depend on are in this four
stages and this is our baseline this is
naive approach and we want to improve
that that's that's the goal okay so
let's let's just think how can we do it
with the feedback control loop so you
might say right now well you know it's a
cute so let's just bound the queue and
and you'll be fine but just bounding the
queue solves only one of these
properties one of these two properties I
mentioned it only protects from
overloads it doesn't really tell you
anything about what will be the output
of the system what will be the average
response time well I said it too quickly
sorry what will be the what will be the
timing properties of the of the response
time so though there was an attempt to
actually implement it using a feedback
loop it's the same diagram as before
just making made more concrete now I'll
say that the input so the set point the
desired value is the average response
time why average response time you might
ask average is not really exactly good
good timing indicator you should be
looking at high percentiles that's what
I said before that that's what we do
there are at least two reasons for that
first it's very easy to implement and
number two for what we're trying to
achieve it's enough so for protecting
overloads and and well nobody really
told us that it should be 99% also we
just did that
and then we compare it with the actual
average response time and in the system
so let's say actual average response
time is 130 milliseconds we set the
desired average response time is 30
milliseconds so the error is 100 this is
input to the controller then controller
generates request drop probability which
is not very very meaningful right now
what I mean is that it will just tell
you what amount of requests should be
dropped in the next loop iteration so
maybe I forgot to mention that in one of
the first slides that in these two
properties we want to in this loop we
want to one protect from overloads
number two we want the average response
time to be at 30 milliseconds but this
only applies to the requests that will
be handled if we cannot handle something
with this within this SOA I will just
tell to the caller
well sorry try again later the service
is currently unavailable or 503 so
control will just say hey this is the
number of this is the amount of requests
we'll be dropping in the next iteration
and then we have our plant which is also
a new thing here and what plan best as
you can see it's called average run
don't drop but it doesn't really matter
input is a request drop probability the
output is average response time in this
loop execution so in order to be able to
to measure that and actually control the
request drug problem the amount of
requests will drop in this iteration it
needs to control all the requests so it
also exposes
some API that all requests will go
through if they are to be dropped it
will be dropped by this thing if they
will make the request if there will be
handle it will be picked to have be
handled the response time will be
measured by this thing and reported and
then thanks to this it can output it and
then in the next loop iteration we get
average response time from the previous
iteration we compare it we input the
error to the controller then we get
amount of requests that should be drop
in the next iteration we get average
response time from the graduation that's
ending and so on and so on and so on and
so on and so on and it works all the
time yeah cool so yeah the only thing we
need to do now only thing that's not
known here is the controller what what
the controller should do very basically
to solve the control problem that's
that's simple so what's the simplest
thing so let's say error is this 100
milliseconds on I mentioned so this is
difference between 130 we have we might
have and 30 we set as a set point so
let's say error is 100 milliseconds how
can we convert this in the most simple
way to draw probability so a number
between 0 &amp;amp; 1
we can multiply it by some magic number
so now it's the controller our new or
initial implementation of the controller
there's no state and in every step which
is take the input multiplied by some
value and we output it and that's it so
let's think let's see 100 milliseconds
and we want to generate a draw
probability of 1 so this will mean that
all the requests will be dropped in the
next iteration so this is 100 times 0 1
to generate 1 right so if you remember
this slide this is the baseline good
mostly bad but good again that were the
response times and this is naive
approach we've just a queue so let's
test our controller with a magic number
equal to 0.1 now let me explain the
graphs here because they're slightly
different x-axis it's now number of
steps
executed in the loop so since every
every loop execution happens every 100
milliseconds we get 600 steps 60 seconds
times 100 at times 10 sorry and we have
two graphs first is the average response
time as an output from the feedback loop
control so output from this this this
block thing and here we have a
probability so this is output from the
controller so this is output from the
plant average random drop this is output
from from the controller and if you see
that something's off on this bottom
graph that's definitely the probability
that valve is 45 doesn't sound like a
good probability value to me
so but the good news is at least it's
bound I mean the response time it
doesn't go to infinity or to some high
numbers in maximum as far as I can see
is 500 milliseconds so a badly solves
one problem it's bound well
try to improve it a little bit if we
just try to decrease this magic number a
little bit by 10 by a factor of 10 it
starts looking better it goes up and
down still but the probability is up to
4 looks better but but this is first the
third section still looks very very
bumpy so if we decrease its even more
you can see it's it's tracking this
value I mean it's it's less oscillatory
but the problem there is here in the
second part you can see that the output
started to track some different values
and then our desired 30 milliseconds I
forgot to mention that the red line is
30 milliseconds mark our set point so we
want average response time to be
somewhere here now the problem is we've
just we've just multiplying two numbers
like this like we did we're not able to
make it go down to this red line we can
make it less oscillatory by lowering
this number more and more but we cannot
drive it to the red line what we can do
here is we can instead of well in
addition to just multiplying two numbers
we can also sum up the error and because
error I mean the sum the output
oscillate around the set point with time
the error is positive here negative here
so this will go to zero and and the
whole sum will converge to this this
difference I'm not going I'm not going
to go into detail why is that but that's
how it works so now if we just just
modify the our controller now it's
stateful initially the sum is zero sum
of error is zero and now we still
multiply it by a magic number one we
found but we also multiply our sum of
air
by some other magic number and output it
and the new state is the sum of errors
and that's what that's what what we did
actually it's called proportional
integral integral controller or PI
controller and if we were to find these
two magic numbers by trial and error we
might be still here or at least me till
tomorrow it actually turns out that this
this construct is very very popular in
control theory it controls 80% of things
that that control theory controls
actually its PID controller but just for
for keeping things simple we can see the
PI controller here and there are many
analytical and empirical methods to find
these two numbers and let's use one of
them I'm not really going to go into
detail very much how it works but how
what do you do basically you take your
plant you apply input in this case drop
probability of 0 and and you measure the
output you run it for some time you
measure it and the purple line is what
you get and input was applied at at time
0 initially because randomness is
because of my laptop what you get what
you've got is something called salt like
regulating process and you can you can
approximate with with an exponential
function that that the green line or
blue line is representing and it can be
described by an exponential function
with three parameters it doesn't really
matter what they are but you can count
them and you can apply a formula
and magic number is now magic number one
becomes a proportional gain and magic
number two becomes integral gain and
these are like this for this case so now
you might answer you might ask well we
just apply it and it will work right not
really
almost what's what's very off on this
graph is the probability so in the
stable state everything's fine
everything's totally fine the first
stage draw probability goes down to some
very negative number and we've time to
go to minus infinity the thing is that
the classical feedback loop tries to
keep track exactly of the red line and
what we're doing response times we're
below the red line so that's that's even
better than the red line so actually we
need to our our desired output it's not
a it's not a value it's a range between
zero and the red line and what this
means for our controller it's pretty
simple actually we just cannot integrate
when we don't drop anything so when the
output is zero or less and if you apply
it then everything's looks pretty good
it even oscillates around 30 in both
cases remember this this are these are
outputs from every single loop iteration
it's little bumpy we can do one more
trick one weird trick to smoothen the
output it's basically calculating the
average of the of the output over time
and this looks pretty good and well it
doesn't track the setpoint exactly but
here's the third thing here's the third
reason why the averages is good remember
again this is the output from single
iteration at interval 100 milliseconds
if we calculate a long running average
response time we'll just get exactly the
red line
that I'm going to show you now just this
is just the final version of other
controller we just implemented p.i
controller here as a controller and we
also smooth anything out with an average
doesn't really matter that much just
just a trick for reducing you the noise
okay so you might ask you might say you
know this is a simulation environment
happening on this laptop doesn't really
mean anything so we need some real like
real world like simulations you'll see
three examples let me let me try to
explain them it looks pretty bad right
three graphs from left to right number
of requests handled by the service
handled not dropped by the service
middle graph number of requests drops
permit per per minute by the service and
the third graph average response time
there are two lines on each of them
because there were two notes so this is
the steady state from the left we handle
17k requests per minute per notes just
free drop requests there was a spike
probably that we don't see really
anywhere else and average response time
is six and eight and seven milliseconds
so we're good now issue one I mean not
issue one example one exhibit one sorry
we increase the load let's start from
the right this time average response
time goes up to 120 and then it goes
back to 30 milliseconds again and what
happened then we just increase the load
very much you see number of job requests
let's say it was zero it wasn't but
let's say it was at zero when we
increase the logs exactly at 2 a.m.
number of drug requests spiked to 150
150 Cal requests per minute now what
happened on the Left graph is a little
bit concerning because the total number
of requests handled dropped as well
intuitively I would say it should stay
the same that's what this controller was
supposed to do but there's a but we
applied so many requests we're trying to
overload the system so much that just
getting the requests just just dropping
this 150 requests overloaded the whole
system so so we were able to handle less
real requests so as you can see it
doesn't also react when the database
that base goes slow it also reacts
properly when ourselves when our machine
is overloaded by any reason and just for
completeness a load decrease maybe from
the center this time we're dropping a
number of requests coming in to the
service two times number of drug
requests drops two times the actual
handle number of requests stays the same
over time and then we see this this
small drops of rapid response time and
it goes back to 30 milliseconds again as
soon as controller realize this it can
handle more within the SLA basically
okay but that was that was simulation
you might ask was it actually useful in
real life and it turns out just three
months after we implemented that it's
become useful at least in two instances
the first one holiday time last year the
whole meltdown / specter havoc it
started like this that it's it started
for us before everyone knew about it
so it turned out that for no apparent
reason
the paint'll agencies and in one of our
system are just going up without any any
reason and the client traffic is the
same nothing particular happens it just
goes up from 20 the maximum time goes
from 15 milliseconds to almost a second
no particular reason so it took us some
time to work around it it was strangely
correlated with Amazon patching things
underneath but we don't have any hard
proof that this was the reason but it
took us some time to actually figure out
what how to work around this but in the
meantime we had our controller dropping
some requests so this 800 milliseconds
didn't become I'd know 2 seconds or
something the only reason we dropped so
little in this case because average
response time was set to 100
milliseconds but in the real world
example number 2 when we in one of the
internal api's we deployed a database
call that it turned out to be very
inefficient
we're unexplained I'll be I'll be trying
to justify myself slightly we're running
explains obviously trying to simulate it
on Lotus environment with a similar
database it was supposed to take three
milliseconds in real life it took 300
and and we were supposed to run 100 1000
of them per second so we could easily
overload the database that someone else
was using that's the key point but but
who had the controller that protected
from actually doing the calls to the
database and what you can see on these
graphs actually is the moment when we
have our system running inefficiently
and then we do a fix and then everything
in blocks so this time top left a number
of requests received by our service so
both dropped and handled top right
number of requests handled successfully
left bottom number of requests dropped
by the service and then average response
time so we're getting the same type of
requests before fix after fix but we
were handling like 60 of them per minute
instead of 60k and you might ask one one
more thing the average response time is
250
it was controller was said setpoint was
set to 30 milliseconds so I'm lying
probably or we missed you our controller
well actually because it was literally
no way for this query to complete within
30 milliseconds what controller was
doing it was just trying to execute one
query you have seen that it takes 200
milliseconds so started dropping
everything then it cooled down with
times tried one request seen it took too
long
started dropping everything cooled down
executed one request seemed that and so
on and so on
and obviously automatically it was able
to figure out that average response time
is back to normal
what else we could use this approach for
other backing services everything that
has some level of uncertainty also on
the front end for better user experience
we could provide some more likely
content for lower bandwidth Network
users just like apple date for HLS some
references here feedback control for
computer system both by philip jenner
i'm just going to say amazing book and
if it's it's like 80% of its practical
theories at the end and you don't need
to read it if you want to implement
something like that so if it contains
like seven use cases of using feedback
loop control for various things
controlling cash cash cash kid ratio
things like that then control system
lectures on YouTube for free by Brian
Douglas it it's it's classical control
theory and no computer science there but
it's it's very very comprehensible and
then to chew articles by Fred uber I
didn't mention in the talk I was
supposed to about how cute some things
overload and and how you can handle
overloads in general and then some open
so open Souls source libraries you could
use I would like to especially focus on
the first one as broker its implements
some of the ideas from the
theory and if you're using ecto in
elixir you can use it as a default not
to default as a dispatch instead of
default pool boy
and then this count and jobs and then
it's our article about meltdown specter
think if you wish this will be available
to their links that was it questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>