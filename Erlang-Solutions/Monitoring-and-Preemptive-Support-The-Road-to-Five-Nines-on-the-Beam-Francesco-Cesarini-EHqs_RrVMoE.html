<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Monitoring and Preemptive Support The Road to Five Nines on the Beam - Francesco Cesarini | Coder Coacher - Coaching Coders</title><meta content="Monitoring and Preemptive Support The Road to Five Nines on the Beam - Francesco Cesarini - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Monitoring and Preemptive Support The Road to Five Nines on the Beam - Francesco Cesarini</b></h2><h5 class="post__date">2017-03-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/EHqs_RrVMoE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Clady was saying actually I'm gonna go
back to my very very first job I was
straight out of university so it was
that phase in my life where I knew
absolutely everything until I learned
something new but then I really knew it
all and then soon after you learned
something more again and then yeah I
think it's almost twenty years later
that you kind of admit and realize you
never stopped learning but back then the
only place where you know if you wanted
to work with airline the only place
where you could do it was at Ericsson so
the year was 1995 and so I ended up at
Ericsson training and consulting arm and
we were given very very strict
guidelines to follow when we were
implementing our code and we didn't
think that much about it you know for us
the whole software development cycle
ended when we went in and checked into
code into clear case and yes I used
clear case when it was still considered
cutting-edge now every now and then your
tests might come back with some error
reports but that was it you know the
project cycle was so long we never saw
her code go into production in 1997 I
was working on Erickson's broadband
solutions including ADSL and that's are
we going to production in 2001
colleagues were working on GPRS which
you know became 3G 4G LTE 5g once again
similar timelines and you know one of
the things we were told that there
excellent
we're very critical about it but we were
given very very strict guidelines to
follow and we didn't quite understand
what these guidelines were we just did
as we were told and it wasn't until
about a decade later when I wasn't
called 24/7 for a messaging solution and
I had to go in and start troubleshooting
production issues that I got my aha
moment and really went in and understood
what the guidelines which were being
given and and that's what I'm gonna talk
about today now how many of you have
Erlang or Alex you're in production and
so keep your hands up but now
if you have never been woken up in the
middle of the night and reserves
relative Acorah take your hand down so a
few people admit to having being woken
up in the middle of the night as a
result of a call-out and your code going
down and note here mine you know came
down as well
and the reason is yeah was my background
when I was when I developed and started
handling and supporting this production
grid this system this messaging system
you know I had you know the lessons I've
learned from Erickson you know and well
it wasn't Erickson it was the telco
space and what is so special about
telecoms you pick up the phone you
expect you hear the two two on the other
end if you don't hear that two two on
the other end it's pretty simple no back
then you knew that it would make the
front pages of newspapers the
legislation stated that no matter what
natural catastrophe disaster was
happening you still had to be able to
Dowe the emergency services and only
there were massive penalties when the
network went down and it was the fault
of the infrastructure provider so yeah
that meant you shipped code which never
never failed and you know what applied
back then still applies today in some
shape or form you know plain old dial
phones might have been switched with
your online gaming financial systems
ecommerce sites your video streaming
your messaging solutions and well you
might not make the front pages of the
newspapers but you know your users will
flock the social media and moan about
yet another outage and I think the real
critical part when you're developing
these systems which never fail is to
avoid single points of failure do you
have any New Yorkers in the room there
are a few yeah do you recognize this
building yeah sorry
no yes so this is the AT&amp;amp;T long lines
building in Tribeca
it's it's an example of brutalist
architecture and I got that I got that
from Wikipedia it has no windows
and it was built to withstand a nuclear
fallout
it opened in 1974 and you can guess what
they used to put in their phone switches
and it's well it is literally is one of
the most secure buildings in America and
the basement has a tank so when I went
in I was lecturing at AT&amp;amp;T and they
actually told me the basement of this
building has a tank with enough gas to
power backup generators for two weeks so
there is a blackout in New York for two
weeks following the blackout you'll
still be able to make your phone calls
and in fact you know after 9/11 it was
the only building of only fully
functional building south of Canal
Street
which was working and phone switches
today have become much much smaller so
those of you who might not know it as
the AT&amp;amp;T building probably know it as
the building where they suspect the NSA
has a lot of computers that's that's why
it's making the headlines these days but
it still has a lot of phone switches oh
no no they're not mechanical anymore so
they're much much smaller so the key
here is that you basically when you're
developing a system with requirements of
high availability you need at least two
of everything and so yeah you need
redundant power supplies you need
battery packs you need redundant
hardware so if you ask Joe Armstrong
he'll say you need at least two
computers because one might get struck
by lightning if you'll ask Leslie
Lamport I think he takes a slightly more
scientific approach but he'll say you
need at least three computers but
there's a rope right in their own way
but you need your redundant data centers
you need your multiple you know
redundant you need multiple cloud
providers I'm sure you've heard about
the story of everyone who got locked out
of the house when there was an outage on
Amazon on ec2 there was another out in
just a few weeks ago as well and even
there now sometimes a single point of
failure is not enough there was one
switch which was handling all of the
International the International trunk
for a city of many million inhabits
and it was an XD for once which it had
been running for three years it had
never been upgraded never required any
support any maintainence it was never
rebooted and it just ran it just worked
and after three years in your Ericsson
approach the Talco provided said we're
not supporting that version of the
software anymore and we need to upgrade
the system and yeah the telco provider
agreed reluctantly agreed and as it was
free releases it usually did software
upgrades in real time but as you know
there were three hops - had to do it
they decided to keep it simple and in
doing so they agreed to reboot the
machine they went in did the upgrade
they rebooted the primary node so all of
the traffic then fell over to the
standby node so once again ensuring no
single points of failure and they tried
to reboot the primary node and this just
it wouldn't start up that board would
not start up panic they try to upgrade
the standby know which had become the
primary node they rebooted it nothing
happened it will it wouldn't once again
it wouldn't start up even there and it
took them a week to actually get that
switch up and running again one of my
colleagues you know forget FedExing new
hardware new boards they would get stuck
in customs in that particular country I
had a colleague actually fly over hiding
the boards in his raincoat smuggling
them through customs
to get this well this switch up and
running again and obviously it was yeah
it didn't look good they went in and did
a post-mortem over you know what had
gone wrong and identified that in the
boot sectors of the hard drive the boot
sector in this hermetically sealed
harddrive the boot sector which was
rebooted was in the external parts of
the drive and so for three years the
head of the disk and move backwards and
forwards never touching the external
parts this very thin layer of dust had
accumulated
on exactly on the boot sector and when
they went in and tried to reboot the
system it just ground to a halt
it just got stuck and they were able to
run it so yeah today yeah they actually
free ones regularly do kind of hard disk
gymnastics everyday at midnight when the
traffic is lowered literally to spread
the dust everywhere but yeah this comes
a reminder of two things one is that it
might not always be the software which
is for at fault you know through many
other components need to consider but B
if you do get an issue or an outage make
sure that you address it before future
releases now I'm not gonna I'm not a
hardware person I'm gonna focus on
software here and this is and basically
you know with an argument that you know
by investing in proper monitoring in the
early design for in the early phases of
your software development you end up
saving a lot when it comes to my
maintenance now this is you know just I
picked the first software lifecycle cost
pie chart I could find online because
they pretty much all tell you the same
thing which is that you know the cost of
maintenance of your system goes merits
between 60 to 80 percent now by
investing proper monitoring in your
requirement specification design and
coding phases and obviously while
testing and integration as well my
argument is that you can actually go in
and massively shrink the overall cost so
it's a cost you have to pay upfront but
you know which will then you know give
you multiple savings and your during the
life cycle of the system itself this
could be years decades you know I'm not
sure about the specific chart but
usually yeah I would guess you know five
to ten years is usually your typical
average your lifetime of system before
it gets replaced but obviously it will
vary from system to system
now proper monitoring you know what it
does it gives you visibility into what's
going on in your system
and more importantly it gives you the
opportunity to react and act on the
information you collect so you use this
that this data for two reasons one is
preemptive support so actioning items
knew before to escalate and the second
is post-mortem analysis so once
something has gone wrong you need to
quickly figure out what it is which has
gone wrong fix it you know so not waste
that much time trying to figure out what
will run fix it and make sure it never
happens again so make sure you start
getting early warnings that you start
sitting and and it's a secret sauce for
having availability when you know you're
writing systems on the beam and there
are three distinct things you monitor or
you collect in any system one of them
the first one is metrics and you obtain
they're usually numerical you obtain
them by following a value at a
particular point in time so as an
example it could be the memory the beam
is using now believe it or not I think
you go back 10 15 years about 80% of all
of the airline installations out there
those maintaining go system had no idea
how much memory they were actually using
it and it just doesn't you know you
don't need to stop at memory could be
how many elements do you have in a table
in an ETS table which could then
directly map tune a number of concurrent
sessions or or active users you don't
have logs which is an entry into a file
or a database and what it does it
records an event or a state change in
your system or in your environment it
could be that your particular call is
started so you you connect the users it
could be that you've got a network
partition so that the network between
your two machines goes down note that
you've recovered it could be a financial
transaction which you're logging so you
know we failed with a credit card
transaction or we were successful and
you then have a subset of logs which we
call alarms and everyone in the telco
space will recognize alarms less so
outside of the telco space and it's one
of the things where I've the my career
always being pushing
into our vertical so started in Telecom
I got into banking why aren't you using
alarms Oh what are they the same with
your instant messaging or with any other
system and alarm is basically set sort
of logs which can be raised when an
event happens so you get a network
partition in this example and can be
cleared when that issue is internal
resolved or and resolution you can
either be manual so it's your system
administrator which tripped over the
network cable you raise an alarm that
you know you've got a network outage he
goes any plugs it back in we realize
that the networks up again you clear it
or it could be automatic so all of a
sudden a node goes down you raise an
alarm that the node is down that user
can't access the database you reroute
them to an active database to a fault
tolerant when we've done that one and
then totally there are two types of
metrics
you've got system metrics which are
metrics which you know could come from
your system itself so it could be memory
utilization disk CPU you know calls and
you've got business metrics business
metrics relate to the business logic of
your system so it could be the number of
attempted logins the number of failed
logins successful logins the reasons for
your logins failing so unknown user and
your bad password and so on now let's go
in and look at memory itself so so
memory usage is one of the key metrics
you know you always have to monitor and
yeah you can probe the total memory
usage of the beam but you know if you
run out of memory yeah you've got a
spike in memory usage and your beam runs
out of memory knowing that you run out
of memory doesn't really help you
you know what memory type you have
caused you you know just cause caused a
spike what caused it to crash and in
this particular example I don't know how
much you can see it in the back but the
fifth of November all of a sudden there
were two small spikes so these are this
is just through the plotting memory it's
got then the amount of memory
used by atoms because you can
dynamically create atoms it might be a
memory leak in your system it's the
amount of memory used by our link term
storage ets tables which once again you
might keep on inserting elements forget
to delete them you might have a leakage
or it could be very valid that you know
that these you know your user base is
increasing the total you memory usage
which is the top line here which is the
summer all these memory used by the
binary heap the memory used by processes
so the aggregate sum of all of the
process heaps memory used by your code
and system memory and we're seeing down
here two small spikes which you know
together causes like larger spike which
is code memory and system memory
increasing so more or less probably on
the fifth of November there was probably
a software upgrade which was done and a
few more modules were actually loaded in
the system which causes little spike and
when they realized that no these modules
actually worked and they made this the
release permanent they may be purged the
old versions of the module causing a
reduction once again in the total memory
consumption and so yeah this is just
interest to see but what's you know
should be worrying here is that over the
period of about a week memory
utilization increases by 50% so this
goes from your slightly over to jig to
over free drinks and yet and and this
should be act as a warning that you know
if this trend continues your system
might eventually run out of memory you
know maybe next month so at least you
you've got enough time to go in and
action it and you notice right here
what's following it here is the process
memory and that gives you time to go in
and figure out are we maybe you know
should we maybe force through a garbage
collection or maybe these inactive
processes which have not done a full
sweep in the garbage collector could it
be that our user base is actually
increasing so this is the aggregate sum
of is this aggregate sum of all of the
process maybe yeah we're having yeah we
have a user a process per user you know
if that's the case we didn't need to
throw more hardware more memory at the
problem
here's another example over a restart
once again anodes run out of memory and
it's restarted and you know the restart
occurred at probably 4 a.m. and for a
few hours
it was yep it took a while to recover
after which it went back to normal
operations you see that now usually you
know and what we were monitoring right
here was the whole message queue length
we took the sum of all of the message
queue lengths of all of the processes
and you plotted it over time when you
get a when your node runs out of memory
you if you might get a crash dump which
could be gigabytes large music we might
contain a lot of data you know good luck
navigating through it or it might be
zero bytes long if you're out of disk
space or maybe heart you know you're
writing it is so large heart figures out
that you're hanging because you don't
acknowledge your heartbeats and it goes
in you know takes down to the M and so
you might end up without anything and
you know so you don't really know and
indeed you could use the observer you
know to find out which processes have
large message queues but without the
historic data you know when you come in
in the morning that depending what time
yeah you come in you know you won't see
the state that you won't see this
historical data it's almost like you're
stuck in front of a screen hoping for
the crash to happen and yeah I've seen
people doing it hoping yeah let's hope
the crash will will happen now I hope to
catch it you know I'm working but
keeping my eye on it instead of actually
you're getting those metrics and using
them later to figure out what went wrong
and here it's obvious here something
went wrong with a huge spike in messages
the total message to be 150,000 messages
they must have been fairly large
messages and and that caused the node to
run out of memory restarted cleared the
process q and involves restarting you
there was a lot of activity which then
evened out now there are you know for
very common types of metrics
of metrics one or counters which you
just go in and increment you increment
them one by one
gauges which is a value at a particular
point in time so this is a typical
example of a gauge where we go in and
pull that the total size of the message
total message queue you've got
histograms which are readings over time
so they could include latency which you
give yet the average latency the number
of requests per second minute hour day
and so on and then finally you have
spirals which is a value over sliding
window count so those are your typical
metrics you tend to collect here's
another example which we detected and it
was an incredibly slow recovery after a
system had been restarted so system it
worked really well
it was restarted at around 12 o'clock
and for about 4 hours the message queue
just spiked to about 4,000 messages and
this is an example of Mongoose a.m.
which was running on a 4 node cluster
where you know after a crash one of the
4 nodes was really slow at restarting
now and yeah had a slow recovery and it
took 4 hours to completely recover and
what happened is each node was handling
about half a million connected users you
know the node crash so you lost a tcp/ip
connection that meant you need to log on
again you need to recreate a new tcp/ip
connection so you need to well fanta
kate the user's notify all of the users
first that you got offline then you're
back online so it becomes a fairly
expensive procedure and in the same
graph right here what we do is we've got
a cumulative counter here which shows
the number of successful media
transactions every login is a
transaction commit a successful
transaction and we see here that right
after outage rip we slowly get up to
about half a million users which get
reconnected
we look at the number of ongoing
transactions and yeah in the grand
scheme of things it seems fairly stable
so yeah and this is also what caused the
long message queue I was talking about
earlier and I'll get back to that in a
second
any questions so far no so you know so
from from metrics we actually had the
next thing you need to be very wary of
is logs and you try to log as much as
possible this is an example of logs you
get completely out of the box when
you're using airline and Alex here
they're what we call the SAS along so
the system architecture support library
logs and automatically you know if
you're using OTP and an error happens
within a behavior the behavior itself
will automatically log an error report
and the desire reports you can either be
or push them to file you can you know
you can display them on the screen you
can yeah
or you can write your own handlers which
do whatever you want with them now if an
error reports results in a crash report
you don't get a crash report once again
from the behavior itself followed by a
supervisor report the supervisor report
the supervisor report is you know
created by the supervisor when it's gets
told that a process is terminated
behaviors terminated and there's a new
supervisor report telling us that hey
I've restarted that behavior and you get
these in OT pinna will have customers
with to 300 nodes in production who have
no idea that their processes are
crashing because the process crashes it
gets restarted maybe a user you know
gets a timeout or you know presses retry
and then they actually get what they're
looking for so you know they don't
realize it and yeah I think one of the
things is you need to monitor you need
to monitor these the logs it's no good
just getting them you basically need you
know to push them in a particular
location maybe a result
I'm sending an email sending a slap
notification or you're making sure that
you go in and and look at them
themselves
now there are cases where you go in and
use the logs later when you're doing
post-mortem debugging as well so it's
always a balance of you of what you do
now these are you know in my view that's
how logging should look like and these
are very wise words from petal and you
know the truth is the log the database
is just a cache of the subsets of the
log I was when I was supporting this
messaging system I receive an email from
support saying oh this user and I got
the phone number on Saturday was really
really angry he never got his text
messages telling him that his favorite
team had scored and he only got them
after the match now this was a system
which had handled millions it handled
all of the short codes for a cell phone
provider in the UK all of the short
codes and so it was a lot of data going
for you can imagine you know millions
millions of messages every day all of
your own your Big Brother voting so this
was back in Britain so it was equivalent
of Britain's Got Talent
X factory or all of these trashy TV
programs where you still used to vote by
SMS so this was about 10 years ago and
right it's almost like looking for a
needle in a haystack how do I prove my
innocence
and you get someone else to admit guilt
easy I picked up you're using this
principle I took the phone number and
looked at all of the inbound sms's with
that phone number
I found that three messages had been
sent on the Saturday this was still on
the live machines you know we we moved
the dogs off every hour but kept two
weeks of live logs for 40 starter so I
found three semesters which were sent
with we logged with this number and it
was the first goal which was scored sent
at the right time
probably the second goal which was sent
probably 20 minutes later and then a
third SMS which was probably the final
score of of the of the game the messages
himself restored there but they were
encrypted so I didn't bother looking at
them now with each message there was a
unique identifier associated to it so
which we created when it entered our
system I took that unique identifier and
I went in and looked at where we did the
logic checks for a system because if
this could have been a prepay user and
that means that 80% of usually of preman
rate sms's fail on prepaid subscribers
because they've got no credit but no it
was a post per user and we'd also log
that he was allowed to receive premium
rate sms's and his account wasn't
suspended so all of the all of the
checks were done were correct you know
so next thing we did if when all the
checks exceeded is we sent the message
off to the SMS see the short message
service gateway which then sent it off
to the phone and immediately the short
message service center gave us back
another unique identifier which we
logged and which was then used whenever
we receive deliver reports so I went
into the delivery port log with this
unique identifier and found that
immediately after sending the SMS we got
back a handset detached message for the
first SMS second SMS further SMS and
then at half-hour intervals the retry
was every half an hour we kept on
banning back handset detach handset
detached hands - three times for every
SMS for each of these individual until
probably half an hour of their matches
had ended delivered delivered delivered
so this took me about one minute one and
a half minute I replied to the email
telling them well it's not us you know
the user was out of coverage or had his
phone switched off so it recommend that
he moves to a better network with better
coverage and also suggests he gets a lie
because you know what is he doing
complain not receiving the text messages
yeah you know you know it doesn't have
better things to do on a Saturday then
you have to follow football I'm not a
football fan and to which I got an
immediate reply back from a friend who
said this is the CEO of the company but
I will you know express myself in
slightly more diplomatic terms that
thank you for your help and yeah relate
to realize also that not only was he co2
computers also chairman of the of the
soccer club so and and that he was
probably the stadium with 3040 thousand
other people and that you know the
network couldn't handle that many
connections that many phones and so he
never got the SMS when he was there so
the phone was probably on but he was he
had a no network on it so you know it's
this is yeah and in these logs they were
not only used for debugging they were
not only used to prove our innocence and
push you push the guilt somewhere else
they were used for billing we knew
exactly you know how many users had
received premium rate as a message how
many had failed we knew what they cost
it was used for audits they check our
logs and they check the logs on an SMS
see and they we use my marketing to
analyze trends to analyze user behavior
so in a wide area of items but once
again you know I keep on saying you know
support and maintenance is 60 to 80
percent of your cost reduce it by not
having to waste years you're looking for
needles in haystacks you know find you
know what caused the problem in half a
minute in a minute and in our particular
case had it been a bug on our end the
log trail would have stopped somewhere
it would have actually stopped somewhere
and we would have yeah it realized okay
between this phase and this phase that's
where you know Decimus fail and then
we'd maybe go in and look was there a
crashed report oh here's the process
which crashed and we'd know exactly what
state the system had been left in and
finally alarms that's the third part and
alarms are
when certain criteria are met it could
be that you're running out of disk space
that you know your system can handle
maybe a anyone from walks up here a
million users on a single machine or two
million sorry so a million was so 2011
so two million uses on a single machine
and if you go above that threshold you
might want to start throttling yeah you
might might want to start rejecting
requests but also you're alerting
you know whoever's doing the operation
that you know you're hitting certain
limits and these alarms are raised when
these criteria are met at the same time
though they're cleared when the criteria
are no longer valid
so you go in and you delete some files
or your the number of connected users is
goes down and there are you know two
types of alarms which are generate and
the only alarms have actually been seen
used the answer the telco space of fresh
hold based alarms so you put in some
metrics and then in Nagios go in and say
oops you know we've we've hit 80% of
memory utilization you raise an alarm
right there those are the only ones I've
been seen I've seen used out there in
the telco space you open a cabinet door
this millisecond you open the cabinet
door you'll raise an alarm alerting the
operator that the cabinet doors open if
an breaks you immediately raise an alarm
the second your sensor detects that your
temperature has gone over a certain
level you know you're raising alarm your
your network connectivity to the
database goes down you raise an alarm
when that socket goes down that
millisecond the socket goes down you do
that immediately you don't do that when
you you know once a minute you go in and
you send your your your your your poll
request and realize you can't reach the
database you do it immediately because
this allows you to actually go in and
react the very millisecond you realize
something is wrong that's the only way
you know you can ensure that you know by
reacting and trying to avoid issues it's
the only way you know to stop you from
fear from from having an outage and as
an example you know also alarums have
different severity associated with them
so a 70% is full alarm my
have a minor severity minor so if it's
during office hours you send out a pager
due to requests and some we might go in
and look at it just make sure that you
know the increase in this usage isn't
isn't too big if it's an 85 this full
major this full alarm it becomes a major
alarm and you might want to call it
trigger a call-out only between 7 a.m.
and 10 p.m. but you hit in a 95% is full
alarm you know you want to trigger a
call-out 24/7 and have someone look at
it immediately just to make sure that
everything is okay do some housekeeping
and bring those levels down again
to to something respectable so now going
back to the previous example we where I
was showing you that the message queues
and you know all the media transactions
the first alarm we got when we were
recreating their error to figure out
what was wrong was a process message
queue major alarm so monitoring all the
cues as soon as we hit the process limit
of a hundred thousand messages in a
single process we we basically raised an
alarm what was of course yeah you know
what causes hundred thousand messages
well trying to figure it out we realized
we had another system limit alarm we
tend to monitor all of the system limits
so the maximum allowed number of
processes you know that you you start
your VM you have something like thirty
thousand processes which you can have
and you can override that and increase
it but the default is forty thousand
works for most users number of a maximum
number of ets tables the maximum yeah
and so on so we immediately got that
your the table count ets table count
which by default is set to 2050 three
was reached and well ninety exceeding 90
percent of limit 2053 was basically a
hundred percent and you know what caused
this oops
underneath we saw invalid app file
application file but also another alarm
which was application MongoDB versions
or different on the family so what had
happened was when doing the deployment
one of the four nodes
the node we've had all of the background
backup had the wrong version of the
database drivers installed and it was a
buggy version three of them had been
upgraded but not the fourth one and in
the shell commands we actually yeah
which we also log every time the shell
command is executed it gets logged we
saw you know where this error was made
it was a manual error it was before they
had automated or all of the push outs so
they fixed the upgraded a MongoDB driver
and got it running again and immediately
you know the fruit put on the MongoDB
driver which was causing that bottleneck
on that particular node went up to the
same level as all the other drivers and
problem solved you know with this level
of visibility it took a couple of hours
to address have we not had this
visibility I would have guessed probably
two three days at least because you of
repeating recreating the error
monitoring and and looking and how much
time do I have five minutes yes so you
know I think an example which I talked
about in my book I believe is you know
is you know when it comes to taking Ted
taking action is you know there was
someone who had uploaded and enough
rides patched on the node and they tried
to fix a problem upload an authorized
patch first time support realized it
didn't solve the problem they deleted
the beam file and after having deleted
the beam file they thought that's it
not realizing they had to actually purge
the module from the beam itself they
they escalated it to second line which
quickly push it over to third line
that's usually is the way it happens in
the airline world and it took the
engineer troubleshooting this issue on
this live switch forty hours only to
figure out that these two nodes were
running different versions of a module
so once again you know that happened
once never happens again
because we'll go in and monitor you
should monitor the versions of all the
modules actually yeah we take an md5
encoding and you generate a unique key
for every single mode
beam file and then compare them and as
soon as there's a difference we raise an
alarm so you know a lot of use you know
there's a lot of use for this these
these metrics you know marketing and
analysts use the business metrics you
know to study long-term trends you've
got developers which used you know the
collected information you to improve the
performance and reliability of your
systems and possibly a troubleshooting
issues after they've occurred
you've got operations who use them to
predict trends and usage spikes ensuring
that you know there's enough capacity
both to scale up but also scale down
just as important
we've got users who've got no idea what
the throughput is in their system where
they could very well get away with your
two instances instead of four and then
just you know fan out during peaks
now obviously DevOps you collect the
data to monitor system detect abnormal
behavior and prevent failure and what is
it you do yeah what do you do it yeah
it's when they prevent failure you use
it it's it's pre-emptive support it's
being able to react in an automated way
before you know a human before even a
human gets woken up out of bed if you
think of it five nines is about five and
a half minutes it's you know four
o'clock in the morning you get a
call-out from first line support just
you're getting out of bed turning on
your laptop you know getting that coffee
is you know you'll probably have hit you
know and used up about half of your half
of your downtime allowance for that year
so most of the time you know whenever
you detect an alarm trigger off a script
which you know automated support which
will do things for you you hit a 70% is
full alarm tar your logs 85% this full
alarm go in and delete all blocks and
you're running script which does a bit
of household a bit of cleaning 95% is
full shut down all of the non-critical
logs and delete whatever you don't need
or FTP them over to a separate machine
as an example and this is just one of
the many out of the many
example I could give you because you
know that also another thing you need to
do is you know is throttling so you know
a reliable system is one that is
predictable and going again and saying
well we're awfully sorry there are too
many users on the system come back later
that is a reliable system it's
predictable versus you not being able to
upload a page and getting its time out
and so what you do is you use the
monitoring for load regulation and you
know to basically push things out and
finally post-mortem debugging is what I
the examples I gave you earlier when
something's gone wrong you know
restarted you've lost everything you
lost the state of that particular node
you know use your logs to understand
what state your database was in use your
metrics to understand the state of the
system on its own and remember you'll
pretend this person right here is your
colleague who gets woken up in the
middle of the night to address an issue
you might have caused and also pretend
he knows where you live
so you know don't give them an excuse to
visit provide them with the necessary
visibility which allows them to to to
isolate and address the issue and you
know this is and this visibility I think
it's critical you need to plan it as
part of your requirements design and
development phases it is not something
you can go in and bolt on as an
afterthought in your system you need to
do this very very early on so any
questions yes
so anything yes so I think my experience
there will be around incremental
counters where you pull the counters
every second and you're able then or
every every regular time interval and
you're able then to predict the spikes
when you see the increase you know from
from from one item to another
we've never acted on them but I think
it's a very you know I'm sure their
systems where you know we're applying
that would be critical and the
challenges every single system is
different there is no one-size-fits-all
so yeah
yes
you there are there are tools out there
which will do it for you so logstash
is the first match comes to mind Splunk
is another which you know and these are
sash tools you also have well tools you
can run on Prem and it's once again it's
very important that you integrate with
the tools your operations teams is
comfortable with you can inflict new
tools and operations because you're
trying to introduce ironing Alex here it
will not work the number of times if I
only received a cent for every time I
got told Oh operations won't take it
over because it doesn't run on the JVM I
would have been retired by now it's you
need to integrate with the tools
operations as comfortable with because
you're coming with a new eco system
basically and familiarity is critical we
heard it this morning in the keynote
it's just as valid with operations teams
and so you give them the visibility even
better visibility in what they're used
to in their tool train and everything
will be forgiven even the skunkworks you
did will bring Alex here in production
okay any more questions so there are two
things there's a blog post you might
want to read on earning and Alex your
DevOps from the trenches where we
describe a few war stories some of which
I spoke about here and there is also a
chapter 15 of the science for
scalability I really go into detail over
what I talked about on your monitoring
and preemptive support so you know with
many of the examples and the book you
can find on BitTorrent if you like to
try before you buy
and and if you like it and yeah you're
happy with it just use this town code
out D which will give you 50% of the
digital copy and give you 50% of any
other O'Reilly title as well so
shouldn't be telling you that but yeah
we'll cut it off in the video later
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>