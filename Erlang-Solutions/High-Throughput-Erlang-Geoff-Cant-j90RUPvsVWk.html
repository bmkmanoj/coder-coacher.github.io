<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>High Throughput Erlang - Geoff Cant | Coder Coacher - Coaching Coders</title><meta content="High Throughput Erlang - Geoff Cant - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>High Throughput Erlang - Geoff Cant</b></h2><h5 class="post__date">2013-04-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/j90RUPvsVWk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I got a wonderful-looking slide deck
because I stole the google i/o slide set
up thing we found a great in theory
until I realized that I needed to
rewrite the rendering thing that
generates that kind of HTML and then
fiddle with it a whole lot so I guess
tip for new players just use keynote
really just use keynote I'm here to talk
to you today about some I guess hard-won
lessons about a high-throughput Erlang
system that i work on at Heroku it's a
system called log plex my name is is
Jeff Kent and I work on the on the
routing team at Heroku I've decided to
dedicate this talk today to pay to duty
and the infinite sadness not only
because it worked me up at 4am but
because I guess pagerduty is almost the
reason that i right Erlang systems
pagerduty is the thing that sends really
annoying alarms to your phone and then
you've got that alert tone set up so
that it sounds like a submarine dive
alarm it's a generally unpleasant thing
but if you have a production system that
people care about you will have a page
of duty account or you will have some
kind of equivalent and I guess I writer
lang because I get less pages that way
mostly so this is not actually a slide
about my incipient retrograde amnesia I
work on the routing team at Heroku and
the routing team has two reasonably
important jobs to reasonably important
systems that we work on we have our big
front end HTTP load balancer now Heroku
is a company who platform as a service
company for hosting complicated with
applications in the cloud and all of
these applications work over HTTP so the
front end load balancer that distributes
incoming requests to the correct
application
and since the responses back is a
reasonably important system heroku and
we also work on another system the
syslog router called log plex which is
mainly what I'm talking about today and
i've included a couple of lengths to
might get hub account which has some
sort of relevant bits of code and
assorted snippets and stuff and things
and a link to your couz github account
where you can in fact find the source
code for log plex so what is a log plex
it's a hired it's it's a syslog
collector and emitter so on the Heroku
platform we have lots and lots and lots
of machines running you know some order
of magnitude larger than that number of
pieces of applications so copies of
people's web server our own
infrastructure software all kinds of
thing well sorry doing all kinds of jobs
on that platform and they all need to
write logs somewhere but you know we're
running on ec2 disk is not really that
fast and is prone to go away when you
really don't want it to and also if you
write logs to desk or great you've done
sort of about 1% of the problem because
getting those logs to a place where you
can read them is kind of maddening so
the approach we took is kind of the
standard thing that you do when you have
a large collection of unix machines is
we run remote syslog so the log Plex is
the remote syslog destination for
everything all of the messages coming
from customer code from our own code
from the platform itself they all get
routed through local syslog demons to
this this big cluster called the log
plates each of those messages includes a
little token in it which lets us work
out how to route those messages to
people interested in them so that you
can think that the messages are usually
about some kind of
topic and that's usually going to be an
application or a piece of infrastructure
or something and frequently these these
topics are divided by customer so you
don't really want messages for one
customers topic ending up in another
customers topic because getting all of
the syslog messages in the universe to
one place is is it useful but not
sufficient task we have these things
which in Lock clicks Lander called
drains it's where you configure the
system to deliver all of the messages
about a topic to some destination and at
the moment we have UDP TCP syslog and a
slightly crazy syslog over HTTP variant
that we use for delivering those to
third-party logging providers other
people's remotes aside servers that kind
of thing Lock clicks allows you to sort
of modify the configuration of the
system at runtime create new topics
change where their output is posted to
all that kind of thing has a little HTTP
API to do that there's one kind of nifty
feature which I haven't seen in a lot of
other things it has a mode where it will
do effectively tail dash F on the topic
for you so you can use the command line
client and say I would like to you know
get logs with a tail for this thing and
the system uses Erlang distribution to
collect all of the messages from those
various nodes in the cluster to one
place so they can be delivered to you a
web browser and if you're interested in
the mechanics of that kind of stuff
there's a link here when the slides are
online you'll be able to go and see the
the log plex open source and see how
that works so this is the infrastructure
or where I guess logical system diagram
that I usually pen on a nameless
five-year-old but is in fact me trying
to draw on an iPad messages have tokens
the token determines which channel
messages sort of notionally pass through
and then channel output is multi plates
to drain
I'll put destinations possibly tales and
we maintain a bit of a ring buffer and
read us so that you can get the sort of
when you do the tail command you want to
be able to get a little bit of preceding
history but we can't afford to keep any
everything so that's where that stuff is
so high throughput Erlang systems my
usual approach if you can apply back
pressure do I'm not quite sure if I am
quoting someone here but I should be
active once is is fantastic it kind of
automatically gives you rate limiting
and back pressure your clients slowed
down to an input rate that you can
handle well job done great short
presentation hope people will enjoyed it
oh wait there are more bullets on the
slide damn logging systems generally
can't do this and I'm going to get to
why in a second so this is yes the final
part of my usual approach so brief note
on syslog the protocol it's the worst
synthesis to the receiver connect data
data data data data data data data and
the receiver says nothing at all there's
no application layer acknowledgments so
the receiver can really only slow down
the sender by stopping receiving and you
either just stop reading off the socket
and then the sender's TCP buffers
eventually fill up and it stops sending
or you close the connection in which
case they will probably just reconnect
you and start sending again and because
there are no application layer
acknowledgments like when that
connection gets cut and particularly if
it gets cut due to an error like packet
loss or your load balancer went all
psycho on your connections which seems
to happen more often than you might
expect the sending side doesn't know how
many
is it actually successfully delivered so
syslog where even the message framing
format by working out where one message
stops and another one begins kind of
unreliable there are at least two
schools of thought that I've seen in the
wild one is we use new line termination
which is fantastic except that now you
can't put a new line in the middle of a
message because there's no escaping
rules and then the other one is length
prefixed and you like wow that's that's
that's a reasonably sensible choice and
you're like oh yes except for the fact
that the aspie encoding of the integer
that specifies the length of the message
looks an awful lot like a unix timestamp
and it'd be really bad if we confuse our
message boundary and started trying to
read a 1.5 gigabyte message and you're
like well okay sure but I mean people
connect to you and they're at least
going to send you the same kind of
framing format on one connection right
yeah not so much so log clicks has this
kind of weird recovering syslog pazzo or
if it gets confused at starts dropping
by its until it finds something that
might be a message boundary and then
tries to see if its length frame doing
if that works will do that otherwise we
look for new lines misery and pain well
at least since like the protocol was
pipelined in that there is nothing that
the receiver can do to slow this ended
out and it has you know reasonably
minimal encoding and decoding complexity
I'm looking at you with daggers in my
eyes JSON so then the pickle that I am
in nobody wants to slow down what
they're doing to guarantee Lord delivery
or at least you know those people exist
but they're a bit of a vanishing
minority when we're talking about the
usual case for logs people really kind
of want it to operate sort of like it's
always done they do a whole bunch of
stuff right atrocious amounts of garbage
to syslog and then expects this log to
deal with it and then that will get now
get annoyed if you lose messages but not
as annoyed as if the thing reading off
of their standard are collecting their
law
messages may be stops reading off a
standard out that generally makes them
very angry so that's kind of that's the
sort of center side of that and then
when we are being a remote syslog
receiver the pic what we have is that if
we don't accept messages then the sender
is going to drop them because they're
buffers are only so big and if we close
the connection then the sender will drop
the messages and you know a few extras
just for just for funsies an extra
wrinkle in this little pickle is that a
sender ascenders connection full of
syslog messages those messages might be
for different clients with different
topics so it's actually unfair to slow
down one of these connections by you
know just reading slower or dropping
messages off of it because that's
unfairly penalizing you know random
people who happen to be on that same
connection pretty miserable little
problem so I inherited the system about
a year ago and the input side was just
not really keeping up with Sanders so
the first things I did has started with
the basics looked all through the input
side to work out was it accidentally
serializing was it making Jen server
calls to a gem server where the M to end
of input processes to Jen servers is not
one-to-one or better any chance of a
call in your input side is probably
suspicious if you need to run that a
ridiculously unbounded input rate it's
table reads they seem to be fast enough
in practice reap concurrency I guess is
an option in later VMs and that seems to
improve matters but I guess if you if
you don't need if the table that you're
reading from is not being written to all
that much it generally works even
without that okay cool done all that
input side kind of under control flip
through it again so calls
wait what this is the message of doom
and pain that you get so now we're so
good at keeping up with the input volume
that we are blowing out the message
queue on the output side of the system
and we run out of heap space and the vm
goes to allocate and goes no give up
completely can't add back pressure to
the input side because we're a logging
system when we already decided that we
can't yeah put the output side is it is
tricky you know sometimes you're not
connected to the destination either
because this is the first time that
you've tried or the connection was
interrupted for some reason it was
packet loss or the other end crashed and
restarted sometimes we're not connected
and he kind of go ok well I guess I
could drop messages while I'm not
connected to the other end it's not
ideal but really you know what more do
you want from me I can't I can't store
these messages on disk because there's
only so much disk in the world and
frequently it's very very very slow disk
in the cloud ok well alright we'll
compromise will will drop messages if
we're not connected to the other end
then your other end will turn around and
do this to you they will run to slowly
and you will still blow out your message
queue because you're busily slowly
pushing messages into the TCP connection
and they're reading them really slowly
and your message queue is just going to
do do do do do do do do all right
pretty sure I promised in the talk
description that there would be some
tricks as the graphic on the slide may
indicate these tricks they come at a
cost and so I don't recommend them
unless you have to here is a kind of
dumb retarded trick but it may staunch
the bleeding this this is a dumb check
that you insert in the middle of one of
your output side processes and you check
your own message queue length and if
it's greater than some number of
messages which probably represents a
large amount of memory commit suicide
give up you'll probably be restarted if
your supervision tree is set up
correctly and you've just dumped a
reasonable amount of messages without
having to read them and process them if
you are like tremendously slow this is
one of the things we're having separate
memory spaces and message copying and
overlaying is great the process dies all
its memory gets collected straight away
with no muscle fuss or anything so I
mean this is terrible I'm telling you to
have your processes commit suicide
because they're going to slowly but yeah
sometimes this will get you through the
week that you need to patch your system
so that it runs well enough that you
don't need to do this anymore I guess I
would also recommend taking it out after
that week because otherwise it will just
sit in there and then everything will
run along fine for months and months and
months and then there'll be some other
weird thing goes wrong that causes the
thing that which causes the thing and
then the process goes poof and you go
why did that happen alright so dumping
the message queue unceremoniously is
obviously not ideal so in that week that
you bought yourself you think right
bounded buffers will move that memory
from the message queue into the process
heap why would we do that well that lets
us limit the size that that can be we
can't limit like oh and gives us no
primitives for limiting the size of the
message
you or the size of the process heat
because the process heap is part of
sorry the message queue is part of our
process heat well we just we get no
control over limiting that growth so the
only way you can limit it is to pull all
of those messages into the process heap
and limit it there and because this is a
logging system we can generally decide
to discard the oldest input to meet our
you know memory space requirement
because when you do have to drop
messages the the most recent messages
are generally more important to people
than messages that occurred you know
half an hour ago whenever cool okay we
bounded our buffer size we drained
messages into our process and we now
silently losing messages it's like yes
we are it's no longer as noisy as the
entire vm exploding and ngos going off
and pagerduty waking you up in the
middle of the night but silent message
loss is not a great characteristic in
the logging system so what we can do is
we can keep track of the loss at least
when we displace a message in our little
bounded buffer running in our process we
can take a timestamp of when we start
doing that and we can keep track of the
number of messages that we have to
displace then when we grab stuff out of
the buffer we insert a fake message into
the retrieval where we say like you lost
this many messages since this time and
that's actually a pretty helpful thing
it means that people no longer look at
the look at their logs dream that
they've may be saved to paper trail or
log lee or somewhere and go well my
process was running fine here and then
something and then it continued and it
jumped over a whole bunch of lines of
code a logging system lying to you is
almost worse than no logging system at
all so explicit loss is has been a
really useful trick
in in the context of log plates so the
version 0 of the code a whole lot of
messages they go into the message queue
the output process reads off the message
queue and sends to the tcp port reads
off the message to you and says the
teaching people that explodes in your
face so then we move it in process and a
whole lot of messages we read them
really quickly off the message queue and
we put them in the bounded buffer and
then we send to the TCP port and we read
some more out of the bounded buffer and
we sent to the TCP port and we read from
our message queue and me put things into
the bounder buffer and so on and so
forth and that's great but sometimes a
gen TCP send will take a really really
long time and because of the law of
infinite perverseness of software or
something rather that will happen at a
time when you're getting a whole lot of
messages being put on your message queue
now while you are blocked trying to do
that Jim TCP send like nobody else is
slowing down they don't care so you run
out of memory and your vm explodes yet
again just when you thought your over
this so we'll add a timeout or call Jim
tcp sanding / 3 hahahahaha no that
doesn't exist which I guess got me
curious because he is like a weirdly
synchronous call in the middle of our
language is generally all all about the
asynchrony so I got curious like how
does jen tcp work under the hood and is
there any way that we can turn that
synchronous call into an asynchronous
call or maybe a synchronous caught with
a timeout something like that it was
only a very large amount of time later
that I learned about the send timeout
option that exists on jen tcp ports it's
it's a bit of like the option that you
sit on a port affects all sins and it's
always the same size of time out so it's
yes I appreciate the Erlang makes some
attempt to give you this functionality
but I'm not convinced that it's it's
really the the answer so to the rabbit
hole find out how Jen TCP actually works
so when you call Jen tcp send it does
this kind of weird thing where it goes
to the inet DB whatever that is and it
looks up the socket and it gets back
this module and then it it uses the sand
operation in this other module to
actually do the sending okay well that's
curious and so you trace through the
code and it's kind of all squiggly and
back to front and upside down because
this is some of the oldest code in the
o-line distribution like this has been
around since it had TCP and it gets a
bit confusing so skip over a lot of
Investigation mod always turns out to be
pro my net for tcp ports and it has this
weird little trick where there's an
undocumented call called Erlang port get
data apparently you can attach a term to
a port and then retrieve it later and I
guess at some lightning talk in the
future I'm going to find a horrific way
to abuse that and make Kenneth and the
OTP team exceedingly mad at me but
anyway so if you go into pro my net and
look at its cindy operation it deals
with some option lists and it deals with
a bunch of error handling and some other
stuff but i guess the important part
boils down to this the port command to
send data down the socket kind of
returns immediately and then it waits
around for a reply message and you'll
like okay so under the hood this is all
a sink and you just won't expose this to
me all right this is a production system
I'm one day from retirement I play by my
own rules
we could just do what it does and add a
timeout okay sweet what I actually did
in practice was did the the fully
asynchronous version where I do the sand
and I start a timer and so the send
completion event will either be a
timeout message or it will be one of
these I net reply messages so I can
continue to process my message queue and
not be blocked why while I wait for the
center timeout and you know you can take
the code that that that's on the slide
and kind of work out how to do the
message version log plates TCP syslog
drain is a module in the open source
distribution that you can look at see
how precisely we did it I guess we
turned a a two-state finite state
machine of where either disconnected or
we are connected and really descend into
a three-state thing where we are
disconnected or we are connected and
ready to send or we are in the process
of sending but the neat thing is that in
none of those states are we blocked on
anything like we can always process out
a message queue and maintain that
bounded buffer great this was kind of
the the first approach that I took to
creating one of these high throughput
processes and it does work it's in
production right now the approach was
basically make all the synchronous
operations asynchronous no matter how
much you know tracing through the bowels
of OTP you have to do to get there we
don't spend much time block we can drain
our message queue but the code
complexity has gone way up like quite
quite a ways up so there's there's a
cost to be paid for this and we have to
be really really careful when we add
third-party libraries into the middle of
our code now because if they
accidentally do anything that might
block you
you're back to square one and your
message cube blows out and the vm
explodes so you you end up with a
process that is high throughput and it
does work and it does deal with a whole
lot of receiver faults and failures and
slowdowns and that kind of thing but
it's fragile so that was the tcp syslog
stuff then it came time to see if we
could add like an HTTP transport for
syslog which I know sounds crazy but
there are worse app day sorry there are
worse ideas out there believe me you can
go googling for some some of those
things man oh man can talk to me about
it afterwards so for the second time
around I decided why not do that
erlanger thing add more processes that
generally seems to be the solution to
many things except possibly having too
many processes so I guess what the the
approach was here that I was trying to
change my output side I needed to add an
HTTP client to generate the request body
might form at the the command that we're
going to shut down the TCP socket but I
guess more importantly I had to add code
for watching the state of that
connection because it's no longer I'm
connected and i can send it's like i'm
connected and i sent a thing and i have
to wait for their response and they had
read through the various bits of their
response before I'm allowed to you know
like send again unless they allow
pipelining which is something I hadn't
thought of but the the TCP syslog stuff
that I had it was really complicated to
add more states to that thing and if you
get any of them wrong you stop reading
from your message queue and it explodes
the other thing involved is Erlang's new
ssl implementation and this was kind of
around the early ar-15 series where
Erlang's new ssl implementation was
seriously rough and it had a number of
operations which just every now and
again
we're just not complete at all you would
do the call to like connect or 27 sends
and bites on the wire or to receive off
of the wire and we just stop we just sit
there forever even better yet the TCP
connection was still established I I
don't even know but when you add that
kind of like unreliability in a library
that you need to call and this process
which is maintaining its it's bound to
buffer the complexity is just was
impossible for me to deal with so moved
the bounder buffer to a separate process
I changed where the messages got
delivered into it so that they would be
delivered not to the drain process
itself but to a buffer process and then
in order that I didn't have to have
infinite amounts of back and forth
between the output process and the
buffer process like I certainly didn't
want a whole like I have a message okay
give me the message here's the message
me a message here's a message to it
every day because doing that once for
every message i mean that's like the
amount of work you have to do growers
proportionally to your input and that's
that seldom works out well for me so
instead the bounded buffer i arranged so
that it would have a couple of states
it's either empty and receiving messages
once it has any messages at all it
notifies the output process that it has
some data for it and then the output
process asks for a bunch of messages and
it gets back a message containing a
bunch of messages and then I've got
really fancy possibly a little overly
complicated as fred will discover in a
couple of weeks in that i could send a
function over to this buffer thing where
i could have it actually frame out the
messages so that i could ask for a
number of bytes worth of messages out of
this buffer and so it would read a
message out of the buffer and encode it
and add the framing and give me back
basically an i/o list and just keep
track of the size and then keep doing
that until it had got say like four
kilobytes or like up to four kilobytes
of data and then return me this like
four kilobyte chunk that
to put on the wire and that was that was
an interesting idea to be honest I
haven't done enough performance
measurement to know if the idea was
worth it but it is a definite
improvement over the gimme a message he
has a message give me a message here's a
message so the slide in the web version
will link to the the modules in law
clerks which implement this idea yeah so
I guess the the first version I like
even doing all of that still didn't work
and that's because my ssl operations
were unreliable I would try and connect
or i would try and sandro try and do
some other thing and it would just like
totally sit there and ignore me for the
rest of time and the worst thing about
it was is that it didn't crash like at
least if it crashed then supervisor
restart would kick in and we would at
least make some progress because this
this event was relatively rare but it
didn't it just SAT there and the TCP
connection was fine and the other ends
going dude where's my data no good so
put children to work scheme add more
processes you've got some kind of
unreliable library library that you
looked at and we went I know I have to
include one of these to make it go at
all but I have a bad feeling about this
it's borne a child get them to do the
library operations don't let them fail
you a first-time started so what you can
do is move that potentially unreliable
operation and I mean really am you know
teaching a large collection of
grandmothers to psychics here but you
can move that operation to a sacrificial
process the the Kinect work happens when
you spawn it maybe it will send you a
message back to say hey I'm ready for
data now because maybe you want to track
how long it's taking to connect two
things as an indicator of their health
you can pre-spawn them if you want
be already connected to this destination
before data even comes in and then they
just sit around and they wait for a
message of like here is the data I want
you to transfer and now go do it and
give me a call back when it's done you
also have to start a timer at that point
because just like with a sink tcp send
the completion event is either going to
be your child saying look look at me
I've achieved or it's going to be a
timeout saying they got lost in the
woods and yeah just give up on them you
can either immediately start connecting
you can just kill them off at that point
it's like they failed for some reason
did they fail because DNS lookup took
too long who cares did they fail because
the TCP connection was going too slow
from cares I if your destination like
most things in the cloud I guess these
days the the destination will be will
have a whole lot of machines behind it
serving it so a failure of almost any
kind retrying will usually get you the
result that you want especially in the
case of like a tcp stall because you
know a packet got dropped and now some
timer thing is going on who cares just
retry it so you can you can kill and
retry it straight away or an idea that I
had that I haven't tried is you retry
straight away and you just leave that
client sitting around for a while and
maybe delay killing it off because who
knows it may come back to its senses
Erlang start timer / 3 is a fantastic
function and you should get familiar
with it if you have not already okay to
summer if clients in a logging system
refused to slow down then they've
basically chosen message loss is the way
to smooth over the fact that they will
produce too many logs at some point for
the system to cope with the amount of
variation in the volume of logs that
somebody can generate is
quite frankly astonishing because it's
one thing if your web server goes along
and it gets a request and it produces
one line of output Apache style saying I
responded two hundred and here was a
user agent and some other stuff and
things sure that's the usual case they
generate 10 15 50 messages a second
dealing with that is pretty fine then
something goes wrong in their web
process and all of a sudden a request
coming in is like five thousand lines of
XML encoded stack trace and the way
syslog generally works each of those are
separate messages so the message rate
has gone from like 50 is set into maybe
50,000 a second but they refuse to slow
down they don't want to like generate
stack traces slower that would be
madness so basically they've chosen
message loss but they will be much
happier with you if the message loss is
deliberate and not deliberate in the
while the Erlang vm could now locate
some memory so it totally exploded and
through shrapnel everywhere kind of
sense they will also be less annoyed if
you make a loss explicit earlene being
Erlang going full async on almost
anything is possible but frequently
comes with a rather large cost in terms
of complexity sometimes it's worth it
sometimes not and the answer is usually
add more processes all right that's
that's really all I have and yeah
questions
okay the the question was we have a lot
of ports and processes running in the vm
does our message queue blow out and do
we change the priority of processes in
order to deal with that we I guess the
the Amazon Instant size that we
typically run has four to eight cores
and they seem to be beefy enough that
the run queue size for us I guess hovers
in the teens and it seems to be
generally okay we haven't changed the
priority of any of our processes I think
we might have included some libraries
which have some slightly higher priority
processes but for the log Plex code
itself I don't believe it uses miss a
process priority anywhere know somebody
EU in the back
so the question is at our 99th
percentile how big is our largest log
entry we have the because of the problem
i mentioned with syslog protocol parsing
and a length header looking exactly like
a unix timestamp we deliberately limit
the maximum message size to 10 kilobytes
in practice what is the statistical
distribution of our message sizes I'm
sorry I don't know yep so if I wasn't
worried about copyright infringement I
would have gone to meme generator and
gotten one of those sweet McDonald's X
many billions served things and would
have said 36 billion served today yeah
it's a hundreds of thousands of messages
a second however we find that the with
with our current implementation
performance wise it can do 10,000
messages a second comfortably and it can
spike to 20 30 maybe even 40 thousand
messages a second on a node before we
get other weird side effects which we
haven't managed to figure out
so we I'm not quite sure if Jake varuda
if you look back through other airline
factory conferences has talked about a
thing called tempo but it's it's a
homegrown I guess low latency graphical
dashboard and so we get graphs of
message received processed drained and
dropped with one second resolution in
near real-time and we do that in a
pretty terribly straightforward and
braindead kind of way every second to
process captures and counters from ads
and publishes those two Redis pub/sub
and when you hit a nap we have called I
think tempo dash log Plex you get a
little it does like long polling or web
sockets or something and gives you just
a continuous chart of those things so
that's what we do for most of the
profiling I guess occasionally we'll go
in with message tracing like the log
clicks system itself all of these drain
processes and input things tend to
implement tend to emit a bunch of logs
themselves and we drain quite a lot of
that into a spoink cluster and
investigate logs after the fact that way
I guess I would also add that in the
cases of vm exploding doing post-mortem
analysis with Crashdown viewer or
something similar has been exceedingly
helpful for us particularly if your load
balancers can cover for a failed node
long enough for you to actually get a
complete dumb complete dumps are
fantastic you get a little web interface
you go to the process view you sort by
you the message queue size or process
heap signs and generally your problem
leaps out at you it's wonderful
I'm not sure that we've had any
particular dramatic gains my general
sense is that it seems to improve I
guess way back when we had configured
the system to run in non SMP mode and
for a time that gave a performance
benefit but when we went through the the
input side and removed all of the
accidental serialization then turning
SMS SMP back on dramatically improve the
throughput of the system question from
the bank so I guess we we have exceeded
the capacity of our load balances in
front of this thing which is just
running stock H a proxy on reasonably
beefy Amazon instances like we now run I
think 10 of those we're doing more than
a gigabit of logs as input the the
cluster size for 36 billion messages a
day is is 40 nodes I guess we're hoping
to be able to drop that down or at least
keep it stable relative to message
volume growth but yeah that's that's the
system as it stands today ok thank you
very much for your time oh one more
yes UDP does have a better framing and
that the frame is the packet yep we had
horrific problems with untracked loss
with it and partly that was kind of a
system design thing but the problem with
running a UDP socket and active once
mode is that it's very hard to get a
multi-process fan out on that because as
far as I can tell only one process can
be receiving off that socket at a time I
may be wrong in that but even if that's
right if you add active once to that UDP
socket then every single message you do
you have to do an erlang like port
command Port Control thing to change it
back into active ones mode again and the
cost of doing that quickly means that
you start losing messages very rapidly
and so we move to TCP so that we could
add some buffering on the sender side to
at least cover you know disconnection
periods that kind of thing alright thank
you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>