<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CodeMesh 2014 - Alex Champandard - Beyond Shady AI (..) | Coder Coacher - Coaching Coders</title><meta content="CodeMesh 2014 - Alex Champandard - Beyond Shady AI (..) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>CodeMesh 2014 - Alex Champandard - Beyond Shady AI (..)</b></h2><h5 class="post__date">2014-11-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/YQJzgIYEbWA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name's Alex from polar and over the
next 15 minutes or so I'm going to talk
to you about artificial intelligence
algorithms in particular moving go over
to the GPUs so my background is in
action combat games I've worked for a
rock star for a few years and did some
contracting for Sony some one of the few
guys in this conference that writes C++
code actually have spent about 12 years
working on gaming I overall and
organizing lots of conferences a game at
a conference and presenting also
different topics and of all those topics
this is probably one of the more
forward-looking or at least currently
applicable but I find it most exciting
and the reason for that is it was part
of a research project in a year research
project so we were paid to look into the
implications of new hardware on current
algorithms on how to redesign algorithms
to work on new hardware so my company
and again dev we have the AI expertise
and we work closely with a consortium of
companies including code play who do all
the GPU side of things that drivers the
compilers so special thanks in
particular to Andrew and Paul for all
their insights and helping with this
presentation as well so when you think
about AI on the GPU there is a lot of
hype and marketing that goes into this
and the console manufacturers and the
graphics card manufacturers are all very
keen to emphasize how these GPUs can be
used for other purposes than just
graphics so they want to turn them into
general-purpose GPUs GPGPU but
ultimately GPUs can't really do much
more than CPUs except run the same code
faster much faster or do more in the
same amount of time it's not you can't
really do anything different on a GPU
you could see people you can just get
better in certain ways so the big
question that I'd like to answer today
is why should you run a micro door why
should you run on graphics code on your
GPUs and it could be a difficult sell if
you're in a games company that it's hard
to get those resources from the graphics
programmers but I'm gonna look at this
from the perspective of a game of
capture the flag and those of you that
were here the earlier you saw the live
version of it this is a video the blue
team is intercepting the red flag
carriers and setting up ambush locations
along the flag the shortest path I'll
show you that in more detail break down
each one of the parts that are on the
GPU shortly but what I find most
exciting about this type of application
is that GPUs open up some new ways to
think about the problem we can think
about all the possible locations on the
map and consider all the enemy bots and
consider all their future positions and
cross-reference all of these things and
let the GPU just do number crunching and
figure out what the best decision is
these are the kind of things that in
artificial intelligence we haven't
really been able to do if you do that on
the cpu we don't quite have enough
resources to do these kinds of things
but on the GPU this opens up new
opportunities so I'm going to split up
this talk into roughly three parts the
first is understanding GPU hardware then
we'll look at the more software side of
things and compute languages in general
and in particular
OpenCL and then I will do some case
studies on three different UI algorithms
which
find the most interesting so when I came
into this project working with
universities that have more Hardware
focus than companies with a hardware
focus I learned a whole lot about the
GPUs that I didn't know beforehand I
think it's very necessary to really
understand the big picture so I find
this very interesting I hope you do too
and this is something you probably
already know that the clock frequency of
most pcs forms are available for the
past 10 years was kind of plateaued out
it's 3 gigahertz and you can
occasionally go higher up than 3
gigahertz
but that's turbo boost and it's only of
a very short period of time so this is
meant that with over the past 10 years
we've put more emphasis and more focus
on parallelism and developing making the
most out of the CPUs that were that we
can build and the big reason for that is
power so on the one hand you have
performance on the other hand you have
power and the two go hand-in-hand if you
have more power than you can boost
performance and vice versa if you have
high performance and you want to reduce
power you could just drop your algorithm
slow it down and it will consume less
power
so if we look at the graph of power
consumption is very similar in it and
reflects the same things that we learned
from the frequency the clock frequency
of the CPUs and this spike up here it's
basically the Xbox 360 and the ps3 those
are the more power consuming devices
that were built in the past 10 years
this is a graph that I borrowed from
Andrew Richards from code play and needs
new spike here's the new consoles and
the new versions of the new consoles are
also trending downwards again so there's
a trend downwards in terms of the power
consumption and some interesting horror
stories that are about devices that were
built to work of 500 watts but none of
the distributors ever wanted to sell
those so it was a difficult time for a
chip manufacturers
to take all their mobile chips and then
turn them into their main processors and
power is still very much an issue now
and if you take your smartphone there's
probably four cores on the modern ones
and if you maximize those over a few
seconds well what happens yeah this will
if there wasn't any Hardware protection
to make sure that doesn't happen
any kind of throttling then it would
literally burn and that's what happened
to the Xbox 360 with the red rings and
many issues that it had with power so
all this points towards GPUs and GP GP
uses using GPUs as general-purpose
computers so why GP GPUs are not other
forms of hardware or why this is not
other forms of parallelism and the big
reason is that GPUs are more efficient
in terms of the world the forest and the
power they do much better than the CPUs
and so the power problem power wall that
we've hit is better addressed by a GPU
than a CPU the reason for that is that
the GPU gives you better control over
where your data lives in memory so you
can say I want my data and bring it in
very close to the actual computation and
that will take very little power to make
that happen or you can have it it's like
a step removed in the workgroup and that
takes them bit a little bit more power
if you have the device memory or you
could have it in system memory and
working with a GPU you have that kind of
control which means you can reduce your
power or increase your performance in a
way that a CPU doesn't really give you
as much control over a third big reason
that GPUs are very important is that you
don't interact with a GPU directly you
never write assembly code for a GPU you
write a shader or an open CL file and
that gets compiled by the driver into
the underlying representation machine
code that's necessary for the GPU to run
but you never see that
so that layer of abstraction allows the
hardware manufacturers to make
innovations and come up with new
instruction sets and new tricks that
improve performance over time and they
regularly do things change quickly
different companies have different
philosophies and they they will do so
and innovate quite quickly and that's a
big reason why Jeep is very very
important it's because there's a healthy
competition for GPUs you can take
existing games and lose the resolution
by a factor of two or four and it's a
good reason to sell a GPU and with
Retina displays there is a thriving
market for a more powerful and better
performing GPUs so if you look at the
trend in general for smartphones the GPU
power on this logarithmic scale is
significantly higher than what is
available on the CPU and this is the
same trend on desktops and this is them
again a graph I borrowed from Andrew
Richards of quickly these numbers are
quite rough but in general that sort of
rough reflects the whole the state of
what things are today so GPUs are
there's more of them now and they're
very good at a few things they're very
good at floating-point computation a
very good at managing large quantities
of information so if you think big
pipeline of data and they're very well
suited to 200 large arrays and images
and this specialized hardware for doing
indexing campaign and all these
operations on images so anything that
falls into this category is a perfect
candidate for working on the GPU the
downsides of GPUs is that they don't do
so well when you don't have a pipeline
of data when you have like little things
that you'd like to do and you're not
quite sure when you will know what
workload you need or if you haven't
random access in memory it's possible it
just really doesn't work quite as well
and if you have
conditional logic or branching in your
code the GPU struggles more so to
understand why GPU struggle at these
things I want to break down the hardware
and this is a processing element inside
a GPU and there are loads and loads of
these processing elements and a
massively parallel type of hardware and
on most graphics cards high-end graphics
cards will of 2048 the more money you
spend the more processing elements you
get and these are like many processors
in a way there's one big difference
however they're not stand-alone
processors that work they work in
computer units or processing units
depending on which terminology you use
so these units basically will work
together so all of the individual items
within one unit are working at the same
time they have a single instruction
counter so they're they're doing the
same execution 16 times in this case and
so if one compute if one persisting
element needs to go down one branch of
an if condition and everyone goes along
for the ride wither and then the else is
also taken by every single processing
element inside the compute unit so this
lockstep execution is why branching code
is not very efficient on GPUs and you
have to be very careful so from the
software side to make the most of this
you'll basically create a bunch of
little work items or data packages
regular pattern and you can describe
your computation as a work item that's
not a function that needs to be run and
then you pass it over to the driver and
it will just assign these on to these
processing elements and figure out how
to do that in this case we don't
necessarily need to have as many work
items as we have processing elements we
can leave some free or we can completely
saturate the GPU and give it lots to do
and that's ideally the best case so in
this case we give it lots and lots of
work groups and work groups or
collections of work items so there's a
loose mapping between the OpenCL
software side and the hardware side so
the work items corresponds with
processing elements there were groups
correspond to the compute units so you
have some control over that but in
general the hardware will schedule
things as necessary so that covers the
hardware side of things let's look at
the other side of the equation there's
software the computer language is in
general and OpenCL so you're all
familiar with shaders this is a great
graphics shader from the guy who created
shader toy the guy who goes by the
handle of IQ and every pixel is doing a
little piece of computation ray tracing
and bouncing off and figuring out what
light is affecting that pixel it's an
amazing amount of computation goes into
creating this image so I'm sure you
could imagine well how can we take this
and leverage it and make it work for AI
and this is another shader that's on
shader time and some more behavioral
shaders doing the same types of
computations per pixel so for every
pixel this shader is deciding where is
the position of all the legs and does
this particular camera ray intersect
with the day and so we're doing a whole
world of computation here just to draw
this simple spider per pixel basis so
and that's the title of my talk they say
if you're going beyond shaders for AI
that we can do better than brute force
per pixel calculation of everything in
the scene using things like compute
languages and OpenCL you get better
runtime performance you can take all
your share data all that a.k information
for the legs the position of the body we
can calculate that once we can store
that separately we can do different
computations for each pixel in witcher
which will reuse that common data and in
general we have a lot more control
for the memory on the execution which
means you can write much more efficient
software and then actually read data
back from there from the graphics card
which helps us build more interesting
applications so in terms of interesting
applications that use OpenCL the latest
Adobe Premiere Pro and can basically do
real-time personname with 4k images and
videos actually and applying filters and
operations on that and it does that by
using open CL to do those heavy
computations on the GPU civilization v
for mac and it apparently uses open CL
and that's what they announced it's and
there's not very many details about this
but the computation is offloaded as well
to open CL because on Mac that's
available by default so they can they
can do a pretty good job of that and an
avalanche game studio in Sweden they use
OpenCL in their tools I believe for
landscape processing and they're working
on a game called Mad Max which would be
out next year sometime and so that helps
them reduce their turnaround times and
do things much quicker by moving things
into open CL so in general if you were
to use OpenCL today I think that's what
I would recommend the first thing in
order of priority of things that are
sensible to do today if you want to
build your tools faster and you want to
take a heavy computation and improve the
turnaround times OpenCL is a very good
choice for that if you have a server
that's running heavy computations of
your game or other calculation and you
want to reduce your power bill then
moving that to open CL is also a very
safe bet to save a lot of money and then
the third point is only the third point
is improving runtime performance I think
it's still quite risky to use OpenCL at
runtime in the game on a consumer device
but that is going to be improving
the next year but more importantly I
think what I'd like to share with you in
this talk at least is a plan for the
future and summarize that two points
that's all you need to know about the
future that you're going to increase
you're going to have to increase your
focus on parallelism that's based on
data decomposition there are more GPUs
out there with lots of little processing
units and you will have to write your
software to make the most of this this
trend is not going to disappear quickly
you will have to make the most of it and
that means you have to think about how
to write software in a way that makes
this possible and how to structure your
code so nathan decomposition is probably
hopefully something you're already
familiar with if you write big for loop
that goes over lots of different options
considering each option separately that
is a form of data decomposition or if
you use vectorization with cindy to do
different elements for each of the
dimensions in parallel that's a form of
data decomposition and to do well on the
GPU you have to take these concepts and
then turn the dial all the way up to
eleven to do a good job of this so in
practice what that means is that you'll
have to write kernels and a current is
basically an algorithm that's expressed
in a data parallel way as a single
function so you write a function whose
result is the emergent result of this
algorithm so instead of having one big
function language which does some
computation you'll break that down into
lots of little computations work items
so here are some examples of
computations that are done in kernels
that i've shown you
of earlier this is position evaluation
so the blue team is picking ambush
locations to try and intercept the
flight carrier the yellow points are
highly desirable have good visibility on
the ambush path but are not seen by
whoever is on on the ambush path
carrying the flag so these computations
can be wrong on the regular basis to
make these decisions we feed in the
results of two other kernels this is a
shortest path algorithm and it
calculates in yellow the shortest path
which is presumed to be the ambush path
the word that we'd like to intercept and
then from the shortest path we can
calculate the distance which is the
grayscale image so the further you are
away from the path or the whiter it gets
and then third kernel it's calculating
visibility so it's speculating in the
future for each of the little red guys
what could he see in the future and
projecting along in a future path so in
this case we can decide if hiding behind
the box is a good idea based on our
prediction of what the enemy is doing so
to write that kernel as to write that
functionality that algorithm as a kernel
in
OpenCL we basically write a function
called evaluate ambush and there's a
keyword kernel I often see our says that
this can be called as a as a function
from the CPU and this takes two inputs
an array of distances and their
visibilities which are both read-only
and the output is a set of scores for
each of those cover locations or the
ambush points whether they're desirable
or not when you execute this kernel each
time it's executed it will get its own
unique index and so we can run that many
times and
different competition for different
ambush location and then here on this
line we have a heuristic utility style
calculation which uses square root and
some clamping to decide whether it's a
desirable location so it might not
necessarily make sense to do this kind
of computation on the GPU because it's
doing very little work and you could
easily do that on the CPU you might not
want to spend the extra hassle of
writing this in OpenCL and offloading it
to the graphics card and you will have
lots of little work items which will
saturate your GPU with their work my
tones are not really doing very much so
it's not necessarily the best current so
a better variation if we combine this
with what's on the previous slide in
this case and I'm passing in an
additional range parameter which is
saying evaluate the ambush locations
between the starting value in an N value
and calculate the ambush score for each
of those and then we compare it with the
best and if that's probably the best
restoring as the winner and then
so basically we're filtering out options
on the GPU itself so that we don't have
to do those types of sorting operations
on the CPU which is a much better type
of computation to offload pretty more
work on to the GPU so the CPU only has a
few really good options to to process so
in this case we can probably break that
down and saturate a few work groups and
it's a good type of computation to have
running on the GPU keep in mind that
each one of those functions will run
each group of sixteen will execute at
the same time so if we're going through
this for loop the ones that are from
zero index 0 to index 15 will execute
this program at the same time so if this
range is really long for one of the
items and really short for another one
then the one that's really short will
have to wait for the worst case every
time because all the work items are
executed in lockstep execution so
something that's important to keep in
mind
so it would be best if we had the exact
same number of ambush locations for each
workout in terms of integrating this
into until your own code if you use
scene and you can write something like
this you create platform divides the
context although these things you need
to interact with your graphics card I
have some great tutorials about OpenCL
online I'm not gonna spend much time on
this you have to load your kernel file
from disk you can pilot create a program
and a run time in your tick function you
can then create memory buffers and put
your data into it and let the OpenCL run
time pass the data over to the graphics
card and let it do all its logical
reasoning processing of ambush points
and filtering sorting so in general
that's the approach that you take to
move AI onto the GPU you identify an
exposed forms of data parallelism in
this case we have lots of ambush points
we can split them up into individual
ones or groups of ambush points we
rewrite a function which could have been
C++ or any other code that we would run
on the CPU into OpenCL which runs on the
GPU and then we use the open CL runtime
to execute that code onto the GPU sounds
simple in theory and I'm going to show
you four more examples in practice
so actually based on that really quick
overview OpenCL I want to ask you a
question do you think that a star and a
star search algorithm would run on the
GPU or raise your hand if you think yes
it will run on the GPU okay raise your
hand if no if you don't think I will
Roland's GPU actually mostly knows
answer is yes it doesn't run OpenCL is
the form of c99 has a few limitations
but yes you can write a search algorithm
in C obviously and it's relatively easy
to write you just need to implement a
bunch of functions for priority queues
push this into this heap and pop
something off the heap and so yes it
does actually work on the on the GPU
follow-up question do you think an
a-star search is faster on the GPU raise
your hand if you think it's faster okay
and do you think it's slower raise your
hand feeling slow about three times as
many think it's slower um it's actually
slower in the case of capture the flag
we measured it 73 times slower it
depends a lot there is a lot of
variation sometimes we measured it over
100 times slower it's on the size of the
graph and the exactly how there are
certain searches that will not run on
the GPU because they're too big they
don't fit into memory and the
performance depends a lot however if you
run many many searches in parallel it
can be up to twice faster and it has
some very special conditions for it to
run twice faster if you have lots of
very similar searches that have very
similar lengths then you can really make
the most of the GPUs otherwise it tends
to be a bit slower and there's a lot of
research in the little is taking a star
making it faster on a GP
but it's not necessarily the most
productive way to spend your time so
this is a graph showing the performance
as you increase the number of a star
queries if you have like 2048 queries
this is how fast they will execute this
is the linear scale so it grows much
quicker or much grow some linearly until
it crashes like here you'll run out of
memory some stage and stop working no
but if we zoom into this you'll see
there's some steps a regular basis of a
256 and 512 and these are basically
properties of the hardware that we can
measure by running this algorithm on the
GPUs so as we do a search number 255
it's still only needs 16 work per se but
when you do number 256 you'll need an
extra work group and so it gets an extra
jump in computation time so you can
measure that in your hardware and their
API queries quick to tell you exactly
how how I will deal with things but it's
it's interesting to see that the time
measurements actually reflect what what
the API is tell me so in a way you're
probably thinking about this is cheating
right you're just taking what is a
sequential algorithm like a star I'm
running it in a massively parallel way
but it's a sequential algorithm so we're
just taking a star instance and just
making individual workloads out of them
and well I think it's a fair thing to do
because you can just take almost any C
code even if it's serial code and just
move it over to the GPU and the only
thing to keep in mind is that your work
groups will have the worst case
performance so you have to be very
careful about grouping together your are
different workloads another thing to
keep in mind is that a star takes a lot
of memory so on each work group you may
only be able to run 2 or 4 out of 16 or
32 so you can't maximize your use of the
GPU Hardware with an advanced
memory hungry algorithm but this
approach of using what is a sequential
algorithm is used very often in ray
tracing so I picked some shader earlier
pixel shader every single pixel is is
tracing those that's my training flow so
ray tracing is basically massively
parallel as well but sequential
algorithms run on every work item so if
you have a voxel ice representation
everyone like this one yeah so described
as as mini cubes that you can trace your
race through then it makes perfect sense
to think of that as even though they're
sequential algorithm to trace array to
do that in a massively parallel and so
one way to do that there's an algorithm
called a digital differencing analyzer
DDA and for short and it's a nice it has
some nice properties it's a basically as
some looks like your line tracing
algorithm so if you click the corner of
a voxel it will tell you and it's very
accurate anymore it's well on the GPU if
you were to grab the code the pseudo
Covenant implemented in C it would look
a bit like this doesn't matter too much
what it's doing you can find this code
online and then when you plug it in and
send it over to the GPU I'll give you
this so and this is one of our
capture-the-flag maps for example and so
we've implemented in lots of different
variations of this algorithm and this is
doing a bunch of great races together as
a wave basically doing it faster so we
do it on a 2d based image but you can do
it on a 2.5 D image as well tracing all
these images and finding if one point is
visible from another problem so even
though things like a star or ray casting
are sequential algorithms and have lots
of branching logic it's or random access
in fact it still makes sense to put them
on the GPU in some cases so
to learn here so now I'd like to focus a
bit more on the parallel side we've done
a bunch of sequential algorithms and put
them onto the GPU and they work
surprisingly well but we can also do
we'll take the next step and the next
step is effectively to make things even
more parallel and a date a parallel
algorithm is going to break up its
workload into multiple different work
items so in this case we could have a
group of sixteen different workloads
which is making on the result of a for
example a path finding algorithm we can
do those at the same time so writing
this type of algorithm it takes a very
different mindset and you might have to
grab one of your computer size books off
the shelf and then dust it off and find
some old algorithms from the 50s or the
60s things that we've discarded because
they were slower on sequential Hardware
but now that makes sense again on
parallel hardware and so taking that
different approach means that you often
get a lot of data for free you're doing
a lot of extra computation but you get a
lot of extra results out of it and for a
that's a good thing because you can
generally use that to improve the
quality of our decisions and make better
decisions about the world so if you were
to take a more sequential type other
than finding a path from A to B it's
called a single pair path file it would
look like this yellow line that is a
single pair but if you were to use a
more data parallel approach you could
treat that as a single source algorithm
which basically finds the path of all
the cells in this particular grid to any
point so in this case we're finding the
distance to the yellow path and that
gives us a sense of how far we are from
the path that we're trying to ambush so
the darker it is the closer we are to
this path and the bots can then say well
am i within 5 meters of this path Aniki
you can do a query based on this so the
algorithm that we use for that is called
bellman-ford more and that's an old
algorithm from the I think it's the 50s
or 60s it was invented by three guys I'm
sure you can guess their names it works
on the grid on a 2d grid like this but
it also works on graphs so if you have a
graph based data structure this is type
at a wake point type network you can
have a navigation mesh is based on
polygons and as long as you can express
your graph data structure with indices
and say well this this is a vertex and
has a connection to all these other
vertices as long as you can put all of
them in this information into a couple
of c-style structs and arrays then you
can run this kind of computation in
parallel so the way the bellmen form or
algorithm works is actually like an
image blurring algorithm it's completely
brute force for every single pixel I
will repeat the neat check if it can get
to anywhere else quicker and if so it
will update its distance and then it has
an internal pointer that it can update
so we're basically doing this blur
algorithm over and over and in the worst
case you have to do it any times but in
many cases you can sort of terminate the
loop much quicker so it's kind of a
extreme brute force but it's very very
well suited to the GPUs it works on the
graph as well you just check all the
neighbors can I get to the goal location
quicker by my neighbor if so I update my
distance the thing about the
bellman-ford meme or algorithm and most
types of parallel algorithms is that
they require some types of
synchronization and coordination so if
you're doing path mining you will need
to update each one of these nodes at the
same time if you don't update them in
lockstep then your you may not get
accurate results or you don't know how
long you need to continue until
everything is converged so if you're ok
with having suboptimal results you don't
need to execute in synchronized
iterations but if you want high quality
guaranteed to be correct results you
have to
synchronize each of the iterations and
that kind of synchronization or
coordination between the different
processing elements is something that
you can't do in shaders when you're
rendering something on the pixel you
can't ask what the neighboring pixel is
doing works with the compute language
like OpenCL you can do this kind of
synchronization coordination
so for to take a more architectural
approach of the algorithm the way that
it works it takes the the graph data
structures these trucks of arrays and
copies them into the GPUs memory and/or
the work group or the compute unit and
then we wait for each of the work items
to finish doing their local blur step
basically and then we keep doing those
in synchronized iterations and then once
we're done we can copy the results back
over to the CPU and this works fine as
long as you have enough memory to put
your graph in inside the single compute
unit downside of it is that when you
want to scale up and you want to handle
larger maps and you'll bump into memory
limit problems and you have to then
start synchronizing between multiple
different workers so it's not
necessarily the most easy thing to do so
you may have to use the CPU to help with
the synchronization in this case so this
is the kind of thing that's OpenCL 2.0
is trying to address to make it easier
for us to do that without having to go
back and forth between the CPU and the
GPU but if you want to scale up
conceptually the way to do it is to
break down the map into lots of
different subgroups of 16 by 16 or 32 by
32 put each one of those inside a
computer unit and then we do micro
iterations within each one of these sub
graphs and then once these micro
iterations are finished we check to see
if anyone still got work to do and then
we just trigger another micro iteration
it starts again and so even if you have
a very large enough of that order of a
kilometer so you can do the whole thing
about
five six seven iterations so to do this
you need the CPU to synchronize all of
this so in general now writing efficient
AI algorithms and making use of the GPU
involves trading off three different
things you have to first consider the
fact that GPUs are not very good at
branching logic and they have to execute
the worst case for every single person
element so they have to go down every
single if branch even if that unit is
turned off the GPU the interaction with
the GPU via the driver as a measurable
overhead and so every time you make a
request it will cost you so you have to
really take that into account and then
if you have to prepare the data on the
CPU then and I also require seven
sometimes so you have to basically find
a balance between all of these things to
increase your performance or to reduce
your power consumption and this can take
a bit of iterating tweaking tuning so
one piece of technology that's
relatively new is a combination of open
CL and C++ together and this is exciting
because it helps you iterate quicker you
can you can sort of find these
trade-offs find the balance easier by
using C++ instead of having to write
something inside a C file and then the
rest of your code is in your game engine
and c-sharp or you can write everything
in a common language and that is a
standard called sickle its provisional
standard from open CL and it will be
released soon so we're working with some
early versions of that and AMD has an
open-source version of this which is
still very early stages but it means you
can really strive towards this idea of
heterogeneous computing making the most
of all your different devices that you
have
most game developers have an integrated
graphics card on their machines and the
discreet graphics card someone from
Nvidia another one from Intel and most
of the time now Intel device is not
being used so if you have some kind of
support for heterogeneous computing in
your application you can make the most
of these unused GPUs using for example
C++ last example I wanted to go over its
just a quick one kind of an to
re-emphasize the same place we have a
visibility cache for for ray tracing
which allows us to discard certain
calculations and we basically generate a
large lookup table to say if point a and
point B or visible together so this is
our capture the flag maps they are
roughly the size of a football field and
then for each pixel we check if every
other pixel is visible and I'm going to
create a large lookup table and this
lookup table looks something like this
and if you zoom in we're basically
having a version of the map for each of
the pixels basically it's an all pairs
lookup table and so the first versions
we did of this took like a couple
minutes to calculate and if you offload
these types of things on on the GPU you
can get them down to a couple seconds
and so this whole thing in memory we
keep a compressed version of it it takes
a couple megabytes and it really speeds
up our runtime so for us it was
worthwhile it's not necessarily
something I'd recommend for everyone but
it's just one example of the kind of
computation that you can get faster by
multiple orders of magnitude by
offloading that type of work to the jus
so if you have to write this kind of
thing in sickle a C++
it looks like C++ 11 have things like a
command group which takes an open CL
queue and then you have a bunch of
accessors you can say please make this
buffer here accessible to the GPU and
then with those accessors you can say
well I'd just like to do the following
code inside a parallel four and this
parallel four is basically a
two-dimensional with grid height grid
width and the work group sizes are 8 by
8 and then for the here of sort of
expanding this out over here and then we
just do a brute force calculation of all
the disabilities for all the pixels and
this is standard C++ code it works you
just have to include some sickle
implementation and you can get this to
run relatively quickly on their GPUs it
takes much less effort to do that then
the tech is still very early in this
early stages and we've we're working
with one set of technologies it's still
in alpha it's been painful a few months
but the last versions are getting much
closer and that much more fun to work
with and very reliable so it's a
something that we'll be using more on in
the future so to sum up it's not magic
you can't just take something you can
put on the CPU and then it's all of a
sudden it's it's better you just can't
accelerate certain things or get a bit
more data in some cases so the GPU is is
a resource that's available for you and
more importantly GPUs are becoming more
and more prevalent there the ratio of
computation power that you have
available from a GPU versus a CPU is
going to keep growing because of the
power or the power rule that we've hit
that I mentioned in the introduction and
to prepare for the future you need to
think about data parallelism how to
structure your algorithms in a data
powerful way
I have the right kernels like well as I
showed you and actually also another
takeaway is that many AI algorithms both
standard ones even if they're sequential
algorithms can just work on the GPU and
with a bit of tuning you can get them to
work significantly faster and old
algorithms from computer science from
the early days that you can take those
and put them onto the GPU and they will
actually perform better and so it's not
just graphics that works on the GPU and
to me it feels like the Wild West of
computing and you know all the problems
you had with graphics drivers 5-10 years
ago and they're very unreliable on PCs
especially and well that's the case
right now for a PCL they're very
unreliable things in the process of
preparing this demo I think it must have
crashed about twice not to completely
reset my laptop and so it is no always
ready for production use but there are
some cases where you can get multiple
orders of magnitude extra performance so
when you get that it's worth it it's
worth in you the shoe tax but it is the
future and you have to prepare for this
and I'm just planting the seed in the
back of your head may not be ready for
it now but a few years you'll realize
that we have more and more hardware
there is data parallel and if you use
some of these ideas from today you'll be
a step ahead already ok that's it for me
thank you
yeah of course if you have any
yeah so I think the big problems are
driver related and you get widely
different driver implementations so
Intel and AMD just recently released
some very good quality
drivers which I've been very impressed
with but they're not installed by
default you have to somehow get them to
install it it's the same problems that
you had with graphics drivers before
steam automatically installed them for
everybody you're gonna have to solve
those problems for things like consoles
it's much easier you could do that
because the hardware is available and
you can make that that investment but
your graphics programmers are probably
going to be fighting for the GPU
resources in consoles are certain level
of expectations with graphics quality on
consoles so there's a mindset change
that that may not happen this generation
so yeah I think the risks are in terms
of the deployment on the drivers and
installation of all that I think it's
it's much easier to control an
environment on it on your own developers
machines to speed up their tools you
have control of that environment where
so you don't have control over your
customer hardware does that Masseria
question
I have not looked into webseal if it's
open seal on online language it sounds
good
I think you run into the same driver
issues yeah no I know WebGL have just
not used the seal version so I think you
still have the same issues of this guy
doesn't have the latest drivers and it's
not installed on this guy's machine
there'll be a small subset of people who
can correctly work your your program but
if so if the browser developers ship
some OpenCL runtime with their browsers
and they take control of that
environment make sure there's a fallback
mode to use the CPU if the GPU is not
available if they're doing that then
it's probably worth looking into if
they're not doing that yet they're gonna
still have the deployment problems of
getting it to run reliably on people's
machines
yeah so there are two levels of the
iterations as the the micro iteration
and then there's a macro and then
they're all sharing the same memory but
within the group I can tell if something
has been dirty so there's a dirty flag
basically that says oh and if something
has changed then the CPU gets notified
and then we trigger another iteration
but all the memory is shared so graphics
cards are very good at storing very big
textures so even though the workload the
computation is split up into little bits
the underlying texture is still a very
big bitmap and so we load copies of that
bitmap into local memory to make it
faster reduce power consumption but then
we can copy it back so the data doesn't
change it's we just need to track that
dirty dirty flag and have the CPU
trigger and the iteration
I I don't know so on the consoles on the
consoles you have a very specific way to
allocate compute units to do specific
things and I don't think they really
thought about that too much would they
well the versions of open CL are
available today in the standard I'm sure
there's a way to like specify how you'd
like to have it allocated but well so
currently the way it works is basically
you just tell the GPU to do stuff and it
tries and does its best to schedule
things but it doesn't go very good job
and so the consoles have solved that
problem by giving the developers lower
level access and I hope that something I
open CL 2 has better control over that
so you can say please run this on a
couple work units but and I suppose
something we've been able to do on pcs
with the current museum
so I think of all the computing
languages if one is to become dominant
its OpenCL so I think OpenCL by being a
standard and being supported by more
hardware companies and manufacturers it
has the bigger chance google is very
political about its use of renderscript
and nobody really likes to work in
renderscript the world is very much
encouraging people to use that there are
OpenCL implementations for mobile phones
they're just not allowed to enable them
you can actually I think if you flash
your phones you can sometimes access be
hopeful CL implementation that is
actually there it's just not enabled so
I think it might take a few years for
the politics to work themselves out so
it's not just a technical problem but a
political problem and similarly I think
in videos dragging its feet will open CL
and videos got this vested interest with
CUDA and so the open CL implementation
you can get from videos like I think
it's a one point one implementation
whereas everyone else is on one point
two or 2.0 so there are some political
issues to sort out but I think if you're
to invest in technology it's best to
invest in a more open standard open CL
is higher quality than renderscript for
example and kuda i think is only nvidia
specific so you're kind of locking
yourself in but yeah the political side
of things is not it's not great yet but
I imagine they will sort it out because
this it's it's a problem that's not
going away and this is it will be here
will have to deal with it and so
OpenCL is that is the best choice less
wife that's why we did all of this stuff
in our PCL I expect most phone
manufacturers will have a version of
OpenCL at some stage in the future but
it's all about the politics of Google
letting that happen any other questions
okay thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>