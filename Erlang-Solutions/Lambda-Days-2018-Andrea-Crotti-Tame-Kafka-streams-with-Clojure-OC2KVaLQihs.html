<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Lambda Days 2018 - Andrea Crotti - Tame Kafka streams with Clojure | Coder Coacher - Coaching Coders</title><meta content="Lambda Days 2018 - Andrea Crotti - Tame Kafka streams with Clojure - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Lambda Days 2018 - Andrea Crotti - Tame Kafka streams with Clojure</b></h2><h5 class="post__date">2018-03-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/OC2KVaLQihs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi everyone thanks for coming so today I
will talk about Kafka enclosure and how
they work together and what we use them
for so first of all I I work for funny
circle in the UK is the biggest closure
shop in the in the UK even if it's not
the big but for closure is a lot of
people and we we are using closure in
cash guys can come the main pillars of
the architecture and I only joined
recently got three four months ago so
before I was doing Python and databases
and other stuff for for a long time so I
I'm not claiming to be a tough guy spurt
so forgive me if I won't know all the
answers but I'll do my best so so just
to show what we're going to talk about
first giving introduction to Kafka
because how many people actually use
Kafka or well use it for work okay yeah
good good more than I thought and
closure and they took together oh yeah
nice nice okay so I I guess yeah I still
have to kind of introduce cask a little
bit and displaying the main concepts and
and why it's good and then just show how
they work together cask enclosure and
show a little demo and then discuss a
more real-world scenario from our actual
production code in a way I wanted to
show more from the demo but it turns out
that this is quite difficult to to
actually show production code without
having to show many thousands of lines
of code which is not open source and so
yeah the example would be quite simple
but hopefully will be nice enough to
understand how what's going on so let's
start with the Kafka and what it is it's
a distributed append only immutable log
what that means is that you have a kind
of a central server which can be however
distributed in a cluster
many different machines and there is
somewhere in like a it represents a
unique and immutable log which events
get added one at a time
in an append-only fashion and the
important thing about this is that is
extremely scalable because you always
have a kind of a window where you look
at things and that means that you can
you will always take the same amount of
time to do a query in Kafka or to do
some sort of operation and that's why it
was kind of it's a very big project as
well mostly Java and Scala and he also
used the zookeeper for distribution
which is another kind of beast of a
project but generally you don't have to
worry about that too much you just fire
it up and it does all the magic for you
and it was actually done for scalability
because it was created internally by
LinkedIn and then open-source in 2011 as
I said listen a relatively new project
but he just got through version 1.0 in
the last well in September October I
think so it's actually now considered
stable even if a lot of very big
companies were using it already before
and by way if you have any questions
while I'm going through things we can do
it more interactively as well okay so
the components of different the kafka
are clustered in a way moved mostly for
you have a publisher if you consumer API
stream API in a connector again
publisher allows you to publish messages
events to a certain topic where you say
this thing happened and you say on this
particular topic consumer where you can
actually consume the topic all all once
readers may want and the more important
ones more interesting for us in a way
are this
API and the connector API so the
streaming allows you to actually stream
data from topics in real time and do
nice computation with that like take
some data from here take some data from
there join it together and do some
computation and then output to another
stream and then the other important one
is the connector API which is a way to
connect the kafka to other sources of
data like any database early both taking
data from them or outputting data into
them so let's have a look at kind of an
example of what a topic looks like and
how from like the official more official
images I found so here you have three
partitions partition can be on different
nodes in the cluster potentially and an
impartation have the number here it's
the offset and then someone can just
write like a producer can write on these
different partitions on the particular
offset at any time and the important
thing is that things never get deleted
or change and that's quite an important
property of the system so that here is
how for example consumers are looked
like so you have consumer groups
consumer group a consumer group B and
they are grouped together for various
like technical reasons and then you each
server is as different partitions and
the partitions themselves will be
eventually kind of redundant so so if
someone is with node goes down then one
node in the cluster that was the leader
for the partition is not the leader
anymore and another one and that that
all that distribution stuff I think is
mostly handled by zookeeper so yeah so
each consumer can then like listen to a
specific topic which is on a given
partition and then get data and consume
data from it and last thing is that yes
you can
you can consume and produce the same
time of course and multiple consumers
can have different offsets so the
consumer remembers where it was left off
breathing so then it doesn't convert the
same thing twice for example and lastly
this other nice graph represents what we
what we actually do in a way a funded
circle more or less so we you have data
sources and then use Kafka connect to
bring them into Kafka and then you use
Kafka streams here to to produce other
topics and then these topics are again
written into some other databases so the
databases are then not the source of
truth so cafes always the source of
truth and in our case at least and the
databases are just used as kind of a
temporary way to allow the actual
different micro services to do all the
nice things I want using proper language
like sequel instead of trying to do what
the fix I mean all the temporary things
that doesn't need to be exposed to other
to other projects micro-service can be
in a database inside the service and
everything else there has to be common
knowledge or anyone could put possibly
then has to be an orchestra topic
somewhere so that's that's really really
short introduction about has been did we
have any questions I said it was the
capital was actually chosen and I think
a couple of years ago with the with
closure eyes is a way to try to scale to
to a very large number of customers
eventually and so the main reason was
for scalability but it's not really the
only one because data data integrity and
auditing are also extremely important
since we are FC a regulated business now
in it also means that things that you
cannot just go in like a normal business
as the database and change things
randomly and not actually have any trace
of who change water sample I've seen
I've seen many companies that for maybe
for certain tables you implemented the
audit log and then for everything else
no no new and and the most funny thing
it was one company where they discover
we discovered actually after both as
almost 3-4 months that they all did log
for some important table wasn't working
anyway so we didn't have audit log for
changes on this important tables or for
ages and yeah auditing and didn't agree
that was quite important six and then
the other thing is can allow you to do
micro services in a different way I
guess and you can you don't have to
worry more so much in how to share the
data between the different services
because that's kind of a soft problem
you have other problems maybe but at
least you don't end up having all these
api's which are then kind of never
enough to to get all the data maybe to
slow it you have all the data in my
place and then you who have needs the
data from different produced by
different microscope you can just
subscribe to it and depending on what
they need and I want to go through this
it is funny this funny graphs from the
confluent for confident guys which
converge shows what the problem is and
you start from nice need to micro
services and then you have this API of
the services that are always too
limiting for what you want because maybe
you want to have you don't want to do
three calls but you want to get
everything you want and that that's also
why by the way things like that girl are
quite successful data that kind of
tactical this dispatcher problem and
then if the services code is too
limiting can we change both together at
the same time easily if you say yes then
I just make the contrast a bit brothers
by changing the code somewhere
if it's a shared database then well is
it if it's a shared database then it
will cost a lot of money and then you
try to encapsulate again especially when
you actually scale scaling up vertically
databases in RDS for example gets really
really expensive really quickly if you
throw the shared database then probably
again the service contract is too
limiting because no matter what you do
people number you just want to get to do
some specific queries that normally if
you do them with an API is always a
thing if you if you cannot change both
services easily then I just want all the
data and then you come up with some
solutions to to sync the data from one
place to the other no you can use even
things like a lasted search or you can
do all sort of stuff to try to to get a
copy a live copy of the database on the
other side and if you actually try I
mean the data will diverge at some point
because of course you can have that for
the physical laws you can have the same
data in two places and expected to make
assistant and then
okay let's centralize everything again
and then you end up again the same
problem so it's kind of a it's an issue
with them with data center companies
that even though you you want to try to
to scale up so I have multiple micro
service and so forth then you end up
having really a lot of like headaches
with trying to to actually do it because
your job done so that's why also the
other fun reason why did cascade guys
really push push for for it and yeah
let's look like a closure now I don't
I'm not gonna give like a full
introduction to closure because I would
take longer than I expected most people
have seen it already at least but just
some of it I some of the points that are
important to also understand why it can
be a good fit for for working with Kafka
so first of all is a functional list
by functional can mean many things but
he has for example all the values and
map filter reduce all these nice little
functions and calf gain away had Kafka
streams API in particular is very
similar in this very he or he also smoke
values map that filter all these
different ways and then by default if
everything is mutable enclosure which is
quite important by default I say by
default because you can do mutable
things if you want but normally you only
really do them for really specific
performance reasons so or similar things
like 90 maybe 95 percent of times you
never need to worry about oh I should
maybe more than modify this back to this
vector in place instead of creating a
new one because yeah you really not know
most the time you never have this issue
and you know a Kafka is also mutable
because it's just like an immutable log
so that's why they're kind of related
and the data centric is also very
important I think everyone heard about
the data is called the mantra in closure
it and just means that you can since the
language is of iconic data is bold and
CODIS data so the two things have kind
of interchangeable in a way and the
other thing where that is quite
important for the community for the
language ecosystem in general is that it
data is superior to functions and
functions are are better than 10 macros
so whenever you can you should try to
keep kind of things in in maps and
vectors and similar data structures
which are easily manipulated by all sort
of tools and then if you if you cannot
do that then just write a function if
you really instead the function is not
enough again then then write the macro
again yeah this works really well with
cascada
kind of simply like a big collection
anyway and then the maybe more
importantly is based on the JVM so he
runs natively on the JVM and there is
also close to script and CLR for example
and but the JVM interoperability is
really it's really nice to use can get a
bit annoying if you really have to do it
all for every call but for that you just
need some little wrappers and then you
you don't even notice anymore you're
working with Java libraries for example
because this means that we can actually
that call the internal Kafka stream
functions in Kafka libraries directly
without having to today I know people
use them also for example sunny circle
use them with Ruby or Python and and it
works but yes it's not not nearly as fun
a pantry another thing is that this is
very different poll experiences is great
I mean I think is the language I've used
one person maybe at least but it's the
language you every time you can reload
everything all the time you can inspect
what's going on you can see the output
of function that I clean in your coders
or we show you in a minute um so yeah so
the conclusion of all this I think you
can say maybe the closure and Kafka are
isomorphic has this morning said the PD
brother says so they're not ready but ok
so and now let's have a look at some
real code so forgive me for the
simplicity of the actual example but I
just have to show something that I could
explain from top to bottom
visually I don't know how is they the
same
coming up yeah maybe see big enough
hello one more
okay so this example all it does really
is that it looks at in topic corner
words which is here and the counts all
the word counts all the words and while
it count all the characters in in the
words and any output that in another
topic and the count so first we have
here the Declaration of the namespace
where there are some requires just for
the logging and environment the
environment library and then here this
is important piece of interoperability
where the import actually imports Java
Java classes and not and not closure
classes and that means from this
namespace import these two classes for
example and then this junk class is just
to get the execute to get the uber job
where it's unimportant in this case then
after that here there's a map to to know
which is the input and the output topic
and and now another map which declares
the configuration for for this Kafka
class that we are using locally so there
is an application ID age is the name and
then the bootstrap server config is
there is the port and the host where the
Kafka broker will be listening on which
is just the kind of default one and then
here they serve the class configure both
share the config is something not going
to go into much detail but is how you
declare that the messages are serialize
and deserialize which in this case is
just it's just a string you know where
IE if i wanted to show something be more
complicated like we have in our case for
sample would be quite hard because even
the json serialize and deserialize is
not part of this library so you have to
convert yourself and and we have it and
part of our code so that's why I also
had to show some quite simple example so
now the actual function that does the
counting is just this war town
and the interesting thing is here that
he simply is simply a pure function
wiper from the login maybe if you want
to call that and pure but if I just pass
something here it simply returns the
number of characters in there in the
world so that's what it does now I will
find a builder now the builder is the
thing that actually construct the stream
k string builder as a global constant in
a way and then here is the important
thing is where you actually define the
the topology in this case so the
topology you first you first take this
you called so actually before I do that
these these syntax methods of this
syntax is part of the Java
interoperability things and what it does
is that it transforms to in the
equivalent in Java would be object
method and then under argument and yeah
so this is kind of a special syntax and
if I take this in spand and it then it
will expand to dot object method or
other argument where dot is a special
form so anyway just this to say that
this is the part of the Java
interoperability and working with them
with the toshka streams and we have in
general libraries you have to do quite a
lot of these things but at the same time
for actual real life projects there are
million libraries to encapsulate all
this enclosure and just show you nice
interflour closure interface so we're
just showing you the bear the bear like
row thing you can do but it's not how we
do it actually
because we felt pain so anyway create a
stream and then you pass the stream an
array which contains the name of the
inputs topic and then here is you map on
the values because in this stream can be
is formed by of keys and values where
the key will don't really care about in
this case and then the values are just
different words and then here we have to
do this reification thing which is again
part of the the Java interoperability so
to make this function satisfy the value
mapper interface basically that's what
it means and the value mapper interface
expected that you take two arguments the
key and the value we don't care about a
key and then we just wait whatever not
and then we just return the world count
on B and then finally the result of this
thing will be written out to another
topic which is with this dot two and any
questions so far
yes yep
yeah you
yeah yeah we show you now in yeah yeah
yeah we're showing the code the code for
first and then show how I will run yeah
anyone else okay so then we have a
configuration here and then the streams
is what actually controls how the
streams run and and now the main
function simply start the streams and
then it stops after like a while so I
show you now how how this runs and I go
to the terminal so I'm not sure if you
can read maybe so the moment we have
three things terminals that are really
busy no way which is one is the
zookeeper cluster but the other one is
the Kafka as the other one is the actual
world count and just kill the word count
I think I mean to actually run I can
show you how to run on the calf crack
cluster itself with some very
interesting is just one command from the
confluent thing is just being Kafka
start and for the zookeeper is saying
and what we actually do in for our
development in general is that we use
just docker compose so then they are all
done in a simple way and there is the
docker compose file in this repository
as well which you can have a look at so
now if I so there are two topics that
were saying so the interesting thing now
is to first I can look at the topics
with this command
Casca card I can kind of consume and
produce things on topics directly this
the world and here it tells me that
there are two elements which
hello world already in the topic and if
I do the same for words count because I
ran it already before well it tells me
there are also two elements here and
this five and five which is the length
of this two words so if now I actually
add some more things to the words topic
and they kill it now I check again if
they're dead
yes so I add the two things and now I
consume instead words count and I start
the actual Kafka string
he will output a lot of information yeah
okay yeah what so he what he did is that
he went through the the things he didn't
receive already and because it any new
word stop and then he will hear this all
the information about the configuration
and all the kind of stuff and then you
simply output here five and ten into the
topic any other questions about this
example very trivial no okay
okay so if we go back to the actual
presentation I this is another thing
which I which is actually what I
originally want to show but then I
realize it was way too complicated to to
show without hearing all the code and
this word for example we do in in our
team which is reconciling repayment and
transaction bank transactions mostly I
mean it's not the only thing we do but
it's part of in a pipeline anyway so
suppose you have some payments and
transaction and come from different
sources actually here I didn't do the
graph per click so they one come from
one topic and yellow comes to another
topic and then you want to know if these
things reconcile so they are and would
to reconcile they have to be the same
amount and they the date have to be kind
of compatible in the right way and then
some other rules potentially but that
these are the two simplest one and then
when it comes is then you output here on
the left into the reconcile repayment so
you put them together and then you out
to them and when they don't assign you
out
the repayments and the transactions
separate areas are reconciled so that's
how how can a simple pipe that would
look you have to import topics and then
you have three output topics for the
actual result if I think about
implementing this in a in Kafka and
Kafka streams what you could do is that
you first you ingest the a favorite
topic in just the transaction topic by
subscribing on them the topic with a
string and then you join them together
and then you amend this join these join
records with the some information about
if you have reconciled or not so this
way just adding a field and then once
you've done that you can branch on that
and to the various topics I will I will
show you now they actually I will not
show you all the code because it's a bit
too much but I show the actual data flow
how the data would look like in all
these intermediate steps just to be a
bit more clear so suppose you have a
bank transaction topic here and the
repayment topic here where you have just
two back transaction and one payment so
the idea is just identify which would be
a usually in our case but just a number
here and then you have the amount and
the value date and the repayment you
have just one repayment and with
effective date which is kind of a
similar thing but just different name so
this day this you can consider this the
beginning of the of your pipeline so
from here you first join everything
together and compute if they reconcile
or not so by joining them I just put
transaction in in in a sub map in a way
and the repayment here and then I called
a function on this pure cab map that
tells me these two things reconcile in
the add the fields they're here and so
the first for the first one the
I transaction one new thing one
reconcile yes and the second case they
don't so here is false
so once I've done that I can actually
branch and then the output could be that
I I did one so if you been bank
transaction ID one and repayment ID one
and this one
reckon size and then this one goes to
the repayments
unreconciled thing is that because it
didn't so and then the very important
thing this hole in this whole scenario
is that what is actually the business
logic if you have a bit of wrappers
around the calf character NAL's and all
that the actual business logic is just
pure functions that all they do is that
they take some maps they do some checks
there's some computation and then they
return some other maps for example and
that's like 90% of the code will be will
be like this all the rest will just be a
bit of wrapping around and and the
orchestration but that's it so if I had
this reconciled function which is some
kind of a convention in a closure to a
question mark for things that return
boolean for example and then I pass here
this map and then this'll map and then
it returns me true and here only needs
in a way is that the record the map that
contains the back toss action as the
these two fields and the other one has
these two other fields and that's it it
doesn't need anything else and the other
interesting thing is of course that you
can also use closure spec and semantics
for actual production in the actual
production thing every topic has its own
Avro schema definition which then
ensures that you have a schema that you
know what data you have but for example
we are also starting to generate our
schema from kosher spec definitions so
you can use this dispatch and then you
can can validate your data that way and
then the spec also generates the Avro
schema
which is then used in production to make
sure things don't fall apart so yeah
also these examples are part of that
project if you want to look but I can
also just to show you quickly that if I
go to that core feature here somewhere
and this is the reconcile function well
yeah this is a you ran some tests
already automatically because and now if
I run reconciles and then I pass
something
and then I pass a payment here
okay yeah so just just to show that
works and to show how you can kind of
evaluate things live in the red pole and
even see the output there directly or if
you want to just go to an actual rebel
can do it here so yeah the last thing is
that this is just instead how you do how
you produce is augmenting you just take
the repayment in transaction then you
you put them together like this you kind
of prefix them with the bank transaction
repayment and then you augment it with
the information from this pure function
and that's all I had to say for for
today and I think it was hard at the
beginning from what I heard like stories
but in the end I think everyone now is
quite fond of definitely fond of closure
closure it works now really well with
Katherine and we we kind of apps quite
close fun with that and for the
transitive property of love we also love
Casca and last thing is that the
motivation is that we're also hiring so
if you are interested in working in
London region office or synapse and
remember Lee talk to me or or just
contact the work site that's it thank
you very much
Thank You Andrea so we have about 10
minutes which is enough time for five
five questions I think okay stop there
could you just put your hands up again
okay thanks for the race of presentation
well I was wondering about about how
come in Kafka how do you deal with
things like it disaster recovery and you
have mentioned that Kafka is your only
source of true and you probably have
more databases that you need to recreate
from that state is it is it really
working I mean is it really possible to
to create those databases in a
reasonable time out from Kafka we to
retweet those those databases yeah good
question if it's possible to recreate
databases in reasonable time I think
when I think when it happened that the
database like recently there was simply
a bag that deleted like a table or
someone memory there is a table a
mistake and what they did is just today
and well that was the holdup but what it
did said just to replace it from the
logs from the the backups in that case
but in a way I don't think you have a
really happened that was like an entire
database that got the hat to be
recreated in theory you it's not so
simple because like databases that I was
mentioned before are in a way or some
tables will be taken directly from from
Kafka stream so they they just can
retake the stream and then you you dump
it to a database tab and that part is
quite fast I mean we don't have most of
the tables we don't have many records of
less than millions it's not it's not too
much but the processor is really fast
Caprican inch of surface but the thing
is that the same database for the
Microsoft will be used for other many
other internal things which are not
storing Afghan work so they I mean you
used to you would still like do recovery
like you normally will do any
or and then the next time that Kafka
Cornett runs it will update the things
that are now wrong for example what he
actually does is that it absurd on that
on IDs that already there so he will
simply override things so you I think
for me the other and for you find the
stoker key is just that you handle that
for from micro-services righted the base
in the normal way that you would do so
you just take the backup and you store
it and then the party capital sandwich
will just automatically work because of
how jobs are working how do you version
Kafka topics and second question how do
you find the transactions with the same
ID and your example you found two
transactions with ID 1 so how did you do
that the dimensioning is kind of problem
they solved by using the Avro schemas
which allows you to actually have we
allows you to actually define new fields
and every time you add a new field
however the only problem is that you
have to make it not known about because
otherwise passing not nullable so
because they're wise you all the the old
messages if you do have a breaking
change the topic we do that as well then
we simply bump the name to like in the
next version and then there is some
something called a schema registry which
can keep keeps track of all them which
is still part of confluence they're the
same ethic which keeps track of all the
latest production topics and and then
you simply use for example a carry
payment-free and that's the latest one
but if another project is still using
repayments
too if that doesn't break it doesn't
break everything cid works but maybe the
new data when only
be added to repay the three so depends
it really depends on the case in a way
so if for example is a topic that no one
is using yet or something like that then
you still virtually but you don't have
to worry about migrating data is enough
to work if it's something that is ready
production then you behave differently
but then there are some kind of good
ways to have them to do deal with that
and that's for the second question the
idea I mean this is just made up things
that just these are not real it's not
real data yeah yeah yeah so long story
short you work for some kind of
financial institution at least something
money were involved in the code which
means that a lot of compliance you know
questions and stuff like that
by default Kafka it is not multi-tenant
so if I have physical connectivity to
the host I can listen to whatever I want
which means that in your case I can
create consume some kind of consumer
which grab someone's financial data and
screw somewhere is somebody really hard
so the question is how do you how do you
work around that how do you make it pass
all these boring banking compliance
issues because like you know in Postgres
you cannot just do that for 15 years or
already in Kafka as far as I know you
cannot do that so far natively so how do
you around it I think I heard everything
but it's about if I understood how to
deal with kind of prime data and
sensitive and you don't want to ever
want to read them the concern so the
solution that is used and funding circle
I think but is probably not this the
only place is to use a vault or both the
Asha core project to actually store all
this the PII data so that means that
whenever something rides into Kafka you
want if there is some data that
shouldn't be written to capture because
not everyone has a
to it it will be written to vault and
hushed and they have stored into Kafka
too to be able to reference it so that
means that that's important for to
disperse the reason you you mentioned
that you don't want everyone to read
everything that will be quite bad and
also because you think after we can
never delete anything so that means that
we if we write something there we are
kind of stuck and we so it's quite a
problem
so with this solution what means that if
there are some application like a serum
that actually needs that this this data
they can still get it but the everything
will be kind of tracked and taken care
by the vault which the handles all the
permissions for top of that so one here
okay can I use Kafka as a durable cue
like for example if my consumer had some
message and threaded and processing how
can i signal that it's need to retry
like what started you can expose that if
your is if your consumer fails and then
he doesn't send the message yeah I think
you don't need to worry about that too
much in a way because well I mean we
normally worry about the more the
opposite that you don't send the
multiple multiple messages more than
once now the sameness is more than once
and the drivers themselves I think are
handling all the replies in case there
are problems with the width actually
sending the messenger I mean that yeah
if Kafka goes down for a while then Mia
is a big issue in a way and I'm not sure
yeah no sure if good things would be
automatically happening or they're quite
sure there is an internal like queue
inside the libraries themself that you
use to do stuff so isn't had to do that
yourself like the talk that the other
guy did this morning here the know who
saw that but they he mention exactly
that they added some retry mechanism and
then the discovery was doing it twice
because there was very ill try inside
there inside the library yeah I mean for
example if my consumer goes down but
before it goes down it actually took a
message from work here so I can I can
lose message right before it goes down
it's right up I see yeah well that's I'm
not sure I mean in theory you can always
manually go back in the offset where you
need to and that's what they they did a
few times when when there was something
similar but I don't think is something
that really if especially I mean if you
are using also things like Kafka
connected to actually dump two databases
then you use this really
it's never really a problem you're right
because when something is there is
already durable in your service as well
and gentleman yeah depends on shall I
break for coffee sorry if you didn't
make a question but you can approach
Andrea
outside I suppose
please remember also to to vote if
there's someone outside with an iPad
let's give another one more shorter
appreciation
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>