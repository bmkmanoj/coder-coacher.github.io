<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Spencer Kimball - CockroachDB: The Road from 1 Node to 100 | Coder Coacher - Coaching Coders</title><meta content="Spencer Kimball - CockroachDB: The Road from 1 Node to 100 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Spencer Kimball - CockroachDB: The Road from 1 Node to 100</b></h2><h5 class="post__date">2016-11-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/pFKrRF_sflE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I'm going to talk today about the
fairly difficult process is kind of a
crisis for us working on cockroach DB
and cockroach lives is the company
behind cockroach TV now related to
disability so we got the system fairly
functional we did a beta in April and we
ran into some serious issues with
stability and it was a bit of an
existential crisis actually so how many
people here are actually familiar with
cockroach TV ok so there's there's a
fair number who aren't so I'm going to
give a quick background sort of synopsis
of the database the first question that
probably is occurring to you is why do
we need another database there's quite a
few of them out there I think there's
probably 20 or 30 that are contenders
you know our contention based on a lot
of experience trying to build things
with databases is that the existing
solutions in the ecosystem don't fill
the need appropriately and particularly
put a lot of burden on the developer
there's this there's this really
terrible choice that you often have to
make which is do I want to use a no
sequel system or do I want to use a
sequel system and they both have
advantages we think that's not a choice
that you need to make anymore in 2016
and in fact you shouldn't make so why
would you choose no sequel well it's got
these built-ins sort of horizontal
transparent very importantly scalability
but you know why would you then use a
sequel well sequel is actually a pretty
useful very time-proven language or
interface to to using databases it's
good for more than just building
applications I mean what does it give
you that no sequel systems don't well
transactions for one right
synchronous updates to secondary indexes
um but it's good for more than just
building apps they said it's actually
good for for things like ad hoc
analytics queries so it's a very
powerful language declarative syntax so
you know the idea with cockroaches there
needs to be a better database it's got
some additional capabilities beyond even
what I've just said but it should solve
for scalability and it should also give
you transactions and something like
sequel so this is actually inspired by
work that was done at Google on a system
called spanner it's an open-source
version so what does it bring well it's
sort of the synergy amongst all of these
capabilities the most important and it's
obviously what we called it cockroach is
that you know it's just be hard to kill
and so integral disaster recovery is a
big part of this it's not something that
you tack on later it's not something
that you have to spend a lot of time
working on configuring it is really sort
of the the guts of the system so this
means that when you know of course you
can survive this is failing and
machine's failings but we actually
wanted to push it a bit further and
survive data centers failing which of
course leads into features like geo
replication we also want to seem to be
very anonymous in terms of its
operations this eases the burden on the
people that are running the systems so
it's self-organizes using a peer-to-peer
gossip network sort of similar to
cassandra in that way it's self heals it
Otto rebalances and again sort of
evocative of the names to be evocative
of this autonomous operation and you
know we also want this to be scalable
sequel so these aren't in the past two
words that you would see together but
it's it's very important you know a lot
of people have sequel a lot of
investment in sequel and the one thing
that they really fall down on is how do
you make that system scale and if you
look at any successful Silicon Valley
company they probably have a sharded my
sequel underneath the covers and and
Google had that backing their AdWords
system it went to a thousand shards
before they replace it I've recently
heard that Facebook has something like
15,000 shards of my sequel so it's
pretty hideous so you actually want your
sequel but you don't want to do charting
yourself so it's a lot of complexity at
the application level it's a lot of
complexity for your operations and then
finally and certainly not the least
important here it's open source and you
know personally and I think this is true
for many people in this day and age I
wouldn't touch a closed source database
of the 10-foot Pole you you really want
to be able to debug through that stack
you want to be able to customize if
necessary
and and I think even more important in
this era of cloud computing you don't
want the vendor lock-in associated with
using something like Dyne
or Aurora or Google spanner when they do
put it into the cloud which is imminent
so you know with those sorts of
solutions you you risk the Dropbox
problem right which is you know five
years ago Dropbox would have gotten it
off of s3 despite and Amazon's most
aggressive bulk discounts for their
usage so they ended up spending about a
hundred engineer years to build an
alternate system so I mentioned before
we had this big crisis with stability
you know a little bit of background here
this was a year and a half into the
development of cockroach we announced
our beta in April and it wasn't stable
then and it was a q2 okay R and then it
was a q3 okay R and five months into the
beta we still didn't have stability we
could stand up a three node cluster
which is typically the replication
factor that you configure cockroach with
but soon as we went to four nodes or six
nodes or ten nodes all kinds of emergent
complexity would crop up and the system
would fall over sometimes it would run
for five minutes sometimes it would run
for an hour sometimes it run for days
but eventually it would come to its
knees and some sort of surprising and
difficult to debug way so I'm gonna in
this talk discuss the diagnosis how do
we finally come to the conclusion that
something needed to be done
remedial actions that we took in terms
of mostly process and management and
then communication so we had like an
internal communication problem of course
but also an external communication
problem because it's an open source
system and then I'll go into a little
bit of a deeper dive on the technical
fixes which are sort of interesting and
some conclusions like you know the big
question I think is was this inevitable
or is it something that could be avoided
when you're building a system like this
so how did civility how did we believe
it or ignore it to such an extent that
it became a huge sort of existential
crisis for the company so you know I
would definitely you know
put the proviso in here that we didn't
ignore it completely in fact it was the
top item on our okay ours each quarter
and everyone was working on it but there
wasn't a sort of directed focus there
was no you know at the same time that
stability was on everyone sort of top of
mine list there was also a rush for
version 1.0 features so everyone was
sort of contributing to those because
they're interesting and they're fun and
you know everyone had sort of a stake
there also there were certain
correctness problems that that always
kind of bumped out everything else if
one came up we had Kyle Kingsbury who's
a guy that has a blog called
call me maybe and that really tries to
prove the the the claims of distributed
systems and he was working on cockroach
and he found a couple things and so we'd
always jump on those that sort of pushed
other things out of the spotlight and
also performance so we we we felt that
performance had a long way to go for
version 1.0 and you can sort of sink in
an arbitrary amount of time in their
performance these things were all
competing and what we thought and it was
an incorrect belief was that stability
would be an emergent property as long as
everyone was jumping on stability things
when they came up we were always paying
attention to them eventually we'd have a
stable system and in fact that was not
the case and it took sir I'd say the
biggest failing we had was the length of
time it took to realize that this is a
problem that we need to do something
different about so or Orwell said you
know the sometimes the greatest
struggles to see was in front of one's
nose I'm paraphrasing but that certainly
proved true for us with stability so we
had a very very experienced team I mean
there's centuries of engineering
experience we had about 20 people and
many of them were our total lifers like
20 years plus and we've worked on
systems as complicated as cockroach in
the past but this unfortunately didn't
save us from the bad assumption that
stability would be an emergent property
but you know on the flip side if we
wanted to look at a positive aspect of
all that experience there was a bedrock
belief that there was a solution to the
stability problem because we'd seen it
before and we had seen these systems
stabilize
and we knew it was a matter of increased
focus in particular and and when we were
sitting there you know I think it was
you know a lot of people are on summer
vacations this was in August and I think
I came back from somewhere and I just
expecting stability to magically been
fixed and it actually had gotten worse
and so that's sort of what precipitated
the crisis and you know we sat down and
had this sort of thought experiment
which is like there's tons of turn going
on in the code all kinds of things are
happening but if we just literally did
nothing else and we have a small team
and the code base was completely frozen
could a team of five six engineers
actually fix these stability problems
and the answer was yes of course we
could it's just so many other things
that sort of cropped in and we're
competing for attention so you know one
one sort of key takeaway from me in this
whole exercise is that belief is a very
very powerful thing and anyone that's
been working in at a company and you've
got a product and there's a feature that
just doesn't seem possible and every
time you feel like wow that would be
really nice but I just can't see a way
to do it efficiently or I can't see a
way that's not going to take away too
much memory and then a competitor one
day comes out with that exact feature
and literally you know you're freaked
out of course an hour later you've got a
solution and you know you haven't seen
what they've done
I mean except for the broad outline but
you know the belief that something is
possible is often enough to provide this
solution so what we did and this is
something we borrowed from Google a lot
of us were from Google originally was we
declared a code yellow
so code yellow is this concept where a
crisis has occurred and it is of
significant importance to the company in
such a way that you want to have a team
that's completely dedicated to it and
anyone that's not on that team you know
needs to basically drop everything else
if they're asked to contribute to the
effort so it's really sort of a
reorganization and a change in focus for
the company so we you know one thing
we've discovered here is that yes while
technical solutions absolutely did you
know solve the stability problem or at
least get us to a reasonable point of
stability the the root cause of it all
wasn't technical the root cause is
really you know a lack of proper team
structure and a lack of process so that
thought experiment that I you know just
discussed really helped us identify some
approaches some of to solving this
problem and in particular to really
clarify what was going on one big thing
was this idea of rapid churn and you
know we felt almost like you're doing a
surgery and you know you're you're
trying to get in there and through the
incision and there's just blood Welling
up you can't see what's going on it's
welling up faster than you can aspirate
it that's sort of what it felt like we
needed to stop the bleeding we needed to
stabilize things so that actually led to
a fairly drastic action that we took
also we discovered that most of the
instability was arising from a very
localized set of packages and so this is
written in go these packages it was
literally three of them out of quite a
larger number and the those packages
were being operated by on by most of the
developers a majority of them and there
was no sort of central review authority
so it's very decentralized effort so no
leader on that project and then of
course because of the size of that team
and it's sort of diffused nature there
wasn't any sort of one brain or at least
a small number of brains that were
taking a holistic perspective when
applying you know that sort of
perspective to both reviews into new
pull requests so this is turned problem
it's really I mean we described it like
this over the five months that we
weren't getting stability but it felt
sometimes like you take a step forward
and just take a step back maybe took two
steps back maybe you just didn't know
there were just so many layers of
complex in the system and you'd fix
something and you know because you know
you'd be debugging in AWS and there'd be
stack traces and there be log files and
go through and you'd identify problem
you'd go and you try a solution you put
it back in and this is
fail in a slightly different way and and
again sometimes would point to new
things or the same thing and it was just
very difficult to tell where you were in
terms of that process so this this idea
of freezing the codebase and working on
in isolation to get stability was a very
attractive one to us you know if for
nothing else I'd say that isolation has
just like tremendous psychological
benefit to developers I don't know if
anyone here has ever had a an episode
especially this happened maybe more more
in the past and it's happened to me
recently but you'd be working in C or
C++ and sort of a new system a new
platform and you were sure that your
program didn't have a bug and then you
started to suspect that maybe it was the
compiler and that's a terrible terrible
feeling when your sort of perspective
opens up to include like a whole class
of potential errors that you just have
no control over and that that whether it
was - or not in this case was part of
what guided and motivated this decision
and what we decided to do was we split
our branch everything had been going
into master and we split it so master
became the stabilization branch very
tight gatekeeping there and develop was
where the rest the remainder of the
functionality went and it was it was
almost an order of magnitude more pull
requests that were going in to develop
so we also froze the dependencies we had
quite a few dependencies English like 45
of them the most problematic dependency
and it caused quite a bit of instability
was G RPC so originally we were using
those sort of native RPC subsystem we
had that quite optimized but there was a
lot of agitation from various I'd call
them the usual usual suspects you know
some engineers love new stuff to
incorporate G RPC and we decided to do
it this is actually pre beta because we
knew we'd need streaming snapshots we
knew there'd be better tooling with it
and we also realized it would be more
difficult if we waited so that decision
was made and it's hard to say whether it
was the right one but certainly gr PC
when you look and you search for
stability and G RPC in the
issues and github you'll have a lot to
read through so we were really earlier
doctors of Dr PC and it was a it was a
marriage that required significant
couples counseling so we actually made
quite a few you know pull requests back
to G RBC to fix things and even in this
stability period which was lasted five
weeks there were quite a few gr PC
problems so we froze those that was that
was a I think a big win and you know
ultimately we came to a conclusion that
this splitting of the branches was quite
expensive so it wasn't free in
particular there was somebody doing
daily merges from the Masters to ability
branch back to develop and there was
also a sort of a moratorium on the kind
of you know things like refactorings
that touched a lot of the codebase and
also sort of more complicated
performance work that other people were
doing so certain things were embargoed
so the sort of second remedial action
was really to define leadership on the
project so this is something we you know
had escaped needing to do before that
and you know it's in many ways
decentralized feels better for a bunch
of open-source programmers but in the
end we came to the conclusion that for
this effort especially we needed a head
chef you know before it was just the too
many cooks in the kitchen ruining the
the soup and in this case in some for
some of the more technically complex
parts of the system there were many
people working on it from different
angles and some of them were getting
really experimental so we we really
wanted to to define that so one person
was responsible so that the final dish
actually tasted right this also wasn't
free so you know the the people that we
put in charge of this had other things
to do we ended up putting very
experienced people on this team and you
know they have management
responsibilities and they had especially
sort of design review responsibilities
and those suffered over the course of
this so finally you know this small team
idea and we really got to test this
hypothesis you know for something as
focused as this bigger is definitely not
better and I think that that was borne
out over the course of the five weeks
ability effort so you know one big thing
that we that we were you know insistent
on is that there would be increased
scrutiny so this is a five-member team
and every person on that team was
following all the PRS that were going
instability but it turns out if that's
the only thing you're doing you can
actually get a lot done so the volume
per engineer actually didn't decrease it
increased which was a little bit
surprising and in overall there was
there was no appreciable there was not a
pre simply less turn in that particular
section of the code but it was all
focused on stability we we thought about
something else as well which was
physical proximity so this was very
countercultural to make a change here
for us we wanted to we've always sort of
you know I guess had an inclination
towards randomly distributing engineers
so instead of just putting people in you
know the same team all right next to
each other which i think is probably the
most common way of doing things we
wanted to have some sort of natural
resistance to the kind of balkanization
that can happen in engineering teams so
we basically we actually randomly
generated our seating patterns and so to
decide that everyone was going to sit
together was a you know kind of felt
like an exclusive club and it was a
little bit of a risk too to go there
I'll actually in the conclusions
describe you know whether I thought that
was a useful change or not so this is
this is something I put together to
actually visualize the progress in this
system when the the code yellow was
implemented so this is a stream graph
and the the there's three sort of blue
sections on the bottom that represent
the core components that were modified
so everything above it is sort of the
other top level components there's quite
a few that aren't shown on this graph
but these are these are the major ones
so this first line here on the left is
when the split started and then when we
merge the code is over on the right and
you can see that there's there's
certainly not much of a change in terms
of that storage
the storage packages in this poor
components at the bottom there is a
tightening around the merge time and and
this was because you know stability had
you know somewhere midway between these
two lines and had started getting much
better and we saw sort of a light at the
end of the tunnel and we were being very
careful about what went into the package
before we decided to reemerge those
branches and so what sort of happened
naturally towards the end of that is a
lot of stuff started getting embargo so
you know there were people working on
refactorings but is like okay you can't
get that in we haven't quite done the
merge you know and it's sort of a week
went by and so a lot of things were sort
of in the wings and you can see after
that merge happened you know that the
thing kind of expands a lot more stuff
went in but the wonderful thing about
reaching stability is that you then have
a way to find regression very easily
right you can sort of sequence these
things in one at a time and decide has
my performance gone to hell has you know
is the thing crashing or seizing up and
that sort of thing and you can just
revert the change and figure out what
went wrong this is a great model to work
from compared to sort of blindly
swimming in a and a sea at night so you
know after we made these decisions we
had to figure out what to do with
communication you know one interesting
aspect of the decision making process is
that it happened overnight and I
mentioned I came back from vacation and
things seemed to have gotten worse you
know it was it's kind of finally like
the the point where I hate this analogy
but you've got the frog and the pot of
water and it's getting hotter right and
at some point that thing's like holy
it's so hot in here and it jumps out
right that's kind of what happened to us
but up until then we were like just not
really paying attention because the
change was so gradual that the contrast
wasn't you know right in front of our
face you know five months in you know
another day wouldn't have been very much
we could have deliberated but this this
this decision-making process happened
very quickly so I described it really as
a tipping point and and then when that
point way to figure out you know how do
we communicate this to the team because
it was really a decision made by a small
group and then also because it's an open
source project you know what do we do
about external communication
so one of our core values that
cockroaches transparency and we actually
take it pretty seriously we do okay ours
and and we we grade them Perry I'd say
you know fairly and if you look at our
okay ours for q2 in q3
you know though there was especially for
q2 there's a big red zero on the
stability one so it's not like we
weren't being transparent with the
failure that was happening there but I
would say that we weren't being honest
and particularly honest with ourselves
and and how severe the problem was and
how existential it was to the company
and you know one just sort of
devastating critique that it rose and
really sort of highlighted the gulf
between what our pre-beta expectations
were for stability and then you know the
the reality five months later was that
stability for us have become a punchline
internally and if someone made a joke
about stability everyone laughed
I'd never failed to elicit a chuckle and
that that's sort of an insidious loss of
confidence for the team wasn't good so
what we decided to do when we made this
decision is okay we're gonna have to do
a good job communicating this so we
wrote up a big email and and and got
that out the next day and we did a
really good job and I just went back and
read it when I was preparing this talk
and you know everything was explained
nothing was left out we did exactly what
we said we were gonna do in that email
you know whether or not everything was
you know a great idea
you know may or may not have been the
case but we did it all we communicated
it all that was good we also set exit
criteria so this is pretty important you
don't want these things to drag on
forever we did do a good job there and
we didn't move the goal posts in the
succeed they're succeeding code yellow
and where we failed though and we got
some feedback about this is that you
know there was a team deliberation in
this decision some people felt like the
decision was railroaded that was the
word they used I like that word but not
so good to apply to your company so in
terms of external communication we
didn't originally reach a decision about
it it was I think it might have been
mentioned but we were much more
concerned with the decision itself and
the internal
comms but you know over the next couple
days it was pretty easy to reach
consensus in fact there were no
dissenting voices you know the decision
was we're an open-source project we
actually do all of our technical
discussions either in github on our FCS
or we do them in a forum which is public
and not on corporate email we also use
git er instead of slack in order to
involve the the community and all of our
sort of chatting and about engineering
topics so we did need to communicate
certainly and you know there's risks to
that sort of communication right like
you know who wants to air their failings
in public you know it's like okay well
you're just gonna invite criticism and
it's not a comfortable thing to do but
we've saw it actually more as an
opportunity right so the reality is that
it's a pretty open project there's a lot
of people paying attention to it and it
was it's not like these people weren't
aware that there was stability issues I
mean many of the PR that were going
through had the word stability : in
their title so this is an opportunity
for us to clarify in to set expectations
of you know here's how long we think is
going to take here's what we're doing
about it and also it felt like an
opportunity if we succeeded certainly to
build trust so all of that said I had to
write this blog post and it wasn't easy
it's soft and it sucked to publish it
and then it stuck to read some of the
hacker news comments right so there
there were you know there's some real
experts on Hacker News and and and you
know there was sort of a mix though and
I'd say that the supportive voices were
by far the majority so you know
ultimately the community was supportive
all right so now some of the more
interesting stuff what caused
instability technically so some of the
symptoms there was like precipitous
slowdown sometimes so slow that it was
indistinguishable from deadlock they're
also out of memory as a big one that's
that was how our NGO programs would
panic otherwise ago programs seem you
know very stable it wasn't random things
like null pointer exception that stuff
doesn't really happen so much and go the
other memory was the big problem
option the really insidious one hard to
find and of course impedance mismatch so
we had some issues with some of the
technologies we chose and you know in
hindsight maybe they're not the right
things but we're stuck with them so the
biggest thing we ran into was
rebalancing and recovery I mentioned
that you know a big part of cockroach's
capabilities is autonomy autonomous
operation these rebalance recovery in
other words you know auto rebalancing
self repair this is a core part of a
system we do it via snapshots so
cockroach has this you know underneath
the covers there's a monolithic sorted
map which you can do transactional key
value operations on and we break that
into these things called ranges they're
kind of like tablets and in some other
systems or shards and a range is sort of
the base unit of replication and it's a
contiguous chunk of the larger key space
so they're they're typically about 32
megabytes and when you're doing a
rebalance or recovery what you do is you
take the source you generate the
snapshot which is essentially package up
all the information that's part of that
range and then you send it to a target
and create a new replica there so these
are pretty expensive I mean most OLTP
operations in the database they're about
a kilobyte maximum so we're talking
about sending 32 megabytes reading 32
megabytes off the disk and of course you
know storing 32 megabytes in memory and
we we originally weren't very fancy with
this because we figured you know what
you know when we when we get to bigger
and bigger systems and we need to you
know start working on the performance
problems like getting latency variants
you know down to a very small number
then we'll worry about making our
snapshot implementation better we could
see the holes in it but we figured we'd
punt that until you know somewhere after
version one but it turns out we couldn't
really avoid getting some one fancier
and it's a good thing we went to G RPC
for this because we're able to use
streaming RBC's which was a big it was a
real boon for you know memory
consumption we also discovered that we
had much to course brain locking so we
had to do a bit of a refactoring and so
we avoided holding critical locks while
doing the generation of these snapshots
so those were some of the solutions that
we that we undertook while we didn't
know what the heck was going on with the
system it turns out that there was a
much simpler root cause of our snapshot
difficulties but it only became clear
when we had cleaned up all of the sort
of more visible symptoms and part of the
problem was you have a 10 node cluster
it's often Amazon you know these log
files you literally have thousands or
tens of thousands of these ranges
rebalancing there's tons of IDs flying
around you know the system needs to
rebalance but you know it's hard to get
some sort of like actual idea of what
the system's up to especially when
there's out of memory errors occurring
and you know the system gets really slow
so we ended up doing all these difficult
fixes first and in hindsight it's maybe
not even necessary that we've done those
difficult fixes but you know what we
discovered was was it was embarrassing
honestly and I'll show you a simulation
of exactly what was happening but let me
just describe the rebalancing algorithm
so what what happens is you know mention
there's a gossip network in cockroach
and each node participates and so every
node will advertise its capacity stats
it'll say you know how many stores it
has for each store how much that's a
storage device how much how many ranges
it has right how many of them are active
how many it holds leases for how much
capacity is have how much utilization so
this information is available to each
node each node takes that information
and generate statistics about the sort
of mean in the cluster in other words
you know what is the average what are
the average number of ranges for my
peers and if I've got way more ranges
than a peer house I'm gonna rebalance
because I'm over full and he's under
full right so that's essentially how the
system works now you can look at the
mean as an exact number and you can look
at the mean you know sort of Fuzzle e
with a threshold around it we were
looking at as an exact number which is
really dumb because you end up with
thrashing and this is this is exactly
what was happening in the system so let
me show you what this is this is a
little d3 animation I put together so on
the left you have the exact mean
rebalancing on the right a Thresh
I mean you'll see that the threshold
Amin finds its equilibrium very quickly
the exact mean there's always somebody a
little bit under the mean and always
someone a little bit over unless you
have some house like a perfect you know
equality and the sizes of the numbers of
ranges so this was going on in our
system it was endless and never stopped
it was going as fast as it could
rebalancing rebalancing rebalancing and
you know it amazingly took us fixing
these other sort of the the more visible
like immediate symptoms to actually
discover this is going on and it also
like I think is a something of a lesson
learned here which is advanced
visualization tools are so can prove
somewhat invaluable and in in in terms
of you know analyzing what's really
going on in a fairly complicated
distributed cluster so there's only ten
nodes so you know we've been running
them with a hundred nodes now and you
get some really interesting emergent
behavior so lock in tension this is this
is the second biggest problem and
certainly led to these very excessive
slowdowns and one interesting feature of
lock contention when it does crop up is
that the the degradation performance is
not graceful it's like one second
everything seems to be working fine the
next second you know all of your queues
are full your clients are experiencing
massive Layton sees nothing's really
happening so we actually got quite
familiar with a bunch of tracing tools
and he's proved invaluable for
diagnosing these problems and finding
out you know which mutexes we're causing
the problems so go has a system called
net trace which is actually quite quite
good it actually has this really neat
thing which was is definitely directly
taken from Google's I remember using it
at Google but you can go to this request
see page you can see the the actual
active traces that are that are going on
in the system and things that have been
going for a long time and you can
actually see you know all the steps it
took where we're in the code it was how
long it been in each each of these
particular states there's also a system
that we've integrated called light step
this is all this is again from a
originally from Google technology
something called dapper
well what it does is the distributed
trace collection and analysis system so
you know cockroach you have ten nodes a
client talks to a gateway the Gateway
will send that out sometimes those knows
it'll send to other nodes there's a
bunch of stuff happening the net tracer
stuff works on a single node but you
really want to be able to trace a a
request from its start at the client all
the way through the Gateway all the way
through all the nodes it it talks to in
order to do that this light step will
pull the trace information from each of
the nodes and put it together using
these request IDs so very very cool
stuff and what these tools allow us to
do is diagnose the problems and fixing
lock contention problems though is not
easy a lot of a lot in a lot of these
cases we had problems because there were
some tricky edge cases and we decided
that you know it let's punt on this
let's put a somewhat course brain lock
in and we'll hope for the best
in order to avoid that coarse-grain lock
and you know solve the problem not hold
the mutex but avoid the edge cases often
required some fairly tricky refactoring
we did that both with raft processing
I'll discuss raft in a second and also
garbage collection so you know when you
rebalance from one cluster another you
got to delete the old replica data and
that's actually so much slopes you have
to go over it on disk and mark all the
things for deletion
so in somewhat you know I guess in some
bit of irony here the tracing tool is
why they were great for finding the lock
contention actually caused instability
on their own there's this really amazing
aspect of the tracing where you you know
in that you can put whatever you want
into each of these spans in other words
you start an operation you add it to the
trace and then you when you you know
something stuck and go and see exactly
where it is and what it's done so far
and we actually put commands into there
and most the time the commands were
things like oh we're gonna do a
conditional put we're gonna do a scan
and you can see the keys and that was
very useful but there were also snapshot
commands in there and so it kind of
crazy it took us a long time to find
this one but turns out that a lot of
these traces were holding on to entire
snapshots and of course this is causing
out of memory problems but the silver
lining of this problem and
and all of these problems actually more
generally is that we were able to
integrate very deeply with with all of
the available tools for example to trace
of course but also for you know heat
profiling so go has some internal tools
we also had to do it for C++ and now all
of that stuff is integrated with our
admin UI so corruption you know in some
of these other cases like lock
contention and you know out of memory
stuff it's kind of like honest wage
honest labor it's straightforward
corruption is scary stuff there's were
the two sources that we've seen the
first is replica divergence so we have
an invariant in the system that all
replicas are exact duplicates of each
other and we periodically take a
checksum and compare and we haven't had
any of those problems recently none of
them in this stability code yellow but
that has been a source in the past
potentially even more troubling are
broken invariants in the system and we
had one of these during the code yellow
so we have this addressing scheme which
is very similar to BigTable - HBase -
spanner and it's it's what's called a
bi-level index and you know so systems
like cassandra they have a consistent
hashing scheme where you can look at a
key and discover where the replicas are
just by a bit of inspection with
something like cockroach or spanner you
look at a sort of first level index
which leads you to a second level index
which then lead you to the data cache
all of that so it actually works very
fast in practice but those indexing
records are crucial for finding out
where your data stored one of those
disappeared on us so that's not good we
update them with distributed
transactions so was that model broken we
didn't really know it was a terrible
curveball to get in the middle the
stability code yellow and it made some
of us despair that we'd meet any of the
goals we'd set but luckily we have this
guy that's kind of like Sherlock Holmes
one of our co-founders Ben and he was
able to intuit what had been wrong and
deduce it from code inspection kind of
amazing especially since this particular
gap in the model was such it required in
order to manifest such an insanely
unlikely series of events and timings
but you know that's kind of leads to
another less
learn here and it's not the first time
I've learned this lesson but you know if
you have a sufficiently large
distributed system if something can
happen it will absolutely happen and
it'll happen sooner rather than later so
raft how many people here know what raft
is more than cockroach I'll have to work
on that so I'll give you a really brief
overview it's a distributed consensus
algorithm it's based on something called
paxos and it's been very popular since
is released only two years ago everyone
uses it it was a seemed like a really
good choice when we started but over
time we've discovered that it's a really
shitty choice for cockroach TV and we
would have been much better using one of
the more sophisticated variants of Paxos
the great thing about raft for us and
one of the reasons we chose it is that
it is it enjoys a very good
implementation and go which is part of
xcd and we share that so we didn't have
to build it ourselves and we benefited
from you know their excellent efforts to
build it why isn't it the right choice
for cockroach or why wasn't it because
we made major changes to it well it's a
very busy protocol so graft is just
aggressive about heart beating and just
staying on top always having a leader
and that's great when your application
has a single raft group or a handful of
raf groups you don't mind if you have
like you know three heartbeats a second
for each one of them but we have lots of
raft groups in fact we have one per
range and in a cockroach cluster you're
gonna have a thousand ranges or ten
thousand or a hundred thousand or a
million or ten million or 100 million
you could even have that money and so
having that many raft groups all busily
sending heartbeats is not feasible so
it's a mess
so we actually thought though that we
could ignore this problem because the
reality is you know modern systems can't
handle a lot of heartbeats and we
thought we wouldn't have to deal with it
until we really had to get to the next
level of scalability turned out we were
wrong there so the problem with raft is
that in its default sort of naive form
it creates traffic and work that's
proportional to the amount of storage in
your system right you don't want to do
that because in a database you end up
with some things they're very heavily
accessed just like any other system and
some things that become very cold right
you shouldn't have those cold things
busily heart beating so we wanted to
make it instead of proportional to
storage we want to make
proportional to client activity and
that's been the goal and there's still
more things to do but one one the first
change that we made was to lazily
initialize these things so when you
start up an ode instead of getting every
single replica that it has you know
participating in his craft group we just
ignore them and it was basically an
on-demand sort of lazy initialization of
the raft groups and that worked fine
the cost of course is that you don't you
don't have a sort of leg up on a raft
group that's going to receive traffic so
you get that first right at that first
read and you got to get in there so you
know that actually led to a further
insight which is that if we don't have
to start these things immediately we can
turn them off and we build a system do
that which we called quiescing so let me
let me show you another neat little
animation that will give you some idea
of this on the Left there's naive raft
on the right there's the quiescing
version and you'll see that there's
going to be these heartbeats and they're
shown in red between the nodes so each
in both of these there's gonna be ten
nodes and there's going to be C 2020
ranges and each one with three replicas
there's to see them all they're all
little colored circles inside the nodes
and you'll see the heartbeats on the
left in the quiescing version what
happens is whenever there's a little bit
of internal traffic we send the traffic
we replicate that to the the three
replicas in the range and then we send a
heartbeat to quiesce it so let me give
you here so on the left you see those
heartbeats that are periodic that are
happening between them on the right what
happens is each each range that becomes
active because right comes in to the
system it does a simple heartbeat to
quiesce the thing so you can see there's
you know almost a 3x difference between
these two cases and it gets you know
considerably worse in other kinds of
scenarios
some conclusions the stability code
yellow is I think roundly a success when
we judged our q3 okay ours we said that
80% of it we gave it a point eight it
was kind of green there's a really nice
thing to see we got ten nodes very very
stable we've actually run 100 nodes now
it's and and that was stable as well
so the the performance goals and the
stability goals for q4 really involved
100 node clusters so code yellow also
was successful as a concept obviously
you don't want to use these too many but
they really did help raise awareness so
in terms of the different things that we
tried split branches feels like
something we didn't really need to do
there's a significant pain there's
someone who became the merge monkey was
definitely like a martyr for the cause
best things psychological benefit maybe
this stability team that's focused with
the leader that was crucial the
proximity not such a big deal we had a
natural experiment we actually ended up
with two and then three developers on
that five person team being remote so we
didn't need the proximity stand-ups
crucial we also had a war room which is
another Google concept and that war room
kind of brought in the rest of the
company was a twice weekly thing and
then we also got everyone involved in
being production monkeys people took
turns and you know people became very
familiar with the like deployment tools
and debugging mechanisms so smaller team
more scrutiny this actually was I think
the the linchpin of the strategy and as
a testament to success this has actually
become permanent so the team is probably
changing and will continue to evolve but
that small team and that scrutiny on
these core components is going to
continue so the big question here it's a
bit of navel-gazing honestly could we
have avoided this instability can you
build a big system like cockroach and
avoid instability and you know hacker
news got some people but definitely say
that you can avoid it you know I'll make
an argument from experience and by
hearsay which is of course a fallacy but
based on that the answer is no I mean I
worked on three of these systems over my
career and there was always at least
months if not a year of instability
which I didn't work on but I'm somewhat
familiar with and the people that are
working on it they had I've heard an
estimate of a year and a half of
instability so 18 months and of course
Oracle took many versions to become
stable as did my sequel what are some
practical ideas for for mitigating the
cost of instability and one of these
systems I'd say that we could have
defined a less ambitious minimally
Viable Product and with that we could
have gotten to the point where we had
stability and then could notice
regressions also you know we just didn't
plan we didn't make a concerted effort
with the belief that stability would
turn into what it turned into that it
was not an emergent property that it had
to be directly addressed and that's
obviously something we could do
differently if we'd started in April
with this team whose sole responsibility
was to get stability I think we would
have gotten at months earlier so that
could be improved Thank You Roger I
think we have just time for one or two
questions before we break for lunch sir
hi thanks for sharing that experiences
actually I went through something
similar so it's close so looking back
what would you say which was the harder
getting the team and psychology buy-in
from the team or doing the technical bit
the team byun wasn't actually very
difficult I think people were somewhat
relieved so you know that you know that
the difference between transparency and
honesty you know ultimately was you know
the code yellow was was hugely
beneficial to the company so vayan
wasn't a problem you know as I mentioned
the one bit of feedback we got that that
was not positive was that there could
have been more deliberation and more
people involved the technical side of it
took five weeks and was a huge amount of
work and it was actually a lot it was
fun though
you know as these things are whereas
like the process was
much fun tasks how do you test the
system do you like what's made tests yes
so you know obviously there's huge
amounts of unit testing but unit testing
only goes so far and then we have
acceptance testing too we use docker for
that boy docker is just incredibly
difficult but you know the mistake which
we made and we've started to correct is
that from the acceptance test in the CI
and the unit test we were jumping
directly into deploying clusters on AWS
and GCE and Azure that's not necessary
and it's actually a really problematic
because there's a very long debug cycle
between you know making the change
getting it through you know PR yeah your
unit test pass whatever you know someone
reviews it you merge it then it gets
deployed when the next appointment thing
happens we do them pretty quickly but
still and then it gets into production
and you're obviously most engineers
aren't you know the production monkey
that week and so you know you kind of
hear from someone else you know the
things crashing I think it might be your
problem you know and and you're like
what can you send me all the log files I
mean it's a disaster you're talking
about like a week-long process when in
fact you should be testing that before
you even merger PR so what we've decided
to do and we've made quite a bit of
progress is to build local clusters on
the laptop I mean this is pretty easy to
do you don't need to use docker which
I'd highly recommend against you know
what you just need to do is run multiple
instances of this thing set a low sort
of split size for your ranges so you
don't have to put you know terabytes of
data into the thing and still get lots
of rebalancing and things and then just
run multiple instances with different
ports and we just have some test
frameworks to do this now and it's
incredibly effective and you can find
most problems in that environment that
you would otherwise have to get to AWS
or GCE all right Thank You speaker</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>