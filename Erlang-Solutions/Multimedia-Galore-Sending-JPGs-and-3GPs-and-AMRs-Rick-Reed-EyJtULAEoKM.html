<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Multimedia Galore: Sending JPGs and 3GPs and AMRs! - Rick Reed | Coder Coacher - Coaching Coders</title><meta content="Multimedia Galore: Sending JPGs and 3GPs and AMRs! - Rick Reed - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Multimedia Galore: Sending JPGs and 3GPs and AMRs! - Rick Reed</b></h2><h5 class="post__date">2013-04-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/EyJtULAEoKM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good morning thank you for giving me
this opportunity to talk to you about
some of the work we're doing it whatsapp
how many people does anybody here use
whatsapp so for those of you who aren't
who aren't familiar it's a smartphone
cross-platform smartphone messaging app
that uses the data plan i joined the
server team that whatsapp in 2011 and
that was my first experience with Erlang
and kind of my my primary focus at the
company is on system scalability that's
kind of my background of course it
wasn't with Erlang before what's death
but it is now just thought I'd start out
with a interesting little day in the
life at whatsapp this was this a traffic
graph from what from last week with a
couple interesting little spikes here we
have any guesses what was going on here
ok nope oh yes hope so this is Pope here
so you can see a little you see one
spike here when we have news that we
have a pope and then there's a second
spike when we find out who it is so it's
one of the interesting things we do it
with tapas look at interesting things go
on in the world okay so Pope Pope's
important in the world what's more
important my guess is if you saw our
talk last year you'll know that sports
fans are important so this is a football
match in Europe you can see a couple
couple goals there and people love to
talk about it when it goes on so getting
more into the the topic of this talk
that also yeah so that also caused so
this is shifted by one day so this is
the soccer game over here you can see
there's a couple little so these are
this is a number of images number of
pictures being transferred between
people and so actually quite a few
pictures
around the Pope's news so and don't know
what those pictures were but people had
something to share so here's an overview
of why I'm going to talk about the
multimedia problem our legacy
implementation the old system that that
I replace this year the architecture the
new system some challenges and
workarounds that occurred while
developing and deploying that system and
the results in conclusions of that so
start out and talk about the problem so
one of the nice features of that one of
the features that are users like about
our app is ability to obviously share
media so whether it's pictures it's
video its audio some of thats recorded
on the the sender's phone pictures that
are taking videos that are recorded some
of it's not a lot of it's actually not
stuff that they get from the internet
maybe or or through email and then want
to forward it to single recipients or
into groups our app has the capability
of creating very easily creating groups
of people that you want to communicate
in it could be about a specific talk
bake it could be a specific group but
you can send media into those groups as
well we're a cross platform app so one
of the challenges with media is
supporting all the different platforms
so for instance Apple being Apple they
have to record things in their formats
which is quick time for video core audio
for audio and none of the other
platforms can play that so the we in
order for our users to have the best
experience we want this to be a seamless
thing for them so they can send their
media into a group or to another user
and not worry about what kind of phone
they have what kind of format they
should record in and and we want to take
care of the transcoding for them finally
this system is store and forward only it
we don't do any archiving of any of the
messages or media that people send it
we're basically just a transit point for
the media while it's being sent between
the users so good problem and it's a fun
problem for me as a scalability person
is just dealing with the growth so we
continue to add users on a pretty good
basis as we add more users each of the
users is actually using the service more
becoming more engaged with the app and
as it turns out on top of that
multimedia is becoming it is growing as
a percentage of the usage per user so we
have these three factors that are sort
of multiplying to give us a graph that
looks like this for for media transfers
so the upper one here is pictures we've
got video down here and audio is the is
the the lower one there but looks pretty
exponential you'll see there's sort of a
normal kind of weekly variation in how
much people send people to send it send
most media on Sundays for whatever
reason but we've got a couple special
events here so this is the end of the
year so we've got Christmas here which
is a pretty good spike and then New
Year's Eve right New Year's got a really
good spike this one I bet you won't
guess what this one is it surprised me
this is actually a valentine's day so a
lot of pictures on valentine's day ok so
our legacy system had this sort of the
same basic capabilities being able to
send pictures and videos and audio but
it had some some problems scalability
wasn't as good as we knew that we would
need to deal with the growth in usage
the transcoding support was kind of ad
hoc it's initiated by the two clients
themselves so the clients would when
they received a media message its CEO
it's quick time I can't play that please
give it to me in mp4 or it's it's a core
audio file I can't play that give me an
mp3 and the rules on the server side for
making those trans coatings was was kind
of ad hoc and finally the legacy system
was in Erlang and our at whatsapp our
first experience with Erlang was in
conjunction with a jeopardy which was
the original version of our front ends
and as we've gotten more experience with
Erlang learning how scalable
it is how reliable it is how easy it
makes our lives now now it's feeling
like a bigger and bigger hammer so now
everything's starting look like nails so
you know nail here nail here oh here's a
screw we'll just hit that harder you
know here's a watermelon here's the eggs
were like Gallagher with with with
Erlang so the goals of the new system
were to improve upon the scalability
give us a path to to grow the system as
as we needed to expand with the user
base keep good reliability and improve
the user experience so just a few words
about what that legacy looked like it
was a lie ta tat web server with PHP we
were running it on do like score servers
with j bods 12 SATA drives and those
just dns round-robin simple like we give
you we give the clients a hostname they
get a IP address they upload their media
they send it to the to the other side
the other side downloads it from that
there wasn't any reference counting so
it was simply a matter of passing around
the URLs for the media and so without
the reference counting we do need to
clean up the media after it's been
downloaded now we don't know when it's
downloaded so there's just in the old
system there's a time-based media
expiration so after certain number of
days all the media gets gets deleted and
then finally I mentioned before that the
all the transcoding was initiated nish
aided by the clients even though the
transporting itself was done on the
server side alright so in the new
architecture there were some new
features that we wanted we wanted to add
so again this is both to help our
scalability and to help the user
experience so first one I have there is
resumable uploads and downloads so a lot
of our customers may be on pretty flaky
mobile networks or they may be
intermittently connected and it's it's
fairly easy to get small text messages
through but when we want to send
pictures which are typically for
about 64k or videos which are multi
megabyte uploads and downloads getting a
constant wireless connection for a long
period of time to do those may not be
possible for all our users so their only
option was to keep retrying keep
retrying and it would always start over
the beginning so we want to be able to
have resumable download so that say they
go out of wireless range or just for
whatever reason their connections kind
of spotty they can they can retry those
those downloads and it succeeds more of
the time we knew we wanted to add
reference counting so we can keep track
of what was and wasn't used we wanted to
have D duping of uploads so that if I
receive something from somebody a piece
of media and I want to forward it to
somebody else I don't have to upload it
again if we already have a copy of that
object that's in trains it to somebody
else in the system so we want to be able
to deduct that we want to wanted server
controlled transcoding so that we could
quickly you know as as phones as new
phones come out or we find out that
there are problems through our customer
support team that with playing
particular videos or what we want to be
able to update those those transcoding
rolls without having to push new clients
so we wanted that all controlled by the
by the server side and we wanted to be
able in the server side trimming is 44
low end clients which don't have the
capability to do video editing on on the
device itself before it's uploaded we
want to give them a simple way to at
least upload a portion of the video I'll
talk a little bit more about what this
is but a portion of the video and have
less sort of trim it server side to the
right size okay so we wanted to do
reference counting so we know we're
going to need database we want to know
what objects we have who's who they're
intended for and the transcode mappings
between different instances of the same
object that are in different formats so
we you know being Erlang
tickets now and fans we go to our our
our first choice on a database which is
in Asia this is all going to be
persistent data so we put those in disk
copy tables we and again we're dealing
with a scalability problem so this is
this database is all partitioned we have
different islands and fragments I have a
picture that I'll show you in a sec we
do all our operations a sink dirty to
get sort of the transaction overhead out
of the way again for for scalability
purposes but to sort of allow us to do
that without creating lots of
referential integrity problems we
collapse all the operations for specific
objects into specific processes on
specific hosts so that everything gets
serialize through a single process for a
single object that way we can not
completely eliminate the issues that we
have with not using transactions but at
least for our purposes do well enough so
here's how we split up the database we
have generally we on our in our system
we we run little islands of amnesia
clusters so an island is a set of hosts
which all share one schema so rather
than having one schema for the whole
system which creates lots of problems
both in terms of just the kind of the
complexity but also makes it really
difficult to do schema operations
because they they take a global lock
over the whole thing and even even in
our small islands at peak load it's very
difficult for us to do any sort of
schema operations so we try to keep our
islands small so this database is
partitioned 16 ways so half of the
partitions go on one Island half of the
partitions go on the other Island and
then within those islands we have pairs
of hosts which each host 11 partition as
a primary and one partition is the
secondary so up on make up on amnesia
Island one we've got perdition one on
this host which is the primary partition
two is over here but they so they have
and I have mentioned it yet but the wii
this tape these tables are fragmented
28 ways those 128 fragments for these
tables so partition one is going to be
fragments 117 3349 65 and so on but it
also has two 18-34 fifty and sixty six
so under normal operations partition
this this host is getting all the
operations for keys that that map to
partition one and this host is getting
all those for partition two if one of
these goes down then the partner takes
over and all those operations happen so
then at that point one host is handling
two of the partitions so we've got these
pairs each of these islands has four
pairs for total of sixteen partitions
and and doing it this way oh the other
thing to note is both the tables are
completely both of these have both of
the tables but they only have half of
the only half of the fragments are
active in each Island so I'm this island
only the fragments that map two
partitions one through eight are active
and the ones that are in 9 through 16
are inactive there's there's no records
in them there's no replication there's
no writing there's no reading and the
reverse is true down here so we don't
get it we don't get any replication
traffic this way and by partitioning our
fragments / hosts we only have pairs of
post talking to each other with
replication traffic so it greatly
reduces complexity of what's going on
all right so in the new system we want
to integrate with the the Erlang
messaging cluster our main our main
cluster and that allows us to do the
upload d duping so the clients talk to
our messaging servers over our
proprietary chat protocol and they
initiate the the media transfer they say
I've got a video that I want to send
here's the hash of the file contents
that's sent to the messaging cluster
which then talks to the multimedia
system uses looks in the database to see
if it have already has that object if it
does then it can tell the client then
the message gets routed back to the
client says here's the URL that you need
to pass on to the to the to the
recipients whether it's one
or a group and and they're done so
there's no need to upload that media
anymore we've already got it's already
in transit between some other recipients
on this some other users on the system
so we don't need to upload that again in
the case that we don't have a copy of
that object in flight already we need to
pick a server for that client to upload
to so we do the load balancing based on
I mean the the server that's picked to
do that upload is done inside the the
messaging servers rather than DNS
round-robin which was what we were using
on the old system this allows us to
react you know pretty pretty quickly to
hosts that go down or become isolated on
the network or are having capacity
issues and we manually turn off uploads
things of this nature so stuff that the
DNS propagation delay would create
problems for us now we can we can affect
almost immediately about what's going on
now that's not completely the case
because after we give an upload URL to a
client they may go away you know they
may drop off the network or somebody may
you know their battery dies or whatever
so there's there's definitely a decay
after we change upload targets there's a
decay of uploads to continue to go to
the original host that we pointed them
at last thing that happens on the on the
messaging servers is the reference
management so when the after the upload
is complete then there's a second step
where the client says okay I'm going to
send it to these recipients and we want
to attach references at that point so we
so that we know that okay we have an
object here are the references to the
references that we create for the
intended recipients and then once
they've downloaded the media they
acknowledge it and we could delete those
references that all happens through the
chat protocol to our messaging servers
all right the legacy system used HTTP
upload and download for the media
objects themselves the media files we
wanted to preserve that just to have
some commonality with legacy system but
we also liked it
it's it's kind of a known quantity
uploading and downloading files over
HTTP has support for things like you
know bite ranges content types but the
things that we'd need to implement
anyway and if we were to do a different
protocol there are some drawbacks of
course the there's an SSL so we do this
over HTTPS so there's an SSL negotiation
each time there's an upload or download
on some of our clients that can be kind
of slow so it adds a little latency but
that's just what it is and surprisingly
support for doing bulk uploads and
downloads is kind of iffy on some of the
client platforms now you you'd think you
know smartphones internet-connected
they'd have really awesome HTTP support
not necessarily the case for writing
applications on some of these platforms
so we've had to do some of our own
custom code to do that well alright so
we needed a web server since we're using
HTTP after a little bit of analysis but
not a super great amount of time spent
doing this we selected yas firstly
because it's it's mature and and had had
the feature set that we kind of needed
it's a nice balance of support for
serving media files so just being a file
server you know just a basic static web
server but also had the ability to hook
in where we needed to so creating
application modules where we want to be
able to look at you know what kind of
trans coatings we're going to do things
of that nature so there's a nice mix
there where we can do our program
ability we can we can insert the
application modules but that doesn't
mean that we can't take advantage of all
the built-in support that's going to
this going to transfer the files
themselves we only really needed a
handful of patches for our environment
and we run it embedded alongside in one
Erlang node alongside our other server
prox that are that are employing the
service
so for object storage we do a simple
file / object model basically this was
the same thing we did on the legacy
system we run on freebsd so it's just a
ufs to file system nothing fancy we do J
bods which are directly attached to the
motherboard we try to get as much
hardware and drivers out of the way as
possible so far so one difference
between the legacy system and the new
system was we knew that images were a
lot more popular in terms of in terms of
object rate I guess as opposed to packet
rate object rate than video and audio
you saw from that bag traffic graph that
I showed you that images are far more
popular so we so we decided to have
heterogeneous storage for those two
different things so we'll put images on
fast storage will put the the big stuff
audio and video which isn't as popular
on to slower spinning disks so our image
servers have six SSDs which are now
getting big enough that it's practical
for us too you'd be able to use that in
this case now we don't actually need the
full throughput that SSDs can give us in
this application but we need more than
the SATA drives can give us so so we're
kind of stuck in the middle at this
point so we have a little more i/o
performance with the SSDs then we really
need we would like to trade off some of
that for some space but it is what it is
we just used a hash directory tree
structure based on the the file IDs of
those objects there's a there's fewer
than a thousand files in the leaf
directories we use a 16k block size and
the freebsd file system this is the same
technique we use for our transient
message store so when I send a text
message to another user or to a group
that gets stored in there if they're not
currently connected to our our messaging
servers that gets stored in our
transient messages store offline store
until they come online then they can
retrieve it so we have lots of
experience with using this kind of
storage so each of the mailboxes is a is
a file on on that
message door and so we get about 4
billion cycles a day on on those
particular servers and so we we know we
have a lot of experience with these and
it's very predictable we don't have to
worry about sort of long-term
maintenance of you know it's this table
getting fragmented do we need to
optimize this index it's all very
straightforward file system stuff which
has been there forever and ever and it
just works right for us okay so after a
file gets uploaded first thing we need
to do is identify what it is so we have
a gross knowledge no notion of what it
is we it's it's an image it's a media
I'm sorry it's a video it's an audio
because the client told us that but we
want to know is it quick time is it 3gp
is it mp4 is it ping is it jpg is it you
know and so on so we go through sort of
a triage process using Earl image
because images are so popular that rate
is too high for us to really be running
external code every time we want to
every time we want to identify an image
so this this library works well being
able to identify most of the common
image types if it turns out that it's
either there's something about the image
that this doesn't that Earl image
doesn't understand like for instance
there certain I think there are certain
JPEGs where it can't give me the
geometry correctly in that case we fall
back to our external code so in some
cases you know if it's an image will or
immediate info and get extract it gets
more it has less error rate in terms of
identifying what the media is and then
for the audio and video we go out to FF
probe and that's really good at
identifying all the parameters that we
need to know so if transcode an approach
you're not familiar it's just the sort
of read-only version of ffmpeg which is
what we use for the transcoding so in
these three cases media info FF probe
and ffmpeg we're forking out of the
Erlang
emulator to go run those that see code
and it's basically an OS command call
but I actually went back and wrote a
base a rip back coat out of OTP and
modified it so that it didn't actually
launch a shell put a shell in between
because it was causing some problems for
some of the things we were doing okay
next thing we need to be able to do is
proxy misdirected requests so I haven't
mentioned it yet but so we put images on
fast storage we put the audio and video
on slow storage but sometimes the images
hanging around for a long time because
they say go to a group and maybe some
members of the group aren't very active
so we have to hang on to that image
until it gets downloaded so it's sitting
there taking space on a fast drive when
we don't really need it there so there's
a there's an archive capability that
moves stuff from fast or too slow
storage which means that now it's on a
different host so when we do download
when we send the message we actually
hand out specific hostname so we say
connect to this this is the host that
this media is on so download it from
there now if the objects get moved
because they got arcot they got archived
or for other reasons got moved off of
that host then we need a way to go get
it so on the back end we reverse proxy
to the the actual host that the content
is on and use jaws ref proxy for that
which works really great except that it
became it was a little bit challenging
to set the arguments correctly so that
the the right query args and headers got
forwarded with the back end request so
that the the target server that had the
content on it could make the right
decisions about it could make the same
decisions that that the original server
would make in terms of what object that
it would serve to that client but
otherwise works really great there are a
few maintenance processes that that run
on that run on this system there's a
Reaper process this is all in Erlang
there's a Reaper process that goes
through the database and
finds all the references that have been
acknowledged so the meetings been
downloaded and references that are
really old so we still have a time-based
expiration on media because we can't
hold on to everything forever because
that we do have a small number of people
that leave our our app and don't come
back so we don't want to hang on to
media forever exist is prohibitive
storage wife so reaper drops the
references to things that would
acknowledge and they're old and then we
have a reclaimer that what goes through
four and looks for all the objects that
don't have any more references and
deletes them now there's a little bit of
built-in delay on that reclaimer so that
for instance if i send an image from if
i send you an image and then you decide
to forward it to a colleague or a group
that we hang on to that for a short
period of time in case you do that so
that you don't have to upload it again
so that's a sort of a configurable delay
but it's it's on the order of a couple
hours usually depending on how much
space we have at the time i mentioned
that the archiving capability so
basically move images that haven't been
referenced in a long time from the fast
storage the fast expensive storage to
the slower cheaper storage and then
finally a cleaning process which goes to
removes were stranded files and most of
the strands come from upload aborted
uploads so somebody says they're going
to send something they try a couple
times they finally give up because they
just really have a crappy network and
can't get it to us or something like
that so the hardware specs we run on the
new system are a little bit better so
and this is that so basically we in our
architecture we try to keep everything
as as common as possible so most of our
boxes are like this they're these dual
octa-core 26 90 s so we have 32 logical
CPUs we pack them with Ram the image
servers which SS the database servers
have 256 gig the audio video servers
have 128 gig we give them so much so
obviously we need the the image service
need this for the database the AV
servers we give it just for filesystem
caching so a fair a fair amount of our
media goes through the system really
quickly so it's sent for me to you you
download it immediately and in that case
it's often going to be found in the file
system cache so it just reduces the i/o
load on our spinning disks which are
obviously limited in what they can do on
our image servers we've got the fast
storage we've got six eight hundred gig
SSD s4 on those hosts and that's for
both the image storage and for the
database storage and then we've got four
terabytes SATA drives on the audio/video
hosts both of these servers all of these
servers are all dual-link aggregated
Giggy front side and back side so we we
have two networks we have a front side
which talks to the client devices and
then a backend which is where we run
Erlang distribution where we do the
media proxying and anything else that we
need to do sort of inter-cluster so
here's a diagram of how this all lights
out so we've got our client up here
we've got our messaging cluster over
here so this is our chat d our chat d
process that it they the client talks to
that over the our proprietary chat
protocol and that's where i mentioned
that sort of the media uploads are
initiated for the upload d duping and
the the load balancing and then finally
after the upload is complete then then
the message is sent by the chat protocol
because it's a message it just happens
to be have media attached to it we use
Erlang distribution in the rest of the
system so the messaging servers talked
to the multimedia servers / early and
distribution they talk amongst
themselves over Erlang distribution
within each and then we use HTTPS upland
download directly to the media servers
from the from the client now within the
media servers we've got as I mentioned
we've got heterogeneous hardware here
slightly because of the disk difference
so the images and the database are
running on SSDs the
the audio-video are running on spinning
drives the database is only on the on
the image side but basically we have
sort of MMD is our Erlang multi media
server and all the auxiliary processes
that run our web servers yaws stud is
doing our SSL termination I'll talk more
about that in a minute we've got MDM
user database on the audio video servers
we we have another clone of the fo
everything except the database itself
and I have one more arrow here we're
doing archiving here from the faster or
to the slow storage and then this is all
sort of horizontally scalable in boat in
both directions as we as we need to grow
so challenges workarounds with this well
one was SSL and this project was
actually being done at the same time as
a different project which was also
http-based aged 2 b.s based and we
discovered that there was with the
built-in ssl with the width or length
ssl which which i believe if i'm not
mistaken the the crypto stuff is all see
basically nifs wrapped around openssl to
do the crypto stuff the ssl stuff
however is all implemented in Erlang and
we found that there was a connection
bottleneck for setting up SSL
connections so which we identified but
basically the throughput the number of
connections we can connect per second
was much less than the raw RSA rate of
the machine so so typically your ssl
connection rate is limited by how many
RSA private key operations you can do
per second on that whatever your
hardware is so our hardware can do
somewhere on the order of 3,000 a second
of for 1k private key our essays but we
were only getting like seven or eight
hundred SSL connections per second so so
probably prior to whatsapp i was
actually a yahoo and one of the
techniques we did so we ran into
bottlenecks with built in ssl as well in
our case it was with apache and we
discovered that we could actually
improve performance by terminating the
ssl outside of the web
server and then and then basically using
loopback connections between the the
external the the thing that was doing
the termination looked at a couple
different options I had experience with
s tunnel but there was a newer one
called stud which looked very promising
how to had a good sort of scalability
model where you can run as many
different stud processes as you want and
each of those is going to handle as many
connections that they want so it was
good for us to be able to scale it out
across our many core boxes so we patched
jaws to be able to accept a H a proxy
style header which so one of the
problems with terminating SSL outside of
the web server is now the web server
doesn't know what the AE doesn't have
the connection parameters of the in
client doesn't know what the IP report
is so we want to be able to have that
just for logging purposes if for no
other reason so we needed a way to pass
that from yas I'm sorry from stud yah so
we so it was a pretty simple patch deal
has to be able to do that we found with
this that the request right was now just
limited by the RSA rate so it's so we
can do a connections at at close to the
to the raw RSA rate of the box which is
good one thing we did run into sort of
the system was scaling up though was
that with the multimedia stuff some of
these connections are very long live
because clients may have connections
that are pretty slow so they're
downloading something large they may be
connected for a long time trying to
download that and and so we'd get even
though at any given time there aren't
that many connections maybe there aren't
that many requests being being executed
the download the long tail the download
could be you know on the order of over
100 or maybe 200,000 ports being used
well on loop back if we're just using
one loopback address we've only got 64 K
tcp ports well there are few somewhat
fewer than that because some were
reserved so in order to deal with that
we just have multiple loopback addresses
so we have a 127 00 01 02 03 and and
stud just for brown robbins between this
so basically we can get as much
scalability as we need in terms of
concurrent connection
our next challenge was with sin file so
I don't know how many of your familiar
with sin file but it's basically if I
want to serve files from say disk to a
socket just to the network in the simple
way to do that is I read from the file I
right to the socket a tree from the file
is write to the socket well if you want
to optimize that and let the colonel
instead of bringing all bring all that
data into whatever your user processes
and then writing it back out so now it's
coming up it's coming into a kernel
buffer than it's coming to user space
and it's going from user space back in
the kernel buffer and back out to the
out to the network drive send file
allows you to basically bypass bringing
having to bring that stuff into user
space well doesn't work so well at least
it didn't for us on freebsd with trying
to use async threads so we want to be
able to do as many of these iOS as
possible in concurrent manner so we need
so we know we need to have a sink i/o
going on because we don't we can't have
the Erlang schedulers blocked on slow
spinning disks or even blocked on fast
non spinning disks so what we were
seeing when when we tried to use send
file with async threads is that beam
would just stall for a long time so you
get a whole bunch of transfers and then
it would stop and then it will go a
whole bunch of transfers and stop and so
throughput was really poorly affected
and this was the case with both file
send file which is the OTP version I
think which started in our 14 or 15 and
also that also with the odds driver
itself so i simply disabled send file in
yas and now everything's getting copied
through you know is doing sort of the
naive read from file right to socket
read from file set right suck it but now
we can we can run a thousand async prox
and to be honest you know modern
machines have plenty of memory bandwidth
we're not we're not constrained by how
many bytes we can copy from place to
place in in the memory at least with
what we're doing may be different with
nuclear explosion simulations but not
not for what we're doing next challenge
indonesia table sizes so as the system
scales up we get
even though this is a store and forward
system there's still a fair amount of
stuff that's in flight at any given time
so per host we got up to in the last
month or so we've gotten up to about a
billion objects / host so there's 250
million or billion database entries per
host so is it 250 million objects and 7
or 50 million references to those
objects there's a limit to how much RAM
we can stuff in a host and we want to
keep this in RAM because we're trying to
serve this as quickly as possible so we
moved from a so the reducer our memory
consumption on the database as we move
from a from a sort of naive here's my
here's my tuples you know here's my
record with nice records nested records
and tuples and and strings and binaries
or whatever to it to a highly optimized
packed format so we took as many record
fields as we could that that could be
enumerated you know whether it was the
geometry of an image or it's the
encoding type or all these things could
that could be sort of expressed as as
packed fields in an integer pack those
all into 60 bit integers 60 bit integer
being the biggest integer that you can
store in a single Erlang emulator word
without having to do extra boxing on top
of it so so you know in some cases that
means we've got three of these integers
and we do some bit string manipulations
to get the stuff in and out but it saves
us a lot of space in some cases we have
keys which were a tuple of you know I d1
and I d2 and that's there's actually
quite a bit of overhead to doing that as
opposed to just having a concatenation
of the two binaries in a single binary
so we did that as I mentioned we hash
the file content to to generate unique
IDs that we can use to upload to do d
duping on the uploads so originally each
of the hashes so the IDS were originally
derived as a one-way hash of the file
hash and so we'd have to store the file
hash to know what it was
by simply using symmetric encryption of
the file hash to generate the IEDs now
we can we can do a two-way operation we
don't store the file hashes anymore so
that's safe space do ultimately we're
able to shrink the the storage down
about forty-five percent so almost
almost in half and so this is what it
kind of looks like now we've actually
just done a database split so we got to
the point where we couldn't fit anymore
in a single box alright well we had to
double the size of the the the database
cluster as I showed you in that slide
earlier it's all sort of power of two's
based off of the fragmenting so we had
to double it because it was just getting
too hard operationally to keep it just
you know is running just as sort of
limits of how much RAM we have so this
is pre split but you know got over a
billion entries in amnesia on this on
this host and then we had eight of those
hosts so there's just a whole lot of
Records going on at any one time but
it's been it's been working really well
for us okay so here's sort of an example
of kind of learning learning a little
interesting things about Erlang and its
associated stuff in OTP so the reference
table is an ordered set key by the ID
and the reference in the reference is
just another big random number so we
need to know is this object business
object how many references so the naive
way to do that would be ok get me all
the rafts out of the table for that ID
and then see if the length of that list
is 0 it worked really well until you
know a few mm few weeks yeah take maybe
six weeks in discover that sometimes you
get a video which is so awesome that it
gets forwarded a million times in a day
so now this goes and gets list that's a
million entries long and all you really
care about is whether it's 0 or whether
it's empty or not so you know just
there's these little optimization cases
just come up all the time so in this
case this could be translated into
basically doing an amnesia
next with the ID since it's an ordered
set the next thing after the ID is an ID
with that ref and I as a reference for
that ID unless that ID doesn't have any
references in which case it'll be
another ID or something like that so
this simple tap this thing basically it
does one query on the this one look up
into em neva which is ultimately going
to do a nets look up and can tell you
quickly whether or not there's any
references to it so this is similar
thing just sort of managing database
growth when the system was launched
doing those clean up those maintenance
processes the the list that would come
back from selects on the database
weren't that big but then they started
getting into the millions at a time and
and passing around a list of a millions
doesn't work great so after you know the
first cut was okay well we'll do the
Select and then stick it in a table and
iterate through the ettes table so now
we don't have to pass the lists around
well even that even that doesn't scale
very well because now the the Select
itself just takes forever flexible
continuations work much better I can get
me a thousand things at a time we're not
that concerned that it's a that it's a
snapshot in point in time we're not
using transactions anyway so we're not
worried about isolation of this select
but this gives with a lot more
scalability on doing those types of
things we've had to do a bunch of DB
migrations like for instance week we did
the we did the packing of the objects
we've got a complete of a record format
so now we gotta gotta sort of change the
engines of the plane while it's flying
so we do that while basically by doing a
lazy migration so we so we have both
tables active and any reads would read
from the new table if there's not found
there read from the old table when you
go to write it back or write a new one
you write it to the new table and you
delete it from the old table and then
deletes would delete from both tables so
this would just allow us to sort of as
the systems running gradually migrate
things from the oldest system the new
system since the urn the old table to
the new table since the new table
smaller we know we're not going to run
out of space because things are
only going to get smaller the more it
goes again we're not doing any
transactions but everything is for
that's operating on a single ID is going
to a single process so it's all
serialized so we don't have to worry too
much about integra firend integrity
problems and then finally when we enough
of the conversion has happened lazily
then we go through do a final select get
all the what's what's left in the old
table and do migrates on those ok
bandwidth was a big thing to deal with
it turns out our users are super hungry
to share media awesome videos and
pictures and the single host would
easily fill up its two gig of uplink
bandwidth without even breaking a sweat
so we've sort of been dealing with a
soaring bandwidth bill and this is where
the server control transcoding is
advantageous because now we can take a
look at well what what are the things
that are costing us the most what are
what are the biggest things what are the
things that are most popular which when
you multiply it out cost us the most in
terms of bandwidth so you know we
constantly been tweaking the media
parameters to cap sort of use so believe
it or not so sometimes a lot of times
we'll gets a video from an iphone which
is recorded 1920 x 1080 so full HD on
the phone at like 18 megabits a second
just ridiculous and coatings which is
going to be sent to you know another
phone which is going to go on a screen
this big so we can buy by looking at
those and saying okay this is ridiculous
this 12 Meg video does not need to go he
sent to all these clients we can
opportunistically transcode those down
and down convert them to something a lot
of cases those really big ones will
shrink by from you know down to five
percent or two percent of their original
size it's it's just remarkable some
other things we do is we count the the
number of thing times a certain thing
gets downloaded and force it down
conversion if it's like a super like i
like i said it's something that's
getting forwarded
millions of times sometimes we have to
sort of force that to slightly smaller
worst quality just to deal with that
issue so what was that where the
lingering issues we don't have any
storage redundancy on this it's like I
said it's an in-flight it's a transient
message store so we're not sort of
worried about long-term redundancy but
it is it is sort of a risk that we need
deal with eventually there's some memory
leakage on the database host so over
time they they grow their ram the ram
usage grows and haven't been able to
track it down yet some sort of
fragmentation in the ettes tables or the
binaries i mentioned that schema ops
under load our are difficult i mentioned
that we're tweaking the the transcoding
for for playback issues that come in
from our customer support people or that
we notice now so so we're training off
squeezing things our bandwidth against
our CPU so we don't have unlimited CPU
that we can use to transcode things so
there's sort of a balancing act that has
to be done between how much how many
things we shrink versus how much CPU we
have to do it and then obviously ongoing
capacity and planning management
capacity planning and management here's
here's how here's what this memory
leakage looks like so there's basically
no growth in the database here but you
can see our Ram utilizations going up
until we do a restart then starts down
here it goes back up we do restart
ghosts aren't sign here so this is this
is one of the issues we we're still
trying to identify this is this is what
cpu utilization looks like on our audio
video service which are doing the
transcoding so it serve follows our our
daily peer our daily periodicity in
terms of traffic but you say we're going
in you know we're using that all the CP
so as I mentioned we have 32 logical
CPUs on these things so there's 16 16
cores hyper threaded but you know we can
get up over 80% utilization on this
system which is pretty awesome so what's
our peak scalability so this isn't the
capacity we haven't hit our capacity yet
but over New Year's Eve we transferred
214 million images that
day at a peak rate of about eighty eight
hundred images per second which is it's
a lot of pictures per second the thing
was blown us away was how much bandwidth
we could push out of just a small number
of boxes so 29 gigabits was our peak
back out pit bandwidth on that day so
this what that holiday week looks like
so here's Christmas here so you know
kind of a peek here and it's more of a
day-long peak for images you notice that
video down here is more popular on
Christmas than it is on New Year's but
the reverse is true for new years for
images images are more popular on New
Year's then on Christmas you can also
see that there's a couple different time
zones hitting in terms of where Chris
where New Year's Eve is hitting but you
can see it's a pretty good spike above
but our normal peaks were so we need a
fair amount of headroom on this system
to deal with that so ultimately it
turned out Erlang is just fine for
pushing lots of bytes but it's not Sarah
Lee good at everything so I mentioned
the SS doing the SSL termination we move
that outside didn't certainly didn't
want to deal with doing like the media
identification or the transcoding of all
the different video and audio formats
when there's a great tool ffmpeg that at
our disposal so we just found that it
was much easier to integrate Erlang with
the existing tools to do that another
thing that this turned out to be great
for was it was for the transcode configs
themselves so a lot of these to these
tweaks require code not just kind so
it'd be hard to create a sort of a
configuration language for you know this
map to this it it's really nice to
express it in in Erlang code and allows
us to deploy the changes quickly we
don't have to reload a config file we
just you just push the code out load the
code and now that the configs are are
are live and this raises the value of
doing the server initiated transcodes
because we can do it on the fly well the
systems running so there's an example
here a couple sample config so this one
so we're doing a pattern match here for
Android devices that are getting video
they can support as as a container type
mp4 and 3gp they can support video
codecs h.264 mpeg-4
I h.263 this is the cap on the geometry
that we want 720 x 480 we give them a
maximum bit rate they can support AAC
and AMR codex on I and so so we kind of
match this for for Android so we can
tweak this for all android devices if we
wanted to drill down even more we have
the device type itself like we want to
do a gs4 we can do this so we can we so
we can be even more specific on iphone
here's an example of that on iphone it
supports mp4 quick time but not 3gp it
supports 264 and mpeg-4 in the default
case we're going to do 480 x 360 at 15
frames a second which is what i think
that 3g that are the three can do the
iphone 3 but then if it's an iphone 5 or
4s or or four then we can do 1280 by 720
at 30 frames a second so they so we can
tweak these things per device we're
going to handle this all on the server
side we can roll these things on the fly
and it just really makes it nice here's
here's an example of taught what top
looks like you probably can't read it
because it's kind of small what top
looks like on one of these systems so we
run the Erlang schedulers unbound so
they're so because we're running all
these other things on the processors at
the same time so we've got being up here
that's using a hundred twenty percent so
that so again this is because this is a
multi-core machine it doesn't add up to
100 adds up to much more than 100 so so
120 up here these are each of the stud
proc so we run 16 of the stud prox and
they're each using about four to five
percent of the CPU for doing the SSL
termination and then we've got a bunch
of these ffmpeg switch are running down
here doing transcoding they're nice so
that they don't get in the way of
serving you know so a single tranche
code doesn't get in the way of serving
everybody else's media but you can see
they use a lot of CPU so this guy's
running we run the ffmpeg single
threaded so that it doesn't doesn't
swamp the box but so this one's using
they're using about forty-five percent
each of a cpu not in like it since there
are sixteen cpus that's plenty
done any questions yeah no we keep track
so so basically so the question was what
what's the database representation of
one of these transcode links so there's
a there's a transcode table which which
is keyed by object ID and then it's a
bag so it has multiple entries each
entry has the media parameters of the
transcoded version and ID of that
version so so for instance you know so
let's say we send a quick time is
somebody somebody records a video on
their iPhone it comes in quick time we
send that to an Android device Android
can't play quick time that that download
default thing would say well you can
support mp4 and 3gp mp4 comes first so
we're going to prefer that so then what
it would look in the transcode table and
try to match are there are there any
existing trance coats that meet those
parameters do they have you know does it
meet the container type does it meet the
codec type does it does it fit within
the specified screen geometry just have
the right bit rate and so it can just
scan through that list and if it finds a
match then it has the ID it can and they
can serve that object instead and then
when we do a trance code we always
update that table with with with the
entry as it turns out there aren't you
know the videos well I don't actually
have the figures I think videos that
there aren't as many transcode links as
I would expect but it about it's about
even split between transcodes that we
that we do on the demand and ones that
are served that are already been done so
I think it's about fifteen percent get
transcoded on the fly because we don't
have a matching transcode and about
fifteen percent are reused because we
already have a chance code that matches
optimizations for a group chat versus
versus a one-on-one chat well only to
the extent that that because I mean this
was actually true of the old system as
well but the same object is sent to
everybody the difference now is because
each person in that group has a
reference we don't have to keep that
object forever so as soon as we know
that everybody in the group that it was
intended for has acknowledged that they
downloaded that media then we can then
we can remove it but we don't do any
specific sort of you know we're going to
pick a transcoding that most of the
people can do we started do that in a
global on a global scale so when we
first started out a lot of the trans
codes were very specific to the device
that was going to but now there's a
little more generic so that we can reuse
as many transcodes as we can so that
we're not spending a lot of CPU on
transcode and we've we've as I mentioned
we've capped some of the media
parameters originally we would serve as
good a quality as we possibly could so
that could be the original which was a
12 mega by full HD video but that just
becomes cost prohibitive especially if
that gets forwarded so so we've capped
that and besides it not only is it save
our bandwidth bill but it saves the
users bandwidth bill too so it's it's
kind of a win-win when we make those
determinations that well let's shrink
this to something that looks good but
it's not outrageous yeah back there
no this is worldwide why is this one so
tall I'm sorry say it again yeah yeah
okay yeah these well I'm are so the
question is why why do we have this
daily variation one reason is because
our users we have a lot more penetration
in Europe which is one reason and this
is and we also have the Pacific Ocean
here which where there's not a lot of
people so they start out in Asia so this
is like Asia India Europe and in some of
the others in it it turns out that
different media are more popular in
different places so so pictures are
super popular in Europe videos popular
in other places and and so this graph
doesn't necessarily reflect what our
messaging traffic look like it just it
happens to be what the picture traffic
looks like and also when we get to when
we get out of your you know Europe's in
what to mostly in two time zones whereas
once you get to north america or once
you get to the America is now you've got
you know six or seven I guess that that
stuff gets spread out over
they're there in the East Coast yeah so
the question is would it would it make
sense to distribute these other places
turns out not really i mean our hosting
provider is basically the same for us
wherever we because this isn't super
latency sensitive we're not doing
real-time voice or video you know these
are bulk uploads and downloads so they
so the latency to where you know between
the user and the the host the the
servers isn't doesn't really affect the
user experience it's just easier for us
to keep it in the US for now
I don't think we are we haven't said
anything about that yeah yeah that's
just a rumor physical limits in terms of
what well most of our systems are we we
feel are pretty horizontally scalable so
you know as so we don't we don't think
that there are we don't know of limits
like we're really worried that once we
get to this many users are this much
usage that we're going to have we're
gonna have to re architect something you
know this this system was there's this
system and there's our address book
system that the one the the one that
helps you that helps keep your contacts
straight those are the sort of the last
two that were built on a legacy platform
and weren't as scalable as we'd like now
everything's sort of more horizontally
scalable on the Erlang platform and so
we feel pretty pretty comfortable with
it sure
no so the question is do we experience
any problems and and I won't be upset if
you guys leave I know we're over time
now in like lunchtime we experienced any
problems with chattiness on the early
distribution and the answer is pretty
much know most of our so so we architect
the system so that they don't it's not a
many-to-many communications thing so you
know we have specialized classes of
servers they're all it's all one airline
cluster but but we have like little mini
clusters of specialized functional
servers and they talk to other groups of
things so the only thing that really
goes all tall is the is the the
distribution of heartbeats which to this
point have it causes a problem we've had
more problems just with the sheer volume
of even just one to one some of our
biggest problems are actually amnesia
replication between a pair of hosts
because on some of our hosts the the
update rates you know the event rates
are so high that that we get these
little we were over running the switch
ports you be on the on the back end
network so we we went through a period
of sort of growing pains kind of like in
the fall last year where we were working
with the hosting provider to tweak their
switch configs and we were and this is
where we started going to do links on
everything to double the buffer space we
make some tweaks on our side so our
biggest problem has been on the
distribution side has really been over
running the network gear because the
boxes are just able to pump out packets
so fast under the network that we
overwhelm switchgear
yeah well I don't know if you saw my
talk from last year I talked some about
that the tooling that we used in that if
you haven't had a chance to look at that
for this it's a lot of actually just
kind of watching the this system I mean
the one thing that that most of our
systems use kind of in as we're planning
capacity while the systems running is a
utility I wrote in Perl so when I first
got to when I first got the whatsapp
it's like okay I got to learn this
erlang thing so I start spent a couple
weeks like you know read the book and
look at the syntax and stuff and like
okay I'm not getting anything done so I
with the Pearl wrote this utility to to
basically monitor to monitor system
statistics like network packet rate and
bandwidth disgaea ops virtual memory
like I think like these guys these guys
are generated out of those systems so
for for the mms stuff and actually for
most of our stuff when i'm not doing a
hardcore like let's find out where the
next bottleneck and beam is it's more
about looking at at these types of
graphs and trying to figure out
especially when we have glitches like
will you know we'll have some sort of
glitch go on in the system we can go
back at those logs of the system stats
and say okay here's what happened it
looks like we had it looks like packet
loss in the back end went through the
roof and so these things stop being able
to talk to each other and that's when we
go back to the network operations guys
and like are you seeing anything on your
switch graphs and and so on yeah yeah
does the publisher
transcoding happens when it's downloaded
yeah the only case the only case where
it happens not any response to a
download is that are those cases where
we're tracking how many times
something's been downloaded and once it
reaches a threshold like okay we've
already spent you know it we've already
sent 20 megabits on this thing so now
we're going to try to down convert it
because it because it's basically okay
here's a threshold for when we'll look
at objects and we'll look at the object
and say okay it's it's bigger than than
our minimal size so if it's so if it's
it if something is smaller than the
maximum size that the recipient is
configured to accept but it's bigger
than our minimal size then we'll just
send it through as long as they as long
as it doesn't need to be transcoded but
once it once we've once we've sent that
object enough that it looks like okay
this is maybe one of those things that's
going to get forwarded a lot then then
in the background will initiate a trance
code to shrink it down to that minimal
size and then from then on people that
come in will get that minimal size
looking at using that transport lookup
table
right great yep right exactly yeah and
you can see this you can see that delay
something when you download a video like
if you know if for instance you send a
bit you record a video on iphone you
send it to Android you can see the delay
you can see the transcript delay because
you'll say download and it'll wait it
there won't be any progress on the
progress bar while it's doing the
transcode and then it'll start trying to
downloading so you can see that delay
yes sir
well so the question is how we have we
tried to front end it with something
that would cash I frankly if I was in
front end it I probably use the same
stuff I mean it you know as ice as I
said sort of in the results I think that
this is pushing bites as fast as we can
and you know we could we could add more
RAM on to those a bee boxes and cut down
the aisle load on on the spinning disks
kind of our so our image servers are
constrained by ram for the database and
size of the SSDs and we just keep
pushing for bigger and bigger SSDs as
they become available on the on the
audio video servers we're constrained by
seeks you know see capacity on the
spinning disks and transcode CPU and so
we try to balance it so that we run out
we run out of everything at the same
time like we run out of RAM we run out
of CPU run out of de seeks all at the
same time and that's what we know we
need to add more machines but so far it
feels like we can we can push as much as
we need to out of these boxes in fact
going in after after Christmas we went
back to our hosting provider and said
you know we pushed this much output what
what happens if we push for X that on
you because we weren't sure what the
difference between Christmas and New
Year's would be and they were like oh
yeah we got we got you covered and
because i think you know like i said
those audio/video boxes can easily push
to get gauge so we could you know we
could we have an upload uplink
capability of you know 100 gig or
something like that but which is a lot
for just a small number of machines
anything else yeah
hmm yes well okay so the question is
what happens if the server that's
hosting a particular object fails and
that's it goes back to one of the
lingering issues I said where we've we
don't have storage redundancy now so if
it's if a server becomes unavailable so
let's say it crashes and it's in the
process of rebooting at that point users
will just have to retry the download so
their downloads will fail they need to
retry those when the server comes back
those downloads will succeed if we
actually lose a drive then we lose that
media which which like like I said is
not optimal we have we've had pretty
good experience we haven't lost too much
media so far like I said this is a it's
not an archiving service it's a
transient message store most of these
cases that media is still on either the
sender's phone or maybe somebody else it
was sent to so it's not like we're
losing something that people are
expecting us to store for them it just
happens that that particular transfer is
going to be lost because we lost that
drive but it's something that we need to
address thing else yes sir
we used to get lots of problems with
network partitions causing amnesia too
right so a question is any problems with
amnesia operational problems with
amnesia especially around network
partitions we've been pretty lucky our
backward our back-end network is pretty
reliable and so it is but that's part of
the reason that we keep our Island small
so even though our clusters big the
number of hosts that are sharing one
amnesia schema is small so the chances
are much smaller that there's going to
be a partition within an island but it
does I mean it does occasionally happen
and for most of our cases it's just a
matter of we we kill that kill we kill
one of the note we figure out we decide
which of the notes needs to be killed we
kill it we started it replicates you
know it loads everything from the pier
and we're up again so it in practice we
it's it's a well-known sort of recovery
strategy and our back in network is
reliable enough and we kept our Island
small enough that it hasn't really
caused this too much pain so far
oh sure yeah right so cases of no down
when the network's not down is just the
pings failed we that doesn't happen very
often for us and you know off the top of
my head I can't say all the things that
we may have done to tweak our
environment you know so we have done
some tweaking of sort of distribution in
terms of buffer sizes I think we changed
the default tick time but I can tell you
what we change it to and we don't get a
lot of cases like that I mean usually
want to know goes we going to note down
it's because the node went down either
its network went away or it blew up at a
ram or something like that it crashed
for some reason they're there very few
cases where we get a note down where a
note it was just kind of in a funky
state where it was up but not up or not
up this far as distribution thought okay
I'll let you guys go to lunch thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>