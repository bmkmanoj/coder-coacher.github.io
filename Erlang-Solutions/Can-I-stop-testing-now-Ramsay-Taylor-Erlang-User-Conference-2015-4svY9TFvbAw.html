<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Can I stop testing now? (...) -  Ramsay Taylor - Erlang User Conference 2015 | Coder Coacher - Coaching Coders</title><meta content="Can I stop testing now? (...) -  Ramsay Taylor - Erlang User Conference 2015 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Can I stop testing now? (...) -  Ramsay Taylor - Erlang User Conference 2015</b></h2><h5 class="post__date">2015-07-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/4svY9TFvbAw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so I'm I'm Ramsey Taylor from the
university of sheffield but i'm i'm
working on the prowess project you've
probably seen quite a few of my
colleagues around and they've been
presenting things the prowess project is
a great big European project studying
testing for Erlang so I've got
colleagues doing test generation I've
got colleagues doing novel testing
strategies I've got various people doing
it from ways of testing and I've then
ended up being the one has to check on
everyone else it's a kind of who watches
the Watchers situation so that's what
test adequacy is going to come in so
what does test adequacy means so you're
all Erlang developers and you will have
nice bits of elanco I seem and hopefully
you're writing tests I've only had one
conversation so far with someone that
said oh yeah I should probably write
some tests yeah you really probably
should write some tests so this evening
evil got your Erlang I've seen you've
all written some nice test for it and
I'm going to suggest that you've got a
problem it perhaps you don't think as a
huge problem and that's the huge problem
that you've written your tests and you
run your tests and they will pass now I
think most normal sane people they write
load of tests and they run the test and
the test pass and they feel very happy
you know my code must work it's great
i'm done i can go to the pub it's
excellent well I I'm the guy but
unfortunately is employed to sit there
and when all of your tests pass I get
worried I think well if you've really
tests and you've run all of these tests
and they all pass and there was nothing
wrong well then obviously you didn't do
the right test Sid you because clearly i
can write 20 tests that will run 20
things but they'll pass and that won't
prove anything at all so the real
question of course is having written all
your tests have you actually tested
anything what have you actually tested
and this of course can be exacerbated
when you get to the test generation you
can spend lots of time and you can write
a beautiful quick chek model and it can
be a perfect quick chek model and then
you run quick check and it generates a
thousand tests on those tests all passed
and you generate two thousand tests and
those tests will pass you donate three
thousand tests and those tests will pass
well do you stop there or with the three
thousand first test have been the one
that found the really critical bug okay
so you do four thousand tests and then
would the four thousand the first test
be the test that around the really
critical bug so so you've kind of got
almost two problems one problem is you
know do I need to do more testing and
the other problem especially if you're
not having to write the test you're
generating the test
okay window a stop I mean I can run
quick check for three years and it
doesn't finally think does that mean it
wouldn't have found anything on the next
day so what we want from a test adequacy
metric and I'm going to suggest to you a
few of those in the next 30 minutes but
ultimately what we're asking for when
we're asking about test adequacy is
firstly have we actually tested all of
the code we built this big Erlang system
we want to actually test it the very
first question is are there bits of it
that we're just not even trying because
that would clearly not be enough testing
the second bit is have we tested in
meaningful ways i'm going to show you
some examples in a minute where you can
test all of your code but you haven't
actually found the bug you've tried it
but you haven't tried it in in the
useful way or in the way that's going to
stimulate something to go horribly wrong
later and then of course if we answer no
to any of these it's also quite useful
of a test adequacy metric doesn't just
say no you need to do more it actually
says I can you need to this bit more
because otherwise you just end up kind
of flailing around or I'll try this so
it doesn't make it happy I'll try that
that doesn't make it happy you can run a
quick check for another three hours and
it still doesn't make it happy it's
really nice if the test adequacy metrics
can give us a bit of useful feedback so
what I'm going to talk to you today is
I'm going to start by talking about code
coverage because that's usually the
easiest one to sell people so so cover
is already built into OTP and if you're
just running rebuy you probably are
running cover in the background you just
may or may not bother to read the output
from it so so code coverage is at least
a start for test adequacy metrics
although i'm going to mostly tell you
white cover is actually not a very
useful test adequacy metric and we can
use some much better ones but code
coverage as a test adequacy metric
certainly can help you with that first
question if have you actually bothered
to test everything and that's useful and
i'm going to show you some some more
advanced coast coverage metric so i'll
help you test it in meaningful ways and
all of that kind of his maybach of code
am I really exercising it but then there
are some complimentary techniques so I'm
going to talk briefly about mutation
testing if code coverage is saying have
you exercise the code that you've
written mutation testing starts asking
well what about the code i could have
written instead or i might have written
not to mention am i going to try testing
my code in a way that i would just never
think of by looking at it
and the third and final point on that
one is often the most useful one you can
do code coverage you can exercise all of
your code and your code or runs and it
doesn't crash and it all execute and it
gives an answer but if you actually
bothered to check whether the answer is
right because that's a fairly critical
limit on on testing if you've run a
thousand tests but you haven't bothered
to check whether any of the answers are
right then you're definitely not doing
it and finally I'm going to mention
model inference I've got literally one
slide on mattress because it needs to be
dropped in model inference is a
completely sort of novel way of of
looking at test adequacy so I sort of
want to mention it to get you to think
about it because I think you might be
useful okay so I think we're gonna start
with some code coverage okay I've got
the dumbest airline program in the world
and I appreciate that this is the Erlang
user conference and that you will elite
Erlang programmers and you can all look
at this and first of all you can all see
exactly what's wrong with it and
secondly you can all say that none of
you would ever write code like this I
needed something will fit on a slide and
would prove the point so if you can sort
of suspend disbelief about this
particular alone program what I want you
to try and imagine is some sort of
thousand and fifteen hundred line like
module and some crazy guy was writing at
two in the morning and then he comes in
to work and he gets hit by bus and then
another guy has to take over maintaining
it so he changes it a bit any tweaks it
a bit and he adds a few bits but then he
gets fired because he was drunk and now
this third person is taking over and and
now so you've got this thing and it's
there and it it works and you need to
test it and you need to maintain it be
try to pretend you can't just glance at
it and know what it does so so some of
the code coverage metrics I'm going to
talk about in a minute and they're the
reduction was actually really pushed by
a NASA study where they NASA looked at
their own cone I think some of it was
their their mission-critical code and
they found things where various people
had worked on and if we take this long
line here and we call that a decision
because you decide whether you're going
to do this bit or not if that's a
decision and then the sub parts of it
the a equals zero the beer is great for
if we call those conditions this this
NASA study found bits of their
mission-critical code where they had one
decision that had over 20 conditions in
it all sort of nested and composed and
conjoined and and there may be a couple
of you know professors of formal logic
in the audience that could have read
them and understood it but normal people
just had no idea how this thing was
going to evaluate so you so ignore the
fact this is small ones
and imagine that this is big and
complicating you can't just glanced at
it so we're going to test this program
and to start with I'm going to show you
cover I mean again I don't know how many
people have even seen the red and black
output you can get from cover two three
okay reasonable number of people
actually look at some people just run
rebar in it so it's fifty percent and
that's all cover will at least give you
this much coverage but i want to show
you why this is really not actually
enough so we want to test this this is
just a function called DV that takes two
arguments int and it it might divide be
by a or it might not uncertain
circumstances so let's let's run it so
we call DV with 0 and five and that
executes the first two lines and this is
my first criticism of cover of course
imagine if this wasn't just be imagine
if there was 50 lines in this top block
and only one in the bottom okay your
code coverage is just hit a ninety-eight
percent or something because you did one
one half of it and that was most of the
lines so counting on lines is a silly
way of counting anyway but nonetheless
we've done one test and we've tried two
thirds of the program as far as covers
concerned so we do another test by five
okay so a is not equal to 0 so we're
meeting to execute the other bit so
that's great we've tried tried the whole
program and okay it's a small program so
it should be probably a small test set
but as far as as code cover as cover as
a code coverage metric is concerned
those two tests give you a hundred
percent coverage you you're finished
hopefully everyone in the audience can
see that no there's a really big problem
with this if we call this with 0 &amp;amp; 2
okay so a is equal to 0 but B is not
greater than 4 so that and is is full so
then we're going to do the bond blog and
then you get a divided by 0 and that's
that's really bad but you can you can
imagine a circumstance where one guy has
written a condition and another guys
come along waiting I don't really want
it to work when it's bigness so he tacks
on a condition and a third guy tax on
the condition and and quickly the kind
of route through this becomes decidedly
non-trivial so so although code coverage
with cover told us that we were finished
we definitely aren't finished so we
certainly want something more than cover
so there there's various points in
between cover and this car cover is
literally just doing what's called line
coverage have you covered this line or
not from there I thought I talked about
decision you can do decision coverage so
you have to take every branch and not
take every branch new condition coverage
where each of the conditions is true or
false but eventually the thing
recommended by this NASA study was
modified condition decision coverage
which is hard to say but is quite good
so so rather than Janine cover works by
just inserting instrumentation to decode
to see which line gets executed so
rather than just saying this was cool
there it wasn't we're going to at least
keep track of how it was called and
we're going to do some evaluation on
what you haven't just evaluated this in
what circumstances you've evaluated this
okay but rather than as I say if if in
our previous program instead of just
having one liner had 30 line sequential
what if you execute the first one you
also execute the other 29 so we don't
necessarily care about these blocks of
code what we're interested in is the
decision points we're interested in okay
do I take this and not take this up
there in various different tests and
thirdly I mean as I say you can do
decision coverage where for every
decision point you take it and don't
take it in different tests but what we
really want in mcdc is to not just take
it and I'll take it but look at all of
the ways of taking it and not taking it
so we had an and so you can make an and
true by making both sides true you can
make it full spy making the right side
true false make your full swimming left
side for me or false by making both of
them false there's various ways through
that and as we saw with this example if
you cherry-pick just one way of getting
through it you can then get the rest of
the code to work even though a different
choice of getting through it might make
the rest of the code fail so what we
want to look at we can't look at every
possible value for a and B so we need to
sort of have some classes of them we
need to look at you know what's
interesting combinations of values and
envy so we look at what the program I
thought were interesting if he's chosen
to to switch on these particular
conditions then let's try interactions
between those conditions I've put
reasonable and brackets because it's not
the the complete cross product of trues
and falses there are some cases that are
clearly impossible as I say whether with
an and there's only one way of making it
true it's not not interesting to look at
different values that make it true and
and conversely there's there's lots of
ways of making it false but some of them
are impossible as we'll see later so we
don't need the complete cross product
but we need to look at all the
reasonable ways of
taking or not taking each branch so
here's some output of a much better tool
that i wrote course mother but it's
going to give us a bit more than that i
think i might show you live just because
it looks a bit cooler so although this
is showing us where are we although this
is showing us we can add another test
and now we've got both decision and
condition coverage so if we include this
one we've tried both the conditions true
on both the conditions false we've tried
the decision true and we tried the
decision Falls but we haven't actually
achieved mcdc coverage and indeed you
can see although I've made three tests
here we haven't found the bug so what we
can get from smother is slightly more
information rather than just showing
that we have and haven't tried things
what we can do with this is then let's
do the last here so smother has a
compatible ish interface to to cover so
we smother compile something then we run
our tests and then we analyze it to file
there we go it's gone green but now we
can actually look at the decision and
talk about not only have we made it
false and haven't we made it false which
is shown at the top but also the
meaningful ways of making it force but I
say there's no meaningful ways of making
it true other than everything being true
but when we make it false there's
actually a sort of matrix as
possibilities we can have one side true
and the other side false one so I force
the other side true etc so that what
what we're seeing here is hopefully
exactly what the problem was that
although we've made this decision false
we've never made it this decision Falls
whilst a equals zero was true and of
course you can see that that's exactly
where the bug comes so so there is a
path to that second branch where does
the division in which a is equal to zero
but we've never tried that one so that's
quite fun this sum this thing gives you
a much richer notion for for your code
coverage than just cover and so
internally this thing is stored as a
tree of possibilities for the decision
it's got a tree of true and false and
then for the conditioners truthful so if
you ask it for a coverage percentage
rather than being kind of arbitrary
count of lines its account of the leaves
of that tree that you have or haven't
hit so it should be a more
representative sample but okay I've just
taught for eight minutes or something
about term coverage on this dis decision
point but
an if statement but of course you lot
are all functional programmers so you
don't really make decisions with if
statements and boolean conditions and
ads you make decisions with pattern
matching that's that's how half your
decision-making is done but actually we
can apply the same idea that we applied
to that to pattern matching so pattern
matching although it's not a boolean
expression you have the same same
criteria as you go through your patterns
either your patent matches or it doesn't
match and we decide whether we're going
to do the next block based on the
pattern being match to the pattern not
being matched so what's mother will ask
for I mean this is the same problem sort
of rephrased as function headers I mean
it's now explicitly 0 and 5 but the idea
is the same what's mother will ask for
essentially is all the meaningful ways
of making a pattern true and false and
of course usually there's only one
meaningful way of making a pattern true
as in to match a pan but there may be
several ways of failing to match a
pattern you can match most of it and
then get this last bit wrong or you can
match most of it yet but first bit wrong
and and so we can look at the same way
that we look for the conditions in a
boolean decision we can look for the
components of a complex pattern and make
sure that you're actually exploring the
different ways of furling that i'd be
interested to know people who
experiences how often this comes up but
I do this sort of thing all the time
I'll have a series of expressions and
I'll put something into the the program
and it seems to hit the wrong one I mean
it hits too early or it skips one and I
can never figure out why it's because I
put a comma in the wrong place or I've
got too many things on a tuple or not
enough things on a tuple a tuple it's
got seven nested lists and so I missed
the fact that it was a quadrant or
triple and and so on and so forth so we
can extend this notion of FM CD see this
notion of modified condition decision
coverage to apply to pattern matching
which then makes it applicable to all
sorts of bits of airline because you
haven't just got function headers I mean
you've also got case statements and this
is a particularly Byzantine example
because I've had to force it to be a
couple and their match on the pair and
so on and so forth but but you can see
that you've got the same kind of
expression is going down through all of
it and so you want you want slightly
more richness from your test you don't
just want to say I've hit each of the
patterns which is what you get from line
coverage and indeed if you put too many
things on one lie
then cover won't even detect that it
will just detect you hit this lying once
so this is giving us a richer notion of
coverage so that's it applied to a case
statement but we can also extend this
even two lists and so on so lists are a
more complicated structure you don't
just have a list with the two elements
and maybe the first one matches and it
doesn't remove but you can also fail to
match that in a range of other ways and
this is often the point at which some
Erlang programs get upset with me
because I I start saying well you
haven't tried it with an empty listing
you haven't tried it with something
that's not a list and you know maybe you
should explore as possible yeah but it's
supposed to fail that's the idea and and
so we were having a bit of a debate
about whether some of these things
should be included or not and I'd be
interesting to hear people's opinions
but having come from sort of
safety-critical systems my attitude is
you've exported this function someone
might call it with something that's not
a list you know you should test that but
I guess maybe you want that one to fail
but maybe it's worth always trying
things with an empty list and it's
certainly worth looking at if you've got
a pattern that has got an explicit
number of elements in the list then then
having a different number of elements so
you fall through to the next one as
opposed to having the right number of
elements but they're the wrong values
and these are different things that are
worth exploring and of course this can
also extend to to some of the
communications pans so we receive
statement ultimately are still just
doing pattern matching and boolean
expressions depending on how you choose
to do it so even your sort of message
passing between processes you can
actually measure that you're covering
you're sending a representative sample
of messages to a process because you've
actually explored the patterns in the
received statement so okay so I've said
all that about code coverage and you
know Co coverage has its place in it's
very good and it's usually the very
cheap option that everyone wants to do
but I mean the first limitation of
course of code coverage is that you've
written this block of code and you've
exercised all of this block of code but
all that we're really all that we're
really talking about is the code that
you have written not the code that you
should have written you know you've
written this thing and it's particularly
it's a particularly bad problem if if
the programmer is also writing the test
because it's quite common that you
misunderstand a problem and you write an
entire program and then you write a load
a test that are based on the same
misunderstanding of the problem and they
all pass on your code so
even having explored your code in all
the possible ways we're still not
answering the fundamental question of is
this thing actually working we just know
that it's not crashing so at the end of
at the end of having achieved code
coverage although you've definitely
exercised everything you've written all
you've said is that I can run all of it
without crashing so what we might like
instead is an alternative so this is
where mutation testing comes in as a
complementary technique mutation testing
I gather it's really popular in Ruby I
haven't tried it in Ruby and I'm keen to
trot an elixir but I haven't tried it an
elixir but mutation testing all the
stuff I've read about as being in things
like see where you mutate your C and
then you see if it still works Oh rather
you see if it notices that it doesn't
still work but so mutation testing the
premises that we're going to take our
program we're going to deliberately
break it in some way we're going to
insert something nonsensical or we're
going to change something or we're going
to break something and we going to see
if your tests even notice so this is
this is back to our original question is
are you actually looking at what's
happening you can execute a bit code and
it doesn't crash that doesn't mean it
did the right thing and as long as I'm
fairly subtle about how I change your
program if I change your program in your
test doesn't notice that's probably bad
yeah we can have a little bit of a
debate about it but the very early
mutation testing tools would just change
characters and of course ninety percent
of the time it didn't compile and of the
other two percent half the time it was a
comment or something so we're doing
something a lot of rich I've got a
mutation testing framework for a line as
well but I'll introduce in a minute that
actually you takes the abstract syntax
tree changes it on a sort of semantic
level and writes it back again so it's
definitely changing something real with
the code so it's definitely going to do
some different behavior and if it can do
some different behavior in your tests
don't stop passing then you really
exploring what it's doing so I'm going
to Trey we're going to try and simulate
common faults at least that's what they
say I mean you can't try every possible
change to the syntax tree because that
would just be ridiculous so you try and
stimulate common faults either with the
system so if you've got a particular
domain i don't know i mean people
working in sort of financial services
maybe you want to tweak all the numbers
and see if anyone notices if they notice
if I steal you know point one of a
kronor off the end of everything
you test maybe you want to be really
focused on numbers other people that are
doing a lot of networking you might want
to deliberately insert kind of latency
or deliberately insert kind of lost
packets or something and again see if
your tests are actually exploring that
but the other thing they often want to
do is comment fall to the programmer a
lot of mutation testing tools are based
on common typos common mistakes common
sort of idiosyncrasy of sync receives if
you know programmers often type these
things wrong where they often get
patterns in the wrong order or something
well let's insert a few of those and see
if the test notice because if I can
insert one over here and the tests don't
notice then maybe the real one I did on
by accident over there hasn't been
noticed either so there's one or two
sources for for mutants so this is the
the gist of the sort of process of a
mutation testing workflow you have your
sort of Erlang source file up on the top
and you have some set of relevant
mutation operators and the first thing
we're gonna do is identify the points
where we can do it if I just randomly
applied mutant so I'd end up with the
same mutant six times and I'd end up
with other problem so first we're going
to look at all the places we could apply
them and then we're going to apply n of
these random mutations and we're going
to spit out n mutants so we're going to
get n copies of your program each of
which has been changed in just one way
and we're going to see if your test
notice I've done it deliberately like
that some people like to have multiple
mutants in in one file and that's
interesting but I quite like to it comes
back to the the last point on one of my
early slides so you don't just want your
your test adequacy to tell you your
tests on working you want to tell it why
it's not working so by having separate
files with one mutation if there's one
of those where the tests don't notice
well at least we know exactly which bit
it is it didn't notice so that's what
we're going to do for each of those
mutants we're going to run your tests
and we're going to get the test results
for that mutant and if it fails that's
good and this is this is where the whole
language gets very confusing because you
you will use to my test passing it's
great and I'm used to your test fails
that's great so so at this point if you
run your tests on one of the mutants and
the tests fail that's actually a good
sign that means your tests are working
because they noticed I've broken
something and in mutation testing that's
called killing the mutant I'm not sure
why it's quite so bloodthirsty but they
talked about killing mutants and the
alternative is it if the test pass
that's bad you know if your tests still
pass even though I broke it that's
called a mutant that's still alive and
we'll try and eat your brains or
something so so test that parcel mutants
are generally bad off although we can
then have a huge long debate about
what's called semantic equivalence so
it's possible for me to change as
possible me to change your program in a
way that does change your program but
doesn't actually change the meaning of
your program and by mutating the
abstract syntax tree this is a lot less
common than it can be if you do things
syntactic leave you mean hated syntax
you just change the name of something
wrong who cares if you actually change
some of the semantics of it then usually
it's different the problem sometimes as
you get if I reorder messages but
actually the requirements don't require
them to be in order well it's definitely
a semantic difference but it's not a
difference that your boss cares about so
you can have a bit of a debate like that
again it's sort of up to you I I have
this sort of safety-critical systems
background where actually the debate
itself was valuable if you take this
mutant that doesn't get killed and you
take your original code and you say okay
I can see it's changed I can see the
test doesn't kill it but we kind of we
all agree that that one doesn't matter
but then you bother to write up two
paragraphs explaining why that one
doesn't matter I consider that actually
a valuable exercise usually you learn a
lot more about your system by going
through that exercise Island just sort
of throwing the message at any order if
we've explicitly sat down and said you
know what order doesn't matter and we've
documented that that that can be
valuable feedback in itself I suspect a
lot of you aren't agreeing with me but
whatever so so semantic equivalence is a
smidgen of a problem with mutation
testing but often that's a kind of
question for you and it's also a
question for how you choose your
operators so although I've made this
Mewtwo mutation testing framework it
comes with a few operators but I'm not
necessarily suggesting you take it and
you run the small number of operators
I've suggested the actually useful
approach for mutation testing as I say
is to have domain-specific operators if
you're really worried about specific
class of problems you should have some
mutation operators that will simulate
that particular class of problems so
what I try to do with this framework was
actually build build the thing that
would allow you to express mutation
operators
in a fairly rich way so you can come up
with some interesting mutation operators
that are relevant to you so for that
purpose we use the the Wrangler
refactoring library which I think may be
quite a few people have seen and quite a
lot of people have seen it as a sort of
Emacs plugin so you can do refactoring
manually it's actually got a kind of
programmatic API in the back and you can
use it through Erlang and you can give
it these various patterns which I'll
show you a couple of in a minute but
that that allows us to have mutation
operators that aren't just tweaking the
abstract syntax tree they can often do
some really quite subtle things and I'll
show you some trivial examples but you
can do pretty much anything you want but
you can write an entire Erlang program
inside one of your operators to change
things that we really like so this is
this is the very simplest possible
operator that you can supply this just
changes a plus symbol to a minus symbol
which is not very thrilling but as I
said we've got three parts to this the
first part is the name which is actually
quite important because when you get
your mutant it's going to say at the top
what operated it apply so this one's
called plus to minus the second bit as I
say the first thing it does is tries to
identify the points at which you can be
used so this is a Wrangler pattern this
x @ + y at their their kind of meta
variable so it can just stand for
variables so so we're looking for
anything that matches this Wrangler pan
and then we've got a an exchange macro
at the bottom here there's just going to
swap the operator in between so that's
the simplest form of mutant is one that
looks for a pan and then it replaces it
with a new plan but you've got meta
variable so you can do some sort of
subtle shifting around of things should
you so desire but you can do slightly
more subtle things as well you can
actually have as I say an entire Erlang
program sitting in here so this this one
sort of swaps the order of cases in an
if statement and yeah half the time that
can pilot warns you about this but it's
still interesting to try it so this this
is going to match an if statement with a
load of guards and a load of body so you
saw one at is of meta variable three
apses of list of metod variables and I
can't remem what two outs is it
something special but um so having
matched an if statement with a list of
guards and the list of bodies we're then
going to pick a random number up to the
length and we're gonna pick another
random number and we can build a new
meta variable called new guards by
swapping some of the guards and a new
meta variable called new body by
swapping some of the bodies and we can
rebuild the new abstract syntax tree
by using our new meta variables and this
then gives us a mutated program but I
again I'd be interested in people's
feedback this is quite a succinct way of
expressing something that's actually
quite a complicated change to the
abstract syntax tree and I'm hoping you
should be able to write some interesting
very domain specific mutations based on
this and you can do vastly more
complicated this is where I guess hard
to fit on a slide but this one is
actually modifying the the time out on a
receive statement so having a receive
statement with a series of patterns and
a series of yards and then after a sump
is our reason after then comes through
with a list of patterns but you can then
modify all of those to be one hundredth
of the length and I say that may have
some really interesting impact if you've
got something that's very sort of
time-dependent but you're not waiting
long enough suddenly are you actually
testing that or you just assuming that
it never happens I don't know so so
we've had code coverage and that's
that's very goodness or positive we're
making sure you explore all of your code
and then so as I said mutation testing
is kind of the complement of that there
are however some limits to mutation
testing you do have to compile all of
the muse I mean there's kind of two
limitations and depending on who you
talk to different people object to
different ones of these but the upshot
is I if you've created a hundred mutant
so you can have to compile 100 meetings
and then again have to run your whole
test seven hundred millions now
personally I've never find the
compilation to be a problem I find if
I've got a five minute tests we didn't
have to run a hundred times as 500
minutes and that's quite annoying but
you can do them independently and you
can do in sort of a batch thing
conversely for some bizarre reason kvick
have a case study we're running the test
takes them a few milliseconds we're
compiling it takes ages i don't know if
it's got a pass transforms or something
bizarre in it so they all subjects to
have it to compile lots of mutants but
whichever way around you you should you
should bear in mind that although
mutation testing is a nice complement to
code coverage code coverage is very
cheap to implement it just sort of
inserts a few calfs messages you can you
can run some other and you can run your
test and it should have unless you're
doing the crazy high frequency trading
the guy I'm a net guy was doing the the
cost messages can have a trivial cost to
you so you should always always do that
if you're bothering to test your system
you should always measure the code
coverage mutation testing is more
something I see fitting in
once in a while when you think you
finish code covering okay now we can do
some mutation testing because it can be
quite expensive it can take quite a long
time to run however you can often use
the results of that to inform what you
were doing before as I say if you're
doing if you're running quick check to
do you generate test or some other test
generation framework it's nice to have a
sort of ballpark of how many tests is
enough I've found situations where I a
cubicle just run 500 tests cuz that's
that's a nice round number a 500 test
maybe he kills half the mutants 2,000
test kills ninety percent of mutants
3000 test skills ninety-eight percent of
amines and then the last two percent are
semantically equivalent okay well so I
don't have to do mutation testing again
to know that next time I do testing I
should probably do three thousand tests
I've now got a sort of ballpark figure
on how much random testing is enough and
as I say given the rest of the prowess
project is quite keen to give you lots
of random testing lots of automatic test
generation tools one really good outcome
from this section was to give you some
sort of ballpark for enough random
testing because otherwise yeah as I say
you can run quick check for three weeks
and you know if another day would have
done it so i think that's mutation
testing i want to measure model
inference partly because it's a little
academic fetish of mine but partly
because i don't think it gets enough
from press model inferences then a
completely different way of looking at
that so so code coverage are I
criticized it because you're very much
taking the code you've written and
exploring it and a mutation testing we
said well we're going to take the code
you've written we're going to change it
and think about the code you might have
written but we're still all fixated on
code at this point all of that is like
I've got some code I'm a change the code
what's going on what actually can be
quite valuable to stop and step back and
talk literally about what was I supposed
to be writing what was I supposed to be
achieving and so that's where model
inference can come in so model inference
started long long time ago with what's
called grammar inference you look at a
series of sentences in a language and
you try and build a representation for
the grammar of the language but then a
lot of people have taken to applying it
for the kind of black box learning of
systems if you have a series of traces
of a system things that can happen
assistant or indeed negative traces
things that can't happen things that
break the system you can look at those
traces in those various are runs to
build up an inferred state machine model
of the
bak bak system so you can kind of say
well I've always seen it do this and
after it does that it always seemed
today you can start to create this
decide the finite state machine or
extended finite state machine model of
what's going on the system however we
can actually apply that as a test
adequacy metric because if you take your
tests and you treat them as traces I
mean a test ultimately is I'm going to
do this expect ways I'm going to do this
expect we have something to do this
expect it's a sequence of things you
expect happen in your program if we take
your test set as a series of traces and
then we infer a state machine what we've
got essentially is a state machine
representation of what your tests think
they're testing we built up the machine
that the tests are exploring and then of
course you've got the question if I okay
we got this this model of our machine is
that what we thought we were testing or
not so if you got a quick check model
quick check models are actually
semantically extended finite state
machines so you can often compare the
two and it's quite easy to see that if
you wrote this this quick check state
machine that had five states and then
you infer a model from your tests we've
only got three states in it well why
haven't we explored the other bit and as
I say you can never explored all of the
code because this is what was written
and he kind of explored all the
variations on the code because you're
very thorough exploring the code but if
you just didn't bother to implement that
thing that you were supposed to
implement you forgot about that'll never
show up with a code based test adequacy
metric so a model based test out of
Christine actually can be quite valuable
as well so I kind of want to throw that
out there not least because Carol and I
are doing some research on it it's
always nice to have people interested so
with that in mind I've got a few
conclusions you really should be testing
but if you're testing you really should
be testing your tests and let's not
start testing the test adequacy test
because that could get very silly and
that's like I'm like next year a new
test adequacy adequacy but you know if
if hopefully most of you have been sold
on the idea that if you're writing a
line you should be testing it so I want
to sell you on the idea but if you're
writing tests you should be testing them
or evaluating them in some way which
doesn't seem unreasonable let us say as
long as you don't take the stack too far
and waste all your time discussing it
code coverage is cheap you know you can
practically for free do this as long as
you're not doing something that's hype
Hypertime intent
code coverage is is you really should
just always run it but if you're going
to do Co coverage please get it properly
because hopefully after seeing my
trivial demonstration if anyone goes hey
man I've got a hundred percent line
coverage you'll understand how little
that actually means you know I've
successfully run every line in my
program it's not a very strong claim so
Co coverage cheap but please do it
properly mutation testing though is it a
useful complement to that as I say can
be expensive and you will get a lot of
whinging when you kind of have to
compile it 500 times and execute it 500
times but is worth doing as a compliment
that again hopefully I've I've
introduces the idea that code coverage
may be cheap and it may be interesting
but you need some complimentary things
and finally model inference is very cool
I think you should at least have heard
the words model inference once a line
conference so that people have heard of
it and then Carol and I can sell it
later so prototypes I've called this
prototypes not implementations I'm very
first one to admit that I'm an academic
not a really talented airline programmer
so if you go on github you can get some
other which is an MC DC analysis tool
for Erlang you can get mewtwo which is a
mutation testing framework for Erlang
and then state charm on sourceforge is
kiril's thing that does model inference
if you want to have a play with that
we've got various Erlang things that
hopefully still worked at least at work
yeah there's there's various things for
getting traces of your airline programs
and pulling them in and trying to learn
models of them even does some some
automatic learning so if you give it a
module that's got standard OTP like Jen
server callbacks it'll start throwing
some values at those and then it'll earn
a bit of model and then based on that it
will start throwing some new values and
it will burn a bigger model based on
that model little throw some more values
and build a bigger model until it sort
of stabilizes so you can just give it a
module and tell it to go it's not
brilliant yet but we're working on new
versions so as they those are there by
all means use them by always don't
expect them to be industrial quality
because I'm not industrial quality but
they do exist and I'd be very keen to
hear if anyone has anything to say I've
been told I've got ten minutes but I'm
ready for questions so as long as the
question isn't please can I run my code
and tests in your tool right now live on
a projector in front a lot of people
because yeah
any questions do you first it it was
supposed supposed to have the same
interfaces cover although I got an email
from the OTP guy saying all you don't
implement this obscure function but
technically you start by smother
compiling instead of cover compiling
your file and then you can run some
tests so I've just run the same test
three times and then we analyze the file
and when we do that it spits out an HTML
file and yeah you're not allowed to ask
but it does a bit of JavaScript in it so
it essentially is colorized so in fact I
can show you the more compelling
demonstration if we if we can pile it
and don't bother to run any tests it's
all red if we then run one test so green
means you've explored everything that I
can meaningfully exploit you to sew a
pattern a function header that takes two
arguments that are both variables if
either never hit it or you've hit it
there there's no meaningful
decomposition because if you use the
wrong number of variables it's not that
function anymore it's the other version
of the function and if you if they're
both variables I can't meaningfully
decompose that so you notice he's got
not applicable in a lot of places I'll
tell you right now this is one weakness
of this is there this certain
combinations that are impossible but
it's really hot convince it's really
hard to prove that they're impossible to
that so sometimes it a lot e you can
probably never reach a hundred percent
smother coverage on most realistic
things so you have to kind of figure out
what the upper bound is but but for
other things if you've explored it it's
green if you've not exploded it's red
but if you've partially explored it it's
yellow and if you put your mouse over a
thing you get more details about the
thing but then of course you can go down
the tree so if you only care about this
bit we can put our mouse over this bit
and just get so again that's that's a
true or false you know we need condition
coverage because we've never tried it
with a equals 0 but there's no sub
components to that whereas if we mouse
over the thing at the top that's got sub
components we also see this kind of
maitre d I say this starts to get filled
in with not applicable if there's some
combinations that are impossible but
it's hard to think of an example does
that answer most of your question it has
a couple of other export functions that
will give you numbers I think you can
get get percentage or something yeah so
you can get a get percentage and you can
get the internal tree I mean so if you
want to use the the tree of coverage in
some meaningful way you can get this
this tree of what you haven't haven't
covered but and you do it by by module
name okay apparently don't get one
what's your name this is why I don't do
stuff life okay you used to do it by
module name I probably change something
but yeah you can get a few numbers out
of it you had a question okay good just
don't look at the code and you might
still have that yeah yeah if you had a
got partially ya know so so it doesn't
really do the proper path analysis
because again that's that's actually
quite hard I'm sure some of you elite
people would do it but I again working
computer science department of course
you get all the professor's tell you
technically it's undecidable in this
bizarre k which is isn't nonsense excuse
not to do it but it is a good enough
excuse for me to not try to do it yeah
yeah you could sort of insert some types
somewhere I haven't done it is this your
answer it would be interesting to do I
haven't done it it's moderately clever
within the scope of one thing so so I
think it's clever enough to know that if
you've got to this point it can't be
that one but it doesn't work sort of
beyond the scope of one one thing it's
dealing with but good question thank you
back
so it inserts cost messages into your
thing like then casts to a separate
process so one thing cubic found is they
they ran these really quick tests and
they would run the class really quickly
and then they'd have to wait five
minutes while the analysis thread caught
up so there's there's a separate server
process that receives the the cost
message and then does the analysis and
that that's a bit of code that really
really could be optimized and I'd really
love to do an elixir because it's really
horrible to do with with this abstract
syntax tree and try to work out which
bit evaluates to true and which doesnt
the value a true but I could quote or
unquote and be wonderful but but yeah so
the analysis process runs as a separate
process and you do have to kind of wait
for that to catch up but it shouldn't
slow down what you're doing is the
theory yeah so I mean we quick checkout
cubicle project partners so they've I
don't know if they've talked about a lot
they have what they call feature based
testing so the trivial version of free
to based testing is to generate a test
and then see what it covers generate a
new test and see what that covers dinner
any tests and to keep going until you
your new tests don't cover anything new
and we have extended it to use smother
instead so it will now generate a test
that covers a new leaf of the
possibilities do not test comes nearly
for the possibilities and again it's
still random testing so unfortunately
it's not intelligently using the
feedback it's just generating a new one
and seeing what it covers but but you
can use it that way and sort of generate
a test set that covers as much of the
mothertree is it can anyone else oh yeah
and it was not my goal but then the guy
from the OTP packaging team came up to
me and said we don't want to support
cover anymore can we use yours but could
you just add all these six features to
make it compatible and nice I want to do
that I pop I'm an academic I'm not paid
to do that but I but genuinely there's
that tension of course my project will
come to an end of three months and as
the previous speaker said um yes
somewhere I think there's a bug so it's
all on github so you're more than
welcome to look through the the bug
reports I think the guy filed a bug
report for there's a couple is something
to do with the way you stash your
results and then you create a new ones
but you can go back to the stash boy I
haven't implemented that it wouldn't be
that hard but I haven't implemented and
it's kind of those things like that
where where people do actually depend on
it and I haven't implemented it because
it wasn't obvious but anyone else go on
yeah but yeah
yeah but isn't that true as in what that
would show up here um so if if there
were no more decision points then
hypothetically might we'll see that
although i can't of assume you would see
the exception I if you were deliberately
making a test an exception and oh yeah
strictly speaking yes but I possibly
naively assumed that if you called the
thing then you would use the return
value for something that would include
some decisions and you would see that
they would be red it's possible that the
very last bit of your program calls a
function that crashes and supposed to do
two more sequential things but that
sounds like a Byzantine it's not an
invalid suggestion but it sounds like a
sort of Byzantine case that yeah I
didn't explicitly considered but I think
you'd be stretching it to say that
covers really useful because it will
find that it thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>