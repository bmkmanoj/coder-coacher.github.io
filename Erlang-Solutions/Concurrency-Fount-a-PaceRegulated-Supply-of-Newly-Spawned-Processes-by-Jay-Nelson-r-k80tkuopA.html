<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Concurrency Fount: a Pace-Regulated Supply of Newly Spawned Processes by Jay Nelson | Coder Coacher - Coaching Coders</title><meta content="Concurrency Fount: a Pace-Regulated Supply of Newly Spawned Processes by Jay Nelson - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Concurrency Fount: a Pace-Regulated Supply of Newly Spawned Processes by Jay Nelson</b></h2><h5 class="post__date">2016-09-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/r-k80tkuopA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi my name is jay nelson i have my code
up on dual mark on github and this is
part of the epoxy library which is
Erlang patterns of concurrency the
library has multiple other things in but
today we're going to talk about a new
feature which is concurrency file the
code is up on master now it's not tagged
yet for 1.1 but it will be tagged for
1.1 when it's fully ready so if you're
currently using epoxy and you just use
master as you're a package manager
include point you've already got it if
you don't know it so beware you should
be using tags so basically a concurrency
fount is like a water fountain of
processes it's a reservoir that contains
pre spawned processes and you talk to
the process reservoir as the API to get
workers for your application so when you
drain the reservoir its refilled through
a regulator and that's the back pressure
technique for controlling how much
concurrency you can actually consume
rather than spawning and just spawned
you since morning until you run out of
memory there's a control mechanism to
limit the amount of spawning and still
allow you to handle spikes so it's a
principled approach i use behaviors so
you can implement in the workers an
actual receive loop with babe and so
you're not just giving them functions
and letting them run willy-nilly so
here's a basic task i I have client
requests and I want to service them and
using OTP a smart erling programmer you
implement a gin server and you're done
then you start to get multiple requests
they stack up on the mailbox you can
still handle them fairly rapidly if the
number of requests starts to build up
and the time to give a response takes
longer than theirs latent introduced in
this process so being a smart OTP
programmer you use a no-reply in your
gen server you spawn a process as a new
pit let it crunch away on the the thing
that takes a long time you reply back no
reply to the request and then when this
guy is done he replies back to here and
so bypasses the gen server bottleneck
that works great until you get a large
spike of requests and then there's
10,000 messages in your queue and
latency comes back again you've got one
worker per request you got memory and
CPU pressure and if you don't cap the
number of processes you have
simultaneously you could end up
consuming all the memory or all the CPU
or crash the vm so I introduced
concurrency control which is another
pattern in the epoxy library it uses
ettes tables so that any process
anywhere can increment and decrement and
then you can have a cap that's
concurrently accessible if you were to
put the cap in a gen server you'd
serialize that you ended up with a
bottleneck in this gen server again so
epoxy tries to get around serialization
issues by adding concurrency via its
table metadata in that case when you
reach the limit you can either refuse to
run or pause but concurrency found is a
different approach rather than use a
fixed cap it's actually going to
dynamically adjust the availability of
concurrency based on how much you're
consuming and how much CPU and memory
I'm available not memory so much but CPU
so
to our problem where we get a big spike
and we've got our queue of messages
we've got bursty traffic and we're
spawning responses we're going to get
bursty CPU 100 processes get spawned
then nothing it's fun than a thousand
process again spawn and that churn when
you spawn that many processes under load
actually impacts everything on the
machine and in a typical system where
people are doing things like that there
were like sending a text message you
send it to the server the server has to
find a connection send it through that
socket wait for the socket and then
reply back that I went through you try
and eliminate that latency and let
another message go through you find out
that the background processes that
you're handing off to the spawns some of
them take a long time some of them take
less time than the spawn actually takes
if it's a very fast like increment the
net stable and continue on your code
ends up in a way that you you have these
quick little tasks and they would
benefit if you had pre-spawn processes
rather than doing a spawn on every work
task request so everybody that comes
from another language is oh I know about
this worker pools worker pulls a
solution for pre-spawn processes that do
the work for me and I'm now going to do
a flashback to erling factory San
Francisco 2015 Anthony Mullen air from
openx his topic was how to pick a pool
without drowning so he said let me see
there's a lot of pools out there so he
searched on github for process pulley
got 12 results resource pool 3 results
worker 18 results connection pool 19
results and then he knew about seven
other pools that didn't show up in his
search results and he tried to whittle
it down and in the end he had these as
its final list pool boy pooler gen
server pool diff discount cheap rock
and these because he was trying to limit
the list had problems that he wanted to
whittle it down because of the
documentation or stuff but we've used
worker pool that's common anaka is owned
by Erlang solutions now so I should put
a plug in for that but we're pools are a
dime a dozen there's a million of them
out there and most of them have
different issues that cause failure
problems and so that's what you have to
watch out for it works fine when you
plug it in and works fine until it
doesn't work and then generally a worker
pool is catastrophic and I'm going to
explain what why that is so let's say
you decide worker pulls to the rescue
and I'm just going to do a simple one I
don't want to use an existing library I
just have an application I'm going to
have a supervisor and it's going to have
20 workers so now I have a fixed number
of priests politan processes the
supervisor spawns among start up there
around forever if one of them goes down
the supervisor will restart it so I
don't have to really write any code to
manage a fixed number of workers well
okay so that was easy but now I have to
actually send a message to one of the
workers so how do I choose them I don't
know their names they're just kids on a
supervisor if I start querying this
supervisor that's a gin server and it's
going to be bottlenecked so I get smart
I name all of them and put you know work
of one worker to twerk three and then I
register the names and then I asked the
registry which ones are there until
there's there's none there and then I
pick one randomly or round well this
gets so I said load balance it's a
little more complicated but this is it
all right so it can't be that hard then
you find out well I've got more work
than the number of workers only have 20
workers and I've got a hundred tasks and
I message let's say I round robin I
choosing my load balancer I've got one
task running and then 19
tasks on the q the mailbox for each of
those workers one of the workers trips
falls down 19 tasks disappear there
anywhere else because I sent them there
and they just disappear so I get a
little more advanced I'm going to add a
task queue and I put my messages in the
task queue that workers pull from there
before they're done they check is there
anything there they pull that gets a
little more complicated now because my
task queue has to be bounded and if the
task queue goes down I'd lose a thousand
of them instead of just 19 you know but
you wave your hands until there's issues
and there's details and then finally
catastrophic failure so here's the
advantage is it a worker pool you get a
typically fixed amount of concurrency
they're supervised on replacement so
that's good means they're always right
it this is good and bad you can just
send a message to it or you could send a
function to it and it could execute that
function and so people tend to do that
not sure what I need two workers for but
i know i need the workers i'll just pass
a function to them i'll write a new
function when I need the best now you
don't know what their undies anymore but
it's convenient so it works out pretty
well but they end up now when you're
debugging you have no idea what any
workers doing because they all look the
same and but they're doing two different
things they're long-lived so there's no
latency when you start a task and
hopefully you picked a library rather
than rolled your own after you started
getting into the issues you say okay
fine let me take one of these since it's
a library it probably has an API that i
can replace and so if this library
doesn't work i'll switch to the other
one that's not quite true because each
one builds slightly differently and you
have to initialize it and so on so
they're not as replaceable as a seam and
you have to deal with this per worker
issue it's convenient in the beginning
if i have short tasks and long test and
I round-robin some workers will stay
busy for a long time
other workers might zoom through six
tasks from their mailbox quickly that
just balances out and disappears in the
noise and you don't notice it but if you
get too much too many tasks it builds up
on everybody and it builds up worse on
the long run task so you have to find a
way to avoid the long running task so
the plus is we've got predictable cpu
usage our workload spread plus or minus
we can use the mailbox it's convenient
but then we lose things if we did this
is a big negative once I decide I'm
round-robin or read them or whatever and
I give a task and it's busy and it's on
the mail queue and everybody else is
free I can't pull it back and give it to
somebody else it's committed and there's
no way to uncomment from the mailbox so
now you have to build a second mechanism
if you an uncommitted you know and then
it's getting really complicated the
issue we ran into which was totally
unexpected throwing random functions to
the workers those functions called
third-party libraries two different
third-party libraries were clever
because they knew what they were doing
and used the process dictionary never
use the process dictionary but I know
what I'm doing don't ever ever use it in
a library that somebody else is going to
use because you don't know what they're
going to combine it with two different
libraries collide on the same key crash
your worker and you'll never find that
it'll happen once every three months or
six months or whatever so now you have a
very advanced worker pool before you
free up a worker you erase the
dictionary you do a garbage collect to
hibernate you like now at this point
you've got more latency than if you had
spawned in the first place so what you
know what's the point we were trying to
save latency there this staleness can
cause fragmentation and GC if you have a
lot of short tasks that impacts things
of GCS running there's no back pressure
when you're stuffing things on a mailbox
nobody tells you hey there
ten thousand things here I mean there is
a built-in one but when you're farming
out to workers as fast as you can that
slowed down by the vm is is minor so
when it beds pools you end up with the
shared task cubes super advanced once
you have worker decay randomly decays
and replaces i implemented one of those
for Cassandra it's called elysium if you
look for that you the happy after life
after Cassandra dies it forced
replacements failure mode though the
supervisor launches replacement workers
turns out that sounds like a great idea
but when you throw a function to
something and it fails you don't retry
it the way you do other things so it's
pointless to restart it with supervisor
and restart it with say an int args so
you're just restarting an empty worker
if you get multiple failures the
supervisor gets overloaded then if they
get repeated the supervisor crashes then
the crash takes down your application
then it's game over and you're rebooting
and you know it's the mess it's really
bad and your log file is so full of all
these crash restarts that you can't find
the trigger so if you eliminate the
supervisor and that solves your
bottleneck problem but now you can't
replace the workers because they're gone
and nothing nothing replaces them so
you'd have to build a supervisor routes
to the side then so these these sort of
bottleneck issues are pervasive and
naive up implementations and the
supervisor really only function is to
limit the pool size load balancing is
hard and what you do is you assign tasks
only when the workers idle so now you
have managed who's idle who's busy and
then they have to pull from the queue
and it's just another mechanism it's
really complicated concurrency file
tries to remove the supervisors have a
clean pull from the reservoir and
independently refill the reservoir so
now there's no dependency between the
workers working the reservoir mechanism
and the refill of spawning
spawning so what I'm doing is pre
spawning sitting there idle waiting to
be pulled and then replacing them with
idle workers so there one use spawns so
it's it takes as much CPU and effort as
if you did a spawn every time but you've
you've amortized the latency of the
spawn to do it when you're not busy
hopefully and then if there's a spike
you know my worker pool might be this
big but a reservoir pool I can make it
extra big so that a spike can be 10x but
then it it replaces slowly so I can
recover recover over time and note here
I put multiple counts feeding one
application I can categorize my phones
so i might have a fount for quick tasks
and a fount for long tasks or felt for
database connections on on an end they
open a socket and wait with the socket
already oh so you can adapt your
categories of concurrency so I've one
reservoir per category you limit spikes
and you have single-use process as soon
as you get one you use it up you throw
it away it dies no garbage collection if
you're good at this and so it saves a
lot overall even though you're keeping
more spawn processes than you need so
there's a little bit of memory usage all
the time even when when you're idle but
you'll never go beyond your max memory
on pre spawns inside the reservoir the
idle unallocated processes are all
linked to the reservoir so if you kill
the reservoir they all go away so we use
direct linking for that but what you get
a kid from the reservoir it's unlinked
so now it's free it doesn't have a
supervisor it's not linked to anything
there's no bottleneck if you need it
linked you can link it yourself if you
need to monitor it you monitor it
yourself if you want to supervise it you
can stick it somewhere else and have it
be supervised that way so we don't need
a supervisor because it's it shouldn't
fail when it's idle
like it should in it and have a process
if you can't get that far you're still
debugging once you task it now it's not
connected to the reservoir and so it's
not going to cause issues for other
workers or the reservoir itself
allocation always returns you one pit or
a list of pits it unlink sit as I said
you can either get a pit and then do
things with it like monitor it and then
send it a message or you could just pass
a list of messages to the reservoir
let's say I I open a TCP socket for sse
and I get 100 SSE objects I collect no
puffer I just send a hundred messages to
the reservoir in 11 function call the
reservoir takes each of those messages
and just gives them to a hundred pits
unlinking them and then returns you the
list of a hundred pits so you task them
they've spun off start it and you've
gotten your pit list back then you can
decide whether you need to keep track of
them or throw that away if you want to
monitor it and link it then you get a
pit do what you want before you met
message it and if you're just like I
need 100 pits let me stick them in ants
and put them on a key you can do that to
you don't have to task them but they're
sitting it in a received loop on idle
when you consume them that's what
triggers replacement the reservoir is
done a slab allocator so there's a slab
let's say a hundred processes 20 20 deep
so then I can take a spike of 2000
without having asphalt spawn anything if
if you pull off two processes I don't
replace them but once a slab is gone
then the regulator gets a message to
replace the slab so the regulator is
working in chunks of efficient spawn
size and it actually creates one slab
every one one-hundredth of a second
right now that's the regulator mechanism
ultimately the regulator is going to be
a behavior so you can plug in your own
governor as to how quickly refill the
reservoir so that you would be able to
use information from the rest of the
application in the environment to
adaptively change the regulator to
control dynamically how quickly you get
new spawn new spawn de so here here's
sort of a message diagram mixed with an
architectural diagram when you use the
epoxy currency file you're actually
implementing an instance of a supervisor
and plugging it into your supervisor
hire work for each category that you
need processes that creates a rest for
one regulator and reservoir if the
regulator goes down the reservoirs
killed the new regulator and a new
reservoir occurred so the regulator is
there before the reservoir so that on a
net the reservoir fills up by using the
regular regulator so it actually fills
up slowly in the very beginning to so
you might have some startup
synchronization but it maintains a
consistent regulation all that all the
time that way and it as soon as I use up
a slab it tells a regulator the regular
supplies a whole slab so if my slabs are
a hundred wide then I have one
one-hundredth fewer messages from the
regulator than I do from client requests
I could stretch that out to 10,000 and
make it only five deep I could make it
three and ten thousand deep it it comes
into the regulator being one
one-hundredth of a second so by making
short slabs I can slow down you later by
making long slabs I can speed up the
regulator in terms of refill
uh basically that dictates the size of a
spike and the latency on refilling and
we're efficiently managing the slabs
because what you ask for peds if you ask
for a slab full of pills I'll grab a
full slab instead of the head of the
fountain so if you know you always need
ten processes at a time make your slab
with ten and a thousand deep and you
grab 10 10 10 10 it'll be really
efficient the spawning cost doesn't
impact client requests because the
regulator is running as a separate
jennifuh same then the client requests
are being handled by the reservoir so
we're now we're interleaving client
requests and spawns whereas before it
was in line and that was adding to the
latency and yeah I said we're messaging
full slabs so you could get one or more
processes as a list you can't ask one or
more with a list of messages built into
the API as a retry it could be based on
repeating you know five times and with a
slight delay you can add a timer or you
I don't think it's there now but it will
have a continuation function so you can
retry based on a dynamic back pressure
out algorithm so if your repeater timer
fails you get back an empty list and
then you as the application can decide
what to do when you can't get
concurrency and then you can get the
status of the fount and some timing you
can exit how quick on average how long
is it taking to spawn spawn slaps and so
on there's also the option to add a gen
event one genuine and register it with
one reservoir you could use the same gen
event for multiple reservoirs but each
reservoir has one reference to agenda
event and by default that reference
doesn't exist so it doesn't notify
anybody
but whenever a slab of pizzas added it
notifies the gen even whenever the
reservoir is empty and somebody says
give me a pit it notifies the genuine
whenever the regulator reference has
changed to a different regulate if you
dynamically try to swap the regulator
it'll notify the genuine if an unknown
message arrives in case i'm still
debugging i can get it through a genuine
and when you stop the reservoir it'll
send a genuine so now you can hook it up
to pub sub and see when important events
are happening and see if you know the
frequency of failure on the reservoir is
get too high maybe you should adjust the
parameters or something like that and
you could dynamically do that if you
swap the regulator then died dynamically
so here's an example that comes in the
library tried to think of the clearest
simplest example I could come up with
and I happen to be looking at somebody
else's hex dump code at the time Hector
tongue takes a block of data chops it
into fixed sized blocks prints it out as
hex and gives you the text version as
well so I thought why well why not take
the data give it to a loader process
which chops it to 32 bytes at a time and
each line goes to a different form
matter and that a collector receives all
those messages and prints out the heck
stone so now I can do a parallel heck
stone I'm using some workers for this
entire graph of processes
this is sixty percent of the actual code
in the example so it's mostly here I
just want to talk highlight the
important things so when you create a
fount it's a behavior so you have to do
a behavior within an it start pit and
send message because when you initialize
it I might have some internal state that
i use to seed the pits as I'm spawning
them whenever the reservoir regulator
wants to create a new pit and it does
you know in a tight loop a hundred pits
pond it calls start pit on my behavior
in there I can do whatever I want and
hand back a pit but concurrency fouts
library itself has a spawn worker which
does the right thing if you just have a
module it spawns it and then returns the
pit and links it the key is it has to be
linked to the reservoir so if you write
your own you have to link it to the
reservoir and I just supplied my
behavior module and I have a function
called for matter which is going to
receive the data if I do a send message
I can use the built-in concurrency found
send message it sends a message but it
returns the worker not the message if
you just use send message you won't get
the right report format data is tasking
a pit with a message so if i get block
of data i have a fount i have my block
of data and i have a collector i issue
the message load data with hex collector
is the pit that's going to be collected
the results and I task the pit so it's
going to send the message to a pit
unlink it and that send me back the pit
so now it's got the chunk of data and
it's already started chopping it as I'm
getting the reply back of which pit
in my receive lip I've created a seed
loop called report data and that
receives that hex dump lines at the end
after I've collected everything it
prints out my nice table so inside this
function report data I'm going to
receive and collect line by line and I
mean I'm going to receive a whole batch
of lines that is the answer to the X and
then I'm going to format neatly and
print the table so so I've got two
functions here format data report data
in my work or receive loop a single
message arrives after unlinking from the
fount it doesn't matter whether in my
original diagram I had multiple I have a
loader I have a formatter those are both
going to be workers but using different
clauses of the received loop so that I
can have one received loop one
implementation one behavior and I could
see how they coordinate by looking at
one receive loop so and when we started
here we called the function for matter
with no state because I didn't state so
this is the fan one loader and format
one collector each of them will respond
to only one type of message either of a
loader message or a format so if we get
the first stage data loader message
remember it was load the block of data
at the caller that wants to get the
results if the data is binary and the
color is a pit then I split the lutton's
I get that many workers as a variable
and then I called curtain signal and get
the pits for that many plus one because
I've got the collector and all the
workers
they all look the same but that you're
going to use different parts of the
receive loop then I send a message to
the collector you need to collect 10
results up here I asked for 11 workers
but I told the collector when you're
collecting you'll know you're done when
you get 10 up this is the formatter when
a worker gets a message format at this
position this was the address here's the
line of data and that's the collector
that you're going to communicate to and
all I do is format the address format
the line as hex send a collect message
to the collector and I give it my pit so
that when I print out the table you'll
get a hex dump and you can see that
there's different pit numbers for each
of the work just as an example so that
you know that each line actually was
touched but a different process it's a
good example that you can work through
and then understand how to use yourself
if you're made a different style graph
foursome and then the collector just
collects them I used in a raid I used
really tricky Erlang I've got 10 workers
i allocate a new array with 10 slots in
it and as I get message back I insert it
into the array it's already sorted
pigeonhole sort you have to sort it
you're using concurrency you can't
assume anything comes in order if you do
assume it comes in order you're shoving
it through Jen server and serializing it
you just defeated all the concurrency
and so then you get a result it looks
like this and here's the memory
locations the hex for each line the
old-fashioned unix style and here's
separate pin numbers for each one that
handled and you see this only had to
handle 11 character
so we probably came back first right so
in conclusion the worker pools are
really great as a generic reuse
processes but the overload leads to loss
tasks complicated queuing there's a
hierarchy and bottlenecks with
supervisors and lead to catastrophic
failure and catastrophic it's a mess in
a production system there is really no
back pressure or it's not intrinsic and
it's hard to add it to a pool you have
to put it in the queue or something and
then some protocol for how you back off
and it gets messy with the concurrency
found rather than adding control
mechanisms to communicate back pressure
and things like that the reservoir
itself is the limiter once you drain the
pool you're done you can't swim anymore
and there's no need to check account on
tour have a supervisor do something it's
the physical presence of processes in
the reservoir is a sufficient data
structure this is a key feature of
advanced or beyond advanced master Jedi
level Erlang is partitioned your
architecture using processes and let the
process itself be a token that
represents a computable state if the
process is there then that element of
the computation is there if it's not
there it's not part of the computation
in your application you can freely
combine concurrency found multiple
concurrency founds you could pick kids I
could have made a foul for the formatter
and a fount for the loader and found for
the collector and then pull them
independently when you're doing a graph
it's kind of efficient to just get as
many pins as you need and then what why
are them all up maybe by sending a
message from one to the other and that
message channel is stored internal to
the process and now the graph is
represented by the press
of graph nodes and the edges are inside
the graph nodes inside the process and
the process architecture is a data
structure that's Jedi level architecture
and early simplicity in the mechanism is
what really removes the bottlenecks and
the hierarchy but the key to making it
work it's the pace regulation that back
pressure is fundamentally built in it's
not something strap-on after the fact
and it doesn't add overhead it's
fundamental to the mechanism and the way
it works any questions so so the
question is can you have a remote worker
pool so so the first question is can I
have remote kids inside a local worker
pool right I could do it on a slap basis
if i replace start pit with something
that round robins two nodes every slap
could be on a different note so then I
can use that as a load balancing
mechanism and I didn't add anything
that's Jedi level right I have a
reservoir my reservoir it's got
saltwater your reservoir has just got
fresh water and but you could have
remote founts just like you can have a
remote gin server it's an OTP gin server
or Jennifer M so it's no different you
would want the spawns to somehow be
efficient and not have your messaging
copying lots of data across the network
transparently so that you know if you
write your code bad it might incur a lot
of message copy but there's nothing to
prevent that architecture style
let's say
good
right so the so the question is dealing
with a database trying to get efficient
pipelining or other mechanism of
interacting with the database nothing
about concurrency file is intended to
constrain your architecture that's the
first thing second thing is remember use
processes to partition your architecture
so if you want five in-flight queries to
the database you could pipeline five but
now they're sort of in the same pipe
because we have this pre spawned in it I
could make five connections to the
database early when there's no request
and so the process has already Idol with
a socket connection to the database it's
just waiting for a query as soon as you
send out a message it forwards it to the
database and then you handle the
concurrency in the processes you would
need to manage the number of processes
in your reservoir carefully in that case
and you still have the issue of long
queries that maybe do joins and other
queries that don't so just like in a
worker pool where you have short workers
medium workers and heavy workers you
might have three reservoirs and choose
that but it comes down to partition
using reservoirs then partition using
kids then partition using graphs dish
and using sockets you take your
architecture and partition it down in an
efficient data organization and then
you'll get a solution that works and you
could trace it through the system
because the pits can reflect to Jenn
events and you can just be observing the
flow as it happened
yeah if you want to so the question is
if when you look balancing to database
whether you pay attention to the sockets
and how they're distributed or whether
you pay attention to the pits I say look
at it alyssia mon Dieu amar github it
round robins to the Cassandra it skips
nodes that don't respond and it has
built in stochastic decay so every five
minutes or so the sockets reconnect and
then they reconnect somewhere else and
it just it just routes itself around
problem nodes and it load balances
through stochastic dynamic traffic and
that's now I don't implement any logic I
just say use this one for a while and
then quit and use a different one for a
while and everything else works itself
out we we turned it on ran it for a
weekend and said it stayed up all
weekend and DevOps said you know you
only had three out of five nodes right
the other two were down all weekend oh
my gosh wait a minute we didn't know
that's great I had no idea that the
nodes were down well in in in the so the
question is is there an API or a way to
interface with other libraries or some
you implement the behavior to start a
pit and in it the pit with a state so if
there's an API that requires a token or
some put that in the init state every
time you spawn a pit you just get a copy
of it so on spawn I'm actually messaging
this is another Jedi or laying trick
that you're probably using you don't
know you use how to send a message
without sending spawn a process that's
using a variable from your process it
has to get copied into the new process
space and now you just effectively did a
message without sending a message it's
actually more efficient because it's
copying the process memory at spawn time
so in the start pit behavior
implementation you would then initialize
the API and even if it takes 30 seconds
you make a deep reservoir you do the
initialization it takes all that time
and now when you grab a pit that's ready
to go and there's no latency there is a
bunch of upfront costs and it made it
cost memory and other things and you
have to balance off you are nothing to
magic you're just trading latency for
memory latency for pre CPU so you're
balancing out when the spike causes the
storm of CPU and memory is it but yep
yep but what I don't want is so the
question is why do we drop workers after
they pulled from the reservoir and used
once why do we do one shot usage Oh Oh
get it so so the question is I
understand why you let them go because
you don't want the serial bottleneck of
putting them back can i reuse them I
wouldn't put them back into the
reservoir I would just keep them like as
soon as you get a pig from the reservoir
it's yours you open it it's the
application the application could
register it a nets and keep it around
for five minutes or whatever it's
slightly poxy there's a concurrency cash
you could stick it in the cache and have
it expire after five minutes or
something but it's it's yours to manage
the red support only wants to pace how
quickly you spawn and pre amortize the
cost of spawning that's really all that
it's doing it's not managing processes
or letting reviews processes it's just
pacing and pre amortized the latency
cost so you would build a second
mechanism and a different technique for
holding long-term pit
Yeah right so the question is using this
technique of connecting to a database
and having a fresh socket every time you
spawn a fresh pit in this case the pit
is a a constraint on how much
concurrency and sockets you're placing
on the database the socket is really the
resource that's controlled it's better
to take the socket put that into Ed's
and when you in when you spawn pit grab
the socket for Ed's and don't let one
process on the socket you could pass the
socket around it's only if you're using
active sockets that you need to be the
owner of the socket if Jedi trick number
four right um if you have a socket that
somebody is open to you anybody can
write to it so you can have multiple
concurrent processes sending data it'll
be interleaved so you got to be careful
about that but when i'm doing the socket
and i know i'm done i can give it to
another pin and they can use it again so
when you have lots of data to
communicate to a database move the
socket to the data don't move the data
to the socket Jedi trick number five I
think there's a lightning talk in there
somewhere thank you okay and
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>