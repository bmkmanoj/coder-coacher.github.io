<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Erlang in High Energy Physics Research - at Fermilab Laboratory. | Coder Coacher - Coaching Coders</title><meta content="Erlang in High Energy Physics Research - at Fermilab Laboratory. - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Erlang in High Energy Physics Research - at Fermilab Laboratory.</b></h2><h5 class="post__date">2015-12-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/J7PDat-KlmY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">alright well hi I'm Dennis Nicholas from
Fermilab and so we're going to tell you
a little bit about early and usage at
Fermilab here as modern said and so what
I'm going to do I'm going to explain a
little bit about Fermilab science
mission and then kind of give you an
idea of how our erlang efforts fit into
that science mission and then rich is
going to go into more detail about one
of our Erlang applications that you know
we're relying on here for running our
particle accelerators so we're here at
Fermilab and as lawton mentioned
permeable the premier high energy
physics laboratory in the United States
or a campus of about 11 square miles and
completely devoted to basic science
researcher and in this RL aerial
photograph you can see this ring in the
foreground is one of our accelerator
rings called the main injector um this
is our high-rise headquarters building
here called wilson hall and ritual
mentioned that also briefly towards the
end of the top care so we're a
high-energy physics laboratory just what
exactly does high energy physics mean so
high energy physics to us means we
accelerate protons or other particles to
very high energy to nearly the speed of
light we crash the protons into other
things and look at the result sometimes
we crash the protons into other part
other antimatter protons or other
particles are other times into a fixed
target this generates daughter particles
that are really what we're interested in
studying and so through this methodology
we study the extremely small we split
the atoms of parts but the protons apart
into their constituent quarks and look
at the very fundamental particles that
are making up matter this also lets us
study thing information about the
extremely large on a cosmological scale
you know understanding how particles
work we're also recreating conditions
near the beginning of the universe and
trying to study the fundamental nature
of the universe and cosmology also here
okay so part of the particles works have
experiments running on our one of the
particle types of neutrinos so we
generate neutrinos with our particle
beams here we send the send the neutrino
so we make the particles here we send
the neutrinos through the earth to our
detectors up in northern Minnesota this
is a diagram of one of our Nova
detectors there this is about the size
of a barn just to give you some some
idea and we also have other on-site
detectors for neutrinos we have mini
boon and micro boon which are you very
much scale down from what the Nova
experiment has their neutrinos won the
Nobel Prize in Physics this year
different neutrino experiments not ours
but so it's a very exciting field to be
studying we're also studying the type
are called the muon new ones are sort of
like electrons alleyne here we've got a
two major muon experiments coming online
in the next few years the g minus 2
experiment involves moving a giant
magnet from Brookhaven National
Laboratory to hear a couple of years ago
and we're working on getting that
installed and getting a beam into that
our other one experiment is called Mew
te for view onto electron conversion
there's a little schematic of that
detector on the lower right there we
also do a lot of fundamental research
and accelerator technology were a key
research institute for doing
superconducting RF cavity investigations
and that's kind of what's shown on the
left here we also have a facility called
fast and they're building a new test
ring called iota to test different
fundamental aspects of accelerating
technology Fermilab's member of several
large international and local us
collaborations were a key player in the
LHC and the CMS detector at the LHC at
CERN we're also look at within the
United States we're supplying some of
the cryo modules for the lco s2 detector
at Stanford
slack laboratory out at Stanford also
besides us collaborating externally
there are hundreds of international
collaborators that come here to join in
Fermilab's efforts at science we do do a
couple of non accelerator based
experiments we recently built the
world's largest digital camera the dark
energy camera which is mounted on a
telescope in Chile we're a key player in
the corruption and data analysis for the
Sloan Digital Sky Survey locally here we
have a smaller experiment called the
kilometer which is really similar as an
interesting results a couple weeks ago
but you know we do have some non
accelerator experiments but most of our
focus is on accelerator based science
and here's a diagram of our accelerator
complex so we have a sequence of
accelerators which from push the beam to
proton beams to higher and higher
energies and then release them to the
experiment so we start off down here at
our ion source we literally start with a
bottle of hydrogen and it goes through
our linear accelerator the Lin Act which
goes it up to a little bit higher energy
at 15 Hertz rate then that is injected
into our booster accelerator another
intermediate accelerator ring out of the
booster we go into the recycler ring
their sector ring is really just a
accumulator for for the protons until we
get a lot of them accumulated and inject
them all at once into the main injector
the main injector boosts again up to a
higher energy you can approaching the
speed of light and out of the main
injector we can send particles to our
various experiments to the muon
experiments to some of the neutrino
lines to other fixed target experiments
we can also bypass the main injector
entirely and go directly to some of
other like the local mini-moon micro
boon experiments that I mentioned
earlier it there okay so that's Fermilab
as a whole and so how do rich and I and
our efforts in erlanger fit into all
this we're in the accelerator controls
Department and so what what we
you in order to monitor and control the
distant complex of machines we have put
literally hundreds of thousands of
channels of data collection all around
you know the accelerator rings at
various points on the accelerators and
the detector and the lines transfer
lines going in between those to monitor
all types of things from the basic
things like temperatures to beam
positions two voltages of various power
supplies and magnets so we're in charge
of the control system to monitor all
that and as part of the control so the
control system takes all this data
streaming from the instruments and
sensors on the on the accelerators and
we disseminate that and make it
available to our main control room to
various displays and graphs that
somebody might eat have on their desktop
watching the beam or two longer term
archival storage so in order to help
understand what we're talking about here
go into a little more detail about our
accelerated control system and so we've
got several buzzwords or bits of jargon
that will inevitably slip into here and
so just want to make you aware of them
as we go along with this the top thrust
today here so act net is the name of our
network transfer transport protocol and
you know will be commonly refer to the
entire control system is that act net
just as a shorthand an FTD a frequency
time descriptor when I say that really
think of a sample period for whatever
data we're trying to read out it's a
little more complicated than that but
really it means the sample period one of
the key features of our control system
is it's got a centralized database with
holds all the information about every
device in the control system you know
where it's at in the network how much
data is available or scale it our data
can be returned to either raw or scaled
into human engineering units we support
both scalars simple devices and array or
waveform devices our front-end
which are the parts of the control
system tasked with getting the data
directly from the sensors and get making
it available don't usually have any
database connectivity and so they return
just raw data and where if we rely on
other services to do the scaling so a
typical device reading request from a
client will specify the device name the
frequency time descriptor or sample
period t bite length and byte offset how
to get into the data so our control
system it's a typical three-tier system
that it's pretty common throughout the
various labs and labs and similar
experiments on the very top level we
have different clients and clients can
be anything from a our full application
console to a simple single application
or a web-based application some
archiving of data or you know other
clients graphical user interfaces that
the user might be watching at the lower
tier we have the front ends the front
end computers are based on different
various architectures but really they're
main job is to get the job the data from
the sensors and make it available to the
control system the sensors we have
hundreds of different types of sensors
um vmebus based sensors of PLC's kind of
counter sensors in our architecture
other more direct connection serial
connected devices network connected
devices you know wide variety so we have
a variety of front-end architectures and
implementations to get doesn't into the
control system in the middle is where we
make the translation from what the
control front ends acquire to the user
interface at the end so we have our
database as the main service we have
other surfaces such as a tomcat server
and alarm report doesn't dissemination
and then what we're going to be talking
about in more detail today thats
implemenators Erlang is the data pole
manager the data pool manager translates
all the requests for all the devices
that the clients are making into
messages to go to Z various front ends
and connect
d proper front end to any request that
it has going on here so then how erlang
fit into all this we've implemented
Erlang at a couple of different levels
of our control system some of our front
end computers use a earling based
framework that handles all of the lists
and message passing and communication
with the control system as a whole and
let's just insert various drivers to
communicate with the proper sensor or
instrument that we have connected that
from him we have some smaller clients
GUI based ones in Erlang but just you
know automated processes like a tuning
algorithm or something like that that
might be implemented in Erlang and then
in the middle we have our data pool
manager again the data pool manager
connects the client requests up with the
proper front end and then rich is going
to tell you more about our data pool
manager here thanks Dennis so I'd like
to first just talk about the anatomy of
the data pool manager and then we'll go
into a little more detail on some parts
of its and then talk about some issues
that we've had and what we've done to to
get around them so in a data pool we
have another three layers system here we
have an interface that talks to the
clients and keeps track of their
requests and is responsible or shipping
the replies back to it we have a job
control layer that does the scaling and
everything and then we have a layer that
talks to the front ends so in this case
we have a client that's talking to the
act net service task and that Agnes
service task will ask the supervisor to
create a a client process that is now
tied to that request and it will only
talk to that client and at this point
the external clients and this client
process will do some handshaking back
and forth building up the request trying
to discuss which devices that's
interested in what FTD is the sample on
all that kind of stuff gets built up at
this point this whole list and then once
all the information is put together the
external client will send a start list
command
tells a TPM that it's ready to start
receiving data so the internal client
process now creates a job and this job
process is goes to the list and does a
database lookup for all the devices so
it can find out which nodes they reside
on whether the request is valid for the
size of the data that these devices
represents it will pull scaling
information there's a bunch of
information that it pulls from the
database and then it also tapped into
our timing service because one of the
things the vpm wants to do is check the
timeliness of the replies from the front
end to make sure that all our front ends
are returning data in a in the expected
time frame we don't want stale data we
don't want missing data so it the DPM
will register with the timing service
based on the FTPS and the request so
that it can kind of track and make sure
everything's running smoothly once all
that information has been gathered then
it starts talking to a front end
registry which is another supervisor or
line supervisor process where each of
the processes that are spawned off are
infusing a simple one for one behavior
the key for these for these child
processes would be the FTD and the Act
net node name so at this point when
these requests go down the front end
registry can determine whether it's
already spawned off a process that's
talking to that front end with that FTD
or whether it has to spawn a new one to
handle the request so so if it's not
there it gets spawned and if it's
already there then the request is just
handed down to it so it can just add it
to its already active request so in this
case right now we spawn off three as an
example and the three front end worker
processes now are talking two front ends
and the front ends on the data
collection event will start returning
data and starts streaming data back and
so that data gets fed back to the job
task the job task as the data comes
through we'll use the scaling of
equations that it's pulled from
database and then those those scaled
replies are sent to the client process
and then they're fed back to the client
that's waiting for it so we have this
flow of data coming up from the front
ends getting scaled and then sent off to
the correct client so one of the big
responsibilities with the data pool why
it makes it a data pool is it's merging
the request to not only keep the load
down I need front end so that then at
handling thousands of requests for the
same devices but maybe just one request
and that we can disseminate the replies
to everybody that's that's the main
reason why we have the data pool manager
but it also allows clients to get a
consistent reply because when in the
past we've had some data pool managers
that were just on a single note and only
handled a few clients at a time and what
would happen is the front ends would be
returning data that wasn't exactly the
same tell these clients because they all
ended up being on different replies and
they were all getting sampled at
slightly different times and by heaven a
DPM that's more aggressively merging
these requests everyone's getting a
consistent value in a consistent
timestamp so right now when a request
comes down it gets sent down to the the
front-end worker and a friend of worker
checks to see whether that client the
device that clients asking for is
already in its request list to the front
end if it's not then it's it simply
expands its request and then resubmit it
to the front end so now it's just been
added to the set of replies but if it's
not if it's already sampling for that
device on their front end it has to
merge the request we remember the jobs
process ID so we know who to send some
of the data back for and if the clients
requests if their lengths and offsets
for the data are fit exactly within the
current request then we're all done
because the data is already being
collected for it and all we have to do
is pick out the subset and send it back
but if the length and offset overlaps or
is in a different
it doesn't quite fit then the front end
worker will expand the request to
accompany all the clients it'll littell
so like if one client were to ask on an
array device and just want the first
element and then another client comes in
and once the fourth element will expand
the length and offset to include all
four elements so that there's just one
reading down to the driver it just has
to return more data in addition to this
there's other constraints that we have
to be aware of since act net is a UDP
based protocol we have maximum packet
size issues and so as we start expanding
these requests and replies to go to the
front end sometimes if we're going to
exceed the length then the front and
worker will actually break off part of
the requests and actually manage to
requests or three requests or however
many takes to handle all the devices but
it does have to to monitor how big the
reply the expected replies are going to
be and what the request is going to be
and then we also have the concept of
fixed lists which aren't really used
much right now but the client has a an
opportunity to say that his data cannot
be interrupted or restarted and so what
we do for that case is we won't merge
requests that have a fixed list unless
the lengthen Asif fits in the current
list we just don't want to interrupt
that stream of data so this was a pretty
complicated module when we were writing
it a lot of nested lists a lot of you
know making sure would tasks die that we
clean up things properly and and all
that stuff and so we really used even it
a lot during this development which was
worked out fantastic we really felt
confident when we were ready to start
using this and in fact it was turned out
to be a very reliable module where we've
never had any issues once the system was
in a running state a unit caught and
kept us from introducing bugs in the old
system the one thing we found recently
was that we weren't testing max request
sizes just the maximum reply sizes and
we actually had a case where someone
exceeded the request sighs and so that
was easy enough
fix but otherwise it's been unit was
very very useful to us when we scale
data that the front ends typically like
Dennis said they don't get they don't
have access to the database so they
typically return everything raw and so
we have scaling transforms that we use
now we have two kinds of transference
primaries and secondary common
transforms the primary transforms are
mainly used to convert raw binary data
into a floating point value and the
common ones are able to do some further
scaling out the primary transforms you
know you might think 43 sounds like a
lot of them but a lot of them are
converting to bite integers or 4 byte
integers to a float you have to worry
about Indian asst you have to worry
about if it's signed or unsigned so that
handles a lot of cases and then a lot of
transforms were developed early on where
not only did they convert those integers
to a float but they scaled it to come
and power supply values or whatever was
available in the early control system
like plus or minus 10 volt supply so
some primary transforms get you right to
a voltage or current just with the one
transform but a lot of devices needed
more than that so then we came up with
common transforms which will take that
primary transfer man then further refine
us so that maybe we're converting a
current into a magnet field intensity or
something like that kind of transformers
can have up to six coefficients and some
of the comet transforms are pretty ugly
expressions and it's amazing that some
hardware needs something that that
complicated we use a in DPM we use some
lambdas enclosures to do this here's an
example of one common transform claws
out of the 44 clauses that it would take
to do this like in this case transform
one is just a simple linear transform so
it only takes two coefficients and so we
this returns a function that will scale
afloat by just multiplying by slope and
the y-intercepts just a simple linear
one of course some of these other values
further and down when you get to
transform 44 it's a really ugly
expression we don't have to you know in
this case here we have obviously don't
care about the last four coefficients so
this returns a closure that are a lambda
function that we can just add apply a
floating point value and we get our
scale theta now when we look at the
primary transform generator like in this
case transform number fourteen it
expects a big-endian signed for by
integer and converts it just to a
floating point value with no scaling and
so what we do in our primary transform
this one clause out of 43 clauses so
takes the transform number to match on
and a common transform and what we do is
we take as an argument the the binary
that's coming from the front end and
then we convert it to a float and then
apply the transform so what happens is
this DPM after reading a database we end
up with a function that we can take that
raw binary and we can get the final
scale transform in just a single
function without doing lookup swallow
data's coming through we just feed the
incoming data right into the right into
the transform and we're ready to send
things out it gives a nice flow of
through the system of data flowing from
the front end to the front and workers
through these scaling routines in and
out to the clients and you know kind of
one of the things we can do is is and
some of the more complicated expressions
but like taking this simple expression
we could do pattern matching to provide
more optimal solutions like for instance
if the offset in this linear expression
was 0 we could put a 0 here and just not
even included in the calculations
because these constants don't change now
that doesn't make much of a difference
for the simple scale like this but some
of the more complicated ones you can
simplify the expression considerably if
if some of these coefficients were ones
or zeros or something like that when
someone
updates the our database table and
modifies one of our devices the clients
want to know about it sometimes and we
have a service called DB news which is a
multi casting service where database
changes are announced and anybody who's
registered for it will receive it so DPM
listens to this announcement and we're
actually interested in two possible
changes one of the rare changes is when
someone locates a device onto another
front end like maybe a front end only
had a few devices on it because some
things were retired and so rather than
have two systems we can have one and we
move the devices over so it does happen
not too often but DPM miners monitors it
and if it sees that the devices move
it'll close out it'll tell that front
end work or task to terminate and then
it will spawn off a new one from the
registry and then it will move the
requests over to that front end a more
common occurrence is that the scaling
gets adjusted for a device maybe you
know the person who originally entered
the device didn't quite get it right or
maybe something's changed or hardware
has been updated and they have to change
the scaling so here we monitor it and if
if we see that a device that we're
reading has changed then we go to the
database and get the new transform and
just move it into place so that as the
data streaming through it'll get it'll
get scaled and what's nice about this is
we don't have to re request the data
from the front end it just kind of the
the data stream will suddenly start
changing to the new scaling scaling
factors we do actually insert a message
into the data stream that says that the
vice information has changed and it will
have some information about which device
got changed so clients if they want to
they can be made aware that something
has changed but you know a lot of status
displays that are just doing updates
although we'll ignore that that update
message and just your the data will just
go to the new scaling factor our
that protocol in the last 10 years we've
added the ability to multicast requests
and receive replies so it's a little bit
further than what just simple multicast
datagrams do and so we're able to
multicast requests to multiple targets
and either receive all the replies or in
this case what DPM does is he sets it up
that only the first reply is received
and the other ones are ignored and so we
can use this for service discovery what
we try to do is in the past we've had
the beefier machines would be our
service machines and all the clients
would know which machine had all the
surfaces but now desktop machines are
just as powerful as some of those server
machines and so now we're able to
distribute the load and move and move
things around and we want to be able to
add and subtract sup so now we've added
service discovery so that a client sends
out the service message and the first
front end to reply are the first DPM the
reply will be the one that the client
starts talking to and so this allows us
to bring DPMS down for software updates
or if i'ma she needs to go down for
servicing it'll allow clients to adjust
so when a DPM does get taken down the
first thing it does with before totally
exits the system is it notifies the
client that it's no longer could be used
as vpm the clients that receive that
special air status now can perform
another service discovery and find the
next DPM that's available we also use
the service discovery and do a cheap
form of load balancing where when the
DPM receives the services of discovery
requests it will delay sending its reply
based on its system load so what we do
is we scale the delay between zero and a
hundred milliseconds based on the load
average and we clip it to 100
milliseconds so a really loaded machine
will still try to send it off around 100
milliseconds later the it allows list
loaded deep into reply quicker and it
works pretty pretty good works well
enough that we don't have to go anything
more complicated than that
in fact we have a quick quick little
demo here right here this is a parameter
page where people can just type in
devices while they're waiting for a more
specific application to be written for
their for their application they can
type an active device name here and you
can see that it's updating data and we
can see up here that we're running the
new ppm on CL x 22 is the one that this
particular client picked out so if i log
into CS point CA sale x 22 and restart
the DPM we will see up here that that
this parameter page will switch over to
another one and you'll see that the data
will continue without anybody really
noticing so if i stop it starts it up
you notice here are already switched
over to clicks 20 and the data is
continuing going so it works really well
allows us to take things down and
applications adjust you know quickly and
it does very little delay or downtime we
right now we're using a custom reporting
system that we wrote we started with
Erlang about four years ago in a couple
projects and one of the early things we
did was write a reporting system before
we really knew what was available I
don't know the Erlang community when we
report an event or an alarm there's a
unique key to identify it so that we can
merge multiple arms together or multiple
events together like in this example
here we have a reader finite state
machine update which with a PID so it
identifies which job detected that the
Fram was kind of slow there's also a
formatted message that accompanies the
key so that you can get a little bit
more specific the set function will set
that something's in an alarm and clear
will kind of say that the alarm has gone
away and then we have a report that acts
as an event it's like a set clear really
quickly and so then at a configurable
interval we will send a report via email
to whoever's in charge of DPMS
and once the report is set we kind of
clear out all the the cleared alarms but
any alarm that's still in a alarm state
the set will continue in the next report
but we are looking into other third
party or community driven loggers and
seeing if we can you know use one of
those instead the DPM uses a sink
library to monitor I mentioned this
before that we we like to monitor the
timing coming from the front end we have
but different state machines that we use
based on which kind of event type they
have to try to do this it's the same
sync libraries in our line front-end
framework and and if a front-end as the
previous slide showed of a front-end
isn't responding timely enough report a
sense we are aware that there's problems
when we started this we were debating
whether it should just be a standalone
thing or should we try to make it into
an OTP application we figured we we
should well we ended up actions as an
application instead of a nice trip just
because we had some infrastructure in
place and we haven't used the script yet
but it turned out that it worked out
well that we want the application route
because our control system up in
Minnesota that Dennis mentioned earlier
it needs a stripped-down act net control
system and so we didn't want to give him
the entire infrastructure and but they
did need a DPM they didn't need Erling
friends they did need some things like
this and by making DPM and application
Dennis was able to just add it as a
dependency to his front end and now it's
bundled together in one virtual machine
up there so it worked out well that we
chose this path instead of the other the
other direction some of the issues
bought some of the more interesting
issues things were working great and
testing and as we initially start a
commissioning everything looked pretty
well but then we had a client that
started asking for a lot of data a huge
amount of data at a really fast rate and
all of a sudden we became CPU bound and
we start eating tons of men
and it turned out that our routines that
were building up the outgoing packet for
act nets were using i/o lists which were
great up to a certain size the client in
particular was asking for 32 pieces of
data with 8,000 data points which ends
up being a linked list of 8,000 data
points and x 32 x 10 hertz so it was a
lot of data and as you saw in the
architecture that there's some message
passing going on so those data data
structures were getting copied from
message queue to message queue and so
ended up being a lot of garbage
collection so after a lot of you know
now let's just trying to figure out what
was going on we ended up realizing that
our mistake was was going into was
making these huge list like this so
instead of making I olas where you know
we use a list comprehension to iterate
through and convert all those floats we
ended up switching it to a bit stream
comprehension comprehension and that in
itself made a huge huge difference if
we're large binaries are just put in the
binary heap and then references are
copied around and there's a lot less
garbage collection and so that was that
was kind of a lesson we learned that I
owe streams are great especially if you
need to tack Stefan to the front of
things so you're not doing a lot of
caffeine but you don't want to make
every little piece of data a separate
piece of an iostream as this kind of
what we learn from this and then plus we
also find out that even though the the
bytecode output use bit streams pretty
well native the native compiler does a
wonderful job building up these binders
it really increased we were seeing at
the worst case we're seeing 80
milliseconds for building up these
messages and then it was like
sub-millisecond level once we had native
and bit stream binaries being built we
also found that merging was too
aggressive because we do have some
devices where their length and also
parameters are not are nonlinear they're
not the offsets don't actually represent
by it offsets because some devices
take up more than 64 K and so they
couldn't fit it in a single packet star
they couldn't represent the offset in 32
bits because our early protocols had a
16 bit offset so they start doing things
were the offset represent which bank add
data or which you know chunks of data in
some certain size so we found these
handful of devices on a new DPM return
errors when you try to merge their their
requests together so either correct
solution is to fix these devices because
now our protocols have 32-bit links and
offsets but they're older devices that's
not a quite a priority for some people
we have a lot of other projects going on
that have our higher priority so we'll
get there eventually but right now our
current solution is that we do have a
field in our database that we can use to
mark devices nonlinear and then DP it
will add a test and DPM and we can use
that fixed list concept to make sure
that no one plays around or adjusts any
of the devices and a fix list which will
be the nonlinear devices so right now
our three layers we have an accurate
layer the job and the front end layer
but we have an experimental TCP
interface as well so that for instance
like Python scripts can connect up to
the DPM and get accelerated data these
are scripts that need to maybe do a
little analysis or data collection but
they don't need the full active services
so we don't have to write an act net
library and make them a true act net
client we can actually just let them use
a TCP interface and just get accelerator
data one of the other things we're
looking into is we do have we use
RabbitMQ here for data collection for
some synaptic displays and that we might
be able to we're looking into adding a
RabbitMQ interface at the top level so
that we can maybe provide a constant a
be a constant producer of data for
particular channels that synaptic
display clients can subscribe to that's
the potential thing down the road now we
also we speak only to acne friends ray
now but there's another control system
that most all other labs around the
world use called epics and it's protocol
is called channel access and one of the
things we could do is add a layer down
there and so that the front end workers
we can have an epic front-end worker
they can speak channel access and that
would allow our control system to speak
to both acted front ends and epics front
ends and the reason why we want to do
this is that we have a lot of
international collaboration going on and
they invariably give to us a epics front
end to drop into the control system
we've ended up either front ending them
with a front end or sometimes if there's
not a lot of drivers even rewriting some
of the stuff but we'd like to be able to
take their efforts and drop on right in
the control system and that have that
extra hop of data and issues that that
might cause so the epics interfaces is a
very interesting one that we might
pursue in 2016 so as not related to DPM
necessarily but as a group and our
controls Department oh there's a couple
things we'd like to do Erlang wises is
look at third more third party community
efforts like lager XO meter um those
look like they can be very useful and
there's no reason for us to be trying to
reinvent that stuff or also finished up
a trial for the wombat tools for
monitoring that we can use for our DPM
and other erling base so we're looking
into getting a license for that and then
we four or five years ago rolled our own
build system which works for us but
hasn't really evolved too much and so we
should really be looking at things that
like Erling mkay or or rebar as a way
for us to to be part of a more active
development cycle and benefit from those
efforts so you know we're preaching to
the choir everyone here is interested in
Erlang but you know we got to say it's
just so that's a real fun language to
use a very interesting language very
interesting new ways of thinking new
ways of approaching things and I find
when I when I'm frustrated with erling
its tip
because I've made a function too
complicated and needs to be simplified
and and then it gets to be fun again and
then things work really well and we also
found that breaking a problem in the
simple cooperating process it just makes
the system reliable and scalable because
each process is just doing one thing
well and and you know talking to each
other through message queues is so much
better than trying to handle mutexes and
other times it's just it's a really nice
clean way to make a concurrent system
and also the Erlang shell is also very
powerful work we're used to using in our
in our group we have a lot of vxworks
nodes and using the Erlang shell is like
a super powerful VX work show not only
do you get to do these great expression
evaluations or even compile code as far
as you know create a new functions and
stuff on the fly but you're looking at
the vm you're checking process state
your starting and stopping stuff it's
like it's a really souped-up OS it's
really cool and the fact that you can
connect the shell into a remote vm is
just awesome we have used Erlang and
some other projects here besides the PMD
pm's the most recent one that we have
done we do have as mentioned in this
talk a framework for front ends itself
that's probably have how many do you
think they have up now degree by
original about 25 Erlang front ends
mainly talking to network devices but we
do have a few that are talking too hard
or devices as well the wilson hall the
high-rise that then i showed you early
on right now the whole lighting system
is running off of an erlang server and
our lab wide fire security reporting
system has also been converted to a
Erlang system so we're really finding
erling very useful very reliable and
really enjoy working with it so I'd like
to thank you for watching hopefully you
all
just because we don't get a little
feedback while we're going on Oh first
of all I'd like to thank Denison Richard
for a very inspiring talk on their use
of our language and generally on use of
a leg at fermilab we apologize for some
connectivity issues we've had earlier
luckily not during the webinar itself
which prevented me in properly
introducing Dennis and Richard but I
will briefly say that between them you
know you have nearly 40 years of
experience of working at 30 laps and a
number of masters degrees and the number
of years of experience of dealing with
pretty much every aspect of software you
know at fermilab so it's fantastic to
hear about Earl I'm sort of at the
forefront of this scientific endeavor
and what I'd like to do now is really
sort of open up for questions so just to
say that and this is another thing that
we could not communicate at the
beginning if you have any questions if
you wish to ask any questions of our
speakers today then you have the
questions tab at the webinars interface
if you use that tab you can post a
question and we commit to answering
questions in the order they were
received we only have a couple of
minutes left so we'll obviously try and
answer as many questions as we can so
feel free to post them and we'll get to
them so to start with the first question
that has arrived from semen or Simon so
Simon is asking Richard and Dennis how
long did it actually take you to build
this framework and what was the most
difficult part of that work the DPM
application or the front-end framework
of the D I'm thinking he's probably
talking DPN right I would think so it's
not specified but it's likely to be
referring to DPM yes okay so DPM we
started it in spring Marge ish I would
say and we've got the to the point of
pushing out in the fall so probably was
like six months of its I would put it
about six months now it's
commission for a while we've only lets
right now by default only the control
system Department personnel has been
using it for the past year recently
we've expanded it a little bit further
so that more users are now by default
using it using the new DPM instead of
the old one the main control room is
still using the old one but we're slowly
increase in the usage and and like
written recently we found that issue
with nonlinear devices from one of the
users outside the controls apartment so
so really about six months to put the
system together and then we started
releasing it to other people to actually
start using thank you for that we have
another question from Richard so Richard
is asking what is the sort of single
most I would say the number one
capability you personally would like to
see added to our line to make it even
more useful to yourselves uh I I'm
actually actually like languages that
have more static type checking and
Erlang's dynamic nature is sometimes can
be an issue for me because I like the
compiler to catch if there's there's a
lot of hers that I think that should
have been caught by the compiler and I
think one thing I'd like to see with
Erlang is is instead of dialyzer being a
separate thing I'd like to see the
compiler to include a little bit more
static type checking because I think
with the specs you know you can specify
specs I think there's a possibility that
there's some easily caught errors that
are let you know that instead you got to
kind of debug it so I think that's how I
would want to Venice they have any you
know we come from a C C++ I've also
worked with o camel and Haskell and and
it's really kind of nice that if it
compiles cleanly there's a really strong
chance that that you're not going to
have there's a whole class of bugs that
get solved by that and when I go to
Erlang you know pattern matching goes a
long way pattern matching does catch a
lot of things but there are some
cases where even pattern matching will
catch a bug and so I'd like to see that
I don't know how doable is our how
feasible it is but dialyzer sometimes
just takes a little too long to do every
compile cycle to run dialyzer so it I'd
like to see that rolled into the
National compile thank you for that so
straight on to the next question Michael
is asking do you use any particular
tools coming from our line for
monitoring utilization of different
parts in the DPM and also in terms of
utilization in the bottlenecks right now
our tools are fairly primitive so like
we're looking at using wombat now which
would buy us a lot of monitoring that we
don't already have we're using just
really system-level simple things even
things outside of the vm like like you
know PS on top and some other unix tools
that can check a process but then inside
there's just some of the statistics that
you can kind of look at but overall
aside from our report logging and things
like that it's not as much as what we
need to do so we do have to get a little
bit better at that point at that part
but not the one thing I might just
mention real quick is that if we do see
right now we have four deep PM's running
and they're all not hardly loaded at all
so I mean if we start seeing performance
problems it's very easy to start up
another airline we have over 70 machines
the run Amman and so right now it hasn't
been an issue because we have a lot of
expansion right house but it is
definitely something we have to look and
see I would have that you know operators
are watching our control system 24 hours
a day and so we do get notified pretty
rapidly if something's going wrong and
we even in the performance of the DPM or
in one of the front ends or something
like that so thank you for that so you
did mention in your presentation you are
a sort of use of one button looking into
that so that I guess sort of answers
that question partially as you said yes
just to say we'll try an answer as many
questions as we can we have a bit of an
avalanche of questions coming in all of
them as much as possible so moving
straight on to the next question and
quite an interesting one from James so
James is asking are your front ends
sensors only or do they also control
behavior of particles etc well yeah so
so they do control the behavior of the
particles as a whole the particle beam
as a whole in the you know so like one
of our one thing we might control be a
kicker magnet a kicker Megan would fire
and redirect the beam from one path down
a different mean life or some like that
and that kicker magnet is controlled by
our control system and so you know we in
that respect and we do control particles
themselves and we have been positioned
monitors that measure the actual beam
going by you know the electrical
currents induced by the beam going
bye-bye the sensors we also have a
auto-tune orbit application that runs in
it it tells front ends to increase or
decrease intensities on magnets that are
used to focus and steer the beam there's
there's magnets that are used to bend
the curve around the ring and then
there's also magnet settle that have
more poles on it so that the beam as it
passes through gets tighter together
because the beam has a tendency to want
to split apart so yes with the front
ends that we control not only do a lot
of read Beck's but there are some
devices they control that that force the
focus the beam and steer it around the
ring so yes thank you for that so we
have a really interesting question that
Yahoo de is asking so we all know that
airline is known to be the kind of
language that is just perfect for quick
prototyping and great speed to market
certainly from an airline solutions
perspective we have a lot of companies
and organizations we work with who pick
a line specifically to sort of beat the
competition and you will come to market
first with their product so Yahoo dough
is asking can you estimate
development time required for this
particular project using other
technologies apart from our line and you
know what would your sort of view on
that be uh I mean certainly this could
be done DPM could have been done in any
language I know we started a colleague
of mine and I started DPM with C++ which
C++ is probably the primary language
although Java is a huge catching up
secondary language if Fermi Erling is
still the minority but you know Dennis I
are hoping that it's a it's a little bit
more he's here but so we started DPM
with with the c++ and we vowed that we
were going to use all the latest
language features and no we're going to
use smart pointers we're going to do
containers all that stuff and we were
still getting seg faults and where we're
getting tired of trying to figure out
how we were violating some of these safe
programming pratik practices and we
already had a bunch of erling project so
I said let's let's see how far we get
with early and we've really started
moving along with us so you know it's I
think it depends on how how comfortable
people are with the language they are is
going to tell you how quickly they can
get to market but you know a little bit
of Erlang blows up into a lot of C code
to do the equivalent work and yet the
Erlang stuffs more concise and
understandable I think the bugs are
easier to find an hour fewer you don't
worry about buffer overruns you don't
worry about side effects you know this
so it's really hard to give you a number
like it'll take you twice as long or
three times as long I don't know if I
can really say that but but there's just
a whole class of bugs I just don't
happen when you program an erlang and it
just makes you much more efficient you
can really focus on the job at hand I
don't know if that's a good enough
answer sorry I think that's perfect and
that reflects the feedback that we get
from the market every day
so again just to sort of say to our
audience we are answering the questions
in the other day we receive and I
apologize but we can only really handle
to further questions and to quickly ask
them a question from Anthony is
basically are you planning to open
source any of this and are you looking
for contributions from the open source
community I I doubt that some of this
stuff is going to be open source
directly because there are a lot of
Fermi specific things in it that no one
would find useful I think those some of
our there are a few applications that we
pull together I think some of those we
might be able to open open source
because they are a little bit more
general there's there's a like in our
wilson hall lighting system we have a
scheduler that that i was thinking about
pulling out because it has more
widespread use in fact i used it in the
fire incident reporting system as well
so it was general enough that i can use
it in both locations and it might be
useful to other people as well so
there's a couple subsets of it that I
would look into there's some legal stuff
we have to go through there's I got it I
got to make sure that our technology
exchange Department I follow all the
rules that firm you like this setup
because we're a Department of Energy
layout and there's certain things that
we have to follow but if they say it's
okay there's a couple of applications i
think i might end up putting on github
and letting other people help you know
develop and improve and use so but not
and that isn't overall I don't think all
of DPM would be one of those
applications okay thank you for that we
really only have time for a single
question and I think it's appropriate to
finish on a bit of a strategic notes
that James is asking a really
interesting one what is the future of
our line at fermilab well I think we try
to become ambassadors for it here you
know and we've got a couple of other
other groups other departments you know
at least interested in you know seeing
what we
done and are you know at least started
thinking about implementing erling
applications also we're definitely you
know committed to continuing using it
for these applications that we've
developed here and I think we see that
spreading significantly you know as as
our front for instance of with our
friend framework we have I think about
like a couple hundred front ends overall
only 25 25 of those or Erling right now
we expect that number to grow and the
other number of older legacy systems to
decrease um we probably will start
looking at other more the client level
you know applications like the tuning
kind of applications that we can do in
Erlang as we get it spread around and go
more proficient and you know again other
groups of firming up or bury you very
disconnected from us in terms of doing
other scientific computing tasks and you
know we have tried to spread the word a
little bit there and got a little
interests and some of the other
front-end developers outside of our
group have taken our Erlang framework
and have installed it on ARM processors
running Linux so so there's it's already
been moved in areas that we didn't
actually do the development and there
they're actually taking it running with
it so it's gaining the a little bit of
foothold here and there and we're just
going to keep pushing it thank you for
that and that was the last question we
had time for now I'm sure that all of
you will join me in thanking Dennison
Richard for what has been a hugely
compelling talk and in my humble opinion
in very tough competition the best
webinar we've had this year and
fittingly at the end of the year now I
would like to invite you to join us
again for our next monthly webinar which
we now which will now be next year
following today we will send you a very
short survey to make sure we capture
your feedback of today's webinar please
also note that the recording of the
webinar itself and the presentation that
was shared will be available for you to
collect on our corporate website at
airline hyphen solutions com now we wish
you all a Merry Christmas a Happy New
Year
thank you all once again and we look
forward to seeing you at our next
webinar thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>