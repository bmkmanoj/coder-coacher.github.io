<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>A Scalability Study of Erlang/OTP: Kostis Sagonas | Coder Coacher - Coaching Coders</title><meta content="A Scalability Study of Erlang/OTP: Kostis Sagonas - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>A Scalability Study of Erlang/OTP: Kostis Sagonas</b></h2><h5 class="post__date">2012-06-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/VrwS_8ESPjQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I have to admit that I have not prepared
very well for this talk because I wanted
to improvise to some extent and also
wanted to show you some things that
hopefully you will find interesting I
don't know how it will come out in the
video also i will tell i will share some
of my experiences in using Elango PP for
parallelization and trying to see a bit
how it can improve or how what's the
current situation is what possibly are
bottlenecks and what can be improved and
I'm glad to say that actually we have
found some bottlenecks but we are
improving them as we speak and hopefully
you will not notice them before they get
fixed this is all done in the context of
an EU project that we started relatively
recently it says there that it started
officially in october two thousand
eleven but in reality the kickoff
meeting was in the first of november of
that month just before the airline
factory in stockholm for those who were
fortunate enough to to be there but in
its supposed to continue for the next 25
years from now ending in September of
2014 we have a pretty good team
assembled we have ericsson on board
along solutions which hosting this event
electricite de france who is using a who
they are using our line for some big
simulations of things that I'm not
allowed to talk about and we have four
academic partners to in the UK
heriot-watt university and the
university of kent and Uppsala
University and the National Technical
University of Athens actually it's a
research center there so the goals of
the release project
is to scale Airlines concurrency
oriented programming paradigm and build
reliable general paper software such as
server based systems on massively
parallel machines so we aim to target at
some point machines with hundreds of
thousands of course clearly we are going
to go into the ten thousands of course
in the lifetime of the project but
hopefully we're going to make changes
both to the language and Valentine
system that will allow airline to scale
in that number of course so we want to
investigate language relatives for
scalable distribution so especially our
partners in heriot-watt University are
leading that effort on the distribution
part the effort that I am leading is the
virtual machine extensions and
improvements and we are working very
closely with erection on that part the
effort that the University of Kent with
Simon sitting there leading that effort
is to create tools for paralyzing and
refactoring existing code also tools for
profiling and testing for errors in the
testing course is more my part I guess
the part of a long solutions is to build
scalable virtualization infrastructure
and investigate what are good
alternatives in that respect and we have
one part so that we actually go to words
there's so many course together with
electricity de france on porting airline
OTP to the blue jean architecture and
actually they have more than one blue
jean machine and they are located in
Paris so I'll definitely be
I would I would be doing that anyway but
it's very nice that they are located in
such a nice place so that's pretty much
what the release is about and what is
this talk about really I wish I knew but
what it is about is to investigate the
scalability of airline go TP as it is
today start from today's baseline in
some sense and see where are the
bottlenecks and how we can detect as
many of them and how we can possibly
improve the situation and we started on
multi-core machines no distribution
right now the idea is that we are going
to also include benchmarks that have a
part of distribution that also run on
multiple machines but for the time being
we are concentrating on big multi-core
machines both of the type of machine
that you will have on your desktop a
typical eight core machine or four core
with hyper-threading machine that many
of you might have on the desktop or at
least the fortunate ones and on also big
multi-core machines and i will show you
some numbers from a big multi-core
machine right now also this talk is
about reporting something that we have
been working for the last three months
on our effort to paralyse dialyzer as a
concrete non-trivial application
overland something that has about 30,000
lines of code and how you can
parallelize such a code base and I will
share with you some experience and
lessons learned some of them are
frightening so I don't know what will
happen with the video afterwards but
anyway and show some performance numbers
and also give a sneak preview of the
plan then future vm improvements for a
la mode ET some of the things we are
doing I don't have them written but I
will tell you what we are doing right
now and finally and most importantly I
want to ask you for your involvement I
want benchmarks that if you have
scalability needs and you have something
that you can share with us either in the
public domain or in a place i can
guarantee that if you give me a
benchmark that you don't want to share
with the rest of the world we can
guarantee that will not be shared with
the rest of the world hopefully there
will not be that many of those most of
you can probably distill the interesting
part of your application in something
that we can share with others anyway
what else whether i do i do let me go to
the first part so we have created or
actually we got from Ellen OTP team a
set of 14 concurrent programs that have
been used for quite a while for
benchmarking of the SMP implementation
and we turn them into scalability into
scalability benchmarks really benchmark
that measure the performance improvement
or the gradation hopefully not
degradation as the number of course
increases and we are currently running
them on three platforms in a relatively
new i7 with four cores and eight logical
threads a four-year-old xeon server with
16 cores and a brand-new aim the
bulldozer type
of machine server with 64 course plus
four cpus with 16 cores each so it would
only show some of the numbers that we
have from the first one and the last one
to show you the two extremes i guess and
there is a technical report for those of
you that are interested to see the rest
of the numbers by the way our intention
is that all of these things will be
public so we have a site for release
where these things actually are there
but we have to polish them a bit but
hopefully pretty soon you will be able
to actually access them and play with
them if you are interested so out of
these 14 concurrent programs I would
just show you four of them and how they
behave as we increase the number of
course and schedulers of the airline BM
from one to the max number of course
that the or threads actually that the
machine can accommodate so that's one of
them it's called the airline hack bench
on an i7 remember this is a machine with
for physical course and eight threads so
we are starting the vm with 128
schedulers and we are measuring the
speed ups that we get as running the
exactly the same work actually four of
them here on the virtual machine running
with from 128 schedulers and actually
you can see that this is a good case
pretty much here we get close to linear
speed ups very very good speed ups up to
3.4 or something like that
on f4 on a for physical core machine and
then of course with a hyper threading
you cannot expect to get the linear
speed up above that this is not a
singlet on oops sorry they are different
they are different it's a different
workload so I don't remember the you
remember Raquel do what they are ok its
parameter to the test yeah but these
numbers are actually more than the
number of scheduler there are so we are
at we are starting that many processes
and sending messages or something like
that there is Mandelbrot which also
shows pretty much the same behavior here
up to four course we have pretty much
perfect speed up and above that the
machine cannot handle this sort of thing
anymore as I said it's just a fork or
machine not everything really looks so
nice here and i will show you one that
oh actually there's another one random
thingy that also shows pretty much the
same behavior there are things that
involve the edge stables where you can
see some very different situation the
one is here and when you start on a
small number of schedules you get some
small speed ups but as you have more and
our process is trying to access the same
at stable there is even a degradation in
the performance so this is one of the
things that we have actually written
down as an improvement that we are going
to specifically look at the performance
of EDS on big multi-core machines in the
proposal and I'm very glad that it
wasn't just something that I dreamed up
as a possible problem but it's actually
something that needs to improve yes okay
so yes we are actually measuring this
too and there will be a report the first
deliverable in the work package that we
have will be the report and we are
actually measuring the reed concurrency
options and one more i don't remember
which one I don't have this number right
now but they in this particular
benchmark they don't help very very much
because they you know there is even a
degradation as you increase the number
of schedules yeah
right sorry these are just exams and
synthetic benchmarks okay they measure
specific things so they are not real
applications in real applications you
will have some part that is accessing at
stables you will have some parts that
are doing hopefully independent
computation in the rear processes you
will have a mix of these things and this
is actually the the next step we want to
actually measure real applications but
measuring real applications means that
somebody who actually has the real
applications has to send them to us
because otherwise we are not going to be
measuring real applications we are going
to be measuring made up applications
that we dream of now how do these things
look when we go to the big machine to
the bulldozer machine that has many
schedulers where we can start the system
from one 264 schedulers and you can see
that we get some better speed ups in the
beginning up to 16 17 18 20 perhaps
course but after that things start to
flatten out and this is not particularly
a typical you get this sort of behavior
in most real applications you will have
some sequential part and under slow will
bite you in these cases
the same thing can be seen on oh
actually let's see what happens on
Mandelbrot and i will do a demo right
now i will just try to connect on the
machine and here it is unfortunately my
dear student is also running some things
as you see here is a yeah this is Friday
night in Sweden you run benchmarks like
that so but let me run it perhaps this
is too small for you to see let me make
it bigger
so I will just keep that that one there
so that you see the processors working
up
no no no I will tell you what this is so
let's say start with four schedulers
first and let's run Mandelbrot actually
let's compile this to be sure that it's
the new one it's actually in native code
this is i have put this is one of the
well it doesn't really matter the disk
as far as scalability is concerned you
get the same results whether you run
native code or not but it just so
happens that this particular benchmark
is running at eight to ten times faster
at least on native code so i have
actually put compile native code as a
attribute in the file to make sure that
this is actually running native code but
as far as scalability is concerned it's
it's the same thing so i can run this
sort of thing some numbers 402 and
unfortunately you cannot see which ones
they are their hours and which ones are
my students but we will get a number
here and this is the interesting part so
now four of them are clearly mine we
started the system with poor schedulers
and we will get its twenty three
thousand seven hundred something this is
the time that it takes to finish this so
now let's exit here and run this again
now with eight schedulers I don't need
to compile now
so the previous time was 23,000
something let's see how this will work
on double the amount of schedulers
12,000 681 so it's pretty good let's see
on 16
and you will see now at some point 16 of
them going on there and actually one
nice thing is you see them they are all
green it means that this is actually
very very very nicely parallelizable I
don't know whether you know this
particular tool it's called H stop I
very much recommend it to everybody it's
really really really nice it's a
replacement for top it's an open source
thing you can get it as a package in
most Linux UNIX distributions so now the
time is actually a bit less than half
before it was 12,000 681 now it's 6200
something and just for fun let's start
it once more with 32 cores this will be
very fast we are not just going to see
them they're being flash lines 3400
something so you can see that in some
cases where you have really independent
computation the base of the virtual
machine has actually been programmed and
has all the right components to execute
this thing very very fast and without
really any bottlenecks at all the
numbers here I'm running on a size of
400 of the image and I'm running with
200 processes so the process is here are
more than the number of schedulers and
it's of course not perfectly balanced
it's not the visible number by 16 or any
any power of two actually other than 20
for such
so back to the presentation
there are other benchmarks that don't
behave as well as good as so actually
Mandelbrot is one of the benchmarks that
we would get something like 40 to 45
time speed up on 62 64 course so it's
actually very very nice benchmark I wish
that all of them were like that not
everything is like that if you if you
have a bit of sharing and contention you
will get something like that in many
cases you will get a speed up that
reaches at most eight even if you are
running on a big multi-core machine and
of course if you have it's you will get
things like this one the one is there ok
by the way its Caesar is very nice for
some things which actually brings me to
my next one our experiences on dialyzer
so dialyzer for those of you that don't
know it's a tool that analyze a source
code if you are have a small codebase it
works pretty fast and it's actually very
effective if you have a big codebase
it's even more effective because it
finds many more things that you probably
have overlooked but it's actually slow
so if we are analyzing the whole airline
moti p on this machine that i just
showed you it consists of more than one
and a half million lines of code every
airline code and in the sequential
version or actually the version that is
in our 15 it takes on this machine 83
minutes in building a big PLT persistent
lookup table for a standard lib journal
and some more applications that are
important it takes 26 minutes on the
sequential version
so we wanted to paralyze this this is a
big application it's completely redone
in Erlang so we thought that this would
be a relatively easy thing to do the bad
news is that actually there is state
data that's passed around there is into
it's written the code the code was
written in a very nice punch functional
style where you are passing an extra
argument which has the state around and
this works very very nicely when you are
executing sequential code inner line why
is that because when you call something
when you call a function you just end
this executing the same process you just
pass a pointer to the extra argument
that you have already in your hip most
probably so you just pass a pointer to
the hip and it's very nice very it works
very very well so how do you paralyzed
code that in pretty much all languages
so what do you do what's the basic thing
to do to paralyze code inner line you
spawn unfortunately the there are many
people that don't realize that spawning
is actually something that has a cost
because you have to actually now you
cannot pass a pointer to the data to the
arguments anymore you have to literally
copy the arguments to the process that
you are creating so now you change the
the cost of the function call from just
constant time in all in the knife or
proportional to this to the number of
arguments now you change it proportional
to the size of the arguments and if you
are passing around big data structures
you are need to traverse them and
actually the current implementation
doesn't just reverse them once it
reverses them it reverses them more than
once for good reasons but it also does
something which is even worse it
actually expands these terms in the case
where there is sharing so our first
attempt to parallelize dialyzer resulted
in a nightmare we just added a spawn
statement and this is actually probably
the part of the video that has to be
edited out this is not a joke
unfortunately this is not a joke we
added a spawn statement on one place in
the code where we wanted to spawn the
analysis function for each of the
functions that we were analyzing for the
modules or whatever doesn't really
matter and we experienced something that
looked like an infinite loop because
something that was running in literally
some a minute hadn't finished after 1
hour so that's my definition of an
infinite loop
so I said something is wrong here
actually it was my student Stavros that
had that so he it was actually very
similar situation to this one he started
doing this on a Friday evening and he
did it at home so he thought that
something is wrong in his laptop or the
connection is problematic so he actually
was working on it all the weekend and on
Monday he sorry you know you have
to see this there is an infinite loop
here said okay but have you tried
debugging this at all or just you know
set debugging what is this he answered
kk he asked me this is a joke and I said
ok did you put an IO format somewhere
because that's how I debug things so we
took out the spawn and put an IO format
in one particular line that I can show
you in dialyzers code and guess what can
you guess
we experienced the same infinite loop so
we took out a spawn and we put an IO
format and we experienced the same thing
that looked like an infinite loop so at
that time I called the Bjorn Gustafson
at the Erlang go TP team and said girl
you have to see this just add an IO
format on that line so he said okay I'll
take a look and he found it also
interesting I guess and he showed it to
actually he told me something else at
the time would solve the problem said oh
don't use io format use a line display
an airline display worked io format
where I offer Matt did not work so he
actually showed it to Patrick is it
that's what I think right and Patrick
actually found the problem I he did so
to make the long story short what
happens if you using io format you send
the data to some other process which
actually copies the data to the other
process so the data was the state this
state that we were trying to paralyze
was something that was in the order of
some megabytes I don't really remember
the exact size but say let's say 20
megabytes for just to give you some idea
so what was happening is that this was a
term that had a lot of sharing and when
we were trying to print it using i/o
format all the sharing was expanding and
guess what it came to some gigabytes
of memory I actually left it running can
you guess how much time it run before it
actually printed the term not a whole
term i just put to print some part of
the term now because you can do that
anybody wants to guess 26 hours just to
print the top most five levels of the
term 26 hours because it has to copy
this term from one point to the other
now it so happens that actually this was
a case that we could fix the problem
because what we were passing was the
whole state but in the function that we
were passing it in there in the closure
actually we were passing it in as an
extra argument order as a implied
argument actually it was only using
parts of the state if this is a typical
situation we think of some structure
some record that you are passing as an
argument and in the function itself you
don't use the whole term but only some
fields of this record or the structure
if you are lazy you might not pass the
individual pointers to these fields but
you might pass the whole term so once we
change that it was a one-line change
actually it was this is the thing that
actually amazed me was it beyond that
found this or you the problem is that
this was literally 1200 lines away from
in the same file but 1200 lines away
from the IO format and the spawn
so anyway so after we fix the problem
actually the parallelization of dialyzer
was relatively smooth modulo finding
some issues in some of the standard
libraries and this is a typical
situation once you start paralyzing some
code you discover all the sequential
bottlenecks that you have in your system
and you never thought you had them
because what's happening is that the
parallel the code that you paralyze now
suddenly becomes really really fast and
your bottlenecks are the sick day that
what you'd haven't parallelized because
you thought that this was really going
to be executing very fast and this is
the point that i'm going to show you how
actually let me take one more just to
give you some idea this is a very very
high level description of how the
dialyzer works so that we understand
what i'll show you obviously you know
daylight it has about 30,000 lines of
code and this is what it boils down to
we initialize the state given the input
and the input here is the number of
files that we want to analyze and we put
them in the state the state will contain
information about all the code that we
have all the type declarations that this
code might have or the record that it
has all the external types you know
various things related to the code and
this is the state we have to pass around
this state will have also something that
will have the types that we try to infer
that we're also going to store there so
while we haven't reached a fixed point
meaning that we have not found all the
information that we are interested in in
the analysis we find types this is a
analysis that goes bottom-up it goes
from the functions that don't
depend on any other functions up to the
exported functions of the modular the
application and then we refine them
based on usage within the module so
there is a an analysis that goes
bottom-up and then an analysis that goes
top down to infer this types and once we
have we do some fixed point computation
here while we are finding more and more
information and we have not reached the
fixed point we do this sort of thing
this might repeat some number of
iterations and once we have found all
these things or the types we might be
generating warnings we might be because
we might be just building the PLT to
store the type information if we build a
PLT we don't generate warnings if we do
if we run dialyzer just to generate the
warnings we will do that sort of thing
so that's actually the idea of
fertilizer how dialyzer works so let me
show you now on my laptop how I've run
dialyzer here on a bunch of applications
actually I run this script which
analyzes ace and 18 do it doc Ayres
compiler crypto dialyzer in its height
there are you can read that's a bunch of
20 applications pretty much and this is
a sequential version of dialyzer
and I've run it with one scheduler and
you can see here what happens these are
all the faces that it does it compiled
all the files if there were nine hundred
and twenty two modules in total it then
cleaned up the call graph it find it
found the remote types it did some
ordering so that it starts the type
inference here which analyzed that many
strongly connected components salon
connect components are possibly
functions that are calling each other
mutually recursive so they are at least
that many functions that being being
analyzed then it ordered this so it does
the refinement in the top down fashion
and then it did another thing like that
for some of these ACC's we have found
all the types that we that we can infer
so we don't consider them any more than
the analysis for the remaining ones we
have to do the analysis again it turns
out that out of a 53,000 strongly
connected components the 20 the 43,000
approximately need to be reanalyzed we
are doing the refinement and then we do
one more type sig here you can see that
most of the strongly connected component
we have found type information but they
are 1539 which we still need to analyze
and hopefully we reach the fixed point
here and that's what's happening this is
the time that it takes to write the PLT
in the in the system in the file system
so this is just to give you an idea of
how this works on just one core and I
will run it now with eight cores here my
machine only has four physical cores but
its high it has hyperthreading so we are
going to be looking
it's while it works and actually i have
h stop here and this is the interesting
part of h top so the compilation what it
does it compiles all the files to correr
lang and already you can see that this
took just 12 seconds when it took what
was it before I've lost it anyway so the
first type sig here took 300 something
seconds let's see how much it will take
now and let's see if we have good
utilization you will see here that it's
pretty good the utilization right now
all the green things means we are doing
the green thing is very good okay it's a
the red thing means that there is some
system time there most probably due to
it we are using its tables here because
we are storing both the code in the edge
tables and the types that we are
inferring but we are using them in a
relatively nice way we are using a
merely as a database but of course there
is some cost but you can see that
actually there is a very nicely
parallelizable problem here we get now
we are running out of work at this point
because we have analyzed pretty much
most of the functions and it has
finished it has finished now in 86
seconds where it took three hundred
three seconds before and now we are
doing the refinement and still you can
see this one is very also very nicely
parallelizable to make the long story
short on this for core machine
in I think I will get something like two
and a half to three times faster or a
bit more it took 15 minutes and
something before now this will finish in
I think something more than four okay
yeah let me just out of curiosity let's
see how this will work the second type
seek by the way you will notice that
even though it analyzes less less
strongly connected components it takes
more than the first one and the reason
for that is that now we have inferred
type information and we are working with
big types with big structures while in
the first iteration we only had very
small types to work with
you
so let me skip this and will show one at
the end of the talk I will just skip
this part and let's go directly to the
to the speed ups that we get this is the
speed ups of the individual faces I hope
that the colors are visible the two
important ones are the two red ones
which are the type sig analysis that
take a lot of time and there is one here
in the middle the black one which is the
total speed up while the other ones are
just the individual faces so we have
paralyzed all the faces and up to four
schedulers we get pretty good have to
have 24 schedulers we get very good
speed ups and then of course is the
hyper threading you cannot expect to get
linear speed-up at that time but even
with hyper-threading it actually works
improves the same thing on the big
machine there we get speed ups up to
overall up to eight nine times eight
times I guess here seven-and-a-half
something the two big
faces they have better overall speed up
but still in this machine you cannot
expect something above the 30 schedulers
it also has to do with the memory
architecture of this machine it has very
little given related to the number of
course it has very little level 2 and
level 3 cash now let's go back and see
if we discuss finished and how much time
it took has finished in four minutes 50
seconds when it was taking 15 something
before yeah anyway so I will I'm running
out of time so I will stop here and take
some questions i can say that as part of
what we are doing we have identified
some of the bottlenecks I've already
talked to one of about one of them the
preserving the sharing when you are
sending terms across processes and
hopefully also across nodes and we are
working on this and I think we have a
very very good solution which we are
actually benchmarking right now and
hopefully this will not be an issue in
future OTP versions we have identified
some other smaller bottlenecks that I
probably should not speak right now
about them there is a big issue with the
libraries some of the libraries have
been written long before the multi-core
era existed so they are mostly
sequential so it you want to use
something that takes a lot of time in
some library it's very difficult
other language today to paralyze it you
have to do it yourself what else the
last thing I want to say is please send
us your experiences and more importantly
send us benchmarks to include in this
benchmark suite there is a very very
good reason to do that sort of thing we
I can guarantee you that we are going to
look at them very closely and we'll send
you back any suggestions for
improvements so this will improve your
applications that's one thing you will
get free consulting in some sense the
other reason is that hopefully these
benchmarks will drive the development of
her language p for the next few years so
if you manage to put your needs their
chances are that the OTP team end or the
release partners will be doing something
good for your for the needs of your
application in the years to come thanks
a lot yeah
like if the whole system in
we're for airline one of the things you
definitely need is a machine with a lot
of cash so the cash that you have in
your machine plays a very important role
so if you want / for this is a machine
that does not have a lot of cash pair
core it's actually a 6-h chip has 16
cores and it only has 12 megabytes of
shared ll2 and another unfortunately
only 12 megabytes of l3 cache which is
shared among everybody so the cash here
is is killing us so the number one thing
I recommend for good scalability and
performance is machines with as much
cash as you can afford on the i7 which
has a lot of cash the the speed ups are
much better yes
obviously it matters how it matters I
have not investigated so far Jay you had
a question well actually as part of
paralyzing dialyzer we had to do some
gin servers and we have global
coordinator that actually gives tickets
and we had to do that sort of thing
actually not for performance we make the
performance worse but we have to do it
so that we don't generate all the
processes at once when we have a bow a
good bound or some bound at least on the
number of memory of the amount of memory
we use yes last question perhaps
something that can be run on its own it
doesn't need an exotic database or some
hardware that only you have in your
environment and something that has a lot
of processes hopefully more than 64
because if you only have forty two
processes you cannot expect more than a
40 to speed up so something that has a
lot of processes and it can be run just
by pressing a button saying you know
bench that's all if you have been
charged with distribution obviously we
are very interested in those two and
this is the next step for the time being
I for practical reasons and for big
because I don't have that many 64 core
machines lying around we are only
concentrating for the timing for in
machines that with a lot of course but
we will go when we pour to the blue jean
we will go to many nodes definitely and
we do have now clusters of machines that
we can run these things so please buy me
a beer and send me benchmarks not
necessarily in that order
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>