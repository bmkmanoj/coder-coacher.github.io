<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Lightning Talk: Talking to Machine - Robert Beene | Coder Coacher - Coaching Coders</title><meta content="Lightning Talk: Talking to Machine - Robert Beene - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Lightning Talk: Talking to Machine - Robert Beene</b></h2><h5 class="post__date">2017-03-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ynVPhzgPBFo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so an echo bind I've had the opportunity
to do a handful of voice integrations
with LexA and despite the end goal being
able to talk to machine it's important
remember we're really just talking about
communication and as I've prepared for
giving this talk in Boston I researched
the history of how people have leverage
technology to communicate over the years
you can find this timeline of
communication tools on Wikipedia and I
found it a couple interesting facts
while looking at this an unintended
consequence of the Gutenberg press that
was invented in 1440 was the need for
copyright laws you could basically make
unauthorized copies of book so much
easier than by writing it by hand and so
in 1662 a copyright law was invented in
England and in 1897 the Oxford
Dictionary introduced the word computer
referring to a mechanical communication
or calculation device now since I'm
coming in from Boston I'd be remiss to
not mention Paul Revere one if by land
two if by sea communication happens in
all different forms it could be a book
it could be a lantern it could be a nod
it could be a wave and despite all of
these forms of communication there's
been a push for decades to return to our
roots to return to voice and in doing so
creating natural language interfaces so
in 2011 this kind of went mainstream
with Apple Siri Amazon Alexa 2014 and
Google home just last year and with
these open platforms with Alexa and
Google home that is were able to add
custom skills so what does this look
like well if I ask my wife Carrie what
time is it or ask her what's the current
time she knows to respond because I said
her name and she knows what I want to
know based on deciphering what I said
even though I said it two different ways
machines aren't all that different I
might ask Alexa ask amazing app for
today's fun fact amazing app is our
invocation name and today's fun fact is
our utterance to put it a little bit
more clearly
utter insist Oh map too intense in the
same way that people have to decipher
your intent based on
what you say machines do the same thing
so on Amazon's platform we will define
an intent schema which looks something
like this and they might include slots
which would represent variables in what
we say which I'll talk about in a second
in addition we have to define our
utterances so these uh pterence is
basically help again map that what you
say to what you intend and since you can
say things different ways you might
define multiple utterances for the same
intent so what does this actually look
like in a Phoenix application well to
talk about that we might want to
understand first the process and the
flow of this voice data gets sent to
Amazon that gets deciphered and then a
request gets sent that request is
received by our API which generates a
response we then we send that response
to Amazon that response is validated and
then sent to the device so if you win
that and that echo dot tomorrow and want
to learn how to do this stuff definitely
come talk to me so let's look at an
example let's say I have an app that
tells us what the primary colors are so
I asked the app order the primary colors
and I might have a function if I'm using
this Phoenix Alexa package hex package
and I might have a function that looks
like this an intent request that takes
three arguments to the connection the
intent and the request object and I
craft a response which is pretty simple
as you can see and it will give a
response like this back to Amazon this
is what gets sent to the device that
actually puts the speech but what if I
want to do something a little more
interesting what if I want to say what
do I get why mix blue and yellow well
now I've got variables those variables
change the response our function might
change to look like this we still have
the same three arguments but inside that
request
we'll have those intent values to take
it a step further we might want to ask
what do I get when I mix my favorite
color and blue well to do that I need to
know who the user is so by adding an
Oh auth provider to my Phoenix
application I can add data specific to
that user and then leverage that when
crafting a response so great I can do
this in rails I've certainly done it
so why elixir well when you craft a
response the factors might include who
the user is what are the slot values if
this is a long-running session where
it's more than just one command one
response what are those session
variables what are those values and of
course what's the intent
pattern-matching
makes this much easier to determine what
to say based on all the permutations
that might exist for a given intent this
can open the door to things like it
choose your own adventure story where in
having different answers given
throughout a given session might change
the output it might change what how the
story goes on whether or not you took
the sword thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>