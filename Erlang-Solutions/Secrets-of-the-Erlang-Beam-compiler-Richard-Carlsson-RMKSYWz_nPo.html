<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Secrets of the Erlang Beam compiler - Richard Carlsson | Coder Coacher - Coaching Coders</title><meta content="Secrets of the Erlang Beam compiler - Richard Carlsson - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Secrets of the Erlang Beam compiler - Richard Carlsson</b></h2><h5 class="post__date">2017-03-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/RMKSYWz_nPo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so welcome everybody the inspiration for
this talk actually got at the Erling
user conference last year when Simon
Paton Jones decided to talk about the
the Haskell core language and I realized
that perhaps there are a lot of people
in the online community who actually
don't know a lot about what the compiler
is doing inside because our line has its
own core language and so on it's the
sort of thing that if you if you're
working with it for a couple of decades
you you kind of forget that not
everybody else knows this so I thought
it could be a good idea to give a talk
about these things and so I'm not I'm
not trying to direct this at any
particular level of expertise it's for
anybody who's either curious about
what's going on or who's really into
compilers and wants to know all the
strange little details and I don't think
I'll have time to go into too many
details though that's quite large so
what does a compiler do while you're
fencing Oh
once upon a time compilers were fairly
straightforward things they read an
ASCII file or and they'll parse it and
at the same time spit out target code as
assembly operations which were encoded
as machine code bytes and those were
loaded onto the computer and we would
execute them simple but of course there
are complications in a more in a modern
world input isn't necessarily ASCII or
the old days edik so you have to decode
whatever input you have if you have
latin 1 or utf-8 or shift-jis if you're
in japan or something yeah so we get
some abstract characters that you can
work with on your language level and of
course the result shouldn't just be a
sequence of raw machine code bytes you
probably want a link over executable
file something
an exit file or an elf file or something
like that and of course that the
generate the code that you generated
weren't wasn't that good it needs
cleaning up so you have to add an
optimization step there and in order to
do that you have to represent these
instructions internally in some some way
that's good for the stuff that you want
to do with it so you invent an
intermediate machine code on which you
work for a while to do the optimizations
and then you generate the target code
the new realize that perhaps it would be
a good idea to actually identify what's
a string and what's the name and what's
the number and so on before you start
the whole parsing and code generation
loop so your token ice so you turn your
characters into a stream of of tokens of
symbols saying okay this was the
integrator 42 it's not the characters 4
and the characters 2
and then you might think that you want
some macros and conditional compilation
and if you're in the C and C++ world for
instance or in Erlang you'd insert the
pre process step process the step on the
token level before you do the parsing
which can well handle include files for
you expand macros do conditional
compilation and then you probably
realize that it would be nice to
separate the parsing from the code
generation now that's not something that
they wanted really to do in the old days
because computers didn't have a heck of
a lot of memory if you only have a
hundred K or something to work with
you could easily run out of memory just
trying to compile a large program if you
try to put that whole program in memory
at the same time these days that's not
an issue
so we separate the parsing step from the
code generation and we build a syntax
tree that represents the code a tree
like structure and we use that to
generate the code in Erlang we have an
extra step between the parsing and the
code generation where a user can add
hooks that do parse transform to
transform this syntax tree any way they
like before it goes onto the code
generation step and the next thing to
add is to simplify things for the code
generation for the code generator by
doing D sugaring which is you take the
full programming language and you try to
rewrite it into simpler but equivalent
constructs so we remove all the
syntactic sugar if you can leaving only
the the primitives of the language that
means that the code generation step will
be will
have to handle fewer fewer constructs
and that means that there are fewer
places that you can screw up as an as a
compiler writer that's a good thing it's
much easier to prove equivalents on on
the language on the level of the
language itself than further down in the
machine code level once you've
discovered you might want to actually do
some optimizations on this level since
it's as I said easier to prove prove
that you're still following the rules of
the programming language once you move
down into lower level code it's harder
to be sure that you've done the right
thing if you do a transformation on on
this level of the source language then
you can pretty much check the
specification and see if these two
things should be equivalent
so you add optimization both before and
after code generation so that pretty
much concludes the whole compiler chain
from start to finish
and most compilers work in the same way
want modular modular details so one for
one thing the choice of a core language
it could be let you just reduce it to a
subset of your full language but still
it's still that language just a subset
or it could be that it's a actually a
different language but which expresses
your source language in a nice way so
how does this apply to Erlang well let's
see the different ways of running the
Erlang compiler first of all if you're
working interactively in the shell you
can use the the C shortcut you just say
C module name or file name and a bunch
of options if you want and that actually
runs the slightly secret module C colon
which implements the shell shortcut
functions that in its turn compiles the
file and if it worked it also loads the
module but how does it compile it well
it calls the the API function for the
Berlin compiler which is the module
compile so it just calls compile file
gives it the file name and the options
now what that does is that well it runs
the whole compilation and if it works it
returns ok under the name of the module
that it compiled and it spits out a bean
file on the side now the this API
doesn't actually load the module so you
have if you call it this way then you
have to load it manually afterwards
that's why this shortcut exists and if
you want to automate compilation through
make files and other things then you'd
prefer to use the RLC command-line tool
where you can pass flags like - D debug
or - I dot I include and you can add
options like debug info warn unused
importance so the RLC is really just a
small T C programmer that's a wrapper
which runs the the real Earl emulator
and passes on a couple of flags to say
run this module on this function and
pass the rest of the options to it and
that module will check if there's a dr.
extension and in that case call the
regular compile file function so it's
just simple wrapper making things easy
for you so if you have a plus option
that's passed verbatim as an option into
the compiled file call but these
shortcut options like - D in - I are
massaged into into a different + option
because they're slightly hard to write
if you you can write them as plus
options with a bunch of quotes etc
because there they have to be complex or
like terms
and the the RLC rapper is actually can
actually act as a front end for several
different input formats so if you have a
yrl file which is a grammar file for the
for the yak parser generator it will
detect this and will it will call called
the yak module instead to compile that
grammar ok so those are the different
ways of calling the urlan compiler now
there's a specific option that you can
pass to the compile file function and
that is the atom binary and if you do
that the compiler will actually return a
triple so this was the name of the
module I compiled and here's the binary
so it won't save it to disk it will
return it to you as an airline binary
object a bunch of bytes instead of
writing it to disk so then the name of
the option is binary but this actually
works for all levels of output it just
takes wherever it stopped and returns
that to you so if you say compile this
but stop at the core level and it will
happily return the core code to you if
you want to do some stuff with that I
can sometimes sometimes be useful but
the most the most useful usage of this
is to to pass the binary option and
compile it completely to a beam to beam
binary and then directly from your own
code called the code loader and say load
this binary as this module name and say
that this was the file path where I took
it from and then it will load it into
memory that means that you can you can
actually compile stuff and load it
without going to disk ever that's useful
when you when you generate generate code
on the fly
so of course as I said we don't really
know what the encodings are our line
used to require that all input files
were Latin one but that was changed a
couple years ago so nowadays the default
is utf-8 and if you have if you really
want your input files to be something
else than utf-8 then you have to add an
annotation to the top of the file so the
the Erlang parser will actually first of
all check if there's an annotation and
in that case change the decoding to use
that instead so once it's gotten its
stream of characters where you have
forgotten what encoding they actually
were in then we can actually start doing
tokenization and in Erlang the tokenizer
is called else can that's actually a
fairly large ish handwritten tokenizer
so it's not using the leaks tokenizer
generator if you want to look at it it's
kind of it's kind of messy but i think
it's you probably don't want to try to
switch to a leaks implemented version
considering the possibility of backwards
compatibility going bad anyway
so this splits your character stream
into a sequence of tokens that's
integers floats atoms strings keywords
like case or end operators like +
separators like commas and parentheses
so it's actually just you just get a
list of tuples saying okay I saw an atom
at line 1 and the atom was foo and then
I saw an integer line 1 and the value
was 42 and then I saw a case case
keyword or an end keyword or a larger
than operator
so you see that the these have a value
that the sub just are themselves and
there's also a specific token which is
called a dock token it looks like this
that's because Erlang code is separated
into forms forms are snippets of code
ended by a period plus white base and
that that comes from its Prolog roots
and that's also used by lit by the
Erlang shell to know when the input of a
complete statement is complete and if
you don't understand this you will have
a bad time when you try to work with the
the Erlang compiler internals there's
also some extra stuff that you can get
from the tokenizer these days you can
ask it to return also the column number
info and the the original text of the
token you can ask ask it to insert extra
tokens for you to to examine like tokens
for every every piece of white space or
every comment that can be useful if you
err if you're doing an IDE or some kind
and these days you should also know not
much straight on these because this
thing which usually means the line
number if you tell it to return optional
information then this will change to
something else and you shouldn't rely on
that representation anymore so you
should use the new API functions like
all scan column to extract information
from the token that's pretty new stuff
so if you're if you're a couple of
version major versions behind those all
those API functions might not even exist
yet for some reason the column number
info still isn't standard so you won't
get column numbers in your reports yet
anyway I hope that will change in the
future
okay so you have we're done with
tokenization we have a stream of tokens
then what happens is that the epp the
Erlang preprocessor kicks in so that's
the C style token level preprocessor
takes tokens in gives more tokens out it
handles stuff like header files include
an include Lib statements
it handles macro definitions and and
calls of macros which looks like this in
your line does conditional compilation
if diffs it does the concatenation of
adjacent strings also something that c
preprocessor does and it does string
ization of macro arguments which is
something that you rarely used very it
can be useful for debugging so this runs
on your token stream and after that has
happened there are no includes or if
deaths or defines a macro calls left in
this token stream all the code that was
in include files is now part of the
token stream so include files are mainly
used for declarations of like records
types specs at Ritter it's generally a
bad idea to put functions in include
files just to screw things up sooner or
later you have but in order to track
where where your code in include files
that are came from there has been some
new position declarations inserted these
are something they do typically never
see on the code level on your source
code level because they happen down here
so the preprocessor will say that okay
now I've entered food hrl now I've just
left food out hrl and gone back to line
17 of your original source code etc so
it tracks this way I think that's also
something that a typical C preprocessor
would do if you really want to see what
your what your code looks like after
pre-processing actually a bit further
down because it does the just the
parsing also but if you want yeah if you
want to really inspect what's what's
happened to your code after most of
these preliminary things are done you
can pass the upper p an option to the
compiler and then we'll stop and output
a dot p file which is really just a
pretty printed source code dump of what
what your stuff looked like looked like
after immediately after parsing so if
you if you're really lost when you're
working with macros or something like
that then you can use this for free
debugging the next step is the parser
roll parse and that's not handwritten I
think in the the olden days it actually
was but it since a long time back it's
been in implemented as a yekke grammar
and yeah is of course the Erlang
implementation of the Yaak parser
generator so that that is called a parse
dot yrl if you want to dig into it see
how what it's doing so it takes these
tokens as input parses whole forms at a
lot at a time and then returns a syntax
tree which is known as the abstract
format this stuff is documented but it's
fairly message to work with so it looks
something like this
here's a here's a representation of a
function that's been parsed so say that
okay you have the function f at line one
it takes one argument it has a bunch of
choices actually just as one Clause at
line one so I I think I put the line one
everywhere here for simplicity
the first the very the parameters of the
function are the variable X it's just
one parameter there's no guard or it
there's an empty guard and the body is a
tuple consisting of the atom foo and an
operation that says plus of the variable
X and the integrate one it's not rocket
science but it's also slightly hard to
follow which is why I a long time ago
wrote wrote a library called the syntax
tools which gives you real abstract data
type for syntax trees so that's a
superset it works on any abstract format
syntax tree as input and you can insert
comments and user annotations on sub
trees print that out and once you and if
you're done with it you can also revert
it back to the plain abstract format for
giving it back to the compiler to do
some more work on it and there are some
utility functions for easy to Versalles
that try in the syntax tools there are
some other things like a more
flexibility pretty printer there's a
thing called the epp dodger which is
sometimes useful for if you want to
parse the source file without actually
expanding any of these preprocessor
declarations and you can do that by a
lot of trickery but it can't handle too
weird uses of macros and there's also
some stuff for digging out the original
comments from the source code and
inserting them into the parse tree and
that's what the II dr. Lu uses to gather
your inline documentation and associate
that with your with your functions
that's all in syntax tools
another part of that that's more a more
recent addition is something called
Merrill which is metaprogramming
extension that gives you quazy quotes
and meta variables so you can once you
include the Merrill header file which in
its turn defines a couple of macros and
runs the parse transform then you can
write things like queue of the string
that can that represents any Erlang
expression and what you'll get is an
extended Earle syntax a black abstract
syntax tree with whatever that variable
was bound to in your environment
automatically inserted and you can even
do matching in case which is so you can
take if you've got an ast that you got
from somewhere you can match on this
pattern and if it matches to tupple
fukken or something then you'll extract
this into the variable X and get that
that subtree of the syntax tree there
are some other small things like a nice
utility function for for showing an
outline of a syntax tree so this stuff
really simplifies when you when you want
to generate code on the fly or maybe
write a parse transform
so what are parts transforms their
simple hooks you declare them by saying
in your file perhaps saying compiled
with the with the option parse transform
and the name of module and that module
actually acts as a callback so the
compiler will after it's done its
parsing it will run your the parse
transform that you've that you've
written in your module passing it the
the forms that it parsed and the
compiler options if you want to look at
those and then you your you can do
whatever you want with this index tree
and then return it to the compiler which
will proceed with compilation so you can
do simple straightforward things or very
very weird and nasty things Merrill is
sort of a middle ground slightly we are
the nesting and you can you can have
multiple transforms for single modules
that don't nothing preventing that
possibly if there's a if you get into
some kind of ordering issue but most
parse transforms that I've seen are they
don't really care about other possible
transforms
some examples are he will see queries
the known as query list comprehensions
for Manisha
if you use those in you know like that's
a parse transform that changes what what
a list comprehension means if it looks
in a specific way and allows you to use
that syntax to to write compact and
efficient monisha queries the unit
library uses a parse transform for
automatically export your test functions
so you don't have to declare those
manually
okay so once the parsing and the parse
transformations are done there's
something called a lint module called
Erland that's executed and that that
does all the checking of the code at the
syntax tree levels and checks that
that's what handles most of the compiler
warnings and errors that you'll see and
if it returns any errors then the
compiler will stop now if it doesn't
then as far as the rest of the compiler
is concerned the code is good go if you
want you can add a minus W error to the
LC comma command line to turn all
warnings into errors if you want a
completely warning free build okay so
now we've entered the compiler proper
then you get a pre expansion step so you
probably know that record that Airlines
record syntax not maps but records
looking like this it's actually just an
alias for plain tuples with the tag and
that's expanded away by this pre
expansion step there are also some other
things happening like automatic
functions generated the module info
functions are inserted and if you had
any import declarations those functions
are expanded to be fully qualified calls
so the rest of the compiler doesn't see
any any of these complete complications
the records are gone they're just plain
tuples there are no imports
you may know that there's a debug info
option knurling so what that does is
that it adds an extra chunk to the
generated beam file called the abstract
code chunk that just that's just a dump
of the of the abstract format syntax
tree for the whole module at this point
and if you don't like your customers to
see this data but you still want to
provide the debug information in case
you want to go on-site and debug stuff
for them something like that then you
then you can encrypt this no one fun
thing is that it's possible to take to
extract this abstract code and recompile
it regenerate the beam file because all
the external dependencies like included
files macros the records
that's all expanded away you can just
give it back to the compiler and
generate a new beam file possibly after
some fixes if you need after this we go
into corer lang correr lang was inspired
mainly by the standard ml language
specification from 83 where they used a
core set of concepts meaning that the
rest of the the language just becomes
syntactic sugar you reduce your language
down to the to the bare minimum and use
this bare minimum you specify what it
means and then you use this bare minimum
to specify what the rest of the language
means because that's just sugar on top
if you've done this planning ahead so
that you started out with a core
language and use that to define your
your the rest of your language you're
safe but if you have to retrofit the
core language into an existing language
then you have some work to do and that's
what we did so you really need to
rediscover what are the core concepts of
your language imagine even imagine that
you try to pick up Fortran and create a
core language word you'd have to do some
serious thinking what is the core here
so you need to figure out what what are
the core concepts and how do you map all
the existing higher level constructs
down to this core without losing any
detail and it may well be that some of
that detail was not well specified to
begin with so we have to figure out
what's the defect of implementation for
instance we had to figure out what what
are exceptions really about in our line
how do they work not all of that was
specified and it took a while before we
could come up with a design for the for
the exception handling on the core level
so we did this work in about 90 remember
97 98 maybe a bit later somewhere there
about squirrel Angwin went into the the
Orlan compiler as a professor as a new
step because working on the Erlang level
is hard as I said it's nice to work on
on on the level of the source language
because it's more well specified than
working down down there and in the grote
machine code but Erlang has its peculiar
syntax which is quite quite good for
writing programs but for for automatic
manipulation of programs it's much
harder there are patterns pattern
matching and there's clauses in many
different constructs
there are pattern matching in function
heads funds case expressions if
expressions receive expressions try
expressions oh and of course the the
equals match operator and all that all
those instances are slightly different
and if you rewrite one expression on the
Erlang level to another
they might look equivalent at first but
they aren't necessarily completely
equivalent for instance if you think
that you might be able to rewrite an if
into a case but then you'll get slightly
different exceptions if there is no
match so you just changed the behavior
of your program some some people might
think that's never mind I get a slightly
different error but for some people that
might break their test Suites so not
funny
and Erlang's variable scope and bindings
or as I said they're very quite good for
a programmer but when you're trying to
automatically rewrite and manipulate the
code these these scope rules and
bindings are very complicated to follow
and maintain and when you're rewriting
stuff core are lying on the other hand
is simple there are quite a few
constructs plain nested scopes like in
normal lambda calculus all variables and
patterns are considered new variables
you don't import them from their
sounding scopes all this makes it much
easier to manipulate and move move
things around on the core level the only
closest are those in case and receive
expressions and also show you later why
we kept the ones in reseed there's
almost no syntactic sugar a few
alternative ways of expressing things
atoms for example are always single
quoted and there's no fancy list
notation so you can't write list 1 comma
2 comma 3 line rest you have to say the
console of one and the console of 2 and
console of 3 and rest so writing that
manually can be a bit of a bore but this
is meant for machine generation anyway
so it's more important to have have it
very simple and concise you can't have a
normal or nice title comments ok so
let's look at what what are the
constructs in quarreling there are
variables plain they can be plain
variable names or function / IOT it
makes it easier to track which things
are exported and so on
there are literals constant terms like
integers floats atoms we do allow
straight strings as syntactic sugar for
for lists of integers and you can have
lists and tuples at constant sub terms
that's also a constant later literal and
then you have consoles for building non
constant lists there are tuples
for total construction there's support
for binaries and bitstrings on this
level and also recent edition maps
there's not been time to actually write
up the any documentation for how maps
work on the core level so I'm gonna skip
that then after those basics
there's funs function of a number of
variables to a body one particular
peculiarities is that you have multi
value groups so coralline well works on
multiple values any single expression e1
is actually the sequence of that value
there's explicit sequencing you can say
first evaluate this and then evaluate
that and there are let expressions and
here's where you see that you can use
these multiple bindings if you have a
expression here that returns multiple
values you buy you bind those multiple
values to corresponding variables and
use that in the body there are no safety
nets so if you should screw up how many
things are actually produced by this
side and how many variables you bind on
this side you could end up with an
initial any uninitialized variables and
things like that
so that's one reason to not to do things
on the core level unless you're sure
that's not that's what you want to do so
if you're just doing some some simple
transformations generating code then
it's probably better to generate Erlang
code because that will be completely
checked all the way but if you're
implementing your own ml then this is
the way to go
then we have electric construct which
lets you define a number of functions
that can be mutual we usually recursive
or just immediately recursive in a body
now normal Erlang doesn't let you do
this but you can't do it on the core
level which means that you can have
locally defined recursive functions that
aren't visible somewhere else and then
there are modules now a module in correr
Lang it's really electric it's a bunch
of possibly mutually recursive functions
and a keyword module you name the module
you list which which of these are
exported you can list a bunch of
attributes and that's it that's a module
but of course you need to be able to
call functions so there's a syntax for
applying a function oh sorry that should
say it should say apply F and this
should say call M F there's a key word
missing here
but for functional application that's
actually the same for both local
function calls and for any fun there's
no difference in corralling just as just
like there's no difference in in lambda
calculus and if you want to do a remote
call to another module houston's this
syntax then there's four compiler
intrinsics that's the kind of special
functions that you typically need when
you implement something on the low level
you want to express that you have
basically the choice of expressing it as
a call to a magic function with a magic
name or you can say that these are
special calls with a special name space
and we went we went with a special name
space that makes it easier to know what
you're doing
these aren't defined at all that's
completely dependent on what you're
doing in your in your compiler what what
you're entering what intrinsics you
cannot win that you need and then we
have K switching simple case expression
of closes where clauses are they look
like like this you have a number of
patterns so you can match multiple
valued expression against multiple
patterns when guard then body and this
this allows us to have just one case
expression both for the kind of matching
that Erlang does in function heads where
you have a number of arguments that you
switch on or that you have a case
expression where you just have a single
argument that you search all these can
be expressed as core airline multivalued
case switches and then we have receive
expressions and these also use clauses
receive a bunch of clauses for matching
the the incoming message and that's of
course just one value after timeout to
action and so it looks looks very much
like an airline receive and finally we
have try/catch and these are much
bareboat much more barebone on on the
core airline level they say run this if
it works
take the resulting multivalued written
and bind it to these variables and
execute body so it's rate if it works
it's just a let binding if it doesn't
work then forget whatever it produced
and you'll get and get some
exception information in one or more
variables that get bound to to these
variables here and you enter the
exception handler code instead and
exactly how exactly what these mean is
also implementation dependent so that's
an implementation detail of the of the
beam compiler core language doesn't
specify so let's look at why did we keep
receive with with caseware with classes
instead of expanding it well because
receive is really a state machine it's
not a simple expression
you don't just pass through it it's a
state machine so if you have normal
Erlang receive pattern when guard then
body etcetera etc last pattern
Weingarten body or otherwise after
timeout you do some action and we're
done okay what does that mean well if if
we expand it to something equivalent
that shows you what's happening we can
say that does this let's reinterpret the
received keyword and say that that that
really does just the initialization it
sets up the timer if needed
it sets up a message pointer to the
mailbox and and describes the head of
the loop here and then it picks out the
first message looks at that doesn't
remove it from the mailbox yet so we've
reinterpreted what the receive means it
doesn't remove it yet pick it out then
we do the case switching now I've
separated the switching from the receive
statement this is always very simple to
compile and then I do okay switch over
the message and if one of the clause is
matched then I insert a special
primitive operation here before I do the
body that says okay we had a match
select this message remove it from the
mailbox this is the one that we wanted
and then enter the body and drop any
timers
and etc and if we didn't match any of
the patterns then we have a catch-all
course at the end which does a next
frame up instead which will leave the
message in the mailbox and advance the
pointer and go back to the loop head so
we could have decided to to say on the
core lay on the core level that we
wouldn't have any case expressions we
we'd always expand it to this but this
isn't nice to work with if you want if
you want to do some manipulations from
the core level this isn't what you want
to see because you you can you can
probably not do anything with this
before you start to break what's going
on in your receives so this
transformation actually happens much
further down in the stack okay so you
can compile core code to beam code by
adding a from core option it's possible
that the RLC command will actually
identify the dot core here so you don't
have to specify from core but if you're
if you're calling the API function you
have to say from core so you can pick up
an existing core file or syntax tree on
the core level pass it to the beam
compiler and say ok finished compiling
this to beam code it's good idea to add
the sealant option not Clint I guess
because then you'll get some safety
checking if you if you've done some
handwritten core code or generate a core
code some some safety checks but it
doesn't catch all possible problems and
if you've generated broken core code
that'll probably cause the compiler to
fail somewhere further down what's
happening here
once you have core well the compiler
does some things like constant
propagation and constant folding which
is when you have something like a
sequence of say that X is 17 and then
you say that Y is X plus 4 and then you
call F of Y
okay but then we can propagate the value
of this because that's a constant that
makes y equals 17 plus 4 but that's a
constant expressions we can fold that so
Y just becomes 21 great that happens at
compile time there's some extra stuff
like type tracking from thing on for
basic stuff like integrates boolean so
there isn't there's no proper type
analysis happening right now but there
could be if somebody had time to
implement it there's also a neat little
option that not many people know about
it's called inline list functions from
list funks and that tells the compiler
to but but whenever you have a call to
lists map or something like that it will
actually inline the body of a typical
map implementation in your code so you
won't actually make any cold at runtime
to the lists module you just have a
piece of code that doesn't looking for
you in your own module now you can
combine that with additional inlining so
if you say or LC + inline then the
automatic in liner will kick in and that
uses a couple of sites and effort
parameters to decide on its own what
what - inline and whatnot really does a
pretty good job of it
you may want to tune the parameters if
you if you really rely on this there's
also some experimental support for
unrolling so you can say inline and roll
up to level 3 so let's just quickly what
happens after this is that you're
starting to go down to to the beam level
and then to do that they first transform
into something called kernel Erlang
which is an unofficial in from
intermediate format could be changed or
something in the future so don't rely
too much on it but you can compile to
kernel to inspect it for instance it's
simpler because it has no nested
expressions only unique variables it
differentiates between tail calls and
norm
calls it that's where the
pattern-matching closest get compiled
into decision trees and lambda lifting
happens which is when funds are
converted into top-level functions so
after this step there are no funds there
are no electric funds nothing there are
only top-level functions which may have
extra parameters for the captured
information before it goes into Colonel
Erlang there's actually something cool
called destructive set element so yes
there's destructive updates going on in
your code do you know if you use the
record syntax and you update multiple
elements of that record at the same time
then that will generate a sequence of
set element operations sort of insert
insert an element into element three of
the topple then into element five of the
topple then in helmand seven of the
topple and if you did that this naively
each of those operations would copy the
whole topple to the next step changing
just one element but
but this trick means that since the
compiler knows that there can't be any
garb if the compiler knows that the
campi in a garbage collection happening
and this is all just the same couple
being updated again and again then it
can replace everything except the first
operation with destructive operations
that actually just insert a value into
the existing couple so let's quickly
describe the beam abstract machine
it's a register machine it's not a stack
machine like JVM similar to the the old
prologue 1 machine but it have doesn't
have unification or logical variables or
something instead it has what's known as
X registers and Y registers and X
registers are registers so they they
contain one one word Y registers are
slots in the current stack frame so Y
one is the first slot etc it the beam
supports tail recursion so there are six
in there are different operations for
making a non tail call and a last call a
last call will discard the current stack
frame before it jumps that means that
the stack doesn't ever grow and it's
also good to know that the beam
instructions are really kind of a
transport format but pretty much what
you can say about the JVM as well the
Erlang runtime system rewrites them when
it loads them into more specialized
internal operations typically figuring
out if if some things occur very often
together then they can be bunched into a
single operation like that so in order
to generate beam code there are some
things that happening or you jet get
variable lifetime analysis and insert
labels as target for jumps you have to
allocate X registers for from different
variables you have to save those X
registers in stack frames in that's why
registers across function call
standard standard stuff really function
called parameters always use X 1 X 2 etc
and there's operation inserted for
making space on the heap which is also
kind of different from how say you JVM
works so whenever an airline program
needs to create a console or something
just says ok I need two more words let's
check if there's enough space in that
case advance the memory pointer and do
that and if you have source line
information from your original source
code as annotations then that's now
inserted as dummy beam instructions
which later become moved into a table
that map's operations to original source
lines there are some optimizations that
happen down on the beam level typically
dead code elimination and unreachable
code people looking at what might a
stations pretty standard stuff and some
things that I don't have time to go
through after that it's just a question
of validating doing some basic sanity
checking and then map the intermediate
code for for beam operations which
internally looks something like a move
the atom foo to the X register 0 and
that will be remapped to the actual disk
on disk and coding like a byte 64 for
that opcode etc now currently about 150
different op codes in the beam and then
the binary encoding is created with
several different chunks you get of
course the byte code itself but also a
table of all the atoms in your module
other little other constant literals
what attributes you had on the module a
table of all the the entry points your
exported functions the table of that
map's source line object code source
lines compilation information debug
information and some other things
okay I hope that's you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>