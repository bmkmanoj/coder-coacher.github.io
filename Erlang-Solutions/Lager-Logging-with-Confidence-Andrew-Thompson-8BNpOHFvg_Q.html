<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Lager: Logging with Confidence - Andrew Thompson | Coder Coacher - Coaching Coders</title><meta content="Lager: Logging with Confidence - Andrew Thompson - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Lager: Logging with Confidence - Andrew Thompson</b></h2><h5 class="post__date">2013-04-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8BNpOHFvg_Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right yes so I work at Bosch oh I'm
like a senior engineer or something and
I also developed for bansho a library
called lager which is - it's just
intended to you know basically make
logging less painful but you know
logging is a really boring topic nobody
really cares about but it should it
should work when you need it right it it
shouldn't be one of those things that is
always an afterthought you should do it
right once and then you never have to
worry about it again
the other thing about logging is that if
you ship an application to a customer
you know the logs are then a customer
facing part of your application they're
gonna look at the logs and if your logs
are terrible they're not gonna be too
happy with you and obviously it should
be perform well and behave well so let's
take a look at what the default era
logger does it has three log levels it
sort of has three because you know you
get info warning error but warnings
actually default to being error messages
you can set where an error a warning and
aptitude with the plat plus capital W
flag to the VM but it defaults so that
you really only have two error levels
warning or info and error warning
doesn't really exist
it uses the Jenna vent because everybody
uses gen events for logging and it uses
it message their processes notify the
error logger that they have an error
using the asynchronous Jenna vent notify
call there's also a synchronous call
called sync notify but error longer uses
asynchronous notifications and if you
want to do size rotation of your log
files there's a special handler called
log MF h dot girl and you you have to
configure that one separately in your
app config because by default the
default file handler does not include
any rotation at all so what are some
problems with this you know not having
any back pressure in the system
sort of dangerous you can just fire log
messages as fast as you want and you
sure like well it's somebody else's
problem now you know I don't care but in
reality everything lives on the same VM
you can't keep offloading responsibility
forever you eventually have to pay for
your you know your excesses
you can't configure any logger at
runtime at all it's you know it was
configured by the app config and then
you're just going till the VM stops and
then you can reconfigure it when the VM
boots up again another interesting
choice that it makes is that it passes
the format string and the arguments into
the gem event and then does a format per
handler now that's interesting if you
want to do something like log a big term
or you know you're doing the format
possibly two times possibly more
depending on how many handlers you have
installed and also if the term that
you're formatting is really large you're
copying that whole term into the other
processes mailbox which can be bad and
rotation and is really awesome
it'll give you these five files one two
three four five and an index file and we
had a you know I've seen customers be
like well I have five log files so which
is the latest one is it one is it five
and the answer is if depends you know
maybe so maybe the latest index is in
the index file maybe the latest finally
using the X file
well we can't me an X file and it's ctrl
e well that's not very helpful so you
end up having to either look at the size
of the file or the timestamp on the file
to figure that or realize that the index
file actually contains the the latest
file name as a decimal or as a decimal
not an ASCII value so you have to use
like hex dump from the shelves figure
out what the hell your latest file is I
mean it works but it's really not very
intuitive and it doesn't it's not like
any other logging I've ever experienced
so
it confuses the hell out of people who
are used to like you know regular UNIX
logging so the other cool thing about
logging or file rotation is that you
know let's say you are filling up your
drive and your file rotation you know
your log files are ten megabytes and
you're keeping ten mil ten of them and
you know so that's a hundred megabytes
right there
but you realize oh crap I only have six
megabytes left on disk and I don't want
to fill up wherever my logging - because
it'll you know start other stuff
crashing so you know let's just delete
the long file well that would work
except that Erlang never checks that the
file handle it's writing to still
references an existing file so if in
UNIX if you if you open a file delete
the file handle but keep that file you
can keep writing into that file handle
until you close that file and then then
the disk will reclaim it but if you
don't ever close that file handle that
file never actually removes so you all
you are still consuming space on the
drive now a lot of a lot of Damons
support the hub signal like a patching
you if you want to rotate Apaches lives
you just kill that chop it and it will
reopen all of its log files you can't do
that in Erlang so you know you're sort
of in big trouble here you you know if
you have a problem where your logs are
getting too big there's not really
anything you can do other than like kill
era logger or remove the handler which
is sort of a lot of you know a lot a lot
of people who are just running an Erlang
service not only developers aren't gonna
know how to do all this crazy stuff it
shouldn't be this hard but it gets even
better
OTP errors when a gen server gen FSM or
whatever crash is it includes the entire
process state as well as the last
message received so if you're doing
something with a lot of data in a
process like you know transcoding a
video or you know building some enormous
list because you're doing email
processing well congratulations you've
just copied that entire amount of data
into the error loggers mailbox and you
know that's you know if that's a lot of
data or if you have a lot of big things
that are crashing that becomes
very large very fast even better it will
try air lager tries to print whatever
you tell it so if you had a 10 megabyte
binary in your state you know copying it
isn't gonna be too bad cuz it's an off
feat binary but it longer will happily
try to print the entire ten megabytes to
your log file and your console in
practice this will never actually happen
because you'll add a memory of the node
long before it even gets close to being
able to do this but it tries really hard
to do that for you which is pretty good
so era logger is basically just a time
bomb waiting to explode in your node if
you google for it like the internet is
full of people struggling with errorlog
or crashing their nodes and there's
nothing they can do about it because era
logger is part of OTP so if you use OTP
you are sort of on the hook for era
logger unless you like uninstall all the
era log or handlers and even then you're
still copying messages around so I mean
this is really unacceptable behavior for
a logger
I really think this is not what we want
to be doing so we can do better than
this
we can make a lot we can make different
choices and we can do better things so
one of the one of the alternatives
that's out there is react error who was
written by my colleague Scott and what
it does is it replaces error log or
handlers with a custom error log or
handler that will truncate large input
rather than try to print it all so
that's that's a step forward but one of
the problems is it uses its debug flat
size to figure out how big the term it's
going to be printing is and the problem
with that is if you Earths debug flat
size on a on a 64 megabyte binary it
tells you that the binary is like 8 8
bytes long you're like oh wait that's
the size of the reference not the size
of the binary so you can crash or you
can crash react error with large
binaries because it doesn't realize that
those are large binaries and when it
when it detects it's in like big term
mode it sort of falls back to this
really clumsy format that it does so it
the message becomes really hard to read
and there's still no solution
for your mailbox getting enormous or
giant terms being put in your mailbox
so you know a little bit before this
came out I had actually been doing some
had been immune to this problem
independently working on call centers
for a company in upstate New York and
the early call center I wrote had the
same problem we were out of memory and
production with you know gen servers
with lots of stuff in their state
crashing so I wrote something called CP
X log which was built around an async
gen event it installed and it installed
an air hand a logger handler to take
messages out of era logger and put him
into CP X log and I and actually I'm
writing a tokenizer
for the i/o format string format so I
could I could integrate trunk i/o and
format strings in print and truncate at
the same time it got kind of it was
really kind of crazy but it did work and
you could actually have multiple log
files each with different levels that
were being consumed different log levels
so you could everything logs with a log
with only errors and a log with on the
info message isn't it that kind of thing
oh and I forgot to mention air logger
only lets you have one log file open at
a time you can you can't log separately
at all so some more stuff that CPX log
could do it could you know you could
change what levels you want it to see at
runtime it also had a syslog back in
which I actually wrote a port driver to
do and you could actually say I want to
look at debug messages for this
particular module so you know your
systems running at the info level but I
want to debug module foo I could see the
messages I could see in addition I could
see the messages for module fool which
is which was handy for debugging but it
was under a pretty obnoxious license the
C pal which I won't get into how much I
don't like that license but it wasn't my
choice and then I ended up changing jobs
and I went to write jQuery for a year
and I won't get in how much I hate
JavaScript either so then I got hired at
bash I was back to doing Erlang again
and shortly after I was hired it's
actually at the Erlang factory two years
ago a bunch of the support team were
there and they were complaining about
how much they hated Erlang logging so I
showed them the thing I the the CPX logs
stuff I had done and then they sort of
pestered the VP of engineering until he
said yes please write us a new logging
library because the one we have is or
the you know we were using react she was
using react error at the time but it
still wasn't where people wanted it to
be so at that point we you know logger
was you know sort of conceived and the
probably building it began so it sort of
I did a lot of reading on both you know
yeah and all the other alternatives at
the time and I sort of tried to learn
the lessons of history to sort of you
know try to try to try to move the bar
up a little bit and do logging better
than had been done before so longer has
some philosophies that maybe the other
loggers don't share you know for example
logging should never ever crash your
node it should it should not happen it's
ridiculous you should never crash a node
because you're trying to log something
it's insane log should be customer
readable we shouldn't dump you know a
forty line couple to the log and have
the customer panic because it looks like
some scary ridiculous thing when really
some ephemeral process crashed with some
trivial error but you know it makes
people panic you know they they they see
that and it's like I don't even know
what's going on is my VM core dumping or
you know what is this I can't read this
it's just curly braces and yeah bad
rotation should be easy you should be
able to rotate logs just like you do
with Apache or anything else it should
it should just work you don't have to
think about you know caning hex dumping
index files and all that ridiculous
nonsense it should just work and you
should be able to you know have lots of
log levels and be able to pick what you
want to see at run time you know if I
want to see the warning messages if I
only want to see error messages whatever
it should be up to you what you want to
see and you should be able to change
your mind
so you know this is sort of the not
invented here syndrome but I did
actually consider working on one of the
existing ones but I found that a lot of
them didn't really share my philosophy
of how I think logging should work so I
decided that you know I was gonna become
a celebrity
and write it write yet another logging
library for erlang because everybody
else who does it is a big big celebrity
so what is longer have it actually has a
forked trunk i/o and trunk i/o is a
library that you use to say I want to
print this term but I only want to print
47 bytes of this term and even if the
binary is 2 gigabytes long it will only
print the 47 bytes and it won't use up
tons of memory trying to do it
I'll live is actually from OTP it's the
thing that gets called when you call io
format so I sort of took both of those I
heavily modified them and I made IO the
aisle Lib fork use trunk IO so that you
can do like an i/o format with you know
format args format string arguments and
a limit on how big the resulting output
should be and then I quick check the
hell out of it to make sure I got it
right I actually found a couple bugs in
Iowa Lib from OTP interestingly enough
that I think one of them was fixed on
our 16 now so but it's pretty solid it's
pretty fast and I have you know a
boatload of tests for it so it works
pretty good it's actually a pretty handy
little thing by itself I may split it
out of lager at some point
another thing that lager does is it will
at installs an air lager handler and it
sits there and it watches for common OTP
errors like gen event crash jennife SM
crash supervisor doing something you
know gen server crashing it spots those
and it rewrites them from giant tupple
of doom into you know something that
people can read without you know knowing
Erlang it has the seven syslog levels
you
a bunch of them you know people don't
tend to use the higher ones because it
sort of gets a bit melodramatic because
you have like five really dangerous
sounding names but you know they're
there if you need them and one unusual
thing that no other no other Erlang log
logging library uses is that it the the
calls to logging are done
you do like logger info but it's really
a parse transform and what the parse
transform does is it captures a bunch of
call site information for you
module file function or server module
function line you can't actually get
functioned with a macro interestingly
enough but you can with Erlang and get
line you can get current node you can
get the pad you get a couple others you
know as much as we can grab Seine leaf
already for you
it also does some it also wraps the call
and some magic to say you know if you're
not consuming debug messages we're gonna
short-circuit out of that so you don't
even have to do all the the collection
of the data so it's nice to have the
flexibility of the parse transform what
else does it do has the ability to route
log messages to custom destinations
based on the metadata it supports both
internal size and date rotation
I stole the date the date specification
stuff from new syslog from FreeBSD it
exports it supports external rotation
you can move and delete external you can
move or delete loggers log files at any
time and it will it'll notice and it
will reopen the file and keep going it
supports the file console and syslog
backends this aside bakken's a separate
repo but you know it's it's supported
and there's a ton of ones from the
community smtp AMQP log lee and then
ones that i thought were a troll
originally but they turned out not to be
the CouchDB and Mongo DB backends I
thought I was being trolled but not so
here's an example of you know what you
would get before and then what logger
rewrites it into so you know gem server
come with reason no match of right hand
valium you know empty tupple in the
crash module in the function handle call
with the era T of three online 55 as
opposed to this which you know I don't
care about a lot of this and it tells me
you know yeah I think this is better you
may disagree but I think this is better
and then we have a tracing example if I
want to get all the debug messages for
the flux capacitor module and send it to
the console I just do longer trace
console and tell it the module until if
the debug level same thing I can trace
to a brand new file if I wanted to bug
that module to a file you know I do the
same thing you can actually specify
multiple things in that prompt list if
you want to trace on module and function
or module and pid' or module and node or
you know any of the metadata you can
actually supply custom metadata so you
can do things like put request IDs and
the metadata and then like be like I
want request I use seventeen to go to
this log file if I wanted to bug request
ID seventeen and you can actually trace
to existing log files too you can send
messages you can extend what that what
that log file is writing by pointing
traces at it so we're sort of gonna have
a Erlang log in the library shoot out
now and the containers here are our
logger the you know the reigning
champion log Perl which is inspired by
log4j which i guess is some java logging
library i don't know i don't know
anything about java so I guess it's big
cool I don't know
a logger which was written for the first
spawn fest was actually written
concurrently with logger but they didn't
announce it and I didn't announce it and
we both released at the same almost at
the same time we're like oh you know
that's kind of weird but whatever sort
of a weird duplication or effort but
and then we have elog from the
inaccurate people and then a lager from
retail which is like a blast from the
past somebody pulled it out of Jung girl
and like resurrected it so I was like oh
it's there I might as well try it and
then fashion law biopsy code there were
actually a lot more candidates out there
but I sort of ignored the ones that had
no to commit in two years or more so
those were the ones that seemed like
relatively credible so we're gonna
quickly cover these I guess we still got
time yeah we're doing okay although
we're behind whatever so nocturnal is a
synchronous gentlement it has an airlock
handler that redirects into the log for
all it uses the sort of log4j upendra
formatter design you know lots of
verbosity lots of modules that seem
unnecessary it has lots of back damage
console file SMTP XML syslog you know
some more it has five log levels not
just like seven I think it you know
decided didn't need quite as many
hysterical log levels and you can change
them on the fly by recompiling a module
and it will you know use the it
recompile as a function in a module that
will then accept the deny messages based
on the log level it's kind of a longer
is a synchronous Jennifer which is quite
an unusual choice and basically the
James River has handler modules
installed it also has an air longer
handler so again air logger messages can
go into a logger
it has file console syslog inscribed
back ends the handler modules aren't
either asynchronous or synchronous it's
sort of up to the handler module so the
file module by example is asynchronous
but the console one is synchronous it's
fine it has something called flows which
are very similar to loggers traces I
actually stole the idea from from a
logger
because they liked it so that's where it
came from it's not identical but I sort
of stole the concept and it also will
recompile itself when you change the
configuration fast log
it's an ancient Finnish game server no
Aaron lager provider for this one it it
will only write to a synchronous disc
lock that's all it does one nice thing
it does do that
to its credit is the only one that it
does this is it has overload protection
if the Jenna vent mailbox is larger than
five thousand messages it will not send
to the mailbox to try to help with
overload conditions there's zero
documentation but I was able to read the
code and figure it out so it was pretty
pretty straightforward to get working
elog this one really kind of strange it
has a registered process per log level
all the logging is asynchronous and I
couldn't really figure out how it worked
it has a back-end per log level but also
a default one and I was sort of
struggling with this I managed to
finally figure out the magic to get it
configured but it wasn't clear so I may
have screwed this one up I don't know
somebody knows how to use this thing you
know
I can send me a patch I've had to use it
right because I sort of did the best I
could
oddly also it uses it uses react error
but it doesn't use react error to
truncate large input it just uses it to
test if it's like a sazzle message so I
was a little confused at that but okay
hey whatever ie longer the blasts from
the past uses a dis log back in for air
logger it doesn't have an actual logger
it is purely a back-end for existing
error longer it uses some undocumented
dish blog api's that are apparently they
wrote just for it I don't know it
synchronizes the disk log per message
and it's not a real OTP application this
one was really infuriating it will read
its config from the current application
it's not like you know application get
env sazzle key it reads it out of the
current application so trying to
benchmark this thing when you don't have
a current application is a freaking
nightmare I had to do all sorts of
horrible hacks to pretend I had a real
application I think I pretend I put the
keys in the kernel config and like
change the group leader to be the kernel
group leader so that the app config
awful awful don't do that
terrible so there's a repo on github
called vlog bench under my personal
account it's a log it's a it's a
benchmark it's a it's a you know
framework for benchmarking log libraries
so it supports all the ones I just
covered including a fork of error logger
that uses synchronous notification
rather than a synchronous notification
and it has a few different size of
messages that can generate so there's
like small simple small large huge and
enormous but huge and enormous are so
bad that we don't we're not actually
going to cover them because it you know
just take days to run the benchmark the
number of workers is configurable so you
can say I want one one process to log as
fast as I can or I want four process the
lock is log as fast as you can there's
no rate limiting right now this is
purely a benchmark of how fast can we
log messages
alright so each benchmark each benchmark
for each library has three phases the
set up the run phase in the finish phase
the finish phages phase is basically to
ensure that the logger the logging
library actually did what it said it did
make sure that it's actually processed
everything and done everything so we
don't you know if you just fire
everything into a mailbox in another
process and say I'm done you know you're
sort of cheating because you haven't
actually done the logging you just sort
of passed it off to somebody else so
there's a little bit of checking that's
done to make sure that you know you
actually do what you say you're doing
results are in operating operations per
second I just take the number of you
know I just divide one by the other and
get you know how many log messages we
wrote per second it includes the time
for finishing and for each benchmark I
did three runs and I did a I selected
the median value so I had to disable the
fast log large mailbox check because it
was kicking ass in the benchmark and I
couldn't figure out why and I was like
oh wait it's not logging you know half
of the messages because it's something
throwing them away because their mailbox
is too large so I had to turn that off
and pour fast log went from being the
fastest logging library not to being so
fast so I feel bad they didn't you know
they they did a good job with that
that's a good choice to make if you
don't care about dropping log messages
if you do not so good and
you know to be fair none of the other
ones do it so I felt in the interest of
fairness I would turn that off a locker
is interesting because it's asynchronous
like all the way down so the finished
step will actually call it called
dislodge sink to make sure that we
actually sink the dis log and don't sort
of cheat by having a crap in the buffer
so you know so here's the first pinch
mark you can see that a lager dominates
it's the purple one if you can't see the
legend this is regular air lager this is
the synchronous error lager which you
can see the the penalty of each tank
versus synchronous because a Cygnus is a
hell of a lot faster this is a log this
is a lager this is a lager
this is log this is vlog for Earl and
this is my personal favourite lager
which is not the fastest it's tied right
now with the synchronous air actually
it's behind a finger in the air lager
it's the fourth fastest so you know I
wouldn't you know that's not great um
here's some slightly larger messages and
it's about the same nothing really
changes the overall food plug goes down
but you no longer caught up a little bit
to the synchronous air lager but that's
about it
so a lager is really fast cuz it's got a
completely asynchronous pipeline which
has its ups and it's downs may be
getting large mailboxes they're bad
though compared air lager to the
synchronous air lager and you can see
that you know the VM will actually
penalize you if you send a message to a
mail of process with a large mailbox you
get the scheduled if you do that so I
suspect why that penalty is there is
that when the mailbox gets enormous that
poor worker is getting be scheduled
constantly cos all it's doing is sending
messages to that to the to error logic
and the reason why we're slow is it's
always checking for rotation all the
time every time you write a message it
checks that you didn't rotate the file
out from under it so that that comes at
a price and a longer constantly thinking
the disk is really expensive and I don't
know what elog is doing what
I don't know Elon is just a enigma to me
I don't understand it so this is with
10,000 large messages one worker is
writing I forgot to mention the last
benchmarks were 10,000 messages one
worker so now the field has been a
little bit changed here the overall
throughput is much lower only at about
400 I met operat operations a second
here
but the only one doing more you know the
only two doing more than 100 messages
affected our boat strangely a lager and
lager all the other ones are literally
doing tens of messages a second which is
the static but that's what they're doing
you know so that's when that happens on
your note you are in trouble so and that
message I was only 80 kilobytes it was
an 80 kilobyte binary that's all it was
and that caused that level of collapse
on all those logging libraries the only
reason a logger survives is it has a it
has a check for large binaries and it
truncates large binary it's the only
reason it manages to survive if you use
a string it gets really bad because you
you avoid the large heap or because
you're not passing a reference to a
binary you're passing the actual string
and E lager can't survive it the only
one like blogger is the only one that I
think the others even Oh a lot of bunch
of them out of memory on that test
loggers the only one that can handle it
so now we're going to charge for workers
with four workers a logger just goes
through the frickin roof I mean it
really pushes those log files and you
know that's that's good actually loggers
now number two somehow I don't know why
but I guess it's a little more parallel
than the others it formats it does the
format in the profit in the calling
process a lot of them pass the format
string arguments to the logging process
and so you lose parallel operations
there so maybe that's why pretty much
the same graph with the small size
rather than the simple size yeah a
logger is very parallel very
asynchronous so it it wins big
but now with the large pile and for
workers you no longer does four times
better than the last test but both a
lager and fast lager out of memory of
the node so a lager is great until it's
not great and then your notice Crouch
younger or youngest elimination is to
sort of chug along the little you know
lager
I guess but it's the only one that's you
know even a credible competitor and it's
not you look you up a longer interesting
it always lawns at about 180 messages a
second no matter what you're doing it's
always 180 messages a second which I
don't know why I guess it's the
synchronous call it does but it's pretty
much constant but at least it doesn't
have a memory that's better than out of
memory I guess so yeah logging is hard
so let's talk about longer tooth I
tagged the art scene earlier this
morning actually so what's new the
metadata is now passed into the back end
so the back end can do magical stuff
with the logger metadata as before it
was just sort of it was provided a big
string and said this is your formatting
here you go so now you can actually do
things like you know have a bank and
that logs the metadata in a way that you
would prefer the aspects there is
because it's a backwards incompatible
change if you have a custom back in for
logger you're gonna have to rewrite it
to support logger to that oh because I
changed the backend API completely we
switched from mochi mochi global to ants
to store the log config stuff so we went
from one of it being one of the
recompile modules and we load them on
the fly kind of applications to just
using ants because it actually ended up
being faster and I had less awful code
to deal with because monkey motion
global is the scariest scary thing
I also added syslog comparison flags so
you can do stuff like log levels left an
info you know log levels that aren't
warning you know read the syslog
documentation if you're curious and how
those were
they can be handy they're just sort of
I'm just playing around it will it has
support now for web machine and cowboy
error messages it will rewrite them to
be like OTP messages it'll make pretty
pretty versions of it only works for
cowboy zero point and above I got a
passion to cowboy to make the error
message is consistent so we could catch
them in logger and reformat them you can
actually store a logger will now capture
the application a module belongs to at
compilation time if it can as well as
allow you to store metadata for a
process in the process dictionary so you
can if you have a process is gonna run
for a long time you can cram all the
interesting you know custom metadata in
just in just a process dictionary key
and every time you log you don't have to
pass that metadata around anymore it's
just part of it it's captured as part of
the parse transform it also supports
printing records with like minimal work
so basically all you have to do is you
have to use logger PR for print record
you give it the record and you give it
the module the record originated in or
where a module that that record knows
about that was compiled with the longer
parts transform and it will actually
print pretty print you know what the
hash and the curly braces and the commas
and you know it'll print your record in
a legible way rather than just giving
you a couple a tag couple with twenty
fields and you have to go and sit with
the terminal you know vim on the other
terminal and feel like well the sixth
field is this and the eighth field is
this this will actually figure out tell
you what field is Waddell so that's nice
it it's more magic in the parse
transform it will notice record
definitions and stash them in the
application or the module tribute so as
long as you know where the module came
from you can now pretty print it
assuming it was compiled with logger so
I think that's nice
console colors fellow named Ed's in
patched are 60 or Erlang to allow and to
not strip and see escape sequences so
that you can now do color in the are
sixteen shell and then he patch logger
to support it so you can actually set up
a custom this is just the one he did but
you can actually configure what you want
your colors to look like and
you know if you want to have a pretty
shell now you can so that's nice and
let's look at the performance of logger
by version so basically on the small one
worker test 10,000 objects or 10,000
messages we've more than doubled their
performance it's nice right so how did I
do that it was some kind of interesting
trickery involved so I changed how often
we check for external rotation to be
instead of every single write by default
now it's a second you can specify the
time in in micro milliseconds or you can
you know always do it still if you want
but you know so worst case you lose a
second of messages before rotation kicks
in or it reopens the file as long as
you're not deleting the files you're
just moving them that's fine it'll just
right for that old file for a couple for
a second and then open the new one so it
seems like a reasonable compromise you
can set it back to the old way if you're
wicked paranoid I don't know also an
interesting trick that I did was to
install a custom handler into the logger
gen event that looks at it every time it
every time logger gets an event it looks
at its own mailbox it doesn't consume
the log message doesn't care about the
log message just looks at the mailbox
and depending on the mailbox size it
tells everybody else either to
synchronously or asynchronously
notifying out of messages it switches
between a sink and sink modes depending
on load so that we can be fast when you
know we're not backed up but as soon as
we start to get backed up we lock
everything down and say you've got to
start being synchronous now so that we
limit the size of the mailbox so let's
do all the charts again I guess we'll go
fast this time so logger kind of wins a
little bit now that the red one is is
the new logger like larger 2.0 the green
one is the old logger so the colors are
all the same I just added one more Tom
so you know we sort of win there this is
sort of the only case you really win by
such a ridiculous margin I don't know
why it's quite so fast in this case but
it is we still win on the simple simple
size but not nearly as much and we went
on the large size still so we still
handle large file large messages without
for workers a logger pulls ahead because
a synchronous is still going to be
faster and they can you know you can
they can still sort of push the trouble
down the road more than I can same thing
it's not a massive one anymore so you
mean like look at look at the old green
one compared to in there where we are
now so you know we're more than twice as
fast bill and we wind still up the large
size so that's great but area logger is
still gonna try to kill you because you
can still get your mail your era log our
mailbox become enormous when era logger
decides it's going to air you know storm
your storm your node with errors so this
is still a problem
Fred Hebert wrote a really nice little
app between he and I we sort of worked
on it together called cascading failures
that emulates a failure he was seeing an
ad year where you know hate a lot of
processes referencing an X table and
that F table would die and all those
processes would crash be restarted the
the tape the the process owning the at
the table would also be restarted then
that stable would crank that process
will crash again and all the workers
will crash again so it sort of emulates
the sort of cascading failures you can
see in production in a line node so
let's look at what error logger does
versus what longer does when you run
that against the node so the top one is
error logger and the scales are
different on this so you can see that
area longer tops out at 12 gigabytes
whereas longer pretty much hovers the
peaks at 700 megabytes but it sort of
hovers in the you know well below that
sort of you know five six hundred
megabytes in memory I wrote like a
little hacked up ERD
sort of thing to monitor memory on the
node but it's pretty obvious that you
know that is bad and this is not as bad
so this is for fifteen fifteen minutes I
gave up because I was like you know I
only have eight actual Meg about giving
about to RAM on this machine and I think
twelve is excessive to show you know
it's a sufficient to show the problem
here so another thing that logger to
that
that has a high watermark in the area of
lager handler installs if you basically
say I want to I won't I only want to
accept X amount of messages per second
from err lager and if you get more than
that I'm gonna start throwing them away
until the next second hits and a log
company I threw away and reset the
counter and keep going
so usually what happens is is when you
get these sort of area log of storms
they're all the same error over and over
and over and over again because you know
a lot of processes are having the same
problem or whatever so it's better to
capture a representative sample and not
crash a note if you don't if you don't
have to maybe you'll survive the storm
rather than for sure out of memory in
the machine so you know this is sort of
an it's optional you can do whatever you
want but it it lets you at least say you
know I'm gonna I'm gonna try to throw
away messages to be able to survive this
this event you know for like if you have
a temporary outage you'll be able to
survive it rather than crash the node so
it's still not perfect
you still can't prevent OTP sending
giant messages to era logger you can use
the format status callback for all of
the gen of Gen whatever behaviors it's
not very common but you can tell format
status to trunk it to filter the state
that's passed to era logger as part of
the log message another thing you can do
is when sending large messages around if
it's going to a process that might crash
or just in general if you use a binary
rather than a list copying that message
around will be a lot cheaper because
it'll be an off heat binary so you know
that that's another thing you can do to
sort of prevent these things from
happening so quick quick wrap up here
productions of logger it's been around
since one that it was a time now ship to
that I believe where Fred Fred used to
work and your uses it there's much more
I don't
I can tell where I can mention but I
know lots of people seem to be using it
now so it's actually it's up to the 19th
most watch repository on github and it's
you know been development now for almost
two years
two years this summer so it's it's you
know it's mature I think or definitely
maturing this graph is a little hard for
you guys to read I think this is
actually from Fred but it shows what
happens when the error longer think the
error log of Q spikes up to 12,000
messages the node just goes away right
missus like this is like what the Green
Line is good but the Green Line you know
flat lines until the no one finally
comes back and stuff happens again it's
like it's a production outage basically
that that's whatever log you're provided
there's they gave me a free production
holiday or whatever so this is the same
sort of graph but you can see that even
this is old this is longer 1.2 or before
so even though the the mailbox ID is
still spiking it's purple now but not
not the same color as last time
even the message thing spikes the twenty
thousand messages logger actually
survived this and the system kept going
and they were able to debug it one of
the other problems with errorlog or is
it if it crashes because the mailbox
gets too big
you leaves all those messages and a lot
of time the actual problem is somewhere
hidden in your mailbox and by just
losing absolutely everything you have no
debugging so they were actually able to
figure out what their problem was once
they switched to longer because they
couldn't debug with error longer because
it would just fry the node alright so
that's about it I just like to thank a
couple people Stephen ASCII sort of
helped me design this originally and we
sort of Skyped for a while and bounced a
bunch of crazy ideas around most of
which actually ended up happening it was
his idea to do the parse transform so if
you don't like that I guess you can sort
of blame him - Fred Herbert um helped a
lot with you know telling me about you
know his production experience with with
logger and ideas to make it better
he wrote a cool sim hash deduplication
and stuff that we played with but it
ended up being
finicky to configure that it was easier
just to do the high watermark thing
everybody bashed over you know putting
up with my my incessant pull requests
and forgiving me some really interesting
ideas like the record printing was
suggested by a couple people at bash
show because they got mad because they
couldn't read the records in the log
files so I was like hell that I hacked
it in an afternoon so and everybody else
who's used it contributed to it filed a
bug complained a lot of the improvements
have been driven by people not a
bachelor using it so I appreciate you
know everybody who gave me feedback and
that's it I don't know if you have any
type of questions given we're so behind
but
yes so this question about if you you
know is the instead of blindly
discarding messages from the mailbox if
you sort of figure out if ones are
similar and and say you know I had the
same message five times that's what
that's what Fred's dedupe work did the
problem with it was a it's expensive and
at that point you can't really afford to
do expensive operations and be trying to
traverse you know you know it's it's
hard to you know you really want to get
rid of that mailbox as soon as possible
when it's you know it cuz it will grow
very very quickly at that point you
really and you know if using for lager
you're using lager for everything else
the only messages you're getting from
OTP are from gen events and gen episomes
and stuff crashing so usually those
messages are very similar and those are
the only ones that are that the drop is
applied to all the log or all the
messages from lager are not subject to
that dropped it's only messages coming
in to err lager so it's a trade off
you could lose valuable information
there but on the other hand it's not
reason it's not very easy to do anything
better right now we played with it a
bunch and this was sort of our our
compromise for now maybe we'll come back
to it later but</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>