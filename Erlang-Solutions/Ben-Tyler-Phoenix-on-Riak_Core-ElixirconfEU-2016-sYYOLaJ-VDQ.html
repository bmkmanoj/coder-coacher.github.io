<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Ben Tyler - (...) Phoenix on Riak_Core (ElixirconfEU 2016) | Coder Coacher - Coaching Coders</title><meta content="Ben Tyler - (...) Phoenix on Riak_Core (ElixirconfEU 2016) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Ben Tyler - (...) Phoenix on Riak_Core (ElixirconfEU 2016)</b></h2><h5 class="post__date">2016-05-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/sYYOLaJ-VDQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay hi everybody
my name is Ben I work at booking I am
very excited to be here to chat with you
about two very cool pieces of technology
Phoenix and react core so first off some
context this is not this is not a talk
where I go here's this amazing thing
I've done in production you should all
do it it scales is so cool this is
totally just me being really excited
about some cool technology and how we
can use them in cool ways so that that's
it just to set the expectations straight
talk notes and code are going to be
available there's gonna be a ton of
content so don't feel like you have to
write everything down it'll be there and
I would be delighted to talk about this
stuff after the conference if anyone's
interested in kind of chatting more so
okay today we're gonna build a stateful
distributed fault tolerant real-time
impress your cat web application so
we're not there yet we just we just
listed them okay so first of all let's
talk about stateful so what is a
stateful web application well it's
pretty straightforward it's just any web
application it has some memory that we
used in one request and then use it
again another request it's pretty simple
but that's actually the weird thing to
do the much more typical architecture is
a stateless architecture so let's go
over that real quick so we're on the
same page so in a state lick list
architecture in the beginning there was
the internet and then we have some users
come in they hit a load balancer they're
randomly distributed over some app
servers usually running some you know
nginx and some application thing and
then those application servers hit a web
hit a database for their data make query
up you go
so this architecture has a bunch of
really great advantages you know one of
them is that it's super horizontally
scalable we can just add more web
servers if we need more capacity and
that's really effective we know exactly
where to find all of our data there's
one place we can do reports over our
whole data set it's really easy SQL is
the bomb and that's really helpful our
load balancing scheme is super
straightforward we can randomly
distribute we can round-robin our load
balancer doesn't need any knowledge of
how our application is constructed and
that's really good but there is there is
one kind of slight drawback to this
architecture which is we don't do a
great job of utilizing the memory
on our application servers by and large
usually my experience our application
servers are have enough memory for our
application code and then whatever data
the user needs in a given requests but
not so much for you know things we might
use from between requests and who is
that even a problem like you visited is
it okay that we're wasting all this
memory well let's look at that so this
table is usually labeled latency numbers
every programmer should know I don't
feel that strongly about it but it is
interesting so this has been changed a
little bit
so it's this github user Kofi Mon scaled
it up so that the fastest operation is
one second a CPU cache reference and
everything has been scaled accordingly
so a main memory reference is three and
a half minutes okay fine you know
reading a megabyte sequentially from
memory is six days that sounds like a
lot more but I'm a scripting language
programmer like if my code is
bottlenecks by reading data sequentially
out of memory I'm like yeah my code is
fast you know that's awesome okay
so doing a network round-trip in the
same data center as eleven and a half
days like okay that's fine and then
reading a megabyte sequentially from
disk is suddenly whoa like four hundred
and sixty think that's so long that's
forever
and so as high-level language
programmers there's not a lot that we
can do to manage our like our cache hit
rate or main memory reference you know
how much often we hit that but we might
be able to change our architecture to do
more reading for memory and do more
round trips even if it involves doing
more round trips that is so this is kind
of wash as I was talking about with Moz
where they moved away from having a data
base entirely and they kept the whole
data set in memory so that's what the
kind of thing we're gonna talk about and
so how does that work trying to leverage
memory in a stateless architecture well
the thing about this layout is that
basically the memory that we can use
really easily waves in the data based
here we can add caching like register
memcache but by and large the thing
that's totally simple if I want to have
more of my data in memory I buy a bigger
faster better database server right and
so given this constraint why is it that
our applications are almost always
structured this way like almost every
web application typically is a stateless
architecture
and so one reason is our workload you
know stateless protocol stateless
architecture kind of checks out right
but I would argue there's there's more
of a historical like a tool set
constraint on why we do it this way and
it has to do with the scope of memory
reuse in our applications in our web
applications and so one of those is our
web at our programs are short-lived you
know back in the day it was CGI it only
lasted for the length of a single
request these day we these days we have
workers but still the life time is kind
of arbitrary they usually terminate
after a particular number of requests
and from others perspective that's
basically a random termination even when
we do have long-lived programs like
we're using an event loop nodejs
event machine any event something like
that are the scope of memory reuse is
still limited to a single core you know
event loops typically are single
threaded and then finally even if we do
something to coordinate between our
different threads so that we have memory
used on a single server coordinating
between servers is really challenging
well you know like maybe you could do
something funky with the 0im queue or
something but that's very non-standard
and so how much do these constraints
apply to us building applications using
elixir in Phoenix well so the first
thing with their workload like yeah we
do a lot of HTTP but browsers have
WebSockets now and we have channels and
so maybe more of our workload is
shifting in this state full direction
like okay in terms of memory reuse
short-lived programs well I mean I'm
pretty sure they're Erling programs that
have been running in for 10 years you
know I wouldn't be surprised so that's
not really a thing we're using the
Erlang VM single-threaded program same
thing we're using the Erlang vm it does
that for us
and then finally the kind of most
magical thing about the Erlang VM the
distribution it just we can coordinate
between servers kind of transparently so
great
let's do it let's build a stateful web
application right so okay so what does
this look like we have two physical
servers not gem servers right and
they're running Phoenix or something and
maybe we want to store some data on them
so we're gonna use some ETS tables just
keep the data in memory and so let's say
we put data for user 42 on server B and
we don't have anything on server area
but so user 42 shows up they want to use
our application now we have a decision
to
we could do some kind of complicated
tricky load balancing like stateful like
sticky and attach them to server B or
our load balancer could be magically of
aware of our data layout but that's
complicated and we're using a lick sir
so we'll route them to server a and just
have them ask server B for the data
server a we'll look for it say I don't
have it like okay so for B we can send
to the other two app it on server B
that's easy we can do gen server call or
cast if we're feeling OTP ish and I'm
done so I have a sleep stage distributed
stateful web application I have a lot of
time left but but maybe not right I'm so
when we think about how this approach is
gonna grow we start looking at something
like this this kind of terrifying spider
web of logical connections like this
isn't the mesh that the Erlang p.m.
generates this is just servers
requesting things to each other and we
find ourselves firmly in the realm of
distributed systems this is uh and this
is kind of a scary world but we're gonna
go there so that's what we're what we're
talking about with a stateful web
application storing some data on the
node but it turns out coordinating state
between different servers is really
challenging so let's look at how we can
distribute it so here's what I say about
distributed systems when it comes to you
know to cross node sending or Jen server
call our cast of to remote kids they are
the equivalent and distributed systems
of print div in web development that is
they are absolutely foundational you
have to have them in order to have a web
application but it's a it's the very
base thing right and so ad building a
distributed system out of ad hoc message
passing or RPC is like writing a web
application that kind of goes like this
right now my SQL query my cool query and
stuff it in some HTML and echo it out
we're done yeah right and so this I
don't want to say this PHP is bad
because we have all written this code
and moreover it has generated fantastic
amounts of value that's why there's all
the dollar signs in the language right
like this
so so this generates value it solves
problems just like ad hoc
message-passing but at the same time we
we have as web developers kind of
converged on a more structured way of
building our applications we've
converged on model-view-controller and
so that kind of brings us to a question
what is the model-view-controller
equivalent when we're talking about
designing a distributed system and so
okay you think MVC I've heard another
three-letter acronym when it comes to
distribute systems cap see ap
consistency availability of partition
tolerance and so I'll define them real
quick consistency if I ask two servers a
question do they give me the same answer
it is one of them out of date or is it
diverged or something availability if I
find a server that's working and I ask
it a question is it going to give me a
good answer or let's say sorry my
friends are unhappy right now I can't
help you and partition tolerance which
is how well I can deal with network
partitions so okay and then the general
formulation for when we're using this
thing to talking about our trade-offs is
pictu okay fine but when we're
approaching this as a way to design or
structure our distributed system it is
is not helpful right this doesn't help
us design something it doesn't tell us
how to structure our communication so
but that's ok that's not really what cap
is about cap is about having a
discussion about trade-offs and our
trade-offs kind of fall into these
different buckets where we pick two and
so you've got consistency and partition
tolerance there's it's kind of
questionable if you actually can have
consistency availability because there's
always going to be network partitions
but today we're gonna talk about things
that fall into this bucket into the the
availability and partition tolerant kind
of end of things and we're going to talk
about concrete patterns that could be
classified this way and so what are some
of those patterns
well there's gossip protocols so like um
server B and I talk to server a and I go
I've got user 42s data if you if you
telling your friends that they I have
the data so if they need it they should
come to me right and then your
information kind of spreads virally
conflict-free replicated data types like
Chris talked about with Phoenix and
presents those are really cool and then
there's this other thing distributed
hash tables and they're they're a
mechanism where you can take a big body
of data and split it into smaller chunks
and have different chunks on different
servers well that sounds pretty good if
what we want to do is take a big data
set and keep it
parts of it in memory around our whole
cluster like that's kind of promising so
great let's do it let's build a stateful
distributed web application using a
distributed hash table so the first
thing we should do is not reinvent rails
not reinvent Phoenix but find some
implementation of this pattern that we
can leverage to build our application
we're going to make fewer mistakes it's
going to be a better experience and so
that brings us to react core and so
react core the thing on the readme
it's a toolkit for building distributed
scalable home tolerant applications okay
fine all right let me let me lay down
some hype it is mind blowing advanced
technology okay this is really cool
stuff and it basically doesn't have any
peer and other platforms in terms of
maturity in terms of the features it
offers it's really cool like it's
amazing
and we can use it because we're elixir
program this is an airline thing like
this is cool okay so let's talk about
how it works so this is the slide you
see in every single like I had to do
this I'm talking about reactor you see
the ring right and this is you know
different colors different servers but
this is kind of overwhelming at first so
let's back off let's talk about a plain
old hash table like in Ruby like in Perl
like in Python right and so hash tables
you all probably know they're backed by
an array let's say our array has four
slots they're usually called buckets
when it comes to hash tables and so when
we want to put something into our hash
table assign 42 to the key answer we
hash our key and we get some number it
doesn't matter what the number is really
just some number and we take that number
and we run some transformation on it so
it will fit into one of our buckets in
this case the transformation is the
length of our underlying array and then
okay now we know where to put our thing
we put answer in index number three
great so a distributed hash table is
very very similar same exact operations
except different servers own different
buckets and so in our case server B owns
the last one and so if we need to look
for this data we know we need to go to
server be cool so let's look at this
slide again where and so the the main
thing is just a question of vocabulary
when react says Vinodh or partitioned
what they're really talking about is a
sequence of buckets that are all in a
row
in our underlying array and we've taken
our array we've kind of wrapped it
around into a circle but that's it and
so when it says a single V no that just
means responsibility by an Erlang
process for a series of buckets we're
gonna put stuff in those buckets and
when we hash to a certain value it goes
to that V node so that's it so let's
build an application using react core
and elixir just to get a feel for things
so we're gonna build something really
simple we're just gonna send ping
requests to different parts of the node
different parts of the cluster excuse me
and so ping pong that's it that's the
whole thing we're gonna hash the
timestamp so that every time we ping
we're gonna get a different hash value
and it's going to go somewhere else in
our cluster okay and the destination is
gonna be a V node or one of those hash
buckets in our kind of underlying array
and so this might sound like kind of a
trivial example but really it's a good
example of distributing CPU work around
our cluster so imagine I have this this
cluster and it can do different jobs and
I want to have certain workers handle
certain jobs while a V node could be
responsible for one class of job so what
are the parts in a react core
application there's a service which is
kind of your high-level API this is
where how people are gonna interact with
your application you have a V node where
you implement your business logic and so
like I said a V node is just a hash
bucket with some code attached that's
the whole thing there's a supervisor in
an application but those are very
typical for Erlang and Aleks your
application so we're not gonna look at
those too closely today so let's first
look at what goes into a service so
here's some code our service is just a
module that's easy we're going to define
one public facing function ping so what
are we gonna do in ping well the first
thing we're gonna do is just like a
normal hash table we're gonna hash
something and we're gonna get a number
back out and we're gonna call that
number our document index that's just
gonna tell us where it goes in our hash
table the thing we're gonna hash is
gonna be timestamp just like I said so
it goes to a different place almost
every time we run this command because
time is ever-increasing okay and then
the library we're gonna use is gonna be
a react core thing but that's okay it's
hashing something we get a number back
and we we send this whole tupple but
that's not actually that important so
now we have a
friends list and so remember what I said
we did some transformation on the output
of our hash function to figure out which
bucket we were gonna put our our data
into and so that was modulo four in the
case of our simple hash table and the
case of react a preference list is all
the V nodes that can handle this request
given the number the position in the
hash ring that we're going to and so
we're gonna call a function called get
primary active preference list and
that's gonna give us a list of V nodes
for this hash number the output of our
hash function we're going to look for a
particular service on our hash ring and
so Chris was talking about service
discovery having multiple different
services in your cluster
this is Reax core version of that I
might have a different service I might
have five services and so I need to
specify which V nodes I'm interested in
discovering when I'm looking for my
preference list or the the V nodes that
could handle the command that I want to
send we're only going to ask for one
because we only want to pain one thing
and we're gonna use this react or you
know library to do it we're gonna unpack
our preference list into an index node
that's just the actual pid' we're gonna
spawn command so we're gonna send a
command to this index node that command
is going to be ping and we're gonna send
it to this named process you know ping
ring V node underscore master that looks
a little funny as named processes go but
it's because react Cora appends
underscore master and so it kind of
breaks with the elixir naming convention
it
maybe I'll have a macro someday to store
that up but okay so that's the last
thing we do but the key here is that
we're sending a command to a particular
V node to a single place and that
command is ping so let's look at the
code that's going to handle that command
so the V node where we do our business
logic so again a simple module we
implement the behavior react or V node
so there's a bunch of callbacks that we
have to implement to satisfy that
behavior there's some boilerplate for
startup but then we have an init
function just like a gen server you know
totally normal we have some value we
return some map with some stuff in it
that gets passed into every callback
that we implement in this case the data
we're gonna hang on to as we do our
callbacks is going to be the partition
and this is just the position in our
hash table that this V node is
responsible for so you know maybe our
number is two and this
note is responsible from two until you
know ten and so this partition is going
to be to that the buckets that this
Vinodh handles and then we implement a
callback we handle command for ping and
we just send pong back with our V node
ID our partition and then there's some
other callbacks to satisfy the behavior
but that's the whole thing so let's look
at a quick demo so I have two servers up
top I have server a and on the bottom
I've server B server a is going to run a
ping it's gonna take the current time
time caption okay this time I went to
server a and this time I went to server
B I'll do it again okay my server a do
it again
server a okay so that's a that's how our
ping service works those gigantic
numbers are partitions they are V node
IDs there are positions in our giant
hash ring so but so that's cool but what
about state I kind of promised that
we're gonna build a stateful application
not just a thing that pings right so
let's build a second react core
application called store fetch and so
this is gonna be kind of predictable
what it does store a key with some data
we're gonna store the data in Ed's so it
just can be kept in memory we're gonna
hash the key to figure out which a V
node we're gonna send our data to and
that's it so let's look at some code so
here's the implementation of our service
store the kind of public facing function
so just like with ping the first thing
we do is hash something to get a number
a position in our hash ring this time
we're not hashing the atom the time
stamp we're hashing the key of the data
that we want to store we're using this
library and that turns into a tuple but
that's okay once again we take that
number that our hash function output and
we asked for all the V nodes that can
handle this command so we want it we
asked for we got this number and we said
hey bucket number two three four five
okay great I will send this to you once
again we're asking for this particular
service we don't want some other process
doing a different service to handle this
and we're only gonna store at one place
and we're gonna use this library we
unpack it we get the actual pid' to send
the command to we send a command to this
pid' our command is a little more
sophisticated it's a tupple with the key
and the data but it you know very
similar to what we were doing with ping
once again kind of funny process name
and that's the whole thing now let's
look at our business logic where we're
going to actually store the data in the
V node so again behavior some
boilerplate or in it is slightly
different we don't care that much about
the partition but we're gonna
instantiate an ETS table and then hang
on to it in our state map ok handle
command we're gonna handle the store
command key data and then our ETS handle
this is very straightforward we take our
data pop it into ETS okay that's that's
easy and then we do the same thing for
fetch but we'll use a lookup instead of
insert okay we're done we stored some
data we've distributed and we have this
bit this is great but you might have
noticed we're kind of only storing this
in one place so and the thing about
storing data in one place is that it's
not terribly robust the slow burn the
slow burn and so the thing about storing
data in one place is that we don't have
reliability we don't have fault
tolerance for our data if a single
server goes down we're gonna permanently
lose data because we're just keeping it
in memory so let's talk about fault
tolerance how many computers do you need
for fault tolerance well it's kind of an
open-ended question but basically more
than one write anything any more than
one so we're going to add a component to
our react core application so that we
store our data on more than one server
and more than one V node so our write
coordinator is gonna do that for us
instead of us directly pushing data to
AV node we're gonna hand it off to this
process that's gonna take care of
writing it to multiple places and then
it'll tell us when it's done so a write
coordinator execute commands on multiple
V nodes that's it the reason writing to
multiple V nodes gives us some data
safety some fault tolerance is that
react core takes care of taking V nodes
that are placed in a row and putting
them on physically distinct servers on
different V Erlang VMs so if I have V
node number 2 V node number 3 V no
number 4 if I have enough members of my
airline cluster that those of you knows
will be on different servers so if I
write 2 2 3 and 4 my dad is going to be
on different
we'll see this in a minute when we do a
demo so what does our service look like
our high-level API we're not going to be
doing kind of that direct work anymore
we're going to be leveraging our right
coordinator so store key and data just
like before we're gonna introduce two
new things two new parameters N and W
and so n is how many places do I want to
store the data if I do one this is just
like I had before if I do a hundred then
my data is very safe depending on how
many servers I have W is the number of
acknowledgments I must receive from the
places I try to write to before I
consider this write to be successful so
if I want to write it to 3d nodes i
might set w to two so I'll write to
three places and once two of them have
said they've got the data I'm good I'll
tell my user everything is cool so let's
do it
store fetch write coordinator do it this
command needs to return a request ID
because it's going to take some time it
can't just do it for it we're gonna wait
for a message back from this process
that will signal that we have
successfully written to at least W
things so we get a request ID back we'll
use that later the thing we're gonna
pass to our write coordinator is just
like what we sent to our V node before
because what the write coordinator is
gonna do is just send to some V nodes on
our behalf but it's gonna manage some
stuff along the way and we'll talk about
that we're gonna wait for a message back
from our write coordinator the timeout
after 5 seconds and we'll do a
selectively receive on the ID that our
write coordinator gave back to us once
we get it then we're good success we
have our data it's it's written to
multiple places and then we have our
fetch implementation so what is the
write coordinator well it's a finite
state machine it is spawned on demand so
jose was talking about the dynamic
supervisor stuff every time I am using
my service to write some data somewhere
I spawn a new write coordinator to
handle this request once it's done its
job and written the data it's done it
exits so it's a simple one for one and
so since it's a state machine I'm not
gonna show you code cuz that's just way
confusing I'm gonna show you a diagram
because that's what you do with state
machines so our state machine has three
states prepare execute wait these steps
are exactly the same as we did for our
pin service and our store fetch service
so prepare takes a hash of our key and
then figure out figures out which V
nodes are gonna handle this command who
is in our preference list we transition
to execute we actually send the commands
to everything in our preference list
that's easy enough we transition to wait
and here we wait and that's the whole
thing and so once we've got enough
responses from V nodes you know you know
enough acknowledgments that I have
written the data I'm safe you're all set
we return back to whoever respond us and
that's that's it for right coordinators
and now we are writing our data to
multiple servers in our cluster so if a
server goes down we still have it
available somewhere else what pretty
cool there is another major component of
fault tolerance and react core that I'm
just going to cover quickly and that's
handoff and so this covers the question
of what if we add a server to our
cluster or one permanently leaves or
even goes down for awhile and so in that
case if we we had a chunk of the hash
ring that we've done that server we
don't want it to stay on a server that's
down because then that P that chunk of
the ring is kind of gone forever
and so handoff will take those bits of
the hash ring and say okay this server
is dead someone else can get them and
then they're gonna be moved over there
it's implemented as a series of
callbacks in our V node module and it's
mostly just kind of some serialization
so I'm not gonna really talk about the
code there so okay now we have some
fault tolerance but you notice I haven't
said a single word about Phoenix yet and
like I've been talking for a while so
let's get into talking about Phoenix
surprise buzzword so Phoenix in react
core together it turns out this is
really easy we just use an umbrella
application so if you're not familiar
you just have two applications in the
same directory tree and you do a little
bit of configuration and then your
Phoenix application can use your react
core application like any other
dependency it's really cool and really
straightforward and so that's that's the
entire process of putting these two
really cool pieces of technology
together that's why I've got all this
other stuff to talk about so what we're
gonna look at what this looks like so
like we have some routes in a really
simple Phoenix application we'll put
store key with some data and we'll get
store key and we'll get it back out and
so this is gonna send data to our react
core application and store the data on
multiple view nodes all that stuff we
were just talking about so the request
looks something
like this we do a curl you know it with
some JSON data in the body and then at
the end we have my key this data stored
under my key so we're gonna look at the
code for a controller normal controller
you know my app store controller define
our store function the first thing we're
gonna do is unpack the request body out
of the connection okay I need I need the
JSON that I'm gonna store and then our
key from our URL and now we have these
two things I don't write a lot of
Phoenix code if this is the wrong way to
do it someone tell me please so now
we're gonna arbitrarily set our n value
to 3 so we'll just put on 3 V nodes 3
buckets and then we ask our service to
store our data on three things and then
return when you're done and then we
render it back out and that's that's
kind of the whole thing we do something
very similar for fetch and so the major
thing I want to emphasize about this is
not that it's weird but like that this
is pretty normal like that might be an
ecto repo thing instead right like our
controller is very standard so okay
let's look at it actually doing stuff so
okay I've got two servers in the middle
I have server a on the bottom I have
server B on the top will have R my
client and so we're going to first put
some data into our application and then
we're gonna take some out so we'll store
under the key Brussels some JSON food
equals chocolate okay great and so we
can see two of the V nodes are storing
to or on this server and one of them is
on that server great tolerance yeah so
let's do a different key clear that out
and this time we're gonna write some
different city maybe the one we're in
and our food will be something different
like currywurst and this time two of our
V nodes are on server B and one of them
is on server a so cool so we have some
we've distributed our data we had we do
different keys they end up in different
places based on the value they hash to I
know let's fetch our data back out so
we're gonna go ahead and grab brussels
and we can see it's served by the place
where the v node was residing on server
a will do the same thing for Berlin and
power same thing so our Phoenix
application fetched that data from
server be presented it to the user and
we're done so that this is kind of a
model of a database free architecture or
kind of like your your web server and
your database are in the same place
which is really kind of funky and maybe
scary but cool so okay Phoenix what
about real time like what can we do
there can we do something cool so
obviously if we're gonna do something
real time we're talking about Phoenix
we're talking about channels and so if
we want to do something cool with
Phoenix channels we need something to
hash on we need a key to manage our
value where our value might be a message
well here is what the struct for a
message for a Phoenix socket broadcast
looks like and you kind of look at you
go okay okay oh what's that
right well that the topic looks like it
might be the perfect thing to hash on so
let's do that let's well rather I did it
so let's talk about a pub/sub adapter I
wrote a really simple pub/sub adapter
it's totally just a hack but it was kind
of fun and what it does is take the
topic of the channel message hash it and
decide which V node in our hash ring
should handle this message and what that
means is that this V node manages the
subscriptions and broadcasts for this
topic that's really cool by the way so
what that means is that you've sharded
responsibility for your channel for your
topics across your whole cluster so
rather than every node in your cluster
getting every message for every topic
when when you send a message it's
directed to a single place and that
place manages the subscriptions for that
topic okay so here's the pub sub adapter
got three functions subscribe
unsubscribe broadcast I didn't do the
more sophisticated things because I was
in a hurry and this is really simple
they're just trivial wrappers around our
service our by the way our react core
application is called pub ring because
pub sub and hash ring and put those
things together so what does the service
look like well it's kind of like the
other services we wrote the pain and the
store fetch so the first thing we'll do
is hash the key that is our topic we get
a preference list the V nodes where we
should send this message to and then we
send the command
that denote from the list that's it just
like the other things so because it's so
similar we're gonna look jump straight
to the business logic is the the high
level API is the exact same process that
we've already done a few times so what
is our V node doing once again we have
the behavior react core V node and we're
going to skip the initially
initialization and stuff and look at the
two callbacks for subscribe and
broadcast unsubscribe is just like
subscribe just kind of backwards so
we're not gonna look at that one so the
first things subscribe
this one's easy we get a pig and a topic
so this pig should receive messages for
this topic when they show up well we put
it into our at stable and then we return
her done okay so we're keying it on
topic so we know that this Pidge should
only receive message for this topic
because this V node might handle
messages for different topics because it
just depends what the hash to and this V
node is responsible for multiple chunks
of our hash ring so okay now let's look
at broadcast broadcasts send the topic
and a message so I'm a member of this
channel and I want R this topic and I
want to tell everybody some message the
first thing we do is pull out all of
those pigs who should receive this
message from our X table we're using
match syntax it's a little funky but we
pull them out and we take those pigs we
go over them and we send them all
message you probably be faster if we did
some cool you know concurrency thing
here but I'm just iterating over them
sequentially that's fine and we send
them a message and that's the whole
thing and now we've broadcast a message
so let's look at how it works okay so I
have three VMs three servers and I have
three clients each one is connected to a
different server this is so you can see
that I'm not just making things up and
the messages do actually go from place
to place and so lobby messages are gonna
hash to this server private 99 messages
are gonna hash to that server don't
worry about the text it's tiny I'll
point stuff out it's different colors to
so so the first thing is I'm gonna write
a message to lobby but a boopadoo foo
bar and you okay it replicates the other
clients good I've made the best basic
requirement for pub/sub and the message
only hits this server I do private 99 is
the same thing message only hits that
server another message to lobby by the
way this client is connected to this one
and this one is only forwarding things
to my hash ring it
not managing any subscriptions it's not
broadcasting any messages yeah so that's
kind of neat that's my pub/sub adapter
so this pub/sub adapter is basically
just a really complicated version of PG
- it doesn't provide any major benefits
but we could do something really cool
with State so because we've distributed
responsibility for topics around a whole
cluster we have more resources we can
invest in a given topic and so we might
do something really cool it could be
just keeping some message history but
this could be something like gamestate
like we heard Keith talking about where
we were doing gamestate on the server
now that's gonna be really challenging
if we're doing on a single server and we
have tons of different games going on
but now we have a mechanism to
distribute this game state around a
cluster in a structured way that's
really cool this is a superpower this is
a big deal my god it's cool okay so
real-time you've got one last thing on
our checklist and pressure cat
sad face not gonna happen okay so so I
was like too quick I jumped yeah okay
so what's what else is gonna happen here
well I I think this is a really powerful
approach I think that this isn't right
for many applications but when you when
you have something where you have a data
set you could fit in memory but more
than on a single box I think it's a
really powerful approach that we could
do a lot of really cool things with so I
think it should be easier to build these
things so maybe we'll make some
scaffolding thing maybe the scaffolding
thing will let you also include a
phoenix application in your hash ring
maybe it'll generate a write coordinator
because those are pretty straightforward
and pretty much always the same until
you get into fancy stuff maybe we'll
have a macro so there's less boilerplate
writing V nodes and maybe kind of a more
serious fomentation of the pub/sub
adapter I made and who knows but it's
cool stuff and I think there is a lot of
potential for people to write really
cool applications using this approach so
thanks very much I really want to thank
the people who have put content in the
world for learning about react core
Mariana Carrera has a whole book about
this and it's wonderful
mark Alyn has given great talks and has
a really excellent example application
Ryan's as Eskie same thing great blog
posts and then project fifo for having a
four
react or that compiles underlying 18
that was important for this talk and
then the source is actually really well
commented so that was great so thank you
very much
yes in theory I'm not familiar with how
ecto adapters work but I think in
principle there's no reason why you
couldn't hide that react chorus service
the high level API like near just
storing and fetching stuff or whatever
other semantics you implement behind an
EXO adapter that'd be really cool yeah
it's a really good question so of course
when you're using real react it's not
just in memory and so they use level DB
for that so you kind of get both memory
and disk persistence the fork of react
core that I'm using Project FIFO
actually inputs rocks DB there instead
which is Facebook's kind of in memory
but persisted data store and so
basically instead of just using plain at
the tables there and kind of putting
things in memory you use something like
level DB or rocks DB so you could kind
of store things in memory and have them
written to disk as they come in so that
was mostly for the talk free
straightforward to persist things I
think so
yeah so that is so that's a really good
question and that's what hand off is and
so my cluster has three members and then
I had a fourth well at some point react
or will gossip information about its
state to each other and they'll go hey
we've got somebody new who's joined our
cluster and they're not doing any work
that's not good so they will reallocate
some of the V nodes from the existing
cluster over to the new one and kind of
rebalance things that sounds a little
scary
because it means shuffling a bunch of
data around depending on how much data
you have but so we a cord that uses
what's called consistent hashing so that
not all of the data has to be moved in a
normal hash table when you resize that
typically you move all the data but in
consistent hashing you only have to move
some proportion of it so yes it moves it
around using handoff</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>