<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Addressing Network Congestion in Riak Clusters - Steve Vinoski | Coder Coacher - Coaching Coders</title><meta content="Addressing Network Congestion in Riak Clusters - Steve Vinoski - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Addressing Network Congestion in Riak Clusters - Steve Vinoski</b></h2><h5 class="post__date">2013-07-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Y6PG6hkUHPM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I'm in the architect
a show is that I have gray hair like the
other guys you've seen our kind of much
younger there in engineering my job as a
speaker is to make sure you don't have a
moment like this when I'm done so this
actually happened yesterday someone
happened to catch me in a photo where I
was just so they made a meme out of it
so who knew how many of you use react Oh
excellent so we can just skip the first
half of the talk and just talk about
whatever you want to talk about well I'm
going to go through react because not
everybody understands but the the
operation of rioc I'm going to cover in
some detail because you need to
understand how it works to understand
the problem I'm trying to solve and I
should also point out that the the work
I'm talking about here is work in
progress this is not a solved problem
this solution that I have may never be a
part of react I don't know yet I still
have a lot of testing to do when I put
the talk in for this conference months
ago I thought that I would have enough
time to do more testing but then I
basically been on a plane ever since so
react is a distributed system so that
means multiple computers obviously and
all the problems that distribution
brings into the picture highly available
so in terms of databases or systems you
can have systems that lean toward
consistency systems that lean toward
availability react leans toward
availability that doesn't mean that it's
inconsistent that just means that it's
eventually consistent that means that
when you store a value in rioc if the
cluster the react cluster is under
stress or under duress you might read a
stale value based on what's going on in
the cluster but typically eventual
consistency is for no more than say a
couple hundred milliseconds highly
scalable system so you can scale the
system just by adding nodes that's the
whole idea is horizontally scalable
if you have a five node cluster and you
want more discs or more compute power or
more you know whatever just add nodes
it's open source so you can go to github
you can download it you can run it for
free you don't have to pay us anything
key value store so it's not a relational
database its key value store it does
have some other features above and
beyond key value and it's written I
think is most people here know it's
written primarily in Erlang it's modeled
after Amazon dynamo how many of you read
the dynamo paper awesome you guys could
be given this talk so Andy gross is the
chief architect at bash oh he wrote or
he had a speaker deck presentation about
dynamo five years later and sort of
looking at dynamo and when it's
published what we've done with dynamo at
bash oh there's also this annotated
dynamo version our dynamo paper
basically on our website where we've got
the text of the paper and then on the
side we have annotations that explain
what react does for particular cases how
it might differ from dynamo some of the
other features i already mentioned or
having other features like mapreduce
secondary indexing is a way of looking
up values on something other than the
key and then full text search it allows
you to break down documents in terms of
words and then search based on those
words or combinations of those words the
one thing about react i think that
really sets it apart from other of its
competitors is it's really easy to
operate we call it the most boring
database you'll ever run in production
we've had customers running rioc not
doing the type of monitoring that they
should they have nodes that died in the
system and they don't even know the
nodes are dead the thing just keeps
running so again it's really targeted
toward availability and by the way if
anyone has any questions chime in please
react architecture looks something like
this we have a number of clients written
in a variety of languages some of these
are built and maintained by basho others
are built and maintained by the open
source community
a client talks to either the HTTP
interface which is built over bashas web
machine framework or the protocol
buffers interface which is Google
protocol buffers that's a TCP interface
the either way you come in you're going
to start using the services that are in
react and you see the react or this is
basically the implementation of dynamo
react or is itself a separate package
that you may have already heard of that
implements highly available eventually
consistent distributed systems kv is the
key value store that is an application
of core as is react pipe how many were
in brian's talk prior to lunch in this
room so pipe is another application of
core and Yokozuna is our new full-text
search engine it's not part of the
product yet but it's available on github
and basically what that is is taking the
solar search engine and putting it right
on top of react core and then react or
knows nothing about storage all the
storage is done at this level so--but
cask is one way of storing data in react
that's the default way the level DB is
an erlang wrapped implementation of
Google's leveldb the memory back end
lets you store data that doesn't need to
be persisted and then the multi back end
lets you use combinations of these
within the same cluster of all this the
Erlang parts are in blue there so this
part is part c c++ that's all Erlang and
then of course the various clients are
written in whatever language is they
happen to be in any questions on this
okay so as I said before reacts a
distributed system so here's our cluster
and what we do to put data in the
cluster as we use a technique called
consistent hashing he was invented at
Akamai in the late 90s basically what
consistent hashing does minimize when
you have to move data and the whole
point of scaling horizontally is that
you're going to add nodes or take nodes
away so you are going to move data
around the cluster when you have to do
that you're minimizing how much data has
to be moved with normal hashing you
would have to move it's like an over or
n minus 1 over n amount of data
something like that with consistent
hashing I believe it's just one over and
type of number of nodes by probably
wrong about that but it sits on that
order and it also spreads data evenly
across the cluster so we use just the
sha-1 function as a hash function and
this is treated as a 160-bit value so
it's 2 to the 1 60 and we treat that as
like a roll over counter so you start at
zero you count up up up up up to do 160
you you and you roll back to zero and
that's kind of like a ring anyone who's
seen any presentation from basho will
see in the next slide the ring you have
to see the ring there has to be a ring
in every bash of presentation or I don't
get paid what we do is we divide the
ring in two partitions 64 by default and
we're just taking that the value and
splitting it into equal portions and
assigning those two different nodes
those little portions are called V nodes
virtual nodes and the vinodh claims a
portion of the ring space and those V
nodes are assigned to the physical nodes
in the cluster so kind of looks like
this we've got your 0 counting all the
way around to back to zero and we're
going to then break that into these
partitions or V nodes we're going to
take the colors correspond to the
physical nodes so starting at 0 you take
the green one that's partition 0 and
it's got a portion of that to to the
160th value
and it gets assigned to node 0 the next
partition gets assigned to node 1 and
you just go around the ring putting them
on different physical nodes the idea
there is that you don't want these
partitions adjacent partitions to be
next to each other because you're going
to store multiple copies of data which
I'll talk about shortly and to find
where the data is or where it belongs if
you're storing data we rehash what's
called a key or a bucket and a key this
is a bucket a bucket is just a namespace
it's a way of having the ability to use
keys for different parts of your
application happen to overlap you can
use different buckets to separate them
so we just take this value bucket and
key and hash it and wherever that points
us into this ring that's the v node
where we would either store the data if
you're doing a foot or retrieve the data
or look for the data if you're doing
again there are some constants in react
that define how the cluster kind of
operates so react is replicating your
data around the cluster that's the idea
is to have that availability if you lose
nodes than other nodes still have copies
of your data and you can still get to it
so the number of replicas by default is
three that's called n R is the read
quorum so when you do a read you want to
get our number of responses from the
cluster before you will return to the
client that by default is n over 2 plus
1 so for 3 of an N of 3r is too so that
means we're going to read three copies
the we being the rioc itself so the
client says hey give me this key read
three copies get two of those once we
get two of those we return to the client
the third one still comes in but it's
done asynchronously the client isn't
waiting for that and similarly with
rights it's also an over 2 plus 1 which
is to when you write three copies go out
across the cluster as soon as two of
those are written by default the right
is considered successful the third one
happens in the background clear even
after lunch clear
so it looks like this you are putting
some value at that place so again you're
going to hash and wind up on that vinodh
what we then do to determine the other v
nodes that should be stored is just
count up the ring so our hash led us
here and we go to the next one and store
another coffee and we go the next one
and store another copy fairly simple so
that I think is enough background for
you to understand this problem i'm
trying to solve i hope but if there are
any questions please let me know let's
talk now about sources of TCP traffic in
rioc client requests are obviously a
source of this whether it's coming in
over HTTP or protocol buffers it's
coming in over tcp so client requests
can be made to any node in the ring the
ring all the nodes are peers there's no
master there are no slaves everybody's
on equal footing you can talk to any
node regardless of where the data
happens to be that node will become the
coordinator for that request so that's
one source of traffic as a client
request another source is the
coordinator itself the coordinator has
to talk to the cluster it gets the
request and says oh I need to get you
know our our number of reads or w a
number of writes to occur so the
coordinator will talk to other nodes in
the cluster to make that happen trying
to fool me with the old oh I got a
question no I don't I'm just fixing my
hair
very sneaky gossip is another source of
traffic where what gossip is is because
there are no there's no master no slaves
there's no single copy of what the
cluster looks like every node has an
idea of what the cluster looks like in
terms of what nodes own what V nodes and
you know what V nodes own what partition
are sorry what part of that ring space
and other bucket properties and things
like that so those are all gossiped
around the cluster so occasionally a V
node will just say hey this is what I
understand and you know talk to some
other note or a node will say you know
this is what I understand talk to some
other node what do you understand is the
state of the ring and they will exchange
what they think is the state of the ring
and then if they differ they will
coordinate ironing that out and I won't
get into the details of that but it's
another source of traffic active anti
entropy is something new in react just
came in in 1 dot 3 which came out a
couple months ago and what that is is
that nodes are looking actively at the
data that they're storing and talking to
other nodes and saying this is my
understanding of this data what is your
understanding of the data and they're
actually hashing using merkel trees to
hash the data they're not exchanging big
blobs of data they're just exchanging
little hash values and saying if these
differ then somewhere in our our data we
have a difference so let's figure out
and Merkel trees are really good for
that so that's another source of traffic
or lying itself as you all know in the
distributed Erlang system there are
pings and things like that that go
around so we're using distributed Erlang
as some of the messages or for some of
the messages that get sent around a
cluster and then erling itself has its
own periodic checks multi data center
replication is something that's not in
the open source react it's something
that bash oh that's how we keep
ourselves in business we charge for that
and what that does is it allows you to
have multiple clusters that sync data
across you know between each other so it
can be unidirectional can be
bi-directional can go cluster to cluster
to cluster so that's another source of
traffic and something
called handoff and everybody knows that
handoff is so I didn't explain it okay
nah not really handoff is what I want to
talk about so this is this gets into the
availability features of react if your
cluster is running it's healthy and
everything's fine then when you do a
lookup the hashing leads us to where the
data should be and those are called the
primary v nodes because that's where the
data is supposed to be we go to those be
nodes and we read the data and we get
the values and we give them back to you
very simple but if the cluster is not
healthy if it's running into trouble if
there are network partitions if some
nodes have died then something else has
to be done if a primary vinodh is
unavailable then a request will go to a
fallback vinodh and as you know our line
processes a very lightweight so when we
need fallbacks we can just spin up new
processes that will fill in for the
missing primary the fallback vinodh will
hold data on behalf of the missing
primary so if you do a store and
supposed to go to primary a and that's
unavailable we'll just send it to fall
back for a creating the fall back if
needed and write it there the fall back
knows that it's a fall back so what it
does is it watches for the return of the
primary and once the primary comes back
then the fall back will hand off the
data to the primary and say this is
actually your data I've been hanging on
to it for you and somebody in bash Oh
came up with the analogy of somebody who
when you go on a holiday you might have
a friend that picks up your mail they're
holding on to your mail for you and when
you return they give you your mail so
it's similar to that are there any
questions about that fairly
straightforward so when we think of our
n R&amp;amp;W values and if we have a cluster
that isn't quite you know fully
operational because of some network
glitch or no down or something if we
wanted to have our n value of 10 let's
say which is a bit exorbitant but let's
go with it we can do you know our hash
and get to this vinodh and find that
it's missing
so we go to the next one that becomes a
fallback and we go you know we count
around the ring looking for the V nodes
that we need to either retrieve or store
the value and whatever we find to
satisfy the R&amp;amp;W once we find those who
return to the client as I explained
earlier so here we happen to find two
and our quorum is satisfied but we keep
in the background looking to put the
data or get the data as I explained
earlier so that's how rioc gives you
high availability is by basically being
able to create these fallbacks and use
the fall backs in place of the primaries
if we look at a cluster that's under
load I don't mean under kind of a normal
load this is like an extreme load I
don't know the details of this cluster
in terms of machine specs or how much
RAM is in the system or how much data is
in the system I don't know I know that
we did this test ourselves based on one
of our customers who really pushes react
very hard so I wouldn't pay too much
attention to the numbers and stuff like
that but you know the nice thing about
this is the graph is very flat it's very
nice you're getting even latency through
the system your throughput is very
regular very predictable and that's you
know what customers want is a very
predictable system so that's all good
and you can see that this test has been
run over about forty four thousand
seconds which is half a day basically so
12 hour test that's one thing that at
bash oh we run tests for 12 20 50 or
more hours because we found that when
you do a 20-minute benchmark you're not
really finding how the system truly
operates so when you see benchmarks out
there people post on Twitter and say oh
this is really fast if it's like five or
ten minutes don't even pay attention to
it if we look at latency of puts under
this kind of load again that the numbers
aren't so important but what's important
here again is we have this consistency
of latency we can see the 99th
percentile is decent and the 99.9
percentile is
you know a little higher than you'd like
but again it's a system that's under
pretty heavy load so let's scale the
system as I mentioned earlier scaling
means adding or removing nodes that's
how you scale react so if you add new
nodes these new nodes come in and they
have to claim partitions of the ring if
you are removing then those nodes have
to say I have these partitions give them
back to the system and other nodes need
to to pick those up either way you're
going to use this handoff mechanism that
I explained earlier to move the data
around the cluster so you're moving data
from node to node based on who claims
the partitions if we look at that if
we're moving or adding nodes or leaving
nodes you can see this part in the
middle here where our nice flat graph is
no longer nice and flat we have quite a
bit of jitter you know quite a bit of
variation there and you can see that the
you know the errors don't really crop up
but the overall throughput has dropped a
bit in that realm latency is also
increase their I don't have all the rest
of the graphs but basically you're
looking at increases in i/o because not
only are you doing client requests to
and from our to the cluster and then
responses back to the client but you're
also looking at inter-cluster traffic
moving data around so your CPUs are used
more the you know network is used more
how many have heard of TCP in cast a few
okay so that's I'll explain that but
there's a problem where and it's in
systems like react that you run into
this problem in TCP and cast basically
cuts your network capabilities
drastically I'll explain why there's
also hear a potential for clients to get
timeout so you have clients that are
making requests and traffic within the
cluster of these very very heavily
loaded clusters could actually almost
deny service to
client if you want to think of it that
way so those are all bad things gcp in
cast this affects many to one type of
operations so you have you know a bunch
of nodes sending data to one node and
what happens is basically you get these
micro bursts where a whole bunch of data
comes in for a node at the same time the
switch cannot buffer everything you know
it has limited buffers so the the data
comes in it has to drop packets because
it's receiving too much data and the
sender's of course see the packets
dropping so TCP says they will back off
and slow down a bit and what can happen
is it's like the old you know we used to
think about Ethernet where you would
have someone trying to grab the ethernet
and they try to grab it and you have a
bunch of nodes trying to grab the
ethernet and it all back off and try
again and of course it doesn't really
work that way but what's happening here
is you have a bunch of notes ending to
one node and they'd go whoa you know all
those packets just got lost I have to
try again I have to back off and I have
to you know increase the amount of delay
in my system and what you can see is say
out a gigabit network you could be
running it maybe 900 Megan or something
and then you drop to 10 meg you know a
huge huge drastic drop in throughput and
the reason I say it affects systems like
react is because again you have these
handoffs where you might have data
that's on multiple nodes being shifted
to the new node that you added to the
system so there's data three copies
three copies going maybe to this new
node or you know that sort of operation
and so we've seen TCP in cast in
production with some of our customers
took us a while to figure it out but
once we figured it out it all made sense
it didn't really make sense at all
before we knew what it was it's very
hard to figure out everybody knows that
led badias right it's my band no
LED bat is something called low extra
delay background transport and there's
an RFC for that it's an experimental RFC
just published about six months ago now
everybody get your laptop's out or iPads
and go read it I'll just wait basically
what led bed is is this delay based
congestion control when you have
networks that are congested what do you
do well as mahesh explained earlier you
you want to start introducing delay you
look at delays through the system and
maybe back things off I think we all
understand that what it does is it uses
delay measurements through the system so
you have a one-way delay measurement
that goes you know sender to receiver
and you can estimate how much queuing is
going on based on these measurements and
you continually make these measurements
to see what's happening and then based
you know based on that it's going to
make adjustments to how it operates but
it's also always going to give a little
bit of extra delay and the reason for
that is to avoid screwing up other flows
in the network the the origin for this
is data transfer bulk data transfer and
something called the micro transport
protocols anybody heard of that other
than reading the slide I know yes / has
so UTP let's call it is basically led
bat an implementation of lead bad but it
was before the RFC was written and it
was created by a company called plikt o
plichta was acquired in 2006 by a
company called BitTorrent you made have
might have heard of them basically what
the problem was that bittorrent was
being accused of you know with all of
its shuffling data around the system
that if you're using your browser and
you're sitting at home but you're
torrenting a movie or something that
movie is sucking up your entire
bandwidth of your network and you're
trying to browse you know memes and
stuff like we do at bash oh and you
can't get your images coming through
because these your movies taking up too
much of the bandwidth so what
torrent wanted to do was use this
protocol as a way of saying look we back
off we let the TCP traffic come through
tcp takes priority over our bulk data
transfer so they've been using this
since 2009 it's part of BitTorrent if
you torrent anything you've been using
it unknowingly perhaps and they have a
c++ library on github called libutti p
which i'll talk more about later so I
wanted to see if I could use UTP this is
an idea given to me by my colleague Greg
Bird bu rd he works in the architecture
group as well and he said we should look
and see if this mute EP thing will allow
us to avoid that jitter in the
throughput when we're doing transfers
we'd like the clients to not be denied
service based on the fact that we're
doing transfers around the cluster so to
build this in to rioc you take your
react and well that's interesting it's
blue on my screen looks much better and
of course you know that that's over
erlang an OTP so we understand that
that's implemented in Erlang and
everybody knows this everybody loves to
look at this code right the inet driver
and that's where tcp UDP and sctp are
implemented so basically any IP based
protocol how many of you have actually
done any work within the inet driver
well I know you had clack yes how many
outside of the Ericsson fold a few did
you enjoy it it's quite a piece of code
it's it works obviously but it's not not
something you just want to go in and
start messing with and you know in that
vein what I did is decided of course
that's written in C I decided to create
a different driver I didn't want to go
into that code even though you TP is an
IP based protocol I'll just stay over
here
so I call it Jen UTP so Jen UTP
basically has the same interface as Jen
TCP the TCP module you know as we know
wraps I think that should say Jen UTP
module wraps access to the driver so UTP
is wrapping the driver so that top part
is Erlang and then you have C++ driver
code and then lid whoops let's go back
libutti p is underneath the driver so
the UTP is giving us the protocol and
then what's interesting about WD p is if
you go look at the source code and grep
you'll you'll find no socket calls you
won't find any networking calls which i
thought was a bit unusual for a
networking library until i finally
figured out what they do is they let you
manage the sockets so you're kind of
plugging in underneath providing UDP
support and they're giving you the
protocol on top of those sockets so if
we look at some of the features of gin
UTP it's a connection-oriented protocol
so like TCP the sockets that you get
from the calls and jen UTP look just
like the sockets you get from jen tcp or
jenn UDP or or whatever they're just
ports on to the port driver so they look
the same it's got active modes just like
TCP so you can say active true and get
messages sent to the controlling process
active false you have to do receives to
get your data active once means give me
a message and then stop giving me
messages until I ask again you can have
binary or list data just like TCP I
think you're getting the point here it's
just like TCP supports sending I could
have put that just like TCP no supports
IO lists which are very cool for doing
networking stuff where you don't have to
flatten out your data you can just cram
it into a an arbitrarily nested list and
then do the send and it also has ipv6
support so that's kind of cool what else
so pack adoption how many have used pack
adoptions in gen TCP so it's the same as
that where you can say you know the
packet is a raw packet or you know
got a certain number of bytes that tell
you the length of the packet you can
also have message headers where the
first part of a message number of bytes
is considered a header to be header by
your application whatever it is it's
given to you as a list the rest of the
message might be given to you as a
binary you can bind to network
interfaces just as you would expect to
be able to do you can also listen all on
all interfaces if you like you can pass
in your own UDP socket I don't know how
many people would actually do that but
it's easy to do for me so I did it
anyway some of the functions that you
would expect to see are well I think
what you'd expect to see listen and
accept on the server side you also have
a sink accept so you can just get a
reference to and accept and once
something comes in then you'll get a
message saying hey this connection just
occurred connect on the client side send
and receive closing sockets sock name to
get your socket information pierre name
to get your peers information and the
port number you can get with the port
call setups and get ops this is done
through the den UTP module would be nice
if it were done through the inet module
that's something I'm going to work on
later and a controlling process
controlling processes where the messages
that come in an active mode gets sent so
it's kind of the process that owns the
socket so if we look at an example this
is just something I was typing in into a
shell and I'm listening on port 0 which
means an ephemeral port give me whatever
port is available and I'm saying active
false I want to call receives and I want
binary messages so it comes back and
just like jen tcp says ok and gives you
a port that's your socket and then do an
async except we could also do and accept
but obviously if I did and accept it
would block at that point would be hard
to continue the slide if I was sitting
and blocking we'd all be here for a
while and the other speakers would get
upset so this gives us a reference and
we can ask the sock name or we could you
could have used the port call to get the
port number in this case it happens to
be 60 70 for
and we then act as a client and connect
on localhost to that port and we're
going to set active true on our client
then we do a receive to get our accept
message and you can see that it's UTPA
sink the listen socket is where we were
doing the accept the reference that we
got from the async call and then the
server socket itself we could send on
the client some data and we get that
because we do a receive on the server
because it's active false and then we do
ascend from the server side and we get a
message in our shell because we're
active true that is a UTP message on
this port with that data any surprises
good that's the goal the goal when you
look at the code inside react that does
all this handoff and everything it's
based on jen tcp and so if i wanted to
replace it it would be nice if i could
just change jen tcp to jen beauty p and
one weird thing is you don't know this
but jen tcp can actually send two things
that are other than TCP I could have
just left jen tcp as the module name and
have it call you TP which is really odd
but I thought that was a little freaky
so I didn't do that internally it's a
c++ library so Erlang driver you know is
c++ it the library itself works via call
back so it's going to do it's protocol
things and it's going to call some
functions that you pass into it when it
has certain events it implements
protocol you implement the socket
handling as I explained earlier the
master branch that I have for this has a
C++ class hierarchy that's called
handlers there's handlers that it's a
hierarchy of different types of things
but they implement handling of sockets
handling of mu TP sockets which are
different than UDP sockets and also the
ports the Erlang port instances I have a
development branch that's on this
machine that doesn't work yet but I
broke that hierarchy into two separate
things so
what that new hierarchy looks like is we
have socket handlers which just deal
with UDP sockets and a listener is a
socket handler because it's not dealing
with you t mu TP itself those are done
with accepts and connect handlers a UTP
handler is a base class for something
that deals with mutti p and handles
these callbacks from the library and
that's derived from a socket handler in
terms of handling events the UDP sockets
are registered with Erlang and when we
have input on those we can you know we
get a call back that says hey basically
select right that says hey you've got
some data and a socket handler will deal
with that and then there's also this
thing in lib UDP that you have to call
periodically checking for timeouts so
that's called every 10 milliseconds
based on a driver call back a Erlang
port driver timer the pork class is on
the other hand there's this base class
for pork classes called a dr v port and
the main port is the one that we open
initially to the driver and we kind of
keep that around because it's handy for
certain things the main port is where
listen and connect calls occur so when
you do a listen at the gin UDP level it
comes into an instance of main port and
that's what implements the listen or the
connect socket port is like a socket
handler but it's a port that deals with
just sockets that would be a listener
basically so listen port is what deals
with listen calls a UTP port deals with
the micro TP ports those would be
clients and servers through connect and
accept so you have an except for that
implements accept and a connect port
that implements client now implementing
accept is kind of interesting in a
system like this because normally what
you have is you have a UDP client that
connects to a listener just like TCP
underneath it's all UDP as I've
explained now in TCP when you do it
except what that means is give me a new
socket based on you know this client
connecting to my listen socket I get a
new socket from the kernel that connects
me to that client
and that's what we want right we want to
be able to drop this into react without
really changing a whole lot so that one
puzzled me for quite a while and what I
did is you know the listen socket is
basically opened with reuse address or
on Mac that you have reused port so you
open in that mode connect when somebody
actually the client tries to connect we
connect the UDP socket to that client so
we create a new socket opened on the
same listening port but connect it and
how many of you know that you can
connect UDP sockets I know you would
know that I know you would know and I
know you wouldn't yeah that's very handy
did you also know there's a gin you DP
connect that's not documented you do now
I know you'd raise your hand for
everything now that's good that's good
yeah so that's how basically implement
the kind of TCP semantics for
connections so what happens in that case
is any traffic from that client gets
sent even though it's on that same port
as the listen port it gets sent only to
that except socket that we created for
that client which is exactly what you
want and then anytime you send on that
except socket it goes to that client and
you can use send you don't have to use
send to which you normally would use
with a UDP socket the inet driver uses
the driver q port drivers have a queue
for data they use I use that for my
reads not my rights which is a little
unusual i think but the problem is i
can't just deselect the socket because i
have to look at all the data coming in
to know if it's UTP traffic or not and
that's really a detail of how the lib
UTP works so I I store read messages or
you know the read messages I store those
in this queue until they're read by the
the client or the receiver now obviously
there's a problem there because you
could kind of blow that out but you can
use just busy port just like any other
driver to kind
tell the upper layers about that in
terms of managing load there's no listen
q so like in TCP you have a default of
55 q spots that if someone tries to
connect they get sent to this queue and
they are waiting for somebody to accept
them I can't really do that I don't have
that capability so if there are no
except errs than any client that tries
to connect just gets rejected and that
again is partly due to the way live UTP
works I've noticed that this libutti p
is really slow when it has to close a
socket because it does a whole bunch of
traffic just to close down and that
that's a bit of a pain to handle because
the port say say you have a close call
jenn UTP clothes you want to close that
port socket thing that you got when you
connect it or accept it you'd like to
close that down and return to the your
airline code as quickly as you can but
because of all this traffic you kind of
have to sit around you're not actually
blocking but you're waiting for a
message come up from the port driver
saying hey I've finally closed that
thing and it's a bit rough and that's
where I've had a lot of trouble in
implementing this lib UDP is not
thread-safe so that is a pain you have
to block now you have to serialize all
access into that library so that doesn't
work so well but I'm hoping that this
split that I did in my development
branch will help manage the lifetimes of
these ports and sockets and handlers
that basically are all doing kind of
different almost different fsms all at
the same time as I said earlier this is
definitely a work in progress if you
look at the code it's not the greatest
code ever you know it kind of works but
it certainly has issues and again I'm
trying to prototype do is kind of little
work as I can to try to prove the
concepts but you have to do enough to
make it worthwhile so I've done some
small-scale testing I'll talk about I
haven't really tried to go back to the
react cluster and run the tests i showed
earlier because airplanes the testing
i've done basically have taken two
machines connected them directly and
just what i want
do is look at TCP vs mu TP throughput
and it's the same at 10 base-t so if you
have slower network ten or a hundred
you're getting the same kind of
performance basically as TCP but at a
higher CPU because they're in the
colonel I'm not but at a higher rate
network I'm already 2x slower than TCP
so that's not good part of the problem
is the way lib UTP works is you when it
wants you to send data it calls you and
says here's a buffer I want you to dump
some data into it or on reads it'll say
here's some data I want you to copy that
out because you now own it so you're
doing all these data copies you'd
obviously like to not do data copies and
I think that's where a lot of the
overhead comes from that and the single
threaded pneus could be a factor as well
now this lower throughput on these fast
networks could be a showstopper may
completely render all of this pointless
for us because data center lands are
usually fast you know 10 gig for example
so that might mean maybe this is a waste
of time always deferring to TCP may also
not be what you want because if you have
a cluster that's under duress you know
it's under stress and you want to scale
it because it needs more oomph you're
going to add nodes you want these data
transfers to happen as quickly as
possible in that case because you want
the cluster to become stable at its new
scale as quickly as you can get it there
and so delaying the handoff based on
other flows going on the system and sort
of backing off from those flows maybe
execu exactly the wrong thing so you
have a cluster that's struggling to keep
up with a load you want to give it more
and G that doesn't quite work we have a
bash o labs account on github and that's
where experimental stuff goes that's
where most of the things that my team
works on and other people in bass show
if it's something that may or may not
show up in rioc that's where it goes so
like I said earlier this may never see
the light of day
it's there it sort of works I broke
something very recently when I kind of
brought it up to our 16b with the binary
versus list delivery sometimes I seem to
get lists when I expect binaries but I
think I can fix that pretty handily and
as I said earlier this handler port
split is something I still need to work
on what I'd like to do is again
obviously we want to test on a real
cluster you know get this to a point
where it makes sense to compare and i
also want to redesign the driver to use
pre my net and you might think are you
mad how many of you looked at pre my net
I know Tony has its you know per my
print I net itself is fairly
straightforward but what that would do
the reason I would want to do that is
SSL if you obey the kind of interface or
the if you're plugged into pre my net
you can put Erlang ssl over the top of
your driver for free and ssl would be
handy for us not so much for this
handoff capability but for this other
capability which is that multi data
center sink that i talked about earlier
if it doesn't work for hand off because
of the reasons I mentioned it might
actually be good for lands where the
data center synchronization occurs so
I'm going to be looking at that as well
and to do that we'd need SSL support for
the capability so hopefully that'll work
in terms of related work the reason yes
firs been raising his hand for
everything I've asked is he wrote a
partial implementation completely
independent of any of my work he had
done this in fact before my work of mu
TP it's written in Erlang and that's on
github as well you can go look at that
basically looking at lib beauty p
because the the protocol is very poorly
documented sorry BitTorrent basically it
isn't really documented well so he
looked at libel UDP and tried to figure
out what the protocol was and then
and it I didn't do that I didn't use his
version because AI thought libutti p
would be a faster path to where i wanted
to get to and this whole ssl capability
is something I'd want to explore so yes
/ told me this morning that if anyone's
daring enough to give him pull requests
he'll take them but he doesn't think
anyone really wants to dive in that
deeply so and that's all I don't know if
you have questions but thank you for
listening and no hard questions Tony
please okay so the solution i presented
is an application independent way of
addressing in cast and while congestion
in general
right so the point here is that you know
there are basically a number of ways of
addressing this some can be application
independent some can be application
dependent you know vary based on how
react works and it's coordinators and
that's your point I hope and that's
absolutely true there are a number of
ways to address this many of which I
obviously am unaware of but you know
meanwhile while all this has been going
on occasionally because I only get to
work on this occasionally I mean react
itself has built in other mechanisms for
you no shedding load and handling backup
and you know things like that have
progressed so there are you know many
ways to handle this is just one
potential way of dealing with it this is
like I said earlier it may not even be a
good way of dealing with it because I
haven't done all the testing yet yes
yeah so right the question is really
about how UTP works and it's a protocol
so you think of the TCP doing handshakes
and things like that and you know it's
doing stuff similar to that it's talking
to the other side and there's messages
exchanged and they decide on delays and
they decide on you know how much data is
in the payloads and all that kind of
stuff so yes there are control messages
going back and forth between the two
ends so client connects and then there's
twenty and thirty bite messages that go
back and forth I don't know what's in
them because I haven't looked at that
detail
oh well they're not so ok so the nodes
themselves aren't kind of saying oh
they're I mean they're they're looking
at delay whenever they send a message to
the other side they look at the delay
and based on the delay they individually
decide I need to back off you know lower
I need to add delay i need to back off
and so it's not that the nodes are
coordinating together to make a decision
each one individually is making that
decision but that's the claim I mean
libutti PR sorry mu TP itself is
controversial in the networking
community some people believe that it
does exactly what it was designed to do
others believe that it doesn't that it
sucks up all your bandwidth I don't know
which one is true but there are no
colonel implementations of this protocol
that I know of
is do we want to change the back off
semantics of TCP uh well write the
problem with that I mean maybe ideally
that would be something that you could
do but in a practical term I would be
unable to do that so this is a way of
kind of you know using something else to
see if it helps with the problem the
flip side of that of course is now
you're dealing with a protocol that
hardly any people understand whereas
with TCP a lot of people understand it
and you can go find out information
easily about it this protocol obviously
is very much unknown known to only a few
people not documented well so maybe it's
the wrong choice just on that basis
alone so again it's no it's not a
clear-cut winner it's just a work in
progress okay if there's other questions
I'll be around I've got a whole bunch of
drink tickets somehow so you know I'll
be around but I'm leaving later tonight
so but if you do have questions just let
me know and again thanks for coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>