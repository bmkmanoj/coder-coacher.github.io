<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Lambda Days 2015 - Chris Brown - Semi-Automatic Refactoring for (Heterogeneous) Parallel Programs | Coder Coacher - Coaching Coders</title><meta content="Lambda Days 2015 - Chris Brown - Semi-Automatic Refactoring for (Heterogeneous) Parallel Programs - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Lambda Days 2015 - Chris Brown - Semi-Automatic Refactoring for (Heterogeneous) Parallel Programs</b></h2><h5 class="post__date">2015-04-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8MgDMfQpiPk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everybody my name is Chris Brown
I'm from Scotland the university of sin
andrews which i think is actually
possibly colder than paul and probably
the only place on the planet at the
moment that's colder than here so what
I'm going to talk to you today about
multicore so we now live in a world
where basically the single core chip is
dead this is very exciting obviously
what I have here is just a picture of a
chip and I believe was actually released
on Tuesday for two days ago so this is
this is kind of hot off the press it's a
hundred core chip so the it's a single
chip with 100 cause 100 general purpose
cause as well not just lightweight units
it also has enormous amounts of cash so
something like in excess of forty
megabytes of cash this is this is
absolutely humongous it also has other
types of devices built-in it's got
network accelerators Ethernet
accelerators those are sports quite
decent amounts around as well and this
is the reality as we can see these kinds
of things is a hold of course I'll come
back in a couple of years and will be
200 cores and so on and so on so it's
almost too good to be true put this a
cache and parallel programming these
these types of systems is absolutely
almost impossible it's incredibly
difficult I'm sure everybody everybody
has at least tried this once the problem
really is that mainstream programming
models just aren't he is they're not
designed to kind of scale up to this
type of parley's 'um I talked about here
just Justin as an example Earl and has
it has a concurrency model built into it
using explicit things like processes
sending and receiving messages this kind
of thing this doesn't work okay firstly
it requires an enormous amount of
expertise why is enormous amount of
experience concurrency is not
parallelism
you can't write you can't like programs
that are going to scale 200 calls using
all fashion concurrency techniques it
just won't work and and also these
processes are not really that
lightweight everybody until I said oh
you can you can spawn millions of
processes and they're so lightweight
they're not you won't get any powers and
pull up but that's just really the tip
of the iceberg it applies to F just
about every programming language on the
planet if you look at languages I see
it's a saying these P threads you just
you're going to have headaches using P
threads after about five minutes and
also applications can be paralyzed in
lots and lots of different ways there's
not do it doesn't exist one
implementation of a parallel program
just doesn't doesn't happen like that if
I come back next year and i'll show you
a different chip you're going to have to
completely like all of your call to work
with it you can't just you can't just
take a program until just one on all
parallel systems it just doesn't work
like that so choosing which not choosing
how to write this parallel cold but also
choosing which of which parallel
structure to exploit is very non-trivial
requires a lot of expertise a lot of
experience you can do it on your own but
it's going to take you a long time so
you can you can basically implement all
possible parallel programs that's going
to take you almost an infinite amount of
time to implement so Jane Andrews we've
been trying to work at this problem
we've been trying to solve it so just
looking back at Earl am i just use this
as an example and to show you what we're
doing we've created a library called
scale this is basically just a library
you can use and advice in other
languages you have similar types of
libraries as well basically it provides
fundable templates so parallel patterns
that you can just basically plug your
functionality into them so that so it
gives you all of the parallel
implementation you need and you just
plug your your units of work into it and
as far as we're aware it's the only ride
we that exists for airline it contains a
number of different parallel patterns
are talked about these in a moment it's
fully nestable so you can nest different
patterns beside each other so you can
have almost an open at the amount of
different types of partisan it's kind of
like a DSL so here's just an example at
the bottom of the slide here works by
calling this function do in the module
scale you give it you give it the
description of your skeleton and you
give it some input items I'll explain
that in a bit more detail later on to
the point of this is it encapsulates all
of the parallel implementation so the
program doesn't have to worry about it
but every time I write an application i
don't like an operating system so i
would a parallel Paul going to do the
same thing so this is just a couple of
examples of the type of patterns skill
provides this one's called a parallel
pipeline the idea here is you've got
some some work coming in here we call
them tasks and then we've got a number
of stages each of these dangers can
execute in the parallel so you can
imagine you got marginal NCP use each
stage you can execute on a different cpu
in parallel so one way to think about
this is like a car factory you've got
these of all gasp you've got a robot
putting on the door robot putting the
engine in and so on obviously each robot
can work on a different car but the same
time so that's how the prowlers in comes
into it and then down below he is just
an example of how this works in a line
so you basically call your Dean function
again you simply pass it a tuple and
this tuple describes what the skeleton
is so this first thing here as a nasen
that just says pipe it's just a label to
say we're using the pipeline pattern and
then we simply just pass in our stages
there's a list and then at the end we
pass in our inputs which is a list of
lists of work that's going to be
executed another example is a farm this
is like a data parallel pattern so here
we've got again we've got some tasks
coming in and then they all get sent off
the different what we call workers so
each of these workers can run on a
different CPU as you can see you get
like a replicated effect so each of
these workers are independent operations
I'll see if you have 100 cause you can
have 100 workers and so on so you get
quite massive amounts of pollen with
this kind of pattern and the course you
can put pipelines inside here so I can
have a have a work of that's a pipeline
and so on or pipeline where each stages
of work far and so on so forth so one
other thing I'm going to talk about
today is refactoring so I'm going to
combine refactoring and this the
skeleton approach to show you how you
can right panel programs quite easily
the only buddy heard of refactoring
quite almost everybody that's good yeah
yeah so it's been around for a long time
we all know what this is right I mean
it's a semi automated approach so I'll
just I'll just briefly explain it very
quickly for those who've never heard of
it it's basically an automated or
semi-automated tool that exists in in
common and environments I could have
emacs or clips or whatever the idea is
you have a menu of different things you
can do like is an example here so you
can do some renaming you can spec the
function and give it a different name
and this is not just a simple find in a
place it's a bit more intelligent than
that so what it would do is it would
rename all of the different functions at
Point 2 thats that warm you've selected
instead and then rename all laws it also
does things behind the scenes so it
checks for condition so you can't give
it obviously Elaine that's not going to
be valid allowing syntax or whatever you
can't give it a name that already exists
but do won't let you do that so it's a
bit more of a disciplined and of
software engineering tool so we factor
things generally that if they're
described in terms of some
transformation and some conditions for
the transformation is actually the easy
bit it's just just changing the source
code so just making the change changing
the name of function f the function G
what the actual the complicated bit
behind the magic if you like is this the
conditions so these are things I can
actually apply this week factoring does
it make sense if I Renee function G 2
h's are going to conflict with another
function called H and so on
and we call these the semantics and you
can have conditions you can apply them
before you try to try to do the
transformation you can have conditions
that you apply it at the end and so on
so we're going to use this technology to
kind of tell people right parallel
software so we developed a in factoring
tool and for airline that helps people
introduced and tuned parallel programs
so it's quite an easy way to introduce
parallelism just at the flick of a
button this is good because obviously
you can you can have a tool that helps
you choose the right abstraction like
parallel pattern you don't have to
really think that hard and if you come
along with a new architecture you just
have to click a few buttons and you get
a new implementation that's optimized
almost instantly the tool will do all
the conditions checking for you it won't
do anything stupid it won't introduce
deadlocks all this kind of stuff race
conditions all this will all be it's
guaranteed that it will be correct and
you can integrate things that
performance information I'll talk about
this in a short while so you know you're
going to get the best performance if you
choose this pattern etc this is just an
example of a kind of a screenshot of the
tool this one's an eclipse so here you
can you can you can see you've got your
your program here you've got a menu you
can choose different refactorings to
apply and so on and the vault of all
undoable so that all if you introduce
something you can always go back which
is another nice feature of these types
of tools you make a mistake you can just
you can just reverse so we've built our
particular tool into Wrangler which is a
real language factoring tool it's
embedded into a number of common ideas
and examples so here's romsey maximas
Eclipse might be of inversion I'm not
sure it supports the full airline
language so it's not one of these tools
that only works with a subset or with
you know the winds blowing East or
whatever it will work for the full the
full airline language and it's also
faithful to your laid out in comments so
it won't produce
called but you won't understand is very
important if your program in a
particular layout it will try its best
to preserve that type of type of style
it has an undue I've talked about that
briefly it's all you can go back
obviously and it's built into lab but
that's not really that and here's an
example of a screenshot here of
Vanderbilt and taking max so we've done
is we've taken this idea of having
parallel patterns and and refactoring
tool and we've kind of bolted them
together and the idea of what we come up
with you as a methodology so this is a
disciplined be like software engineering
kind of way of thinking put more
structured than just randomly using
online processes and spawns in the
scenes all over the place so the idea is
you start are you on the left this this
white box says sequential program but it
really doesn't matter if you start off
with a program it could be parallel
could be sequential does it matter and
then you look at that program you gained
some evidence about it so you look at
the structure of the program so you have
to do a little bit of thinking or the
supports the program doing how's it
doing it okay and then what you can do
is you can use a cost model which I'll
describe her shortly so you can get some
information about how long particular
parts of the accord of running and what
what structures give you what
performance information this kind of
thing and then you can use our
refactoring tool to introduce your pal
ilysm you once you've identified the
structure you can just use the tool to
introduce the panel isn't for you and
the result of this is a thing on the
right a program with a parallel program
hopefully with some introduced skeletons
I can then execute this if it doesn't
give you the results you wanted or the
new chip comes along you can simply just
try a different one try different
refactoring etc and then and then you go
out we keep the way so we've purposely
done this so it's not too compiler it's
not magic box that just generate cold ok
those things basically don't work
compilers that generate a lot colder
been around for a long time but with
since the seventies or the 80s and they
don't work they work for incredibly
limited set of examples and introduced
very simple types of powers and work
with what we've got to you we want to
take any kind of program doesn't matter
what we want to lose power it's a whole
completely different league of open
space so these are just some examples so
refactorings not just it's not just an
ad hoc thing it does have some science
behind it and I'll just go over these
very very briefly and so there are rules
that exists out there that kind of help
us choose what kind of refactorings
we're going to implement these rules
here are taken from the skeleton
community been around for a long time
quite well known in that community what
they say is if you take the thing on on
left you can basically rewrite it into
the thing on the right so these are
might be factoring just a higher level
of refraction so this first of all says
if I have a composition so I apply s1 to
s2 i can rewrite that into a pipeline
where i have two stages s1 and s2 so see
it's quite obvious another example here
if I have a map where I I have the work
of function as a composition of two
things i can rewrite this so i put the
composition on the outside and so on and
so forth so depending on what your
application does depending on what the
cost models come back with one of this
kind of stuff will indicate which one of
these things you should choose because
not in all instances choosing the same
rule laws give you the best performance
this is very important and we also
define our refactorings just in a kind
of semi formal way so i said before that
the factoring czar a transformation and
a condition so what we've got here is
just a way of kind of describing what
they are so this this this thing here
with the kazakh box is like a function
to think of it as a function that takes
an expression and transform
it into another expression can be
factored things can come have Sable's
you can chain them you can do choices
all sorts of things so this is just an
example of what what what these rules
look like so this rule here is called
pike comp and takes an environment and
an expression so this this is the thing
that the program is selected they've
selected this expression and they want
to introduce some kind of parallelism so
this bit that the top line says is I
want to match if not if the thing that
the user selected matches it so if it's
basically a list comprehension where it
wears for a function application here I
can simply refactor this into a call to
scale with with a pipeline and then
we've got some conditions down here so
we check that things in sculpt etc etc
just another example here I've got
something a bit different this example
I've got a an already existing skeleton
and I want to rewrite that skeleton into
a pipeline so this rule just says if I
much something that looks like this i
can rewrite it into this pipeline and
then i get my ass in condition to you so
this this transformation will only
proceed if these conditions are met and
this is just an example of what these
refactorings actually do so here at the
top of the slide I've got this is just
an example piece of other line so that
the users written some function list
comprehension with some function
applications a year if we select this in
our factoring to it will rewrite it if
you want to introduce a pipeline that we
write it to the thing on the bottom or
it has a call to scale but I apply where
each of these functions becomes a
different stage in a partner okay so it
might it might not trivial to you but it
what's interesting is about two years
ago at the EUC conference we run a
tutorial on this and most of the
audience are quite experienced the line
program
firstly what we got them to do is to
take the manual cold and rewrite it into
this I mean after elders they got just
trying to run the things it was crazy
you know miss you missing a brace off
you miss a bracket whatever you get the
function wrong just doing this
automatically saves huge amounts of time
really does and this is a very simple
case very very simple case so we have a
number of these refactorings implemented
so we've got things like we can
introduce and eliminate pipelines farms
all sorts of things we can do some
chunking this is optimization
refactoring that allows you to kind of
goop together tasks to make ours a more
thought into this is good if your your
your computations are very small so you
want to make them as large as possible
to get the most performance this is a
good this as i get a tuning the fashion
and all the versatile so you can if you
need to do something you can always go
back so you don't need to worry about
making mistake i mentioned before i
mentioned cost models and some one thing
that's important with these tools is
obviously you can introduce the powers
and how do you know which one to
introduce how do you know what we factor
when you're supposed to apply okay we
don't want it to just be and i like a
magic black box we want to actually
inform the programmer in some way about
what they're supposed to be doing so
what we did was we developed some very
high level cost models but these these
are these are models are not intended to
be completely accurate they're just
supposed to give you an idea of how what
your performance is going to be if you
choose a particular if actually it's
almost impossible to get these hundred
percent accurate anyway and so just
quickly describe what these mean the top
one this tells you if you have a
pipeline you can basically predict the
performance is roughly the maximum of
the pipeline stages so whatever you're
slower stages I'll be roughly what your
pipeline performance will be plus some
time to copy and the map is slightly
different so you take all your your
workers and you divide it by whatever
smaller the number of workers of the
number of CPUs you
it on plus some time to send your tasks
out and some time to get your tasks back
so this gives you with estimates farm a
slightly different the farm you
basically do the same thing so you take
the cost of one worker you divide that
by how many you've got and you take the
maximum amount of time between how long
it takes to send an item out how long it
takes to send the item back and how long
it takes to compute this gives you a
very rough estimation so just don't show
you an example here this is a demising
example and the idea is you take a
picture with some noise in it it could
be a satellite image or a TV picture or
whatever then you try and filter it to
get back to get back to the original
image so the algorithm is very simple so
here we just have a simple function
what's important to you I just wanted to
show you the point of this example is
that this is the structure of its been
identified so the programmer has
identified that this thing has some
structure that the original core might
not necessarily look like this just to
show you as an example so you've got
some you've got some functionality he
away you've got two functions you do
some geo referencing and then you do
some filtering for a bunch of images
that are coming in in a stream and this
is your denoising function so I want to
refactor so I want to introduce some
power ilysm but how on earth am I going
to do that and there's so many different
possibilities that could do here so what
what I've got here is just an example
this is just these numbers you're just
using the cost models so in this example
the first step I'm going to do is take
that then introduce a pipeline and I can
do that because my refactoring to tells
me it's the best choice because it says
if you've got to composition which is
basically what this is so I've got two
functions filter and gof it's going to
be roughly this time because I take the
worst time for each function and / sorry
i multiply it by how many images I've
got it's going to take this amount of
time water
if I introduce a pipeline it's going to
reduce to this so it's going to go from
65 to 47 just define to juice a pipeline
so I think okay I'll do that and so I
select this piece of code and the tool
gives you back that's the skeleton thing
and then I run it and I get the goods a
decent performance but and then I think
attune it quite dramatically after that
so once I've got this this pipeline
stage entry factoring the tools giving
me I can then go on and shoot and in
this case so it's saying okay the
pipeline estimation is going to play
this time but if you introduce a farm or
Maps sorry a map in this case you've
only get it down to this it's going to
go up in 47 to 17 that's quite good it's
okay I'll do that I think and then the
last last stage of this refactoring i'm
going to even optimize it even further
so i'm going to go from 17 down here to
about 2 so i'm going to take my calls
that we're factoring tool introduced i'm
going to be fact to it even more to just
a farm you in the first stage and they
get it down from 17 22 so this is just a
summary of that kind of process here and
what this graph showing is along the
bottom this is the number of course or
12 24 in this particular case putting on
a 24 core machine and going up up on
this side of the speed up so this is how
many times faster this is running
against the original sequential version
and the blue line indicates the first
the very first step we did is just
introduced a simple pipeline to just
talk around composition and made it a
pipeline we do get some speed up just
over one it's not very good also the
dotted line to your show this is what
the cost models will predict it so in
this case it's almost exactly what the
cost model predicted the solid red line
this is this one here is the second
stage to introduce a map into the second
part of the pipeline we get a bit of a
a boost in performance it goes up to
about four and but then it flattens off
so it's still not very good and then
this version here the kind of the light
brown color this is when we go to the
final stage where we also put a farm in
the first stages pipeline and as you can
see this kind of scales all the way up
almost so we get about a speed of about
eighteen or twenty four cores which is
pretty good it's also pretty good
because all the users have to do is
click a few buttons he hasn't had to
actually implement anything just
selected a piece of code and introduce
some patterns there's two minute process
so going from kind of going from this to
this but a few clicks of a button is
pretty I think it's pretty good it's
just another example this is a call the
samoylov this is just a very simple
example from computer algebra the point
to you I wanted to show you was but the
blue line here it kind of starts the
tail off of it here and this is just
using a simple farm pattern so these is
just introduced afar but it's kind of
tailing off and you've got a 24 core
machine it's getting about 16 16 speed
up till good when you can do better so
what the the tool comes back with and
says if you if you use chunking so if
you group your tasks into into groups of
four hundred you're going to get a kind
of a speed-up of here so you're going to
get linear speed-up so he does that and
then reality it's false but it's not
exactly there so there are plenty to
speed up so that sounds pretty
impressive I think so this is an example
of effort by talking about performance
but tool delivers in performance but how
does it deliver an effort and what we
have view is just a table of different
use cases that we've given to some
industrial contacts that we have at
least so convolution is a simple example
it's it's very similar to the de-noising
1 i've just shown you these are actually
done in c++ my other line we have we
have tools for C++
similar things but the principle is the
same so took these guys for convolution
about three days to to like the parallel
implementation from scratch by hand and
these people are fairly I would say
there were experts in doing this so when
it went to for them it went for three
days just about three hours and that
includes the time it takes actually
understand how to use the tool as well
in this example here and colony it went
from something like a day to an hour
this one is probably the most impressive
the basic end to this is an elec uhler
dynamics use case it's about something
to do with tracing how molecules
interact with each other in gases quite
a complex a use case but it went from
something like five days of
implementation down to five hours so
obviously this is not just about
performance it's about effort so my
it's going to save companies millions of
parents in effort using using tools like
this and obviously if you're saving on
efforts you can increase effort in other
places so you can and concentrate on
making sure your software is doing what
it's supposed to do more than optimizing
so that's the basic idea of refactoring
so using the factory tool to introduce
these patterns and but there's something
else I wanted to talk about and that's
called program shaping so all of the
examples I've shown you here are pretty
easy they're pretty simple okay in
reality programs don't me look like that
they don't look as simple as that
obviously and you have other problems
such as extra functional properties like
memory usage is kind of thing so another
another kind of dimension to refactoring
is something called program shaping the
idea of this is it's another refactoring
step but it can perform before or after
your you've introduced your power
awesome so it can be done to to get the
program in what we call the shape so you
might have programs are lots of
dependencies you might have called all
over the place
you can use your factoring to break
those dependencies or tied your called
up or to make it into it into a way that
then you can just simply introduce your
skeletons okay it also does are things
like you can you can improve your your
memory usage my transfer at transforming
different data types I'd listen to
boundaries this kind of thing to reduce
copying and it does all sorts of
different different things this is just
a simple example again does anybody know
who this is lots of it small the images
sorry no it's not sighing and painting
John's no clothes Joe I'm strong yes
yeah as a Joe Armstrong is a co-inventor
of early in case anybody hasn't met it
anyway the point of this example is
we've got some images coming in and and
we've got we want to merge the two
images we want to take this image with
this image and put them together so what
we do is we take the first image and we
make all the black pixels white then we
simply in this stage we take each white
pixel in here and replace it with a
pixel from this image so it's a very
simple image merging example so here
we've put the viking hat on on Joel's
head because he lives in Sweden so so it
was nice appropriate so this is the
basic cold you don't really need to care
about what it does not really that
important the main part of the code is
here you have you have three stages you
read an image you do some conversion and
merging then you write it up okay so
it's again it's quite simple so what we
do first is we go through our our and
refactoring steps that we did in the
first part of the talk so I first
introduced a pipeline here I make each
each thing as parallel stage they'll
read convert merge or parallel and then
I do some refinement so then I introduce
the farm so I take the second part
reading parts and introduce a farm and
then I do the same for the second stage
so the result is a pipeline where each
stage is a farm so this should give me
good speed up flight
no so we run this on a machine reason I
have sinking of the Titanic is because
the particular machine we're running it
on is called Titanic a very appropriate
name for for multicore server okay
basically it crashed it didn't work we
didn't get any results whatsoever from
this cold anybody you think why not
locks no no not close but not know so
the reason is its memory bound so this
implementation uses lists in airline
listen along horribly inefficient the
only body has ever used them and the
Hollywood II Men rebound the actually is
not necessarily that they are men we
found in themselves but the problem is
you've got so many images coming in and
each time you pass the image to a worker
you have to copy it so you get huge
replication and in your images if you
have got thousands of images coming and
you're going to end up with millions of
images floating about it's horrible so
you just you just run out of memory
trying to the list it and so you might
think you've got you've got the most
efficient implementation but you can't
run it solves a bit pointless so what we
got is some refactorings that kind of
get that get past this hurdle so example
is Lester binary so you can take these
lists and make them binary and binary
tune Erlanger basically pointers so
instead of copying the list you this
copy a pointer much more efficient you
can convert it to an ets ets is like a
mutable type of thing and you can
convert binary cets it's like an
updatable table and what I'm going to
show you today is just buying the list
of binary so I can go back to my cold
and I can I can take the bits and the
refactoring to we'll just rewrite them
so it'll just take the d list
implementation you write it to a binary
implementation it will do that through
all of the cold wherever it's it's
referenced so it does it to you again it
also does it not only does it do in the
pattern matches but it does it's inside
the differential
bodies as well and the results are
actually much better so I can actually
run it now so what we got here this is
all using a binary version the divided
line here is just using a simple just a
simple to stage pipeline and there's
normal data parallelism they're
obviously a flat miss out it's not very
it's not very good the the next one is
the kind of the blue line that's the
best form that's how I plan with two
farms that's what the refactoring who
wanted us to do it actually it was the
best but we just used the wrong type of
implementation wrong data type and his
other lines the black one was was a
manual implementation but this manual
implementation is not on my uniform it
wasn't just spawning millions of
processes it was done in a reasonably
and controlled way and it's actually
quite close it's not bad and the green
one is a farm where each worker is a
pipeline so it's kind of the opposite
the inversion of the of the best all
it's also not bad what also this shows
is that the tool is comparable to a
manual to a good manual implementation
so not trying to show our tools will
always give the best that they are
within the best if that makes sense so
really the last thing I want to de-spawn
it to talk about today is and how do we
know these things the right so one thing
I've kind of said at the start of the
talk that a tool and in fact thing to
won't introduce cold that's incorrect
that gives you called that's wrong that
won't run and that gives you wrong
results and there's lots of ways to do
this one way but we've tried to do this
is to show it using basically equational
reasoning so this gives us a soundness
this gives us a big aren t or some
guarantee that these things are doing
what they're supposed to be doing I just
wanted to very briefly talk about how
we're doing that so what we've got here
on this slide is some functional
semantics for our skeleton
implementation and by function I just
mean they're simple they're simple
functions so we've taken things like
spawn so our skeletons implant in Erlang
so we've taken along abstractions like
spawn and send and just made a very
simple functional behavior for what they
do so we have things like spawn send
we've got our scale do which is defined
in terms as forms and receives this is
an emitter for a farm and once we've
once we've got these dis base functional
semantics we can basically take our our
sequential program so there's a very
simple sequential what what this is
showing us if you've got some function f
that is applied to some input you can
eventually rewrite it until until you
get a map where you can get a farm so
you can simply do that by taking your
definition of scale do in lining it to
get this aligning that again until you
get the lambda here and so on and so on
and so on and then eventually you get
this this map f at the bottom so you're
basically just a very systematic process
of taking the input and the output from
the tool I'm trying to get from one to
the other doing simple functional move
iting okay so that gives us just a
guarantee that our tool is actually
doing what it's supposed to do you're
very important I'm just going to briefly
mention also I the title of my talk said
heterogeneity but I haven't really
talked about that vladimir and the next
talk will will go into a lot more detail
about how we do and I trogen
heterogeneous computing for airline so
how do we do offloading two GPUs and
things like that part of that work is
that we've extended our skeleton library
so our scale framework to include
offloading so you can also have parts of
your your skeletal pattern that work on
GPUs or whatever device you want and
scale basically does all the offloading
for you so it works out the correct
ratio of tasks to offload and so on and
so forth I let Vladimir go into that
much more detail so we call these hybrid
skeletons for a mixture of CPU and GPU
skeletons but we also have refactoring
that work
with with these as well so this is just
an example here I've got at the top here
this is my my original code so I first
refracted it not using any hybrid stuff
I just do a normal introduced map
refactoring result of that gives me it
gives me a simple parallel program and
then I can introduce a hybrid I can
introduce the GPU component so if i have
a cpu component and i have a GPU version
of that i can simply extend it to be a
map where where it it can dispatch tasks
to both the cpu and GPU worker as well
we're currently working on ways that you
can generate this this GPU component
okay that's kandi in progress but the
rest of the framework is in place and
this is just some very preliminary
results just to show you what our hybrid
versions are doing so here we've got the
blue line it's a simple binary version
this is image merge again this is the
same one where we put the Hat on Jules
head showed you before so the blue line
is just a simple binary version we
slightly improve it with our our binary
to ets refactoring it gives it a slight
improvement but not much but if we use
our hybrid implementation so this is
also using a GPU offload tasks in
addition to using the CPUs we actually
get quite a big big difference here so
it goes from something like that must be
about 12 to 14 something like that so
you get you get that kind of boost of
performance which is quite nice so if
you got if you've got a server with a
GPU in it so you may as well use it to
get some some performance and this can
be the difference between days and hours
for some systems so just in conclusion
and hopefully I've motivated why it
won't use refactoring and we think it's
a great idea because it guides the
program through all the steps they need
to paralyze a program it's not just a
blind that's their tax and cold type
approach it's much more disciplined
it you can have all sorts of nice things
in the tool you can warn a user if they
do something stupid so obviously if you
introduce a pattern that's going to give
you slow down you don't to do that so a
tool will will inform you that you're
doing something wrong it avoids all the
common problems with parallel
programming getting your head in
spaghettini locks with with all these
processes and messages and things just a
nightmare to deal with it helps with an
author all understanding an intuition
with fighting parallel called so go back
to this idea of compilers at generate
cold know the reason you don't want to
do that is because you don't know what
it's doing if you've got production
software you don't want some black box
magically changing your cold it's not
really very good so in fact into is
completely transparent so the tool it
produces it's not him it's not lying
what it's producing skeletons reduced
boilerplate is another nice poppin see I
mean functional program is all about
high order functions you don't want to
have lots of boilerplate code everywhere
and of the whole point of it and so
allows you to concentrate on getting
your program light rather than rather
than its implementation so it kind of
motivates that message of functional
programming nicely think I just fresh up
the funneling side just this one one
point before I finish we need people
basically to help us with this so if you
have any ideas you have any programs you
want to try with our tools anything any
idea suggestions please get in touch
because we'd love to hear from you we
have a mailing list you can join if you
like you can get all sorts of news and
updates on our tool progress we've got
things I developer workshops you can go
to you can help us develop the tool if
you want very very welcome to our people
help us we have tools for Erlang and C++
so if you like of the languages you like
or imperative languages don't be shy you
can talk to us want to hear from you
that's basically all i have to say
anyway but thank you very much for
listening
Thank You Greece do we have any
questions question one thing I don't
understand is it sounds like this could
be really affected by something like
cocaine problem like you find basically
a little local optimum localizable right
so do you guys backtrack and maybe try
things that the cost model does it
predict is the best solution or how do
you call it as duration yeah so the
question was can leave some kind of
optimization or heuristic to kind of get
what what's the optimal refactoring or
the optimal configuration the answer is
yes we do that like we have a paper that
we have one paper that uses lot
hillclimb about Monte Carlo which is
another optimization to get to the
optimal point and also in the process of
writing a paper that uses hill climbing
Monte Carlo London searching various
other techniques and tries to compare
them to see which one's the best and you
can use things like optimization
heuristics not just for choosing the
might be factoring and also choosing a
mapping so I talked before about hybrid
skeleton have cpu and GPU components how
do you know how much of your data to
send to a cpu how much you send to a GPU
is non-trivial problem so one one
example of an optimization heuristic is
was is it can give you back the optimal
mapping you need then that information
can then be fed into the tool
well I'm not being sure the question if
it makes sense but does the do you also
do things like yes we do and I who
described only that kind of stuff here I
believe Wrangler has built in i think it
probably might have fusion mr. fusion
built in but was certainly aware of
those types of optimizations and that
wouldn't be a problem to apply them here
so thanks again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>