<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CodeMesh 2014 - Alejandro Ramallo - Marketing, Big Logic and a Pinch of ACID | Coder Coacher - Coaching Coders</title><meta content="CodeMesh 2014 - Alejandro Ramallo - Marketing, Big Logic and a Pinch of ACID - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>CodeMesh 2014 - Alejandro Ramallo - Marketing, Big Logic and a Pinch of ACID</b></h2><h5 class="post__date">2015-01-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/UnCyO3ypnsM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">yeah I thought you know what has to
decide with historic good morning
nice a nice day so we're going to see
how can I be a silver spikes so Olivia
run myself probably the only one
fantastic
I don't engineering business trajan
degree from Washington University and
I've been playing for liberal to
marketing and technology company mark
global marketing Imagi and then
co-marketing architects as well above
that a light cover startups back in the
day I've been architecting marketing
themselves automation technology seems
necessarily a start occurring with
openstep I know anybody touched one of
those but basically ladies what came
before the mac OS macro is open step
whether they acquire next i was wearing
in world objects from the further
education server they migrated from
objective-c tangela because with java
and never patched j2ee it's got but i
was really interested in something that
someone is it did which was called genie
and world history computing platform
especially java spaces and ignoring is
too little space say worked without work
with commercial
and nowadays in are so smart of
margarito retouching mini Esper's of
what we call it that one time data
mining is intelligence it's called clear
air and also extract system say the only
marketing site in these our company
through roughly 20 slides of the
marketing background so no technology
until slight 30 I think so this is a
whole story of marketing in one slide so
marketing evolved in the last 60 70
years from a very low approach to a high
resolution approach or higher resolution
approaches so we started with mass
marketing so is one message for
everybody people didn't talk about
segmentation that point in time
basically that is only one segment and
that is the whole market and I'm the
market here and I have a single product
on a single message to share with
everybody so is like you know seeing the
world in black and white in a number of
years later brun management appeared and
segmentation appears so this idea of
dividing people based on demographics
social demographics psychographics so
trying to understand the right message
for each one of those now as a marketeer
you can only handle six or ten segments
right because you need to produce six
different messages six different
marketing campaigns so that's a medium
resolution approach and and then CRM
appeared as well which is one-to-one but
thoroughly focus on business to basics
right not really into consumers I'm
going to speak about marketing to
consumers so in the last in the last 10
years internet mobile social networking
have to change absolutely everything in
terms of consumer so the way you consume
stuff is complete different before you
will go to the store nowadays you go to
install with your iPhone and you compare
the prices
what you're looking very amazon ebay and
possibly you go there you touch the
machine and say yeah this is the machine
they want but you're not buying their
you're buying on the phone so as people
we are increasingly more complex and we
have higher expectations in terms of
what we want from brands so a the first
one thing that we want to do is we want
to avoid having content pushed to us
right we want an experience that is
holistic I don't want to have these idea
this is it happen 20 years ago or 10
years ago when I go to the bank I see my
bank account and it says I have 1000
pounds then I go to the ATM and it says
I have 500 and then I go call the call
center they don't know you know what my
balances right so that goes against the
brand experience customization we want
if we now understand that if you can
have my data we really want you to
tailor everything that you do for me
right I want us a message that is
tailored to me I want a product that is
tailored to me so these consumers are
increasing complex and nowadays they
connect to each other now the velocity
that these people have and we have in
terms of spreading messages it's much
faster than the market here rather
market is still using all technology to
split messages yeah they can jump into
your network and that it can do some of
that but again we are against that as
human beings we are increasingly putting
firewalls to protects us from from from
that marketing approach so we are more
powerful than ever before and this is
what what happened is that word of mouth
has become the most trusted form of
advertising for the last 10 years all
the marketing research has still has
been telling base right so the best
advertising is us ask spreading the word
so in the last 10 years while working
for this company we developed something
we call consumer engagement which is a
different approach to marketing instead
of pushing content what about pulling so
what how am I going to do for them for
the consumer to come to my brand and
there are a number of
the key key element surreal to that but
the main one is to use game mechanics to
use mechanics and that changes
everything in terms of platform because
now my business logic is getting more
complex this is not just a fad filling
in the form and having somebody in the
database and then doing some analytical
and push an email now I need to have
different mechanics for different people
when you think about mechanics you can
think about loyalty programs maybe
remember ruffles in some ways but all of
them combined together right we build
that ten years ago very successfully we
tripled a brand in that America by doing
that and we handle we're going to see
Mary Farrell through other slides and I
have data that I collected from Mary
Farrell and that's in my internal to be
but I have facebook profile to have
stituted profile and obviously what
happens here is we start to see the
variety right say I don't define those
data models in in my in my previous
world I I own the data model right and I
can move from transactional data bases
to data warehousing to whatever big data
thing cause you want because I own all
of that and is easy to move from from
from one box to the eye in this case I
don't own the models and they might be
handers of them so the traditional
technology is starting to fall apart
when you try to do that etl how I do etl
ridee extractions former load to gather
information from one store and put that
into a 360 degree view or to consumer
and that's big data and everybody talks
about big vaina and and and and they
when they talk about big data I talk
about these three things right and we
all understand that we're going to see
that we think this is incomplete and
easy and it's based on 10 years of doing
of doing consumer analytics but the one
thing that is very important as it has
been important we understood this you
know 15 years ago actually and we've
been blessed by this is data itself is
not the asset the asset is being able to
execute what I want is a market is to be
able to interact with this consumer
whether I need data or not I don't care
if I could do it without data yeah
I would be the happier so action of a
knowledge is what we're after right and
what is actually knowledge is very
simple you know it's something exaction
if I can act right but people start
talking about now at about action of
action of knowledge and we're going to
give you at risk about that so
intelligent data analysis is a quest for
action of knowledge you can replace that
and say big data analytics if you want
or data mining or knowledge or database
discovery but that is what we want to do
right so every time we do analytics
we're after actionable knowledge because
if what we're after is something that is
not actionable babies there is no
purpose right there is no business value
in it now how we do that usually right
and this is actually based on an ideal
architecture that we came out a number
of years ago and we said in all this
marketing interaction there are two main
loops the first one is the interaction
loop or the execution loop or the
automation loop is an autonomic
controller right i should be able
because i want to engage hundreds of
millions of people i should be able to
have a machine that i provide rules on
the machine thinks right because i
cannot do that with people if i need to
improve on cook a call i need to enjoy
100 million people how do i do that i
cannot have 10 million people in the
call center right or 1 million
marketeers i need to be able to delegate
that to the machine and on the other
hand i have the learning loop you can
call it a big data loop right so every
event that happens every click that the
consumer does everything that he says in
Twitter everything that he says through
all my grandma gated channels should go
into the tulips it goes into the first
one because i need a reaction from me
and it goes into the second one because
i want to learn if this is something
already knew about then there is nothing
to do if this is something new some new
pattern arising from that i need to
transform that into something that i can
push back into the automation loop now
what is the issue well we start talking
about technology and we yeah we can put
a data warehouse here in the
with you we were using a very well-known
data warehousing technologies and bi
tilts and nowadays you can do Hadoop if
you want but the thing is every time I
derive new knowledge new knowledge is
going to be a combination of new
attributes that I need to start
collecting or defining and new rules and
therefore I go into design time I need
to call a DBA so that i can start
shaping the data model right now you can
see how it goes then I need to tell the
guy that is behind the rules to create a
new rule and then by the way there is a
mapping between the rules and database
and there is a mapping within the java
coding between of the database so i have
all our rams i have drools or rule-based
engines and suddenly i end up with six
or seven different server web middleware
right and if you look at that and that
is part of the keynote today basically
what you have is accidental complexity I
didn't want out of that what I wanted is
simple tulips I want an event to go into
my loop if there is something new I
inject the new rule into the other one
and the other one simply execute based
on what he knows but the problem with
actionable knowledge is the actionable
is an extreme sick quality attribute so
knowledge or data is not actionable per
se I need somebody or something to act
upon it and in order to execute this
kind of marketing what we need is a
combination of different quality otra
vez as well and I'm not going to have a
slide on the cap theorem but the caps
our own is behind these right so I have
scalability issues that have consistency
issues only channel brand experience so
if I have 10 different channels in which
your touch points in which the consumer
is going to interact with me how I
ensure that every everything I say is
the same is always on right if you are
on a store and you spend one hour
customizing a shoe for your son you
don't expect the shopping cart to fail
but it does and you will never return as
I did and then you want real-time
decision
the loop the automation loops should be
in real time because we are expecting
real time and by real time I mean soft
real time so less than 450 milliseconds
blink of an eye and then you have highly
connected data so this is not the normal
dataset anymore I have social networks
and I need to be able to use that now
all detergent angles are very bad at
managing relationships and finally I
need it as a marketeer plasticity and
agility right and the plasticity means
agility we touched on the keynote right
is being able to move quickly right the
lead time to market leave time to value
but plasticity is another one that is
that it's been puzzling ass for ages and
is to be able to easily mold this thing
to be able to easily change now if all
my business logic if all my marketing
logic is spread out in Java code in 1000
classes what is the business logic how
can I manage that and that happened to
us so where is this account well that
this can be spread out in 10 10
different classes because in my camp
because of these in my can because of
that what I wouldn't want it to be able
to do is to have all the business logic
print out in a single page so i can
understand as marketeer what's going on
say that is what happens with the
marketing thing also today they lack
this capability to execute so we need to
really use a complexity in this chart
and that's what we set up to do and we
said what we need is big data yeah but
we need big logic we need so my
information model needs to go and follow
that and it needs to be relational now
people interestingly they think that
relational technologies is secure and it
is nothing you know far far from that
even there are some technology that
there are religion and they say they are
not relational because they want to
separate themselves from from note from
from cql databases so we need we
national because relational is a way of
composing deriving data out of data
right and we functions right away the
functional approach
that we can apply today that to the
right in your data that's what you took
you know if you follow some some of
these topics you must have heard about
intentional or derived relations or
views in databases right now that is a
paper call our avatar petty which
introduced something called functional
relation programming that was in 2006
I've got the keynote from the guy who
disabilities is no longer in line I've
got it is not working in my kingdom
because there's no versus if anybody
wants that the papers Steve online and
this paper inspired the guys from a
closure and they and which nikki to
develop the atomic and it inspired us as
well and what you can see is very
similar in their approach but with a
different focus so sick your tables
we've done that no it didn't work is too
is too cumbersome you know you will need
to change the tables or fix tables key
value is not relational documents not
relational objects yeah we thought about
it you know but it's too complex again
it's not flat right object-oriented
graphs again the object-oriented graphs
I call or gently grabs an ear for J for
example that was our first idea and we
really thought about it but then we
realized no we need to stick with that
design this needs to be flat right if I
can start populating properties all over
the place how can I compose things
easily so the two final ideas is entity
attribute value which is what the atomic
closure is using or rdf which is a
resource description framework semantic
web language to connect data so let's
take a look at an entity active value so
if i wanted to make the world flat I
need to go down to the smallest
information component or Adam and that
is eav that was invented even before
relational databases so if i have an
entity that I call 123 and then have
attributes and values so each one of
those rows is a cell in a Renee
database right so your original did was
I would have the consumer table with 50
rows the first row will be first named
the second road would be married and the
third row would be dead of birth what
we're doing here is were flattened out
that table saying no we want the cells
so there is a single table called eav
and i can define everything that i can
define a seat girl in eav but we thought
we said we need to marry those two loops
but we also need to marry the work
because of work with the database so if
the web is a database why don't we
consider rdf an rdf is like UAV with a
minor twist this is not a V this is our
value is an object to what goes as a
subject can I'll also go as an object
right so again this is the same thing
the word thing about obviously audio is
that the users x SD values so everything
is a string but is a tight string as a
value and everything is represented by
every entity is represented by your eyes
or your else if you want right so but
that's not enough right so we had this
this requirement to gather the
information from many different sources
it's someone listen to me
no not sure no worries what I've less
this is not my music which which it
would be more in the metal side of
things so we need to go to information
from multiple sources right so we need
namespaces now RDF wats are that thing
so you find out if I have another color
I can be fine well i got these data or
these days for its customer file but
these dailies from facebook right so now
have a very simple and interesting model
in which i can gather any any type of
data and an integrated so the other
thing and we share and will show that
with with thank you we share that with
with day Tomic is fats are immutable but
we learn these not because of the
philosophical approach that which he he
has which is brilliant but we'll learn
this because we were market is and as
marketeers we need to remember
everything because otherwise we're going
to draw wrong conclusions right so we
needed to have facts are immutable so
these records are we talking about
should be mutable and one more thing we
learn to other things big data it's not
chaste volume velocity and variety is
also variety and validity veracity if
i'm gathering information from multiple
sources what I'm going to have is
incomplete information contradictory
information and by the way as consumers
we lie right and as a marketing I want
to know that and I want to know which
one is a lie which was on and then his
validity right so I am married well for
how long right so having been structure
all those life-changing events which are
key for making a understanding of a
person into a model now I'm not gonna
I'm going to try and tap
veracity what I'm going to show you next
I'm not going to tackle validity because
some constrains i'm going to show you
but this is far as a child as well so we
end up with this this is how our working
for measurement model should look like
so i have a subject predicate object
does an rdf triple i have a graph so i
can have a namespace for each data each
fact that i'm recording i have a
probability degree or veracity degree or
certain two degree or whatever you like
to call it and it's something between
zero and one so he's a weight of this
fact and then i have a transaction ID or
a time step telling you when these was
recorded and this is why i say i'm not
tackling the validity because i'm not
saying from when to when this is body
but it's something that we can do and we
didn't do it so far because it's too
expensive but basically these also
allows you to do temporal queries i can
go back in time and say tell me
everything about mary on monday or tell
me everything I know about Mary today
and this is also a graph so we decide to
create website semantic data space or
LSD which plane as with these are the
others with marketing on acid we've been
dreaming about these we've been doing
sort of eav back in 2003 then I you know
came here I didn't code for six years
but you know these idea was still there
we I've been trying to do something with
RDF and semantic web since 96 which
tells my age but so what is what is it
this is an in-memory distributed
scalable high-level filter on the
directive blah blah blah semantic data
base right so I'm going to go through
that so the record how we how we stole
informations you already seen the table
this is an erlang representation this is
build an airline and a livery of see mmm
so the databases that is immutable or is
it is literally mutable temporal and pro
mystic facts so basically I record a
subject the subject these are the times
I'm not going to
that because you're going to bore to
that but basically subject is either a
URI or an anonymous ID the predicate is
always a URI so a predicate is actually
an entity right so you can talk about a
predicate I can say a predicate ease a
property and ease a subclass off these
other property and I go into the
ontology web languages an object is it
anything right it's not only a URI or an
anonymous ID it can also be a literal
and a literal is a string with a patch
metadata or not and a graph is another
URI denoting a logical partition of the
data the transaction ID is going to be a
hundred twenty abid conflict-free k
order identified this is something that
it was created by Twitter but then
adopted by a company called boundary and
is the process called flake so basically
that gives us a unique universal unique
ID based on the type style finally the
degree is a float between zero and one
denoting your certainty your weight your
and we have another one which I haven't
shown you if you're busy which is a type
because I want to be able to define what
is stored in the database as opposed to
what I've generated on the fly is no
asset is it it is eventually consistent
right he's on a pen on this tour now we
haven't the way we we do this is an
append on this tour is a set of these
statements and by the way we're doing it
really approximates a committee to
replicated this data type is an asset we
have improving that yet and there are
some corner cases that we require more
work but basically we're going to our
strong consistency or stronger
consistency eventually consistent model
it is relational but but it's not secure
so I can apply relational algebra to
these structures I can apply relational
algebra to eav and that is exactly
atomic dance and we use a declarative
Korean language based on the data log
same as atomic because is relational
data log I'm going to touch on that I'm
going to show you a little bit of a demo
so written in Ellen we use basha react
or as our platform we spend six months I
don't know if you know well you probably
know bath show react kv the cutest or
cavies build on react or react Korra is
a generalization of the dynamo Amazon's
dynamo architecture basically what is
behind every no sequel database out
there so Cassandra among to be all of
them are following dynamo now we spend
six months building our own dinama until
we realize that react or was open source
and public and we threw away everything
we've done so it talks about the quality
of what the russia guys have did with
rear court we've got an airline client
we have restful api services we have
binary servers using bird which is
basically Erlang's organization and we
do that because we want to avoid paying
the tax on the erlang side of things in
interpretation and we're working on a
java client on top of bird at the moment
so how you use this thing so you have a
cluster and this is a very simple simple
diagram i'm going to give you the
complex one in a couple of slides say
you have a cluster of these things you
can scale out all modes are equal I
store data indices on that and I can
have rule-based reasoning on top of that
and every time I query I'm currying
either directly database or the data or
the indices or going through rules this
is a more complex one so this is how
react core structure things so
everything that is surrounded by yellow
box is a pattern implemented by react
course so react core is a distributed
computing platform it gives you
consistency hash in hint at end of
gossip to synchronize the state of the
cluster with with all the nodes because
all the nodes are recalled there is no
special knowed it has anti entropy so
when you
replicating data you're going to have
inconsistencies so react or gives you
passive and active and to entropy active
anti entropy is a number of processes
working and synchronizing making sure
that the replicas are in sync and
passive anti entropy happens when you do
a read and if there is a conflict you
might be able to resolve it now and they
manage obviously no liveness now the
interesting thing about react or is that
ok these machines might fail so what
they've thought about and this is part
of dynamo as well is they thought about
establishing another layer of
indirection so you have V nodes are
supposed to notes so all that you
structure all your in our case our
database is a collection of mini
databases called V notes now the vinos
move between physical nodes so if one
machine goes down all these four vinos
will be relegated and that is react or
doing all the work for you now on top of
that you need to be able to do request
handle requests so we implement that as
finished state machines in early e and
on top of that so that is for a typical
api that i'm going to show you read
right remove on top of that we have the
daily log coordinator which is
responsible to pass your data log rules
and queries and go through the request
coordinates to get the data and until we
have the local client the embedded
client i'm going to show you the
embedded klein narrow line in a moment e
and finally we expose that through
through api services so we touched on
react or i'm not going to go into into
into more detail there basically is it
muscle of the centralizers I've told you
all know za recall now the interesting
thing about consistency hatching that
we're going to see is how we use it so a
little bit of code this is how we do
distribute rights so I write a statement
written in many places right so that
right is coordinating the right across
all the replicas according to your
parameters you can just say that I
defined that i have three replicas i can
say want the free to be consistent
otherwise give me an error
there's a three-year of three I want a
quorum I one two or three for this to
succeed or 113 I can do an async right
as well and I get a promise I can yield
the problems later on the same with the
removed nether remove is not is actually
a lie there is no remove in the database
so what you can do is actually flag a
copy of the statement with the degree of
zero and that is basically saying this
statement is no longer true in this
moment in time so windex the data and
week and we ship that into a maximum of
six covering indices in different
collation orders so if you want to match
button match the data by graph and
wildcard scenes SPO then you use the GSP
oh if you want to search all the data in
all the graphs for a single person or
entity you you go to SB OG and you will
give me a value for s and workouts for
vog but all of that is handle
transponder for you I'm going to go
quickly you notice is hatching probably
so nothing nothing more interesting here
now the thing here is that every vinodh
averted represents our partitioning that
ring so that is every time you do it
right we're hashing leaned assists and
shipping those to the right partition
got to go quickly active and entropy as
we mentioned is based on react call we
do actually deviate from the way they do
it because of you they do they have a
key value store what we do is every time
you do a read I'm sending you the data
but I also send the US signature the
signature is the collection of all the
transaction entities this data have seen
right including Tom stones so in order
to get this data from the three
different replicas I'm also saying we
charge on technology that I visited in
order to come up with his answer if
those don't match there is a difference
in vision a there is an issue because
this is issued between the replicas and
you know that you can do something with
it
how do we do reads we right buttons so
pattern is a statement with workers and
variables right and then I do a read i
sent the pattern i get result set i can
do it take it take is a read followed by
a remove and this is from java spaces if
anybody's seen jealous spaces or keygen
spaces this is their their idea so it
allows you to do work flow kind of
implementation so you put something very
in a queue I take it async reason is in
takes you can also define a transaction
LED and you're going to do a temporal
query so give me everything from last
Monday as we said before and there is
another common that is a history current
the basically lists everything that you
know about a statement all the Thompsons
all the different versions that you have
written so every time I write a
statement with the same degree but
difference on suction ID the store is
not going to do anything is I don't
pretend to that but every time I write a
news is the same thing with a different
degree is a new version of that
statement finally I can do simple
collisions and patterns but in order to
do more complex ones we are implementing
conjunctive queries of this level at the
moment so we you will be able to do
joins at this level right it is very
important matching yeah take hitter we
touched on that
so the interesting thing is this one so
we needed to come out with a way of
expressing rules so we thought about
basic using data there is a subset of
prologue and if you combine with many
extensions is a superset of secure so
it's more expressive that in c ql when
it includes negate negation and function
symbols and aromatic comparisons lee
blog is our implementation of that so it
is the cleric logic drumming and korean
language based on the data log but
adapted to the early rdf data values you
can make deductions based on the rules
expressing lee block it works top down
the difference with prologue if you use
probably it works one couple of a time
we cannot do that we need to work set
out of time there is a number of a
family of algorithms known as query
sub-query that actually do that there
are a number of other algorithms to
obviously implement data log but we
decided to go to 44 qsq and in
particular we're using one that uses the
data flow network to represent the
execution of the of the algorithm we
added built-in protocol's and function
symbols we haven't got aggregates we're
going to add aggregate and we haven't
got negation yet now solving negation
and aggregate is more or less a sec so
the solution for both is the same so
that's what we are about to do so this
is a syntax of lib load so what I'm
doing there is reflecting and a piece of
code that is going to help me define the
Ganga foreigner like the creators of
early so i'm going to say that a subject
one is a colleague of the subject to
ethan and leaf both created the same
product and both are not the same that's
the first rule the second rule says
subject two is a colleague of section
one is subject one is a colleague of
subject to and they're not equal the
third one says they know each other is
there a colleague's right
and if I know you you know me is a
reflexive property now if in the next
stage of i'm using autologous i can
actually go and definer a in ontology
web language i can say nose is a
reflexive property so you will drive
that for you so because we're just about
to finish let us quickly do them and
this is where everything can go wrong re
and then you know and usually it happens
with me so yeah so what I'm going to do
here is I'm going to create a client you
know to connect I'm going to define a
table take with table function and
otherwise function that's going to help
me printing out some of the results I'm
going to use a prefix mapping because
your eyes are very long what we're going
to do is map short keys to your eyes
these are called prefixes in in RDF and
and web de Caza joint i'm going to set
i'm going to set a perfect saying Earl
equals two erlang org right so now
instead of writing HTTP orlando or /
whatever i'm going to do Earl as a
prefix neither you're right and
obviously each one of them is another
you're right I can use obviously I can
use whatever scheme you want i'm going
to write that now and hear what I've
received as a context saying yeah in
this case is the first right so i
initiate that i'm going to do the right
again so is five milliseconds and
obviously i'm in a single machine and
running 64 v nodes in a single machine
all of that in memory and this is a
transaction ID is
now i'm going to create a pattern right
so that button is going to say bring me
from comas demo graph anybody who's it
created of anything but I want to bind
that to two variables and now i'm going
to do a read/write so i rep with the
with a pattern and I got back the four
stamens out of written I'm going to do
another button that is not going to work
because i'm going to read who knows who
but nobody knows anybody what have just
written these this guy's career airline
so what I'm going to do now and I'm
going to go quickly because we haven't
got enough time I'm going to define a
rule set which is a rule so that you've
just seen in the presentation right this
one that's a rule set that's a piece of
string now I'm going to do this
i'm going to say i'm going to do a query
i'm going to show you the query here
sorry so these are the qualities are the
rules set that have just created and
these are the queries that i can do so
for example give me who is music a
league of who now there is no data
defining colleague there is a rule
defining click so colleague is going to
be a derived property so I'm going to
say now who is colleague of who i'm
going to use a prefix mapping that i've
said before because i want to use short
URLs there and there you are so what we
got there is that claps know what is
equally with Joe places a colleague of
Mike closes like a league of Robert and
so on so forth right so what i'm doing
here is using the rule to derive the
data now i can actually go there and say
now who knows who
right and again if you go back to the
rule nose is the right relationship
based on colleague colleague is at the
right religion base and Kreator Kreator
is the only extensional property or
extensional fact meaning is the only
thing that I've stored in the database
everything else doesn't exist and
deriving in real time finally they can
do something like who is Joe am strong
and the only thing I've written database
is Joe created earlier but now I know he
he's a colleague of all these guys and
he knows all these guys right so
finest light that is for anyone
interested how we do that so we write
this query language as a string we parts
that we generate a natural synth extreme
we optimize it we're using plenty of
optimizations that you can find in in
all the books i'm going to show you the
book the Bible if you want to replicate
this you can do it it will take you two
or three years and and you need a decent
wife and lovely children to be able to
support you now when we execute at the
moment we have a single monolithic
coordinator doing doing that but it
delegates to the request to different
vinos for the small requests what we
what we're doing now is that we can
identify complex queries within your
rolls but those complex words can be
directly implemented in the vino we call
them conjunctive queries there are many
definitions of conjecture queries for us
a conjunctive query is a join query
exclusively based on extensional data so
data that is stored in the database so
no rules so when you give me the rule
said I will optimize and we will find
within each one of your joints which
part of the joint I can ship to AV node
and I will do like barrel and if you
want to go through this these are the
two books that you need and from here
you need to read roughly 2000 papers
which is what we're in and you probably
just study 10 very well and you need to
understand living more liberal about
hypergraphs I need we in particular
myself I knew nothing about it i
spending month learning about
hypergraphs things like a cyclic hyper
graph algorithms gy your reduction hyper
tree decomposition of hypergraphs all of
that we have implemented in Erlang to be
able to to do this if anyone is
interested in the actual leaves which I
couldn't print in this presentation
happy too happy to go through that and
they're plenty more books that we that
were recovered from from libraries that
are available in
to understand how to do something I
these so at the time so what I want to
do is every time I do the query ache I
should be able to give you another
another set of rules why because I want
to do multivariate testing I want to
test multiple rules right you must have
heard about a/b split and as is well mvt
is multivariate so multi variables in
the a B right multi abs all together so
if i can combine what is the price i'm
going to offer you what is a message
that i'm going over with it right
promotion where is the right product and
i have different rules to define that i
can test by doing the run rubbing you
know everybody that hits the web server
I impact with the evil rule you can
store the rules and if you store the
rules we can compile them and optimize
them otherwise you can provide me the
string as Jesse
so so the pen see if it is an extension
relation a statement that I store in
database or system and I am deriving if
it is assuming there I'm stored I can
miss around with the degree so basically
if I learn that i'm not sure that you
are married what I will go there and
said because because something happened
with you and the brand that you know
some interactions showing me that you're
you're saying one thing on one data set
and you're saying another thing one
another this is what i can do is have an
algorithm that visits all of that and
say well from now on the degree of trust
that i have on this triple is no hundred
percent based fifty percent and what I
haven't shown you all the queries I can
express degrees so i can say give me all
the colleagues as long as the confident
degree is more than fifty percent what
is the next step which is the other
answer to the question is what about
their derived data so for that we need
to implement base algorithms on the
dialogue engine itself so that i can
create and compose probabilities now the
probability of one factor now the fact
is not judging the multiplication of
that is more subtle right because one my
costs the other one so what is it if
they're very independent there is a
straightforward computation of that it
is not independent you need to resort to
Bayes networks or yeah Markov chains or
stuff like that which is something I
don't understand yet but but part of the
team do so basically we need to we need
to move into that direction but if
you're stalling the data you just play
with the degrees right and you can
always combine time and degrees in every
query and when I when I do a query by
degrees always a greater or equal down
so if I say give me everything that is
0.5 degree it means the give me
everything there is about 0.5 degree
okay and the new blog language supports
that as well in the syntax
yes and every time you change is a new
version of the statement remains there
now because it's in memory we're going
to do archiving we do in archiving
process and we go back to disk you need
to find what is a window of time they
you want to what is a history window of
time of the warning memory obviously but
you cannot change a statement what you
can do is write it with a different
degree which is another statement and if
you want to change the subject or object
is another statement so RDF is itself an
immutable model and we are just
following that good
thank you very much guys</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>