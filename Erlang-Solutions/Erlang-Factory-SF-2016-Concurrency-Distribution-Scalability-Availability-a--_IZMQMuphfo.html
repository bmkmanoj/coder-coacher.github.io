<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Erlang Factory SF 2016 - Concurrency + Distribution = Scalability + Availability, a ... | Coder Coacher - Coaching Coders</title><meta content="Erlang Factory SF 2016 - Concurrency + Distribution = Scalability + Availability, a ... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Erlang Factory SF 2016 - Concurrency + Distribution = Scalability + Availability, a ...</b></h2><h5 class="post__date">2016-03-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_IZMQMuphfo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so they always put me right before
lunch or right before beers in the hope
that now will probably wake everyone up
so yeah my name is Francesco you know
being doing airlines since 1994
initiative the computer science
laboratory Ericsson and then later with
with our link solutions and you know
this story or what I'm going to talk
about today is not about the book but
about the whole process of writing the
last four chapters of this book so how
many of you have written a book here a
few of you have yeah so you'll know that
if you go in and write a book as soon as
you're holding the paper copy it's a bit
like having children you forget all of
the hard work you forget all of the pain
and next thing you want to do is go off
and write another one and so I wrote the
first book ironing programming in 2009
it came out in 2009 and immediately we
wanted to go off and then well and write
a book the follow-up book the book on
OTP and you know that project like we
got delayed until 2012 when we finally
are kicked often started writing this
book and when I usually write books what
I do is I take the training material
here at that exclusion which are
developed I start use that those
examples it's taken your quite a few
years to develop them they've been
tested battle proven and I start
lecturing and I continue lecturing and
when I'm done I go back I think of where
did the students struggle the most and I
make sure that text is really clear and
then I start thinking what did all of
the smart students you know what hard
questions did they ask and those end up
becoming side bars or side notes and and
then you know that's it then I throw it
over to my co offer who usually
goes in and removes all of the corny
jokes and and it goes backwards and
forwards a few times and and that's
exactly what happened with designing for
scalability so album yo we went in and
described OTP or the behaviors design
principles and chapters 2 through 12
went relatively fast it took maybe a
year year and a half to do now I then
finally reached chapter 14 now 14
especially in the States it's never a
good number and you know in chapter 14
what I was trying to do is you know this
seems to be this popular belief that by
magic just because you're using airline
your system will never fail and it's
going to stay on linearly to infinity
and well those of you have actually been
doing it no that's you know there is a
myth that's not really the case and I
wanted to go in and you know and bust
the myth you know go in and explain that
yo there is some truth in it the truth
is you know that there is no magic
behind it and that indeed you know all
of the concurrency all of the error
handling all of the semantics we've got
in airline make it much much easier but
they don't necessarily you know solve
they'll remove a lot of the accidental
difficulties you'll come across but
interesting it's still a very hard
problem to solve and so you know the
architecture was that this chat was
supposed to be one chapter on how do you
go in and start architecting your node
and how do you connect nodes together
and I start writing then I write a bit
more and I continue writing and I was
actually looking it up a couple of
months ago and found a letter which are
sent to my editor own in a few weeks
I'll be done with the chapter i'll be
sending it in that was March of last
year so I'll got at least maybe one or
two weeks left that was March of last
year in november i was finally done with
chapter 14 and what happened was you
know writing writing putting down ideas
on how i do things ended up turning into
a two page 80 pages
and there was no yeah there was no
training material it was it was all I
was just doing a big brain dump over
everything I do when I go in and start
architecture in a distributed system and
you know and these 80 pages basically
resulted in all the steps involved in
designing an architect need distributed
system and you're documenting all the
patterns you need to deal with along the
way and then pointing out everything in
to think about when it comes to
resilience when it comes to your
liability when it comes to availability
and so on and you know that's when we
realized after that this chapter was in
fact four chapters that it no no this is
way too much you know and we knew it but
I was stuck weary of steve was stuck as
well we will just break it up into four
chapters and this is what we ended up
breaking it up into you know so first of
all you know how do we go in and deal
with distributed architectures you know
systems which never stop so how do we
make sure that your system is available
even when you know will respond to your
response even with things around that
are failing and finally you know scaling
out and having gone in and broken it
into these four chapters that's where
your things slowly started falling in
place and if these four chapters you
know I'm going to focus on in you know
in this talk now you know and and so
starting with chapter 14 the first you
know chapter one chapter 22 chapter 12
all cover yo you know you saw how you
start with a process and you're making
that your basic building block you give
it a behavior you put processes to get
into supervision trees package them into
applications and you know put your
applications to get in your stand alone
though yep that's fairly straightforward
but and then OTP is always done a great
job at telling us how to architect
everything on a single node what we
started looking at at that point was
your distributed architectures you know
how do we start combining together
distributed you know dis nodes and how
do we make them interact with each other
and we quickly came to realize that
right there in your OTP does a great job
at providing a common terminology but it
stops at one single node
and in the process of writing this book
we were also working with the release
project where we create its optical
decode the distributed airline component
ontology and it's a paper which you can
find online where we you know try to
describe names and give names to
components in distributed architectures
and I'm just going to cover some of them
here but everyone who discovered was
using different terms so some people
were calling think of a free tier
distributed architecture we'd have front
end nodes so that's maybe where you
would be running or your web servers
which you know handle all of your
apparel HTTP requests they convert
everything into airlink terms and pass
them on to back end nodes also well
which we refer to here is logical nodes
and they maybe go in and hit some other
nodes you're possibly even written an
airline or some other language which you
could run a database and so you know we
started defining this architecture and
how these nodes are grouped together how
giving them specific names which you
know would then become resin and then
that's the terminology we start using in
the remaining chapters so you know the
important ones here to think of a
front-end knows so these are nodes which
are usually client-facing you've got
logic nodes these are the nodes which
contain your business logic and the
final you've got service knows these are
nodes which will provide some form of
service it could be a service towards an
external API it could be a database it
could be it could be a cache it could be
a payment system and so once we describe
this you know we went ahead and started
providing a set of steps yeah which you
know I walk through when defining an
architecture and the first step you know
was going and start dividing your
functionality into node types so what
you want to try to figure out is what
responsibilities do you want to give
individual nodes and see it as your
microservices see it as you know
self-contained functionality which
allows you to go in and optimize these
no
was based on the particular hardware
deployment so you could have your front
end nodes which you'll become very
memory intensive because you need to
keep a lot of TCP IP connections open
you could have you know service nodes
where it becomes very IO intensive where
you're storing a lot of data on this for
example and so by breaking it up into
your smaller services you know almost
and the loan services you're then able
you know you give each notice specific
responsibility and you're then able to
start optimizing the hardware or all of
your deployment strategies your network
connectivity and so on so first that you
know with any distributed architectures
you know divide the functionality into
into into node types and what we then do
is we start grouping these know types
together to create what we call node
families so node families are those are
the same type within the same cluster
and then once you have a cluster you can
start deploying multiple clusters now
you're following me here so yep great so
once you've divided in a known internode
family the next step you need to take is
described you know what distributed
architectural pattern you're going to
use in probably ninety percent of all
airline of all cases when when we're
dealing with airline or even a lecture
for that matter the most common and most
basic architectural patterns did the
fully meshed one so you've got your
airline nodes and they're all connected
to each other I mean a fully mesh
architecture can handle I think most
most loads of probably a good majority
of the systems out there now it does
have it does have its problems obviously
as you know your scalability is limited
to maybe 70 to 100 nodes you know works
being done to increase that you know we
heard that from Kenneth yesterday but
you still have certain limitations so if
you're not sure that yeah you usually
start with that you start getting
something out and running so you know
that's one of the patterns another very
common pattern which we're seeing out
there and it's you know thanks to react
or is
the Dynamo principal so you know this is
the slide you've probably seen at every
single bachelor presentation and if you
are at the bachelor presentation and you
don't see the slide you can be sure that
the speaker the presenter is not being
paid or doesn't work for bad show in
this particular case I actually borrowed
from the bachelors not paying me but
what you do with the whole dynamo
principle is that you go in and you
think of a key space so you've got a key
space and with your key space you break
it up into you know 2 to the power of
160 potential keys and you assign each a
certain number of spaces you know a
certain number of keys to a particular
partition and then you should've I'd
these partitions across multiple nodes
so in this case we've divided the
partition into into suitable was it now
into 6032 into 32 no 16 notes sorry into
16 notes and what you do is you have
your particular session key so you have
any unique identifier unique key and
what you do is you hash on that key and
it gives you what we call the v note so
it's it's one of these little partitions
and once you know the V note you'll get
the routing to your physical node on
which that session or that particular
nate is stored and what you then do is
on every physical note you have a
certain number of partitions you have a
certain number of v nodes and by using
your consistent hashing and virtual
nodes you can then take a step forward
and start replicating the data so you
know this and this physical node and i
know'd one will contain v node 1 and V
note 5 you could theoretically could go
in and replicate vinodh one here and V
node 5 here and using this replication
what this gives you is that if you lose
a note you know the requests get routed
to your backup node without having to
rehash all of the nodes which are still
up so you just end up rehashing these
notes but you do that balancing and
consistent across your existing nodes
and you know using this approach you
basically use your react or as a way to
distribute your data as a way to you
have you know fine you get your fault
tolerance and also to go in and allocate
your jobs due to job allocations and
we're in San Francisco but just a few
weeks ago Goldman Sachs went in and
described their whole job scheduling
within governments and how it works and
did they schedule about a million 122
million paths today each task you know
running between three seconds to a few
minutes and they're doing it using react
or amusing exactly this architecture
where all of your kind of react or nodes
are there an act as some form of a giant
switch they received the request and
they will for them to the right nodes
and then what you do is you hook the
different no you're different react or
notes which you run all your business
logic to front end notes so you receive
your front and requests which then
forwards them on to your logic nodes and
four in Rome to the logic notes they
then get executed and then moved on to
the service node so you need a certain
task you pass it on to the service node
you need to store it persistently yo you
pass it to a key value store maybe
somewhere else and using these types of
architectures you know you you basically
start having no single points of failure
and you know your actual core here will
will increase to about you can increase
this to about 70 80 nodes guess is you
know you could probably run Twitter on
10 20 of these notes all of the Twitter
traffic on 10 point of these nodes
Goldman Sachs actually had seven notes
they ran all of their traffic on seven
notes you cross it over seven notes and
it's yeah it's a question of routing
messages job specs your replication of
the session state and by creating then
these islands you know that's where you
start getting all of your heavy
scalability so I think you know with
Twitter you know the biggest issues or
not the routing of the messages it's a
search functionality
serving serving the timelines and you
start doing these in these islands all
around you react or cluster now this is
another star topology this is another
typical you're almost linearly scalable
architecture and I've seen systems you
know with the star topology expanding to
millions of requests per second all you
do is you start adding you know more
service nodes to the tails and you're in
contracting when you don't need them
another you're very common architecture
is you know service the service bus
architecture considered a bit
enterprise-e by some so if you think
that service buses or enterprises just
think in terms of micro services which
is pretty much the same idea you use a
service bus and to it you can then
connect your front end nodes you can
connect your logical nodes and yeah and
the end your front end nodes and you and
your service nodes and so as you're
building a distributed architecture you
know you need to start taking into
consideration now you know scalability
and also reliability these are things
you know I'm going to get back to in a
second but these are things are start
thinking you know how do we ensure that
there's no single point of failure with
this particular architecture how do we
make sure that you know it's not you
know where are the bottlenecks and then
how do we scale another really
interesting distributed architectural
pattern which I've actually never seen
in production and an airline system
we've played around with the ideas but
we've never done it is you know
peer-to-peer I mean this will be the
most scalable distributed system of them
all you think of BitTorrent you know
thick of kazaar where all of the logic
nodes act both as a service server and
as a client so they're all identical and
they will connect to each other and
using this principle we can then go in
and start creating our islands right
here with front end nodes with your
service nose expanding the system as and
when it's needed and once again you know
this is your connections coming up and
coming down and if any of you have ideas
over
architectures you're using your
peer-to-peer come speak to me I'd love
you know I'd love to get some concrete
examples of it so these are I think the
most common distributed architectures to
think of and you know once you've put
them in place the next thing to think
about is you know what network protocols
do you want your nodes so to talk in
between each other with you know your
node families which are connecting what
network protocols do you want your node
families to use when communicating and
know so far you have only mentioned
distributed airline but distributed area
is not always the right approach you
know we've seen you know existence with
mpi zeromq UDP ssl in some cases it'll
go as far as using rest mqp SNMP XMPP
and cutie so there's a large variety of
protocol's you can choose and you can
just pick anything as long as it's not
korba your yeah you're fine in this
particular case I mean this is your
typical you know front end web servers
which are customer facing you so all of
these nodes are you know facing the
internet they're facing public in secure
networks and what you tend to do is put
you know firewalls if you are you still
using physical machines you know that
you you will have separate networks you
know facing your which the clients would
like to communicate and then a separate
network internet so you're actually
physically separating the front denotes
from from the logical nodes in if you're
using your running in a cloud
environment you'll do the separations
logically as well but in this particular
case you know it wouldn't be too smart
having a firewall and that having
distributed airline running between the
front and node and the backer nodes
anyone yet gaining access to your front
end knows even if they're stateless with
then you're using distributive airline
be able to do whatever they wanted with
the back end nodes so what we tend to do
here is just use for example SSL and
then be very careful over what
functionality we allow you know to go in
between the front and in the back end
node so once you've got in and you've
defied you know
what network protocols you're going to
use to connect your nose and your no
families in your clusters together and
mine you could be a mixture of many so
you could use distributor and within
your cluster and then maybe tcp/ip
across clusters as an example once
you've done that you know the next step
you need to think of is defining your
interfaces and beginning to make design
decisions over you know what state and
later models you know over your state
and data model you know what you know
what data you need in what nodes and
what will the interface is in between
these different nodes look like so you
know you might be working from a proof
of concept you might be working based on
stories so you might have user stories
so you know the many different ways of
starting to pin down this particular you
know do these particular interfaces but
it's really important you start doing it
at a very early stage and that's from
the fourth stage therefore step here you
look at and understanding you once
you've done your interface you'll go
back and look you know what data do you
actually need to start duplicating
across what nodes and how and what this
gives us basically is you know what what
this step does is it will go in and
validate that your choice you did in
step one with your note functionality
what functionality you want to put in
the individual notes you know you might
realize that you've put functionality in
this node which maybe requires data
which is in this know that so it might
not always make sense you know to do a
remote call every single time you're
going you know you're going from one
note to another you want to reduce your
data redundancy that's another thing you
don't want to duplicate data across
multiple nodes if you don't have to and
once you validated your design choices
in one where you pick the node types
next thing you need to do start thinking
of standardizing eric guys making sure
that all your AP eyes are very similar
here from one
to another and yeah at that point what
you'll have is here you'll have your
different nodes your connectivity
between the nose interfaces and an
interface which you can start developing
and that's more or less the point where
you need to start thinking of failure so
you need to start thinking of
availability availability be basically
defining the uptime of your system over
a certain period of time and what we
tend to talk about is your availability
being the result of your system having
no single points of failure which I'm
going to get into in a second they have
to be full tolerant resilient and
reliable so you know starting with full
tolerance you know fault or instability
of a system you attract predictably even
under failure so in this particular
example you know the client is going and
sending a request to the front end node
the front end node passes it on to a
logic mode and something goes wrong in
the logic note it could be the network
connectivity towards the logic no goes
down it could be a process crashing in
the logic node it could be the node
itself that had like no crash it could
be the hardware crashing or it could be
a very busy node and non-responsive note
is equivalent to it that node and we
don't know if this node is incredibly
busy so what happens if we either return
an error or a timer but what is
important is that we do return something
back to the client saying yeah I was not
able to service your request or I'm not
sure if I was able to serve as grants
maybe maybe not but sending macro
response sending back not a response
basically makes a system known fault
tolerant so something worked or
something they don't work we don't know
that that's acceptable and you then
start dealing with that issue in the
client itself you're next thing you need
to start thinking about is resilience
and resilience is basically the ability
of a system to recover from frail so you
go in reform from failure so you go into
your client you send your request and
say the front plane crashes it restarts
quickly or another fun thing kicks in
and starts taking over the request so
when the client resends the request you
know the request gets passed on to a
logical node and we get a response back
so you know that's another problem you
need to start thinking you know what you
tend to do is where we say let it crash
what you tend to do in this case is
actually put your effort into the
recovery of your system and you need to
make sure that when your system restarts
your restart actually gives you a
consistent state so that if it's you
know corrupt data or a corrupt state
that you've actually addressed and
resolved the problem when you know we
starting to recreate in your state from
scratch now the third thing you to start
thinking about here is reliability so
reliability of the system is the ability
of a system to function under particular
predefined conditions and these
particular predefined conditions could
include and will include especially if
you're working with the tribute systems
errors or inconsistent data so assume
we're actually going in and sending a
request which gets forwarded to a front
end node this request and you know gets
handled biological node which crashes or
times out the front end node received
that request that and gets told that
look there's an issue happening and
forwards it onto another logic note this
logic note then handles that reply sent
handles the request sends back a reply
and the client has no idea that
something's gone wrong behind the scenes
yeah that's a reliable in service system
so even though something's gone wrong
we've still have service that the
particular request and so you know
looking at reliability when you started
thinking of resilience reliability and
fault tolerance that's where you know
for every interface you you've
implemented in your function you need to
start thinking about a retry strategy so
if you think here just look here we send
the request to the back end node do we
know if the back end node here has
successfully executed that request we're
not we're not sure and we don't know it
could be that it's an incredibly busy
node we get a timeout we
executed on another note which is not as
busy so that it might happen that we've
maybe execute that request twice it
might be that you know this request is
never executed because the note crashed
before it was able to do so and so that
decided as executed was so for every
interface function in all your nose you
need to you need to start picking a
retry strategy by redress strategy would
mean that you know do you want your
request to be executed at the most ones
at most once means you execute the
request and then you fire you forget
about it so you want it to be executed
at least once so you want to make sure
that you've executed it but if you're
not sure if it's been successful yet you
continue until you get a positive
acknowledgement or you know there's a
third one which is exactly once do you
want your request to be executed exactly
once and exactly once is the trickiest
one I'm going to get into it in a second
but exactly once as long as you've got a
network you cannot guarantee that you've
executed something exactly once because
you might have executed it but then you
start getting network issues before the
acknowledgement comes back so what what
you know is that either you've executed
it so sexually or you've left your
system in an inconsistent state and I'll
get back to that particular thing so for
every interface function your nose you
need to pick a retry strategy so exactly
once at the most once or only once once
you've done that you need to start
thinking about your data sharing
strategy what you tend to do with your
data is you distribute your data for
scale and you replicate for availability
so you know this is the strategy you
need to keep in mind and so another very
typical pattern we get is you know
assume we've got an e-commerce service
where you need to be logged on in order
to to buy things what we're doing here
is we're showing to share nothing
strategy with a share nothing strategy
you know client will go in it will set
you know we will log in the login
request gets sent to a login node and
the session gets
or in that particular logic node right
here I know the client goes in it gets
routed somewhere else and you know in
that session gets stored elsewhere but
these two sessions are not shared so
basically they share nothing strategy
now if a fault that happens right here
and you know we send in a request the
logic node fails we forward the request
on to another node assuming explained
one you know the low which receives it
only receive restoring session to it
returns back an error unknown session
forcing the client to log on again and
then recreate a new session on this
other know so this is the share nothing
approach so what you do is you handle
failure often up on a client level so
usually you're by going in and creating
another login request another approach
with data is share something so assume
you know we go in and we only share
sessions across at least one computer so
we go in we create a session and which
gets stored on both logic nodes on both
back and knows and we then decide to buy
a book what we don't share across knows
though is the shopping cart so we could
buy a book here then the node terminates
and when we go in and buy a train set it
gets stored on this node and at that
point yet we only have we've lost the
book you know this node might come up
again we might you know reread near the
book from a persistent storage we might
try to merge the two nodes together
later on that's usually done in
libraries you know we've eventually
consistent algorithms but this is the
share nothing church sorry to share
something approach so that's the second
thing you need to think about then the
third thing is the third approach is to
share everything approach and you know
with the share everything approach what
we have here is an example of your
primary primary replication where we
copy everything we go in we have a
session which is copy with by a book it
gets copied on all nodes we go in we
lose a node you know we buy something
else you buy train set it gets added
here
and then when this note comes back up we
copy over the data and we decide to
remove a book and it gets removed on
both nodes so obvious you know this
becomes your most resilient system but
also the most expensive one to run and
you know did this here is an example of
your primary primary replication in
concert with primary secondary
replication we're in primary secondary
application you can read and write on
one node but you can only access the
data on the second node and you know one
of the things we need to start dealing
with when we looking at share everything
architectures or network partitions and
you know there are many things you know
what happens if we get a partition
network you know we get a book on one
side the train set on the other and then
the network comes back up again you know
we need to go in and start merging these
items and you know depending on the
tools and the algorithms you use you
could use you know ventral consistency
described in the dynamo paper you can
use TRD tease yeah there are many
approaches here on doing it and yeah i
think you know go to one of these talks
you know if you want to dive more into
them but once again these are one of the
things you need to worry about and
finally the something call in the potent
so retry strategies what happens if
you're doing say for example a payment
now what you know when you're doing a
payment you want to make sure that this
payment happens only once so what
happens if we go in we want to do a
payment we send back a reply the
payments successful but the request
fails the response fails back and we get
a timeout we go in and we send a request
again it's now up to the business logic
of your system to ensure that you know
that payment goes through only once and
you know and their various ways of
dealing with it but you're often hear
you notice that it's a duplicate payment
you send back your duplicate reply the
client gets back saying out your payment
went through not a problem and you
you know this is once again a very
fairly expensive approach but its
approach you need to do when you need to
guarantee the consistency of your data
and that's where some people endure
potent scums in which is near the
ability for the same function to be
called multiple times we're changing the
state only once so when you're dealing
with payments for example it's usually a
two-step commit where you go in and you
first reserve the funds and you get back
a unique identifier once you have that
unique identifier you use it to commit
that payment if you lose that unique and
n fo you don't commit that payment
within a few hours or a day then the
reserves you fund you finally get freed
an even simpler example is cashing in a
check you've got a check and you've got
a unique identifier you can only cashing
that check once you try to cash it in
again it will fail because it's once
again you know the transactions already
been made so when you've looked at all
of your you know the consistence and
availability in your recovery strategy
and your data sharing strategy you might
not realize it but you started making
trade-offs and these are trade-offs
which will start appearing in your
system and looking at availability so
Nirvana would be you know to reach this
little point here in the graph no we
want systems which are really strongly
consistent and highly available that's
really yep that's that's the ideal
scenario truth is well you know if we go
for exactly once approach this is
obviously the most consistent approach
you know we know that the date is going
to be identical across nodes but it's
also the least available if you need
consistent data across to knows you
might be forced to take your whole
system of offline to guarantee that
consistency yeah as soon as you get a
network partition you know you need to
take your system offline you or make
sure you've got a hot standby which gets
trucked out immediately with at least
once you know it starts becoming you
know you start getting you know your
data is not as consistent but it becomes
much more available and obviously the
most available in
is that the most ones the whole
fire-and-forget approach you know
fire-and-forget is no typical when you
for example dealing with non premium SMS
you send SMS and just hope that nothing
fails there in between and if it does
fail well you know the system sends a
few million SMS is per day you'll lose a
few you know you yeah you you won't get
blamed for it instant messaging is the
same you know you still go for the most
once you know payments you know that's
the exactly once approach and so this is
you know these are these are the
trade-offs you've done with your
recovery strategy but if you're looking
at the sharing data you know once again
the most available system is to share
everything so it's sorry the most
reliable is to share everything you've
got data copy the more copies of the
data you have the more reliable system
has but that comes at a cost it's very
expensive to actually copy all the data
cross nodes and be it's even more
expensive to keep it consistent and and
that's what it hits into it availability
so if you need strong consistency it
won't be available and just like you're
the exactly one strategy you might have
you take your system offline to
guarantee the strong consistency they
share something you know becomes is not
as reliable so if we're only sharing for
example the session but not the data
yeah we might not have to log on again
but we might lose a book so your
liability is lowered but your
availability goes up and finally the
share nothing is the most available
you're not sharing any data is you're in
effect sharding you lose a few nodes
you're fine but you're losing the data
associated with those nodes and so yeah
so so those are an extra step so for
every interface function you need to
pick your retry strategy and for all of
your data you need to pick your sharing
strategy across the nodes and node
families and you will work in five and
six yo you will most probably go back
and also start reviewing all the
strategies in 124 you know what is the
functionality of knows your
architectural patterns and you're
sharing of data so once you've done once
you've decided about your fault
tolerance that's when your whole
scalability comes into the picture and
the fact is that you know when you're
scanning out you have to carefully go in
and integrate all of this
with the consistency and availability
models and in fact the design decisions
you already made will now go in an
impact how your system scales you know
back in the good old days you used to
have centralized servers and to scale
you scale vertically you got a nice
bigger faster shine your computer and
yeah you waited one and a half years
your program would run twice as fast now
unfortunately this this approach is very
limited you know service can only get so
big and the bigger they get the more the
cost and try to picture no si model
those i model the assignment tells you
how to not go in and scale vertically if
you think of it and what we're trying to
picture here instead is an OSI model
which tells you how to scale
horizontally and your horizontal
scalability is just you're solving your
scalability scalability issues by so
throwing hardware come on you know a bit
of your commodity servers at the problem
or a few more instances on amazon and
the problem all of a sudden you start
getting there with weave scaling
horizontally is that you need to start
distributing your data and you start
distributing with you know dealing with
your network and these already you know
design issues decisions you've taken and
that's why I saying you need you might
have to go back and revisit your
distributor architectures because if we
look at if we look at the whole sharing
of data right here sorry if we start
looking at 2222 you know scalability so
this is actually have trade-offs and
scalability the exactly once approach is
the least scalable of them all so you
know it's not going to scale at all
because you need to guarantee a response
and as soon as you lose your network
connectivity or something abnormal
happens you need to stop at least once
you know you keep on trying until you
get back a response so you know that
that's you know it's least consistent
but it becomes much more scalable and
that the most ones that's your most
scalable one you know you don't need to
store in the state you send off a
request
and you don't need to wait for a
response you don't care about the
response you just assume that it's
executed and if you've lost it well it
doesn't really matter and that's how
your your coverage strategy affects your
scalability sharing data does the same
you share everything it takes a lot of
networking resources you know CPU and I
oh you know to share the data and so it
becomes really scalable obviously it's
still your most highly available share
nothing you know scalability is a bit
better share nothing the show so I share
something it's bit better sure nothing
your architecture well becomes almost
linearly scalable it's in effect
sharding but then again you know it also
hits your availability because if you
start losing data you know depending on
the semantics of your program you start
having problems having you know come
this far the next thing you need to
start looking at its capacity planning
in your system and you know usually are
the only way to load test an airlink
system is using airing itself the same
applies galaxy so you know test be using
beam and what you'll find is usually the
hardest part in capacity done is
generating enough load actually to
saturate your system I mean Francois
this morning was talking about your chat
rooms with your 3 million 4 million
users you can't really go out and ask 34
million users to start playing a game at
the same time and say no failed crash
bang you actually need to start
simulating intelligent usage of your
system and you that you using no tools
often you need to write your own and
what you do with the capacity planning
is you know you start pushing
optimizations two things which are
outside of your control so you start
loading your system and start pushing
all of the performance issues to
external services you're more often than
not you'll have to write also your own
simulators and that will tell you you
know the free things you're looking for
is first of all breaking point what is
the breaking point over system what load
can you actually
managed you know how many simultaneous
users per second can you handle how many
messages per second you know things you
know and you're using these things to
then understand how your system will
behave under extreme heavy load and will
this data you then use it for load
regulation and back pressure so your
back pressure is you know the ability to
reject calls so assume your system you
know that your system will break after
million users what you do is you start
rejecting requests you know if you are
if you have a silica million users
simultaneously logged on and the second
thing you need to think about Israel
decorations and regulation is usually
used done through queuing and what you
do is you start pushing out you know
when you've pushed out you know your
your performance bottlenecks your third
parties what you need to do is make sure
you don't start sinking your third party
AAP is and yet we've had you know we
were low testing some of our VoIP voice
over IP systems and we were testing him
against simulators at one point not
realize we thought we were testing
against simulators have put it that way
and in fact yeah we were actually
connected towards a VoIP provider and
this was your three days before going
live and managed to sink that whole VoIP
providers infrastructure causing them a
problem free 24 hour outage they were
not too happy they kicked us out we were
not too happy because we had to
integrate you know three days before
going I do another VoIP provider but
that's where you start you know don't go
in and learn that you know your your
your third-party api's don't have any
back pressure go in and actually start
putting load regulation here itself and
what you do is you use queuing in load
regulation which will basically handle
your peaks and troughs so if your
third-party API has a service level
agreement of 300 requests per second
make sure that you allow a maximum of
300 requests per second to go through
and what you then do is if you start
getting an overflow here so if the third
party I can't handle enough requests at
this point you start reject
calls you know or requests which which
we various being sent back their clients
ask them to retry later and you know
that's capacity planning at this one
once you've got your capacity planning
you need to start developing what we
call a cluster blueprint and a cluster
blueprint is used for scalability will
describe you know how many fronting
knows you need for every back end node
and for every service node and so when
you start scaling up and adding more
capacity you know you know that if your
front and handles you know a thousand
requests per second and your back and
node two thousand requests per second
you know that for every two front end
nodes you need a back end node and yeah
and at that point if you've proven that
you've got no single point of failure
that means if you can lose any node have
issues you know lose network
connectivity other parts once you've
proven that all of this works you know
go in and you've got your cluster
blueprint you've done your load
regulation you've put in your back
pressure you know frothing requests you
could actually use for load regulation
and back pressure there are your two
really good libraries out there jobs by
old figure and safety valve by just
produce Anderson they're the ones we
tend to use once you've done that you
know the next step is looking at
monitoring and pre-emptive support and
this is this is another different story
I'm not going to get get into it now but
what you need to make sure is you know
you need to have full visibility in your
system you need to be able to detect
that some issues might escalate if you
want you know five nines availability
you need to address issues before they
happen and that means are getting early
warnings of these issues and that's
where your monitoring and preemptive
support come in so you know this is
basically what happened you know this is
what I ended up describing and it ended
up taking six months it sounds simple
now you make your architect knees
assistant if you're simple but you know
putting in it all the forts you know in
the whole thought process which goes
through from you're starting from a
single you know from a symbol node to
actually you know document all of this
ended up taking the time it takes and
yeah the 10 steps no the tendency steps
towards architecture learning system now
includes you know splitting up your
system functionality into your
standalone nodes if your buzz were
compliant you're very welcome to call
them microservices decide what
distributed architecture you're going to
use so you start thinking in terms of
distributed architectural patterns you
want to go react or do you want to start
using clusters peer to peer and decide
what network protocols you want your
nodes your node families in your
clusters to use define the node
interfaces that's incredibly important
because your dhoti interface has become
your entry point to these nodes yeah
that's usually what you're going to test
that's usually what you're going to load
test and define your data your state and
data models across these nodes try to
reduce the need to duplicate data and
well you're keeping full tolerance and
scalability in mind once you've done you
know with all your API is your pick your
restraint strategy so exactly once only
once so exactly once at least once and
at the most once and when you've done
that you know pick your data sharing
strategies across families load families
clusters and types right you reiterate
through all of the previous steps until
you've actually managed to get all of
the trade-offs you need because picking
your distribute architectural
architectural patterns you will have
made trade-off between reliability and
scalability and you need to go in and
find you know the right balance design
your cluster blueprint which will look
at their all your ratios you need for
scaling both up but scaling down as well
and identify where to apply back
pressure and road regulation for your
capacity planning learn how your system
works under pressure before you go live
not once you've gone live at three
o'clock in the morning when you know
America is waking up and yeah and
everyone starts using your system at
least for those living in Europe and
finally differ i define your operation
and maintenance pros what you want to do
is define all your system business
alarms and metrics and make sure you've
got full full visibility and you know
the last piece of advice is don't go in
and over and
air systems you know I know you all want
to implement then you'll become the next
machine zone or the next wats app but
it's not important to get something
which works fast or is highly scalable
you need something which works and once
you've got something which works you
know bring it to the next level and if
you start thinking with these things in
mind you sort of having concise api's
and data sharing strategy you know you
don't need something massive as long as
you've architected it right you know
something small which will work out to
end can easily then be scaled and you'll
give you the resilience and the fault
tolerance you know you've got a lot of
the semantics in airline and their own
virtual machine you know out of the box
which gives it to you so you advice
don't over engineer it don't start
thinking about you know 20 million user
chat rooms with three million users
start thinking in terms of chat rooms
which will work and handle everything
and then from there take it over to next
step yeah and that was it you know this
is the book we've actually started
production now so for those of you who
have been waiting for a copy hopefully
you'll have the dead 31 in about a
month's time for those of you who don't
you can I'm sure you can find it on
BitTorrent if you like to try before you
buy any questions or any thoughts yes
it's every airline project should always
start with a proof of concept if you
want to fail you want to fail on a small
scale and so yeah that's how i would do
the modeling and what you need to
address in your proof of concept is
always your concerns you might have so
if you've got concerns on scalability
your POC needs to show that your system
can scale linearly if you've got
concerns about reliability you need to
be able to prove that it's reliable so
you pull up network cables break
machines or whatnot that that right if
you break the machines yes Mary yes but
yes so yes so that that's really there
are no models I'm aware of and not only
and the reason I wouldn't go down the
model who protect your proof of concept
is that you get so much your data yeah
every system behaves differently so what
might be a bottleneck in one system
might not be an issue in another so it's
very hard to model yeah call ya
yes so true yep and and that's where you
know that's where you're looking at
interfaces and look at network
connectivity you know you need to start
yeah that that's where that mindset
comes in you know assuming that
connections go down and yeah and yeah if
you're yeah yeah yes exactly exactly and
that's right that's correct and I think
that's where as I'm seeing your network
protocols and interfaces come into the
picture so it's not just an airline API
which you know I refer to here but it
could be any standardized protocol and
obviously or your connections could be
secure as well as we're seeing these
days ok I'll be around you feel free to
reach out if you've got any questions
thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>