<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Bridging the Divide (...) - Christopher Brown, Kevin Hammond | Coder Coacher - Coaching Coders</title><meta content="Bridging the Divide (...) - Christopher Brown, Kevin Hammond - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Bridging the Divide (...) - Christopher Brown, Kevin Hammond</b></h2><h5 class="post__date">2014-07-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/BEwto8IG0Bw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Thank You Simon okay so I just want to
start em by giving you a story about
computing in general or what it's like
as computer scientists today we are
still quite in the dark as you can see
here about 30 years ago we had the 386
came out which was a pretty slow device
it's about 12 to 40 megahertz and speed
at the time it was fastest relative
terms at very stall about 10 years later
the kind of the clock speed started to
kind of ramp up so we're getting clock
speeds of about 300 megahertz this was
the introduction of the Pentium it's one
of the first chips that i started to
program on the dawn of kind of this
century that kind of ramped up even
further to about into the gigahertz so
kind of looking at 3.6 gigahertz this is
a pentium 4 2006 so about eight years
ago the clock speed hasn't really
changed we're still kind of about three
gigahertz so but what has changed is
we're getting more cause so instead of
the single core we're now getting dual
core and a couple of years ago we're
kind of starting to get six eight cores
this is very common now in desktop
machines but still the clock speed is
almost the same still about three
three-and-a-half gigahertz and so what
does 2004 look like so this this thing
here is what we call a xeon phi it's a
60 core x86 chip you can actually buy
these off the shelf they're very cheap
so this this one device contains 60
cause that's that's quite that's quite
an increase from the six we had two
years ago last week i was on google and
i and i decided to see what kind of the
current hardware trends were for kind of
tablets and mobile devices this is what
we call an AMD Mullins is very
interesting because it only has four
cause it doesn't have 60 cause i canna
shown you but if you look at look at
this this diagram there's lots of real
estate on the chip that's kind of
devoted to lots of other very specific
things so we have a lot of the lot of
the ship the orange bit is just for
graphics we have certain devices for
phone coding video for networking lots
of memory we have interfaces to connect
them to other chips and so on this kind
of thing is it's very powerful because
it still has lots of processes we can
use so this example the graphics call
alone has something like 128 cause
that's double what the 60 core processor
or that I showed you previously and
these kinds of things are very very
common in gaming devices for things that
xboxes and things like that are very
cheap and they consume a very low amount
of power so only just over 4 watts can
power this thing so you get huge and
computing power for very low power
consumption but what does the future
like this is the present what does the
future like so what one thing to think
about is is is we're kind of going
exponential in the number of course but
perhaps we will see a system with a
million cause none of that would ever
happen perhaps it will and and what
we'll probably see is not millions of
very large x86 type cause we're probably
lots and lots of very small very
dedicated cause lots of hundreds of
thousands or millions are very very
small and processes they're probably not
just going to be scaled versions of
today's multi-core you probably get a
kind of device that maybe has hundreds
of dedicated entered integer units again
hundreds of floating-point unit so this
is kind of like a GPU but perhaps a bit
more enhanced than a GPU you probably
only have a very few heavy weight cause
so only a very small amount of the of
the computer will be and taking up a
very kind of heavyweight general purpose
cause and we'll still get specialist
units for graphics networking
authentication
we may even get some soft core so these
programmable kind of hardware you see a
lot these days I FPGA but the kind of
that the real message is there going to
be massively heterogenous so they're not
just going to be one type of core
package and a chip they're going to be
hundreds of different types of course
packaged together it's probably not
going to be uniform shared memory
abstractions like shared memory in
global variables is just not going to
work but of course we all know that
because we're lying programmers of
course these things are just not going
to be compatible even message passing
just won't work this is just an example
of a machine or two machines that we
have access to at the moment one on the
on the left he is called Lackey it's it
has 32 nodes it has xeon 55 60 2.8
gigahertz and the one on the right it's
called hermit has 3552 nodes each note
has something like 113,000 cause this is
absolutely enormous don't think anybody
has managed to kind of find an
application that actually uses and if
even a small percentage or one core one
node on there on this machine so these
these kind of machines do exist these
huge mega core machines but nobody knows
how to actually use a multi program them
does anybody have an idea what the
fastest computer in the world is anybody
have a slightest clue yet yes so the
fastest computers actually surprisingly
or not surprisingly located in China and
this has over three million x86 cores
it's it comprises imp like 16,000 nodes
as three of those are young fires in it
and over 3 million heavyweight general
purpose cause absolutely huge
I wouldn't like to say when I'm on stage
okay so it's not just about the large
system so okay that's great we've got
systems with millions of course but I
don't think that's really the only thing
that's important today so everybody here
i'm guessing has a mobile phone
everybody has a smartphone these days
the lord spoke to someone yesterday
still either an old nokia brick in his
pocket that was quite surprised at this
here for example your samsung and sony
phones typically contain about eight
course four of which what we call dark
so they switch on and off depending on
the on the energy and the application
usage these are great because you get
great energy trade-offs but you still
get decent performance and basically
we're seeing a world where the only
thing that matters is parallelism this
is the only thing that's kind of
emerging in really coming forward if we
don't solve this challenge but no matter
what we do everywhere else that just
won't matter because we'll be so far
behind that you'll never be able to
catch up so this talk was about really
about for gumming heterogenous systems
so what how can we kind of combat this
how do we how do we do this now in a
line that there are things like bindings
exists or very low level bindings that
we need to use to program these
heterogeneous devices so one such kind
of binding is is one that I've taken off
github it's actually very nice binding
it basically laps up opencl another line
so you can kind of call opencl directly
from the airline the user is required to
provide a kernel that's written in
opencl you can generate some kind of
kernels very simply but it's very
limited to what you can do and
translating it from another line
directly it provides all of the setup
and offloading and things like this you
need to get the get the GPU or whatever
going but you still have to do this
yourself
and you still have to acquire all the
parameters and you have to convert
things to pointers and all this kind of
horrible stuff there's just an example
of what this kind of code looks like
it's not very functional at all as i'm
sure you can agree we kind of have to do
some sets of ping then we have to get we
have to build a kernel we have to find
where the colonel is we have to create
loads of buffers we have to set whether
the read or write only we have to find
context we have to work at bite sizes we
have to set Colonel arguments and we
then have to read sorry in cube buffers
we have to then wait for the device I've
massively simplify this the actual code
was something like three or four hundred
lines just to call very simple GPU
colonel but this is a kind of very low
level and programming that people need
to use so programming at this very low
level is extremely difficult I'm sure
everybody hopefully can agree with me
here these kind of mainstream
programming models I opencl CUDA
pthreads even airline processes are just
too low level as I say here for an
average programmer but I think even for
a program who's an expert the two low
level really difficult to get right and
not just the fact that the low level and
fiddly but many applications don't just
have one way to paralyze them there's
lots of different ways you can paralyze
most applications so spending days weeks
months writing these incredibly low
level and opencl or whatever
applications and then you find out
there's a better way to paralyze it you
have to completely rewrite the thing
from scratch it takes another three
years or whatever and choosing which of
these the best paralyzation to use or to
exploit as incredibly non-trivial doing
it trial and ever so just trying and
just just keep trying look the different
polarizations and turn until you find
the best one is going to take you
forever it's very caustic technique so
what do we want we want basically a way
for people to think in parallel this
really requires completely new
high-level programming
constructs so high level in the sense
that we need to deal with hundreds of
millions of threads perhaps you can't
program while roaming about things like
deadlocks you just have to get rid of
them and the same as communication you
just can't deal with communication so
Erlang is nice because it's concurrent
but you still have to deal with
processes in communication this is too
low level you also can't possibly
program these things without any kind of
performance information you can't drive
a car without the speedometer well you
can but you probably crash into a brick
wall or something the same thing with
this these all all these things need to
be included and abstracted and people
need to get into this mindset from the
beginning that they're really thinking
in parallel so what we have is a project
called palisades which is a three three
and a half year project it's comprised
of 13 partners across Europe multiple
partners are airline solutions we've got
agh elta an Delta soft there's various
of the other partners across Europe some
quite large industrial partners SC CH
and mellanox it's called coordinated by
Kevin who I think is sitting in the
audience somewhere and now our ideal and
powerful asus is to completely change
this low level approach so it's about
trying to think of a way to think in
parallel and the way we do this is
instead of starting at the top we start
at the bottom and by that i mean we
first identify components in our program
so what are the things one of the units
of work that we need to deal with is a
kind of the lowest level of entity like
in parallel parl ism and we're using i'm
going to show you we're going to use
some semi automated refactoring
techniques to kind of do this kind of
stuff for us so even just thinking about
components is not enough we're going to
provide tools that allow people to
identify introduce these things into
their code it's about thinking about the
pattern of the parallelism so it's not
about thinking in terms of spawns
receives kids all this kind of stuff
about thinking about structure what is
the structure of the powell ism we're
trying to achieve typical kind of
patterns are things like map reduces
sure everybody's aware of and then it's
about structuring these components to
make parallel program so taking these
low-level components and then slotting
them in to the pattern that we want and
we're going to take performance energy
etc all into account and we're going to
do this using tool support and then
possibly we might need to restructure so
once we want to be formed a parallel
program we might think actually there's
a better one or we're not using this
type of architecture anymore we're going
to use a different one so then we can
just restructure it and this is all done
using dual support and we're doing this
football's legacy and for new program so
this is not just we're not just taking
cannavaro and contrived examples here we
are looking at quite complex legacy
applications so this is kind of that the
power phrase high-level approach if you
like so we got at the top we've got
sequential cold and here's Earl I we're
also looking at a lot of other different
languages so we're not just targeting a
line here porches is a general one it's
not and constrain to a particular model
or paradigm then in the middle you've
got the sausage machine the powerful a
sausage machine you've got some parallel
patterns here you've got some things
like performance information coming in
here and then at the bottom you get some
parallel cold you get parallel Erlang
I'll see so you put a line in here you
get parallel low-lying out and then
you've got some kind of big heterogenous
system you want to run it on sorry then
you map it to the available resources
yeah so what you're Kevin said is you
can't put C in and get along out but not
yet so what this sausage machine in the
middle is a refactoring tool so this is
a tool that allows you to take your your
sequential code and introduce kind of
annotations or patterns to get to the
parallel version so what do these
patterns look like these are just a few
examples this example here is a task
farm the idea here is you
kind of some some units of work coming
in and then you farm off the
computations on two different cause or
devices so each of these w will execute
on a different machine or a different
thread or process or whatever and then
you wait for them to finish and you
gather than you and so on this is reduce
so it's very similar but the operation
as a binary one so you each time you get
two inputs and then you execute that and
then the result of that goes to the next
stage and you can kind of do these in
parallel each stage in parallel the
pipeline is like a function composition
but in parallel so imagine you got a
number of functions you got some stream
coming in here you apply first stage to
the to the to the unit in any pass the
result to the second stage so while
that's computing the first stage can get
the next result and so on so these have
been around for a long time there's
nothing new about these parallel
patterns have been around since since
the 80s Google Map Reduce I think is
probably the most common example of a
parallel pattern and that's simply just
using a combination so it's simply just
using a map combined with of a juice
there's nothing really complicated about
MapReduce so generally we need to we
need to nest and we need to combine all
of these types of patterns together to
form a complete kind of system complete
parallel system so we've done this for
airline we've created a skeleton or
pattern library for Erlang it's called
scale and it implements them lots of
these parallel patterns you can kind of
think of them as like pluggable
templates so you you want a particular
type of power lism you just choose the
one you want and then you just plug your
bits in its it's fully nestable so you
can nest different patterns together
it's like a DSL for parallelism is one
way of thinking about it you can
download it here the general API is very
simple you just call a function do in
the module scale you pass in the type of
pattern you want this is kind of an
estable thing and then you pass in your
inputs and it handles all the powers and
for you and it's completely implemented
inner line there's no magic going on
here it's not
see or anything it's calling Erlang
processes in the background but it just
provides a nice high level higher order
interface to the parallelism so these
are example this is just a pipeline just
what it looks like here is kind of like
an example of how you would construct a
pipeline using scale you would call do
and but you'd pass in the top level you
pass in a pipe and then you for each
stage you then construct a new skeleton
so this this tuple here is a skeleton
and then you can you can pass in nested
skeletons into the middle and then you
pass in some inputs and the values in
this input are basically streamed to the
stage so the stream to the first stage
and then when this is finished computing
it's resulted stream to the next stage
and so on you kind of get this this kind
of streaming workflow another example of
a skeleton is a farm this is like a data
parallel kind of pattern this is an
example of how you can construct one
it's simple again you just have do and
then you put this time you have a farm
and you pass in your scale your your
particular skeleton and m is the number
of workers so you want to have and more
than one work I hope to get some power
ilysm and then again you just pass in
your inputs so there's a kind of your
top level units of work so just another
example of constructing farms there's
something else we need so I mentioned to
you before we have this notion of a
component so this is the unit of work
this is the computation we want to do in
parallel and we can simply wrap that in
what we call a Sikh so the Sikh stands
here for sequential we can just wrap a
call to our our function the thing that
does to work in a Sikh and then we can
just embed that in a farm so seek is
like it's like a sequential skeleton so
here we have a farm with n workers where
each worker calls worker the function
worker and then we can just wrap that in
the do where we pass in this could be a
list or something like that
and using the right type of pattern
really matters so here's just an example
of some kind of performance results of a
matrix multiplication example what we
got going along at the top is a speed up
so this is the number of times faster
against a sequential version and what we
have going along the bottom was a number
of course we have running the blue line
is excuse me it's an erlang
implementation not using skeletons
that's just using spawns and receives
kind of a very naive way of implementing
this as you can see the performance is
not very good so which kind of shows you
that just spawning things is not really
the right way to get powell ism it's
great for concurrency but it's not to
power l'hel way of doing something and
it kind of falls flat so you kind of get
the speed of about two and then it gets
a bit a little bit more about 4 and 16
cores and it falls down again to about 2
on 24 cause you're kind of not really
using that 24 core machine very well the
the one in the red line is just a task
farm and as you can see it kind of races
up to about speed of about 16 and then
it kind of dips off and goes up to about
just under 22 and then we can try a
different type of pattern this is one
with just simply chunking where you
group tasks together so just a different
type of pattern and you can then get
slightly more performance or almost
almost linear the reason we get a dip
here is because it's a NUMA architecture
so you get two lots of 16 sorry two lots
of 12 so there's just a little bit of
overhead copy and data / that's that's a
hard way she was of them the patterns
okay so for the rest of the talk I
basically wanted to describe to you just
an example of how we can do this the
right way so how we can take an example
a sequential program paralyze it using
our skeletons showing some refactorings
and possibly showing how we can put some
of the work onto a GPU an example I'm
going to use as a nun column E and it's
basically this this diagram is the only
thing that you need to look at here the
idea is is you start you do some
decomposition of some value
you pass it out to a number of ants who
then go away and find some solution
based on that once they've finished you
collect you collect all of their
solutions together you then pick the
best one so you pick the best solution
that they give back and then you repeat
so you give that best solution back and
then you feed them out to the ants again
and you keep repeating and repeating
until some predicate is matched or some
some way of solving the solution is met
okay that's quite simple basically just
bit a function composition with with a
loop but if recursion this is kind of
what the basic sequential code looks
like what we have here is just a
top-level function and colony it takes
some parameters don't need to read what
this does it's not important the only
thing that matters is first we get some
results or we call our aunts the results
of that goes into the variable results
we then pass the results into a next
function called pick best and so on and
so on and then we call them and call me
again so we call it recursively okay so
it basically it's a one-to-one
correspondence to the to this diagram
that I've just shown you nothing nothing
special so what we're going to do is
we're going to use what's called
refactoring Simon give an excellent
presentation yesterday on on refactoring
and we're going to use Simon's tool
Wrangler which is no language factoring
tools it's just an example of the
screenshot this is Wrangler embedded in
Emacs it refactoring is basically a way
of restructuring code so that it doesn't
change what it does or specifically for
our purpose it doesn't change its
functional behavior and it's an idea
that you can kind of refactor and review
your code and then keep keep changing it
so you can keep restructuring it
automatically with very little effort so
parallel refactoring is kind of a new
approach to parallel programming it
allows you well we think it allows you
to think in parallel because it it
really guides you through all the steps
you need to take to get from your
sequential program to your parallel one
it provides a database of these these
parallel transformation
you can just apply it can give you
things like warning messages so if
you're doing something stupid it will
tell you and obviously you can't apply
patterns that don't fit and if they
don't fit you can use things like
profiling performance information and
integrate this into the tool to let it
let it kind of guide you through the
steps and it's much more structured than
just using spawns everywhere which is
not really a very structured way to
program I don't think it helps us get it
just right it's much more functional
nicer way cleaner way of writing
parallel programs I think this is just
an example so here we have very simple
kind of list comprehension so we have a
list inputs for each element in the
input we pass it into a function
composition where we apply f3 and the
result f2 and then f1 so we can take
this code and we factor it into a
parallel pattern where it simply
introduces a pipe and then it for each
of these function calls that just become
stages in our pipeline so this is kind
of one click and you get the code on the
above and it transforms into the chord
in the bottom and it's parallel with one
click and there are many of these these
refactorings available they're all I
think in wrangler now you saw that the
version of angry download I think
includes all of these the things like
pipe introduction they all have
elimination so you can introduce any can
remove the pattern if you don't want it
so the basic patterns pipes farms Maps
trunking all these sorts of things are
available and also undoable so they're
not just reversible but Wrangler has a
really nice undo feature so if you make
mistake you can just revert so let's go
back to our example and so here we've
got our own colony example and we want
to introduce powell ism so how are we
going to do that the first thing we're
going to do the first thing I notice is
we've got the dependency here so results
of this passes into the results of the
next stage so this is like a pipeline
I'm simply you're just going to select
this this bit of cold and then I'm just
going to introduce a pipeline
can do that and then I get this cold as
a result this is the result of the
refactoring tool what it's done is
introduced a new definition called pipe
it simply there wrapped up the call to
the function that I had before but it's
created a little fun to pass in the
parameter and in the next stage it's the
other function so it's just basically
wrap these up into a parallel pipeline
okay so the next thing I want to do is
as I want to farm these because I want
to get some more power lism from this so
then all I have to do is simply select
this this top-level seek this bit see
that I'm highlighting and introduce a
farm and I can simply do that just using
the factoring again so then I get to a
farm here number of CPU workers it
calculates for me so I don't need I
don't have to do that the rest of the
code remains unchanged but what about
the recursion they're still the
recursion is a problem so I still need
to deal with that and but that's okay
because it exists a type of pattern
called a feedback so i can simply just
select the recursive block and transform
it into a feedback and what this does is
it takes the pipe the thing that i
introduced previously and once the pipe
is finished it basically recalls calls
it again until this this function
condition is met and it also introduces
the function condition which which it
derives from the base case of the ant
colony function so it's done all of this
automatically and the only thing we have
to do next is just to instantiate this
so we just make this a call to do
calling we're passing in our feedback
and the parameters and then we return
the result so all of the stuff in red
the refactoring tool is introduced for
us and we haven't had to type any of
this ourselves so what kind of
performance do we get from this quite
decent so again the bottom line is the
number of calls on the machine the top
the line going up at the top is the
speed up what we get is a kind of speed
up going up to distant
10 4 12 cause and then again up to about
14 and then it kind of tails off again
so quite good I think considering all
we've done is just clicked a few buttons
selected a few patterns and we instantly
get pretty good speed up I think but
what about the heterogeneity Ivan kind
of I haven't kind of told you about this
yet what if we were to take this farm
and make one of the workers offloaded to
a GPU instead of a cpu so we can have a
mixture of workers in our farm some are
working on a cpu and one or two are
working on a GPU together that's okay we
can do that as well because we have a
new type of heterogenous farm it's
exactly the same as the old type but we
differentiate it by Asik CPU or seek GPU
and you'd simply need to supply the
worker the GPU worker and you can wrap
these like you did before so you can
build a farm or you have some cpu
workers and some GPU workers together
together with some input just like you
did before and but more than that we can
use refactoring tools again to kind of
generate this so this all this open CL
bindings that I showed you right at the
start at the talk you can get all of
this for free because the tool can just
generate this what it does is it uses
tools underneath like dialyzer to get
types and things of Colonel arguments
and things of that and then it just
generates all of this opencl bindings at
the moment you still have to provide a
colonel but perhaps that will change
some point in the future but it
eliminates all of this horrible tedious
and very ever poem writing of opencl I
think we also have a static mapping
technology that kind of derives the
optimal number of workers so the optimal
amount of CPU and GPU workers you need
to give a farm so you don't need to you
don't need to worry about that so let's
go back to our our result I saw our
example I need to do is just take the
farm this time I'm going to make it a
GPU
but I call a different function it
generates the new function GPU solve
find solution and that function contains
all of the opencl generated code and
calls the colonel so the result of that
is much better and so what we've got now
going along the bottom of the number of
what are called symmetric processes
symmetric multiprocessors he's kind of
correspond roughly to the number of
workers but it's a bit odd on a GPU and
the 30 here kind of corresponds to one
worker it's a bit backward and going
going down to 1 corresponds to I think
28 you can you can get an illusion of
number of workers on a GPU by changing
the the local and global address spaces
i dontoh to bore you with that because
the tool will do that what's interesting
or if we look at the speed up here we're
kind of getting a speed up now of about
just under 30 so before we had a speed
of about 14 and now we've gotta speed up
of 30 you're kind of really utilizing
that GPU architecture so the kind of the
whole point of this is we're really
trying to get to a new kind of
methodology a new way of thinking about
parallel programs the idea is a program
would start off at the beginning they
start off with an application it may
contain powell is Amit makin may not
contain foul ISM it doesn't really
matter what is important though is
identify some structure so they identify
maybe it's a sequential structure like I
did with the ant colonies or maybe is a
function composition with some recursion
or something like this they then trying
they then figure out from that initial
structure what kind of pattern they want
to apply so they've got kind of a
database of patterns they can just apply
to it and these are all available so
they can apply one but more but they
don't just do this blindly because we
have performance information that we can
use the filter to get kind of some of
the best or some of the worst
configurations that are clearly useless
or clearly going to be better
so we can immediately just eliminate
kind of half of them we can imply our
static mapping which gives us the number
of workers so it gives us the number of
GPU and CPU workers for each
configuration and then we simply use the
refactoring tool to choose or I want
that configuration with that number of
workers and it instantiates the program
and then we run it okay very simple and
if it turns out it's not the best one we
can simply just go back so now we go
back to here we choose a different
configuration a different mapping a
different refactoring and we've
instantly got a new parallel program
that might work better if you're porting
it to a different architecture it might
be better for that for example so just
an example of this static mapping that
I've mentioned this is another example
image convolution what we have here is
it's only the structure that I'm
interested in we have two stages we have
an aisle in a P so we read an image and
then we pass that image to it to a
function that processes it so it's just
a simple composition we first generate
all the configurations that we can apply
so all the different types of patterns
or the refactorings we can apply to this
so we get lots of different ones here
the Delta is a farm but the circle is
just a normal sequential composition the
parallel bars are pipeline so here for
example we can just farm we can take the
composition just farm it we can farm the
first stage and then compose it with the
second stage we can take the first stage
composite with a farm of the second
stage we can farm both stages we can
make a pipeline we can make a pipeline
with the farm in the second stage and so
on and so on and so on and I mean it's
kind of almost an infinite number of
possibilities you can of paralyzed
ations you can apply and this is what we
have here in this table so here we have
the different configurations and this is
using performance information in
profiling we can kind of estimate what
kind of performance we're going to get
from this so we can estimate the runtime
and as you can see the number of
configurations that are clearly going to
be better than the other configuration
so some of them are going to be terrible
composing for the sequential version is
obviously going to be terrible can be
five and a half seconds if we have a
pipeline that's going to be slightly
better three point eight if we farm one
of the stage as we get sudden massive
increase to 1.6 kind of three or four
times faster and so on the best one is
if we farm both stages with a pipeline
in the middle we get some 5.4 seconds
which is about 12-something times faster
and then when we run this what we can
observe so here we've got along the
bottom we've got the number of CPU
workers in the first stage going up the
top again the speed up but what we've
got each line corresponds to a different
number of workers in the second stage so
we put the second stage onto a GPU who
put the first stage on to a cpu and each
line corresponds here to a different
number GPU workers in the second stage
and using our mapping we can kind of get
almost almost to the most optimal
solution so you know our mapping and
predicts you want six CPUs in the first
stage 3gp or workers in the second stage
and where the star is you can see it's a
speed-up a 39-point 12 but the best
speed up was 40.9 one but it's it's
almost almost the best but considering
you don't have to do any work to get
that it's pretty pretty amazing so I
just wanted to kind of just to start to
conclude I hope I've kind of really
motivated that this many core
revolutions really upon us we're no
longer living in a sequential world
anymore I don't think so how do I was
really changing it's very very rapidly
kind of in the last ten years it's
really really increased its speed of
change and were already in the megacorps
eva so we already have machines with
millions of cause but most of these mega
call machines are heterogeneities
annuity and energy is very important
because
we're starting to get with smartphones
and tablets we don't have using energy
so we want to be using more of these
small chips rather than the very heavy
weight general-purpose ones and
programming these things at the moment
is just too low levels so concurrency
techniques don't work for parallelism
you can't use a concurrency based system
to get massive parallelism you can get
kind of two or three to a four speed up
or something if you're lucky on eight
cause after kind of a few days hacking
but you just you're not going to scale
it it's not going to work you need to
expose mass Powell ISM and the only way
to do that is to work at a structured
level not at a full cess level so we
think patterns and particularly
functional programming really helped
with this abstraction so I mean this
nice functional high order way of
dealing with Paul ISM is great because
you can control the parallel structure
you can control the threads and the
workers the offloading everything for
you again you can control side effects
particularly in Erlang so you can you
have more ways of controlling what
things are going to be done in parallel
you can abstract a lot of the detail
away you can avoid so many problems
generally as well a fee if you can
identify sequential structure the
parallel results will always give you
the parallel version will always give
you the same result because it's correct
by construction and automation is very
important so having these refactoring
tools really dramatically increase
decreases the amount of time you spend
in development so you're not spending
all your time learning opencl bindings
you're not spending all your time trying
to get them to work working out how to
marshal data I'll to offload workers
you're not spending all your time
working out what's the best number of
workers I need here what's the best of
them will work as I need there and if
all the tools do it for you so you're
kind of a big toolbox of different
things you can use to get the results
you want that's basically more or less
all I wanted to say just 11 notes before
I finish and we are growing a user
community for power fades
so if you're interested in what you've
heard today there's a lot more that we
have going on in the background it's
kind of just a teaser talk really please
join our mailing list you'll get all of
the news items you'll get access to all
of the software we also have C++
software as well as ur lying so
everything I've said today works for C++
that that you interested in that you can
chat to the developers on the projects
you can go to developer workshops and
you can help us track bugs and fix them
which is really important he can skype
at this address and we're also looking
for people who would help us develop
these tools so if you open source
developer and you want to help us please
get in touch we're really really pleased
to hear from you but that's basically it
so thank you very much for listening but
we've time for a couple of questions
anyone yes
thanks Louis for exciting talk um the
question I have is you showed one of the
last pictures I think that was about a
call and we're talking about millions of
course and then you had benchmark up to
30 course and still the best like
performance that God you got was in for
course sir I was wondering any thoughts
on that this yes that's what you're
talking about so what we have here it's
the diagram is a bit that the graph is a
bit complicated so what you have going
along the bottom is the number of
workers in the first stage so what we
have here is a parallel program where we
have two stages which is each stages is
in a pipeline so we have a pipeline with
two stages and each stage is a farm so
the first so what we have going on the
bottom or the number of CPU workers only
in the first stage so what you can see
here so this yellow point corresponds to
we have 4 CPU workers for our and
because it's red we have 3gp workers for
p okay is that clear yes but you were
talking about mega things you know mega
cause and you have 16 focus over there
and still the best one is he's like okay
well we have to start somewhere but the
point is this I mean this is scaling
just I mean even to get 40 feet up it is
still quite impressive in today's
standards of powell ism I think
one day in the better for example and we
we need to do that let sorry so I'm
shouting but when we have three million
cause we can use the same techniques to
for example say well we'll choose the 1
million core system the works better
with the appropriate number of GPUs it's
after two ish we're demonstrating the
principle we certainly hope that we're
going to be using at least one hundred
and thirteen thousand core machine
within the next year we hope that's
going to be running airline or next year
we can be hope to be able to give you
some results from that that we're very
close to your million
so in your front of your first
benchmarks we talked about Numa and that
there was a lack of deer in performance
how do we address Numa problems in your
pipelines so that was probably come down
to the implementation of the pattern so
the pattern wouldn't change so the
structure wouldn't change but the
implementation would so you perhaps have
an optimized implementation of the
pattern that would work for Numa but our
principal was still hold you'd still
apply the same pattern at the top level
there's only the implementation would
change so a program wouldn't have to
think about that they would just choose
I'll choose the Numa pattern instead of
the okay so you're just off normal
number two yeah problems too okay and
related question on just a processor
architectures so for example but C++ or
Java you can set threading affinities
with various libraries you can set
scheduling affinities you can use Numa
control task set to get good process
affinity with the parallelism in Erlang
we don't have those kind of run time
features what two or three features
would you like to see and the Erlang one
time so that we could better exploit
parallelism this way um ok that's it's a
good question yes I mean I think okay so
I mean there's a trade-off with having a
system I go along because it does all
the scheduling for you but it has the
limitations are you can't control what's
happening as much I guess one thing what
one thing that we've experienced up to
now is memory usage that's a big problem
in our line but but we're figuring out
ways to combat that so I guess not quite
sure how to answer your question yet but
perhaps we could adenyl one day we'll
have the answers or we could talk later
about that
sorry guys and it seems like the you
know optimized number of workers and
also this is about refactoring the
colors so it seems like you optimize the
code for specific target systems yes so
you would have to refactor if you switch
between the different mega assistance
they used but that's better than we
writing a code from scratch oh yeah of
course so are there any plans of having
it run time in any way any time I didn't
love having these optimizations like the
number of workers and stuff in runtime
in the runtime yes I mean so one part of
the project deals with dynamic
optimization as well as so would hear
what I'm showing you a static
optimization but also dealing with with
new virtual machine methods that deal
with dynamic optimization and what they
will do is they'll take the component
model that i showed you briefly and then
they will use that to optimize the
optimal load for the optimal balance of
components to the underlying machine and
then it will do this dynamically so at
one time they might change or they're
mites offload different workers to
different available hardware so this
time next year we might show you that if
it's working cool thanks
thank Chris again more loudly okay
thanks Chris thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>