<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Complex Event Processing on Arbitrary Data in Real-Time | Coder Coacher - Coaching Coders</title><meta content="Complex Event Processing on Arbitrary Data in Real-Time - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Complex Event Processing on Arbitrary Data in Real-Time</b></h2><h5 class="post__date">2013-03-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/3zsyjkNnh3o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">then describe how we go event across the
sand and how are you event description
and how we knew event processing and
then have me take those those events and
distribute the debt event crossing
students across nodes and and hardware
and we're going to talk also a very
small not about and the open-source
implementation of the Corvus which is a
probably called America's where we've
taken the kind of the heart of our
product but not necessarily distributed
aspects everybody the event processing
as I said then did we packed it up in an
open source application that we're
getting ready to release here very
shortly and finally because I've had a
ton of questions about them and talk
about our use of SML as a as embedded
optimization language for early let's
talk a little bit about what ten I of
those which is my company to give you
guys an overview of kind of a why we're
doing what we're doing so this is the
one sentence marketing steel for time
this is what we like pitch to people as
we're walking down the hall with them so
ten i keep CIOs and the teams off the
hot seat by transforming the data
centers cloud platforms smart sensor
arrays into autonomic computing
infrastructure that are easier to
operate have few and have few more
costly and scheduled failures this is
our business case for being so what was
that actually from a generic term when
the selman really great from a from a
sales standpoint you're talking to a CI
always like yeah don't like being in
front of my CEOs and yeah they are such
a veiled we were down
for 10 mins that cost 30 million dollars
and so that sounds great but when a
genetic standpoint what does that
actually mean so what it means
fundamentally is that we take existing
known failure patterns and we combine
that with operational data and that data
can come from places like log files and
develop spatulas collect d data key
message dis laws nagios outputs egg come
from almost any point in your
operational infrastructure we can take
center telemetry strange basically time
series data and we can take also like
vegetable documents that have import to
your system infrastructure we take all
of that and we take these known patterns
and we combine and research we are those
patterns to the data we're getting in
real time so that we can take action now
in the very simplest case and for many
of our customers this is the case those
actions are simple lurks there their
emails or there they are pages they go
to an operational person say hey do
something the best cases for the
customers that are more advanced that's
us actually taking control of their
network infrastructure and making a
change to to support continued operation
if you have a simplistic example we
could say that we have a data center in
the state of sin we have a bunch of
products on these racks are routers that
serve the servers that are on the other
side of that rather that are
consistently racks and let's say that
this router is starting to drop packets
and we start seeing cpu load and I
unload on those servers on the other
side of that router start trailing off
and on this side of the router the ones
that are still exposed
we see that load start to trail up so
recognizing that pattern any simple case
saying to a sending an email to operate
state hey this is happening now you're
this bracket is falling out of service
provision a new rack and and do
something about this Oh in the best case
we provision a new rapper selves we
bring that we bring that back online and
we deploy it we test it make sure it's
good and then once that's working we
take you over back off line and then
notify someone to say hey this needs to
be maintained so that's what we're doing
that let's focus on looking at this
quantity of data apply these known
feathers and and then doing something
about them but what this means is that
there are things that we don't do so we
don't do machine learning we're not
looking for unknown patterns in the data
in real time we're not trying to to
surface those patterns as the data
passes pills don't talk about when s
that we do want to do Charlie we don't
need post-processing we don't do that
query we don't do the forensic analysis
of the data we want to let me let me
real tick you like this our goal is to
solve the problem either as it's
happening or before it happens that's
where we find a trans mount of value the
forensic analysis figured out why
something failed is very valuable as
well ton of companies doing that coming
up with those reasons we want to take
them want to do it as it's better we
want to take proactive action such that
the system doesn't actually end up
failing that's the goal so that I
machine learning one thing we love to do
is look at the data post post event
surface new patterns and feed those back
into our system and that's a little bit
further off so given the nature of the
data the better title for this talk
might be complex event processing on
arbitrary semi-structured data in real
time another thing we're not knowing
we're not going in a few processing
we're not looking at
arbitrary text documents to make any
kind of decision like all of the data
that we are looking at is already
structured in some way it's log files
there tend to be we're going to learn
more detailed intend to be structured a
certain way so what do we do when we
start seeing this data first we want to
structure it into something that we can
look at then we want to take we want to
make it uniform we want to make it a
turn it into something we can reason
about turn it into something we can
cheaply and easily process quark-gluon
is a time we're getting a quantity of
data we're doing this pad imagine all
the more uniform the data is the more we
can do it and then we want to process it
so let's start with a long time this is
a very simple law bottom might come from
a moment application let's say we have
an IP address and a space separated and
new line separator we have an IP address
a timestamp an aerco since from an HTTP
log and the file that the user we're
trying to access a straightforward
simple automatic conversion to an
arbitrator term that we can look at is
really simple we just do space
separation turn that line into a term
and do a little bit more so that we can
take the floats and then and the
integers and turn those into clothes and
integers so we can match in an
appropriate way and if we want to we can
add a little bit more description or
actually our customers can supply a
little bit more description that will
tell us that an IP address should look
like a tuple of they should be parsed on
its own and a timestamp should turn into
an airline time tuple and the rest in to
remain but we actually don't want to do
this on hol up we would actually rather
do this we don't want to know about the
data our system and the one we're
introducing here has no insight on what
the data means as no insight on what
what individual terms signify it only
has you put the new technic you give it
inside him
pattern you described and we want to
maintain that we might have as little
inside as possible on the actual data so
we could take data from multitude of
sources and simply match those against
the patterns that that that you
specified so we can get data from a
bunch of different places so we take
data from any place that is actually
willing to give us data and we that
comes into the pattern recognition
system comes into the boundaries of our
system gets parsed into this and close
to the kind of locations where there
this words needed there's a lot of
instruction goes mennonite you're not
going to talk about a lot about that I'm
thinking on the of the pattern a feel
free to ask questions when you get
towards the end so we can data phone
from say apache logs from from the
message log and from arbitrary
application we try to do something with
those with that Vega so we let me step
back to spend so we can lead in a
paternity in the structured data but
it's not uniform like de the Mozilla log
has six or seven fields and some of the
managers and some of them two poles and
then the key message log has other
fields and some of them are atoms and
some of them are strings and some of
them are other things and the same with
a finally we can process debited there
was a bunch of stuff because we want to
connect something that's more consumable
that's more uniform so what do we do we
actually turn these into triples and if
some of you had access to kind of more
traditional machine learning it's
similar very similar concept we turned
the entire term intergraph and let me
process that graph into a set of a list
of tuples of three values in each tuple
represents an element in that graph
represents a part of the term the first
is that we can we generate a unique ID
for the entire raft entire
and this is a uuid that's only property
is that it has a very high probability
of being unique throughout the system
not guaranteed nothing in their stupid
system is guarantee but a high
probability at an element ID that
identifies the path through the graph
for this particular element so if you
think about a thing about turning in a
line term and I'm going to some more
detail in a second if you think about
turning in Erlang term into a set of
nodes set of vertices and edges that
path of the Child breach vertices to get
to the scale of the edge is a unique
representation for that for that element
in that graph so we give it an element
idat then we have the element value for
the scalar types that's just the value
itself for the for the container ties
for the lists and tuples estate of the
quantity of elements that mr. tuple
contains and that something gives us a a
way for us to represent this term in a
uniform manner so I'm going to just
really quickly go over how that the
element ID works so again if you think
of the container types the tuples and
the lists as nodes in the graph as
vertices in your grab the thing that
matters the node type and the position
of the child within that note so what we
do for now is we give we give each level
in the web 16 bits as an identifier the
first three bits and this is so
turbulent easier knowing it's beautiful
the first three bits are the tag types
that we need to know to employ list or
what
good thing it is and then the last 13
bits our position element so the thing
that we found is that for most relying
term signal that we're crossing at least
they don't have a ton of depth so you
might give two levels of def if we
leveled up and four levels of death they
don't tend to go much deeper than that
so we can actually because we need your
16 bits for each level of death we can
actually usually stay under 64 or 128
bits for each element ID and that makes
it basically in most systems a single
instruction for rusty no comparison on
that keeps it extremely cheap at the
downside up we only have three big 13
bits for positional information I
remember correctly and someone who come
in this pretend that means they each
tuple earliest only has about 8,000
8,000 positions that it can actually
represent that works for us now and it's
very easy for us to change that in the
future build represent grass of
arbitrary get at the cost of additional
comparison at the cost of a more
expensive comparison so basically so
basically we turn each other people's in
this we're bringing this tuple
representation up again into a list of
these a set of these of these triples
where we have the generated ID the path
info and todd night in action mode is it
just ends up looking like numbers and
the battery so in this case we have four
elements and the first element is a
tuple and the second element is a two or
three days personas to both for bags an
integer and strength so it ends up
looking like tuple packed information of
four values another typical path
equation of four values and the board
the four values of the scalar
information in the first one another to
put three the three values of scale
information
and the scale is here and of course it's
not space or by the last month now this
has a bunch of unique properties one of
the nice interesting properties is that
we can move around arbitrarily there's
no implicit there's an implicit form you
prison ordering is not important because
we're only matching on elements and as
we see once we get into mass description
this bad seriously affects how much
actual data we have end up having to
move around so let's step back go ahead
and talk about patterns we've we've got
a pattern description language and this
is a very simple kind of trivial
description that I pulled out for
example purposes it's it's somewhat miss
P with the or language termed a
tradition and in this case what we are
doing is we're then find we see these
little ? sort of variables we're saying
and we want to see first off three
values three individual unique values
that all have the same IP address that
are all four four values that have file
information and those files are
different from each other so we want to
see a pattern that has three four four
errors for the same IP four three
distinct files and then we want to see a
lot of bunch of it why you want to do
this is an open question what's good for
illustrative purposes so what do we do
initially you turn these same patterns
glutes enemies the elements of the
pattern into the same triple structure
that we used before with a little bit in
for additional information to to to
identify variables this is my favorite
part of all this this simply becomes a
unification simple unification with our
incoming data string and in fact you can
go to an orb expand bulk and look under
unification and you have like maybe a
hundred lines of code there it does this
exact thing in a prep not sufficient
Amanda but a similar manner to uh to
what it is that
we do so it takes the problem decompose
the session of the solution in the
simple case the not mr. PK selection
very very simple and so we get a few
other this is where I was gonna go on
further we get a few other interesting
bits from from discipline we get cheap
filtering of that we only have to to
look at the data that matters to us and
we get we can get distributed easy
distribution easy pulling part of the
actual unification so it's going to
filtering first so this is our value we
just saw it and I've gone off the screen
there the same exact thing just to do
highlight what we need and so if we go
back up here we can see that here we
only care about four values in fact we
can we care about this one of the
throwaway value we care that this one is
at an explicit value a realized value
and then we need see these two so we can
take that apply to our triples and
initially like on the scale we can cut
that other value down to just a subset
that these are the values we actually
need to move around our system and we
even further further on say that only
values with the scalar need to move
around our system for this particular
pattern e to move toward this particular
patterns and soon and because these are
scalar values we can actually use filter
techniques that are very very very cheap
for us like bloom filter based approach
and their other courses that we use that
that becomes part of the match and we
can sub set the amount of actual
difficult work we have to do at the
consumption area in the borders / system
rather than consuming it in in the
reunification the expensive matching
that we have to do to to realize the
pattern and another thing to to know to
not only be we can do this kind of
filtering based on these kind of hard
scalar values
well so we we end up with a larger
amount of value larger amount of data
looking at a pundit control perspective
from the highest level perspective but
that they'd actually shrinks overall in
the amount of data we have to move
around or our system if you look at this
if you look at this pattern their spears
it's also my train of thought there are
five at least five individual things
that are going on the first three things
are matches that need to happen and they
actually happen in order the first match
things that happen to be battle the
second match piece to happen in D valid
and the third match needs to happen to
be valid and then the fourth-best needs
to happen and it doesn't have any
dependencies at all on the things ahead
of it the whole the whole pattern has a
dependency on all those elements but
between these in the top three there is
no dependencies at all so we can
actually pull it apart and do those in
parallel the same and if we depending on
the nature of the system and whether
this actually makes sense or not we
could actually do those top four in
parallel and apply this as a third
action on top of those matches now
depending on the nature of the day a lot
of other things this may or may not make
sense and this very simple approach we
don't gain a lot by it but this is
concept that they naturally falls out
and indepence the orient way which
patterns need to happen to for which
other patterns in which patterns can
happen in parallel and then pulled up to
the top level pattern that Bulls I mean
almost a thing you feel like a simple
topographic sort of your dependencies
and then displaying those pennies
dissolved into different processes and
inner line into different nodes it
becomes a very cheap very easy thing to
do which you cut a bit of that that's
one of the core values of Y of the
principle of our system that we could do
arbitrary matching on arbitrary complex
events on arbitrary large pieces of data
and a scaling factor for us is hardware
of course in the real world there are
the intricacies that are involved in
that actually be the case but that's the
principle we we try to hold to doing
this kind of come up breaking part of
the data
doing the compilation analysis on the
events and applying those inside of our
our system so let's talk about miracles
this is the view of the phone extremely
quickly this is the open-source kernel
of our system the things that this does
it does the data flattening that we've
talked about sto does the event
description and recognition does the
matching in unification the things that
it doesn't do this there's no um it
there's no distribution involved in this
we're not releasing our whole system
with just releasing the core of our
system and pattern matching the the
parsing the distribution unification as
a library that you can make yourself if
you'd like and we of course using a
district or of our world and all of our
distribution and mechanisms are built
actually around that be so i will today
i will talk about SNL and specifically
milton which is an SML implementation
that we used as a as an extension
language in certain parts of our system
that are critical path parts it makes
sense because it's a CPU bound to
processing bound approach to drop into a
language is a little bit more efficient
or life and there are there the
traditional choices there are C and C++
other ones if you want those are the
more traditional ones we don't want to
use those actually much less C++ and C
for the simple reason that while they
give a speed they don't give us a lot of
protections they don't there's not a lot
of controls in place we if we don't have
to you don't want to do our own manual
man
management we don't want something
that's not going to tell us if we're
we're doing like types incorrectly or
something like that and we must try to
use a functional language nairne than in
a pair of language in any case so we
went out and we explore the different
solutions out there and ended up on SML
and again particularly milton for a
couple of different reasons the one they
one of the big reasons for milton is
that it's only capable of using a single
process you can you can use again
spin-off threads and do concurrent
things in hville too but it multiplexes
those off the 10 s process the 10 s
spread that it is capable of using and
for a lot of application this may be a
downside for us this is a actually a
huge upside so we can use Erlang to
control Milton to to kick off the new
instance of the Milton interpreter when
we need a pin it to the process and have
the total control of our embedded
language total control of the thread
resources its using from relying
invested simplifies the amount of
control we get the other very nice thing
is less a less a feature of Milton much
more feature of SML in general is it's
kind of very nice type system ahem Lee
Miller tie system for those of you that
are not familiar with it you can think
of it as perhaps Erlang having dialyzer
built into the compiler and you have to
satisfy all the the dialyzer warnings
before it actually successfully compiled
so we get that dead removes a whole
class of possible errors for us and of
course the usual things with a
functional imaging of garbage collected
has a really well thought out consistent
type system that can we removes a lot of
problems for us please trim you'll see
integration when the nice things about
Milton is integrating with C and
therefore integrating with airline
that's the method when you're doing a
linkedin approach is really just a
matter of describing the function is
calling calling it no Rapper's you need
to create in see there's no there's no
hoops you have to go through for
integration with their legs very nice
thing another thing for us although we
our optimizations tend to be small
milton this whole program often it's an
extremely aggressive optimizing compiler
that there needs the source for the
everything is going to nepal even the
libraries for itself does aligning and
processing in all tons of optimizations
to get the most performant compiled
thing that it can get so what that means
is that we end up getting the speed of
the very least about camel usually of
c++ and sometimes you see along with all
these other benefit so we give the
advantage of this these low-level native
bits in our line without paying you the
cost and getting actually a few benefits
that wouldn't would have occurred
otherwise so i said i fear i don't do
this quickly but I'm hoping there's
gonna be tons of questions we're on
where I oh where oh where I we're 10 I
oh we're working on a autonomic networks
pensioners autonomic networks American
Eric I'm a technical guide were company
of two people my business partners over
there I'm saying he's the business guy
we won't hold that against them two
months because you use a elapsed
engineer and this is what working on and
i'm open to two questions
so then is completely fine so there's
actually more than just a camel in
Milton once we could kind of narrowed it
down to an ml become the sports but see
was obviously an original choice is the
default 80 was actually in there too
which is not a bad language although
actually digress and go into some some
reasons why that was between o campbell
and ml it really came down to us wanting
the simplest thing that could approach
except solve the problem campbell is
very very very complex if you look at
how the sources to me at the very least
they're they're much more complex the
name all time I feel confident that
within fire department Milton which has
a not a huge community I feel confident
with it I can go in within a few days
make reasonable effort and solve my
problem I haven't much less confident
about that in oakham on it's not simply
it's not the source of that or anything
just a much more complex East the other
thing is that uh Campbell uses own
native of native birds and I want to 12
over that and if I even came up i want
to over that from your lying side I want
this is an extension language not as a a
whole language itself so I have much
less control over that and on the other
line side then I have with with ml once
again limiting it to a single cross
connection works very very well for for
this use case we don't we can't get that
other line I meeting with early you
can't get that with Widow camo the other
thing that kind of bothers me and this
is on a very abstract front is that a
camel basically has a guild a global
walk around this library so even though
you have the multiple processes and
they're taking up system resources
they're not actually currently
processing once they pass into
the real camel library and that debt I
mean it just like other than actually
like a camel quite a bit in one of our
cases one of our early one of our tests
where we were doing week the option
phone camel would've actually been a
very nice feature but in general ml and
the milton serves a much more than a
much better approach there the big new
tenant when we did a little bit of
preliminary research on was was ADA
actually again very good compiler is
part of the DCC infrastructure it has a
reasonable tight system though the
biggest problem for us when theta was
kind of twofold first off its not really
functional and that's perfectly fine you
can do an imperative language obviously
we're all comfortable and C or C++ do
these things native this lumbers models
but we prefer functional language if
possible the other thing is the type
system is complex and it has the feel of
an ad hoc type system where the
implementers of language and I had no
idea this is true i'm giving you the
sense that you get from using it is that
they would come up with rules for a
eight high and then work on some more
types and no we need more goals so as a
rules on them to that and ooo here we
need more moles so it had some more
rolls to meet the problems as they rose
whereas henley the healing velvet Isis
in the cheese no camel and SML and
haskell they came up with a rule it said
here's how tight should act and apply
that rule across the board consistently
and from an understanding what's going
on perspective that's actually a huge
win at least in my mind other questions
sure somebody
some stat for make your calls from so is
there like a common transport format
basically is so its up being something
like a so we actually kind of have a we
abused beneath him structure a little
bit and we end up with end up with
something very like a traditional port
driver where we have the Erlang side and
have the ML side and we were basically
passing messages across those and we got
a very thin-skinned of see that kicks
off the animal path and sets up a queue
to move that day then everything else is
done on the airline side India and the
and the young EML saheb J legs a
wonderful thing but we are using the
native Erlang format it we're not
modifying anything but we take control
of the we take ownership phone from her
line we pull it apart we process it on
and off again and milton has extremely
busy integration so there's no over been
doing that that we do what we need to do
and send my messages to respond that's
what it's an ml they've had their
preference see terms of every text but
you remember in such way it doesn't feel
like that that really is what it is like
she becomes the mutual mutually
intelligible language between the two
the two BRS oh not to be honest as a
ml's out of the MS pages between the two
languages other questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>