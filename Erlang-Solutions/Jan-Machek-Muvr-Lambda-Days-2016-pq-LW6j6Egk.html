<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Jan Macháček - Muvr (Lambda Days 2016) | Coder Coacher - Coaching Coders</title><meta content="Jan Macháček - Muvr (Lambda Days 2016) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Jan Macháček - Muvr (Lambda Days 2016)</b></h2><h5 class="post__date">2016-03-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/pq-LW6j6Egk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">alright well thanks
duction there is indeed going to be
today we're going to fall
on the more exciting scaling of training
of big data pipelines so just to give a
brief introduction of what this thing is
well I decided to go to the gym and I
downloaded these various apps that I
could use to keep track of what I'm
doing and I found them to be absolutely
terrible so all I had to do was type
stuff in and and I hated it so I thought
well let's let's build something that
will help keep track of what I do
automatically so I don't have to type
anything I don't have to you know keep
an eye on a whole bunch of devices or
dare I say use a pen and paper so I
thought hmm how do we go about it well
the first experiment really was to see
whether it would be possible to even
identify what a user is doing based on
some sensors so back in the days i had a
head pebble I thought let's record
accelerometer data so I had a watch on
my wrist and I did some exercise and
well here's what they look like there's
another one here's another one and
another one and another one it doesn't
really matter what they are but even if
you look at an entire exercise session
you can probably spot that well it seems
that three four and five looks similar
there might be the same thing and
equally six seven eight and maybe nine
is kind of similar now what I thought
looking at this data that I've collected
is well you know we can tell what it is
so surely if a human can tell a
difference between a picture like that
surely a computer could do the same
thing too and that's how the project
started and it turned out to be a lot
more complicated than then I thought in
any case here is how we went about it we
did a few we took a few samples from a
training data set so all the data that
I've collected we wrote a little program
and we thought
can we train it so in this case we have
three different samples there are three
different entries from a training data
set and here is how they run in XY and z
axes and we trained a particular
classifier I'll get to which one it was
and what we did with it so we trained it
over 50 iterations and we looked at the
results and this was great I don't fan
tastic look at it this is a classifier
that makes absolutely perfect
predictions on a small data set so we
know terms and conditions applying but
we thought this is great it will work
for sure it weevil but when users
exercise there's a little bit of a
problem and the main problem is it's not
enough to just tell the difference
between exercise and no access to tell
difference between different exercises
but also exercise versus other stuff you
see if the users exercise then once they
finish they just held still that would
be cool because we know that no movement
ninos exercise that's great except the
users don't do that they finish
exercising and then they take other
phone they write text message and they
sit down and they walk around so the
second experiment was can we tell the
difference between exercise and other
stuff you know drinking cups of tea
going to the shops going to the next
wait we did a similar thing right we
have all the exercises as one label and
stuff in between as another label and we
did the same thing and you know this is
a bit more realistic but we thought it's
still adequate so let's build it right
we were somewhat please this is okay so
this should all work right so so we
built this there are a couple of hidden
things and a couple of problems that we
encountered as we were building the app
one of the earlier versions
did all the analysis on the server side
which introduced latency now the latency
was actually really low we had wonderful
code latency was in the order of 500
milliseconds to a second but
unfortunately when you use an app that
should react to your immediate movement
or to the thing you do or to the thing
you see 500 milliseconds is a killer
it's impossible to use imagine an app
that counts the number of repetitions
that you do if it's delayed by half a
second it's you get confused the apps as
you've done three and yet you've done
for right and it's actually quite heavy
so you go what's what's this stuff so so
so we had to throw all of that away and
we had to move all the processing on the
phone so the app that runs on the phone
does all the forward propagation it does
all the signal processing and we moved
the model computation to the seller me
that's where it stays there right so we
have all the data from all the users and
the question that the server side code
solves is can we discover better models
or better parameters for the models that
we can then shift back to the phone so
that it now recognizes new exercise or
it is better at recognizing the existing
exercises so for this demo we're going
to store the test data and the training
data in just CSV files in s3 and that's
what you'll get if you if you pull the
source code and the model parameters
will write them out to s3 as well again
in reality you do something a bit more
sophisticated than that but this is a
good start and we will have a sensor
like this one which will send the
accelerometer data to the phone and we
will do the forward propagation on the
model that we trained and the answer
that the phone will give us is what
exercise is this block of accelerometer
data okay now one of the reasons we do
this of course is in terms of
computation time we are going to have to
pay for that well I say we I will have
to pay for that this is going to cost me
like 20
owns this demo because we're going to
run a couple of high computermachines
for about 10 minutes and no costs money
this stuff is great the users pay for it
it's uses batteries users computers so
we want to shift as much to this side as
possible but unfortunately you know some
things just have to be computed on the
server so in this entire picture you had
to look at this wonderful architecture
right but what's missing well what I
didn't mention is how to find the best
classify and what this classifier might
be how you train it at speed that's a
bit of a challenge so if I have all that
you have is I don't know 10 megabytes
worth of training data great you know it
will take a couple of minutes on a
computer like that if you now have
gigabytes that's a bit of a problem now
to make that happen to actually run the
trading co dat speed you need to build
to the right infrastructure and you
don't want to click you need to write
code that builds the infrastructure so
you can spin it up and tear it down and
so the first three we're going to solve
in the next 40 minutes the last one
unfortunately that's that's your job
you're going to have to go and collect
data and exercise and label so that's
actually quite hard that's actually
really one of the hardest things to do
now the data that you can get in the
github repository is couple of sessions
of my exercise so you can get started
with it but if you want to build an
exercise app in anger this will take
months and months of work anyway so well
let's begin with this tackling this
classify a beast and what it might be so
we're going to start with a simple
multi-layer multi-layer perceptron an
artificial neural network so at its core
it's actually relatively simple thing
you get the vector of inputs and you
have some weight numbers just a floating
point number so a vector of numbers a
vector of numbers you multiply them
together and then you sum them together
and that's what you spit out that's the
result of this one
unit and the unit has some function
attached to it so you might want to do
different things you might want to do
just a rectifier i went to the
hyperbolic tanh sigmoid we'll get it but
but in principle it's not too hard you
combine these into a network so take my
word for it this network does this
computation which if we pretend that the
results are are integers it's an
exclusive or look at you look at ingram
this is fantastic you can in fact
implement anything any nonlinear
function using this setup using this
network so what's the hard thing well
the height thing is not the inputs their
death pretty obvious not the output it's
these little numbers alright so again
take my word for it these are the right
numbers but how do you discover them so
if i have only what are they five six
seven units then it's actually
relatively easy you can try every
possible number if in case of mover you
have something like this that's a bit of
a problem so we have 1,200 inputs
because we take 50 samples per second so
we give we get to second windows or
accelerometer data and we have x y and z
axis so that gives us three hundred
samples in x and sorry four hundred
samples and x four hundred samples in Y
and four hundred samples in in Z so 1200
inputs 1200 numbers and we have quite a
few layers and what we get out of it is
that the outputs where it says score a
score bisco see that's the number of
that those are the different exercises
that we recognize and again what gets
spat out is a vector it doesn't actually
say you know exercise a exercise be
exercised see it's the value at index 0
the value at index 1 2 3 those are the
scores for each given exercise and what
we actually do on top of all of that
stuff right so now that we have this
artificial neural network that takes
1,200 inputs producers as many outputs
and the number of exercises that we
recognize we then run a sliding window
through the over the input that we
receive again two seconds would be too
long remember I said if the input if the
feedback is delayed by half a second
it's unusable so we couldn't possibly
wait for two seconds and then say oh
well two seconds later you've done this
haven't you well we have this moving
window and we move it by 50 milliseconds
into the future and so every slice we
run it through the classifier and we get
the results so we get okay the first
window is label a that's the most likely
thing for it to be exercised a and then
we move slightly into the future time
elapses right so what's the next one and
the classifier might say oh it's a so we
have overlapping ace so it's really very
likely that the thing in the middle is a
and then again 15 milliseconds in the
future and so and so and so as a result
of doing this classification we get
essentially time slices and exercises
what is it given this block of data from
0 to this is now what 750 this was a
alright well let's see how it works
let's actually have a look at something
so we're gonna do swift just for good
measure now i'm gonna do a just a demo
and just so you see what's going on so
this program will essentially load the
accelerometer data i have some stashed
away in a CSV file load them up display
what kind of input gets in and then of
course make make the predictions given a
model and model parameters that I've
discovered somewhere on the server so i
have this magical file that describes
the shape of the network that describes
the weights between the units as you've
seen i computed someone gave it to me
right i have it that's great and i can
do the forward propagation so given this
input as an example so this is 400
values of a particular of one axis i
think this is let's see access to so
this is the z
alright brilliant so 0 x 1 y 3 z and
these are the actual raw values that's
what's coming from a sensor and z points
in this direction so i was moving up and
down perhaps you might notice that the
values are actually quite noisy that's
what the sensors do not what we can do
about it strangely the same noisiness
comes from both pebble old version and
the new pebble color colors pebble as
well as the Apple watch so it seems that
z axis this one is just noisy sensors
are noisy so if in case you want to
build something and you think this is
reliable it i mean accurate and reliable
it's it's noisy it's probably reliable
it's not very accurate anyway so we get
this and then the question is well what
is it right so we have our wonderful
exercise classifier we say ok classify
this data do the forward propagation so
all the data at the inputs of the neural
network all these multiplications and
sums and out comes the result and the
result is well there there it is right
so we've done biceps curl from zero to
hundred and thirty-three seconds that's
quite a while right test lab data what
followed was triceps extension what
followed was another biceps curl and
another bit of triceps extension that's
the entire csv file that i have if I
someone gave it to me as a as a block
this is what the phone would say now of
course we do this in real time as it
arrives now you might be thinking well
that's actually easy to do and you might
be tempted to you know crack open Swift
or anything else and start writing for
loops you know for every input for every
weight do the multiplication I mean you
could but but this is this is a parallel
thing right you have a vector x vector
sum you don't want to do this in a for
loop and it turns out that iOS and I'm
pretty sure it's the same thing for
Android
you can write these operations so these
activation functions that we had you can
write all these operations as vector
calls so this video SPV neg vvx in
horrible names but they do this in
Hardware in special Hardware in special
instructions of the CPU and any hardware
that's that you have installed on your
device and it will do it in one take so
to speak it's not exactly one take but
it happens extremely quickly and it
saves a lot of battery power so for this
if I want to exponentiate every element
of this input pointer I can do it in one
call rather than a for loop if I want to
do add if I want to do a power again one
tick operations so it's really really
quite cool now the downside of doing
this is these are beasts from the days
of Fortran so you get stuff like the
first parameter is an unsafe pointer to
float now you should be worried right
unsafe pointer is not generally good
gets worse c is an unsafe mutable
pointer so you know oh that's our off
right it's because a lot of it comes
down to hardware you have to lay out the
memory in an appropriate way and then
you say okay mister GPU mr. DSP do stuff
and write it to this area of memory if
you get it wrong you get terrible
results as in just random numbers you
get sec faults you get all sorts of
goodness so I'm not sure where the rust
would solve it but you inevitably you
have to come down to interacting with
hardware you know similar things happen
throughout anyway right so mobile app
that that's kind of nice right so we
have we have a mobile app that can
receive the data from the sensors can do
the forward propagation and it can
identify which exercise it is does the
windowing it runs in hardware it saves
uses batteries and it's really quick
fantastic but
I said that someone gave me this
wonderful file that describes the
network and the parameters how do I get
my hands on it that's the difficult
stuff if all that we had were I don't
know six seven units just like in the
exclusive-or example we could probably
try brute force approach just try every
possible number if I have multiple
layers and if I have 1,200 units and
then I have like a thousand units and
then 500 units and then 30 units at the
output there is no way on earth that I
can just try every possible float so i
need to find nearly optimal solution i
mean i would like to find the best one
of course but i just don't have the time
for it so what i really want to do is
apply many different optimization
techniques and say in every step of
training every iteration of trading day
of this neural network the question is
am I going in the right direction do I
go up or do I go down there are many
different ways approaches that will
allow me to discover whether I'm going
downhill or uphill they each of them
suffers from different problems there
are challenges in local extrema if you
think about it right so if I have a way
we function if I can end up in this dip
where the most optimal dip would have
been here most optimal that's terrible
you should um you know slapped me for it
the optimal that would have been here
right but I end up here well that's the
trouble with particularly these
multi-layer perceptrons they are really
computationally intensive but they
generalize very nicely so I don't have
to do any pre-processing on the input I
can almost throw this no easy and put at
it and say give me the result anyway
let's try it right so what do we do we
start with labeled data so someone gives
me these blocks so accelerometer data
and label so this stuff is exercised a
and this other stuff is exercised be and
this other stuff is exercised see and
then I will construct an empty
multi-layer perceptron by empty I mean
initialized at with some I'll say random
but with some weight and then I will try
to fit the model this network to the
data by following some of these
optimization techniques and then once I
have exhausted by number of iterations
I'll evaluate its performance is it
giving the right predictions and I will
use the test data for that so training
data accelerometer data and labels then
I will fit the model and then I'll take
the labeled data from another data set
you don't want to do the same thing
right but they have to be the same
exercises perhaps from a different
person or me again doing them later and
the question I have is is the model
accurate does it give good predictions
amongst other things all right and
there's this whole pipeline right so
I've used big words right but no need to
worry add collection to curated test of
training data this is like you know
wonderful you're running a gallery well
you're not you just need to look at the
inputs that the users are giving you and
say is it even valid input so we we ran
this thing and we had a whole bunch of
beta testers and we found to our dismay
that about sixty percent of the users
are giving us completely wrong input I
mean you think this is impossible and
I've I struggling to believe it myself I
mean the user interface is simple and
then we watch the users actually
exercise and they said I'm now going to
do exercise by subscales they pressed go
and the thing says get ready to do
biceps curls so the users one isn't all
about it and then they did something
completely different I couldn't believe
it instead you come on you said that you
were going to do this as they I changed
my mind so about sixty percent of the
users were giving us completely unusable
data so what to do with it well we threw
them away I mean not the users the data
we you which uses where the culprits and
we just ignore it we didn't tell them
poor guys right cuz oh no you're doing
we're really well you'll hear you'll
have a free app but if you're building a
big data system
and machine learning system particularly
with one one that relies on human input
be ready to delete stuff that'll be
awful input anyway so so we have this
curated data something that we all
looked at I mean we the engineers looked
at and thought this is kind of okay so
what do we do well we'll have to load it
preprocess it somehow so if the data is
in a CSV file we need to transform it in
some way with a need to construct and
train models on the training data so we
might have different shapes of the
network different models and we want to
try out the best one we will evaluate it
of course we'll save the models
parameters weights and biases and we'll
save it somewhere somewhere that the
mobile app can load and then it can give
better predictions and we'll pick the
best model so the best model would be
the completely accurate one now that's
probably not going to happen so pick the
one that performs the best alright so
let's program so so we have you know a
whole bunch of boring code that loads
staff and its processes command line
parameters what we really want to be
able to do is to set up a spark context
so because we will run it in parallel
right we're training multiple models at
the same time for multiple users so we
really want to paralyze it right from
the start so set a basic um you know
prerequisites to even run this code so
what then well we have the pipeline
right on what I said it's machine
running pipeline means that you load and
pre process the data so that's this
thing and that gives us the training
data set so now a collection of matrices
that represent the XYZ coordinates with
the label and we have the test data so
that's kind of cool and then we fit all
the models so we have model templates
which is a collection
of these potentially quite good models
and we'll run them for each of them
through a pipeline and then because this
is a demo will do for each print line
production code you you'll probably want
to use some sort of logging framework
actually only kidding this goes into a
parameter seller but but but for the
demo this is good right so what this
ends up being is the results the
parameters and the performance of each
model and the task is pick the best one
all right so how about this pipeline so
it takes a training data set and a
testing later set and a thing that can
persist it and it can be applied to a
model template and it should return a
persistent model some sort of handle
that says I have saved the model
parameters somewhere in our case it will
be s3 in some other cases you will
probably want a bit more advanced
mechanism so again think some sort of
parameter server all right so what do we
do well clearly we say s21 so we'll
extract the labels and and the data
because these are these are tuples will
construct an initial model so remember
initial model is the multi-layer
perceptron in our case initialized to
some random I'll say random but
initialize some initial values of the
buyer season of the weights and they are
selected at random differently every
time I create a model it starts off at a
different place if I start if I created
the same one every time well that would
be sort of pointless right I wouldn't be
able to pick a better one rather every
time I ran this it would give me the
same result so I don't want to do that
instead I then want to so given that I
have an initial model I want to train it
so a little bit of spark right but this
should all look fairly familiar at least
a scholar people so take the train
examples and labels which is a resilient
data set that might exist on multiple
machines partition it
rubit create these mini batches create
matrices from it associated with the
initial model and then fit the model to
these batches of examples and labels and
return the model that I have now fitted
we want to do the same thing for testing
evaluation rather so I'll take the test
examples and labels and do the same
business except the trained model is now
the trained model right i am now saying
how well does it predict these labels
and we simply evaluate it and finally
because this is a demo i have an eyeball
and save so again real life get rid of
print lines but the rest saves stays
right we have a persister that takes
care of saving all the parameters
somewhere alright one final thing and
then we can get cracking this thing
which just builds matrices and vectors
now you might be thinking well that's
funny what's the big deal right it
builds matrix the big deal is in the
library ND for j is a numerical library
for java so it's a tensor algebra but
the cool that's cool in itself but the
really cool thing is that calling ND for
jade create gives you the right
implementation of the matrix by a right
implementation i mean either for the cpu
or GPU so when I didn't say multiply
matrix by matrix if I have a GPU
matrices it runs on the GPU without me
having to do anything I write normal
code but when i say multiply it can
happen on the GPU if i have a matrix
times vector you get the idea vector x
vector there is one slight unfortunate
thing should probably say that very
quietly if we're recording to those ok
to those of you who are enterprise
program
Enterprise job Spring Framework right so
+ d 4 j use this spring now okay as a
side effect it uses one of the utility
functions which will result in you
pulling in the spring framework as a
dependency I have a pull request don't
worry it's going away so all right so
that this seems reasonable so we should
be able to run it we now have a program
that implements the machine learning
pipeline when I run it its output will
be the model parameters so let's do it
now I could do this or I've just done
right the problem is this machine is a
bit too small to do it it has many CPUs
and all that but I'm now running eight
spark slaves because I have eight models
to train and well I mean never mind all
this input right that's fascinating but
the main problem is yeah I've pressed
see this is this is the problem you'd
expect things to be a bit quicker on
this machine and it's not I mean it's
kind of nice that I'm getting
practically no idle time but I don't
want to wait and there's a problem in in
context switching so if you look at how
much is spent doing my work thirty-two
percent versus how much is spent doing
all the other stuff that spark is doing
all the context switching all the
network traffic between the notes it's
terrible but you don't really want to do
that now an interesting point here to
say is if you're running if you're doing
some sort of machine learning on a
laptop do it in one process it's not a
parallel job do it in one thing running
on all CPUs walk away leave it overnight
don't try to paralyze it so running it
locally clearly isn't going to do
anything useful so let's get rid of it
killaloe java that should do it so
flushing all transports that's all gone
that's all gone wonderful now
what do we do I have a slide and then
then we'll do this training locally a
doesn't work let's go to the cloud this
is what we really want if you have a lot
of money know if you have a lot of data
if you have a lot of competition to do
so tick tick if you have little time
take because we only have about 120
minutes to go and if you have a lot of
money now you know in this case it's not
going to be a major deal but when we
actually run it on our production data
it takes actually quite some time and
you know a run costs us to the tune of
hundreds now all right so so if all of
this is true then we're going to run
this oh one more thing before I do it's
empty just as a I'll refresh just so you
believe me we have an ec2 account with a
credit card attached to it and there's
nothing all right all right so let's
create something
excellent all right so we're going to be
creating a few things ten of them so
hopefully AWS will cooperate and our
internet connection will cooperate so
let's see okay this is now costing us
money we have ten slaves we have one
master and ten slaves so what I need to
do now is spend about eight minutes
stalling because these slaves will have
to be created we started them from
scratch there was nothing we created
these machines they will get spark
installed they will get the right
version of all the blast libraries that
implement all the linear algebra
functions they are compiled specifically
for those machines so these are sea for
a text larges so you know the biggest
machines we can get our hands on so they
come with a particular CPU we can
compile the blast libraries which
implement that goes into days of Fortran
write the linear algebra operations your
matrix time matrix vector times like to
that sort of business but these can be
compiled specifically for that CPU so I
have a particular version that matches
that particular machine so it uses all
the instructions that are in the Z owns
that are in those machines if you wanted
to do GPUs you'd pick the pudgy for
instances just a word of warning they
like to kernel panic quite a lot so
there's a bit of a hoo-ha it's going to
be fixed soon so I keep hearing anyway
right so we started with these machines
from scratch we need to install spark on
it we need to install JVM on it we need
to install all the monitoring tools
that's that's happening right all of
this stuff is is happening so let's
let's go back to my slides where I can
take some time to stall and buy them by
next two minutes they should be ready so
what we have is this again I've code in
the mobile that receives data from the
watch that you've seen 1200 inputs
400x400 why 400 said it does all the
forward propagation so
it immediately tells the user what's
happening it can give immediate feedback
which is really really really important
if you're doing something that does
stuff that users interact with it the
users see so if you're doing computer
vision it has to be immediate you cannot
delay things if it's exit it's even
worse with exercise actually because
imagine you know the user exercise is
actually quite hard and manages to do 10
reps and the app says 70 you don't want
to be anywhere near that user anyway so
that's all happening here the app
submits the data and the label data so
the users will say what have I done and
it ends up as CSV files in our test and
training data set from that we have
written our little machine learning
pipeline thing that constructs a
multi-layered perceptron it then trains
it and it saves the model parameters
again in s3 that's the app downloads now
this isn't production right in reality
you want something a bit more juicy so
the app doesn't of course submit stuff
to s3 that would I mean it could but it
doesn't the real app interacts with
annika cluster that holds the
information about the state so think
about user profiles where I ask the user
what is your name doesn't really matter
but it's age bracket weight height sex
which allows us to cluster the users and
to train models for each particular
cluster of users because they move in a
different way and the data doesn't end
up in s3 it ends up in Cassandra so
again we have a couple of machines that
store all the accelerometer data they
don't end up as CSVs they end up as
proper column structures in Cassandra
split into test and train of course the
model gets trained so that sort of looks
the same more machines different kind
but very very similar the output again
ended up in Cassandra so we don't want
to save these files and
the question will be which one is the
best if we had just CSV files we
probably struggle a bit and the mobile
app can request them from the Aqua
cluster so that sort of makes it a bit
more complicated but if you compare it
with what is being demonstrated today
it's not that terribly different
hopefully anyway so this is what exists
and now I have demonstrations so we
should be in ssh ready state okay that's
not good that's not good at all well
let's let's keep going we're still
connected so that's good we'll get it
we'll get somewhere so next up I have
this s3 bucket which contains the test
and training data set so so you can tell
this is all the CSV files that contain
label data so 50 samples per second
actual value of XY is their acceleration
together with what exercise is it and we
use that in our pipeline to actually do
all the training so let's go back to a
distributed ml profit i mean this will
all be wonderful right so let's not get
too excited and see how this is
progressing so this is looking good
machines exist that's kind of nice but
this isn't I wonder if the if our
connection is blocking something if it
is then we're in sort of trouble because
you know let's see could I could I let's
actually try it out right
it will reject it because I'm not going
to give it the right keys yeah this is
this is firewalled to death I'm afraid
that's that's just not happening well no
need to panic I have one question AV
people can I borrow your Ethernet cable
let's see all right so I have an
emergency option I'm told that this will
work so go straight to the source now
let's see at least you know it's a live
demo ok so how are we doing actually I
have no idea how to read all that stuff
let's disconnect from Wi-Fi and see if
we're still connected okay so something
is still happening that that's
encouraging so I'm going to probably
have to restart the script I mean that's
the that's the kind of cool thing on
about AWS right so they're gone let's
make a new set yeah you stop
wait for these to be gone all right
let's start again so this is unfortunate
because we've allowed now lost five
minutes but here we go okay here's one
we'll have a few more in just a moment
ago okay
so we have about according to my
timetable about five minutes to ssh
ready test a state so I'll take your
questions in the meantime I know this is
slightly unorthodox but let's see if
this gets to somewhere useful in the
meantime yes thank you I was you know
reading stunned silence yes is there any
code on github or will be available good
good question um yes there is a lot of
code on github as it happens okay okay
you meant for this project all right all
right good news there is so you have the
thing that I've shown you all that
you've seen today is here so you have
the iOS app in its entirety everything
from the user interface to the
interaction with the devices we have
pebble we have Apple watch with code
that runs on pebble code that runs on
Apple watch so you can start with that
in the latest code you will probably
need the latest Xcode so the beta
whatever it is you know Steve Jobs
reaching from the grave with the hand of
death saying this is deprecated we won't
compile it now you get the mover
analytics which is the Scala code there
in fact there are two things you can try
the python experiments so we have an
experiment in neon that you can run
locally based on the local training data
and we have the spark code that you see
in front of you we have the code for
pebble so this takes you back to well I
mean you know let's let's look right
this is C and a see with very
restrictive runtime environment if you
build code for pebble you get about 16 k
of memory for your program and data so
you know don't malik stuff it's just
static bytes that you need to play with
you get no logging and no memory
protection so this is this is great
programming
when it crashes it crashes so if
anyone's done any embedded programming
the pebble has a watchdog timer which
serves as a kind of an OS and if you
don't reset if you have a like a loop
that doesn't reset the watchdog timer it
will kill your app and so the pebble
will say your app has crashed you press
the ok button and it replaces the memory
that your app lived in with the memory
of the pebble OS and you've in effect
rebooted what went wrong who knows is
there a way to find out no there isn't I
mean there is you debug by app blog it's
pretty grim so you get that pebble
unfortunately doesn't come with any
testing framework so we had to write our
own which is kind of nice until you get
to let's see if I can pick it up until
you get to the real horror of actually
sorry of actually using it which
includes a lot of reinterpret costs so
if anyone's doing C++ reinterpret cast
is the magic I know what I'm doing just
believe me lalalalalalala here have some
memory so that's that's there as well so
you can amuse yourselves with that this
is the occur cluster that I didn't talk
about so user profiles distributed
domain cluster char doing multiple JVMs
that's baked in many other talks that
are delivered on this this I should this
should come with a warning this is a
failed attempt this is a terrible idea
we thought we'd have a common library
that can be that we can then run on iOS
as well as Android and we've written in
C++ thinking we can surely we can
compile it for Android we can compile it
for for iOS and turns out that well
technically we can but none of the fancy
stuff is available so all of the vector
operations we can't do and try as we
might you know C++ 14 a very fancy where
I'm terrible C++ program right
I mean it's very complicated language so
this attempted to do a lot of the signal
processing and it ended up oh yes if
anyone's doing matlab knock yourselves
out so we've done all the code that
you've seen and swift in c++ and really
don't do it it's a terrible idea and
again it's full of this sort of horror
which I suppose is Swift's unsafe
mutable pointer so it doesn't really
matter you have to do this dirty stuff
at some point anyway so that's also
available and final thing that's
available is mover open training data so
there's a very small subset of the
training data so if you want to build
something completely different say you
don't care about mover at all you want
to try your own exercise data analysis
there is some data that we've collected
that you can that you can try out all
right so I've stole enough let's see if
it failed it failed and I will thank you
for your attention but hopefully it has
oh that's good that's good that's good
that's good so we're now installing
everything yeah okay not too bad not too
bad 50 wow that's good okay our studio
who's doing our hey there you go it's a
horrible language isn't it nevertheless
you get support for it in in sparking
you can connect to this spark cluster
using our studio would actually write
your little our experiments and query
the real data which is actually quite
useful even though horrible so this is
not looking too bad at all though it
will when it finishes it will have some
errors that's expected so no need to
panic okay that's that's looking good
that's looking pretty reasonable almost
it's going to start ganglia it's going
to start the spark master we'll see how
many CPUs we have when we should see
that we have one master and ten slaves
doing nothing at all really because we
only just prepared this infrastructure
so nearly come on I mean these are the
biggest machines that we could buy so I
would expect them to be a bit quicker
than this but anywho right all right all
right
yes go right that's that's a good
question so you can the users can opt in
to receive the to send the data and we
are talking a kilobyte per second per
user so you know in a small app it's not
a massive deal but in a larger system or
if you have a lot of users it's actually
becoming problematic and second problem
with this particular app and with
exercises is that users exercise at a
particular time all day so we have this
massive surge in the morning and then
nothing during the day or almost nothing
during the day and we have again massive
surge of input in the evening second
thing if you're building this kind of
app you should consider is you are
storing biometric data so what we have
done is taken a similar approach to you
know ad tracking so we have a the
ability in the you know full-blown app
to reset biometric ID that's the big
word rival essentially we generate a new
it for that user so if you're a user
okay so we're good right don't worry
this is expected so we should see that
it's sitting there doing not much at all
yay good let's see spark so we should
see 10 slaves one master Idol that's
kind of cool we're going to submit the
job and I'll keep get back to your
question so cluster submit and it needs
the the address of the master to submit
it to goody alright so that's going to
be happening do you know how long it'll
take roughly we can finish I mean you
can we can all have a look at it and and
stare in all how is this is sitting
there so if you're building a thing that
records biometric data you need I think
to give the users the ability to
disconnect it's going to hurt you
because
you won't be able to reliably connect
the training data but it's good for
users it really is um I'm surprised that
not many people or that I'm surprised
that many people opt in to share their
data and all these exercise apps it's
ridiculous I'm frightened anyway I think
we have to cut the wire figure if you
don't mind the last question no I have
this garden swim and battery lasts for a
year and it's quite good at predicting a
lot of stuff yeah my swimming style the
number reflects that I do number of
strokes per lab yep so what's good
briefly comment was this architecture
give me a user if I opt it for your
biometric data sure um it gives you the
ability to recognize and classify any
kind of exercise so that's the really
really powerful thing we can train it to
recognize nearly anything where we get
movement from a sensor so of course
we're doing exercise where the sensors
don't move we're stuck but we can
classify swimming we can classify you
know mountain climbing we have an
experiment even more powerfully and this
is something that's you know really in
the early stages we are trying to help
people with physiotherapy so we can
because we can train models specifically
for a group of user or even specifically
for a particular user we can assist that
user with exercise they do for physio so
we can train it to do you know a
particular movement and then when they
the user repeats the movement we can
track that again and report it to to the
physio people the challenge with this oh
that's a really good question no we
can't we're really working on it we're
really trying to figure out how to do it
it's a very good question what is good
exercise mean what is this notion of
good in exercise our physio we
no it would be great to know right I'm
sorry oh it's okay it's okay can you all
please applaud yeah</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>