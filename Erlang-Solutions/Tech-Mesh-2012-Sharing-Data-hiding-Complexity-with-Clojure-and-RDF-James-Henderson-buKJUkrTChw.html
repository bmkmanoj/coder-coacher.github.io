<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Tech Mesh 2012 - Sharing Data, hiding Complexity (with Clojure and RDF)  - James Henderson | Coder Coacher - Coaching Coders</title><meta content="Tech Mesh 2012 - Sharing Data, hiding Complexity (with Clojure and RDF)  - James Henderson - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Tech Mesh 2012 - Sharing Data, hiding Complexity (with Clojure and RDF)  - James Henderson</b></h2><h5 class="post__date">2013-08-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/buKJUkrTChw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so yeah first of all thank you to you
guys so as much as just said i
understand i'm getting between you guys
and beer so I'll keep this brief so
first of all say Malcolm can't be here
today unfortunately so if you do see me
looking up at the slides and looking for
Muse that's probably one of his slides
and just give it back then are we
talking about sharing data and hiding
complexity and how we manage to do that
in deutsche bank using closure and rdf
and so i'm actually under graduate
scheme at deutsche bank I've been there
about a year now and I was first
introduced to closure about six months
ago by Malcolm and then started working
for him shortly afterwards so this is
really sort of an experience report how
we found enclosure how we're solving the
big data problems and when I say solving
the big data problems I mean solving the
big data problems not solving the big
data problems and how we can use the the
patterns and paradigms of closure and to
actually to actually solve these
problems and I'm also going to talk a
little bit about the power of graphs as
well so first of all just give you a bit
of background I'm working in the in the
risk domain in deutsche bank and of
course like all banks and there's a
there's an awful lot of pressures on the
banking industry at the moment not only
from the media but also in terms of
financial and regulatory pressures as
well but we've still got the same
problems in terms of in terms of the
data we still got the same questions of
how we can process data faster I mean
that there's always the requirement to
process data faster so the whether it be
sort of the Oh getting the overnight
batch done fifty percent quicker moving
possibly to even real time risk if
you're coming from the risk dept point
of view we've also got data increased
volumes as well and this is particularly
true in the in the risk and P&amp;amp;L area
where the more the more data we can
analyze and the better the more
information we can gather about our risk
position the more accurate figures that
we can report and also we need to
actually report data more precisely and
so one of the recent regulations over in
the States the dodd-frank regulations
and require that we and we can actually
deliver an initial margin call on
counterparty so we need to know
position with respect to any given
counterparty so that we know for example
that we're not exposed to another lemans
or another Greece or whatever whatever
the next one happens to be so I'm like
to introduce you reader Connor to
Conway's law first of all Conway's law
his organizations which design systems a
constraint of reduced designs which are
copies of the communication structures
of these organizations so actually I
think that was originally meant as a
joke you know like one of the one of the
Dilbert cartoons but unfortunately turns
out to be quite close to reality as well
again quite similar to the Dilbert
cartoons but can we throw in action and
shows that for systems for example that
a built up organically around these
sorts of communication structures often
ends up with quite a lot of quite
distinct components to the system
depending on sort of what team to
develop them and what the organizational
structure was at that time the upshot
for this on a tech from a technical
point of view is that we sometimes have
data flowing through this system and by
many different protocols by many
different systems by many different
formats and if we've got Bob trying to
send data to Alice here through all of
these different mechanisms how can we be
sure that the data analyst gets out at
the end is the correct data as put in by
Bob at the star also what does it all
mean what you've got so many different
terms and so many different sort of
concepts spread about systems that once
once you move these different concepts
from system to system you can't really
be sure that the meaning has been
preserved throughout and for those of
you looking up here thinking I'm about
to do some sort of financial rendition
of the element song don't worry I'm not
taking that would be actually you've got
quite a lot going for that there but the
thing you say the data does not the data
does not typically travel well and
that's one of the problems we're trying
to solve so the conclusion of all this
is that we need to make our data models
span the systems that the data that data
is relevant for so make sure that the
data meaning as you put it in at the
start is reflected in the meaning of the
data as you get it out at the end does
this modeling scale though if we look if
we look at traditional modeling what's
what tends to happen with it so as we
scale up we obviously we get an increase
in complexity
so every new requirement that comes
along every new bit of data that we
required to do every new report that we
required to produce the complexity of
the overall system increases the
deviations and by deviation see what i
mean is terms of in terms of the agreed
standards the agreed formats it's only
natural than in a larger system there's
some sort of slight deviations from what
you originally intended will come about
and as we scale up we noticed more and
more of these so for example if you
remember back from the original slide
we've got at the bottom here avoiding
being blinded by that thing we got
linkage reference and linkage reference
to I've got no idea what these two are
I've got them all the differences are
then with their formatted differently
and diminished I'd be surprised if
people in the original system you either
also when when we're trying to pull
together lots of different concepts into
the same system we tend to have to make
the concepts themselves become more
abstract so to make them more powerful
to be able to deal with more and more
situations and of course when they when
they do become more abstract it usually
means that the understanding that we
have of these different abstracts and
concepts decreases as well and that
again that only that can only introduce
bugs we also actually get a few problems
when when data escapes it's the main
model so data when confined within a
single system can actually be quite well
defined so the developers know what
they're talking about the business users
know that the terms that they're using
and how they're using them bad it they
escapes the domain model any assumptions
and that were made about those about
those terms and concepts are lost okay
they could they might well be documented
but more often than not those
assumptions and meanings are lost and
any any form of local terms any form of
local variables don't usually become
global ones it's very very difficult to
see sort of those terms that we saw on
that slide become a globally accepted
and term for whatever that whatever the
concept happens to be so implications of
this a primary tool for flexibility
flexible data processing does not scale
we've got to think of something else
what are we going to solve it at the
moment at the moment it since we just
we're just throwing more code at it
every new requirement that comes in
every new regulation
we put we built in another component we
put a new component into the system
we're fighting with a code hosepipe if
you like just firing more and more code
at the problem so I'd like to introduce
you to rdf and rdf that is a w3c
specification and that's essentially
based around entities attributes and
values or triples and for those of you
been to rich hickeys talk about the
database as a value remember he actually
uses quads he puts a transaction in
there as well but for rdf we just cope
with these triples so the example of
this is the cat sat on the mat we've got
to subject a property and an object and
in rdf this is represented in what we
call end triples format which I've got
an example of down here at the bottom
and so for example here ginger the cat
is sat on the tech mesh hotels map and
we can also sort of combine together
these triples mnn triples as well so if
we're making stately if we're still
making statements about ginger we can
say that ginger is also type cap
important thing here is we can also put
metadata in rdf properties are entities
just as much as ginger is katie's
methods so for example we can say things
about the property sat on we can say
that if we were to represent sat on
inner in a user you I we could use the
string sat on we can also put in a
little documentation comment saying for
example sat on is the action of sitting
on a given object granted for cat
sitting on a mat you probably don't need
it when you scale up to financial terms
rest assured you do so what are the
features what are the benefits that we
get out of rdf so first of all your eyes
uniform resource identifier these
provide globally unique identification
and transparency but the important thing
that rdfa is combined with the URIs we
also have a name spacing go going on and
this allows us to do federated
governance so if I go back here and I
look at the URIs that we have in the rdf
model we're actually namespacing these
we're saying that ginger is in the
namespace of animals calm so okay it's a
contrived example but you can see for
example if I if I was to use for example
a trade a trade source system could I
could have various properties about a
trade
and the trade source system with the
experts that are in there will have full
governance about the terms that are in
there namespace and that's that to me is
the best the best way you get the people
who are right there in the systems they
know the terms they know what they mean
they know how to document them properly
RDF is also storage and transfer neutral
when you think about it at the end of
the day it's just a set of triples you
can just sort of write the triples out
to a file as assertions but we're not
we're not worried about whether you sort
of store them down in terms of Jason
whatever whatever are the format you can
name trans transfer them over the wire
there's a couple there's a couple of
sort of ideas that how best to transfer
them over the wire but at the end of the
day we again we can just transfer them
over in it pretty much shoot on them
into any format we like and finally as I
say the metadata the metadata is
actually a very powerful on this and
allows you to make assertions about the
and sort of the quality that Asian
system to format the data and then any
assumptions that are actually made while
while designing the system so a couple
of quick observations before we move on
firstly rdf has the power to break down
silos so where you have got silos at the
moment for example between a trade
system between a risk system between a
finance system rdf has the power to span
those silos so for example the triples
aren't unrestricted to being within a
particular namespace I can say for
example that things in finance related
to things in risk i can i can make
assertions about properties in the
different systems i can say that for
example one attribute here is exactly
the same as one had to be over here and
i can make make assertions and
inferences based on those it's also very
non-disruptive way and if can be applied
piecemeal and you get you start to get
the benefits of rdf and as soon as you
start to apply it it's not the sort of
thing where you have to convert your
entire architecture oh it's hard you
have to start seeing the benefits you
immediately see the benefits because you
get an increasing documentation
straightaway you get an increase of more
formalized and terms and concepts and
you get those benefits straight away
so I'm going to talk a little bit now
about how we've actually applied this
and within DB and we've applied apply
this in a system which we called the rap
or the risk access point so within the
risk domain just set the scene a little
bit within this domain we've got and
what we call the ODS and then which is a
large data story set essentially
essentially it's a big Oracle box and
which which which houses all of the the
risk information and sends it down to
off the downstream consumers but
obviously we don't want all the
downstream consumers coming in and doing
sequel queries on our database right
that would be crazy as a backwards
compatibility nightmare so what we want
is a system to get in between those and
act as a as a query buffer if you like
for the for the rap and it exposes
business level terms rather than sequel
tables and columns we're actually quite
fortunate really in that the the tables
in the ODS are all built up from a
metamodel you can see I'm talking quite
a bit about meta today hopefully it's
not hopefully it's not too matter for
the summoning afternoon and the table in
the ids are all built for metamodel so
that means that we know about all the
relationships between entities
attributes and and values we already
have that data we actually get that in
the form of an XML in the form of an xml
file rap actually passes that and turns
it into RDF what why do we go mean why
do we go to all of that effort of
pausing editor and turning it into RDF
well first of all we've got a
disambiguation of terms again as I said
previously so the rap can actually
support multiple data sources as well so
we need to disambiguate we need to
disambiguate between the terms that are
coming in from each different data
source for example one of the early sort
of user requirements that we had and was
too and the user wanted to merge the
data that they were getting the risk
data they were getting out of the ODS
with some static data that they provided
us and what we could do by merging these
two different data sources we could
treat them as one data source and this
is where it really see the power of RDF
as a graph so if you think if you think
bout RDF is a graph the problem of
merging grass is actually quite a simple
one and we get a lot of benefit out of
that I'll come a bit more onto the
paragraphs a bit a bit later on
we also get really for free this this
data dictionary so the documentation
with rdf because everything's a uri to
get the documentation the idea is that
you grab that you are i pop it in a
browser and you hopefully you get
something helpful out as a result I'm
not sure how clear this image is I'm
guessing you probably can't see much of
it and but but the important things now
here is that we can provide this
documentation now at the moment the URLs
so the ER is that rat produces are all
within the rap name space unfortunately
the systems upstream don't provide this
service but what the rap can do is
provide this service for them we can
expose their documentation and so many
downstream consumers can take a look at
this before they before they need to
make any queries so it's all it's all
very well talk about metadata and
sinister there's probably hold
presentations you can do on metadata I'm
not going to bore you with that so
essentially let's talk about actually
sort of querying the data and and and
what then what we can do with it so
because of the structure this metadata
forming a graph we thought it was only
natural for the format of sorry the
format of the query to be just like a
query on a graph right there seemed that
seemed like quite a natural way to query
it for us we allowed a shame consumers
to create what we call a view it's quite
similar to a sequel quite a similar to a
sequel view and in a lot of ways which
is actually why we call it the view but
here we have a viewing graph form so
what this is this this is your this is
your typical authors and books query so
what what the user is looking for here
is to get all of the authors at the top
here and in the views we allow them
attributes and children attributes here
in in the circles and children here in
the squares obviously the top ones are
not not child of anything is the root
node right so this query is essentially
saying get me all of the authors for all
of the authors i want the surnames and
for each of the authors i want to get
all of their books and their books
titles does that kind of make sense
we can actually repair we can represent
this in rdf of course that's just that's
just a graph so we can represent that in
rdf and again here the syntax is that
there's a few little sugars going on
here so i'm not going to dwell on this
too much but essentially we can say that
a view is like a selection where we
select select the author we select an
attribute we've also got a child node
here which is booked this is related to
a bio relationship and we want an
attribute on the book so we can we can
turn this quite easily into a in turn
rdf form and we actually merge these in
we actually merge these rdf queries in
with the rest of the metadata and so the
queries we can perform on both the view
itself and the metadata a viewing again
they're only viewing one data source so
using the metamodel and the rap thing
actually generates the sequel that's
required add to actually get to get the
results back on but as and when the view
is realized so the important thing here
is that the user doesn't need to worry
about the tables the columns that joins
whatever then if you think about it
there's actually quite a few different
queries I mean if we flip that up query
upside down for example we say we not
worried about the authors I just want
all the books and fruit but I want to
get its author that's a different query
fundamentally to the first one but what
the wrap does is work out which way the
relationship goes wit works out which
way the join needs to go in the database
and takes away all of those concerns
from you em ok so this actually allows
consumers to navigate the one-to-many
relationships in either direction so
when when the when the results are
returned we then actually start to then
actually start to get some of the
benefits of closure because closures are
inherently lazy languages gotta support
for lazy sequences and the result set we
get back from the database is an
inherently lazy results set and so this
whole process from generating this blog
maybe not generating the sequel but
certainly receiving the results
converting to an intermediate data
structure and then out to whatever
format we happen to be we happen to be
required to produce this is all done
this is all done lazily so the advantage
of this is that we can pass
several gigabytes of data through means
even several tens of gigabytes of data
through the through the wrap all within
the memory of a standard JVM in fact the
only thing that isn't lazy is this is
this process right now at the bottom
which actually streams it out to the
clients and some enos that's that's the
kind of process that says hey guys you
can't be lazy all day you know you've
actually got to produce some data so
that that essentially pulls pulls the
query through the system so we've seen a
little bit in terms of how we can use
rdf and to query data in this format and
how the how the formulism rdf actually
gains us quite a lot in terms of safety
and security I want to talk a little bit
now about the power of grass and so to
go over a little bit about an existing
pattern in functional languages for
traversing graphs and other data
structures and also want to share with
you an idea for how we can combine this
existing pattern but also the views from
within the format of the views within
the wrap and come up with an actually
really declarative and a really powerful
data modeling tool so it's not it's not
just views that we store in rdf in the
wrap we actually start our conflict
there as well and again it the config
forms just as much of a graph as as the
data as the data itself for the metadata
itself so for example we can say some we
can make some assertions and make some
statements about the about the structure
of the wrap infrastructure so for
example we've got three hosts here each
of which contained each of which
contains an instance they can actually
can say more than one instance which is
one which is why we split them up we can
also make other declarations like for
example a host has a hostname an
instance has an HTTP web service which
it might will need the HTTP web service
actually needs a port as well why do we
bother write this this seems like an
awful lot when you can just do it with
Java properties files so we obviously
get the benefits of disambiguation the
metadata I'm not going to think I've
covered that quite enough i'm not going
to bore you with that again but one of
the advantage of this does get us is
that we can also query the config so for
example what i can say about the config
its things like what host am i running
on what sort of service is dining to
start up what ports do i need to start
them up on
who is allowed to administrate these
instances and these will form queries on
the config graph we also get a benefit
of actually merging together multiple
configuration sources so within the bank
it's actually very is very important
that I think it's even a regulatory
requirement and the developers are not
allowed access to for example production
database passwords or product any other
production secrets or for what a for
whatever they happen to be but what we
can do and is create two separate
configure us one that developers are
allowed to see and one that they're not
the production support hold we can merge
these two together at runtime and treat
them as if they are exactly just the
same graph right it just comes back to
them to that simple graph merging
problem we also sort of dis something m
and disambiguate between sort of static
and dynamic config here what I mean by
static conflicts or a conflict that
starts up at application startup but
also dynamic config that results is for
example as a like a user administration
a user administration screen and and we
can also combine these two together and
say we don't the the application using
the conflict doesn't need to care
whether it's statically configured and
emic config secret convict doesn't
matter it's all the same source actually
that that's that's a slight light we did
within the code base we do actually
distinguish between code with Pastor
sorry config with passwords and conflict
without just so that we know to be extra
careful in the areas of the system that
do have access to the passwords
information see in in functional
languages we actually made quite a lot
of use of the what we called the zipper
pattern and the zipper pattern is a way
of traversing data structures in a quite
in a quite declarative form sorry is
that echoing you're getting an echo okay
just checking em see we use this a
zipper pan to navigate over several
different types of data structures sound
for example enclosure we we get xml
zippers if you think about it actually
the XML zippers are actually quite close
to X path of you and if you ever used
xpath that's quite it's quite similar to
that and we use a modification of the
zipper pattern which works on graphs
touches it sits on github going to feel
free full free to try out as loaded
tutorials and everything so here's an
example configure ah I guess I guess you
probably can't see very much this I'll
take you through the bits that we need I
senshi on the top here we've got our
host and we've got details about the
host we've got production instance we've
got a couple of different services here
one HTTP 14 swank of course we're
working enclosure so spanks actually
very useful on this one and also some
information about the JVM than it needs
to start up as well so things like the
memory of the JVM that we need to start
up and all sorts of other configuration
data so don't worry you don't need to
read all of that it says you be then
import the graph into graph sip excuse
me and we do that by importing the RDF
triples in the in form of subject
property and object so you take my word
for it that that's all that's all the
data that was in the previous graph but
exposed in triples format we then need
to build up the zipper instance now what
those if for instance is is the data
structure combined with the nodes that
we're currently visiting now for an XML
structure that's quite easy right
because the the node that we start off
with is obviously going to be the root
node but with a graph there's no such
luxury me wait where do you start off
when you're traversing a graph so what
we do is we combine the configuration
graph with a root node and in this case
I've chosen this host right up at the
top here to start from we then navigate
through the graph and the way we do this
so can you can actually see this at all
ok excellent and so we then navigate
through the graph we then we say we want
to start off at this zipper I want to
navigate the instance edge I then want
to never get the service edge and I want
to get all the nodes back so going up
here I let's navigate the instance edge
or to navigate all the service edge and
then I want to get all the nodes that
are there so we expect to see the HTTP
service and the swank service we can
also do things like find out the swank
port and the zipper also allows us to
make filters in the query as well so for
example if I never get the instance in
their service i'm currently at these two
nodes this vector notation here means
sort of keep an idea of where you are
but I'm going to take a quick detour in
order to filter down the nodes that
you're currently holding so this this
filter here for example says I only want
two nodes with types white service which
is this one here so there's swank
there's the type their Swank service so
what we've done there is we filtered
down the notes only the ones we actually
need in this particular case when we
filtered them down we come back to the
node you further you are out originally
which is this one here we then navigate
the port get the node there and that
gives us 4005 the defaults langport I've
actually used it one in this case
because we know that we're only getting
and we know that we're only getting one
result back if I'm following that so far
yeah that's right there's a battement ok
so the zip Burton actually takes on two
forms in the closure API and you're
right that one of the forms is that you
can navigate through the graph and make
changes to it and when when we're
querying it we tend to use this that we
tend to use this more declarative query
syntax it's just essentially a touch of
sugar put on top of the actual zipper
see actual zipper syntax and at least
not in this example may and I don't I
don't tend to use the modification here
either then I haven't been found a use
case really for it really say
yeah i say i'm sure you can and these
zippers work perfectly well on those on
those cases as well let's say we just
have another another sugar that we can
use so graph sleep actually allows us to
merge in as i said merging multiple
sources of config as well and that's
quite easily from within graph sip we've
also written like it was anyone at gym
Weber's talk yes so the neo4j talk
there's actually a version of graphs it
that's out the can query neo4j as well
and again again that's up on up on the
github page if anyone wants to write on
down afterwards feel free to ask Anna
Adele free so we've seen how we can use
sort of the paragraphs and to query the
config so now I want to share with you
an idea where we can combine these two
ideas I've just presented the idea of
having user-defined views and this
zipper pattern to create a declarative
query language that we can use to define
define data structures I'm going to make
a bit of a jump here back to xml so
here's the config that I I think i think
xml slightly easier to understand in
this context so here's the config
information we had earlier for the for
the host but this time expressed in XML
format so quite a lot quite a lot of
time qualitÃ© of f and quite a lot of
code I find goes into getting input data
in some form so like this and
transforming it into some other data
structure and shipping the data
structure off to someone else it seems
to be quite a common Pat as simply quite
a common problem and without without
really a comment solution every time you
have to define the transformation that
you require so for example using the
traditional zipper syntax I find
actually one strange thing about the
closure community and apparently in the
closure community you don't say
someone's code sucks you say that it's
not idea Matic closure so if this is an
idiomatic closure please accept my
apologies for that mmm so using
traditional zipper syntax what I'm
trying to do just build up a data
structure for the configuration that I
can query in whatever way I want oh
ass on to wherever I want so what I have
to do I have to go through all of the
hosts I then have for each of the hosts
I have to generate a map it said how
many people here are familiar with
closures in sector 5 okay yeah this it's
a lisp it's announced our brackets right
yes actually essentially yes and so that
the braces here denote a map which is
essentially a key-value map the square
square brackets are vectors and what
else I think that's all we've got there
really embraces a usually funk function
application sorry brackets usually
function application and so here we're
generating a map hostname and instances
and for each of the maps we then need to
do a sub-query we then need to do
another query on that XML admittedly we
can start off from like where we were at
the host of the host made but again we
need to do a query instances for each
instance I want another map then for
each service I want another map and for
every part in the data structure it goes
on and the more complicated the data
structure the more of this you've got a
right I mean it doesn't get it doesn't
any better really when you're using data
log syntax right beta dog makes your job
a little bit harder because in these
logs queries only really return
flattened rose you know it's almost like
the 90s and we going back to relational
databases again come on guys we deal
with data we deal with data structures
now we're enclosure so using the data
log syntax for every level I need to
make a data log query so I need to make
a query for you to the hosts I need to
use the outcome of that to define what
the instances are and I need to use the
outcome of the instances to do another
query on the services if I want any form
of nested data structure of course I
could just I could return flat and rose
but it's much easier to query a nested
data structure than it is to query
flattened rose so to step back from them
like on
okay so to step back a moment we can
actually use the closure the idea of
having the closure zippers as metadata
for the date air for the data structure
that you want to come back so we use
include us again we're using closure
zippers as metadata for the data
structure that we want I want you to
imagine now so you hope you've got a
rough idea of the sort of data structure
I'm trying to get out of this I'm trying
to tease out of all of this code right I
just literally want to close your data
structure of a representation of the
input data so imagine how you would
actually so convey that to a fellow
developer how would you tell them how to
transform and that that code if it was
possible to write that transformation as
a description of the transformation
rather description of the algorithm that
you have to go through this code would
be a lot more understandable so the main
idea of this change is to introduce a
map construct into into the zipper
syntax and this will allow data
transformations such as these that we
write every day to be transformed into a
more declarative syntax saying exactly
what you want rather than how you want
it so the enhancer per query would look
like this it makes you run you through
so what I'm saying is that I want a data
structure here which starts off at the
route goes down all of the hosts and
then I define the data structure that I
want I want a map i wanna i want in the
map i want a host name how do i get the
host name all I query it using this
little query here and you can see how
much of the boilerplate has been taken
away by this little syntactic sugar this
M this carrot colon one here instantly
is just a little bit of closure metadata
saying that there's only one host name
for any given host so that way we don't
have a list of one element we just have
the element itself likewise for
instances what's the query for instance
to get the instances part of the map
well I go down instances I go to any
instance and then I want another map
here i'm defining exactly the data
structure that I want back I then say
each instance as an instance name how do
i get that well here's it here's a query
for that each of the services again so
what I'm saying here is exactly the
format of the data structure I want back
everyone following that way
brilliant okay so the result of that is
then this data structure which actually
enclosure looks from remarkably quite
terse and now the possibilities with
this is that you can query it using any
of your standard closure core functions
so just yet getting or what whatever
else it happens to be just it's just
like the same as traversing a normal
closure data structure you don't have to
worry about all the boilerplate going
through of working out which queries how
to flatten it how to unflattering when
you get it back any of that so on that
bombshell I'm going to sum up
essentially what we've shown is that we
can use we can use RDF and to really
disambiguate the data we have we can use
software like the rap to be other query
databases using this RDF and we've seen
how the power of graph can really lead
to a good declarative syntax for this
data transformations which we do every
day thank your watch on how to set
questions
and we did actually originally gopher
for Sparkle is actually very good sort
of query language for grass but again it
comes back with this problem that he'd
only returns flattened rose I haven't
okay if that if that's the case and i
haven't writing but to be honest if we
did okay but but if we did that mean we
don't we don't have any Sparkle
endpoints that moment and so is that
yeah I say I mean I'm sure I'm sure if
we could use a spark with a point for
that sort of query then it would provoke
pretty frightening that some advantages
bit
so this is for it doesn't at the moment
I said it's only been a spare time hobby
project of mine side I mean it's a good
idea it probably something I'd look to
extend it with that's if you can fork it
on github and provide I'm sure if you
have it set on would you like the ugly
dresses for that again we're we're
letting a presentation for that okay
thank you do turn governor's since in
the Elliott anybody assume anything
and we don't to be honest and the only
thing we do we could do with that is to
apply to actually turn them into a quad
and then had the last one that as the as
the governance of the of the assertion
so to say that for example this
particular person made that assertion at
this time and you you can then combine
that and say for example how much do I
trust that person making that assertion
at this particular time in terms of in
terms of security we don't we don't
actually make those we don't actually
make those assertions I say at the
moment it's and it's a it's really only
experimental prototype at the moment
we're only drawing down data from from
one sister at that moment but I'm sure
that if need I'm sure that if the need
arose then and then we could then we
could actually include that form of
security
yeah yeah and again and again we said we
certainly could and we did we did
actually purchase I've led a while back
but what we did find that the
cardinalities were actually included in
the the metadata that we were being
passed in so they cut the cardinalities
rule on that side rather than on the on
the actual query side the query itself
doesn't need to worry about the
cardinalities you know I mean if we've
if we've got one author having multiple
books then the metadata already knows
what the cardinality is and in that I
believe we do actually use the owl owl
oncology's for the cardinality okay
there's no further questions near thank
you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>