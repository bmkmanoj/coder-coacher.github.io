<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Lambda Days 2015 - Nilanjan Raychaudhuri - You say big data, I say Fast data | Coder Coacher - Coaching Coders</title><meta content="Lambda Days 2015 - Nilanjan Raychaudhuri - You say big data, I say Fast data - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Lambda Days 2015 - Nilanjan Raychaudhuri - You say big data, I say Fast data</b></h2><h5 class="post__date">2015-03-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/uS4Ntcz3mTQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">master of ceremonies for this track and
so I'm going to be very brief in and
introduce nilanjan who's just come here
from Berlin from the front typesafe and
you will be talking about big data or
fast later on an exam thanks a lot Klaus
will never London please I'm actually
not sure why I'm going to talk about
them so I'm really out of my comfort
zone here I'm really not sure place lots
of people love you like a show down here
I'm not sure we'll see how it goes so
this idea about is something that I'm
investing a lot this year especially
looking to a space where when lots of
people solving this this type of
problems where the speed is your issue
ok and then this presentation is going
to be essentially giving you an idea
about that space and how that space is
really evolving ok and when I get
excited I tend to speak really fast so
please stop me and ask me lots of
questions if you don't ask me questions
will be done in 10 minutes ok so unless
define some terms here so that we can
all be on same page here Big Data who
can go for it what is big data oh whoa
all right what else
quiet a velocity on the balloon and all
other attempts to guess no one able to
handle that no money but okay all right
are unmanageable data the size of the
natives unmanageable who else not a big
dinner abstract no not tractor tractor
dayum okay unstructured the answer what
else ok that's a PS if you guys are
almost here in the same group here big
data is a buzzword alright let's set
that buzzword buzzed work for collecting
aggregating and making lots of money out
that's what big data stands for ok so
that's the space here ok and if you are
lots of people ask this what's a number
what's the size of the data hundred plus
terabytes of data let's say okay I
personally think only handful of
companies have this problem okay these
are the googles and Twitter's and
Facebook's of the world ok now for
others out there this is the space if
that's evolving ok fast it now what is
fast it where speed is important for you
ok where speed is important you want to
do real time analytics to save or make
money ok so our presentation here is
going to be little bit abstract ok
because the space is evolving yes
roblins also basil do cable we can
persist
in America volume and you sold those
four words a date on the fine otherwise
you lost the data that's okay justice is
beta right so so another characteristic
a fast rate are very good here I think
if I is that is that it's an extreme of
data coming in at a real time ok so
there's you read the data if you can't
read that data there's high chances and
falls you lose that day because it's not
stored anywhere so the idea here is that
or essentially my presentations here is
going to focus on this space here and
how people the rest of us rest of the
company a rest of everybody is kind of
solving this problem but behind this
there isn't a big idea here what is I
believe eventually we all going to
become a data engineer okay we've been
looking i personally looking into this
space from sidelines to be honest with
you that's why I'm completely out of
conference one here okay and watching
this space evolve but what's happening
what's really the is changing so fast as
is becoming almost important for any
programmers out there to kind of learn
this space to be eventually all be going
to be like what's programming is
essentially about managing data right
and we're going to change the size of it
a little bit and I'm going to make it
really fast you're going to say we want
to do this information in real time okay
so let's take it back stick here and
look at the big picture okay what is it
means when you say I want to solve a
data problem here okay i'm going to
start saying fast / mediums by state
okay essentially all are driven by
business on point right so business
starts with the problem hey I have this
click swing happening or I just deployed
it a feature out there I want to see how
that feature is doing this is good for
management because you want to make
decision based on data make sense right
I wish my decision lives from all the
data collect and figure out whether this
is working for us so that's the problem
then a group of it could be a collection
of team engineers scientists data
scientists all comes together and kind
of model this okay how can we slice and
dice this this unstructured data okay
benefit in this model and this is the
space we're going to look at they're
okay and do some sort of data processing
pipeline we don't know what it is yet
okay some sort of pipeline where the
stream of data coming in okay clicks
happening a network logs happening
something is happening all these events
are flowing through your this pipeline
system okay so that you can produce
something fast story in cash in memory
radius whatever so that your business
can conclude the result okay
traditionally what is to happen this
cycle is to be mud or end of the week ok
now what businesses are saying why
should i wait for end of the week I
wanted right now I just push the
featured introduction I would assure
that this is working ok so before types
if I swap for a company called
livingsocial ok the team is to work with
the part of the good job is how can I
get users email address that was the
goal of a tag team what we used to do is
deploy code sometimes used to push code
which will run in production for two
hours because I don't know whether this
will work this screen looks really
pretty but I don't know whether the
users will like it right this kind of
decisions are becoming very common here
to see I want to see how its performing
rest of the stack is really not
interesting to me at all ok this thing
is all about resource management here
here is where typically your Hadoop ER
or missus what are the structure comes
in and beyond this is really not
interesting our focus will be right here
any questions
so let's look at some examples let's see
how to build something where I want to
predict breaking news okay no wonder
what it says what's going to be hard to
know what's happening like one of the
nine way to do that do they I'll show
you how we can implement this okay at
the end of the presentation is looking
at wiki edits okay it's very interesting
to you see some some information
something happened weakly it's a very
good friend a good source of information
because you see that page is getting at
edited concurrently multiple answers are
jumping we try to produce it's amazing
how much content gets added to wiki by
the way amazing anyway so what we will
do is that if we've been feeding that
data as it's happening to do a log so
what we're going to do is that will take
that information that's what typically
people do dump it as fast as possible
somewhere some storage some log even
some events or somewhere then we put
this data pipeline we can take this
events as it's happening process them so
that maybe you can alert somebody maybe
we can elect our editors out there so
they can say hey let's watch out for
this page because something might happen
another possibility of this kind of this
kind of another possibly this kind of
architecture is are detecting network
intrusions right wouldn't be nice while
this network events happening be logging
it we can learn some mission block
learning algorithm on top of that
figuring out patterns that there could
be some events happening someone trying
to connect is simple maybe someone bring
a DDoS attack on us and figure out as
fast as possible so lots of companies
out there already doing it here is an
example almost similar architecture but
we are mapping into some some products
some some tools out there this is a
faster data analysis by pinterest they
have they recently published this blog
post i think that he presentation is
short of confidence and francisco last
week as well what they're doing is that
they are monitoring what's happening in
their app
okay users clicking pinning something
all these events are again dumping into
some sort of even close in this case
it's kashka okay then finally they're
running this part here spark is our does
data high fly this thing right here okay
running spark there and finally pushing
into something which is fast to query in
this case they're using something called
male sequel why sequel sequel is a very
interesting place in this kind of
pipeline processing because here is
where your data analysts will come in
they might not be programmers write the
query data and create this nice insights
and dashboards another interesting are
our application of this is Netflix the
fix is great over watching different and
so one of the things Netflix cares about
is recommendation who know browsers
movies nobody does that anymore right we
all are driven by who recommend
something recommendation engine now what
they trying to build wouldn't be nice if
I can show you what's trending right now
but just dumping all the information
what's who is watching what movie right
now might not be interesting to you you
might be interested on movies that
people are watching you also like so
some sort of filtering as it's happening
that's this is how the architecture
looks like not only one stream the
people at what people are doing now
they're getting this multiple even
stream data coming in this very huge
size of data okay and going through this
pipeline so that they can run machine
learning algorithm on a stream of data
these are called online machine learning
algorithms so that on live data they can
come up with some recommendations is
anyone out here actually doing something
similar yes what were you doing
customizing ads but customizing ads at
unreal tournament yeah okay okay anyone
else doing similar things here the
reason I asked is because I were to
learn from you I mean after this
presentation we should get together
because this is a very very interesting
space I want to see how others are doing
it anybody use the restroom yes based on
either oh so whatever if somebody
recommend something and trader you were
using either the recommendation engine
ok ok now this are all great writer sins
so awesome if you can implement
something like this but there are
definitely some hard bills in there so
what I'm going to point out some of the
hurdles again out see this is a space
that actually evolving we don't have a
perfect solution yet so one of the
things the biggest problem in this space
here is this lambda architecture isn't
very comfortable with hard about this
lambda architecture typically when you
go to the organization's you you already
might be having this this layer up here
ok so even if happening you're putting
it somewhere and then you have this
batch process that kicks in and mines
this data typically that's what the
organizations have when you go in and
see okay I want to do some real time
analytics on that are going to run
machine learning algorithm on real time
data then typically this lambda
architecture was also introduced as a
patch as a hack okay so that you can add
parallel build a speed layer so that if
this sticks and hours to do you have
some sort of layer which takes minutes
to perform this task the biggest problem
with this architecture is if you want to
do it is duplication of logic you have
to implement something for batch process
you have then have to rewrite something
which works for the speed layer as well
so we need some sort of tool which can
address this problem this is a big
this biggest bottleneck out there okay
some tool which can address this in fact
one of the recommendation out there is
as the tool evolved that's what that's
what i think of personnel happen this
will disappear if you can have a stream
doing all your work by even have this
layer at all the sitting problem with
this this model of faster data is you
have to be you have to have some sort of
predictive latency okay and and that's
where things becomes really really
challenging and in fact hard because as
the size of the data grows making
running computation on them going to
take time simple math right simple logic
so some of the tools that's worth coming
out with this is called probably stick
data structures what they're doing is it
it okay i didn't want to give you you
don't have to give me one hundred
percent right answer it's okay if you
have a one percent error chances there
look at this example for there's a DBA a
battery is doing some amazing work in
the space so but she's working on
something called blink TV where you can
you can see that here i want to run a
query but i want to rescue to its time
round this for your data within two
seconds I want the response okay run
this where I want the area question to
be 0 what one person point one person
the idea behind this is remember if you
feel not Twitter or yahoo you just
cannot throw hardware at the problem I'm
sorry you cannot because the problem
with that is hardware takes money it
costs to do this stuff so this kind of
approach go to Ghana is becoming very
interesting because you can now write
quarian constraint on resources say you
don't have all the servers in the world
or processing the world to run this
query okay I'm accepting to have some
errors it's okay to have some layers
another interesting tool is coming in
this space is something called tachyon
have anybody heard about this road
before so once you go into this space
where you doing large compute
computation sandy running on multiple
nodes you're going to get into
situations where you'll have I copy the
same thing I catch same thing here here
here here what's the point would it be
nice if I can have it some layer and all
this noise can share that some sort of
shared memory ok if this this duels are
are getting created or coming up with
this new tool so that they can address a
problem in this space only machine
learning so obvious fear once you
started collecting this data the next
thing you're going to up for bling is
that how can I run algorithms on top of
that the problem with typical algorithm
says that you don't have time you don't
have time to train your algorithm it
takes a longer time to train and say get
you don't have all the data set that you
want to train your algorithm you want to
come up with algorithms which can learn
as it goes so this only category in
which part is doing lots of stuff here
I'm showing an example of k-means also
is is also pushing again businesses
pushing boundaries here is saying ok how
can we come up with how can we rewrite
some of this a machine learning
algorithm for streaming version when it
doesn't work with all the sets a tiny
learn within each element
and the next thing we want to create as
something that he'll see with building
that data pipeline okay this are this is
embarrassing ok so somebody created a
command-line tool which was faster than
Hadoop cluster to solve the same problem
so one of the things that that I'm
noticing what's happening in the spheres
is that Hadoop is not your golden hammer
anymore it's great for solving this big
data problems for faster data is
actually not the tool for you and this
definitely cannot work I cannot write
this code to do the processing so we
need it we need the computation engine a
model and a nice language at dsm which
allows me to write this this code we're
going to see some examples of that this
thing becomes very important when you
run this are streaming algorithms well
because most of this algorithm was going
to run forever 24 verse 7 then it
becomes very very critical to have some
sort of monitoring in place which can
monitor you not only that sometimes when
you have this online machine learning
algorithms running you have to tweak
them as it goes so here is my wish list
and you guys have to do it for me the
wish list is this if I we really want to
come up with such as reaming platform
which can solve our fast data problem it
almost needs to have some of this okay
it has to have something that programmer
friendly that we like to do you think
very perfect I took care of us first
then it has to be fault tolerant he
mentioned ready for even if a click
stream is coming a data is coming is it
important for you if you don't want to
lose it then you have a certain swearing
fault tolerance however maybe we should
replicate how how should we deal with
that if my my my socket winds down or my
note crashes
and and it's Act has to have this higher
throughput and low latency right some of
the tools and specially how do for an
example doesn't fit here at all maybe
maybe sometimes command-line tool is the
right thing to do and it has to
definitely integrate with back back
system because that's why the most of
the companies have made some investment
right so there is a bunch of things out
there and and that streaming platform is
not there there is nothing like that but
one tool that comes really close here is
this guy called spark streaming and I
think this this tool has enough momentum
to address some of the issues that we
talked about in the wishlist it almost
takes on feel the things that we talked
about so what is string anyone
comfortable with spark here or no spark
here I okay for wisdom sorry i'm not
going to spark here at all haha you
don't have enough time but i'll go very
high level idea if you have anything
confusion please let me know okay so
sparks streaming what happens here this
this data line might be very obvious so
an input stream this could be your the
weekly changes coming in or our network
logs coming in and this is a company
which does the data pipeline for us
right a streaming platform which takes
this and the waves park swimming work is
that it splits in two moves multiple
smaller batches and you can define the
batch size for an example you could say
all the data coming in batch it per
second so within a second whatever data
flows through the system will be in one
batch
it turns out it's perfectly fine for
eighty percent of the application out
there eighty personal frosted
application there is matching going on
so it's really not element to element
real time here but that is okay for
almost eighty percent of the problems
out there in fact you can tweak the
batch size also you could say primarily
milliseconds or 50 milliseconds then
these batches goes through this spark
engine this is our core which allows us
to do computation that means when you
send data here this guy will take care
of sending it your cluster of node doing
all the configuration and finally you
come back with the process data this is
very typically either feeding to your
radius your main cash or your your some
storage one of the thing about Spock
streaming is I find it very very useful
it allows you to integrate with set of
resources out there mom was for all the
sources it has an adapter and this
project has so much momentum out there
people are jumping in and building new
Hannah just for them so show me the code
okay what I'm going to do here is that
we don't go back and see if I have to
implement this how can I do that using
Spock straight since i'm not doing in
hadoop it will not take an hour
what I'm going to do here is that
meaning I should run the code so i have
is known application running here which
which has a nice module to to kind of
type attacking to the IRC of wiki and
get all the changes happening ok so i
have done that and i'm going to run my
spark streaming application here i will
run the application first and then go
through the code maybe that will help
little bit how it works ok also a number
three here ok it's written a note so
sometimes it works sometimes it doesn't
come on there's a shell startup time but
is by the way the streaming has already
started running so that's good so we've
seen the like a quickie events against
that right now it's happening pumping
lima this is probably because of the
whole dress things that happened
yesterday and this is what we are doing
here I have a I have a second of batch
so what i'm doing here is that on every
batch i'm reading the Ross raw wiki
edits and trying to figure out which one
has taught most it right now we don't
have everyone has a one sometimes it
shows a five or six depending upon the
like I said node has some issues but the
great thing about is that that let's
show the fault tolerance of spark
streaming here it will automatically
reconnect once that shows up here okay
so I mean essentially getting the source
and running completion on it I will keep
it running behind the scene and while we
look at the code is this phone readable
to everybody I'm this code example is in
Scala but i don't think the details
really matters so first thing i'm doing
here is and i'm creating this the spark
context this gives me a this gives me
this guy here this guy up here ok so i
create that next thing i do here is that
create the swimming context which is
this guy here ok now rest of the code
let's go through it first thing i do is
that when it created a swimming context
i created shake point here this is a
default tolerance
okay if I something happens while i'm
reading the live stream here get saving
into a sub location here so that i can
read it back again what's a fault
tolerance in action right there now i'm
saying here a spot streaming i would
read all the changes from this socket
running on this book this is where we
unconnected to the north that's running
there okay now this is probably lying
number 72 27 is probably the most
important thing now what i'm doing here
is that saying wiki changes that I've
got a got here which is an abstraction
something called this stream this and
imagine this is a box okay box with all
the patches inside it and say I want to
take that line the string that comes in
the string looks like this the string
looks like this here would be that
string here okay for each line of that
takes I want to split it ok so the batch
could have multiple lines in there I'm
splitting it on each line I so I split /
in then I'm taking this each line that's
actually the raw string is adjacent I'm
parsing into just an object this is my
data processing pipeline and building
transformation on top of them that's why
having Scala and functional and
programming or so important for you if
people at your hearing functional for
being everywhere this data this big data
space and fastest in the face with push
functional programming to mainstream
because you almost have to hear and
finally extracting out the page URL and
string first field starting with count
once a week and I see a URL I mark it as
a one day comes to this is probably
what's happening as a stream I just I
just cleansed my data here essentially
now I'm running the actual computation
what i'm doing here is that I'm saying
running a stateful operation as the
stream going through what's the state
full operation I'm saying every time you
see for a given key which is a page URL
increment Abbott ok reducing the number
number of times
chattering so that's actually going to
give you this this guy here produces
this kind this comp right here if I see
the same URL coming within the second
then I see see if you amounted up the
streaming being a sliding window as it
going through the stream there will be
some elements that's going to fall out
of the window for those elements I am
decrementing the comfort epic science is
moving to the window there will be
elements falling out and finally I said
that dilution of the window here is 55
minutes so every five minutes window you
run this computation and finally I'm
filtering out saying hey whichever
element is greater than zero if I'm
building a real world we might say
greater than five if an a document is
getting edited five times within a five
minutes or 10 x 20 x then we'll consider
this document to be hot it's something
that we should look at once you do that
in spark this is kind of very
interesting nothing actually happened
yet I just read my process description
do this tool then finally when I says
stop the print is essentially printing
the stuff into the console when we
finally get start this whole process
kicks in the avatar Malaysian actually
never happens because it's very hard to
kill streaming applications very very
hard they haven't built it they on that
assumption of screening application
streaming application is going to run
forever but essentially that's diately
any question
comments this is crap anybody this is
cool is just another transformation
everything right there it's having a
transformation to the string okay this
is like a Forex assignment yeah yeah
I did show you the code doesn't it okay
so under the hood what happens if we run
this but essentially happening here is
the code that piece you saw in my and my
ID there is called driver program forget
something don't think they do you start
what spark does is that fix that code
that you written in a driver program
goes to cluster manager could be
anything okay it's an abstraction hey I
have this piece of code can you run it
in cluster for me yeah so what that
means is that it will take for an
example all the transformation we doing
like a map operation could be sent to
multiple nodes here each of this node
what I'm saying is essentially a JVM a
process running that knows how to
compute the run your code now this is
okay this are actually a very trivial in
real valve I can't read five minutes for
what so if i could oxygen network
intuition like this the one of the
interesting thing that are then I
struggled a lot i'm going to put all
this good in hip hop by the way because
i'm going to experiment a lot with the
online machine learning algorithms is
it's going to look something like this
first three lines are pretty pretty
trivial by the way the look of those
three mins I'm along spark to Ronald
free course I usually make it star on or
whatever you have you if you're running
in hdfs missiles you specify that URL
here hey now this is the interesting
part right there there a streaming
version of algorithms that showed up in
sprach recently one of this is running
Caymans right game it makes a lot of
sense for this kind of problems that
that I can take all this need to a
network access log and kind of can
create cluster of them in any of that
shows distance from the cluster would be
something that we should look into that
very unusual it doesn't fit with other
access model right but one of the
biggest challenge of this any kk me and
said in a data scientist here we can
give some input here is essentially
coming with the value of K its own as a
problem right so we there this most of
the streaming algorithm needs to have a
model that can learn as it goes okay so
your k can change duty your network is
accessed the way next week it might be
accessed it differently right so you
your algorithm needs to evolve at this
one that's one of the biggest challenge
here the next thing about most of the
screening our algorithms fishing for
k-means here and have something or half
life that means your clusters will not
stay the same your clusters will move
you change as the data flows to the
system this half-life means that don't
give importance to the old data in you
will consider the new ones sometimes a
decaying factor after that it's kind of
pretty trivial essentially once you
create that then you can have this this
training data sources that means you now
essentially have to create maybe I
should go back here we essentially have
to create two streams one string that
you can feed into k-means as a training
data so that he can train them another
stream isn't anywhere you want to
predict the balance on this is your real
data source and this is a stream that
helps the k-means to live so so the
algorithms like this are are becoming
very important to solve our problems in
this space all right so why why this is
better it's actually better because most
of the tool out there it's not a perfect
solution I mean this is another
interesting space where types it as a
types if you are investing some time on
if there are certain areas in obviously
in a spark streaming or streaming in
general we feel like we have more
experience so why is it better because
of this this essential model right one
of the things that is doing here is that
it fits into the rest of the big data
ecosystem
that's why you hear this far beginning
so hard popular because it kind of
integrates with everything out there
okay so you're building this another
important thing is that when you write
an algorithm for batch process you can
also write them for streaming so some of
this ecosystem is actually pushing the
spark across the limit here and becoming
popular any questions butter any
thoughts any anything you guys want to
come in just a quick comment them though
as possible then you will not believe
this introduction because if you do
computer programming the IP I will run
the situation where like one why part of
your transformations is take my bandages
which four minutes garbage for sure so
you have to be aware of that that SAP
example we have the scar textiles of
things like that don't have to turn it
off in our sparkled basically because we
kind of do pure functional programming
within the calculation
yeah so okay so the point here is that
when you do a spark and function
programming you actually should do
functional programming but yes the
problem with that is that equal accurate
resources so depending upon how big job
you're running objects at will be
garbage collected I said because we will
have they are flowing around here around
your cluster and you have to be pretty
aware of that it's tough yes live only a
higher code looks but also how it
performs and unfortunately if you
strictly one hundred percent loss
program then unfortunately this is so ya
know you're too what else yes
no it's pretty hard to estimate the
first of all sometimes as if that's kind
of difficult to actually know that your
algorithm is doing correctly I think one
of the hardest thing about cave-ins is
to find the key number and there's ways
to do that because if you seek amy is
million for an example all the data will
feel like closer to each other so there
is a number the first thing that we have
to do even if you do streaming algorithm
figure out the cake and again a rinse
and repeat essentially I mean this is
you might have to tweak it as it's
running so that's one of the things
about this here knows that run your
application and your application is
learning do you have any store this
accumulates they value of K from
communication is inducing tonight like
around knows because if not then when
every one of your nonsense tell you
occasionally behave differently and each
node can have a different value for K
yeah but so a synchronizing a sookie is
part of the streaming model so the way I
was implementing that I was almost
passing the model around but yes most of
the streaming we're going to happen is
the part of the flow where you pushing
the Alex let's do some dashboard in
memory or whatever there will be another
pipe going in to maybe maybe HDFS or
casting or babysitting all this
information or acosta this is very
common you you read the data from Africa
process pipeline dump the transformation
so that another the data pipeline can
pick the transformation and further to
our
yes using the same process very good
point so so that I guess cook everybody
got a question should read it ok so so
saying the streaming version of
algorithm does look different it has to
be different then because it doesn't
have this whole whole building model
based on large data set what I am
pushing for is this stools going to get
better and better that's where the
momentum is I am pushing for is that
your batch you don't have to have a
batch system anymore you have streaming
all words
not necessarily I mean the suitcase
swinging isn't as a part of an example
there is more than that like about that
way I was building the pipeline right
maps and fragments and filter imagine
that being reused in both and batch and
streaming that's a huge possibly the
only that you can run sequel on stream
data in spark I can dump things into a
table as the data is flowing through so
that my data analyst comment quite in
that data so that kind level of
integration is kind of very important
but that's a good question anyone else
yes what what do you think about your
purchases on the storm and also what is
it wondering other questions on the
programmer friendly API remember one of
the wish list was getting sparks a big
storm there because the the reach API
said that if gate with spark is
completely non-existent and strong and I
think that's the indication of fraud
thank you very much
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>