<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Stop Guessing and Start Measuring - Benchmarking in Practice - Tobias Pfeiffer - EUC17 | Coder Coacher - Coaching Coders</title><meta content="Stop Guessing and Start Measuring - Benchmarking in Practice - Tobias Pfeiffer - EUC17 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Stop Guessing and Start Measuring - Benchmarking in Practice - Tobias Pfeiffer - EUC17</b></h2><h5 class="post__date">2017-07-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/C4hqcLwxs3A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so you might know the situation
someday you're in some meeting and
somebody goes like oh this thing is too
slow and they're like what do we do
about it you're there you're taking a
meeting no.2 on battery power so you go
on and then somebody says like that
thing where we repeat that thing 100
times that's way too slow and you go
like ah I know just what I need to do so
then you out ooh wipe out your Oh whoop
your okay maybe here better you wipe out
your shell and then you say like okay
got this I'm gonna see if we can make
the sing faster or okay this is very
good right now and so you type a module
that you say like okay let's see if we
okay keep this okay tonight okay well
okay now I'm bound to my desk but we'll
manage
okay so I rewrite the module that you
can see here that says like okay I want
to repeat a function it's a recursive
implementation and that's what we
currently use in production let's say
and we say like that that's what we have
to do so now we find the time of
function in Erlang and we measure it and
it takes two hundred ten microseconds
like okay but what about if I just do a
plain old enum each on some list and
angle I go one at 65 microsites though
that's faster great and then well people
always say that tail recursion recursion
is so fast on the Erlang VM so I'll try
it a couple more times and it's still
faster so that did success we know
definitely that all those function
people of the recursion we're wrong all
the time killing my enemy each is much
much faster and we can all just go home
because clearly this was to talk about
benchmarking and everything is fine it
is not because this is clearly an
instance of I have no idea what the hell
I'm doing here
so there's so many problems with what
I've just showed you that I
where to even begin like you did it on
your development machine while you were
taking notes probably select was running
you were in battery power then it was in
the shell it was not like an actual
compat pro with no production like
environment and then also it's repeating
of something it said really a bottleneck
have you measured that do you know that
that's the boiled egg no you don't you
just wiped it out then you also took an
arbitrary number of 100 things so it's
it's quite bad because like whatever you
repeat that many times
I guarantee you that the function that
you have in between that you call that
many times that's more likely to be the
bottleneck than whatever you repeat
there but there's much more stuff that
we will get into during the presentation
but even if we assume that repeating
something is the bottleneck this was a
very very bad benchmark for us because
it was no benchmark I just had one
invocation and doing that one invocation
something else might have spiked up and
eating all your CPU garbage collection
might have kicked in and that's why that
one run was botched so here's a proper
benchmark where we define like okay we
want to have 10,000 elements because 100
is also a little bit small as we had
before and then we define one for in um
each we have also one for list
comprehensions and one for recursions
and now this benchmark will be run for 5
seconds over and over we will aggregate
all the results and now let's have a
look at the results and then we see it
actually recursion is much faster than
the in of each approach and also faster
than the list comprehension and even by
quite a large factor like over two times
faster than this comprehension and still
a lot faster than in a Meech what you
will learn from that we can't just take
random measurements at some point and
see like ok this is my bench like now
observe this one time it was faster
that's not how these things work so we
need statistics as well to prove what's
faster and here as we can see recursion
Assessors so that's my talk stop
guessing and start measuring benchmark
practice as was introduced
I'm Toby you can find any interwebs at
as a tractor we also have a look at the
library that I offered and maintain
which is called bench II and I'm from
Lees Ferry where we run to Alex the
applications and productions and
otherwise Ruby and where I also use my
benchmarking knowledge to make try to
make Ruby and rails faster and to make
and Alex even faster when you Rimmer
then he'll f linskey apart
bah blah so your intestines curcumin
from this plant yo coming from between
man yesterday Ella pool baking a nice
caboose :
a cocoon of Ottoman adore the very hot
mousse over there in Pakistan have seen
the yet broad glass so needy
ECAD corner Skadi oka teary glassy Aron
Yavin herb consultant so you have even
repair men me
may your listener to refused in the
probe menu a Apple stock own okay there
I mean saucer contents a failure so yeah
to adhere the plot a little sense can be
desolate y-yeah
photo sense car so that's gonna have oh
the video am so actually I wanted to
show you a video of my rabbits but so
some reasons running very very slow
right now as a as a little I don't know
it works as a little excuse for talking
to Swedish to all the non Swedish
speakers for so long and why do we get
back on topic with this video because
you can think of wench marking labeled
of these rabbits that eat their food you
put some input to the function and the
faster they chew through that that's
what you want to measure in a benchmark
so you basically have a race of free
rabbits eating something very professor
you want to see which rabbit is faster
and sometimes you also just want observe
if one of the rabbits is slower or
misbehaving so if day because if they
eat too slow for insensitive might be
bad so I hope that recon tells you a bit
for hammering some broken Swedish at you
so ah this doesn't ah this doesn't work
yeah okay
oh thanks so n talks often we have
either talks about the concept where we
hear about like all how accountable and
them but then we sort of miss the
practical application of how we can
apply that contact that content into our
daily lives and in the other hand we
often have talks to just show how one
tool works and
like oh this is this tool but then
that's fine you know how to use it for
programming language X but you can't
transfer that knowledge to something
else so what I try to do in this talk is
I try to marry both of them together so
I'll tell you about the higher-level
concepts and then I show you how they
materialize in Benjy and in Alex series
so you can use it right away by the way
who here would consider themselves like
primarily Erlang developers most okay
primarily Alex here see okay cool but so
it's good that I put in the work so the
last bench release which is 0.9.0 has
some nice things to make it run easily
if you want to call it from Erlang so
like all the functions are now defined
on an atom like in a proper Erlang you
can also use atoms as keys and
everything so it is usable from Erlang
but the bridge is still not too easy
because so I forged that because like
Erlang packages are know also like the
RIBA three pages are on hex p.m. and we
all run on the same VM they would be
very very easy to call a lexical code
from alone it's not and it's like I
don't know from from the JVM world on
the other side they always say like if
you release the library release a jar so
that everyone can use it that you'll be
a good JVM citizen I don't think we have
that in the beam VM community or Alex
here quite yet and so I was very sad
because was very hard for me to set up I
also don't know that much Erlang but
there is a river free plugin which is
RIBA free Alex it compile which is
actually from Beryl DB someone from
borrow DB also gave a talk yesterday but
I think he's not here it's sort of works
by having problems with it so there's a
list of issues if you know Reba free and
any lifted stuff if you won't help me
off of that that's cool but you can call
it from our line so this is also usable
for you and the library and I hope I'll
show you and community to maybe use it
all set them an Alexa project that then
has your along as the defensive user
from there so before we get into the
needs of actual benchmarks we have to
talk a bit about fear we first because
like so what's the difference between
profiling and benchmark we also had
talks about profiling here and what
profiling does is profiling gives you
these nice graphs like a flame guard
cause same graph or
tres that goes like this particular bit
of that took that in that much long and
or dead longer than the other thing but
profiling adds a lot of overhead because
it has to add lots and lots of
instrumentation everywhere so the Oval
execution gets slower usually I think in
the BM it's not as hard but if you do it
in Ruby it takes forever and so this
profile is very nice to see like what
part of your function if you're not sure
takes takes up all the time whereas in
benchmarking we don't add to the
overhead we just see like how fast is
something and it's better for really
comparing implementations and everything
when we do benchmarking something that
we have to ask ourselves like what do we
want a benchmark like what's the valued
every one optimized for and we have a
couple of options we have run time which
is the most popular but we could also
optimize for memory usage because maybe
my memory usage is blowing up and that's
why I have to scale up my servers can
also benchmark for throughput or some
custom benchmarks and honestly my
favorite custom benchmark is the Phoenix
wrote to 2 million bucks sockets
benchmark so what a benchmark therefore
it's like how many WebSocket connections
can we make to one singing machine
running the Phoenix the web framework
for Alex here that is very good with web
sockets and it started out at about
100,000 and within the course of I think
a couple of days they made it to 2
million by doing some performance
improvements and even when they had 1
million 2 million sorry 2 million
clients connected Joe's a little troll
that he sometimes is posted I think a
whole Wikipedia article in there and it
still delivered to all the clients
within I think 1 or 2 seconds so that
was quite amazing and so that's a custom
measurement gets not just run time or
memory buddy but something different and
it ran on a very huge machine but not
even MEXT out yet so it stopped at 2
million because they set the file limit
handle for 2 million because therefore
they would never get that high it's one
of my favorite benchmarks of all time
personally so what are we going to talk
about here we're going to talk about run
time because run time is what people
mostly talk about and when she does get
support measuring memory consumption
we're working on that so there's like a
pull request where we try to get that
feature in so we can also see because
sometimes it's a trade off sometimes you
make something
sir but therefore you consumer memory
for instance if you introduce some
caching or memorization so that also
important information for you to know
when you optimize your functions
another thing we just always don't want
to know like what's fastest but
sometimes you also want to know how long
will this thing take that I just did and
so for me for Benji I wanted to know how
fast enormous orders and Alex here
because to compute some of these
statistics I need to solve all the
runtimes that I have and if I run a
micro benchmark it's very easy to in
five seconds got a millions of inputs
while might be amused by one of
thousands but I could have millions of
inputs I need to sort and that's why I
also see like what's the maximum value
that showed up in my sample set and so
you can see for a list of five million
elements or maybe you don't see that
well I should maybe still stand here it
took a little bit more than two seconds
which is still doable when you run a
benchmark you can run you can buy the
extra two seconds in the end I would say
but that was just want to see if it's
feasible or F or if I have to implement
some fancy algorithm that other than
linear time which I had no intention of
doing and so still don't have so the
other questions of course are did we
make it faster and what's fastest and
here I have a little quiz for you what's
the fastest way to saw the list of
numbers largest to smallest we see the
benchmark here so we have a list of
10,000 elements we shuffle it and then
we have three candidates first gain
weight again first let's go through the
candidates first candidate just sorts it
and uses the custom sort function so
that we say that the first thing is
bigger than the second so that ends up
in a reverse list then we have okay we
just sort a list and then we reverses
and then we have a clever trick because
we know we have only numbers we say sort
by minus that value which will also end
up in the reverse list so now the task
is for you which one of those do you
think is the fastest of these options so
who thinks that the first one the custom
sort function is the fastest one person
okay three okay we get a slightly
different result each time who thinks
that sorting and then reversing is the
fastest that's about like say twenty
people and two things that salt
is the fastest a bit more than before
and so what actually fastest is the good
ol just sold and reverse and it's
actually over four times faster than
salt by - value and that to me was a bit
mind-boggling when I first encountered
it back in Ruby because I thought like
what but like I really tell it what to
do in otherwise they just sort of
stupidly reverse the list like what's
what's the point and but when you think
about it a bit more reverse in Erlang is
probably implemented in C so the very
very fast operation because we need to
use reverse a lot of times when we do a
at a recursive function and then we need
to reverse it in the list in the end
sorry and so reverse is highly optimized
and lady salt is also more optimized and
passing a custom function and that's
what just one of the cases where your
intuitive guests oftentimes fails I mean
I would say about one third above the
room got it right but the other people
got it wrong so that's what we need
benchmarks but we can't just trust our
instincts but at this point you might
also ask like isn't that the root of all
evil because you've heard that somewhere
right like somebody said pretty much a
performance optimization is the root of
all evil you've all heard that I guess
for me it's more like not reading the
sources it's the root of all evil
because people just quote papers but
they don't ever read the papers all
right people read the papers but too
many people don't read the papers so if
we look at the paper then it says we
should forget about small efficiencies
say about 97% of the time premature
optimization is the root of all evil so
it's there clearly you're right you
don't need to benchmark we don't need
performed optimization everything is
great
don't ignore one of the followers of
modern programming said so in 74 the
very next sentence in that paper if you
just read on says yet we should not pass
up our opportunities in that critical
three percent a good programmer will be
wise to look carefully at the critical
code but only after the code has been
identified that goes into profiling and
general application performance
monitoring you should know what is slow
and what actually matters so that I can
improve it he also goes on to say that
he thinks that the 12% improvement in
performance is really significant so
everything that at least gives you 12%
performance improvement is very very
good and everything that I'll show you
here is at least 50% and not it's not
twice as much
performance or predecessor sorry it also
goes on to say what I just ended that
oftentimes the first judgement that you
make your intuition is very very wrong
about what's fastest and what's slower
so you always need to measure so once we
have covered this space as we cover
another base because let me just say
benchmarks what even is a benchmark what
are the benchmark cover and there's many
many different types of benchmarks and I
personally like to liken it to the
testing pyramid I don't know how
familiar people in the living space out
of that but it's very common Ruby that
you have like first unit tests that just
tests a single unit like one single
function then you have integration tests
cup of part play together and then your
feature acceptance test we mostly do web
applications so it's mostly some
clickity thing that goes through the
whole browser and tests everything on a
user level and I like to put benchmarks
on the same scale so what I would test
with unit tests it's micro benchmarks I
just test one small little function
micro benchmarks do a bit more together
and the application level goes through
the whole stack that's a bit hard to
imagine I think at least for the
application benchmarks - what's an
application level benchmark you might
have seen this this is the I forgot its
name right now I'm sorry they did
something shoot up where they compare
web frameworks like how fast can they
handle HTTP requests and update
something in a database and that sort of
thing and that is lots of different
languages application level benchmarks
can even be independent of the
programming language because you just
use a tool like WR k2 that just fires
HTTP requests edit server measures how
long they get back and then get like the
meantime the throughput and everything
else that's an application level
benchmark so what are the properties of
these different benchmarks first the
components involved go up as the father
we go to the right because micro level
benchmarks just what I showed you before
we saw the list very little components
and then it gets more and more at the
same time the set of complexity gets
higher so if you run an application or
even a macro level benchmark on let's
say a web application and you don't have
any data in your database what's the
point you don't have anything to
benchmark against
also the execution time goes up I can
get good results with micro benchmarks
very fast but the bigger what I do is
the longer
takes but at the same time on a positive
note the confidence of a real impact
goes up because when we do these micro
benchmarks like I showed you before the
one with repeating something if I make
that even 200 times faster there's no
real guarantee that it would make my
application to run times fast it felt
like it won't so a certain rubric
committee was lots of hype that now reg
X is a fast and Ruby like ei but that
will make my implication maybe 0.5% fast
I mean it's cool that we do that but it
doesn't help my application as much as
many people will make you think it does
but at the same time also the chance of
interference gets bigger so if you do
actual HTTP requests to some server
somewhere you might have a network split
or whatever like something might break
so you might get extra wrong results
also if all the setup complexity I
showed you these application level
benchmarks there we found out that some
of this benchmark like they run on like
an 80 core server whatever but some of
these language implementations just
spawned 8 frets so they are not using
all the course so the the comparison is
basically irrelevant
I would say most of the time when
someone shows you shot like this they
will tell you that this is the gold
middle this is what you should aim for
and it's a very good spot to be in so in
that development I usually just take the
code that one controller action like
that one HTTP request triggers and a
benchmark that code that's fairly easy
for myself to set up and I'm still very
very confident that it will have a real
impact of what I do and at the same time
we'll just talk about micro and macro
benchmarks see because as I said
application level benchmarks that often
language independence so we'll focus on
that so let's get to good benchmarking
first and foremost when you want to do
good benchmarking you gotta ask yourself
what am i benchmarking for what am i
trying to improve am i trying to improve
the user experience am I trying to
improve the server scalability does this
end point really matter like if it's a
very slow end point by the codes once a
week honestly who cares because like
maybe if the users my CEO maybe I care
but otherwise I don't but if it's a even
slightly slow endpoint that gets called
2,000 times per minute then I really
care about it and so that's what you
gotta be conscious of secondly
tell people what you run your benchmarks
on first
it should be very close to your
production system or as close as you can
possibly get and otherwise results might
differ for different Alexa versions
different Erlang versions or just with a
different operating system or whatever
is underneath or a different version of
the database so tell them what you do
which is why Benjie when you started for
the first when you start the benchmark
it will print out already a summary
that's a new feature in the report nine
of like what's the the process you have
how many how much rum do we have how
many CPUs we have so the goal of that is
whenever somebody just copy and paste a
benchmark download Benji I want to have
all the information that I need from
that benchmark to see what I can do and
I want to help the people to write good
benchmarks and provide the information
to everyone yeah
also please create an interference free
environment I sort of blamed slack
before and I'll do it again
sometimes slack just hogs I don't know
fifty percent of my total CPU time and
then when I run a benchmark it's twice
as slow as it would normally be
so really close spotify close lag close
all of that and be sure that really your
program has the time to run because it
has on the server and you don't want
your local machine interfering with any
of that and also one of my favorite
mistakes for benchmarks when people post
on the phoenix mailing list o Phoenix is
so slow a for the most fast is that
they're still running in development
mode with like everything logging turned
on to the fullest and that takes most of
the time of the actual benchmark so try
to get out of that also something that I
realized is might be slightly more con
controversial here is garbage collection
we can't turn off to the best of my
knowledge please correct me if I'm wrong
we can turn off garbage collection in
the LLVM
but why would I want to do that
especially for micro benchmarks if
within one of those micro benchmark runs
garbage collection triggers and hits you
you get a spike and that's you probably
can't see it quite well maybe it's
better on the monitors this is a graph
that also been generates for the bench
HTML plug-in that shows you a
chronological list of all the runtimes
and you can see that there's a very blue
base level like almost all the measured
times are within
here but then sometimes spikes go up and
I'm I can prove it to you right now it
could be something triggering on my
laptop but it could also be or I'm
pretty sure at least some of the
templates garbage collection trading
like okay I need to collect some garbage
now and for micro trends for micro
benchmarks that really ruins your
results because it sort of gets the
average rate up where it shouldn't be
but of course for application level
benchmarks and micro benchmarks please
don't turn off garbage collections
because garbage collection is a real
thing that your program does get me to
another point you need to have a really
correct and meaningful set up that
especially also the applies to what I
said before like the phoenix development
like your environment should be the
production environment it should have
all the caching everything that you have
in production and not just your
development event because usually with
debug information is very very slower if
you do compiled languages it should be
compiled with - - optimized - mine's
optimized free that's a lot of the
faults that I see with benchmarks in the
wild and also not completely related to
a length because Eldin doesn't have a
JIT yet and I would really really love
for a long temperatures so if someone
hears in the OTP team please make the
JIT happen I mean we saw it's it's in
the works but I would love it especially
jittered languages like the JVM as a big
model and often time I see people make a
benchmark of the JVM they just make one
call or just make code for the first
second and vary that oh these results
are so bad clearly the JVM sucks it's
like no the JVM especially with big
applications you gotta give it sometimes
five minutes of warm-up time like for
smaller benchmarks maybe a couple of
seconds but just executor functions
don't get the measurement and then after
it's warmed up after it has done all the
cheating because during cheating is also
slower then can you get the extra
measurements out there something else is
very important to me is that inputs
really really matter when you do
benchmarking I see so many benchmarks
but people just take some arbitrary
input and don't benchmark against
multiple different inputs and inputs are
like the skyline they are very very
different in size and shape and
everything and sometimes really weird
inputs can be your worst case of
performance so I believe the worst case
performance of quicksort is an already
sorted list on reverse order list that's
the worst case
and maybe one the benchmark for that for
the worst-case performance and otherwise
you also need to have different input
size to see how behavior changes and now
it's time for a little story that I have
for you so I said like I work at livery
we're a same day delivery company and so
we have an application that is called
the career checker and one time our back
snag lit up I was boom okay well boom
boom okay boom like that so the boom and
I'll you what what what's happening and
I go with Alex ADB connection error I
was like what what's happening like
actually super fast like and like super
reliable and everything like why would
that happen I look in the lawsuit like
okay DB timeout
15 seconds I was like what did we do an
SQL query that took long and 16 seconds
that can't be
and so like what the Korea trick is does
is like we have Korea set of our Android
application and they push the location
updates via a socket to our career
tracking application which then goes to
admin back-end so we can see where the
careers are and a very common thing that
we do is we want to get the latest or
the latest location of a gram and okay
this goes bad again so this is what we
do and I saw that the fold was in that
code so was like okay I know this I know
benchmarking I'm known the benchmark
this so this is the benchmark it's it's
a real case as I just told you and I'm
going to go through the different
implementations very quickly this one
appear is a database view which is what
we used until up that point which is a
database view that just test like really
all the latest locations of all the
careers as the database view and then we
just get the one with dead career ID and
then we just get that one location and
the ones down here are mostly the same
so we just say like okay we want to get
from the cure location so we want to get
a member of our curry ID we want to
order them descending by time and we
just want to get one and we want to get
that one and the only difference between
those two is this one is full custom
it's called because it does a directory
ID that thing and that one uses a
something that we've already implemented
a scope but that user sort of like an
array
two things so it might be slower so and
I also saw that for that one career for
which that back was happening we had 2.3
million locations in the application
because there was a bug where it
resubmitted locations all the time and
we didn't reject them which we fixed in
the meantime so I took I took a database
with those 2.3 million locations and I
ran it
I run a benchmark and I saw or you might
see that the Rif courageous and order
and the full custom is much much faster
than using the latest cure location
thing like that one was super super slow
now we're super fast so we're great make
the fix deploy to production boom it
goes it goes HTTP connection errors I
know what but I fixed that one I bench
melted I know what I'm doing
not Alexa TV connection error and lots
of them more and I go like what is
happening I don't know like I benchmark
this I I know my I thought well I
still do so what happened again I looked
at like which of course I wrote back the
deploy and I saw it like okay what
happened and I looked at it again and I
saw that the Korea location for which it
was questioned right now were curious
for which it had no location at all like
there was no location so I wrote another
benchmark where I said like okay these
are the career at least that I have and
by wait this is my favorite Banshee
feature that I don't know if any other
benchmarking framework is I can give a
map of inputs up here and then I just
say okay Banshee I can present some
configuration options like pd's take
those inputs and then whatever the input
is as passed in as a first parameter
into my benchmarking function so I can
run the same functions or the same set
of three functions with four different
input so it will run twelve benchmarking
jobs so I can have a good overview of
which function performs in what way with
which input a feature that actually
originated when I was contributing
something to Alec C because I chose a
was like oh but how does it pay for
Fatima's I would like good question this
let's do that okay so I have one that
the big one with the 2.3 million
locations I have one with no locations
one with 200,000 locations most 20,000
locations so now if I run that and the
old means now like what the code was at
that time and we can see the
at our latest cure location the database
view the old thing that we used it's the
slowest when we use it with the one of
the many many locations with the 2.3
million but as soon as it gets less and
less locations it becomes the fastest
again it's way way faster than the
others and yeah especially for the case
with no locations the other ones are
super super slow and this one is fast
and this is so counterintuitive because
we always think like ok I want to see
how fast that function is let's just
throw like the biggest amount of data
that I have added and if it can handle
the biggest amount of data surely it
will be fine and there wasn't the case
here so just another time to see it here
my more custom versions they're the
fastest when we have many locations but
afterwards they're by far the slowest so
what happens I ran in explain analyze
and Sigma on it and I noticed that it
didn't use the korea ID index at all so
at that time we had one index on korea
ID and we had one index on time it just
used the time index too which makes
sense because we want to sort time
descending these so it's all at the
times of all locations descending lee
then just look through and try it for
the first time to find the one with the
Korea T if we have no career ID at all
it goes through all locations
hence it takes so long so of course
what's the solution to that it's a
combined index we didn't have that
before which is obviously a shame but it
ran fine until the time so created a
combined index on Korea ID and time so
that it first looks a curry ID and then
can look at time which is what we mostly
do because we all we ever want to have a
location for a specific career in that
application so on the combined indexes
now we can see that consistently
consistently the full custom
implementation it's the fastest and so I
was very happy with that deployed it but
before I did that I was like wait a
second before I meant something else up
I read another benchmark and I wrote a
benchmark for insertion time because
especially when you start adding in
access to records over and over your
insertion time might go way up so that's
also a very very important thing for you
to measure optimally at all times that
you see like ok if I add this in there
is a trade of my insertion time goes up
but since we read their data we're more
than we inserted and we
never update a location actually and
it's fine
and that's just something like if you
take nothing else out then you should do
more benchmark from a pls benchmark with
multiple inputs it's also very
interesting I I have the example later
on as bonus slides but when you when we
do tech optimization it can be that not
tackle them if we do recursive functions
and the performance characteristics of
body recursive functions and tell
recursive functions can be different by
the number of inputs so for smaller and
put the body recursive function can
actually be faster than here recursive
functions and then but for bigger inputs
like 5 million characters the functions
become faster again so thanks for
sticking with me for so long so I waited
the excursion to statistics until the
end to tell you all the funny numbers
that you actually see there and not to
give people away with that in the
beginning and to talk about these
statistics I figure like I take the most
commonly spoken language between all of
us which is a common denominator Alex
here so the average you know it's pretty
easy we take the total time and we
divided by the number of iterations it
starts to send a measure that you see
I'll get into why it's not the best
measure in a moment so then we have
standard deviation which was also
brought up in the keynote this morning
which is basically we always deduct
whatever sample we have right now from
the average and we square that and we
add that all up and then we divided by
the number of samples and the standard
deviation is a measure of the spread of
values that we have and of course in the
end we also take the square root again
so why why do we do that
again because we won't have a measure of
like how spikey our distribution is like
how many outliers are there and this is
again the world one times they go by
time that when she generates that show
you like here's some spikes but that
might just be in the beginning or
something you want to see how spikey it
is and this is another graph that bench
generates it's a histogram and it
basically takes runtimes and puts them
into buckets so you can see that here in
this runtime buckets there's quite a lot
of them and there gets fewer there so
you see like this bucket is the one you
hit most commonly with your runtimes
but you can also see like why does this
graph or go all the way to the right
because there's a little little bastard
that you don't even see because some
runtimes
were in there and those we call outliers
because they're not typical for the
distribution so they're too far away and
that's the one things that you want to
see like how many are there and that you
might also want to get rid of and by the
way this is a distribution it has very
low standard deviation a finger was even
below 10% at a very good standard
deviation if we had a distribution with
like 50% standard deviation this will
look very different and also in the
grass before you might not have seen it
in the normal bar graphs you see here
this little like feeling that goes up
and down that's a measure of the
standard deviation so they can already
see like how big is the standard
deviation how how does this how do these
results varied you can see if they sort
of close together and the standard
deviation bars overlap then it can be
like okay
maybe it's just randomness that caused
this result so something that I'm a big
fan of is the median and here we go into
why a benchmark the sorting time because
the median is basically you solve all
runtimes that you have and you just pick
the one in the middle and you say like
okay this is my median it's so the
middle value in there and why is the
median so cool it's not affected as much
by outliers as a little example so this
is a distribution where we have about I
think was 15% standard deviation the
average is here and I don't think that
the average here gives me a good
indication of what the actual
performance of the system is because I
know 90% of all my run times I actually
faster than the average so it's a very
bad value because it doesn't give me a
good good knowledge about what's the
typical value or what's a valued atom
there most including it also does give
me a good measurement of like what's a
bad value that I'll account a lot of
times but the median is up here and and
that's very very let's say typical value
or I think it's a better representation
of the benchmark results as a whole than
the average here also box plots when she
also generates as one of my favorite
things so who has seen the box plot
before who you knows what I actually do
and like what all the lines mean Oh
okay still a cup of people so for the
others I'll say so the one in the middle
is the median we just talked about that
and then from the median so the medium
is I have to be careful with this and so
the median is sort of in the middle and
once you have that built you take the
other two halves and you also cut them
into halves so you go here and here so
the upper and the lower half this is
where this line comes in and deadlines
comes in that's Steve the quantile so
you sort of chop it up into four regions
and the two regions in the middle are
the quantities that you look at and this
is quantity two on quantity free and
this one - this one is the interquartile
range so so if you go up from the upper
one by one and a half interquartile
ranges you see this one year
so this says like okay shows you it
gives you a measurement okay results of
the run times go all the way up here and
if something is even outside of that
inter contouring you draw a dot which
means this is an outlier it's okay down
here again because it's so far off tell
interquartile range so that's that's the
box for it's very handy for see how are
the values actually distributed oh so
okay so a little bit more about Benxi at
this point I think about Ben Chi I've
written a small benchmark library
already in Ruby but what I really liked
about functional programming and
immutable data is that let me to this
design of Benjy which I personally think
is very very nice we have one if you
don't know what the pipe does in Alex
here it basically takes this argument
puts it in as the first argument there
and then whatever is returned from here
is put in as the first argument into the
next function and so on and so forth
which beautiful itemized and so what I
do is at first we get a configuration
when she is initialized with the
configuration then Benjy and gets system
data and all of that during the whole
process we have a big I call it suit as
a big data structure and all of that
data that we gather through these
separate steps is added as edit to the
suit so for instance here we had a
benchmarking job then
we call measure which actually runs the
benchmarking job and produces role
runtimes then we generate the statistics
and then we have different formulas that
based on all the data that I've gathered
so far can give me console output CSV
Jason and HTML output percenter graphs
that you've seen so far and this is also
an official interface to Bengie like
right now we already use the more
convenient been she run interface but
this is basically what it does
underneath and is also officially
supported interface so if at any of the
stages you want to go in there grab the
results do something yourself of it
you're welcome to it that's what it does
and that's how the data structure does
what the data is what the data what the
data structure looks like and as you can
see there is a key for each of those
major steps that said like this one it
has a config the other one for system
jobs run times and so on and so forth so
that's very easy to work with in the end
as well so there's a couple of plugins
that I mentioned a little bit who love
this is um okay
what um I go on maybe if I hold just
close to my chest it seems to work he's
coming can you okay good let's try it
again so one of my personal favorite
benchmarks is the flat map vs. mapped or
flattened benchmark you can see here
again with multiple inputs and so on and
we have two versions flat map and nap to
flatten and when on the benchmark Bay
okay
when okay so yeah really close sorry
so when we run that benchmark what do we
see back in Alex here 1.8 is something
that I wouldn't have expected because a
flat map is far more specific than
mapped or flattened but method flatten
outperforms flat map by quite a huge
amount and it's like what why is that
happening that's not my expectation at
all coming from Ruby or Scala or
whatever and but you know it's fixed in
alexia 1.4 so here you can see I mean
the graphs have looked the same but if
you see flat map is no up and mapped the
flatness 1.5 times slower than flat man
so how did that happen well it was
actually the first benchmark I ever
wrote of the first version of bench in
was always weird for me that it was slow
because it should be fast the flat map
is specific so I wanted to fix it at
some point by a look at the flat map
Cohen was like oh well so lil maybe
another day and at some point I opened
an issued Alex here was like a sled may
be slower than map flattened and our
larynx lists flat map is also way way
faster and that's like this needs to be
faster I think and Josie answered within
18 minutes and within those 18 minutes
he wrote two alternative flat map and
limitations that are both faster than
what we had at this point I was like
what
so I benchmarked them and both of them
are fast that I still don't know how he
did that in 18 minutes I think I always
think I'm a pretty good programmer but
that's something that will always boggle
my mind and admire Joe say so in the end
I just say said thank you for
benchmarking and that's also what I
won't giving you like do these
benchmarks if you find something that
also slow and in the library people
usually appreciate it when you give them
the bench and be like here that's also
I'm also part of the Wallaby team right
now and we know we're a bit slow and
also slower than Hound but where we're
trying to get faster but we leave at
least half the benchmarks have a
baseline to see do we get faster does
the performance optimization that we do
actually matter and so as to say thank
me for benchmarking I want to thank you
as well enjoy benchmarking and
yeah okay so any questions otherwise
feel free to come up to me I'm always
happy to talk to people and he said I'm
very sociable no questions I post the
slides online there's also some bonus
slides in the ends of merge two and
merge free and stuff but yeah thank you
a lot
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>