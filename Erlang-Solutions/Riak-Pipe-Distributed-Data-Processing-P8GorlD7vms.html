<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Riak Pipe, Distributed Data Processing | Coder Coacher - Coaching Coders</title><meta content="Riak Pipe, Distributed Data Processing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Riak Pipe, Distributed Data Processing</b></h2><h5 class="post__date">2013-02-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/P8GorlD7vms" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">now before we start can I ask because
anyone heard of react pipe before okay
is anyone using react pipe to do your
own development work it's separate to
say the usage of reactant you might have
in your cluster not perfect okay because
this talk is really meant to let you all
know as Erlang developers that there is
some tooling out there to let you do
distributed data processing work on a
dynamo based system for a scalable
distribution of pipelining work
basically so you could argue that we're
with structurally different too what's
achievable with storm if you know of the
storm data processing platform Twitter
you storm quite extensively but it's
possible to achieve exactly the same
results in a fault-tolerant way using
react pipe now first of all who am i I'm
part of partial aamir so I'm part of our
London office which is basically about 9
to 10 months old now so we're still in
the phase of really establishing
ourselves in Europe but the team is
growing all the time and I'm one of the
client services engineers let's go okay
now my first question was who is using
black pipe now my second question is
who's using react or does anyone know we
are core okay so
that pipe is an abstraction layer on top
of react or and I hope that I might need
to go through my course of the first one
third of this presentation is going to
react or in a little bit of detail to
let you know exactly how we structure
our dynamo implementation it's an
abstraction layer that instead of V
nodes which is the level of abstraction
you have access to in react or it's a
higher level abstraction that allows you
to create stages and pipelines and
fittings which allow you to basically
run a sequence of processing functions
on input and send those outputs to other
processing functions across your cluster
so that valleys react pipe in a nutshell
it's not developed by me it's developed
by someone much much cleverer than me
Brian think you can find him on Twitter
as hobbyist he's one of our senior
engineers and works on a lot of the cool
stuff that's in react in fact all of
this work with react or react pipe kv
because there were pieces of react
they're all separate early applications
that you can integrate into your own
stack and do your own data processing
work or your own storage work and it's
all on github so go in and download it
and build it so what's react core at a
high level description is a toolkit for
building distributed systems ok it's a
it's the lowest level of abstraction on
a dynamo system available to you that is
in the airline community today and it's
used as a foundation for a whole bunch
of other subsystems if you remember the
architectural picture that that Stuart
showed you just previously the pieces of
react as a database engine rkv storage
application and the react search
functionality all sit and communicate
directly through react or into into the
V nodes and it provides this idea of
virtual nodes virtual nodes are an
abstraction mechanism on physical nodes
that allow you to divide up a key space
some sort of hash hash space of some
kind that you can allow vinos to be
distributed a number of inos on a single
physical node which means that when you
add new physical nodes instead of
handing off instead of dividing data
partitions from existing physical note
you're handing off these virtual nodes
and saying I don't what I'm not going to
look after these that whatever these
vino's are managing these worker nodes
are managing I'm going to let this new
physical note do that so it's a way of
minimizing data reshuffling but
distributing whatever your hash space
problem is whether it's in react terms
some sort of storage problem or in data
processing terms a number of what of
worker processes you want to make
available across the cluster to actually
do transformation work on data by the
way please feel free to jump in at any
point and just ask any questions that
come up that
as I said before it's up online on bar
shows github account now if you want to
be able to pull out react or play with
it build your own applications and you
don't want to look through the reactor
code base itself then there is a
separate application called basher banjo
which is an example application that
takes a MIDI file an audio file and and
asks different nodes distributed on a
cluster using react or to write that
information to audio sinks that might be
attached to those different servers so
you can play music from different
servers basically now without scaring
everyone with what what's involved with
react or this gives you an internals
idea i'm going to make these slides
available at the end but this is all of
the underlying architectural foundation
that goes into react or that we expose
to you to be able to control this dynamo
system distributed system like I said if
you if you just want to work at a high
level abstraction this is where react
pipe comes into play but you can always
drop down to a lower level and talk
directly to vinos to to grab hold of
that compute power or that storage space
or that memory space so I keep talking
about a dynamo system react core
implements what you need to be a dynamo
based database engine now a dynamo
system it I think Stuart's already asked
this question but some of you will have
heard of dynamo before it's based on
research work by amazon that was put
together around 2007 and it's this idea
that that you want to build a system
that allows you to consistently spread
so
you to take something this hash space of
information and I'll show you a diagram
that gives more detail into this later
on but take this hash space and then
allow you to consistently access
particular V nodes that are on physical
nose across that cluster and it's a way
of coordinating that work so data
distribution consistent hashing tunable
replication management so this concept
of fault tolerance is built into react
relies on this n value of three these
three replicas being stored on V nodes
that are on physically distinct notes
all of this is powered by that that
level of abstraction based on Amazon's
dynamo research work and the purpose of
their design at the end of the day was
low latency and high availability ok so
they will they were willing in there in
the example that they give in their
research papers at the shopping cart
example ok they did they did some
research work and they determined that
they were there was lost revenue because
it was taking too long for customers to
go through the purchasing stages of
building their shopping cart and
actually purchasing and paying in going
through payment gateways everything else
that process was taking too long it was
that the latency times were too high and
people would come back to it later never
come back to it and never finished a
purchase and they were losing revenue so
they they realized that the only way
they could increase revenue with their
existing customer base was by lowering
the latency time to build your shopping
cart and go through your purchasing
habits and buy whatever you want so
that's why low latency is at the heart
of a dynamo system as is high
availability because you know
the day and node a physical server that
goes down is just extremely high latency
right just as long as that server is
down it's never going to be able to
respond to any requests and then it
comes back up again and then that's of
the length of time it took to start
receiving requests again so a little bit
more about consistent hashing now
remember all of this is part of react
core I'm explaining this as a precursor
to why you want to use react pipe to do
your data processing work on top of the
dynamo system it's a hashing technique
that allows you to do minimal amount of
reshuffling of of data of information
stored on that ring state tolerant to
divergent client views and coordinates
both replicas selection and replication
so all of this what we call tunable
consistency what you know in a in a
sequel database is some kinds of Oracle
my sequel postgres all of those engines
are acid compliant they've all got query
engines that enforce acid compliancy by
introducing locking mechanisms during
that right process or that update
process to coordinate a sequence of
events before that is then flushed to
disk in some way with with a dynamo
system you can enforce certain copies of
your data certain replicas to be
required to respond to confirm that that
data was written and that gives you a
much more fine-grained control over
whether you're willing to accept
slightly higher latency for a request to
guarantee that your primary replica
nodes have actually got that data and
stored it or whether you're willing to
sacrifice and become slightly more
eventually consistent so accepting
network topology and asynchronous
transfer for Layton sees instead to get
lower latency so it's a trade-off you
get to choose when when you work with a
dynamo system so this is just a
diagrammatic representation so you we
use 160 bit into the key space we divide
that up into a fixed number of
partitions those the number of
partitions are defined at the start of
the creation of that cluster so you
start by defining hundred and twenty
eight or 64 or 256 512 however many you
need based on the the cluster topology
that you've got and normally we help
customers and open-source users
determine whether they want to be using
a smaller number of data partitions or a
larger number of data partitions so
these partitions are claimed by physical
nodes in this case these four nodes are
all working together to store a subset
of the key space and overlapping subset
of the key space to give you that fault
tolerant replicas go to the end
partitions following the key so I run my
my consistent hash over my key and it is
then located somewhere on the ring on
three distinct nodes and that's by
default your n value of three cluster
membership again is still part of we are
core already built for you everything to
do with process monitoring to do with
calculating fullback nodes and basically
administrative work for bringing in new
physical nodes that will then take
ownership of virtual nodes ring
management is a gossiping ring state so
what one node knows about another node
knows about another node in that in that
peer to peer based system that makes up
a dynamo system and finally
and off so this handoff management is a
I like to use a small analogy for this
so if I've got an N value of 3 I store
three replicas of my data ok those those
replicas will be stored on V knows that
we call primary replica managers ok so
if one of those primary replicas goes
down so a physical server goes down then
what happens is the next time that piece
of information is written three copies
are then pushed into the cluster but one
of the nodes can't accept that so it's
written to what we call a fullback node
and that fullback node is like your
neighbor who's next door picking up your
mail and it'll hold on to that mail long
enough for you to come back from holiday
and when you're back it'll hand over
that mail again and all of this is
managed for you by react or so now we've
covered react or in more detail in
basically what a dynamo system is how
we've implemented it what what kind of
level of control you have over the
system let's talk a bit more about react
pipe the key concepts are as follows
unfortunately pipe itself because it was
based around the word pipelining you've
got this concept of pipes and fittings
it's a bit of an analogy around the idea
of building a pipeline and so the
terminology is a bit bit interesting at
time so the idea is that you have a
pipeline and that pipeline contains
fittings ok these fittings are functions
their airline functions that run inside
a worker process ok each of those
fittings has to follow a particular
behavior and react pipe itself make
certain behaviors available to allow you
to do certain typical data processing
work and then the partitioner itself
decides which vinos again this is where
the Dynamo ring comes back into play
which vinodh will be handling the
processing work that that fitting is
going to be performing ok now what does
that look like
in real what in the real world I guess
from a diagrammatic perspective you've
got what core does you know you want to
access the ring you give it a key it
will hash the key it will find it on on
find a replica or depending on your end
value and your your cure and value it
will respond to you with a number of
replicas and give you your data back out
that's how cool works now how would pipe
work well this is mrs. pipes abstraction
on top of core so you might ask for a
piece of data that data get gets passed
into a fitting that fitting runs on that
vinodh performs that transformation work
you might take that output and pass it
to a new fitting to do some other
transformation work you might converge
your fittings together into one vinodh
to basically aggregate some results set
of some kind and then eventually you can
respond back out okay so it's this
approach where you can take advantage of
these v nodes to distribute workload and
processing work and and collect results
and return results in a in a highly
parallel fashion it's rings and call the
same thing
yes a court react core is the name of
the airline application we've built that
actually represents a dynamo ring yeah
okay so core is our implementation of
dynamo okay so let's take an example
right let's bring it to something a
little bit more high-level and something
we can discuss him in more detail so
some sort of pipelining processor I
might want to perform I might want to
render some HTML markup based on some
comment that I I want to fetch from my
database or wherever so I fetched that
and I get that my Jason record so this
is all this would be done in one fitting
and then once that fitting is done the
output of that fitting the job that it
performed was to give provide this
output and then that output itself might
be sent along to another pipe another
fitting process and that fitting process
will actually then convert that Jason
into HTML and then pass the output put
right back out of the pipeline or pass
it on to another stage of the pipeline
which might construct your HTML system
template so how does that actually look
in terms of a line code well it's quite
straightforward to break it down into
three pieces okay so piece number one is
this piece what I'm doing is I'm saying
I want to create two fittings okay I'm
going to tell you which module that the
function exists that I want to execute
for that fitting for that transformation
logic that I want to perform and I'm
going to name that fort for logging and
debugging purposes and if i won
paths and custom arguments over to that
fitting as well this is your boot
strapping your your setup for your
pipeline yeah I used it to data in we
are worried a lot to external controls
for data or anything else sorry can you
say that again are you restricted in
this code to operate inside of the VIP
I'm or any making play external calls no
you can you can make any airline who you
like in that function yeah yeah so you
could do any kind of data processing
work you could farm and out to another
external application and then wait for
it to respond back to you yet most
definitely then what I'm doing is I'm
sending 3 comments I'm sending three
pieces of information into the into the
pipeline okay and so those are going to
be sent down the pipe and then dealt
with during those intermediate fitting
stages and finally I collect the results
back up i get the comments that were
rendered from the pipeline and do with
them what i like basically and that's
where the processing function itself is
defined and again there are a bunch of
pre-built behaviors that allow you to
basically bootstrap and do
initialization and completion logic
around the particular fitting function
you want to perform and this is optional
you don't have to tear down the pipe
itself you could reuse the pipe many
many times just an example that you can
optionally tear down the pipe and say
send send that yo F your eye marker when
I'm coming when i'm done with that logic
so this is what an actual fitting looks
like the really only
bit here is this process function this
is my gathering of the function that I
want to execute from my module I'm
running it on my input I'm getting my
results and I'm sending my results
onward and that that really is a essence
what your functions would look like your
fittings would look like as part of your
pipelining process so you basically
you've got these building blocks where
you say I want to do ABCDE different
things there are going to be separates
fittings in my pipeline at the very
start I create a pipeline itself and I
create a number of pipes and pass in
inputs that will be transformed sent on
words or eventually returned out the at
the pipe the real key to taking
advantage of react pipe is that close
connectivity has to the Dynamo ring okay
remember because you have this
consistent hash function which
determines which virtual node that
operation that fitting is going to be
run on top of you have the power to do
things like take my output from you know
run this on every vinodh I have
available but then send all of the
outputs from that to a single vinodh
that's going to do some final work load
on that and it's that control process of
particular by constructing a hash
function that then could points to the
same vinodh will allow you to to gather
results and basically do more data
processing work than MapReduce but map
reduces essentially the reason we built
react pipe to begin with and is a
perfect example of where react pipe
takes advantage of this distributed
processing logic
yeah the hash function I've already said
this funnels inputs to a V node and
follow is a hash function that we've
already defined for you which allows you
to aggregate those results so again it's
that second fitting that you can say run
the consistent hash function could
follow which will mean that your
aggregate all results you'll send all
the inputs from the first fitting into
that second fitting on a single vinodh
so it's this it gives you the power to
do these sorts of things so you start
off very simply okay I'm basic going to
take advantage of every single vinodh
and run some function on all of the V
nodes and finally I'm going to reduce
that work down on a single vinodh very
easy very simple but now what I can do
instead is I can do something called pre
reduce which was not possible in a
previous version of reacts MapReduce
implementation which is now possible by
taking advantage of react pipe the way
it does so you can map so you can gather
information from all of the V nodes you
can then reduce all of the work on the V
notes at a data local to a physical node
reduce them down on there and then
aggregate them together oh so you're not
shuffling you're not moving as much data
across the network to do the processing
work again it's this like I might end up
having depending on my number of V nodes
and a number of worker processes and
maintain I could have millions of
messages traveling all over the network
trying to respond and run these fitting
functions / / whatever pipeline I want
to to do workload on but taking
advantage of these pre reduced stages
where you can take a hash function that
says actually send gather all of the
the information from the V nodes that
are on this physical node and just send
them all to a vinodh on that node
they're not having to do any network
transfer reduce pre reduces some kind
and then and then push it on words so
you reduce that to effectively that the
final reduce phase is no more than the
number of vinos you have available in
that cluster now all of this is based on
some research work by Eric Brewer and
and other members in the distributed
systems community and it's based on this
staged event driven design architecture
event queues are managed by the pipes
this is something i'll talk about is how
we've implemented that in react pipe in
a second but essentially queues are
attached to every vinodh which allows
you to apply some sort of pressure if
some sort of compute task is taking
longer on one particular fitting in that
in that pipeline then they then the rest
you can essentially have some sort of
control over the amount of back pressure
that's across that entire system it
helps to keep the memory footprint under
control and cues can be size cap to
limit the backlog the research papers at
this link and like i said i'll make the
slides available so please do read that
paper is very interesting paper and it
basically explains a lot of what was the
design decisions behind pipe and so in
react pie peach vinodh managers RQ so
what does that look like so now sumed in
to a particular vinodh and there's a
series of queues and the v node itself
has a worker pool and each of those
worker pools pulls from one of the
queues to perform that fitting function
that you want that transformation work
you want to perform where does this all
fit in in the real world this is really
nice it's very abstract at this stage
I've mentioned a few times that that we
use it in react so let me just go into a
little bit
about I won't go through this you've
already heard more than enough about
what makes react different to other
database engines but what how we use
react pipe at the moment and it's
something that we are trying to push
through out the entire infrastructure
that we've got in fact we're planning
even potentially to replace kV directly
with a with a react application so what
MapReduce is fundamentally is typically
before MapReduce you'd run a sequel
query you gather your results the
results will be sent into your
application layer and your application
layer would do some sort of
transformation work aggregation work and
then push that information back into the
database again now this was fine when we
had hundreds of gigabytes of data now
that we have hundreds of petabytes of
data it's not possible to move it's too
costly in both latency and bandwidth to
move all of that information into your
application layer not to not not even to
forget that it's highly likely that the
application layer itself doesn't have
capacity to take in all of that
information to do the processing work
itself there's just too much information
there so instead of pulling data out
transforming it and pushing it back the
focus of MapReduce is to push the push
the function you want to perform on to
your database engine and say you run it
locally on what information you have for
that subset of your data okay so this is
what we do we move the data processing
work to the data itself in react scales
more efficiently and takes advantage of
compute power on database servers at
scale
so for more involved queries Alex
specific implementation this stage
allows you to specify input keys you can
process data using map and reduce
functions which are fittings in react
pipe
and you can at the moment do it in both
javascript or airline but you guys are
all going to be running along sir okay
you can actually have a look at the
MapReduce implementation the pipe itself
in in react kv and again this is
something we just want to replace kb
entirely with with this implementation
rather than hooking them together but at
this stage it's still as an embedded
piece of kv so a MapReduce example what
I'm sorry I didn't write in Erlang it
just wanted to give you a really really
rough example so this is the string that
is my my reduced function ok and I've
got the map the query I want to perform
and in this case what it's doing is
sending me back the key and the number
of it counts for the word pizza that was
found in whatever the value object in
that kv Paris so this is done by passing
that string into the JavaScript engine
which does the whole lexical analysis
parsing and and building that st
representation that erlang can then read
and returning a result set of some kind
so again this is a high-level overview
of how pipe is used in react you can
farm out you can run fittings functions
over your data acted at the location of
the data itself and then aggregate
results for some sort of business
advantage or or collection process
analytics process of some kind sidestep
a little bit into react cloud storage
I'm not going to go into the s3 offering
Stuart's already covered that but i want
to show you an example of how pipe is
used there
our MapReduce implementation is used to
do some fitting work to aggregate
information so basically our cloud
storage product is a thick proxy that
sits on top of a react cluster and
breaks down those data blocks into one
Meg Pete chunks that are then stored to
the cluster
so how how the CSU is react type well
because it's a multi-tenant product we
need a way of calculating the amount of
of data that a user is storing in there
s three objects space and allows us to
build them appropriately or then when
they're running a customer's running it
locally to build their own internal
customers in some way because again this
is something that is set up on a private
cloud or ec2 if you want to run your own
CS cloud over easy to sew the map
function itself looks through the files
it belongs to a user and emits tuples
which is basically the file ID and the
size of that file and some sort of
integer representation of the size of
that fire the reduced function sums up
all the file sizes together and you end
up knowing basically how much data a
particular user is using in that cluster
and the pre reduced function allows us
to sum up so taking a party of react
pipe we can now pre reduce that
information and do it data locally so
the number of messages that are
aggregated in the final reduce phase is
basically the number of V nodes in your
in your cluster current users of react
type I'd love to say that all of you
guys have become react pipe users after
this talk at the moment batter obviously
you three add pipe some great
discussions on the mailing list some
open source uses maybe it's hard to know
because you just need to download the
code so you add
as your dependencies in rebar and grab
it and pull it in and and you're good to
go and we never know whether you're
actually making use of it or not so we
did have discussed with different
customers of us who actually wanted to
take advantage of pipe itself in their
react cluster and communicate directly
to that to do some advanced analytics
work we're not sure whether they're
actually doing that at this stage or not
it's hard to keep track of what
developers are working on because
there's no visible interface reacts
abstraction level pipe is not getting
utilized as effectively and efficiently
as it should be and it could be and this
is something that's on our future work
that we're trying to improve ultimately
current users I don't really know other
than Basha so I can tell you is
extremely stable though which is great
because we've basically battle tested it
in you know 100 node clusters and larger
so we are we know that it works at scale
now someone asked me this at lunch
actually storm is another distributed
data processing environment a platform I
know that Twitter use it and it's
gaining a lot of popularity at the
moment and again it these kind of
platforms are not designed to compete
with Hadoop they're not for your deep
analytics work they're not for your 24
hours extremely complex functions and
queries that you want to run and then
derive large reports that can be
sentenced bills to your customers or
whatever it's designed for kind of
real-time data processing work
so how what's the difference between the
two well in react pipe we describe it as
a pipe and a fitting we have a dynamo
based implementation we do something
called at most once processing so when a
fitting is used to run some sort of
computation work over a an input that
fitting isn't then used again in that
pipeline you can create another copy of
that fitting and chain them together but
the fitting itself you can't re you
can't send the output easily directly
back to the same fitting again so it's
what we call at most one processing and
there's no such thing as branching so
anyone who's more familiar with the
details of storm storm allows you to do
branching work where different pipelines
can converge and diverge and break off
into other processing work there's no
way to do that in react pipe at this
stage storm uses a terminology sprout
about some bolts it steered master water
worker nodes and relies on zookeeper so
there is a master slave architecture
there which means that you don't get the
fault tolerance that you'll get from a
dynamo based system at least once
processing so one fitting is able to run
its input work and produce an output the
convened be sent back to the same
fitting or the same bolt in in storm
terms future work well we're always
doing performance improvement work the
larger you make this these these
clusters and that the further you push
react and the kind of large-scale
installations we have at the moment you
find edge cases in any distributed
systems as erling engineers you guys
know it well when things are moved into
scale there's a compounded level of
complexity when debugging edge case
problems so we're always finding
performance improvements that we can put
into to pipe but don't worry
do that work for you because it all goes
into react anyway so any pipe users can
happily take advantage of performance
improvements as we find them and make
them are the language fittings it's
something I mentioned earlier on there's
no external interface to pipe at this
stage so you can either treat it as
something you access via MapReduce in
react or if you want to dig lower you
can pull it out and access it directly
by airline but if you want to access it
by a lure or JavaScript or Java for
doing much more comprehensive work where
you don't have to sit and dis and talk
directly to reiax MapReduce and you want
to run your own custom functions with a
much more close-knit level of control
that's we've not yet exposed the ability
for other programming languages to do
that what we expect that will eventually
happen is that the vinos themselves will
expose a you'd still at the moment we
still have to define pipelines in Erlang
but the vinos themselves would expose a
TCP port connection that would allow an
external device to connect to a pass in
fitting functions to conform to some
specification and then could get run and
run through the ER line for a long vm
that's not some of the plans that are on
the cards at the moment and again like I
said the external interface itself would
let you stop and start pipes would let
you throw inputs into pipelines and
collect outputs by almost always
expanded in standard out effectively you
could pipe stuff directly into your
react clockwork you're a rag pipe
cluster and do do that transformation
work more resources on react pipe the
github code itself is up online and open
source the readme has a lot of detailed
implementation notes see if we really
want to know exactly what is going on
here and how it connects
directly to react or you can find out a
lot more there fasho didgeridoo now I
mentioned to you by show banjo thank you
bashia banjo is the react or example
application for playing a MIDI file in a
cluster just to experiment with building
a career core based system the digitally
do is a rewrite of that that runs
through react pipe so it's still going
through call but it's going via pipe
this all this work is summarized really
nicely in a research paper written by
Brian Fink and the PDF link to it is
there and again I'll distribute these
notes and it should all be accessible
and clickable any you know as with any
open source code there's always a
mailing list at github issues and you
can of course tweet Brian and he's more
than happy to talk things stir about
react pipe I'm not sure I need to go
through this slide any further but who
we are in general we're a bunch of X
Akamai engineers founded in 2008 we
build large-scale distributed systems
react is our main product at the moment
and with that comes our bunch of Erlang
applications that stack together to
build a react cluster and we specialize
in storing critical information at scale
officers in the US Europe and Japan now
other libraries as Erlang engineers this
is just the side plug into other tools
and technologies that you might find
useful like I said before we are core
build your own dynamo systems lager and
I think I heard the sum up guys are
using lager to as their logging
framework that's got a lot of endpoints
you can basically route you're logging
information into wherever you like as
well as a lot of flexibility with story
on disk web machine is our rest-based
HTTP server and we've got many other
projects again they're all up on github
so feel free to check them out we like
we enjoy basically being some of the
larger companies that are really heavily
contributing to the airline ecosystem we
also do a lot of interesting work
surrounding the VM and tuning that and
doing performance improvements directly
to the airline beam vm now finally where
am i from I'm from basho amia this is
our London office it's quite a small
cozy office but it's got a good good
team of people so feel free to drop us
an email and we're on london time so
were our different from you guys but
much better than waiting for the u.s.
guys to wake up anyone have any
questions yeah
look you correctly you're using pipes in
town
yes also use it for events into it into
fitting access different data set for
you if you like if you write it in
Erlang inside the same viet well even
across vm implementations yes you can
but we don't expose an external
interface for other languages yet yeah
yeah yeah that basically cards actions
for all types well we yeah week well
actually okay so one of the reasons that
we are at most once is that it's
possible for a fitting to tear down the
pipe you can that fitting can forcibly
say I don't know what to do with this
output anymore I'm going to kill this
pipe which means that pipeline could die
before reaching completion this is not
something that happens in storm storm is
at at least once which means that you
will eventually reach the end of that
that spout and bolt chain but the bolts
don't necessarily mean that at the end
the output that you get is what you're
going to want so because again if you
will maybe suffered some error condition
some way through that pipe lining
process but yeah you can you can because
of the way you compose pipes fittings in
your pipe you can ensure that this will
always input will always be run first on
here before the next so you can impose a
logical ordering of events I don't know
whether you'd consider that
transactionally compliant enough you
want will be
is there any car bra not yet it's on the
cards but it's not in there yet it's a
bit hard to implement on a dynamo system
so now if anyone wants to know anything
more I think you've already seen this
link from Stewart but please feel free
to drop us a note get in touch with us
and we're happy to talk more about react
yeah these perspectives yes the concept
of right ones through many because from
to presentation I don't see any atomic
operations on the data yeah we don't we
yeah we know we don't we don't have any
transaction system inside react that's
by design because the implementation of
a transaction system introduces some
sort of right coordination and that
right coordination either introduces
extreme Layton sees that don't fit the
use case that a dynamo system fits or
introduces a single point of failure of
some kind and that's again something
that we don't want to accept in react
yeah is it possible to to the
concurrency because for instance and
system like form one of the major
selling points is actually that you
meaning that is there is some type of a
worker that is taking longer than the
amount once you can just all move these
work types is there something
automatical in hives or healthy nothing
that's exposed yet and even in Erlang
terms you'd have to dig through the guts
of the system to be able to do that and
derive information about which fittings
are taking too long to perform their
operations yeah sorry yeah I want to go
to trial the first question a little bit
and you use this framework to do some
kind of streaming trips yeah that would
mean like we needed something in your
database updates it automatically kicks
off processing again yes yes
yeah home a career carlos you mean
running pipes is on a separate or like
the end notice the same vm just not
messing up each other but what do you
mean by messing up each other some
research for example was not possible to
rings loudly I need this current here
yeah yeah but I see you means in said
you I think you we talked yesterday so
you have a react or application already
so what you're thinking of doing is
running on the same vm a separate type
of application that has its own dynamo
ring not for everything I do so I
wouldn't want to run everything on time
yeah thumbs up on my own code of laws
yeah that that's gonna be tricky it's
definitely possible but we definitely to
talk about it place if you're offline it
from here yeah ok i think that's that's
everything thank you everyone
I</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>