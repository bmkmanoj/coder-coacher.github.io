<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Malcolm Matalka - NoDB: A Database For Lightspeed | Coder Coacher - Coaching Coders</title><meta content="Malcolm Matalka - NoDB: A Database For Lightspeed - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Malcolm Matalka - NoDB: A Database For Lightspeed</b></h2><h5 class="post__date">2014-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8g66NOs2HwQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you very much my name is Malcolm
if the more observant will notice I'm
from karna I'm very uh somewhat popular
Swedish company so this talk is about no
DB karna has not gotten to the space
industry this title actually comes from
a conversation some friends had where
the semantics of no DB would actually
make a lot of sense if you were
travelling in spaceships going at light
speed and had a lot of latency between
them and i did a cardinal sin and change
much my slides this morning so sorry and
it's after lunch so i get kind of sleepy
after i eat so i might end early just so
you know so I work for karna where a
Swedish company we handle ecommerce
payments we're through invoice um we are
heavy users of Erlang and my role mostly
was on the second generation purchase
taking system which is meant to be
highly available multiple data centers
all that sort of stuff they're already
working on the third generation so what
I'm saying now is already obsolete but
the last two years been pretty good
experience learned a lot and this new
system is going from a monolith to more
of a broken up a distributed set up with
lots of nodes that can be killed off
very easily and come back without
disrupting service for anyone and one of
the components that we built for that is
called no DB which used to synchronize
data between the multiple systems so
it's a database in a very loose sense of
the word where it's actually an overlay
for multiple systems all storing their
own data and its key value based and it
replicates data between those systems
making sure that each portion has the
latest
a version of the data there it favors
right availability sends we're across
multiple systems with different s la's
if one system is down you want make sure
you can always write to the other system
and that means you don't want to be
synchronizing with them on the database
transfers or anything like that so you
prefer writing over consistency it
allows for large Layton sees between a
right happening and it making together
system in practice it's only you know in
the orders of hundreds of milliseconds
for database to get to get around but in
theory you could have you know days or
weeks or in theory infinite Layton sees
so what is ventral consistency it's very
popular there's tons of talks about it
if you have a lot of questions Chris
John Michael over here has done multiple
talks a bad venture consistency this is
the definition I pulled for some ACM
article where guarantees have no new
updates are made to the object
eventually all accesses will return the
last updated value so it's kind of a
difficult concept at first where if you
do a put on one part of your system and
then do a get you might not read what
you just put you can even travel
backwards in time where you read a value
that was there before you did the put
it's confusing a lot i would say at
first and it means you have to write
your code in a very robust way where you
don't actually care about what time is
for your system to very popular ones are
reacting Cassandra react is what we use
and what I talk about will all be built
on top of react cassandra has some nice
features but it didn't really suit our
use case here and it's a it has a
different look on what eventual
consistency is and the properties for it
and so like i said you don't have a lot
of control over what you will read
verses a right so why would anyone want
this sort of thing
if you read the hypertext blog basically
every third blog post is how we're a
bunch of idiots for using metric
consistency but it actually has some
really nice features that are very
simple to wrap your head around might
may not be simple to write code for it
but at least it's a it's very easy to
reason about one thing is scalability so
for react if you are running out of CPU
RAM or disk space you add more nodes and
all of a sudden you have more of this
across all of your machines so you can
scale out in that way and it roughly
comes out to be about linear
availability if so for react and the
know to be model favors right
availability so if some portion of your
network is down or some machines are
down or anything like that as long as
you can talk to some group of the
machines you can always right you can
control that so even if you talk to only
one node and your whole cluster of nine
or whatever is down you can still write
that means you if always being able to
put data into database is important you
can set it up so that always happens geo
locality if you are a company that is
across continents you often don't want
data to be owned by someone in by a data
center in Asia when your clients are in
the US or if they're moving around a lot
so with a masterless setup where nobody
owns the data and it is across all these
setups you can always write to the data
center closest to you and your it will
be accepted so you get good latency in
that case relative to possibly locking
across the continent or locking a row
something like that and finally this is
sort of a new thing that we have coming
out in our system but it turns out much
consistency is really nice if you want
to cache database reads you already have
written code that depend that doesn't
require the latest bit of data to be
there so when you cash
you don't have to do anything but we
fancy about it you just expire it over
sometime and then you read the next one
from the database when you're ready but
a put you already don't expect it to be
able to read what you put back so it
simplifies that whole layer it makes a
lot of cash bugs if you'll run into go
away and some examples eventually
consistent systems a DNS is probably the
example of the oldest in global event
existing system when you update an IP
for DNS record it takes time to
propagate around that late and see is
unknown you can take from days two
minutes amazon of course i don't know
what their current setup looks like but
they are famous for writing the dynamo
paper which is what react and Cassandra
are based off of and really sort of
creating the eventually consistent
public movement i would say in a lot of
ways their goal there was to be always
on so when they're not working they're
losing money that's very similar
problems what karna has where if we are
down we lose money and amazon solve this
by making a masterless setup where
doesn't matter who you're talking to
whether they can talk to the rest of the
network you can always put values there
and then they push it back to the
application layer to resolve any
mismatches than those values so if you
add an element twice you can resolve
that well if you add on once and it
shows up as twice because you guys were
partition there you can resolve that on
the application code and financial
tractions transactions are oftentimes my
god will think that there's some sort of
acid transaction but i guess don't
really have in sweden if you ever
written a check in the US you right how
much money you want to give to someone
on a piece of paper and you hand it to
someone and then you walk away and then
like three weeks later your money
disappears so that's not a lock that's
being held for three weeks an example of
where i'm actually consistency i think
Willie's play a big part or shine is the
interplanetary internet which is
actually
sounds quite futuristic but has been
used already the basic premise here is
you have a network where there are
massive latencies for any operation
going across large distance when I
looked it up the latency for the one-way
trip time for a light between us and
Mars it was about three minutes so huge
Layton sees you can't do an ask the
transaction when it takes you know six
minutes around time just to know if
you've actually successfully taken a
walk out you also have a set up we're
going to lose a lot of packets the
ordering is not going to be controlled
very well and your error rate is gonna
be very high luckily the error rate is a
big problem for our networks but the
idea of having to hold on to data and
then the ordering not getting to the
other system in the order that you wrote
it is a issue that we have and the
solution for the interplanetary Internet
is AG this setup called delay tolerant
networks where you have many components
that all store their data locally until
they know they can hit a system because
they expect most systems not be
available all the time and once the
system comes back online they push it
out again and the whole and at the
application layer you have to be aware
of the fact that you're going to get
possibly the same data multiple times
and not in the order you expect so it
looks like eventually consistent system
and the data that you send through a
system like that also has to be workable
what's as well another example very
consistency for those who don't know
this is the Borg from Star Trek which
are a cyborg life-form that assimilates
other groups and all around takes over
worlds and destroys them they are a hive
mentality where they can communicate
with each other quickly instantaneously
they also assimilate the knowledge of
other civilizations pull it into their
collective consciousness and a model
by what they know based on that the
movie first contact is a time travel
movie where they go from the future into
the past and during this movie the
knowledge that some of the bore I've
come back and makes it to the is
transmitted out to the rest of the
collective and it's incorporated into
the collective just fine so the board
can clearly handle out of order data
trivially time is not a big component
and how their database system works they
internally a cube could be considered
transactional where each individual
component inside of it can happen
instantaneously and with a consensus but
across their spaceships they have you
can have blackouts in time and Ricci Qin
Singh reordering this is hieu the first
Borg to get an individual identity of
his own which ended up destroying his
whole collective but that's different
story the interesting about him is he
was cut off from the rest of the
collective for some period of time and
he learned new things while he was cut
off once he was a bra put back into the
wild all of the knowledge he accumulated
was merged back into the rest of the
system so this is just like a network
partition happening where you have this
one lone guy cut off and when he comes
back all the knowledge he has is it
reincorporated into the system alright
so with all that what is this really and
notably is more of a model than anything
else the two open we have two patients
of it right now 14 different types of
system that we work on so it's not like
one unique piece of code and it's not
one product you can really put into a
code base although we're working on
bringing it together and the like I said
the main goal is to take separate
components of a system and allow them to
replicate the data between them so they
each have
same database and look the same it works
across our transactional store and or
scroll saw are eventually consistent
store of course the fine print here is
that you are eventually consistent
between those stores so even though you
have a transaction inside transactional
store the actual movement of data will
not show up until it makes it across
replication so you can't make any
assumptions about that and no to be
itself doesn't actually store any data
it is just a way to look at your system
and to replicate data around on one
system it doesn't even have an API of
its own you use the transactional API
and on the other system it does have an
API looks like a key value store it
replicates reacts API quite a bit so
it's not even a consistent API for you
to think about and the main properties
is allows a door replica out of order
replication of data doesn't consider
time or least wall clock time in a
meaningful way and it is very nice for
the use case that we have right now
which is going from a monolith to more
of an SOA architecture so in general I
would say components should own all
their data but when you're breaking up
this big piece into a bunch of smaller
pieces you often need this is a
transitional step where everyone can see
all the same data and modify it and it
goes around to all the other systems and
then once you've made it to the point
where you actually pulled a lot of the
code out you can start making that data
actually owned by the other components
and at which point you can start turning
replication off and they can have their
own data model their own whatever but as
an in between steps has been very handy
and a very useful component in making
the break-up go much smoother i would
say because of the replication of data
you can have the same component running
on both systems for a while until you
trust
on a new system and they will both sort
of function mostly just fine as long as
they understand the semantics of their
replication and then once you know this
new system is good you can just flip off
the component on the other one and
you're ready to go the main component of
no DB is a vector clock this is a pretty
obvious diagram as long as I don't
explain it to of vector clocks in action
the basic premise here is just that when
somebody in distributed system updates
the value that actor puts himself into
the vector clock and increments some
value and then once it makes it around
the system everyone can say hey is this
new value in line with what I know
already or did somebody change at the
same time that I changed it and in the
first case it's fine you just take that
new value because you know it's the
successor to your value or you might
actually have a value in the future as
well so you know you can throw that
value out because any value that you
have already has that value incorporated
into it in the second case where two
people have modified the piece of day at
the same time you know you have to do
some resolution on it so and you will
get two copies of that data and you do
something to it to make it one version
again this is pretty standard for using
something like react they're building
the ability to handle siblings into it
directly if you model your data certain
way that's before we started working on
no DB so nodb every piece of data you
replicate has to have a function that
takes two values and gives you one or
completely fails and that's very
important because since to add since two
components can update the same value
twice you never want to lose rights so
for the case what you guys have updated
you always want the ability to compose
these I together so since both our sides
are in Erlang this is very easy for us
because you just implement a resolver
library and you put
both sides for a case where you have
something like no to be but different
languages on the other side you have to
make sure you implement these resolvers
the same way and no DB is kind of the
obvious first implement or a use case
when you learn about vector clocks you
have this thing and that tells you the
the rough partial ordering of events and
use add a piece of data to it because we
are you going to do so the
implementation would it be like say it's
an erlang we have two systems one is the
one that I mostly work on which uses
react as its data store this is a
distributed purchase taking system where
we have a whole bunch of nodes currently
we have to react clusters one in in two
databases one in each our data centers
one in each data center and with no one
to be we can replicate across data
centers and even the other components
doesn't matter where they are either
since it looks all the same manisha is
the system of record so anytime you
populate one of the other systems the
Manisha system which is the first the
legacy one is where you pull that data
from we use RabbitMQ as the transport so
all we have a it's a pretty simple q set
up where we just replicate in sort of a
connected graph since we don't have a
lot of systems at the moment in queues
and we want it to be very simple very
understandable so everyone sort of
listens on the same group of RabbitMQ
brokers the same cluster and pump data
into it everyone sucks it out and pulls
into their local systems we support
real-time replication which is the way
it functions most of the time or you're
just doing operations and as they happen
they get sent out to everyone else
there's no sort of clever the scale
we're going at it has no sort of clever
buffering or anything like that you can
do a chop operation as it comes we also
support manual exports
that is always from the Manisha system
of record where you say you know I've
had some downtime for every reason or I
know something happened a bug happened I
need to replicate the last days worth of
operations out so the main user side we
have a layer on top of Manisha that has
a trans lat transaction log on it and a
code can listen in real time to the
transactions and they can also walk over
the history of the transactions up to
like seven days in the past something
like that so the real-time replication
works by listening in the transaction
log and then takes the key value
operations pulls out which ones that
cares about packs them up into a no to
be operation message and then sends that
through rabbit we the reason Manisha is
the system of record for this is because
we can iterate tables and all the keys
in a with a very easily in it you just
do a dirty operation where you go
through and get the next value and
continue and push it out that's much
harder in react at the moment it's
possible but it is not a use case that
is meant to support and the ability to
replay the transaction log is come in
extremely handy several times we've had
a bug or two in our integration with
RabbitMQ on our side where we lost some
messages due to a network partition and
while we had to fix that bug of course
we also had to make sure that any data
we lost actually made it there so you
just say let's rerun the last whenever
this event happened the vector clocks so
this is an old boniva table that we have
so it's a lot of legacy there about 10
years or so and you can't just take
every single piece of data you want
replicate and then add modify the value
in Manisha and add a vector clock to it
so we use a
shadow table wherefore we have a look up
for how each table maps to a vector
clock table and then any time you do an
operation you go and you pull out and
you do a read on both of these it turns
out to not be that expensive for us but
it would be nice if we could have them
all together but too much effort there
the react side so the react side
actually a person doesn't or a lover
doesn't realize they're actually
interacting with react they know there's
semantics match react but they don't
actually know what's behind that they
use a direct no DB API and it basically
just has your put get delete options
when you do any of these options the API
makes the operation on react happen and
at the same time pushes it for
replication any importers are just
listening it on rabbit and then pushing
in to react and they have a flag that
says don't replicate this we have to
handle the case where rabbit is down for
whatever reason either Network partition
or maintenance on it so we locally q
things because we can't do a manual
export and we don't have a transaction
log on these machines we just build up a
little local q on with a disk log and
then once those come back shoot it off
again and in this case we had more
control of the data model since it's all
brand-new and the vector clock is stored
with the data rather than as a separate
which is if you think about it it's the
only way you really can't do this
because if you stored it as separate
keys you could not be confident that
when you read the value and you read the
key you're actually getting values that
were consistent with each other given
the remodel react so what happens when
you update some data first you have to
get your previous data this includes the
vector clock if you want to know what
you're modifying and you have to make
sure that the mark a shin you're going
you're adding to it works for the most
part data in no DB has to look like a
lattice where you only go one direction
on it you can never modify a value going
the opposite direction because
have no way of knowing if resolving that
in the future so after you get your data
you bump the vector clock so you say
this is who I am I'm modifying this
value let me increment my entry in the
vet o'clock by one then you write both
of those back and you replicate it and
for the Manisha system we actually have
this nasty little piece of code where
anytime you do a transaction or a write
operation it behind your back looks at
that and sees if this is something that
it's going to replicate and it sneakily
pulls the vector clock out on its own
and then modifies and pushed it back we
actually had an interesting bug one time
where somebody realized that this was
slower than they expected so they change
the read of the vector clock to a dirty
read amnesia and all of a sudden you're
getting inconsistent values because your
dirty reading a value that somebody else
has either not updated or the middle of
updating or they don't know that the
difference actually doesn't no need to
serialize on this so that's fun and on
manisha since we have transactions we
can do any resolution of siblings on
right time so you go and you get your
data and you need you're going to put in
you make sure that their makes
sense and resolve it if they don't and
then you write whereas on the react side
we do it in the opposite way where we
always just push data into react doing a
right and then on ready to resolve any
siblings and we will fix those siblings
on an update otherwise you'll just keep
on resolving it as you do reads so the
retrospective on this is everybody hates
using this actually it's hard it tells
you when you're wrong and people don't
like to be wrong and that's why they're
wrong when they hate it because people
oftentimes want to do things root system
that they shouldn't do and they don't
realize how wrong they are because
there's nobody tell them that they are
and luckily the while it feels like a
prison sometimes the semantics we have
actually inform you when this is going
on so it complies
mains and then people have to come to us
and wonder why we're such jerks making
them use it out of events are
ridiculously hard to do especially if
you're used to transactional store of it
even if you think you have it down just
like some weird edge case comes up we're
like you just thought something would be
there you know you shouldn't have but
your code expected it or your multiple
layers of distraction away from two
things and you didn't realize something
else depended on certain things being a
certain way so we we hit some bugs with
that every now and then but it's not so
bad they're pretty easy to solve usually
you just so the semantics of know to be
are so simple they're hard to work with
but this they're simple once you there's
simple in general so once you know
what's wrong you can fix it and being
able to replay rights is great we have
several times saved ourselves from
losing a bunch of money because of this
even on the react side where we can't
iterate tables when our transaction log
we we log a lot of keys out to that to
the disk clog SS log to the actual just
logger log and we've had multiple times
where we realized that we've lost
something and you can we scrape the logs
and then just rewrite that little piece
of data out which turns out to be really
helpful and this is actually the the
nice properties of it being databases
rather than message queues because the
data is always there so if you lose
something or somebody process piece of
data wrong or anything like that your
data is still on the other side and you
can just retransmitted out rather than
having to recompute it or something like
that if you can even figure out how to
do that so all of our data is still in
the databases all the time and that
makes it really convenient when you've
messed up for future work we really want
to unify the code bases they were
written in haste by one person on each
side neither of them was me and
there's a lot of things where it's
mostly doing the same thing but not
quite and we also didn't like really
understand what things should have
looked like at the time we just needed
some replication working because it's
how we could get the system going so
there's some technical debt there that
we are working on cleaning up for our
case rabbitmq does actually add much
because we really need guarantee
delivery and we know it's a certain set
of people that we're going to us pass
data between and we so if something goes
wrong like your rabid em queues
partition or anything like that or some
way in which you lose data for our use
case we always want to get the
acknowledgement back that the data has
been pushed into the database on the
other side so for us more of a direct
communication is what we're after and
we're moving slowly into that area where
we have some sort of service discovery
system and then those you just say I
want to replicate this type of data tell
me who to talk to and there you go the
other thing we want to do is make this
easier for non erling systems we have
one other component that saving two
major components and then another
component that's pulling this data off
and we're getting a possibly fourth
consumer in the near future and right
now our data is actually just a term to
binary and they want to write their
component in Ruby so there's a little a
little problem there we're probably
going to go which is something like
Jason or something like that for the
actual wire format and one idea we have
is to have postgres support where we
have other components coming up that are
going to be in a real relationship ace
and it's unclear how to actually fit
this into the model but it'd be really
nice to give them the chance to put
their data into database they like and
then slowly and then pull that data out
of the legacy system as needed but again
that has a lot of uh I just don't know
enough about postgres and we don't know
if I progressed through that yet
so any questions Thank You Malcolm
unless he promised he finished early so
we have plenty of room for questions so
I didn't can't quite get how you use
RabbitMQ as I understand no TVs on the
top 30 keys on the bottom varies rabbit
so there's so you have no DB is on the
very top and then you have like react
and manisha and the rabid em in nobody
is using random q to push data between
those two components so anyone that
wants to listen on data changes or
pushing data changes in will write to
rabbitmq right and my other question is
how many real glass tears do you use
right now we have two clusters each with
nine machines in them so far this model
is scaled pretty well for us we have one
cluster in each data center and because
latency is an issue for this we should
be able to add you know like one in the
States or UK or something like that if
we ever got the opportunity right and
then the piece of data comes into no DB
and you have to write it how do you
decide which area class there to put it
into since we're using better clocks and
you can tell when you have piece of data
whether this is new for you or not we
actually just write it to everyone so
right now it's inefficient a sense that
it's a fully connected graph but it's
not a problem in practice because
everyone knows can tell whether it's
worth writing them or writing for them
or not Andy moderate the number of
database operations for US isn't so high
that that's a performance problem right
now right but I mean in the beginning
you said that one of the motivations was
that if you have data centers in
different continents then you don't have
to send your data also the I'm sorry i
phrase that provide Andy the nice thing
is that you don't have to worry about
the latency
for data getting there but you can send
all your data everywhere so eventually
consistency in general is nice in the
sense that data can travel all around
the world and it doesn't necessarily
matter how long it takes to get there
but you know that there's no necessary
owner of it where you have to consult
with that person with that server
whether it's okay for you to write that
value or not right and do you have multi
data center replication between the rock
nodes nowheres so we're just using no DB
for that right yes you're right yes
we've already had it there we know how
it works so we figured it wasn't worth
the extra duplication in functionality
you said you don't delete any data how
long do you expect to keep up with new
data um so our data is usually pretty
small in of itself we have compression
compression turned on for everything
going into the database so the growth of
the data has been too big an issue for
us on top of that because at least on
the react side we can just add more
nodes to get more disk space not too
concerned about it in that regard and we
are also only replicating a subset of
all the data on these two systems so the
ones that they really need to operate
both sides so the amount of data going
between the systems is actually not so
huge okay thanks hi I'm could you tell
us a little bit more about the conflict
resolution siblings thing and I think
you said that there could be a failure
scenario what would that be and is it
possible or when is it not possible to
do that kind of automatically so the
failure scenario and all these is
basically call whoever's on call and
have them manually fix it we haven't
actually run into that problem yet but
in the end with anything like this it's
always going to come down to a person
for us we strive to make most for data
immutable which means even though you
should have
a resolution for it if it is immutable
then you know you never have to resolve
any conflicts Pat Helland has a pretty
good talk about this or paper forget
which where he just tries to point out
various ways that you can make data
immutable just to make your life simpler
and instrument system we have for the
things that we do resolve usually you
try to make look like a set or we have
some counters that we use where it's
just you always take the highest value
so it really depends on the specific
piece of data if you read the literature
on something like crd tease if you can
fit your data into that which you should
be able to do for most things then
there's generic algorithms in there we
aren't mature enough I would say to in a
system implementation to have something
CR DTS used but it's a it's a definite
direction we'd like to go in the long
run you said that you first of all let
us come in transaction do you have or or
what kind of data volumes you pass for
you systems your website is something
like 70 million transactions per year
and my second question is how do you
deal with data which are you need to
have relationships between data from
what I understood you have a key value
type of setup in your refactoring or but
how do you deal with with the
relationship of data so the second one
for relational data we don't really have
much of any we might have things that
are pointers to other things so some
kind of relation but for most of the
data that we are replicating between
them we basically denormalize it and
pack all the data needed um inside a
particular object and then other side's
might decompress it out to multiple
tables but still keep the data in lined
in that way and the actual transaction
volume is not super high I think we're
pushing a error on real time
um several like 100 per 100,000 per 10
seconds all right several hundred for 10
seconds something like that so not a
incredibly large number for one-off
exports weekend depending on the type of
data we can push something like so like
20 30 thousand updates per second just
similar to the earlier question about
conflict resolution at the application
layer does that mean well you mentioned
you had both systems for written in
Erlang so that keeps them consistent but
if you did have that would constitute a
bar go take it if two different pieces
of at the application layer were
resolving slightly differently that
would mean whether the data the eventual
consistency model becomes on stable and
data would bounce around yeah definitely
that'd be a pretty serious bug for
something like that I would try to do
like some sort of property based testing
so you just define your resolvers and
the properties around them and then use
something like quick check if it exists
in all the languages one would use would
that affect performance um I don't think
so the so for depending on what kind of
your underlying databases whether you're
resolving on reads or writes for the
first of all for the most part we don't
hit siblings so we're not resolving
particularly often it's a pretty rare
occurrence I think in the dynamo paper
they hit siblings like point zero
something something percent of the time
so in that sense performance wouldn't be
an issue and foremost pieces of data the
actual resolution functions are pretty
trivial like you might do a set Union
that's most languages that's a pretty
fast operation assuming the and the sets
are usually pretty small as well unless
you're really pounding a piece of data
yeah hey thanks well this thanks Michael
thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>