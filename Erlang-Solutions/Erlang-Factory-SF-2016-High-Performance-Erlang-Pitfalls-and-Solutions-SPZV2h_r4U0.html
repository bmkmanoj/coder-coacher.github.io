<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Erlang Factory SF 2016 - High Performance Erlang  Pitfalls and Solutions | Coder Coacher - Coaching Coders</title><meta content="Erlang Factory SF 2016 - High Performance Erlang  Pitfalls and Solutions - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Erlang Factory SF 2016 - High Performance Erlang  Pitfalls and Solutions</b></h2><h5 class="post__date">2016-03-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/SPZV2h_r4U0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello guys my name is singing hi Lou and
we are from m speed him from machines
own and our team includes Shiva Igor be
buff to Jessa and me and today we're
going to talk about a little bit about
hyper phones Earl and hippos and
solutions basic agenda is like the
following okay first I really talk about
what we do with airline a machine song
and we will extend to talk a little
about what are the issues and roadblocks
we met during our project and some of
the genetical lessons we have learned in
terms of pushing the performance with
Ireland and wish here and some of the
hour profiling techniques and again we
will focus on a specific case study with
high polymers counter which will be
given by shiva at the optimism we use
around two currently we are viewing a
high performance data processing system
and when we start this project
performance is one of the first order
like powering our you know objective so
we are thinking about how to make it
faster one of the typical design that we
start is a pipeline tasks for example we
have requests coming in from one node
and we have a receiver clicks auto
requests and we push it to another
processor which does some logic
processing and where we get the result
will put it to the sender which you
which dispatch all the result to the
different nodes and it's it's kind of
natural to map all the these three
components on the different process in
Ireland
yeah it works well for for a while and
next day then where we push to the
performance or we increase the scale and
we were then we met with some problems
and we are thinking about how we can
exchange data faster between processes
one of the solution is the Earl and term
storage which is kind of like a shared
data structure used to for communication
between processes one processes where
one process will get get a key and put a
value which is key and another process
will use the same key and get value from
it so we we were interesting we're
interested in what the performance is
with this ets table and for example we
want to set up a experiment here the
experiment is like this we we want to
keep count keep me updated cut off
certain keys and we increase the number
of processes from 1 to 16 and note that
we dare to like the critical lines here
and this line is like the number of
physical course in our expert in our
machines that are being experiment /
minda down and this is a full core
machine and this is the other line is a
number of logic course and we are kind
of expecting the performance to go like
this because we are approaching the wave
approaching the number of logic course
we're expecting the performance can
saturate and when we approach the number
of critical number bloody cross I think
the problem is what happene saturate so
this these are the kind of the update
operation performance we are expecting
and when we really run the result run a
test or we got a result like this so as
the number of process
increases when we kind of go steady up
up up up then instead of saturate and
keep a high level like this the
performance has a sudden dip and we are
with increased number of processes we
just have a worse performance so this is
the experiment on ten keys and so we're
can not convinced and we tried a
different set up set up so we do a one
key tankies up to a southern keys as you
can see one kisses but for one key to
performance is really bad because
everybody is trying trying to contend
for this one lot for this key and as the
number of keys growth we are getting
better but we still have this kind of
dip after we use up all of other
coursing in our machine the reason for
that is that the ETS cable has locks so
although it's fine grain locks it's kind
of what it does is kind of partition all
this storage space on two different
tables by be4 i think it's 64 since 1 16
b and however we're still when we are
trying to learn a lot of process trying
to acquire access access a lot of keys
there's still lots going on and this can
be traced from with like a GDB tracer
which we will discuss a little bit later
so next is message passing this is like
most like natural way of program program
learning early right so we passed one
message to another process and that we
are also we're also interested in how
efficient how much messages we can push
from one process to another process or
from em process to another end process
here we have an expert
meant we have 10 standards and 100
receivers and we are trying to figure
out how much message we can send a
receive among a period of time within
one second and we run it for a while and
as we can see this the number of kind of
like Wiggles around three million per
second next we we try to be more generic
kind we increase the number of centers
from 1 to 100 and we increase the number
of receivers from 1 to 1,000 and we
observe this observed number of message
with process as we can see em yeah
depending on different scenario it's
kind of different but our experiment we
use a micro pro where it's kind of the
best we can do is below and 10 million
per second so don't get me wrong this is
actually a really good we tried a lot of
other which either a lot of other method
passing frameworks and its really really
really difficult to beat Ireland in a
generic setup and for our line itself I
think why you know we have this
limitation is that when we are adding
messages from one process to an edible
process it requires to log on the
message queue and furthermore when we
when the when the process is run off
message and went is activated by an
income message the scheduler has to has
to do a lot of new scheduling for
example you have to add it to the run
queue and this reversal involves a lock
and if the if the process is run running
out of messages to go to sleep and the
schedule will try to steal from other
scheduler and which also involves locks
so all this lost 600 give us an overhead
all you know what we observe is that
pre very modern micropro we we can have
this limitation about 10 millions
message per second and we did it on a
pre power for 656 core server the
limitation is a little higher but it's
around 22 million so what we are
thinking about is can we achieve more
can we break like 22 million on my
server or 10 million on a macbook before
that I think the optimization thinking
is that are we solving a much more
difficult problem so do we really need
to pass so much so many messages between
the processes so the first step in
optimizing our organization is to
minimize the message change so we do out
in ourselves that we know that maybe
there are some slack you can borrow for
example at first we introduced this
three-stage pipeline design what if we
merge the receiver and sender into the
processor and we we put everything on
one process such that we don't need to
pass the message from the receiver to
processor and from the processor to the
sender and and they in turn and the in
addition we can petition our request and
to do the sharding and we can avoid the
communication between all these charts
so by by limiting the number of a
message exchange we really can help us a
little bit by solving a easier problem
so here is a like a toy example so we
are so now we are a militant ating all
the communications we are purely doing
other suppose we're doing curate
computations and how fast it can be
without command that the experiments
here is that we keep
doing the small computation with us and
without and when we finish without
passing or receiving and a message we go
to the next loop and a week and we are
interested in how many loops it again
finish within a one second and
experiment experiment setup is kind of
same we start with one process on the
node and we gradually increase the
number process from 1 to 100 and as you
can see the as we as a number of process
increases we kinda like the Wiggles
around what kind of like Wiggles around
80 million instead of like I have a
sudden dip and note the absolute value
here it's like 80 million it's like 100x
the IE of the ETS experiment we did and
it's a good 20 x 2 33 x the max exchange
and with like the first design we have
x3 a stage pipeline this is the first
one is like a before we solve it try to
reduce the problem like from application
level and then the second one is a
message patching so suppose we are
trying to send a 1000 message from
process a to process p so instead of a
invoke standard received 1000 times each
with one message can we do like like
invoked send every sip only 10 times and
each time we send a bachelor / 100
message so we repeat the experiment that
we did on with 10 sender and the 11
hundred receivers and this is the and we
also put in the unpatched solution as
for comparison and as a show show
the dotted lines below and this and as
we can observe further was there a
clearly a huge gap between the vaginal
mesh solution and it to be more generic
we also a repeater repeater experiment
prom with standards from 1 to 100 and
with receivers from 1 to 1000 as we can
see all there are clearly gaps between
dotted lines and the sidelines meaning
that and overall we can observe a 5 X 2
30 X like more message passing rate per
second depending on the different
configurations and the we so we talked
about how to reduce them and talk about
how to reduce the problem how to do the
message passing and then the suit the
suit solution is domain specific
operations so what I mean is that the
engineer and we know the specification
of our product and we know what the
communication pattern is going to be and
so instead of using all those generic
locks and stuff if we understand the
problem we're going to solve we can try
to devise some lottery structure which
is faster so for example in our project
we have this requirement that have these
applications that we have one processor
who is delivering a lot of message to
multiple recipient so in this can set up
we can actually we have a situation for
one writer and multiple reader and kind
of situation and we can actually explore
some more specific more efficient low
freq data structure so photos for the
standard side we we can help have a
process received a receive order request
we do some processing and after we after
we get the result of processing
we can put it into a sham remember
shared memory data structure which is
lot free and from the receiver
perspective we only need to devise a
similar way to read from this lock
retailer structure and actually actively
pulling it so Merlin doesn't provide a
generic way of solving this so we did
this by implementing our own if
functions and then note that we don't
actually don't want to like have other
processes receivers like busy spinning
and they're waiting for the waiting for
the message to come and like waking up
and sleep so we also design our own
notification and technique intend to
reduce all this waiting time in this
setup we actually achieved through pod
out through so put out 600 mega a
million message to second on a 12 minute
and set up so next I'll talk a little
bit about some of the generic lessons we
learned our during our process of
optimizing a lamp application first up
and try to use Charlene and one of the
first problem with sawing our system is
that the way we were just picking our
optimization with fans act like when we
push low increase the load and we will
have some note that has increased
latency and the increased memory and we
figure out that there's all the request
is coming into this one process and it's
just not fast enough and handling all
the porn stuff and it's get it's getting
laggy and and after a while it get get
to switch out by the schedule and all
the message queue cube-like backed up
with increased memory so so the solution
is for that is avoiding having a single
process handling every
instead instead we use the pool of non
communicating charts and that we have
other requests with a kind of key and we
do around ph 2 on the keys and it's
distributed distribute them try to
distribute them evenly on two different
charts and after that we the bottleneck
is gone another does the second lesson
we learn the exact tree at about data
structure as we know a hash matter is
kinda like is very generic kind of a
data structure in our software new uri
and the Erlin itself provides us
provides us with a quite well spectrum
of hash map map kind of like and
containers among which we have ETS set
we have the new orlean maps and we have
the process dictionary so we actually
curious about the performance of all
these containers so we did some
experiment this extra ms about how fast
we can feel up by a container so
experiment is like this we have ETS set
we have maps and we have processed
dictionary and we try to insert 500
million element in into the containers
respectively and we try to measure the
time it takes obviously the last time it
takes the better it is so we observe
that in this experiment a process
dictionary is a little bit better but
the advantage is not very obvious
however we did our next experiment so
what what do this experian is about is
that we have we have already a field and
container so we pick 18
90 and process to generate for
comparison and we have we increased the
number of already allocated keys from 1
to 100 k and what we are interested in
is we keep keeping picking one random
key inside inside and updating it and we
are interesting the number it takes for
100k updates as we can see here at first
they grow like a kind like similar and
even even the maps is better and after I
think 33 is really too i think the
earlier maps changed its data structure
and and then afterwards the gap begins
to show and however the in the meantime
and the performance of earl and process
dictionary is pretty stable so I know
that it's kind of more natural thing to
put your put a map into a state of your
engine server but for like performance
critical applications we tend to use
precipitation airy and the third lesson
we have learned is about it's about the
limitation of our number of processes so
at at the lesson what we said that we
don't want to rely on one process
however we also doesn't want to have
like a limited number of arbitrary
number of processes and the reason for
that is that what we create process
there is some kind of like cost with it
although although we know that learning
process is like lighter weight than the
Linux process of Linux turns red there's
still some costs to associate with that
especially when we create a process and
when we do the process resolution we
have this global to approach lock and
and and we all we did this experiment
that will launch like
100k process at the first then you
observe a freezing a period time in iran
vm that nothing where nothing moves
afterwards the scenes beginning to move
then at the end of experiment with
destroy or the process and the you
observe a similar kind of thing like to
freeze in vm and this will result like a
spike in latency and what we did is we
try to have fixed number of processes
and where we try to distribute all work
among all those fixed number of
processes the sorcerer topic about here
is about how to do the profiling as we
see where performance guy and one of the
like key thing about performance is
murder mega mega and however what we
need to pick our profiling to carefully
the one of the criteria is that we want
the marriage to to be a low overhead
because if it is more because a lot of
our applications are timing sensitive if
we have a high over heading measuring it
kind of shifted the landscape of how the
how the system works and sometimes you
get a different picture so yes that the
criteria is low overhead and the first
like kind of very handy to we use this
piece tak and what it does is where you
type we stack with the process idea it
will print all the stack trace on all
the threat of your own vm and by
glancing this or screening this quickly
you can check whether i have a lot of
lock locking happening whether or
whether i have a lot of scheduler like
idling or doing like nothing and
peace that comes with the GDB package
and what you need to do is like goopy
stacker like and the PID and if you
don't have pissed at you can emulate it
quickly with GDB GDB attached processor
and do thread apply and all and the
back-trace and added a tryst at length
so the second tools that we actually use
the most is called EP I know that it
means a little it means a like Earl and
enhancement proposal from both of the
guys but actually hear it means easy
profiling I guess when our colic Igor
developed this module he didn't know
what it is and so as I said it's
developed by our colleagues and it's
actually based on the gdb module and
with relatively low overhead and the
usage quite easy and it it will produce
a cat or call ranked based a compatible
file which you can used for
visualization and a easy analysis the
usage is very easy so it contains three
steps first you click the you collected
runtime data what it does it you have
for example you have a learning process
running and you do the attached and get
you a remote console on and you do start
file tracing for ABC and whenever you
need to you need to sample it for 10
seconds and it won't stop it will
produce I do produce a file called ABC
trace then the second step is an alarm
is to do offline analysis what you can
do is you can help you file anywhere and
bring your bring your airline console
anywhere then you do the convert and it
will convert it into the into a cob
reign and compatible the server step is
utilized and analysis here we have
actually a
example so this is the key cash
co-anchors I realization of an
application and as you can see here
there's a give you a lot of information
it gives you an how how many times I
Erlin function is called and how much
self time it is it consumes and it also
gives you an cost act for you to trace
who is calling this repeatedly and what
we basically do is we look at this graph
and you can figure out figure out how to
improve the system last but not least
it's we have a we use system level
profiling here we use Oh profile and I
think also you can there is some other
tools like perf the reason we use this
is that we have some new functions in
our in our system and sometimes we want
to go to the deeper level to understand
what's going on inside incentive and
Oprah file itself is very powerful it
has a lot of functionality but the most
simple way of doing it is a first
initializing initialize your profile
them and fire up your profile with your
test command then then it will generated
report and you click the report and do
the realization kind of same thing here
is an example about what kind of
realization a dude it is its kind of
cocoa graph and what you you want to
focus on is the highlighted pattern
highlighted boxes in the past for
example on here or we find that out then
we trace a tryst along this highlight a
cell phone or we have a kind of fun and
a lot of expected a memory allocation
here and probably our nerf is doing
something funny and so I want to bring
the stage to Siva here because we in
terms of performance bearing is also a
is very important and one thing about
marrying is that we need to count a lot
of events and during our development we
found that a lot of counters at that
doesn't suit our needs and so we have a
project to develop a high-performance
counter here so let's welcome Siva hi
everyone um I'll talk about a specific
problem we solve that machines oh it's
it's basically counters we try to use
some of the existing counters and we
faced a few issues so we decided to
implement our own library before we
before I show the implementation I want
to explain the compare the existing
libraries and then show some of the
bottlenecks and then move on to our
implementation so the reason we need
this high-performance counter libraries
we have millions of events happening in
our system on one single system per
second and with some of the counters are
like very frequently updated by hundreds
or thousands of processing and we need a
very good approach to this contention
problem of where many processes update
one single counter the libraries we used
our excel file some and then I put ETS
and also like we try to use the 80s
update counter API XO XO meter itself
has two kinds of counters one is
accelerometer native counters it's
implemented using ETFs tables internally
and then there is another counter
colleagues emit a fast content it's
supposedly very fast but the XO meter
native counter beats it at scale and
then ETS we just use the 80s update
counter with one single table
across the whole system and that doesn't
seem to scale well so coming to our
benchmark environment this is the yeah
eczema turn a two counters user cts
tables ETS doesn't scale so X or meter
native counters it uses per schedule or
ETS tables but the ETS I used was one
single whole table for the whole system
yeah yeah that's what I am pointing out
if I use one single EDS table it doesn't
scale if I use if I use many 80s tables
like accelerometer uses it scales well
so this is the benchmark environment I
used our platform had two cpu sockets
each CPU socket had 14 hard way threads
and then each hardware thread has has 2
hyperthreaded CPUs on it so in effect
it's 56 CPUs on our system and the LAN
runtime was 18 with SMP enabled hear the
results for the existing libraries like
you notice if we compare for just one
single process EDS tables EDS table
performs really fast but then once we
start using many number of processes
that update this single idiots table it
doesn't seem to scale X or meter fast
counter it starts out really good but
then once we move on from two processes
it's it also degrades XO min so the
accelerometer counter it starts a little
lesser than X 1 meter fast but then
after a few problems after moving on
from two or four processes it scales
pretty well relatively and obviously
experimental native counters are the
winner here by a large margin
I'm just to repeat what I said
accelerometer counter scales well but it
seems to be very slow just to update a
counter and fast contest doesn't scale
well the reason ETS doesn't scale well
seems to be that there is a lock
contention in ETS table the solution we
came up with was to implement our own
metrics library it's called MZ metrics
the we took the per CPU kind of table
approach accelerometer had and we try to
optimize it in our use case so before I
move on to our implementation I want to
explain the passive you data structures
we use and this kind of a refresher for
you on just a refresher on cache line
Cashman is just a chunk of bytes in the
main memory like say on x86 it's about
64 bytes so addresses 0 to 63 fall in 1
cache line and then 63 to 120 64 to 127
fall in the next cache line so when a
cpu tries to modify a bite it has to
fetch this whole cache line and then
modify it cannot just address one single
bite at a time now I want to before
before I move on to perceive you I
wanted to this show some false sharing
with cache line and show the effects of
it so in this in this example there are
two cpus cpus 0 and cpu one they are
trying to mod CPU 0 is trying to modify
struct foo and cpu one is trying to
modify strut bar they are totally
independent but then an luckily the two
structures fall in the same cache line
so initially initially if we assume this
is the state the cache lines in the main
memory and both the cpus haven't yet
touched this cache line so if cpu 0
initially fetches the cache line this
cache line is duplicated in l2 and it
also goes into internal CPU 0 cpu1
issues of fair
gets it into its l2 and it was also a
knot in a Cell one now say CPU zero
issues are right it has to send a right
or invalidate request to main memory and
also the other cpus that have this cache
line shade and then the other cpu CPU
one has to invalidate act back and then
the ACT has to go back to cpu 0 and only
then cpu 0 can actually write to it so
until now it has to be waiting for this
cache line to be present in its memory
now say CPU one wants to access this
cache line which is present in CPU 0
cell to cache cpu1 issues a write
request which goes to main memory which
gets can order to invalidate and then
CPU 0 has to invalidate send it back
transfer it to main memory and also
transfer it to l2 of CPU one so the
reason it's called file sharing is that
conceptually these two processes are not
really shading anything but because the
two because the two data structures are
in the same cache line it ends up as if
they are sharing some data structure so
to summarize this is this from an old
article on lwn they try to do 500
million counter updates on a quad core
system so if they try to do a single
counter update like if all the CPUs try
to share the same cache playing on the
same counter the whole bar shows the
time taken for 500 million updates but
when they try to split it or cross
multiple cash planes per each CPU the
blue bar is the is the one that that's
the amount of time taken per each CPU
obviously blue bar is very small
relatively and now if we consider the
difference between the total bar and the
blue bar the red bar is the overhead of
actually the all the CPU is using the
same cache line
another thing I want to point out is
like if we consider the effects of qpi
quickpath interconnect this effect will
be even higher because the quickpath
interconnect bandwidth is much lesser
while we are on this topic I also wanted
to point out one more thing Intel
transactional memory there is a new
feature called la collision in Intel
hardware which is currently disabled
because of some hardware bugs but once
we implement once we support this in
Erlang vm lock implementation will not
be needing finer grained locks in
smaller critical sections like say ETS
but again this is disabled on x86
because of hardware bugs it's
implemented in G lipsy but I think
airline vm it doesn't have it yet so
coming to our matrix implementation our
design it is basically based on per cpu
counters and we used an if we use
Atomics to update the counters in our
library the creating a new counter it
requests a lock deleting a counter
requires a log and then read our update
countess no there is no lock required
here it's it's an atomic operation now
to compare the new library we
implemented versus accelerometer the top
bar is the new library that we
implemented and if we compare this new
library is almost ten times faster if we
if you look at the point in blue bar
it's at 48 processes processes trying to
update one single counter it shows that
if there are 48 process is trying to
update one single counter XO meter
counters takes one second but then the
new library takes just point 1 seconds
now to put that in perspective if you
think of a system with 48 CPUs XO meter
almost uses one whole CPU if one whole
CPU to just update counters if there are
1,000,000 counter of minutes on the
system but then with our with our new
library we almost free of that cpu we
have pushed our source code here at
github.com / machines own / energy
matrix so in conclusion we explained we
showed that we found message passing to
be expensive when we try to do it at a
massive scale and we also showed that
there is when we create a large number
of processes there is a log that seem to
be causing a contention and we were not
able to use existing libraries for fast
counting so we had to come up with our
own implementation so just to summarize
the solutions what we suggested until
knob we said sharding is very useful in
many instances and then try to limit
message passing when we are trying to do
it at a very high scale level or even
backing them and when we are trying to
create like huge number of processes
it's better to limit the number of
processes than trying to create
thousands of processes and then figuring
out there are contention easier and
lastly lastly our mg metrics contests
are really fast thank you everyone we
are solving some really hard problems at
machines own and we are looking for
people to join us if you are interested
you can go to missions 1.com / curious
all right thanks
so your question is if we observe like
issues
you see the schedule order just died out
where you had 56 active work use and
then eventually now because we whenever
the issue is observed trace down to the
park we have in there afterward picks up
and we try to keep them if as I like
that if myself as like sure that self as
possible so after that we didn't really
realize that any problems in schedule
but then maybe there is but we cannot
directly pirates from with your new
functions should buy me the time
it depends on I guess it depends for the
counter library say we it's just a few
cycles where we atomically update a
counter but then yes it depends on the
library some of the difficult I mean
most of the mythos we are doing in a few
microseconds not they are not spending a
lot of time in into this so maybe
somewhere of the order of five to
fifteen thousand in microseconds or to
see so here we don't do a lot of complex
operations we implement our notifier and
the kind of key is that we don't notify
every time L turn for every messages can
so we can I use a micro matching so
because we don't want to wake up all the
recipients of very messaged notifying
yeah we can notify but not to the food
actual data
okay yes we do have they might see
people now so we do our own benchmarking
for me I one pinch marks for application
and this one more for optimizing the
performance at the lower level but we
also have a benchmark score on the
application level that we do on a daily
basis to see when we are compared for
the solution do we have that
I mean we have I guess you can talk
about Miss much more right or maybe next
weekend presented by but we have an
application-level benchmarks that kind
of measure the performance of the
application itself and that is show
where we are compared to other solution
maybe next time you can present</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>