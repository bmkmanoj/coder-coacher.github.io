<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>1 3 5  Anne Currie    Distributed Systems - Code Mesh 2017 | Coder Coacher - Coaching Coders</title><meta content="1 3 5  Anne Currie    Distributed Systems - Code Mesh 2017 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>1 3 5  Anne Currie    Distributed Systems - Code Mesh 2017</b></h2><h5 class="post__date">2018-02-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/KAjzO7FVRmw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so my name is Ann Curry and I have been
in the software industry for well over
20 years and the first half of my career
I worked in distributed systems and so
I'm gonna talk to you a little bit about
the problems the issues we encountered
with the distributed systems back then
and what maybe we can learn from them
and what we can't learn from them what
we can't take forward these days I work
for some very pleasant folk out in
Amsterdam and in London called container
solutions so back in the 90s when I used
to work on quite a variety of stateless
and stateful distributed systems we had
a very clear motivation for using them
there were very difficult to develop
they're very difficult to code they're
very very difficult to operate but we
didn't really have any choice but to use
them and the reason why we didn't have
any choice was the underlying hardware
that we were running on at the time was
really crap
it was resource wise the machines were
very small they had very limited
resources limited on a single machine
access to memory to CPU they were slow
and not only were the machine slow the
networks were very slow and very
unreliable and all of this combined to
make systems quite unresisting falling
over all the time we tend to forget how
unresisting zhh were but if you were
around in the in the 90s even in the in
the 2000s you remember that you couldn't
run your laptop your desktop for a day
without your appraising system falling
over that it was just absolutely things
fell over all the time we got very used
to the idea that things were falling
over so we use distributed systems in
what they were thought they were
originally designed for which was to
increase resilience and performance by
spreading out horizontally horizontally
scaling because we couldn't
because we couldn't vertically scale we
just didn't have access to the resources
on the individual machines or all the
networks but it was horrible it was hard
it was painful the operating it in
production was was absolutely horrific
but it was needed we had to do it and
then through the 90s and into the 2000s
something changed Moore's law started to
kick in and actually machines just got
better got a lot better
suddenly you did have those big machines
with good resource availability on them
networks got a hell of a lot better a
hell of a lot more resilience and
suddenly we didn't only have the option
or it's a scaler our only option was no
longer horizontal scaling using lots of
small machines we now had the option to
go for vertically scaling and running
things on bigger machines and the
monolith was invented partly to take
advantage of this new world of big
machines which were considerably easier
to architect for and partly because a
lot of frameworks like Ruby on Rails
very good frameworks arrived to exploit
this new big server world so
fundamentally Moore's law kicked in and
it saved us from the horrible experience
that we were having running distributed
systems and writing distributed systems
and one of the reasons why distributes
it well actually in my talk today I am
going to talk to you about what were the
reasons why distributed systems were so
difficult what what were kind of the
unusual unexpected error cases
associated with distributed systems that
used to constantly trip us up
the interesting thing is that well we
have the bigger machines now in fact
machines are growing a lot bigger still
growing at that considerable
considerable wet rates we now have
extraordinarily access to fantastically
large machines even in the cloud so why
are we thinking about giving up
monoliths because we we went to the
monolith for a very good and sensible
reason that was much more it was much
more straightforward to architect
monoliths and to operate more that's so
why on earth are we thinking about
giving them up now why why are we
talking about micro services why we cuz
of have we gone crazy because this but
there was a really good reason why we
changed so why on earth we're thinking
about about moving back and I will be
talking about that today because I don't
think is always the reason we're why we
think that we're going back to
distributed systems again and it's
actually very very important that we
understand why we're doing it because
the reason we think that we might be
doing it might be different to the
reason why we are doing it and you would
take different architectural choices
depending on your motivation it's a very
microservices distributed systems it's a
very powerful architectural technique
it's a very but it's also one that can
easily go very wrong particularly if you
don't have a clear idea about why it is
you're using it you can easily but there
are so many architectural decisions that
you can take approaches that you can
take it's very important that you decide
you pick a particular one and you know
why you've taken it so so what in the
mind is used to make distributed systems
so difficult it's something that is both
a combination of hardware problems and
also people problems as well so things
that haven't actually changed it's a
certain extent it's the laws of physics
in a distributed system you have more
things so you have more hardware
components and when you have more
components there is more that can go
wrong and there's more that can go wrong
in strange ways strange ordering issues
that can
you've also got the fact that people
humans have a lot have a great deal of
difficulty getting their head around the
kind of problems that can occur with the
distributed system and you will
constantly hit that if you're building a
distributed system if you're accessing
it expecting a distributed system you
will find that your developers don't
believe in a lot of the error cases and
will come and talk about what effect
that might have later but it is a
definite issue so what kind of problems
do we have that are fairly specific to
they're not entirely specific but
they're more or less specific to
distributed systems that's your
developers because remember although we
spend a load of time and thinking and
pain over distributed systems in the 90s
most of the developers most of the
architects that you're working with now
we're not surround them so they haven't
experienced it they've they are they are
children of the long summer of the
monolith they have been working in an
environment which is considerably easier
to develop it for very good reason
that's why we chose it but now they're
thinking about going back into into a
horrible winter is coming style world of
distributed systems they're going to
have to think about a whole load of new
problems problems and issues that
they're going to have to be they're
going to have to be coped with in any
distributed system design of any
complexity so you do need to have to
think about it and the first one is what
time is it so on a single machine it's
quite easy to work out what time is this
you just call the the clock on the
machine that will tell you what time it
is within the distributed system there
is literally no such thing as a global
time now you might say oh well you know
Google have their glow go global clock
that has the old GPS and all that kind
of stuff the Google of Google that is
still not a global clock I still that's
just made the race conditions a little
bit less likely to occur it's closed the
windows but it's still not really
genuinely a global clock the easiest
thing in a distributed system is just to
not use the clock as a way of defining
order
if you can if you can think about doing
things in a way that doesn't use the
clock doesn't use wall time might just
it might use relative time based on
things that happened within a particular
session that's not a clock so that's
just a an incremental counter of some
description I mean if you can try not to
use the time now if you're a bit unsure
about whether this is really true I had
a chat too I was at a conference a
couple of weeks ago and I was chatting
to data dogs so they do tracing and I
was chatting to their APM engineer and
saying and I wanted to know I thought
well then they'll definitely be exposed
to the issue of different times on
different clocks because it's it's
actually one of the things which is most
frustrating in the services system is
that when you get Diagnostics in from
different machines it can be very
difficult to work out what order things
happened in if the machines have
different clock signs and they always do
have different focal times so I
suspected the data dog will have would
have run into this so I asked them what
kind of a spread of clock times do you
typically see across a distributed
system that you're tracing in and they
said well do you know 10 minutes is not
unheard of you know it's it's and these
are systems these are distributed
systems where people are probably making
some effort to keep the clocks in sync
but fundamentally it's very difficult to
keep clock something different every
machines clock will be running at a
different rates when just sending
missing messages takes time even if
you're sending messages sync messages
all the time so every machine in your
system which you're probably not
they will start to drift and it's where
they drift it'll be those if you're
relying on ordering based on clock time
to do anything in your system to make
guarantee consistency or whatever
it'll be where that drift occurs that
you'll get bugs that are very difficult
to diagnose so the easiest thing is
actually just to be very aware of this
and try not to use a global clock is the
way that you define order within your
system if you possibly can
there are usually ways that you can find
your way around not doing this if you
think about it but if everybody's just
doesn't if it never even crosses
people's mind
then you're you're gonna end up and in
difficulty if you think about it it's
not impossible you can design around it
the next thing is consistency now we do
talk a lot we are quite obsessed at the
moment with with highly consistent
systems I was a native in the 90s we
couldn't have highly consistent systems
because that was just way too slow
I'm a consistent system relies on you
effectively locking what's going on or
across your system which is
extraordinarily slow and somewhat does
somewhat seem to defeat the objects of
having a fast distributed system but if
you don't then really what you're
looking at is having States being
updated on different nodes within your
system and then eventually being merged
together there are different ways that
you can you can safely do that but you
can eventually merge your system
together it's fine that totally works
you but you have to plan for it because
the upshot of that is that clients
querying your system if they could they
you don't know which node they're going
to talk to they can get out of date data
they can get stale data they can get
data but will that predates the response
to their previous requests for example
so you can you do need to be aware of
that you can design around it you can be
less demanding from your clients you can
accept a fair amount of inconsistency on
your data but again you have to plan for
it and design for it if you don't plan
and design for it then everybody's gonna
have a very rude awakening so another
thing that you need to be aware of is
and plan for and think about at design
time he's reliable versus unreliable
messaging when I talk about this I'm not
talking about UDP I'm not certain versus
TCP just let's let's assume that
everything's TCP at the moment it's not
what we CCP but let's assume it's TCP
for that the argument here I'm really
talking about even where you've got a
nice reliable network if you've got a
stateful service over here talking to a
stateful service over here and they just
exchange messages with one another that
is unreliable messaging and the reason
is that this state that these stateless
server
stateless services can fall over at any
time because that's what's take the
services do that's what they're designed
for and they can just lose your message
so say message a the service a sent a
message to service B and it's a hundred
percent sure that B got the message
because it was acknowledged and then B
falls over a doesn't know what happened
to that message doesn't know if it
processed the message yet it doesn't
know if it passed the message on to
somebody else
doesn't know if it was halfway through
processing the message has absolutely no
idea what happened to that message as
soon as it loses if a is sending a
message to state the service B it has to
cope with the fact that it doesn't know
what's going to happen next just doesn't
know what's going to happen to that
message
now there are two ways approaches
potential approaches that you can take
that you can just design in your systems
to accept message loss and duplication
and out of order arrival and actually
that is perfectly doable you can
definitely do it it's difficult but it's
definitely definitely doable there are
lots of examples of so for example
services that regularly send unreliable
messages like video or audio there are
there are for all of these types of
systems there are designed ways to
handle message loss or out of order
message receipt you can take some of
those approaches and use them within
your own services there is another
approach which is you say well I don't
really want to have to design to handle
message loss and unreliable delivery and
I kind of saw Oh
delivery or all the other things that
can go wrong when you're sending
unreliable messages you can put a
stateful service in between to hold the
message and make sure that it is not
lost and it is processed and delivered
so something like a queueing service a
reliable queuing service you can you
have can have your stateless service
over here pop the message on the queue
it knows that they keep the queue
acknowledges that it has the message
right now it knows the message is safe
and the queue won't lose the message
until B has said look I'm I've processed
it B falls over that processing the
message the queue will make sure that
somebody else processes them and
so that is a reliable way when you've
got to stateless services of ensuring
that they can talk to one another
without losing messages the downside of
it is it's really slow compared to just
sending a message and handling the
message loss going through an
intermediate queue is massively
massively slow so generally we didn't do
that very often when we were in the 90s
trying to make things a bit faster but
it does make things vastly more reliable
and it's more of an option now
particularly when there are quite good
managed queue services out there another
thing to be thinking about is speed of
message transfer and to me this is a
very very important thing to think about
when you're designing a micro service
architecture people often talk to me or
ask me or discuss how big should a micro
service be but I think that's the wrong
question it doesn't really matter how
big it is to me what's really matters is
what's on the interface what kind of
messages are you passing between one
micro service and another microsomes
because internal messages between a
component between threads or within a
single machine are very broadly speaking
a hundred times faster raw without
adding any any overhead to them than
messages between two two machines that
are in any way
remote from one another even if they're
just sitting next to one another in a
datacenter so you've suddenly got a
hundred a hundred times late maudette
latency there with processing that
message then if you add something like
RPC you're adding another tens of
another very considerable bunch of delay
if you're adding if you're using rest
I think rest is about twenty times
slower than RPC so all of these things
add latency into your messages so you
need to be very careful when you're
splitting up micro services that you
don't have highly latency sensitive
messages going between your micro
services unless you want to force those
micro services to be co-located on the
same machine which is which is a
all compiled together by a library which
is an approach I've seen taken but
basically be careful what's the messages
are that are passing between your
microservices if they are latency
sensitive and they're part of your
mainline operation and you really can't
afford them to slow down massively
possibly you don't want to have them on
your micro service interface so
something to think about and and with
all of these things
errors yes in a distributed system the
error tooling that you're using in a pod
against a moment it's probably not ideal
for your distributed system you will
almost certainly everybody I've spoken
to who's doing this successfully it's
hard to throw out their old error
handler error tracking monitoring
systems and put in new ones and in fact
it hasn't even been a one-time operation
where they've just been able to say all
right yeah I'll get rid of my old one
and I'll put in this new one that just
distributed system sensitive and it'll
be absolutely fine they've had to keep
replacing them because actually the work
we're doing with the distributed systems
at the moment it's quite cutting edge
and tools come out that might last you
for a bit but as your scale increases
you quite often actually I've seen a lot
of people will outstrip the capacities
of the tool that they originally
produced so fundamentally what I'm
saying is don't get too wedded to your
tools and the distributed service
invited distributed system environments
you are probably going as I have to keep
upgrading and replacing them so and
you'll certainly have to upgrade and
replace the ones that you are using for
a monolith another thing what are the
many other things that are difficult in
distributed systems is monitoring
whether your service or where the
individual services are happy or not but
are they are they successfully operating
lots of currents monitoring systems
monitor end to end and that's where it
ends when point to point so they will
give you an alert if a server is poor
they'll give you an alert if a web
application is no longer responding but
that isn't actually the kind of problem
that you want to know about in the
distributed system environment you
really want to be designing your
distributed system to just come up and
if a server falls over fine that's just
business as usual that's a mainline
error case you just have an Orchestrator
that brings your system back up again in
most cases just because the server falls
over you shouldn't get woken up at three
o'clock in the morning the system should
just automatically handle that so that
kind of monitoring and alerting it's not
really actually that useful to you you
do not it's an error in itself if that
server falling over is causing a problem
to your system and the error there is
not that the server fell over the error
is that your system went offline
it's an end-to-end monitoring problem is
your service still available to clients
is it offering something that your users
would consider to be availability or not
I think this is this is something that
is generally now referred to as
observability as opposed to monitoring
you want to move over your alerting
system to be observability issues your
users your users experience of the
system has declined below a certain
acceptable points rather than a machines
fault over which shouldn't if you've
designed your system correctly cause
your users experience of the system to
decline below a certain points that
should be alright so you do need to be
thinking about these things quite
differently so when I was so that I
spent an awful lot of my career first
writing distributed systems and then
supporting them in production and one
thing I always found at a large part of
my job when I was supporting the
assistance in production was taking
developers and training them in how to
diagnose problems how to solve problems
with with the stuff with the distributed
system when it inevitably fell over and
the thing that I found most difficult to
convince them of was the level of
coincidence that happens every day
it's a distributed system you've got a
lot of components all interacting all
talking to one another and the
interesting thing is that that I think I
think it was one of the earliest because
today Chris I think was talking about
how narrative is not a good way of
describing what happens in his
distributed system because it's not
really that the narrative is is all
best-case this happens this is what's
gonna happen but that isn't really what
you end up spending your time focusing
on once you're actually operating the
thing there the narrative the best-case
story everything working happily you're
not even looking at system when that's
happening when when you're looking at
the system is because something's gone
horribly wrong the narrative no longer
applies and things are too complex
generally for you to have a narrative
around every possible error case there
are ways to handle that to force
yourself to have a narrative around
every possible error case which is
actually what he also talks about as
well which is FSMs
very useful tool for that developers
hate them I've noticed because they
because it forces you it's its unearth
all of the unpleasant error cases and
edge conditions and things it forces you
to think about them and I have tended to
find that in much the same way that
developers don't believe in coincidences
they don't really like looking FSMs
either yes so they're going to need to
get a different mindset so they could go
into need to be much more they're going
to need to catastrophize a little bit
more than they they generally do and
it's not surprising that they have
difficulty envisaging the error case at
the the race conditions the error cases
associated with distributed systems
because human life is not as I said
entirely full of coincidences it's often
I think when I was doing this I found I
got better advice from fiction than I
did from reality because reality doesn't
contain a lot of coincidences but
fiction is is driven by the coincidence
so you'll get much better ideas of
things that people have thought through
but looking at fiction
then you will do by just observing
what's going on in your everyday life
and this quote here by Sherlock Holmes
and mr. Spock I always found to be the
best single best piece of advice when
diagnosing distributed systems problems
that when you eliminate the impossible
whatever remains however improbable must
be the solution I can't count how many
times we found bugs in systems that we'd
show it to the developer or the
developer would be there and they'd say
I just don't believe that could happen
that just couldn't happen it's way too
improbable but in a distributed system
in a computer in any computer system
even in a just a multi-threaded system
things happen at such a speed and such a
rate that your normal human conception
of what's likely or unlikely just
doesn't apply you know a million to one
it's the old Terry Pratchett thing the
million to one chance happens nine times
out of ten a million to one chance is
actually something you're gonna hit in
production every day so you do need to
be aware of that but it isn't humanly
understandable so you do need to think
about this Oh something else that I
noticed constantly when when teaching
developers how to debug a distributed
systems in production is they do have a
tendency to to approach debugging even
actually was it what is not the right
way they have a tendency to want to look
at the Diagnostics so they'll they'll
say what just give me all the logs and
I'll look at all the logs there are a
lot of logs you do not want them looking
at the logs to find out whether a
problem occurred if you want to see if
you want to just show them a very good
example this didn't exist when I was
doing this but if you want to show them
a very good example of diagnosing
complex problems with lots of potential
variables get them to watch an episode
of house house quite liked house the
idea so house a person was a sick person
would come into houses hot doctor houses
hospital and no one would know what was
wrong with them they didn't just
a whole load of tests and look at them
the results in fact they did there was
one episode where they discussed why
they didn't do that they didn't do that
doesn't work what they did instead was
that they came up with a theory they
said you know I think that person has a
lupus and then they would selectively
choose Diagnostics to attempt to
disprove that theory so they would say
okay well if they had lupus
then they wouldn't have this result or
they would have this result who wouldn't
have this result let's go and look at
those results see if we can rule in or
outs that particular theory hypothesis
and they would rule that out because it
would turn that they wouldn't have lupus
and they would then come up with another
theory and they would go through the
same process again and that's actually a
very good way of diagnosing problems in
complex distributed systems because
there'll be a lot of logs you don't want
people sitting down starting logs three
weeks later they will be back and you
will not be no further along solving
that problem come up with a hypothesis
attempt to disprove the hypothesis I
still love this stuff
it's really it was really enjoyable but
it was quite time-consuming and I never
want to do again uh-huh
so this is an interesting one bugs in
distributed systems are driven by
coincidence the main driver of
coincidence is traffic
it's things happening on your system
events events dear boy events the it's
it's messages coming in it's different
things that can possibly overtake one
another in ways that you weren't really
expecting there is an issue with this
which is that problems distributed
systems problems are most likely to
happen at times of peak traffic which
are times at which it is most expensive
for you to have an outage a problem in
your production environments you know
when when is the traffic related
incident likely to happen to you if
you're an e-commerce company it's
probably the middle of the afternoon on
Black Friday when you've got a peak of
traffic that's vastly higher and for a
long period of time the time at which
you absolutely
we do not want your system to go out of
action so one way that people people
handle that is that they they just make
sure that the system is vastly over
provisioned so that there aren't
effectively so many messages per machine
that's very expensive but that is the
way that's quite often we used to do but
nowadays and this didn't exist for me
and I I kind of wish it had there's
chaos engineering and what I think is
interesting about chaos engineer it's
like obviously the concept of forcing
errors to occur on your distributed
system it's a very good one
but what I like about it most is there
is the idea that you can force errors to
occur at times when it isn't necessarily
quite so expensive for you to debug them
so rather than the the the problems all
occurring at times when it will be
incredibly expensive for you to resolve
the problem because it's it's the last
shopping day before Christmas or Black
Friday you can engineer problems to
happen on Tuesday morning is it 2:00
a.m. in February when you really have
very low traffic so it's it takes the
pressure off takes the cost associated
with failures down and that's actually
the thing that I can actually very
rarely hear talk about but I like I you
know I'm very interested in the bottom
line so this is what most interests me
about chaos engines so really what I
want you to take away the question I
want you to take away from this is why
are you using microservices because they
are really difficult they're really
difficult so it was a stateful
distributed system stateful
microservices very very difficult to
writes very difficult to operate in fact
I realized that I found this this
conference quite interesting because so
many of the talks and perhaps that's
just I because I've been self selecting
which talks like ot
I think almost all of the talks that
I've been to so far have been are about
effectively making distributed systems
problems obvious to the compiler
they've been about imposing types that
surface some of these distributed
systems problems so people are aware of
it
and looking for ways to avoid it by
effectively just not allowing your code
to contain these issues but that bits
you know that that's that's nice down
the road but a lot of these these more
esoteric languages I'm not necessarily
going to solve problems that that are
occurring right now because right now
there are armies of folk going out and
building Micra services without really
understanding what's what they're
getting into because when I talked about
why we'll be using Microsoft sis in the
90s I said we were using it for
performance and we had to use it
performance we had to think about
performance all the time when I talk to
people now about why they're using micro
services generally unless it's like
Netflix they don't tell me it's for
performance in fact they specifically
say look I don't mind forgetting better
performance I'm not going to say no to
better performance but I'm using micro
services for the decoupling I'm using it
because then I that way I can have teams
working individually on mic recessed
without having to interact so much with
one another and I can run my development
teams in parallel more effectively and
that is interesting because what they're
using micro services for is to make
their lives easier but they've picked an
an architecture that can make your life
one hell of a lot more difficult it's
amazing how many people have conferences
say oh we built a Microsoft System and
then we had to throw it all away because
it ran like a dog because they had puts
latency certain sensitive messages on
their interfaces for example or we built
a micro service and what other kind of
problems that they have but they just
wouldn't talk to one another because
they'd got everybody working on their
own thing and they had no overall vision
for what consistency of API is or how
things should deal with one another
defining what or what the behavior each
of each service was with micro services
I think there is a perception that it's
easier it's not easier there are
certainly ways that you can write micro
services to run more slowly and be
somewhat easier and that will
probably involve loads and loads of
reliable cues you put an asynchronous
communication that will run very slowly
but it will be an awful lot easier to
develop if you want it to run at
reasonable speed it will be very very
difficult to architect by either way I
don't think you can kick your architects
out quite yet
because you do need someone to say well
hang on a minute snow our overall plan
is we're going to use an awful lot of
distributed cues how are we going to you
know we want to use link Rd everywhere
to simplify we want to have consistent
API formats we want to all use wrestle
interfaces or we want to all use gr PC
Wii U I think everybody does need to
make simplifying decisions of
commonality around what's going on in
their Microsoft's environments from from
well my experience from the past this
was definitely the case my experience
from going out and talking to a lot of
people now which I do is that the ones
who have become successful have often
had to go back and put in a lot of
consistency into what they're doing and
they've had to use architects so yes
micro services can be absolutely amazing
it's a very cool and the trouble with it
is it's a very cool powerful
architectural technique and we are all
very inclined to over engineer micro
services is something where you can over
engineer like crazy or under engineer or
just really badly engineer and it's it's
very easy to get it wrong and it
requires a considerable amounts of
experience to get it right the one thing
that we do have now that we certainly
didn't have in the nineties and I really
wish we had was really good stateful
services as a service if I was doing
micro services today I would probably be
looking at FAR's although I think that's
that's a little bit the tooling isn't
there quite for that yet
but I'd be looking at stateless
containerized applications talking to
manage - stateful services so I never
had to manage all right a stateful
service ever again because I've been
there done that never want to do that
again that is very difficult to do so if
you want to do this kind of
really really think about using the
tools that are available to you do not
write them yourselves
in fact so quite a few I went to an
interviewed a few folk recently about
their micro services implementations so
the FT very advanced on this a source
very advanced advanced on this and well
and one of the things that they
mentioned very early is that they never
wrote their own stateful services they
always bought them they're ideal if it's
all possible they bought them hosted if
they couldn't buy them hosted because
they weren't yet available hosted then
they would host them themselves but they
were just desperate for them to grow a
solid hosted version that become
available so they could move to it
because they didn't want to do this
themselves and rightly so so if you want
to read other I'll be around for the
rest of the day always happy to talk
about anything particularly this if you
want to read more about it there's a
chapter in the free ebook that I've just
published called the cloud native
attitude you can just google that and
find that that's available from the
container solutions website that this
talk was also on the new stack so please
do read the book it's free and it's very
it's not just this kind of stuff it's
kind of a very basic entry-level kind of
explainer of orchestrators and
containers and distributed systems and
just to try and get people a reasonably
straight full background in it anyway
thank you very much for listening today
I hope you have some questions
hello hi thanks for sharing their
experiences I wanted to ask about the
chaos engineering you mentioned that it
might be useful technique for us do you
have any ideas would you fact in the
systems in terms of chaos engineering
what's resources and what would you look
for
well now you're asking me a question
that's I I'm not qualified to answer
because I never actually used a chaos
engine in in production I know that it
would have been useful to me what would
I have found well I think I would have
focused on the things that that did more
or less did fall over more often so I
mean the obvious thing is just to use it
to fire a lot more effectively fire a
lot more fake requests into your system
because that's most likely to be what
you're what will really trigger
expensive outages in real life so I'd
probably start from that I mean the
classic is you just kill machines you
know even if you if I may work because I
like to do things cheaply I would start
with like a real you know a chaos monkey
I would just have somebody in my team
who I know manator does the Charis
monkey who on the day that we were going
to the cows testing pulled down pulled
out network cables well actually if
you're in a figuratively pulled out
network cables turned off machines just
starts easy but where I have seen this
described something that I thought was a
very sensible approach that I hadn't
heard before that hadn't heard discussed
was that that's the most important thing
is that you tell folk that you're going
to do it and you tell folk what it is
you're going to do so the idea is not
actually the system falls over the idea
is that your developers are aware they
believe it you know what I was saying
that people don't believe it they don't
believe in coincidence but they do
believe in chaos monkeys I suspect so
really you know you just say well you
know you don't you don't say well you
have to fix this because statistically
speaking every VM falls over every 200
days and we've got 200 VMs
therefore they will be available over
every day you say you have to fix it
because of the gas monkey that is my
suspicion of what's going to happen when
you start talking about gas monkey just
someone running for the door question so
I think with with micro services and
moving from monoliths the micro services
people just kind of gave up on
transactions right it's kind of like
well that was a really nice thing and
now we don't have it anymore so I mean
what it's just a lot of its realness
it's just a lot of sitting back and
thinking hard about what the interfaces
are between your micro services you
don't have to put it interface in
everywhere you know you can decide that
actually generally speaking people
aren't in most cases like I'm seeing
people aren't just realizing everything
from scratch with Microsoft says that if
you're if you're a successful company
you've got some money coming in from
your monolith and you don't want to
chuck it out and to go to a bitter fruit
giant Big Bang projects where you move
over to micro services that would be
crazy talk Frank frankly I'm not
surprised people don't want to do it so
they kind of split off so a lot of the
decision-making is what are you going to
split off how big should that be and a
lot of the discussions I hear about it
are you know split off the bits where
there's an awful lot of churn and the
code so you've got a lot of developers
who are constantly battling against one
another part part of the other thing you
need to be taking into account is what
are that what's the message flow going
to now be between that monolith and
that's microservice you know do you once
said you want to be there are certain
things you probably won't want to split
and there you might want to keep them
all over here or give them all over here
but that's something that you just have
to think very hard about your messages
and your interfaces where they go you
can do one more quick one
did you have one some people some people
have used introducing myself is a way to
go to support multiple languages often
because someone wants to play with a new
language or you know using the you know
is that a good reason or have you seen
good outcomes with that so leading
question there yes I I am somebody made
a joke to me earlier about about mother
saying to say to their child go and find
out what your brother's doing and stop
him that developers go and find out what
your developers are doing with Mike
reservist and stop them if they're using
it because it's cool because they just
want to buck around and do something
dangerous in production like user
language that they don't necessarily
know how to operate or supports that's
not for me a good reason for using micro
services I don't object to the use of to
using microcircuits because of the
radical decoupling and the ability to to
parallelize your teams I don't know I
can see the logic in that I'm just
saying that if that is your logic you
need to be aware of it and you need to
make architectural choices accordingly
but for me because it's cool because I
get to play with some technology danger
free is not a good reason at all and it
should be stopped under my watch it
would be stopped
[Laughter]
okay let's thank the speaker again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>