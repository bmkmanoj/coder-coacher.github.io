<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Cloud, Distributed, Embedded: Erlang in the Heterogeneous Computing World - Omer Kilic | Coder Coacher - Coaching Coders</title><meta content="Cloud, Distributed, Embedded: Erlang in the Heterogeneous Computing World - Omer Kilic - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Cloud, Distributed, Embedded: Erlang in the Heterogeneous Computing World - Omer Kilic</b></h2><h5 class="post__date">2014-05-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/S91efnwh7nM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">it is working cool hi everyone I
remember I'm going to talk about
embedded stuff today but I'm also going
to talk about heterogeneous computing
and the sort of core processes that
we're using as part of our both embedded
and actually server class or desktop car
systems as well so my talk is going to
consist of two parts really the first
one is going to look at the challenges
that we have the sort of issues that we
can tackle with utilizing different kind
of architect different kinds of
architectures and the second part is
going to be a little update on my EUC
presentation last year which is about
the airline embedded project so we're
going to look at the sort of challenges
that we have in modern computing systems
just a general overview of things and
then look at what sort of devices we can
employ to speed up certain operations
that we have in our systems and how it
can be your float things to make them
more efficient and also to reduce power
consumption which is a major problem
these days how can we program them how
what sort of methodologies we can employ
to program these devices and I also
talked about our little exploration with
the Epiphany for the parallel aboard
that we have been lucky enough to get
our hands on and as I mentioned I'll
update you on the state of our lying
embedded the framework that we have been
working on and hopefully we'll have some
time for questions and answers so we
have hit the frequency wall which means
that we won't be jumping from say having
three gigahertz or four gigaros
processors now to 20 gigahertz processes
so the jump we had from say a Pentium 22
pentium 4 and I can see that we've also
at the latency whoa we can't really keep
increasing the cpu frequency in the
hopes that we will get more data in and
out of our systems oops
that's not my presentation maybe I
should just use the keyboard memory
bottleneck so for a while the memory
system have has caught up with the CPU
and we didn't have any issues in terms
of trying to get data into our system
but now we find that it is becoming a
problem and certain processing devices
what am i doing it's all the club mutter
I've been drinking all the way sit back
up yeah so certain devices like GPUs
employ some very interesting techniques
to get really really high speed memory
access and I'll mention that in a minute
and also our software is getting more
and more complex to a point that we
can't really just keep throwing more
processing power at it in the hopes that
it will automatically become more
efficient there's this beautiful local
Alexei quite depressing low called
Amdahl's law if you're not familiar with
it it simply takes that maximum speed up
you can gain in your system is or
through parallel processing is set by
the amount of sequential or serial code
that you have in your system so if you
look at this graph you can see the speed
up you can achieve by having a large
amount of your system in parallel so you
can offload the calculation on to other
devices or other nodes and distribute
the workload but if your system has a
lot of sequential code or lots of the
data dependencies in your calculations
then you can't really or float that
anywhere it has to be executed in
sequence we also have some hardware
limitations we have yield issues with
the latest nodes that we have we're
getting really really small processors
or actually transistors in our processes
now and the more we try to reduce the
size the less successful results are so
there's some increasingly amazing
innovations that's coming out of silicon
manufacturing but still the sizes that
we're dealing with are
really really really really small and
that there are lots and lots of problems
associated with it and also wiring and
interconnect is becoming an issue if you
consider the fact that we have billions
and billions of transistors in our
silicon devices these days I think you
must have heard Joe say that you can't
really get clocking right or they
weren't able to get clocking right and
so on there are several issues like
propagation and stuff still present in
these really large devices that we have
as well so wiring an interconnect is
another factor that holds us back which
simply means that we can't keep putting
billions more transistors into our
devices in the hopes that we will get
speed ups also the more transistors we
pack together the more thermal problems
we have as well so even though these
transistors are quite small um they're
not ideal devices nothing in the world
is ideal of course it's all very
depressing now so there's this
Commonwealth physical property of each
transistor called leakage current in any
given moment most of the transistors in
your system might be idle but that
doesn't mean that they don't consume any
power because of the manufacturing
process and knows because it's not a
perfect manufacturing process they just
leak power and the more transistors you
have the more powered a leak even though
each transistor can leak a very very
small amount of power if you consider
the fact that modern CPUs have billions
of transistors it all adds up and this
all of them to power consumption it is
becoming the number one criteria when
people design systems almost in all
fields so we have initiatives that look
at this program on data data center
levels and we have people looking at how
to improve the power consumption on
mobile processes and mobile devices and
even companies that sell lots and lots
of standard x86 class server hardware Oh
actually forgot to mention moves low
if you're not familiar with it it's a
law that states that the number of
transistors you can put on an integrated
circuit will double approximately every
18 months or two years or every process
technology we're getting to the end of
that we can't keep you know following
that using the standard CMOS
manufacturing practices that we have so
as going back to my previous sentence
even companies like HP who produce a lot
of server grade hardware are now
declaring that the standard
methodologies we have to deal for
dealing with these sort of processing
requirements are not going to be enough
and they're not sustainable and we also
have this beautiful thing called the
Internet of Things this is not very
visible and I just put this here just to
show you how every field is now
demanding computing power which means
that all of our devices will have
multiple cores and cameras and stuff
displays that we don't even know what to
do it and so on but this all adds up
into more power consumption and more
processing and it for us engineers it
means that we have to find better ways
of dealing with designing these systems
now here's an example to the
organizations that i mentioned Open
Compute Project looks at how to reduce
the power efficiency in data centers and
it has some pretty large companies and
entities that collaborate and trying to
find new ways to cool servers and
systems but at the end of the day if
they keep using the same technology this
is not going to be enough they have to
keep looking or exploring the
possibilities of integrating non x86 or
non Warren Newman based architectures
into their stack in order to overcome
the problems that we have today it all
sounds depressing but there is a
solution and we actually use this a lot
heterogeneous computing we use highly
specialized special-purpose processing
devices for speeding up certain things
certain applications this
sentence is pretty obvious but that's
you know what it is it makes sense to
dedicate a certain silicon device which
is tuned or custom designed to deal with
a certain issue to do that and let our
CPU deal with the sort of
general-purpose stuff instead so the
advantages of using the specialized
devices can be in the orders of 100 x
improvements is not unheard of
especially for stuff that requires a lot
of processing say image processing or
sound or video processing by using
specialized devices you can gain a lot
of speed ups and this is not just in the
in terms of raw power but in terms of
energy efficiency as well and it all
boils down to being able to divide your
workload into well devices that can
execute things in parallel and I'm going
to talk about those in the next couple
of slides well before I jump to that
there a couple of different classes of
devices that you can use you can just
buy asics that are non programmable of
course application specific integrated
circuits you can buy DSPs and GPUs and
program them following the sort of
frameworks and methodologies they have
so they're not fully custom devices or
you can go for attaching say a field
programmable gate array into your PCI
Express slot and then create your own
fully custom processing architecture so
here's a look at a one of AMD's
marketing slides with brains and waves
and stuff coming out of it it clearly
states that you know that there's a cpu
and GPU in the system with a dedicated
memory bus in between them and serial
workloads are best executed on cpu and
parallel workloads work best on
architectures that have the capability
to process lots and lots of data very
efficiently like GPUs and they also have
this apu which deals with essentially
it's a combination of a GPU and the sea
view
when you look at the data sheet or the
marketing material for heterogeneous
architectures these sort of colorful
stuff comes up and this morning when I
was going through my slides I this just
made me think of cats with lasers coming
out of their eyes it's beautiful so
let's look at some application specific
accelerators or Co process as you can
add to your system these are all
commercially available boards you can
buy them and add them to your systems so
for instance this is Annette FPGA board
it lets you attach 10 Gigabit Ethernet
ports to your system and instead of
having to deal with processing all the
frames on your cpu you have a quite
beefy fpga device a vertex 5 there which
will let you implement your whatever
protocol you are using whatever
filtering you want to do and the idea is
to offload as much processing to this as
you can and let your CPU deal with the
classification of packets coming in or
well stuff it's it can do without
breaking a sweat basically then another
this is not a common processor ready I
mean research institutes and a couple
people might have this but every one of
us in this room uses these class of
devices their GPUs graphics processing
unit now GPUs have to work on massive
parallel or massively parallel execution
models because if you if you think of
graphics rendering there's a lot of
pixels that needs to be rendered and
there's texture textures and all sorts
of different things that need to come
together in order to form a scene for
your display or your image that you're
trying to display to make sense so the
first generate or if the first kind of
GPUs were you know you could program
them but you have to use OpenGL or a
graphics language and abuse it to make
you know something else of all the nice
processing cores you had in there so I
remember seeing a an application I think
it was an SSL accelerate
that was written using shader memories
and essentially reverse it well hacking
it was such a beautiful hack using the
graphics language to do SSL acceleration
but that's not the case anymore we can
now use these as general purpose
computing devices so there's a new field
now gpg general purpose graphics
processing units GP GPUs if you look at
an architecture of a graphics processing
unit your modern desktop computer might
have four cores eight cores or a little
bit more maybe but GPUs have hundreds of
course but they're not don't think of as
a pentium 4 94 is this 2005 so don't
think of them as your I seven core or
you know huge monolithic core that can
do anything and everything they're quite
fine grain devices that are designed to
do very small amounts of functionality
that is distributed across all these
hundreds of course on a massive amount
of data so it's not uncommon to have
hundreds of cores on a GP or compute
units on GPUs another general purpose
use case for these accelerators or Core
processors are the nvidia tesla which is
actually a GPU but it's packaged in a
nun GPU format for instance there are no
monitor connectors or anything like that
so you can port well you can use the
paradigms that you're used to
programming GPUs and just use one of
these specialized accelerators or this
is quite interesting this is a xeon phi
coprocessor by intel this is actually a
slim down a collection of slim down
pentium 4 cores it sounds quite oh
really using such an old device maybe
for these kind of purposes but they made
it so that there's a parallel execution
pipeline and
if this gets exposed as quite a few
chords to your system I don't remember
the exact number i think it's somewhere
in the region of 200 or something like
that because each core can actually do
for instructions in a round-robin
fashion so the point is you can buy
these of generic accelerators or Co
processes for your systems and start
hacking on them but how do you actually
program these things I mean you can just
write some rosy but there are better
ways of programming them and media came
up with cuda for their geforce products
and then later tesla and other
accelerator cards the one we were really
interested in is opencl it runs on
pretty much all the platforms and it's
gaining traction in the embedded market
quite significantly and not just on CPUs
and GPUs I'll talk a bit more about it
in a second and this is a bit of an
extra stripped slide really so it's not
just about again CPUs or GPS there are
lots of optimizations that you can have
on your cpu's vector extensions there
are different kinds of architectures
such as the cell architecture that was
in PlayStation 3 and we're going to talk
about epiphany in a minute intel mi see
is the Xeon Phi and so on so there's a
lot of research in going into how to
accelerate things in a meaningful
context so the first generation of these
accelerators you have to program them
usually in C using the proprietary api's
but the problem is you don't you can't
really poor to your software if you
write it using a very low level sort of
highly specific API and there have been
in the past couple years in fact there
have been architectures that have been
obsoleted so you know all the investment
that has gone into coding for those
architectures is pretty much lost since
you can't buy those devices and
more so as I mentioned interested in
opencl which is a generic framework the
open computing language that is
supported by quite a lot of companies
and runs on quite a few different
architectures as well so the idea is
that I mean as with all generic
abstractions it's not you know it
doesn't work as you'd expect fully
functionally on all the architectures
without any changes whatsoever there are
certain things you need to consider but
the idea is that you should be able to
buy minimal tweaking run your code on
GPUs CPUs or DSPs or architectures that
supports OpenGL functionality so it
allows you to write celite code and you
should be able to as i mentioned run it
on different architectures so algeria
supports fpga support for instance or
rather opencl support on their FPGAs
adapt eva has opencl for the epiphany
and you can run opencl on your intel
cpus as well but in order to make use of
this the key point rather is to look at
your program in a sort of data parallel
way if you can map your application to
run on a lot of course then using open
sale makes a lot of sense but if your
application is fully sequential then
there's really no point in looking at
this at all this is the I find this
interesting because this there's a
difference between the desktop world and
the embedded in terms of how you
actually transfer data in and out of
stuff are using opencl in desktop you
you generally have different memories so
you know your memory structure is not
really uniform so if you want to move
stuff around you need to physically copy
it from one ship to another now that has
some latency of problems that may have
some latency issues associated with both
on embedded devices or you know most of
the system on chip devices that supports
this
core processor functionality it's a
single chunk of memory so you can just
do direct copy and without having to
physically move it across chips and so
on that doesn't sound too interesting
when you speak it out loud really
sounded more interesting in my head
sorry guys so why are we interested in
opencl all of a sudden because we have
this really interesting platform called
the parallel aboard coming out very soon
now the parallel aboard can be thought
of as the Raspberry Pi for the
multi-core domain because similarly
sized it's similarly sized it's going to
cost ninety nine dollars and really the
processing power you have on it is just
tremendous you have a 16-core RISC
architecture that's extremely low power
I'll talk about it in a minute as I link
zinc system-on-chip device which has too
hard ARM cores as an implement it in
silicon not emulated in the fpga logic
and quite a significant amount of fpga
logic in there so on a board that's
credit card sized you have also gigabit
ethernet and USB and all sorts of inputs
and ups HDMI as well so all aboard
that's credit card size do you have a
dual core arm processor a significant
amount of fpga logic and a 16 core
processor this is exciting and we've
been fortunate enough to receive one of
the early prototypes which you can see
outside actually and it doesn't look
credit card size but you know it has the
16 core processor on it and we've been
looking at a little demo application a
vision demo which is unfortunately not
working today because we ran some
problems but the architecture on the
parallel aboard is going to be well with
the way the zinc chip is structured is
that the programmable logic has a direct
connection a really high speed
connection to the hard processor cores
that we have
and all the peripherals are controlled
by linux running on the arm processor
which we also use to run our langa and
the epiphany core processor is connected
or interface to the processor system via
the programmable logic which provides a
really high speed interface between the
zinc chip and the Tiffany course that we
have so interesting facts about this oh
so this is the architecture for the chip
itself it's a tile of really small lean
risk course this is the previous
generation that run what that's one
gigahertz the new ones are going to run
800 megahertz each one of them has very
small local memory so it's not for every
application unique units if your
application requires loads of local
memory to be present this you won't be
able to use it but you might be able to
split it down into chunks and distribute
the workload and they also provide a GCC
toolchain and as I mentioned opencl
support the reason we're sort of excited
about this technology is 6 this is the
64 core version of the chip and here are
some stats you get 10 gigaflops of
performance for 2 watts which is
remarkable you won't be able to achieve
this this sort of performance using gpus
or any other device really so and also
they're relatively easy to program you
can just put a see application or c
function to run on each core and then
architect your system to you know
connect to different functions within
the tip where if you're using opencl you
can just load kernels into each core and
then use it to do data parallel
computations on it and another
interesting thing is that it has the
facility or the support to link external
tips together so you can connect up to
64 64 cores on a single board
salat of course which is exciting oh so
the demo that we have been working on
actually consists of multiple devices
since this talk is about heterogeneous
computing this is the agenda system so
if there are USB webcams looking at a
little books that we've built and
they're all connected to raspberry PI's
which true network are connected to the
parallel port and the idea is that we
use the raspberry pi clients to grab
frames of the existing books that we
built and then they just essentially act
as little frame grabbers and then we
deal with processing those image frames
on the parallel itself so this is what
the stage looks like we get a shot from
each angle for instance north camera
grabs this area this triangle over here
these are obviously idolized sort of
representations there's a bit of skewed
in there and the idea is that by
processing all the four frames together
we can triangulate the position of the
ball after doing some edge detection and
circular health transforms and a few
other image enhancement stuff and thrown
in there as well so the idea is that we
have and we use Erlang for all this we
use our line for coordination and simply
grab frames using opencv you don't use
it to process stuff you just use it
because it has a nice webcam interface
and then those get transferred over to
our runtime system running on the
parallel ax which gets off loaded into
opencl and then we send the results to a
little GUI application help you wrote in
a using HTML so open still no line
Erlang is not that great for crunching
us later when its own but doesn't matter
this is where opencl fits in so in our
implementation airline provides an
environment around the opencl kernels
that we have written and it offloads the
processing to the Epiphany course in
this case we use
again erlang as the orchestrator and the
sort of controller for dealing with all
the frame data coming in and sending
results back so it's essentially acting
is glue between all the heterogeneous
knows that we have and we rely on the
monitoring and supervision facilities
and also the fact that we can do message
passing quite nice and easy using a line
parallel ax is if you've done opencl
programming before parallel is a little
different than say programming a little
g-force device how little they don't
doubt really little so the work sizes
are limited because your GPU might have
hundreds of threads or coors but your
apparel Ella is the only has 16 or 60
for core so you need to rethink the way
you split your data and also as I
mentioned earlier the local memory can
be or the lack of huge amounts of local
memory can be a problem usually you can
work around those limitations and we're
working with pre-release software so we
run into the usual sort of issues but
other teva and brown deer technologies
who implement the open sale in parallel
they're quite responsive and we've
received some really useful feedback
from them and you're developing our
application so the state of parallel on
or rather relying on parallel ax we have
packages for ubuntu arm hf that runs on
the board and we have the open sale
bindings we just need to tidy them up a
little bit the good thing is that both
the packages and the code to drive the
epiphany cores will be releasing them as
open source as an open source project in
a few weeks time if you're interested he
can I I must mention that most or bulk
of the work on this is done by my
colleague advertised it was outside just
by the parallel ax so if you're
interested in if you're interested in
talking about opencl on parallel that he
would be the person to talk to so it
very excited about this and we're hoping
that this will bring in a new bunch of
people to the airline world and we're
hoping that o-line is a Erlang will be
an accepted solution on the parallel a
platform now I'm going to jump to your
embedded side of things and talk about
my project you're lying embedded project
and a couple of you might have heard
about this already we've been blogging
about it and talking about it in a
couple of conferences and really we've
received an increasing amount of
interest in using Erlang on these small
devices and listening to the feedback
that we have received we decided to
create a generic framework around these
little embedded boards that we have like
raspberry pi but before I get to that I
want to talk I have a couple of slides
about the sort of stuff that we're
dealing with in the embedded world today
so these are challenges and
opportunities opportunities at the same
time the traditional embedded
development methodology of having a team
of you know one guy doing the software
one guy doing the hardware that's kind
of disappearing with the availability of
really cheap platforms like the
Raspberry Pi and the fact that now you
can design an entire product in your
bedroom including the mechanical
enclosure for it using your 3d printer
and finding you know people to support
you to fund you on Kickstarter so the
combination of these three factors are
is really letting people who might not
necessarily have a commercial experience
in going from idea to product and it's
letting them do some really cool stuff
so just for you know inspiration go to
Kickstarter and browse a technology
category there's some really cool stuff
happening there this is probably a bonus
point really at the fact that we have
these available but it also it also
proposes this challenge to the
traditional embedded engineers in which
they have to reduce their time to market
and they have to stay competitive so
they need to find ways to reduce the
development time and the
effort that they put into creating
software bits and pieces to drive these
things and the silicon manufactures are
trying to cram as much possibility as
they can into chips which is a good
thing I'm not complaining about it but
this is a fairly old system-on-chip
device actually it's it has to court ARM
Cortex kors nice and heterogeneous
there's a hardware accelerator there
there's a GPU there there's a dedicated
image signal processor for connecting a
camera to it and probably driving the
screen as well so timer interrupts yards
i squared see many I squirty scattered
around camera interfaces GPIO so you can
see that there's a lot of stuff
happening there and silicon by its
nature is parallel it everything happens
at the same time so you need to be able
to deal with all these things happening
and really trying to deal with these
using like bare bones methodologies like
vectored interrupts and stuff it's
really not fun and actually doesn't
really work very well so just add to the
collection of images this is actually a
block diagram of the zinc chip that's
used in the parallel aboard to top of
the arm and sort of all these different
things there's also programmable logic
in there as well so you can add all your
stuff in there your customized hardware
sort of blocks that can speed up your
applications or other things and you
know that's great but you have to manage
those as well just out of the complexity
traditionally these devices are
programmed in c and that trend is
increasing as well and it kind of makes
sense to write device drivers and see
there's nothing wrong with that but it
all it's also nice to have the features
that we have in Erlang that makes our
lives easier when you're dealing with
concurrency and failure
so and luckily our lang has lots of
features but it has the facilities to
connect these external bits of see
drivers and stuff into the runtime so
that we can you know built on top of
these and deal with the higher level
issues using our line so accessing
hardware is a key part of creating an
embedded system obviously there are two
ways you can do that you can either map
stuff directly wire dev men and then
twiddle bits registers or you can use
colonel abstractions but generally this
is very messy on top of Linux so to make
art well to make embedded engineers
lives easier or embedded designers lives
easier we are today introducing Erlang
ale and it stands for active library for
embedded and we're hoping eventually not
at this current moment in time it will
be what OTP is for telecoms hopefully
unfortunately I wasn't able to make this
public because the internet kept failing
on me so give me a few hours and this
repository will become public so in a
few always check this link you'll find
some code so what Al does is it brings
embedded peripherals or the way you
interact with embedded peripherals and
maps that into the your leg domain what
that means is for instance we take
interrupt handling and turn that into
messages arriving at the process and you
saw that you can write your interrupt
service routine zor interrupt service
handlers in a line so it provides easy
to use familiar abstractions for our
language and in this current version
we're using raspberry pi as a reference
platform but the way it's structured
that is the actual hardware bit or the
actual hardware drivers are not really
woven into the higher-level framework so
you should be able to support that so
the platforms quite easily and it's
released as an open source project as
well so it's based on a little library I
wrote called PI hwm this
is a very lightweight abstraction on top
of the kernel modules and in one case a
little memory map driver at the moment
we have gpio and gpio interrupts
supported but we have some code written
for these external or extra rather
peripherals in works as well so
commenting contributions are most
welcome and if you promise to contribute
back I will even give you a demo board
so yeah can we find me after this I'll
show you a picture of them forward in a
minute but just to give you an idea
salty how you can you sail let's say you
want to blink an LED because blinking
LED is fun um so I'm going to be known
as the blinking LED guy now um so first
of all you set a pin as an output
obviously before he can write to it so
we just we use both AP principles nice
and easy we just set a pin up as an
output and after that he writes a one to
it and then after that we write 20 nice
and easy this is something more
interesting and let's look at intro
planning so again for before we can set
a pin up as an interrupt we need to
declare it as an input pin otherwise we
won't be able to get signals into our
system so after we do that we call a set
int function and the second parameter
here is the interrupt condition in this
case we will expect to receive a message
when the state of the pin goes from low
to high ie a rising edge so and then we
just oops and then we just wait for a
message that will be sent from the sort
of lower layer ale when we receive an
interrupt we just blink LEDs so yeah
this is very simple again it's a let's
say it's an alpha release at the moment
again the offer is valid if you promise
to contribute back i will give you one
of these boards which is also open
source you can download the source files
for this honor line
calm so in order to support all these we
are also we re-engineered our packaging
procedure at all we provide packages for
raspbian on our downloads page but
really soon we will be built start we
will start building for other embedded
platforms as well so parallel is done we
just need to polish it a little bit so
where's our line you might ask I mean I
only mentioned Erlang in one bits in how
we can use it to orchestrate all these
crazy heterogeneous stuff that we're
dealing with and that's you know
essentially as the orchestrator we use
our lang not to necessarily process data
but to coordinate data coming into the
system and offloaded to relevant
processes and then later you know deal
with the higher-level decision-making
mechanisms so early in our system is the
orchestrator it sits on the control
plane and it deals with uploading things
into relevant processes and I had cloud
in my talk title I don't know why but
you know this slide is nesting off to
explain that we're doing okay on the
cloud front this was tweeted just today
this morning handling 27 billion
messages in a day is pretty awesome so
on that note thank you very much and if
you have any questions I'll do my best
friends Adam</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>