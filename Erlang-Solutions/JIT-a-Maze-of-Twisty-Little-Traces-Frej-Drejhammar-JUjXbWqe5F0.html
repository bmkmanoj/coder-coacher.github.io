<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>JIT, a Maze of Twisty Little Traces - Frej Drejhammar | Coder Coacher - Coaching Coders</title><meta content="JIT, a Maze of Twisty Little Traces - Frej Drejhammar - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>JIT, a Maze of Twisty Little Traces - Frej Drejhammar</b></h2><h5 class="post__date">2013-07-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/JUjXbWqe5F0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">the original title of this talk was your
in the maze of twisty twisty little
traces all alike and I intended to draw
a parallel between software development
and an old-school adventure game in that
you're not really sure where you are
neither are you sure where you are going
but the organizer change the title so
that's not perhaps that clear anymore
but we go with this title for today but
before I start those of you who doesn't
know me I'm a researcher at the Swedish
Institute of computer science where I
usually work with programming tools
programming languages and distributed
systems but enough about me this talk is
a brief introduction to the beam
just-in-time compiler which I've been
developing for this past year and a half
I think and then I talked about it last
year so this is more or less an update
of what I've been doing since then this
talk is structured as follows I start
with a short introduction to
just-in-time compilation the beam
virtual machine Eric stenmun gave a good
overview already so i can probably
shorten that part i'll tell you about my
goals with this jit compiler before I
describe the tools that have been using
and when that it's done i'll go into
details about how jetting can be done
for beam and then i'll walk through the
evolution of the beam jet before i wrap
up with some future work and questions
at the end just-in-time compilation is
an optimization technique that is used
to improve the performance of dream
based languages it's not nothing used
it's been
fairly commonly used it exists jits
compiler for many cool languages such as
Python small talk Java JavaScript I so
it's about time that aaralyn gets 1 i've
previously mentioned beam and Eric
talked about it before the break beam is
the name of the hour-long lecture
machine it's a register machine it has
approximately 150 instructions which
during loading are transformed whether
people optimizer to around 450 macro
instructions and the beam is written in
C it's mostly handwritten see some of it
is generated and unfortunately there is
no authority of description of the
semantics of these instructions we have
only the C code but when I was asked to
build this JIT compiler I set up a
number goals for for it the goals are
that I want to do as little manual work
as possible because I'm a computer
scientist and manual work is boring you
want to automate it I wanted to preserve
the semantics of plane beam so I didn't
want a user to see any difference
between running the jetting version than
compared to the traditional version i
also wanted a jit to stay in sync with
the traditional implementation so if the
OT people people find a biograd an
instruction that should automatically be
possible to get it I shouldn't have to
keep playing keep up with them and I
also want the native code generated at
this state of the art and the key thing
to try to pull this off was that I
wanted to
parce and thus track extract the
semantics of the beam operations from
the c code that implements them that way
I could skirt around this problem we're
not having a specification and then use
the information i extracted from the
implementation to produce the input i
need for my jet but before i go into how
this can be achieved i would like to
motivate them why because we have at
least eric and its cost is here no it's
not there but there ah there you are we
have the hype the hype people here so
why would you want a jit when you have
high well there are a number of reasons
hi a jet will automatically give us a
cross module optimization we have a
potential of saving memory as native
code tends to be much larger than the
corresponding beam code and also if
you're doing tracing as part of
debugging you will if you're using high
pep to switch to completely interpreting
your code or at least modules and if
you're having a jit you could instead
just let the GTG the tracing version
version of your code and also a big
thing is that modules stay independent
from the target architecture so there is
no need for cross compilation or and
there is no need to actually couple the
release binaries the release modules to
a particular version of or particular
field of your emulate with that out of
the way I tell you about the tools that
I'm using to actually pull this off
the main main tool I use is llvm llvm is
a compiler infrastructure which contains
a collection of Andrew of modular and
reusable compiler and two chain
technologies it uses a low-level
assembly like language called llvm ir it
has backends for a lot of CPUs lot of
optimization of the my stations and two
as a complement to clang until 2l of em
there is front-end called clang which
parses c-type languages so C C++
Objective C so on and it in turns page
more or less as GCC but as output it
produces lod MRI are and that compiler
is also packaged in the form of a
library so using library which is called
lib clang you can parse the sea module
and get hold on the abstract syntax tree
which is exactly what you want if you
want to extract the semantics of code
implementing the beam instructions so
that was the background get let's get
back to the to the problem the JIT well
the initial approach we have chosen is
that we want to make a tracing jet and
the tracing it works by first having a
profiling stage where we find out which
parts of the code that are frequently
executed and when those parts have been
identified we start tracing the code and
we trace the execution flow until we get
back to the place we started a trace
from because then we have found a loop
and we assume that loops are probably
where we spend most of our time and when
we have this trace and we are found the
loop we generate
native code and then we run it instead
of running it inside the interpreter and
compared to some other tracing Jets we
have decided to do this on the level of
the emulator so we are not following
single instructions we are following
parts of the instructions so we're doing
some kind of sisk to risk translation on
the fly when we do this so in order to
do this what do we need to have support
for in our virtual machine well we need
three basic execution modes one is
profiling figuring out what's hot the
next one is tracing figuring out which
exactly is this hot code path we're
following and then we have to be able to
run things in native code and to do that
we have to support an efficient way of
switching modes that means turning from
the profiling mode into tracing mode and
back again and then also passing state
from the interpreting world to the
native world where we were running
native code and then also we have a big
part which is native code generation we
have to actually from our trace previews
native code and for that we need the
semantics for each instruction and I've
previously hinted about that we use
sleep clang to parse and interpret the
airline beam implementation and we do
some transformation on the code of the
main interpreter loop we flatten out all
variable scopes we remove loops and
replace them with explicit go tues and
ifs we may call for thrus explicit with
her and more or less return
nicely or at least roughly nicely
structured c code into a spaghetti of a
lot of go to San single basic blocks so
what we're more or less creating is the
control flow graph and then when we have
the control froblog fro flow graph we do
like nestle addresses on all the
variables in it and the illustration
here is a part of the control flow graph
of the airline interpreter it's probably
a tenth of it I didn't couldn't fit
anything more on the side with that it's
time to look how the prototype has more
evolved during the last year we have
passed through at least four different
distinct versions of the digit system
mark one was the 1i talked about here
last year and now i'm up to mark for
which the one I juicing right now mark
one had everything needed to implement
Britain in charge it it adds support for
profiling meaning that and that was
implemented by synced simple counter so
we had the compiler insert profile
instruction that places in the code
where which could be the head of loop
and then when we passed through these
instructions we just had a counter and
we incremented each time we post by it
and when the count was high enough we
said that they it's time to start
tracing and that has the drawback that
eventually will compile everything but
for toy examples hello world looks and
things like that it searches purpose
and the only thing you actually had to
add to your implementation here is the
implementation of that profiling
instruction I said that when we found
something that was frequently executed
we turned on tracing and to do that we
actually had to recreate we had to
record all the basic blocks we passed
through and we do that by switching to a
new version of the interpreter which we
generate from our CFG so that works so
in the way that for each bass boat
passed through we put that we record the
identity of the basic block and the
program counter value at that point in
in a lot and if that log becomes too
long we abort to trace and get up but if
we are lucky and get back to the profile
instruction we started trace from we
have a found a loop and then it's time
to generate native code but this
switching between emulators from the non
tracing to the tracing version then we
use a trick so err Eric before the break
talked about that said that aaralyn uses
something called direct threading to
optimize instruction dispatching so here
on the left you see you have a direct
threading example where the problem
group program counter points into code
memory and each rope code is represented
by the address of the code that actually
implements so that particular
instruction but if we want to be able to
switch between the modes we have to add
some in direction and that's what we do
here we convert
the airline beam to use something called
indirect threading instead so now what
the power program can program counter
points out is actually just indexes and
those indexes are in today indexes into
a table and that table in turn contains
the address of the implementation of the
old code so if we want to switch from
one implementation of love go to another
we just have to switch these tables
unfortunately that cost us some
performance so this graph here shows a
benchmarking sweet it's the benchmarking
sweet Costas and his team uses when they
want to show off beam so but here in
this case the the height of the bars are
normalized runtime so I normalize the
runtime of each of these benchmarks
tests in a way so I can compare them to
down modified system and it's done in
such a way so that if if my modified
system is takes twice as long to run as
the unmodified system you get a runtime
value of one if it's just as fast takes
the same time you get to run time where
value of zero if my modified system is
say twice as fast it runs in half the
time then I get a benchmark value of
minus 0.5 so positives bad negative is
good so you can see here at almost all
cases except to it costs me performance
to do this switch to indirect threading
so that's something we'll have to live
with further on and I have to try to
make up in some way when we run native
code and there are bars you see are one
standard deviation so you can see how
noisy DS well
sir and if you can see the legend up to
the right the I plot them so two left in
the blue blue bar all of them are zero
because that's what I normalize it
against but you can at least see how how
the unmodified system how noisy it is
because you can see the error bar so
that's the cost for indirect threading
now to native code generation we
generate our native code but just gluing
together I are fragments that correspond
to the basic blocks we have traced and
when we do that we have to insert guards
to make sure that we actually follow
this trace that we are traced because if
we execute code that will lead to
another they did the different execution
trace we have to detect that and when we
have done this we hand off our IR trace
to llvm for optimization and native code
generation and these fragments of IR we
glued together those we extract from our
CFG and we do it by chopping up to CFG
each basic block into c function which
we compile and then load at runtime so
we have templates which we can quickly
live together so the figure illustrates
that process where everything that's
blue is done ahead of time when we build
a system and did what is it it's pink
thing part to write this down at runtime
now I only have one one of these modes
which is left to describe namely how do
you call native code in the mark 1
version of the JIT
well when we switch from interpreting
mode to native mode we pack up all the
variables that are live into structure
and headed over to the native code which
in turn unpacks it and flexing the value
in its temporary and we do more or less
the same when we switch back and when we
switch back we jump into the emulator at
the particular basic block where we
should continue this works it actually
works it's correct but the performance
is pretty pretty bad and the reason for
that and I should say that pretty pretty
bad means 66 to sign of seven times
slower and the main reason for that is
that if we have piece of code looking
like this in our emulator in the
emulator in the unmodified system the
compiler would generate assembler code
corresponding to this CFG you see it in
some way sets allocates a register for X
sets is in some way it branches on it
and then in one branch and increments
its and the other one it decrements it
but now when we support that way action
can restart instructions halfway through
an instruction then it's forced to do
something like the 12 part here on the
right where after each time it sets a
register or variable it has to actually
store it back into memory so in this way
you even if your execution this
executing this fast path inside the
interpreter you have to pay for the cost
for loading each value before you use it
and storing it back into memory when
you're done Paris this mark version mark
one word version
us with some minor tweaks it was
actually the first version that could
compile OTP and pass the test suite
without crashing and in order to
actually pass the test suite in a
reasonable amount of time I had to do
some small tweaks one of them was to
make the profile eat I'm aware so
instead of just measuring execution
count I start measuring execution
frequency and also I introduced
blacklisting so places or locations in
the code we're all almost all the time
started tracing our produced code that
always almost failed then I blacklisted
those to not just repeat the lead spend
time trying to trace something that
didn't pay off also I added some things
like garbage collection of traces so you
could actually push code low new code
things like that it didn't impact
performance in any way but it was time
to do something to get down on this
overhead for loading and storing at
least so to do that I figured that the
unmodified system is pretty efficient
yarn has made did good work optimizing
it so I tried to modify it and changes
at least as little as possible and then
instead have a completely separate trace
interpreter which were used for tracing
and then also as there is a big problem
of jumping back halfway through and the
implementation of an instruction I said
that hey we can only enter the
interpreter at instruction boundaries so
then I needed a further version of the
interpreter which are used to just call
execute until
the next instruction binary and to do
this I we used some implementation
tricks I used lightness information from
the control flow graph and happily it
turns out that if you're looking at the
world set of arrivals that are live on
entry to to each at instruction
boundaries there are only eight distinct
sets so it's actually possible to
efficiently choose which set of live
arrivals to restore and also when we are
calling native code package that native
code as just an ordinary function with a
set of arguments and the arguments are
the live arrivals which are live at on
entry to the trace so you can just call
it using an ordinary function pointer
also during cleanup this cleanup
interpreter which we use to execute
until the next instruction boundary they
can be easily formulated as just
functions which take the lie variables
as arguments and then Taylor cursive Lee
calls the next basic blocking sequence
and then at the end pack still packs up
to the live variables and processes back
to the interpreter this self performance
remember I said that the last time it
took six to seven times it was six to
seven times slower now if we look at the
graph again you see the word worst case
is 0.75 or so so instead of six two
times six to seven times slower we are
just twice as slow so
at least that was an improvement
improvement also it turns out that
changing things in the basic interpreter
loop have big impacts on performance so
probably there is a large potential of
optimizing this further by placing code
in the right places and in the right
sequence also while you're running mark
two I notice the property that attempted
to fix in mark 3 it appears that we
quite often compiler trace which is not
representative meaning that when we run
try to run it natively we almost always
fall off that race during the first
iteration and to try to avoid that I
added a new mechanism which are called
trace along that means that the first
time I've found the loop I don't compile
it directly instead I have it all right
let it hang around for a while and the
next time I come here I trace it again
and then I trace it and see that I
actually followed it trace the previous
trace and if I manage to do that this
multiple times then I can be pretty sure
that what i have is represented in trace
and then i go on to compiling
unfortunately that in turn leads to at
instead of blacklisting things after we
have traced we have traced and compile
them we now blacklist things already
well tracing so we find a lot of places
where we never find the representative
trace and the solution to that is more
or less to allow more than one path
inside the trace
and then instead of just generating
native code when will it traced certain
number of times we instead let the trace
grow and when we've traced from the same
place multiple times without the trace
growing then we know that well now
probably have seen everything we will
see so now it's time to compile it
unfortunately then we get another
performance it namely that it appears
that a lot of la VMS optimization and
also native code generation is pretty
sensitive to the number of basic blocks
will give it to the size of the control
flow graph so to try to solve that I
introduce something I call trace
compression so if these problematic
traces if you look at them you find that
there are things that looks like loops
inside a more or branches where which
separates the trace in two paths which
almost which share the same prefix does
something else and then have the same
tale parts like the one here on the left
so the boxes or basic blocks so you can
see there this one has the same basic
block a top here are two different basic
blocks but then here the lower part
details they are the same sequence of
basic block at the same instruction
pointer instruction counter values so
the solution is then to try to compress
this trace into something looking like
the one on the right so you actually
make an explicit branch to the two parts
and then much it back together and then
so that we can at least reduce the
number of basic blocks we hand off to la
am still the compilation / had pretty
much dwarfs everything else
so i'm also i should say that i'm
running llvm on an optimization level
that corresponds to minus 0 2 if you're
running GCC you can see here there are
bit of noise down here the green bars
are the the normalized runtime for
running the jet with the jet active from
a previously cold code and you can see
there is one here something like four
hundred seventy times that's not very
good but if we zoom in a not everything
is lost we are actually faster than the
unmodified system in some places and I
should probably tell you what these
different bars means so you have the the
red bars that is the system running
interpreting digit is completely disable
the green one is running with the JIT
enabled and the gray ones are running
our are just running the benchmark when
everything that is going to be jit
compiled has been JIT compiler so there
are you can see that there are some
cases here at least we're even running
when the JIT enabled we are actually
faster than the unmodified system that's
good if you instead look at the
performance of the just and the
performance on the generator native code
like in this code this slide
you see that it actually looks pretty
good at least on that part so there are
some cases where we have problems these
this large group here on the left where
we are where we make repeated calls just
function calls we can if we study that
we feel we see that these guards we
insert in the native code they are
pretty costly and that's where most of
our performance goes here and also these
two peaks over here where we have these
are the ring benchmarks where the code
we're tracing or the code where cues
execution executing behaves in the way
that it's scheduled out it gets a
message get ready lien does something
and then it's fairly loud again it's a
it's in some way it's a loop but it
passes through the scheduler and as we
previously only been interested in loops
we have said that everything that passes
through the scheduler we can't trace it
these problems quite nicely show up
should show our show what we have to do
next where my next year's work is
located well the first one I see that we
should not fixate on finding loops we
must be able to handle these these
things that are typically message
handlers we're shedule out something
happens we'll get the message and then
we have to do something fast and then
send out for response and then get
really loud again also in order to limit
the cost of the guards we have to
implement some kind of airline where
constraint a constant propagation so we
could should be able to element
eliminate many of them because many of
these checks shake that this thing we're
calling now that function that is
actually that function and we do that by
checking the value in memory where the
instruction pointer points to and that
is some something that's actually
compile times constant for in our case
although it's located in rap because
it's code and code is read only mostly
also will eliminate much of these of the
overhead we have when we when we unpack
intermediates inside code that should be
also be able to improve our situation
the next big thing we have to do is to
increase performance in plane
interpreting mode you saw that we had an
overhead pull to almost twice as low
that's not good and you saw we had this
enormous overheads completion and a way
of masking that at least is to move
compilation to it's a separate thread
we can't have a soft real-time system
which goes up to goes off to lunch
because it's compiling native code also
but that's a long time goal is to
actually extend this tracing mechanism
to extend outside the main interpret the
loop which we we have limited ourselves
to right now because then we could
perhaps do optimizations inside its or
other frequently called things well
before we go we go to questions I like
to thank Erickson for paying me to do
these fun things and also particular
Kenneth Linden is here here here no then
you're his colleagues can tell him that
I really appreciate that he understands
the process of research and is not
pushing for immediate and fantastic
results and and that he lets lets these
things things take time well thank you
questions
so what you do with topcoat loading hot
code loading well you know the code and
run it's nothing it's nothing special
unless you mean perch because then you
actually have to track the traces your
for a trace you have to track the
modules it touches so if you purchase
all you'll you put you remove those
traces that involves that module so then
you of course you take a hit in
performance until you have profiled in
jetted dead parts again these parts
again have you compared those benchmarks
to hype yeah but I haven't showed them
because these have benchmarks are
clearly chosen food to show up type so
I'm not in their league yet but
hopefully one day I'll beat
seems that there's no more questions so
thank you bye</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>