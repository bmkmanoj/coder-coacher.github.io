<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Keynote: Propositions as Types - Philip Wadler | Coder Coacher - Coaching Coders</title><meta content="Keynote: Propositions as Types - Philip Wadler - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Keynote: Propositions as Types - Philip Wadler</b></h2><h5 class="post__date">2015-11-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/OGF-TGd-CIo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you to model for that introduction
so thank you for citing me in the 1500s
thank you to the organizers for inviting
me and thank you to all of you for being
here so good evening are you all ready
to learn about the hilarious subject of
computability theory all right well
let's get started an algorithm is a
sequence of instructions followed by a
computer now you and I are used to
thinking of a computer as a machine but
originally a computer was a person the
person who executed the algorithm
algorithms go back to Euclid's elements
in classical Greece and eponymously to
al-khwarizmi in 9th century Persia but
it wasn't until the 20th century that we
have a formal definition of algorithm
when Alonzo Church Kurt girdle and Alan
Turing all made proposals within a year
of each other it's like buses
you wait two thousand years for a theory
of computability and then three come
along at once why did this happen so as
the 20th century dawned David Hilbert in
good engine was one of the foremost
proponents of formal logic and he had a
program not a computer program but a
program of work and his program was to
put all mathematicians out of business
because what he wanted was an algorithm
that given a statement in formal logic
could decide whether that statement was
true or false so he wrote about it in
this book and it was called the and shy
Dukes problem because it sounds better
in German a key assumption that Hilbert
made was that logic was complete that is
that if you had a proof of something if
you've proved a statement it was true
and if a statement was true then you
could prove it now he first published
the inciting problem I think in this
textbook in 1928 no it goes older but
this is the textbook 1928 but in 1930
this man Kurt girdle published his proof
that any logic powerful enough to
represent arithmetic is in complete and
that meant that Hilbert was forget my
use of a technical phrase here screwed
so this is one of the earliest
functional programs it represents
statements of logic and proofs of
statement and logics as numbers using a
clever technique called girdle numbering
it actually is a functional program that
checks whether a particular number
represents a proof of a particular
statement and by using all this
machinery he managed to write down a
number that corresponds to this
statement is not proof
mmm this statement is not provable so if
it's false that means it's provable and
you proved something which is false
which means your logic is unsound and
that is a complete disaster it means
logic becomes useless if it's true then
there must be a true statement that is
not provable and your logic is
incomplete which is not as bad as being
unsound but is still incredibly annoying
especially if you are Hilbert so as soon
as this proof came out people began to
think that the introduced problem was
undecidable now as they thought it was
decided then you wouldn't really need a
formal definition of algorithm you would
just write the algorithm down and it
would be like Justice Stewart's
definition of pornography I know it when
I see it right you could just recognize
an algorithm by looking at it but if you
wanted to prove there was no algorithm
for doing something then you needed a
formal definition so the race was on in
Princeton this man Alonzo Church wrote
down the first definition lambda
calculus in 1932 and in 1936 he put
forward what's called church's thesis
which says that anything that a computer
can do you can write down in lambda
calculus and he used it to show that
indeed the entry donks problem was
undecidable so here is lambda calculus
it's the world's smallest programming
language this is the complete thing it
just has three constructs and it's the
world's coolest programming language
because it was designed a decade before
the first computer was built for years
me and my friends have my colleagues
and friends have worked on functional
programming languages which have lambda
calculus at their core and for years
industry has pretty much ignored
everything we've done except now all of
a sudden lambda calculus has become
trendy and you find it in C++ and Python
and drive and Java here is Duke the icon
for Java looking very smug because Java
along with Python and C++ now has lambda
constructs well congratulations Duke you
finally caught up with where Church was
in the 1930s yeah you guys can applaud
that
so Kurt girdle came to visit church at
Princeton and he thought that churches
solution was thorough his precise words
were thoroughly unsatisfactory so Church
challenged girdle he said well you come
up with your own definition and I'll
show that mine is as good as yours and
girdle did this he came up with a second
definition of algorithm what we call now
recursive functions and this was written
up with attribution by churches student
steven cleaning and girdle and clean he
went off sorry church and cleaning went
off and they proved indeed that the two
definitions were equivalent and church
went back to girdle and it's not
recorded but basically he said so there
and girdle said oh your definition is
the same as my definition hmm my
definition must be wrong then
the impasse was resolved by this man
Alan Turing who was basically an
undergraduate at Cambridge and he came
up with what we now call Turing machines
and the big difference in what Turing
did from what church did or what girdle
did is philosophy Turing basically
engaged in a bit philosophy saying well
what could we expect a computer to do
we're here again by computer he met a
person the person executing the
algorithm and he showed that a Turing
machine would have all the capabilities
of a computer and Turing showed that his
result was equivalent to churches and
hence to girdles and this finally
convinced Turing's philosophy basically
finally convinced girdle that the three
of them were correct people often argue
about whether mathematics is invented or
discovered but when you have three
independent definitions that turn out to
be the same that is powerful evidence
that you have not invented something you
have discovered a fundamental truth it's
not just sports fans that are impressed
by a hat-trick Kurt girdle was 28 when
he undermined the life's work of David
Hilbert who was 68 Alan Turing was 23
when he resolved the argument between
Alonzo Church who is 33 and Kurt girdle
who was then in ancient 30 so to all you
young people in the audience you know
what to do
please keep teaching your elders when
we've got it wrong okay that's
computability theory that's just the
prelude now let's talk about
propositions as types so this man got
hugginson again at a very young age he
wrote his doctoral thesis and in it he
introduced not one but v two forms of
logic that we use to this day the two
most popular ones natural deduction
which is what's written up on this slide
and sequined calculus which will skip
over by large he also introduced the use
of upside down a to mean for all so
again for all you young people there's
another goal for what you can get done
in your PhD thesis so I'm going to take
just two of these bits so here's the
definition of implication and at the
upper left corner there you have the
definition of conjunction ampersand and
here they are written out in modern
notation you'll notice that modern
notation looks exactly like Jensen's
notation the only difference is that he
writes his letters in German and I've
written mine in English so let's look at
what this means how many of you are
familiar with basic natural deduction
okay about half of you so we'll do a
quick refresher course in how natural
deduction works so this says so this
formula here is read a implies B if a is
true then B also must be true and that
backwards looking see is a backwards
looking see it stands for consequence be
is a consequence of a and how would we
know that a implies B is true well
assume a is true and if assuming a is
true you can prove be then that means
that a implies B so this is called an
introduction rule this is how we
but a proof that a implies B how do we
use or deconstruct approve the a implies
B well if we know a implies B and we if
we have a proof that a implies B and we
have a proof that a is true then we have
proved that B is true because if you
know a and you know a implies B then you
may conclude be the rules come in pairs
they labeled with an i and an E I stands
for introduction and it means it's a way
of constructing the given kind of
formula e stands for elimination that
means it's a way of deconstructing or
using that kind of formula and this was
Jensen's key insight is that the proof
rules come in pairs they had formal
logic before but Jensen was the first
one to formulate things in pairs so
let's see how that works for a different
connective conjunction so if you can
prove a and you can prove be then you
know or you can conclude that a and B is
true you've proved angry so both a and B
hold what do you know if you know and
behold well one of the things you know
is a must hold of course the other thing
you know is that B must hold so again
these follow the pattern that the
introduction rule constructs the
relevant construct in this case a
conjunction and that the elimination
rules get rid of so given a and B we get
rid of it to get a or get rid of it to
get be by the way if their questions
this is technical material people should
ask are there any questions at this
point you shouldn't be afraid to ask
questions cuz I'm sure other people will
have the same question I will press on
but do stop me with a question if you
have one oh I know how to check not if
you followed what I just said okay
enough people are nodding tony hoare
isn't but
so here's an example of a proof I want
to prove is that if I know bna it
implies a and B so to use a technical
phrase again this is what we call bloody
obvious but if it's bloody obvious then
there ought to be a proof of it right if
we couldn't prove something as obvious
as this there would be something wrong
with our notion of proof so let's check
that we can prove this so remember the
way you prove an implication is by
assuming the hypothesis so let's assume
B and a is true well what do we know
from that well we can conclude that a is
true and from that we could also
conclude that B is true and oh I've got
proofs of a and B so I've got to proof
the a and B is true and now I can
discharge my hypothesis and so without
assuming anything at all I know B and a
implies a and B so there's a simple
proof for you now the reason that Jensen
wanted to put the proof rules in pairs
is so that he could simplify proofs he
wanted to get rid of round about proofs
in particular he wanted to show
something called the sub formula
property that says that if you have a
proof then the only formulas you need in
it are your hypotheses and your
conclusion and any sub formulas of those
the reason that he wanted this is one of
the things that follow from this
immediately is say you have a proof of
false with no assumptions then a proof
of false with no assumptions would have
to consist a false and all of its sub
formulas so the sub formulas of a
implies B are a implies B and a and B
and all of their sub formulas in other
words the parts of the formula and the
same for a and B so what are all the sub
formulas of false it has no parts there
aren't any so there are no parts to
false trying to construct a proof of it
you would just look and say okay so
there has to be a proof that only uses
false and the parts of it
nope there are no rules that look like
that there's no rule that has false on
the bottom and nothing on the top it's a
lot like what part of no don't you
understand so that gave him a very
simple proof of consistency as well as
this much stronger idea that you didn't
need any roundabout proof so how did he
get rid of roundabout proofs well the
idea is if you haven't this is why the
rules come in pairs if you have an
introduction followed by an elimination
you can get rid of it right so this says
how do we know a implies B is true well
you assuming a we proved be but then if
we have a proof of a we can conclude be
well yes because instead of assuming a
and proving be I've got a proof of a
right here so I just used that proof of
a and then from that prove be now note
so this is a simpler proof in that it
doesn't introduce a implies B it might
be a much larger proof because what the
dot dot dots don't show you is how many
times this assumption is used remember
here we used it twice so you might copy
this proof many times you get a bigger
proof but it will be simpler similarly
if from a proof of a and a proof of B we
can then prove and B and from that we
conclude a well there's a much simpler
way of doing that right we proved a
directly just use that proof so you can
simplify proofs and that guarantees the
sub formula property and that guarantees
things like consistency but let's see an
example so here's the proof we had
before that be and a implies a and B and
let's say somehow that I proved be and
proved a so I know bna so now by the
elimination rule I can conclude a and B
but as you might guess this is a more
complicated proof thing you need and
indeed if we look we see oh there's an
implication followed by an elimination
so we can simplify that the way we
simplified is we just take this proof
and copy it the two places where the
assumption occurred so if we do that we
get this right we've just copied that to
those two places
and now we have this and notice that
we've copied this introduction on top of
this elimination so you have an
introduction followed by an elimination
and we can simplify that so here we've
got proof of be proof of a B and a
conclude a just use the proof of a
similarly here we can just use the proof
of B and there is our simpler proof so
you can see this one involve things like
be a implies a and B which are not sub
formulas of a and B but this one just
involves a and B and two sub formulas
and be so we've established that the sub
formula property is true congratulations
you've just all proved something deeply
profound the trick by the way is knowing
that this process always terminates
that's the hard part of the proof and
I'll skip over that but you can show
that this process always terminates as
long as your formulas are finite ok so
that was gentan in 1935 now remember
church had his lambda calculus and he
originally introduced lambda calculus as
a kind of macro language for those of
you that remember what macros are for
riding down formulas and logic but when
he did it he realized it was sort of
powerful and he said there may indeed be
uses of the system other than its
application as a logic which turned out
to be profound because it turned out to
be a model of algorithms as we saw but
also it turned out that using it as a
logic it was too powerful because using
lambda terms you could in essence right
down formulas that were infinitely long
and then the sub formula property
doesn't apply and it turns out that your
logic is inconsistent by using an
infinitely long formulate you can write
down a proof of false so for a logic
this is bad news so lambda calculus was
a huge success right it was a model of
computation except just as now people
sort of look askance at functional
programming right you
saw girdles response everybody's
response was like that they didn't like
lambda calculus like Oh recursive
function says they're good during
machines we like those lambda calculus
was sort of pushed to one side but
church still wanted it as a model of
logic and so he he used Russell's
solution for getting rid of paradoxes
and added what are called types so now
the things in red are formulas of lambda
calculus and I've extended lambda
calculus remember I said just three
things so this one has just five things
so in addition to lambda expressions we
can also build pairs and extract the
first or second component from a pair
and let's look at the typing of that so
if M is the term of type a and n is a
term of Tybee then the MN pair is a pair
whose first component has type a and
whose second component has type D and if
L is an a/b pair we can extract its
first component which of course will
have type A or its second component
which will have type B if X is a
variable of type a and using X I can
build up a term and of type B in which X
appears free then i can write lambda x
dot n and I've now bound my free
variable and lambda X dot n stands for a
function from A to B and of course if L
is a function from A to B and M is a
term of taipei then l applied to m the
result is of type be the result of
applying the function clear enough any
questions so here's a simple example of
a program to give you an idea of how
this works so lambda zed of the pair
second of zed comma first of that is a
function and if Z is a B and a pair it
returns and a and B pair and all it does
is it swaps the two elements of the pair
so it builds a new pair whose first
component is the second
Zen and his second component is the
first component set so it just swaps the
two parts of FZ and you can sort of see
that from the type right takes a beat on
a B and A two and A and B and here is
the proof that this function lambda said
second of said first of that is well
typed so assume set has type DNA then
second of Zen has type a and first as
that has type B and then the second said
first of said pair has type a and B and
then lambda said second of said first of
that is a function that given a DNA
returns an a and B and you can evaluate
programs so how do we evaluate lambda X
n applied to em well it's a function and
this says if M were the value of x
return ends we just do that everywhere
the X appears there we replace it by M
and we compute n with every instance
with em replacing every instance of X so
we just do a substitution to do
computation and I'll show you an example
of that in a minute and then if M is a
term of type a and n is the term of type
B I can form the pair MN of type a and B
and if I take the first of MN what is
that it of course is just M and
similarly second would be just n but
I've not written that down so as an
example sorry here's our program again
well and right so this is steps
evaluating a program and in fact if it's
well typed we are guaranteed that these
steps will always terminate that unlike
for algorithms in general the halting
problem is solvable for typed programs
in fact the halting problem is trivially
solve all typed programs halt so this is
fantastic news all types of programs
halt it's also worrying news it says
okay that means typed programs cannot do
everything that a Turing machine can do
because they always halt my friend my
colleague Connor McBride hates it when I
say this he says no no no you can also
do a type you just keep doing it until
you're tired and then you can compute
anything that you can view otherwise
just set the number of steps that you're
willing to do in advance and then you
can do it in a type of way but
nonetheless I think there's fundamental
trade-offs which we as engineers
appreciate untyped lambda calculus you
can compute any computable function
typed lambda calculus very powerful but
cannot compute every computable function
it's guaranteed to terminate and the
first proof of that in fact was written
out by Turing so let's look at
evaluating a program so on the left
there is the program we had before and
on the right I've just built a YX pair
and then I take my swap function and I
apply it to a YX pair so what do I do
well i substitute YX pairs for z so the
next thing is just i've substituted YX
for each occurrence of Z in the thing
above and notice so the first time
notice that I've got a function
introduction followed by a function
elimination now I've got pair
introduction followed by pair
elimination and so I just need to
compute what is the second component of
YX and what is the first component of YX
and they are of course just x and y so
now we have evaluated our program ok at
this point I'd like you to stop and
reach under your seats you should find a
pair of rose-colored glasses there so
please put on the rose-colored glasses
you will then not see red you will only
be able to see blue and when you do this
you will all go aha because you will
have noticed that the first set of
slides I presented for logic and the
proof of the sub formula property and
the second set that I presented for
lambda calculus and how you go about
evaluating programs are identical
you may all now say so this is a very
deep idea so this is very shallow
expression of that very deep idea this
is a nice picture due to Luca cardelli
which he called the curry Howard
homeomorphism but if you think about the
idea the deeper idea and logic it's
often called the curry Howard
isomorphism and this is because a form
of it was written down by Haskell curry
and then something very much like what I
just showed you was written down a bit
later by William Howard in this paper so
he first circulated as Xerox notes in
1969 and then in nineteen eighty in the
curry festschrift he published this
paper laying out very neatly in much the
same way I've shown you the
correspondence between natural deduction
and lambda calculus so this is sometimes
called the curry Howard isomorphism or
the curry Howard correspondence and
other people call it propositions as
types other people notice that many
other people like brewer it's sometimes
called bhk because brewer Harting &amp;amp; Co
magar off the intuitionist had very
similar ideas to this already back in
the 1920s deploying in the early 1970s
started implementing computer programs
based on these ideas for doing proof
checking so many people contributed to
these ideas so although many people call
it the curry Howard correspondence I now
think the least controversial name is
propositions as types and this indeed is
the name that Howard uses and you can
see that in this we have not just
propositions as types but also proofs
correspond it to programs and
normalization of proofs corresponding to
evaluation of
grams so you get all three steps so it's
not just a shallow idea there's a lot of
structure that is preserved by this
correspondence between logic on the one
hand and typed lambda calculus on the
other so you now are probably all
thinking the same thing I thought when I
first saw it which is just oh that's
kind of cute that's just sort of an
accident that those two things look a
lot like each other but it doesn't just
happen once it happens again and again
and again there are many different
formulations of logic and many different
kinds of logic and pretty much every one
of them that you can name corresponds to
excuse me every one of them that you can
name corresponds to an interesting form
of computation so the original instance
of this is Jensen's natural deduction
and churches typed lambda calculus but
the logician Roger hindley in 1969 came
up with type schemes for lambda calculus
and a little bit later the computer
scientist Robin Milner came up with the
type system used in ml which has been
inherited by pretty much every other
functional language that's typed
including Haskell and nowadays we call
it the Hindley Milner type system but
there were two independent inventions of
the same idea once by a logician and
once by a computer scientist similarly
the generalization of the idea of type
schemes is type abstraction and this was
formulated as something called system f
by the logician job and as polymorphic
lambda calculus by the computer
scientist john reynolds again the
logician gets there a bit before the
computer scientists
the same thing happens modal logic there
are many many different kinds of modal
logic and pretty much every one of them
corresponds to an interesting
computational idea and one in particular
corresponds to monads which can be used
to model things like state and exception
in functional languages and it was very
nice of bodl to say that I invented
monads but that actually goes to live
nets and the use in category theory
which we inherited in functional
programming is due to your Jenny o majhi
girdle did this amazing embedding of
classical logic in intuitionistic logic
which really pissed off the intuitionist
by the way because they were like yo
intuitionistic logic is is special you
should use intuitionistic logic not
classical logic classical logic does too
much and then he showed no everything
classical logic does you can do inside
intuitionistic logic in particularly he
showed that the formula a is true
classically if the formula not not a is
true intuitionistic allah and then one
of girdles proofs corresponds exactly to
what we now call continuation passing
style which is a standard technique in
functional programming and then in 1987
Johnny Trott again came up with
something called linear logic and only
recently have carrison pfennig pointed
out that so kohei honda did something
called session types so these are types
for writing down protocols that are used
for communication among processes and he
was in fact inspired by linear logic two
of his connectives come directly from
linear logic but the other two are just
send and receive messages which clearly
had nothing to do with linear logic they
were sort of just hacked in and then in
2010 Louis Charis and Frank Fenig said
oh actually those other two that look
like they're hacked in they just
correspond to tensor and par the other
two connectives of linear logic which
was amazing and I'm really excited by
this because obviously concurrent and
distributed programming is the most
important thing we have to do and there
are many solutions of course the problem
is
there are many solutions which is the
right one so if there's one that
actually corresponds to a logic that
might be a hint that that's an
interesting place to look pretty much
every functional language has lambda
calculus at its core so anybody so
understand right I've talked about the
discovery of the formalization of
algorithm right three independent
discoveries now we have logic and typed
lambda calculus and again they
correspond so not three independent
discoveries but two independent
discoveries and a very deep
correspondence as we saw propositions as
types proofs as programs normalization
of proofs as evaluation of programs so
deep correspondence so again I want to
make the claim that lambda calculus is
discovered now all functional languages
have arbitrary bits that are very
clearly invented but all functional
languages have at their core lambda
calculus which is discovered so this is
my invitation to you to use programming
languages that are discovered most of
you use programming languages that are
invented and you can tell
so this is my invitation to you to use
programming languages that are
discovered the same ideas as we saw
there's a correspondence to proofs and
as a result lambda terms are a very
convenient way of writing down proofs
that fax whole and so pretty much every
automated proof assistant in the world
again uses these ideas at their core and
so things like the proof of the four
color theorem that was done in Cork is
all based at root on the ideas that I've
shown you today and these also turn into
programming languages with advanced type
system something called dependent types
so then if you get interest independent
types it is the notion of propositions
as types that led in Howard's first
paper for him to propose dependent types
so let's conclude so I talked about how
Turing's contribution to a large extent
was philosophy so if you'll forgive me
i'm going to indulge in a little bit of
philosophy to conclude what would happen
if we try to talk to aliens so this is
not an imaginary exercise we have done
this the spaceship Voyager has on its
side this plaque which is to talk to
aliens if we ever find them so on the
left here we've got a map of where you
would find earth so the point in the
center corresponds to solve the points
on the outside correspond to different
pulsars their marks on these which
correspond to binary numbers which are
the frequency of waves from those
pulsars and then the length of the line
is the distance of the Pulsar from Earth
and then on the right you have a picture
of Voyager with a man and a woman
standing in front of it and there's a
schematic diagram of the solar system at
the bottom what would happen if aliens
found this so I think the bit on the
left they'd probably be able to work out
what binary means they probably
with ulcers they could work out what and
they would almost certainly work out
that the length of the line
correspondent to the distance between
the point in the center and these
different pulsars so they could work out
where we were what about the bid on the
right well if Star Trek is correct
aliens will look at it and say ah they
look just like us except they don't have
pubic hair but if Star Trek is not right
and aliens have a very different
perceptual system the event on the right
they might just think oh that's
indecipherable squiggles we don't know
what will happen so some kinds of things
are easy to communicate to aliens other
things might be harder there's actually
a film called independence day where
aliens are destroyed by a computer virus
here's the computer virus you can see
that it's written in C it's actually a
dialect of see that only has open angle
braces and many of you like me might
find the notion that you could give a
virus to an alien computer written in C
a bit far-fetched right even if we just
sent a program in c to aliens I think it
would be a lot like the thing on the
right-hand side of that plaque right
they might be able to decipher it or
they might not it might just be
indecipherable squiggles to them as it
sometimes is to us
but what if we sent them lambda calculus
lambda calculus I think would be more
like the things on the left hand side of
the plaque almost certainly aliens will
have worked out natural deduction and
logic and therefore they must know about
lambda calculus by the propositions as
types and therefore they would be able
to understand decipher a program written
in lambda calculus of course the symbols
would be different but they'd be able to
work it out so should we call lambda
calculus the universal programming
language well let's ponder that for a
minute some people have talked about
multiverses this is often used as an
explanation why the universe works looks
the way it does well why is gravity
enough to clump things together and make
stars and planets why is the week
electron force just enough that thinks
clump together into matter just a little
bit greater or a little bit smaller and
that wouldn't happen the universe would
be very different and so people say well
there are lots of universes and we're in
the one that has matter and life you'll
universes have gotten so much into the
popular literature that I found them in
books by Salman Rushdie and this is from
a play called constellations it has one
of the best stage directions I've ever
seen you know normally if you read the
text of a play it begins with this long
bit in italic and it says okay the stage
has a house on it here and there's a
door there and there's a table there and
there's a gun sitting there that's going
to be fired in the third act and on and
on this one has the best stage direction
ever it's just one line it says in the
text a horizontal rule corresponds to a
change of universe
so it's a lovely place see it if you get
a chance anyhow this idea that there are
many universes comes often up often in a
popular literature and in physics and
astrophysics and lambda calculus natural
deduction I you might have a better
imagination than me but I cannot imagine
a unit I can imagine a universe with
different gravity I can imagine a
universe with a different weak electric
constant I cannot imagine a universe
where modus ponens does not hold I
cannot imagine a universe where we don't
have natural deduction I cannot imagine
a universe where we do not have lambda
calculus so we cannot call lambda
calculus the universal language that
would be wrong because calling it
universal is too limiting so that's it
I'm done I just want to conclude by
reminding you that if you do have a
tough job than what you should think is
that this is a job or lambda calculus
I have a question before Google here
here they're very bright lights shining
in my eyes before he proved that
ensiling problem is not decidable were
there more mathematicians like Hilbert
looking for you know ability to prove
every problem with with algorithms with
with computers yes oh they always see
the only one the little dick trying to
do it no no everybody bought into
hilbert's program it was extremely
influential Hilbert's work was extremely
influential and everybody just assumed
that the insurance problem would be
decidable until girdle dropped his
bombshell and then they changed their
mind and they thought oh maybe it's
undecidable so how about what began the
race and that was why within a very
short time of girdle coming up with this
result you had three formal definitions
of algorithm appearing so I wonder how
many mathematicians were there who
actually knew it I mean they were afraid
to lose their job but just didn't talk
about it at the conferences or whatever
so if he'd actually succeeded of course
whether you could do the computations in
practice was another matter he just
wanted to be able to do the computations
in theory just as the three definitions
of algorithm that came up were rather
inefficient nobody would actually want
to compute with the Turing machine
nonetheless Turing's work and the other
work had profound impact on the actual
creation of computers a decade later and
Turing was one of the main people
involved in that ok I have a question
I've read several of your papers and
I've also read this propositions as type
paper typed paper and it's really
interesting and educational like the
introductions of many of your papers
which may be the monads paper all of
these things they
have a lot of context a lot of
historical context and I find it really
interesting all the time to read this
perspective from you and I wonder often
when I'm reading these things you know
you you do a nice job of relating the
history of computation to actually you
know computers and computing things I
was just wondering if you have
suggestions for anybody whether or not
you are a JavaScript developer or a PhD
student in logic is there some sort of
suggested reading that you could could
provide for you know the history of
these things because other than and then
the description that you provide I find
you know many things describing things
in the wrong direction I find a lot of
your descriptions very nice so I was
just wondering if you could provide some
recommendations for things to read for
anybody okay I'm supposed to repeat the
question the question was I've read lots
of your papers and they're very nice and
then it went on to mention there is a
paper of this talk called propositions
as types you can download it from my
website it will also appear i'm told in
communications of the ACM in december so
if you want more details where you want
to go over it again more slowly have a
look at that paper and then Heather
asked what else should I read on this
subject so there are several citations
in that paper for people that want to
follow things further so I recommend
those I think one thing that I didn't
mention is the preface to girdles
collected works which describes girdles
life sorry the let me try that again the
preface to Jensen's collected works
which described Jensen's life it omits a
couple of minor details like that he was
a Nazi
but he pissed off his fellow Nazis
because he wanted to learn logic so he
studied logic from Hilbert and from
hilbert's assistant béarnaise a Jew so
they didn't trust Jensen and when the
war started they shipped him off to
Prague and then when the war ended the
Allies Marchant Prague they locked up
all the Germans including Jensen who
wrote this is ok i will have time to
complete my proof of the completeness of
second-order arithmetic but they forgot
to feed the prisoners and he died of
starvation at a very young age so that's
another interesting part of the story
and you can find that in the
introduction again since collected works
and it was reading Jensen's original
paper that serve got me into
understanding what this correspondence
was about so I highly recommend to
everybody here if there's anything that
you liked it's interesting for goodness
sakes go off find the original paper and
read it right you don't want to learn
about things like pre and post condition
proofs from the people want writing
about today you want to go off and find
Tony whores original paper and read that
and similarly for anything else for
other people not sitting in this
audience
so I strongly urge you go off and read
the originals they are wonderful so just
any bit of computing that you like
Heather just go off and read the
original that's well worth doing and
then my final recommendation to you
about what places to understand the
history is there's a lovely book called
logic comics which goes through the
history of Russell and canter and so on
and it's a bit over the top sometimes
but generally it's pretty good I highly
recommend that and if you want to learn
about the history of Turing of course
read Andrew Hodges book do not watch the
imitation game yes yes and so looking at
the list you had it always seemed that
the mathematicians found the things
before computer scientists why do you
think that is and you think that it will
ever go in the other direction so
there's actually one counterexample here
which is linear logic the paper on
linear logic is is massive and the
mathematicians were having trouble
refereeing it so it got published in
theoretical computer science with a
little note at the beginning saying this
is very important I'm publishing it
without it having been refereed by the
editor so linear logic was already known
at the beginning to be of direct
relevance to both mathematicians and
computer scientists and then as I said
when kohei did session types part of it
was based on ideas from linear logic and
then much later turned out oh it exactly
corresponds to linear logic which was a
surprise but John Reynolds I mentioned
with was the other guy who is just
behind Gerard for doing polymorphic
lambda calculus and the basis of data
abstraction so all abstract data types
basically trace back to those papers by
rod and Reynolds which again are well
worth reading but rebels it turned out
had done something called syntactic
control of interference which was about
trying to tame
concurrent processes published a few
years before linear logic and it's not
identical to linear logic but there are
close relationships between them so
people sometimes call syntactic control
of interference Reynolds revenge so just
once the computer scientists sort of
beat out the auditions but basically the
logicians always get there first they've
been doing it for much longer so what
since there's this correspondence
between you know computational models
and logic systems the natural question
to me is the untyped lambda calculus
what sort of logic system does that
correspond to or is it just an an
inconsistent 10 very good question right
what logical system does untyped lambda
calculus correspond to it turns out that
you can make a close correspondence
between untyped lambda calculus and
curries paradox which is a way of
proving false using an infinite formula
so I'm not going to go into the details
here but there actually is a
correspondence and this also lets you
then in a logical way bring back the
full power of recursive function theory
into typed lambda calculus remember I
said typed lambda calculus always
terminates but once you add back in
recursion which you can do in a logical
way using Korea's paradox you get back
the full power of computation and this
is why all the functional languages I
mentioned in fact are turing-complete
because they support recursion so this
is I think the most interest it's a
great question because this is I think
the most interesting aspect of
propositions as types which is it's so
powerful that it doesn't just work for
every logic but even for places where
logic breaks down for paradoxes it does
something interesting there as well
so I have a question you keep talking
about this computer science thing do you
think it has any any application to
general knowledge I think computer
science has any relationship to general
knowledge that's a very interesting
question I think what we've seen how
deep computer science can be we often
think of it as you know oh I'm just
hacking away I'm and I'm an engineer and
there's a lot of very interesting
engineering in what we do but there are
also very deep ideas how many people
have heard the phrase computational
thinking only a few of you okay now
you've all heard the phrase
computational thinking stands for the
notion that patterns of how information
is processed are very important in
understanding any natural science and in
fact I don't work in a department of
computer science I work in the school of
informatics and informatics was a name
coined to capture the notion that ways
in which information is processed are
absolutely fundamental and universal or
maybe even more than that and are very
important in understand people have used
these ideas now in understanding physics
and understanding biology patterns of
information processing are very
important and I prefer in fact the name
informatics to the name computer science
and there are only two things wrong with
the name computer science the word
computer and the word science but the
word computer doesn't really belong
there because it's not about the
machines it's about processing the
information as Dykstra has pointed out
calling it computer science is a bit
like calling astronomy telescope science
but of course the other thing that's
wrong is with the word science i
profoundly believe that informatics is a
science and if you are a science a real
science you don't need to stick science
in your name so let's stop there thank
you very much
let's go get a drink</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>