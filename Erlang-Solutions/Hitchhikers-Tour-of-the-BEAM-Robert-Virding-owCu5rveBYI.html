<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Hitchhiker's Tour of the BEAM - Robert Virding | Coder Coacher - Coaching Coders</title><meta content="Hitchhiker's Tour of the BEAM - Robert Virding - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Hitchhiker's Tour of the BEAM - Robert Virding</b></h2><h5 class="post__date">2013-04-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/owCu5rveBYI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">right next person is track talking is me
okay
I'm Robert so what I'm going to talk
about is give it an overview of what the
beam looks like internally or parts with
what the beam looks like internally to
give you some feel of what's going on in
there so this is the Hitchhiker's tour
of the beam right it's not a guide it's
not deep enough to be a guide visit tool
and the first thing of course to try and
describe is what is the beam what do I
mean by there being what is it doing
right and well too many things with many
different parts of it and one thing is
it's a virtual machine for running a
line that's really what it is it's also
an interface to the outside world
I mean be no point in running ailing if
you can't couldn't talk with the outside
world and has number of ways it took
basically a long has two ways of talking
to outside world it's got the concerts
and ports which look like processes and
NIF's which look like function rooms so
the beam must support ports and nibs and
it does contain a bunch of useful
built-in functions so-called bits a very
busy and I'm not gonna go talk about
describing a line but I will say a few
things about the a line because it
affects what how they're being works and
what they're being supposed to do
and these are properties the Aling
system so in our system it it needs it
depends on its defined to contain
lightweight massive concurrency it uses
asynchronous communication asynchronous
message passing it needs process
isolation so well the concurrency is
based on processes and it needs process
I process isolation to be able to do the
error handling to be able to build
proper fault-tolerant systems it needs
to be able do continuous evolution of
the system while it's running which in
this case it means it needs to hand
code code loading dynamically and it is
soft real-time or supposed to be soft
real-time so you have the concept of
time and timeouts it does not guarantee
it will always be able to feel fulfilled
the real the real real time people would
not call don't call our land real-time
and these properties the beam has to
support if it can't do that it's not
being it's not implementing Allen and
has to be able to do this efficiently
because when we write out on code we
assume this we assume we can start lots
of processes and cheap through that we
assume we can send messages and it
doesn't cost too much etc so it just has
to do it it has a few other properties
of the language as well
none of these are especially specific to
having quite general type of things so
in common you'll find it other languages
as well so yes there's a mutable data
and if the beam can support muting
mutable data properly that's fine it
makes it more efficient
we have pattern matching and if you look
through it our line pattern matching is
ubiquitous so to make efficient code
there should be good support for pattern
matching for doing pattern matching in
the machine and it's a functional
language which means we call functions
all the time so function calls should be
relatively efficient now none of these
things are specific to Allen they're
quite common things the other things I
was talking about the Allen system some
of them are quite specific to our long
and you won't find them in our other
environments as well in many ways the
other the other said if you just popped
back they are more operating system like
then the more common type properties of
operating systems rather than languages
so how do we do this so they end up to
run Erlang the beam needs to support all
this at least
so we're gonna look at a few things
about the babe we get a look at the
contact called schedulers look at
processes a bit about memory management
message passing a bit about multi-core
and a few other things as well as we go
and we're not gonna die really dive deep
into anything you're just going to brush
over the top things the basic well we
won't say the building stone of the
Aling system is a scheduler and it isn't
in one sense it's a semi autonomous
small virtual machine running to be okay
so it it tries to operate as
independently as possible of other
schedulers in the system of course it
can't because there's running running
the same one at one-hour Lang system and
of course that they will be talking with
each other and so on but tries to do as
much work as possible
independently and by default well the
system starts up one airline scheduler
in a separate VM thread so you have one
with one VM thread per showed you where
the system is running and it says try to
run a simple as possible it for example
contains its own run queue so it
contains its own set of processes has
tried to manage and run things like this
port by default the owling system will
start one VM thread one scheduler per
court in the machine but you can control
that you can set how many schedules you
want how you can lock them or bind them
to cause and this type of thing as well
if you want it or you can just let the
system do it for you I think we saw on
the previous talk sometimes you don't
want to let the system do it for you
you can for example if you want to start
multiple schedules per cool you probably
want to reuse cases where that's
reasonable to do so the the machine to
be tried to spread the a line tasks or
processes over the multiple schedules
and it does try and do a bit of
balancing work to try and keep the
shakers reasonably well balanced and
once well every peered did about 40k
reductions reduction in this case is a
function course the name reduction comes
from the from the bad old days when it
was implemented in Prolog and Prolog
doesn't do function calls it does
reductions and that name is stuck
literally 25 years it's done probably
about 23 years since we actually did a
reduction in this system at all anyway
it's still wet and what happens is that
the schedules will run separately and
the first schedule that that reaches
this limit which I think it's about 2:40
quote 40k reductions now that schedule
will decide on the master and it will
tell the other schedulers it's the
master and it will go out and try and do
some balancing and try and optimize the
workload between the schedulers and move
things around if possible just try them
detecting things that aren't working and
stuff like this and will actually
suspend unneeded edges if it finds has
done not enough work for all of them
what can actually suspend the schedule
as it goes it does its work by putting
things on schedule as run queue so if it
wants a scheduler to take a process it
will tell the judge you'll put a job on
it than that on that schedule is running
queue now to take over process or do
things so very rarely action actually
synchronously tells the scheduler
everything works through the run key
which keeps down the need for sync for
synchronization between the shed and
then when the Masters done it tries to
balance it stops it goes back to being a
normal scheduler and you keep on going
until the next time someone hits this
limit and it takes over the work of
being a master schedule so this this
master
around it's not any way to find it not
one shitless to defined as being a
master so one of the main things are
scheduled adults of course is scheduling
processes allen process and the nailing
process can be in a number of different
states so it can be running actually
doing work it can be runnable so it can
be statistic being the run queue it just
hasn't it not a lot it's just not easy
to run for moment or it can be waiting
so if you're waiting for a message it
didn't receive it hasn't got a magic
message which matches it just sits there
waiting for it can be exiting it's been
killed loss dying and it's not the
process of going down so in the same way
here that when a process is killed it's
put on the run queue with information
that the system is now to kill this
process and then when the scheduler gets
to it's during the run queue it will end
up kill the process it can be garbage
collecting in the same way Wireless
garbage collecting into the garbage
collecting state and doing stuff or it
can be suspended you can actually
explicitly suspend the process to stop
it from running and then resume it
afterwards their calls and system to do
that so the process can be in these
states and again each user has its own
working when shipping processes the
important thing to remember is that when
a process is waiting for a message it's
suspended it's not it's doing it is done
it's in a non busy wait so it's not
sitting there polling or anything like
this it's just sitting sitting there
waiting and when a message is sent to
that process then is scheduled in
putting the run you so it's perfectly
acceptable works no trouble have a lots
of processes just sitting there waiting
for messages they don't cost execution
time until actually someone sends in the
message so there's no problem in having
lots of processes monitoring suspending
waiting do
it costs memory but that's it it does
not cost CPU time to do that and when a
message arrives process becomes runnable
it's put on the run queue and when it
when it becomes its turn to run it will
try and match that message against
receive state but neither it works and
will keep on going or it's not not a
message it wants it you know become
suspended again to go waiting you didn't
wait for the next message running
processes do not block a scheduler I
mean in what I mean here is they don't
take over the whole schedule and that's
done by well when a message arrives and
has to wait for a message become
suspended this next process come in and
while the process is running it will be
rescheduled after I think 2000
reductions that process will be
rescheduled and let the next processes
run if you'd come in so you can not
block us block and Aling scheduler by
having a process that just continually
executes code that was a bit about
scheduling and processes memory there
are a number of separate memory areas
different types of memory use for
different uses in the system some of
them these are actually more so we have
processes so each process has its own
heap which it managers by itself ets
tables term storage they're in a
separate area they're not in any process
they're outside the process area we have
the atom table we have the large binary
space I think you might have another
name but I think that sounds a mic or
something different but large binaries
binaries that bigger than 64 bytes
exists outside it process area this base
for code of course there's base the
timers and there other things as well
I'm not going to go through all of these
just do some of them the easiest one is
the atom table so all that immobilize
and there's a global atom table
containing all the atoms
that means handling atoms is fast
comparing atoms as fast you're basically
just comparing the index into the Atma
equality comparison which means there's
really never any use for not using atoms
as attack if you put an integer in there
it will not go faster because then you'd
comparing the integer value when you're
comparing atoms you comparing the
indexing of the atom type same thing but
atoms are never deleted so once you put
an atom in the tables they're forever
which means that you can fill the atoms
and when you do fill the atom table if
you crash the system and that means that
you should be careful of dynamically
creating atoms in your application so
you shouldn't do things like using atoms
for unique identifiers and stop talking
like that because then you will fill the
atom table and you will crash the system
the atom table is big enough so by
default that will not not happen I think
the default size is 1 million atoms or
something like that and you cannot write
code that contains 1 million atoms
different atoms you just can't do that
so just through normal code usage and
stuff like this you will not fill the
atom table but if you dynamically create
atoms while your application is running
you will eventually there's just a thing
to be careful of there is there are two
calls this list to existing out in the
list the binary adder will bind it to
existing atom which will only create
data if it already exists which gives
you some form of control over that for
example but again don't use that unique
stuff they're not unique that's also the
problem so if I create an acid with a
funny name someone else can correct
that's that and were the same funny name
and we suddenly got the same atom unique
anyway that's the atom table we have the
large binary space so as I said binary
is bigger than 64 bytes aren't stored in
a process they're stored in a separate
memory area which is very nice for
something
means what I'm sending these large
wineries around I'm just sending
references just sending pointers into
the binary spaceball I'm not copying
large climates which means you can write
applications where you're sending large
wineries with long processes approach
between processing and processing them
because you're just not copying so you
can actually do video stream video
through allons-y set whaling processes
sending throwing processes binaries
which is very nice
the downside is that it can take a long
time for the system to detect that a
binary in the large binary space is no
longer being used basically work this is
where the system works is that you
reference count the binary for every
process that the binary is passed
through and it can't decrement that
counter for a process until the process
has been garbage collected
that's the only way the system knows
that this process is no longer
referencing the binary after it's done a
process garbage collection and actually
if full garbage collection we'll see
what that means in a moment so you in
the worst case you can get you can you
can have be creating lots of large
binaries you will be streaming them
through processes and maybe some process
or some processes very seldom do garbage
collection which means you take a long
time for a binary action to be reclaimed
the space the binary to be reclaimed
that I actually can crash the system so
I was talking one guy who ran into the
pathological case so he started to
outline nodes and he was trying to see
how fast you could send binary this
between the nodes just to work out
really communication between nodes and
on one node he was quickly created by
knee about a megabyte big then he
spawned off a few thousand processors on
that node which then connect sent the
binary to a few thousand processes on
the other node just sent them across and
after the processes to do anything they
just accepted the binary oh I'm so good
measuring how fast it went and after a
while the second node crashed so what
was happening is the bindings were being
sent over every time they were sent over
to create new binary there were large -
I went to large binary space they were
sent to the process the
process didn't do anything he's
receiving processes and took a long time
before they garbage collected and memory
ran out and they heard the whole system
crashed now fortunately was running on
the Mac so the crash dump the dumping of
the crash dump crashed as well so he
didn't get any information out of it all
there wasn't even a crash dump at the
hint a bit after bit of working out we
worked out this listen this was what was
happening again all these binary sent
over they are filling up the system
there was no garbage collection there's
no garbage section being done in a
system that crashed so you just put a
gut call for the garbage collector in
the loops and suddenly the whole problem
went away but much slower of course but
problem went away so that's a real
problem
enik it can't happen yet stables so it's
table date reverse tables outside all
processes that's one of the goals of why
you had at tables so one of the limits
the limitations in the Aling
implementation having separate process
heaps is that you don't what you don't
have really big process does that
affects the garbage collection time so
you want edge tables outside processes
so you can make truly baguettes tables
so you can have it's tables or gigabytes
of data in there they are not implicitly
garbage collected what I mean there is
that in its table is not it's not
explicitly not removed until you
explicitly deleted so you move no one
references it's still there it's like a
process a process that does not go away
until you get to explicitly kill it
right next tables not go away until it
explicitly deleted but of course the
data in the edge tables garbage
collected so if I delete if I delete an
element to the edge table the space it
use will be reclaimed of course but yet
stable itself does not go away until the
killer a problem with this is that every
time you access an edge table you are
copying data from between your process
down to the edge table and back again
there is no way around this
that means accessing large tables or
scanning large tables can mean you do a
lot of copying
that's why for example you've got these
matching select operations which allow
you to do more intelligent scanning and
restrict limit the number of things
actually copy into your process that's
also something to be aware of but they
can store large amounts of data I mean
as I said literally you know gigabyte
it's tables so so that which was one of
the reasons for having it for creating
its tables the other reason that you
wanted more constant time access to
Esther tables which they do provide yeah
there's question I would assume if it's
a large barn it's in the large binary
space but I honestly don't know the
question was if I were a reference in a
Nets table to a binary is the binary
copied into the a table or not it's in
the binary space okay yeah that's what I
would expect yeah one thing to remember
that stables is in some ways they're
processed like so there's the more less
link to the process that created them so
if that process dies the S tables
deleted which could cause a bit of
trouble with you know expecting process
heaps
yeah each person is a superhero so all
data talk process is local to that
process and in the processing you can
well by default when you create a
process it starts off tiny it's
somewhere about to between two and three
hundred words big the process reports
the minimal size of the process and as
you grow the data grows your garbage
collecting it and growing this heap as
you go along the song you can give
options to create a process with a
larger minimal heat which would be quite
reasonable if you know your process is
going to going to grow become big you
can do that at start and say this
process minimal heap is going to be
bigger than this that can optimize quite
a lot you can do it at a per process
level or for the whole system you can
say the whole system no process is going
to be less then 1 megohm words or
something like this of course you have
to be careful that means you can't
create that many processes because
suddenly they're all the processes are
very big and you don't have a fake
memory space for that so it's a
trade-off or you can do it as a tapper
process level say ok this process I want
the minimal heap size to be this because
I know it's going to get big I don't
want to this means that sending messages
between processes means copying the data
there are no references from one process
to another process each process the data
and the process is completely separate
and isolated which is what you need for
the error handling so one of the main
fundamental requirements vary handling
is that a process can crash and not
affect other processes not ruin the
system for other processes and to do
that you have to make sure you don't
have references between processes we run
control references at least and the
current system does that by as I said
having a separate process heat per
process note one thing this is not
required by the language the language
doesn't say how you implement it the
language says
don't you have processed isolation the
language says you can crash one process
and that won't affect the data and other
process seeing you have immutable data
you could you can share data between
processes and they can crash separately
they won't affect each other seeing you
have a mutable data so this this
property of having separate process
heaps using one sense sort of an
implementation detail that's a very
fundamental one affects a lot of things
so for example if we didn't have
perceptive process heaps we wouldn't
need to copy messages when we send them
we wouldn't need a sacred area for its
tables a lot of other things like this
but we do have it and there are good
reasons for doing it not know there is
there is no maximum heap size per
process you can set the process grabs
everything it can until there's nothing
less than the whole system crashes the
account you can set the minimum size but
you can't set the maximum size but then
you get the question you for now about
separate process heaps and message part
sending a message means copying the data
is all that copying very inefficient
right
I'm copy every time I'm sending
something I'm building the data
structure in one process and copying it
over to the next process and building it
there and so on isn't that very
efficient well yes it is sort of perhaps
maybe right but there's a but of course
there's always a but one thing it does
do which is also fundamental in the
system it does enormous ly help you
garbage collect in the process garbage
because it means I can garbage take one
process at a time I don't have to do
anything else with the rest of the
system while I'm garbage collecting my
process I don't have to synchronize with
isn't sharing data I don't do any extra
work for the rest of the system to allow
my process to garbage collect my each
data separately
and getting rid of synchronization and
locks is a big big big win right if you
were here for the last presentation when
the guy from concurrence was Alexander
from Couric's was talking the problems
they had with that type of problem or
the problems they discovered would that
shows anything you can do to save
synchronization to win and having
separate process heaps allows you not to
do that and it's a big win it also means
that every time you do a garbage
collection you can garbage can garbage
collect each process separately I can do
a stop and sort of stop and copy garbage
collection on that process so I don't
have to do a real-time garbage
collection is not this and it because
the process heap is so small that the
garbage collection time per process is
so small that it still keeps his real
timeless of the system if I was doing
say a whole heap garbage collection that
can take a long time he's from a long
point of view a long time you know then
I would have to do a lot of extra work
to make sure that the guard would
clicked and does not interfere with the
system and that's effort to do that
seriously it is it's not always certain
you actually gain anything by it because
the garbage collection becomes more
inefficient it takes a longer time and
most of the time you say by not having
to copy data
okay here the question here about them
was sending a binary from one process to
another yeah the next yeah you'll
explain that much better than I more
detail than I do it that takes up this
problem yes there is a problem there yes
there is a solution but yes you have to
know what you're doing to use the
solution I guess the best I'll leave it
I'll leave it to them director take care
of that the next talker yeah yes yes
it's a known problem for this way but
yeah the government and also the garbage
collection garbage collector becomes
much simpler and that is not something
to laugh at garbage click in error in
the garbage collector typically you see
it through garbage collections later
when some pointer is pointing in the
wrong place you know absolutely no idea
what happened you see the effect of
something it could have happened one too
many garbage collections earlier you
have absolutely no idea it just crashes
because the point of reference is wrong
so keeping the garbage collector simply
is in itself a big win and as I
mentioned having a real time collector
in one heap doesn't always make doesn't
work give you anything I myself did some
experiments every single course it was
easier of having a single heap and
having an interactive collector on one
heap in the whole system it was
interactive enough so they had the soft
real-time lists of the language and it
made message passing really fast I could
send big messages around the system as
fast as anything put a real application
on it and it was basically no difference
in speed so yes I could have faster
memories sending messages but the
collector became worried efficient
there's just no way around that as well
so the result was the net result was
about the same speed which is very
depressing
well I've done a lot of work but it was
very good very good it was doing its job
properly - yeah the benchmark went fine
but the application yeah you percents
not worth the effort and it's much more
difficult and some of the bugs were
really hard to find so the garbage
collector at the in the system at a per
process level it's a copying up this is
pretty standard fare okay seeing you do
it this way you can use both the
standard standard technologies for doing
garbage collection on process it's a
copying garbage collection which is very
nice it means you're copying the live
data and just leaving all the dead stuff
there which is very good in this type of
language busy offering you creating a
lot of data and most of it becomes old
very quickly just look at the live data
it's actually less work it's also a
generational collector which is based on
the heuristic that most data dies young
so if I if I just clicked on the young
stuff very often I'll get most of my
garbage I'll find most of my garbage in
that I just save all the stuff that's
been done so that's what's done
generally called the new space and after
a while after a number of collections
any data still lives passed down into
the old space and that is a very good
heuristic for this type of language so
it means that not much data
unnecessarily ends up in the old heap of
course you will have to garbage collect
old heap eventually as well and there
will be garbage in there which means you
have to collect the whole system the
whole process but general that will be
quite fast anyway so that's a very nice
heuristic about it you can do a bit of
tuning to process hips again this is
type of stuff that's you set the tuning
and sometimes these things go much
faster you change the value slightly and
something's going much like what you can
do is you can set the minimum heap size
I mentioned this before and that allowed
means that the process heap will never
go under that size which is fine if
you're Pro so you know your process
become big occasionally you can set this
it will just make it more efficient when
the process heat grows and shrinks but
you paid the price in memory so if you
choose it for the right processes it's a
big win if you choosing the wrong
processes you're wasting memory you can
control how often the garbage collector
does one of these full sweeps how often
it does one of the generational
collections of the new space and how
often it does a full sweep you can
control that when you do it it is black
magic in a sense you can set the
parameters after how many calls and
sometimes things go faster you change
the value sometimes it goes long it
forces the collection more often okay so
which means you're doing more garbage
collection but you're reclaiming memory
faster and so it uses less memory and
the big win here if we claimed large
binaries faster so if you think you've
got a large binary problem maybe a
solution would be to turn on low the
full sweep after for a place for a
process or doing an explicit garbage
collection for process just adjust to
get back your magnet or binary is faster
but the collections less efficient
because you're scanning a lot a lot of
stuff which in one sense is unnecessary
sometimes will give you a win
and this is just a few interesting
things about the system we have
something called the async thread pool
and that's a collection of other threads
threads operating system threads at the
Allen system manages for you to do stuff
and it's both we have the problem that
file arrows can be problematic it takes
time I mean yes disks are must faster
they're compared to everything else
going on in the system reading reading
and writing data from the system takes
the top of it from a file thanks to a
long time and you definitely don't want
to block the scheduler a whole scheduler
while I'm reading a few bytes from a
file because blocking a scheduler that
does affect your real-time that's of
your application because while that's
what the schedule is blocked it can't do
anything else anything else that might
be vulnerable in that scheduler you will
just stay there because the schedule is
blocked the content so using the async
thread pool that allows the system to
move I'll file i/o operations out of the
scheduler into other threads into other
operating system threads and put the
work there and allow the blocking allow
the blocking to occur in those threads
while the scheduler keeps on going they
do schedules the work in another thread
and keeps on going
which is nice it keeps real Primus in
the system up to r15 they will know it
the system did not by default allocate
start in the a3 they sync threads bills
are passed those which in the emulator
which allowed you to set the number of
them now from our 16 I think it's 10
threads by default users to do this that
just starts off these extra operating
system threads it can use for doing them
the asynchronous operations and
you know ain't no caveat sport will you
creating well everything you're creating
more threads of course in the system
which means you're doing more context
switching in the threads and so on in
the system so nothing's free you have to
choose what you want to do I mean you
could turn off completely and then just
do everything as blocking operations in
the schedule take your pick
as it's we're saying it's we're saying
it's a plague or cholera right pick it
is a hit yeah but if you've created them
with their created the file IO actually
uses them by default and you can if
you're like writing linked import
drivers you can write your linkedin port
drivers to use them as well if they're
they're a bit of testing you can test if
I have them I can use them but the eye
net driver does not use them because
apparently the is not necessary because
the the socket interface the only
interface is generic enough on all
systems so you can do non-blocking
operations on them without having to
start going to async threads so the
system can can do non blocking
operations on on on the network I know
stuff that doesn't it doesn't need to do
yeah
okay question the question was if you if
you're running on ten cause you probably
thought you'll get 10 schedulers what
does it mean if I do plus a 10 that
means you'll start 10 threads more yeah
so you won't be you'll start extra
threads for doing you know follow a set
and I mean how many you need of course
depends how much follow you do the
father I oh you're doing so if you're
not reading doing much file stuff you
don't need them if you're doing a lot of
file stuff you do yeah disk disk file
yeah file system files the disk files
were put this way reading from the disk
other unique links file systems things
they can be fast or slow depending what
they're talking about
I'm reading reading stuff from the disk
is slow comparatively yes it's much
faster these days and used to be but
compared to what you do in the operating
system is much slower and if you're
doing a lot of file operations then it
might be a big win to have met many
higher acing threads I don't know if
there's anyone from bash show here you
use a lot of special I know uses a lot
of racing threads because they're doing
a lot of file operation someone said
someone said you said plus a 2024 but I
know that was just someone who said it
wasn't someone from you so I can't
verify that but it's a big number anyway
much bigger than 10 you're doing running
threads now
but they're not kidding okay
yes use this yeah and the disk is slow
compared to everything else I mean you
can do you can do or fall of a line
during one disk right worried so yeah
why not end up with two things this is
just a brief overview of the some parts
of the system how do you crash the beam
well best well I think the best way to
learn how not to do something is to
learn how to do it right I'm not saying
this is all the alternatives but anyway
yeah fill the Attic table when you do
the one-millionth and one atom try to
create that you crash the system no way
around that
you can overflowed the binary space
you're just creating and sending through
so much so many big large binaries and
they get taking so long time relatively
so long time to clean up just fill the
binary space what you just fill the
memory and crash that there are
applications actually do that
uncontrolled process heap growth so what
I mean by that the process it grows a
process grows as necessary so one needs
more space that grows the process more
data in the process heap more messages
in the message queue just keeps growing
it and eventually the system will build
we know memory left the system to take
and that crashes the system yes the
question large enough a very long
message queue there was a question a lot
very long message queue is one long
enough to crash the system high can't
give you a figure I mean it depends
what's on the message queue of course
yeah
a typical typical case where that can
happen is you have a logger and you're
sending lots of logging information to
the logger and the logger is doing
something smart like writing into a disk
or something like this and it can't keep
up with the messages and the message
queue slowly grows and some of the whole
system goes down it does happen yeah
input recursions good one
we don't check that the systems not
check that will happily recur still
deaths right and if you're just making a
bigger stack eventually we'll run out of
memory and crash the system then you get
some message some something saying you
couldn't allocate this process couldn't
allocate something which was a hundred
make it big or something like that just
running out of space you have a lot of
data the alexander was mentioning what
they what they had something with how
much was it 170 Meg of memory no more 17
gig of memory I think 16 or 17 gig of
memory in one process you don't need
many of those two crashing system if you
want to talk big memory to talk to Eric
he's from Kelowna and they use a lot of
memory so yeah noobs errors and nificent
linked in port drivers you reference
zero in an if they crash the system
dress the whole airline system
everything same thing in a portrait
LinkedIn driver the system cannot
protect itself against that so so when
if you're doing things like news for
LinkedIn drivers make sure you don't get
get errors in them especially if you're
taking code from somewhere else right I
can do it sometimes you can actually I
think they fix it now but I think in our
14 rough early are 15 you can crash the
system by starting up the observer in
the wrong way or the rel tool in the
wrong way you could crash the system
doing that because that was run run
internally in the system and you got you
got to generate a memory reference error
you crash the system actually I seek bus
I think on the on the Mac which I
haven't seen for about mid-eighties like
to get a single buck well as always
segment while later anyway yeah so
that's really there's no way around that
so be careful especially if you're using
borrowed cocoa from somewhere else the
alternative of course put important and
run in a separate operating system
process then if it crashes the won't
crash the airline system but it's slower
because you're sending miss literally
sending data between the operating
system processes but it's safe yes I
have question was how many orders of
magnitude slow I have absolutely no on
you you know it's like everything what
you want to pay for if you want security
the element of this system then you put
things in other operating system
processes and pay the price in
communication costs if you want faster
communication you bacon in the system
but then you then you pay perhaps pay
the price that you might crash the
system so I I don't know I haven't
really done any serious tests for when
we developed our line we were quite
paranoid about crashing the system so we
happily put everything outside yeah
that's me I want to show one thing both
here in relation to the previous talk
yeah a lock example here this is a small
program a little bit it's very packed
it's very pathological but if you look
here and look at the time it takes to
run this application compared to the
number of schedulers you're running you
find the more schedulers actually the
more cause you're using the slower it
was getting okay and this this was not
inherently a sequential application this
was starting thousands of processes
doing work the more cause you had the
slower it went which is something you
wouldn't expect and the data you could
look at the data the wrong way that way
the other way around
here you can see the speed up as you're
increased number of schedulers and it's
actually getting slower so the more
showed you the worth more cause you had
the slower was getting which was not
quite what you expect and this is purely
a synchronization bug or synchronization
fault problem so it spawned a bunch of
processes which was creating timestamps
what about three or four thousand
processes they just created timestamps
using a line color now I'm just
comparing
and then after a while they just send
their reply back to them to the master
saying yeah all the timestamps are
growing which which is what they were
testing the problem was they were using
a line : now and Alan Cole now has a
wonderful property that it's always
monotonically increasing so for every
call of now will be bigger than the
previous one so sorry for those who
don't know that returns the current time
in microseconds Topol of three elements
in microseconds and it guarantees to
increase which means if you're running
on a multi-core you have to have a
centralized now there and you have to
sync rise and lock access to it so you
can increase it and do it and this
okay this was pathological but that's
all they were doing we're reading this
and with a blocking cause by doing this
synchronization made them go slower the
more call you have if you change that to
do an operating system call into the
timestamp it went faster as expected so
I just went what I want to say here is
synchronization does cost so any of
anything you can do to release it less
than it is actually a big win in it
often cost much more than you think so
yeah that's about it
sorry no picked one too many that way
well the question is can you spawn
process concurrently well yes you can
spawn kill spawn --is you have to let
yourself yeah no no you spawn one
process at a time the spawn operation
synchronous it returns when the process
has started so if you want to do
concurrent spawning then you spawn spawn
as like that do that yeah yeah to be so
honest I'm not certain I would guess
that between this setting up and when
one wins if the master doesn't do
anything there's no reason there is no
master I mean apart from that that's all
does then then it just a shared like
anyone else right
you couldn't suspend the master in that
case
yeah so the question was what why we
have these values for switching and
stuff for TK's because that's 20 times
2k and 2k is there's a limit for context
which within a process and why that is
2k well used be 1k and that was
basically how sort of a unit of time
that was small enough to be not big that
was okay was a reasonable value for
rescheduling processes that's less
slowly grow so I think I think Kenneth
said something like the 2k is probably
too small now because it's so so fast so
now all you do back or move for you
honestly I don't know I the problem okay
the general problem is in general the
problem is that if something goes wrong
somewhere with some process it can be
very hard to work out which processes is
actually the error so I just was my
process is the one that actually runs
out of memory doesn't mean it's my fault
really might be some other processes
actually the fault which I should be
killing they can be hard to work out
who's who's actually to blame right that
was the original rationale behind it
yeah so you were first than you
it does shrink
the process heap shrinks it recognized I
don't know the limits for it but if it
recognizes our first under garbage
collection and it's much smaller than it
was before it will shrink the size there
but it won't shrink below this minimum
heap size you've set and there was one
more than I then I have to go so it's
big it's it's it's a function call every
function calls one reduction yeah and
yeah it's kept in the process just just
a counter you can when you're doing
NIF's now set reduction counts so you
can say mine if is doing five reductions
or the equivalent of five reductions or
something which you should actually do
if it takes time anyway now I have to
stop my time is up so thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>