<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Tech Mesh 2012 - Gaming: How to deal with too much Success - Enrique Paz Perez | Coder Coacher - Coaching Coders</title><meta content="Tech Mesh 2012 - Gaming: How to deal with too much Success - Enrique Paz Perez - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Tech Mesh 2012 - Gaming: How to deal with too much Success - Enrique Paz Perez</b></h2><h5 class="post__date">2013-08-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/srhyHZPqUQc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">today's topic is a pretty catchy that's
how to deal with too much success well
this is a wasted well the company just
over grew the old architecture and we
are now involved in a transition phase
with all the systems obviously with no
downtime and giving being able to
deliver to our end users in the same way
so this is what we are going to cover
today first of all introduction about
spil games who we are what do we do and
how success became a problem for the
company and are we trying to tackle that
issue dealing with scalability and three
levels so being able to scale the
throughput being able to scale our data
storage and also being able to scale as
an organization so okay this is me I
have a background in mobile java
development and i started doing aerlyn
for one of the few spanish airline
companies lambda stream and then i did
aaron road already in the tech telecom
world and beginning this year in january
this year I joined the spill game screw
though spil games is basically a game
platform gaming platform we do develop
games some of our single player games
and mobile games but the thing that we
do most is licensing licensing games
like galaxy life or real-time games
social games as well here you can see a
picture of one of our portals where you
can see you can choose plenty of games
you have most played game so you can
actually go faster them on up times
advertisement solution integrated in the
portal and you can get something
references going on with the App Store
link in there so we are just social
casual gaming platform and we're serving
two more than 190 countries worldwide
that was really surprising to me when I
joined the company because I didn't know
the company by name
and well basically when we started
looking at the data it was surprising to
me in some countries I wouldn't expect
to have even an internet connection you
know and they were playing our purposes
like wow okay and we deal with more than
100 million unique users per month we
consider a unique user person that comes
back at least a second time in a month
and spends at least 10 minutes in a
portal so it's not like accidental click
on google something someone that
actually goes there in place and we're
going to move the platform as well so
basically most of the business we were
doing in the past with web but we're
also going mobile and native for iOS and
Android of more than 300 employees and
offices in the Netherlands and in China
and as a company we make our revenue out
of advertising and also any user
monetization its monetize yeah
monetization and okay this is impressive
graphically where you can actually see
what I mean when I say success water was
a problem for the company so between
2006 and 2012 these are the numbers of
the unique users in millions in the
campaign per month and it was well
pretty high of course to support that
growth we needed to grow as well so this
is the number of Engineers in the tech
department of the company from 2008-2012
which is like an impressive growth the
problem was did we actually have control
of what we were doing well we had a old
architecture based on a lamp stack with
engineers instead of Apache and mostly
PHP with some pythons well my sequel as
a storage and memcache solution so this
is more or less how it looked like we
had the portals and the third party
games that were just using a big API to
talk to our services those services talk
to each other to retrieve some
information either from memcache or from
my sequel databases and compound a big
response and take it back to the client
so
the entry point was mainly HTTP and then
there was this horizontal layers of
services they also communicated to each
other via HTTP so in complex requests
that requires several services to
interact together differed different
pieces of the user information that
meant there were a couple of HTTP jumps
per request which is really not very
good for performance and we started with
a single player static site in 2006 and
well we went through a big boom also
with Facebook involved in as a gaming
platform and we grew to a multiplayer
social gaming platform and on the way we
added the ability for users to search
for games and to register themselves or
we can actually remember who they are
offer them products that are more in
line with what they have played before
we added email notifications and keeping
track the high scores and users can
actually feel interested in coming back
and play again and also add friendships
so they can actually compete with their
friends and well since we have
friendships it's nice that we actually
allow the users to express themselves in
the platform so they can actually
generate their content and comment on
the staff they like and stuff they don't
like and we keep track of the activity
feed so you can see a friend of yours
has been playing some game and you
definitely want going and beat him
social networking support were
integrated with facebook and also with
hives which is a big social network in
the netherlands so we added a lot of
stuff very quick and we're I
intellectual problems so very common
case but some of you has been here
before basically when you put more
people on a project and the project
actually goes slower because managing it
is just mess you have no possibilities
for sharing the data because it's being
used from everywhere just using native
lives and it's quite difficult to scale
it and scaling by adding servers is
definitely far from linear so it's a
very nice thing you had something that
you thought you could scale up to a
point but now because of those
horizontal jumps
orders and because you cannot scale your
data anymore it's really difficult and
all of these King mainly because the
lack of a clear architectural vision so
we had too many horizontal dependencies
was the lack of maintainability in the
code we wanted to grow so fast that
independent teams were developing same
features in a slightly different manner
but they ended up in different code
bases so it's double the code to
maintain and it's really difficult and
on top of it the deploying and releasing
mechanism was started to feel a little
bit like you know release and pray
because you really never know the effect
of the change you're making so yeah we
were a successful company but it was
starting to feel a little bit like this
you know juggling in there how long can
we actually keep this up so we decided
we definitely needed a complete new
approach and for that we wanted to come
with a full architectural solution so we
can see that the architecture is like
the DNA of the company because it not
only affects the way you develop
software it's also the way you think and
the world you design stuff and how you
apply that knowledge to the process of
developing and managing the product so
we expected a series of things from this
new architecture this new big thing that
we had in mind so we expect to be able
to scale a scale the throughput of the
traffic so well the growth was very fast
we want to be able to keep up basically
but we also want to scale the data
storage because that was a limitation in
the past and this is actually very
interesting thing because it's nice that
you want to scale the throughput and be
as fast as you can handling a lot of
concurrent requests but even if your
software is really really fast the
problem is if your storage is in one
place well a certain point where a
worldwide operating company so if you
have user in Indonesia and maybe the
connection is acting so slow that
doesn't matter how much you optimize
your system just going to the data
center to retrieve the data for the user
just hurts performance badly
we want to scale the storage but also
for bringing the data closer to where
the users accessing and finally we want
to be able to grow the company in a
sustainable way because that was
something that was not really happening
which were part of the issue I mean just
trying to keep up with this level of
requests we started bringing more people
in creating more teams and pushing hard
but it was not really a nice scalable
solution in management of human
resources either so these are the new
technologies that we came up to use for
the new yorka tech chure it's like funny
because two of them we were extensively
used in the past so we decided to stick
to engine X because we were very happy
with it basically we could do a lot of
stuff it was flexible we could add
modules and it's nice and we decided to
keep my sequel as well but it's like
yeah I want to keep my signal because we
have a huge volume of data stored in my
sequel and because we have very nice
database experts in house at the moment
to one of them is actually speaking here
in London in the perk on a conference
and but we definitely needed something
else it's not only my sequel that we
need we need something that allows us to
migrate data from one place to the other
someone that makes a sharding
transparent to the programmer something
else and as a cord available language we
chose Ireland and well why we chose turn
around developers in the room I see a
couple of them okay and basically
because well it's a functional language
something we like and it's a very high
availability language it's designed for
telecom solution and it has proven to be
really really really highly available
and it's very nice at concurrency
distribution and fault tolerance so we
can actually insulate the request
independent process request goes wrong
doesn't affect the rest of the stuff
that's going on systems are alive and
they are robust and they can maintain
himself we can distribute them between
the different clusters this is a very
nice bullet point we can do more with
less with airline we're
it's less code and that few code is easy
to maintain easy to understand and
performed really well and there are very
nice studies actually that I mentioned
at the end of the transpiration the
presentation i will show you some links
about migrations of sober in ericsson
from c++ systems to ireland system and
in app and doubling the performance with
one quarter of the code which is like
wow and and then finally because we
weren't alone basically i mean dropping
convincing enough but a little bit scary
you know if you go to a technology that
looks really nice but it's like you're
the only one using it so well we're not
the only one all these companies are
using airline one way or another so with
there's a big community and we could
actually manage so these are the
different layers that we expected to be
present in this new architecture we were
talking so we have a presentation layer
and it is how we actually deliver the
content to the users and client-side API
and then the first thing that goes
slightly difference is the server API
and the application model are separated
and they didn't used to be and the
purpose to this thing is that we keep
the server API very strong and very
robust so that any change that we do in
the application model doesn't really
affect the end users so you can actually
change stuff without being worried of
requests that you need to maintain for
just all compatibility sake or some
stuff like that and the other
interesting thing is that we have added
a storage platform layer on top of the
physical storage so no longer the
application model will be directly
interacting with the physical storage
which was a big limitation for us the
application model we will interact with
the storage platform and storage platen
will hide all the physical storage
specific things like how you share your
data how you query your database it
really does matter ok the programs in
the application model should just care
about the application model ok so before
we mentioned three things the throughput
scalability the data
scalability and organization scalability
so the first one of them is how we
actually manage to scale the throughput
well we want to be able to scale
horizontally in every layer of those
ones that i showed before because we
want to be more efficient and because we
want to make her well we also want to
make the interlayer communications
redundant fault tolerant but you know
one problem that we had in the past it's
like well it's very nice to design for
lots of layers because it makes
everything clear and pictures looks very
nice when you draw in a napkin but in
real life the more layers you have the
more jumps you also have to do the
communication between those layers can
be an issue if you have a lot of
overhead like we used to have so for
this thing you actually we actually
managed to use different things so we
have now airline clusters in the layers
and the early nodes in those classes are
actually communicating to each other
with Erin RPC which is very easy to
write and it's just in the end it's just
TCP communication so there's no not such
an overhead like if you do HTTP calls
like we were doing in the past so there
are more jumps but much more efficient
and of course we have because of feran
concurrency support installation and the
smaller chance you make in each layer
the issue for you is just to fix stuff
without affecting a ton of other things
so this is a big picture on or how we
actually pile those layers so this is
the server API and here you can see the
application model and the storage
platform so starting on the top some
things that you can actually notice is
that the your there's different number
of interface grouped by functionality so
user functionality will be in the user
interface high school related
functionality will be the high school
interface profile functionality over
there and you can scale this one
separately so you can assume this one is
one airline node and the other one is
another along node and you can just
scale them independently
so this is nice but the key factor for
the interface and thing is that they are
very well defined for this interfaces
what we use is something really
old-school but it actually works very
nice for us we use a big XML document
and that might be a surprise actually
but it's actually the something that
works very nice have a big xml
documenting a project that project we
can include some normal dependency we
use a rebar for managing the
dependencies on our source code and that
big xml document gets sort of compiled
through us an xslt and then we use a
project that's called picky does anyone
here know about picky well big is a very
nice thing and that allows you to to
define your structures in a language
very easy very very easy to read so we
transform our XML into picky data
definition language and then the picky
stuff gets compiled and magically
generates not only airline records for
you that define all your data structures
and all your inputs and outputs from the
interface in a very strong manner but it
also generates a validation for those
inputs so if you just provide something
to one of those interfaces that doesn't
really fit into the typing system that
you have defined in the XML document it
will complain immediately and also it
generates translators for that thing so
it generates code that allows you to
pick one of those airline records that
picky generates and transform it to
protocol buffers or transform it to
Jason and you don't really have to write
a code for it because picky does it for
you and that's the way we actually keep
it strong right so we define it in XML
document and then we have a lot of
generated code from there that allows us
to keep a very fixed structure that we
can reuse different ways okay so that's
about the interfaces second thing the
application model is the one that
actually implements at the logic so for
example in this case for a user node you
really
need to access different buckets in your
storage platform and combine them
somehow okay that's actually business
logic that goes to the application model
layer and another thing is that if you
see the arrows they're not strictly
vertical they go any way from the
interfaces to the nodes in the
application model they go actually
crossed and and this is dynamic which
means we can add a new user note here
and without any specific user action
there the interfaces that actually need
a user node to work will find it and
we'll start addressing traffic to it
magically and that we do a by scanning
Network periodically with ARLA so it
periodically each now and then the
interface is know which type of
application modal nodes they require to
work they will just find those and they
will start to address traffic to them
and okay well but that's not really
efficient you know although since you
add a new node you have to wait a little
bit for the network we risk and so you
can actually find a new node and start
putting traffic there well it's
definitely more efficient at human
action okay and the cool thing about it
is that the other way around it works
instantaneously so if this guy goes away
immediately the interfaces were using it
will find another user know that they
can actually use and that doesn't
require network scanning or nothing like
that's just basic monitoring and was
pretty cool so that gives us the
possibility to a scale as I stated
before in every layer basically we can
put more knows of type user if we want
because the logic that is implemented
there is really really complex while we
need less less notes for the high score
because that's fairly simple ok and
finally the storage platform organizes
the information in buckets so it does
mainly two things it shows the
application model and ideal interface to
interact with the data so it's like ok I
want high scores for this guy there you
go and the interface is fairly simple
no not definitely no sequel and it
groups whatever magic is happening below
here the storage platform interface it's
also defined with picky so in the end
what you get back is a nice record which
feels that is protobuf encoded to
actually help dealing with a versioning
issues that you might have and it hides
all the complexity doesn't matter how
many actual sharks you are using in your
database solution for storing the user
info because when you request to the
user bucket that one will go and fetch
it for you you don't really need to know
as a user something very nice all right
no just a detail about how are we doing
this because this is something that was
a small issue for us and start getting
more and more complex like okay we have
a big airline cluster here and we want
to be able to scale individual
subsystems so we are releasing a lot of
airline nodes in same a okay but if
you look at the picture again of you
just reminded the connections are
vertical there are no horizontal jumps
is always from the interface layer to
the application model to the buckets
right so if you will just keep the
cluster fully connected there's a lot of
stuff that we don't need and we have a
huge number of open connections just to
keep the cluster connected and we don't
need it okay so we are using for
deploying and a hidden nodes in norlin
so each node only specifies the notes it
needs and then we have a library that
will find us notes through the network
and it will only connect to those so
well yeah okay but sounds like
overcomplicating an issue that shouldn't
be that difficult well this picture
actually shows you well if you have a
server with approximately 30 airline
nodes running in a server and if you
have a class or four machines if you
only connect to the stuff that you
actually need you have 280 akon 88
permanent TCP connections just for
keeping that vertical flow
completely ready right well if you
connect the whole cluster the whole
graph of notes there you'd have 6216
well ok it's a big number but well now
go and scale the size of your cluster
and it's huge basically and the
difference is just I mean it's a waste
of resources it's a lot of stuff that
you are just having open to keep the
cluster connected well you definitely
don't need it at all because you will
never use those connections all right so
I've talked about the throughput now how
do we actually scale the data well this
is the current picture on how spil games
data is organized we're basically
holding all the stuff into an Amsterdam
nossir dated master data center and
everything is there so it does matter
for users in Australia the data is in
Amsterdam and that's definitely a
bottleneck so we want to go to a
solution where we can actually
distribute according to the traffic we
have and while this is just an example
in a picture but this view the different
data centers around the world right but
not only that we really want to be able
to have also these blue things here at
our satellite data centers and of course
have the full data center network
completely connected now what are these
satellite data centers why are they
different well master data centers are
supposed to be there always ok hi a vial
of all this stuff but you know we have
been actually looking and stuff
happening in all the companies and
relying permanently on what happens in
your data center and for example
recently New York all these facts with
Amazon data centers going wrong okay we
don't want to be there and for that we
have created this satellite data center
solution so this satellite data center
are and i'm skipping to forwarding to a
couple of slides later memory only
storage just make your stuff go faster
okay and it
eventually write the stuff in a
permanent data center but would that
means is that you can speed up really a
lot but if you lose a satellite data
center nothing happens so we're going to
scale the data and we want to share the
data based on the user location to
improve the access to the data and want
to be able to migrate the data and that
migration we can do it in three levels
first of all we want to be able to this
bucket illusion that i mentioned before
that's the only thing that is seen from
the application model want to be able to
change it like okay now I want the
application model to see another feeling
I don't care if it's in the same table
or three tables away or whatever and so
we want to be able to migrate from one
version to another essentially something
that our platform allows us to do just
what defining that you for example want
to go writing always in the new version
while reading in the old one if
available so you can actually migrate
step by step and you can individually
decide that for a global identifier you
want that piece of data to be in a
specific version of the bucket and
knotting another so you can actually go
jumping one by one but also we want to
be able to easily migrate the data from
one chart to another I mean imagine you
just add another sharp because you need
it you want to be able to keep the
balance okay it's not really helpful you
have three huge shards and then once
that's empty it's like yeah okay you
have another shark so you should be able
to handle now but you really need to
migrate that data because otherwise you
are in the same situation and also for
the whole purpose of giving the user
closer access to the data you want to
bring the data closer to the user by
migrating it so if you have a user that
used to access from France it was going
to the answer damn master data center
but the ninth time in a week that the
user access from from LA then we
probably want to migrate the data to one
of those data center in North America
okay
so in a big view how does this all work
we have global identifiers with similar
meaning that you have in key value
storage and every bucket gid operation
is atomic so that's really important if
you have a bucket that represents the
high score for user and the GID that
represents a user every operation you
have there is atomic it doesn't matter
if you are storing that high score
information in non my sequel tables all
of them chartered the moment you
actually perform the operation through
the storage platform you have the safety
that if you are doing some operation for
a bucket GID it will be completely
atomic and then we'll have two types of
storage we have crushed storage that
will make our whole system to go fast
and persistent storage and two types of
operations optimistic and pessimistic
okay what I mean with this the global
hbu is optimistic operations will go
always to cash first and they are meant
for stuff that is really important to
access quickly but you don't really care
a lot about it being persistent so it's
not sensitive data if you lose it well
it's not a desired thing but nothing
happens okay for example imagine you are
storing something for a user's activity
feed something that you really want to
be available as real times as possible
so that the user friends can actually
see what the other guy is doing right
but if you lose it like well okay I lost
that this guy was playing this game for
a while it's not really that important
as long as I don't mess with the guy
records and all this stuff it's not
really important so those operations go
always to cash first and then eventually
get persisted to this while you have
other operations that are really really
important or pessimistic operations and
those ones you really want to ensure the
data is persisted so first of all
I mean for those operations you don't
really care if it takes long you really
want it to work so you issue the
operation wait until it's persisted into
disk then you update the caches is
needed and then you return okay so
something I mentioned before satellite
data centers they do not have a TV okay
they do not have persistent storage they
only have the software of the storage
platform and Akash so we can actually
manage to put satellite data centers
just in an area of the globe to speed up
the traffic we can even rent satellite
data centers on the cloud for a couple
of weeks if we expect a huge peak in
traffic in some area okay and if we lose
one of these well it's definitely not
something you really want to happen but
it's not a big impact basically because
you lose what was cached to for
improving the read access and you lose
those operations that we're performing
an optimistic way so those operations
that you had already stated okay it's
very nice again if i can write this
persisted as soon as possible so others
can read it but it's not really
sensitive information well you lose that
because it's eventually consistent so
eventually we will be reading a TV but
the moment you lose it the stuff that
might be a lot of stuff that was in this
cash it has not be persisted yet okay
the other scenario is what happens with
master data centers so dealing with
catastrophes like something goes really
wrong I mentioned before that losing a
master data center is not something that
our storage platform takes well it's not
a desired effect right so well we know
this stuff happens so what's actually
the scenario were facing when it happens
well in this case we have a very low
level my sequel replication between both
TBS and if you just lose one of the data
centers what happens is well on the
optimistic operations in the cache of
that data center it's gone
are gone all the information that was
cash there well no not anymore but the
latency of this my sequel replication is
what's going to determine how much
sensitive data you're losing basically
that's the only thing you're losing
because for the rest you will have you
will be unable to access all the data
that was stored in that data center for
a while until your operator analyzes the
situation and says okay this is really a
big catastrophe an earthquake or I know
I mean with an excavating machine just
breaking some wires or something really
bad that should not happen or well this
is just a temporary denial-of-service
attacking we can fix it first or
whatever so you have that interval of
half an hour on an hour where your
operator decides to actually modify the
dns rules to route the traffic to
another data center okay but that's just
down time for one data center but your
sensitive data has been only lost for
the time that it gets for to these my
simple replication to actually put the
data in another data center storage okay
so I don't know just guessing but cross
ocean depending on how your my sig will
behaves I mean it okay something but
you're losing data but it's not really
horrible okay and yeah so that's it now
another small bit about this optimistic
pessimistic operations now that you have
seen the flow well just one more thing
about this one it's actually another
curious thing is I mentioned it well
okay the data centric go that goes down
so you can actually access what's in
there that's true but remember we wanted
to bring the data closer to the users I
think I mentioned like a dozen times
already so the thing with this is that
in that datacenter you will have data
stored for the users that are close to
that place because the users have their
own G IDs so if you lose it is not like
all your users
are gone or all your system is down for
an hour because you've lost a data
center that's not actually true only
those user that were physically owned by
data center cannot actually access the
system but the rest of them are still
able to access that's actually something
very important so a little bit more
about optimistic and pessimistic
operations in this figure you can see
more or less how it works so basically a
data center receives an operation for a
bucket GID and first thing it does is
just checks ok do I own the GID is it in
my storage I might be responsible for
this identifier ok yes a.m. then well if
it's an optimistic operation first thing
pushes the operation into the system as
soon as the system accepts the operation
that's it the flow controls there's no
warranty that nothing has actually
happened but the system has accepted the
operation that's exactly what you want
you into something I want these to be
stored and I want to be able to do it as
fast as possible and keep working ok so
eventually the local cache of the data
center will be updated and the data
center will persist the data there's no
error handling no type of control just
use that issue something please quickly
store this for me here for a while ok
there you go and on the other hand you
have the pessimistic operation and for
that one you want first of all the
system accepts the operation but it
doesn't return you the flow it persists
with confirmation so if it fails it
returns you an error if it doesn't then
it updates the cash and dang it returns
you the information about how the
operation went ok so that's a
pessimistic operation that will you
normally expect when you definitely rely
on your system to be storing stuff now
what happens if the GID information is
not in the data center that's receiving
the request well in this case if it's an
optimistic thing again same issue the
data center accepts the operation then
returns the flow then what's happening
while you keep working is the data
center updates the its local cache and
then
use a wide area network bus to perform
an operation in the data center that
actually owns that data so goes there
and look at the order is the same that
in those other pink boxes out there so
first in the remote data center the cash
is updated and then the data is
persisted again no confirmation no error
handling if it goes wrong you I mean if
you expect something can go wrong and
it's important for you that it doesn't
go wrong don't go for an optimistic
operation okay and then you have the
pessimistic right and this is actually
the slowest case in the whole platform
like okay you want an operation to be
absolutely safe but you are not doing
not not doing it in the local datacenter
so the data center just gets a request
and say okay well I don't know in this
GID so first thing it goes to wide area
network operation which has obviously
some overhead because probably has to
cross the ocean to get to the data
center where the data is then persist
with confirmation then updates that
cache of that remote datacenter and then
it comes back updates a local cache and
then it gives you the flow so its
long-term operation okay okay and
finally how do we plan to improve the
scalability of spell games as an
organization well we want to have teams
that can be adapted as quickly as
possible to a very fast changing market
and we want everything to be able to
deliver some end features and we need to
restrict the need to learn or know
everything because it's really difficult
to find a person that can actually lead
the project and also be a genius as a
developer and also know a lot about QA
and also about how to deal with
databases it's really difficult find
someone that knows about everything so
with the model that we have we intend to
have cross-functional teams with people
with knowledge of specific areas working
together to be independent
for delivering a product so how does
this relate to the layers that I showed
you before well you can see the
frequency of change in the product in
front of the complexity of the progress
themselves so you see okay the storage
platform is probably a very complex
product but it doesn't change a lot okay
same thing for the wide area network box
bus then you have stuff that even if
it's not so complexed you definitely
don't want to change it a lot like the
API for example okay changes in the API
are always difficult okay we support
them with this mechanism i mentioned
before we can handle them but it's not
like you really want to be changing it
all the time you'd better think about
what you're doing okay so then you also
have some other stuff in the middle the
event handlers the buckets for the
storage okay it's not definitely not as
complex as the storage platform
mechanism right but you might want to
store your data in a different bucket
because it offers a different view to
the programmers that fits better into
the business logic changes into the
application model and then finally the
stuff that changes the most this in the
end we're a company there even a product
that is not backend for telecom solution
which receives a lot of requires high
availability but is a no real user
seeing what you do okay they see the
effect of what you do because with your
company they can call faster and they
get better network coverage whatever
this is different in the end users are
actually judging what we do because they
go to our portal so they use our native
applications and they enjoy the
experience so the circles are also
representative in size so we have
architectural experts taking care of the
complex things we have back and
engineers taking care of the
intermediate complexity back end stuff
like designing the buckets or thinking
about a business logic and then finally
we have front-end engineers that should
actually
make the user experience as nice as
possible by improving the elements in
the user interface and creating nice
event listeners for the javascript stuff
to make all the things go smoother so
this is again same layers how we
structure our people so there are
there's a very nice point on these
numbers okay we need five percent of
database specialist because we still
obviously have a huge volume of data for
those users so we need database
specialist for it but we don't need a
lot okay because there's few people
actually interacting with the physical
storage the complexity is gone to the
storage platen so storage platform
application model and server API needs
to be maintained by back-end engineers
and since the core technologies we're
using in there is airline we need air on
people okay that's not really surprising
since we declare before that we are
adopting Airlines our core technology
what is surprising is that we are
stating there that we only need a twenty
percent of airline people for a company
that adopts air long as core technology
and why is that well because the most
changing thing in our application is the
stuff that we actually delivered to the
user okay so we really need night front
nice front-end engineers to make the
user experience better while these
twenty percent of back-end engineers and
architectural experts focus on
delivering platform for those guys to
work better so to actually automate as
many steps in the workflow as they can
so it's nicer if those front-end
engineers don't even need to know about
make files or these type of things they
don't care they know a lot about what
the user wants to see and that's there
but that's actually the value they have
for the company they know about the
users and the users are our judges ok so
the group of back-end engineers needs to
actually take care of them take care of
that seventy percent of the guys
delivering platforms that allow them to
develop faster and the labor faster
products that the user is gonna like and
deploy them and test them in a very
sustainable way okay so what have we
learned in the process of this huge
migration well first what we've done
already is set up the whole tooling
around the development so monitoring
tools logging mechanism anything you can
think about this directed is related to
development but not directly development
itself we have heating all the functions
that we're being called to those all
services that use talk to each other
behind a solid API this means a for
third parties especially this is very
very important we can reuse the stuff we
can change anything that we want and we
will not affect our third parties so the
game companies that actually developing
games for us can talk to that API they
don't care what's behind and we don't
care about hurting them by changing
application stuff because it's
completely hidden between behind that
API so this step actually gives us the
freedom to change things and we have
tested the interlayer communication with
live traffic talking about volume here
and we have checked it actually works
very nicely we have set up a development
and deployment processes and we have
full vertical slices already life so
piece of an interface with a couple of
pieces of application models and a
bucket that is being used for storage is
already being hit but light traffic
replacing the old functionality and it's
working very nicely so what are we doing
now is facing the main organizational
challenge so our landscape is changing a
lot in the development development wise
people is being you're organising
different type of teams there's new
technologies going on want to make them
more independent by improving those
platforms we give them to work and we
usually want to train the current
workforce into all these ideas we're
adding the geographical storage
capabilities for for our storage
platform okay which is already solid on
life and being used but still as I
showed you before only one master data
center that's all we have at the moment
and back-end and front-end are being
released now probably as we speak
because we're replacing stuff every day
okay and we will have the next data
center in early 2013 so with weather
rush up with this geographical
capabilities because we're going to need
it okay and big overview for us an
architecture version vision was crucial
because it helped us providing more
focus to the developers we keep them
interact they focus on what they do best
and they deliver decreases the risk of
chaos remember in the beginning I
mentioned lots of traffic bringing more
people in house put them to deliver
anyhow and they start doing stuff and
definitely they start delivering but you
have collisions you have duplicated
functionality difficult maintaining we
control the information flow and very
nice thing to remember use the right
tools for the right task this is very
online with also the conference theme
right don't be afraid to switch to a
different proven technology we have
proven that airline fits our scenario
for back end gaming systems because it
scales better because it allows us to
distribute the load it works wonderful
for us but also don't be afraid to stick
with proven technology like my sequel
okay we have put some stuff on top of my
sig wall so it's not really my simple
for the developers anymore some of them
they don't even know what we are using
for storage but but in terms of physical
storage my sequel really work for us
with huge volumes
data our only problem with we couldn't
shard because everybody was using my
simple directly so as soon as we hide
that and we have already done it then my
simple can be partitioned and our
in-house experts can actually work with
it I'm not saying my sequel is ideal for
any type of storage just be careful here
I don't want any stones thrown at me but
for us it just works and then hide the
implementation in a layer exposed to a
very strict API please as strict as
possible type checking input validation
output validation as well so you don't
actually through stuff that your clients
don't know what to do with it and put
efficient communication between those
layers and that was it for me so now if
you have any questions please
yeah I think with the mic will be better
for recording there we go so have you
got any numbers for light hands once you
started deploying the new Earl and
version how much more efficient it was
I'm going to like quantifiable numbers
like you know Yahoo's half a number of
servers yeah I can tell you some stuff I
didn't put the numbers on the slides
from purpose because at this moment of
the migration it's a slightly difficult
difficult to get accurate measurement
because there's a lot going on basically
some of the services hasn't been
migration to the new application model
so what we have built is airline
adapters that talk to the old PHP stuff
and return the exact same information
format in the new API way so basically
well how you actually measured data
meaning it's a the even if you do it
really really nicely you can be as good
as your solution was because you're
using your own solution well so why then
I'm so brave to stand here and tell you
this better well the thing is that we
have to factor the API so is solid and
together with the API we have refactor
some other stuff like authentication
mechanisms that used to go between those
services happening for Perry requests in
different horizontal jumps and now all
of that is completely changed so even
when we are calling the old services
being adapted I think for our staging
platform the response time used to be
from all requests from top to bottom
like hundred and ten milliseconds or so
and with a new one we're around 70 and
still calling the old PHP services so
still not even in the native airline
implementation which will be much faster
and for other things we also have well
for example the implementation where we
tested the whole interlayer stuff was
just a profanity service to just filter
out words from the usual comments that
thing used to run
10 PHP services now it's running in for
airline servers and the servers are at
twenty percent of the load or so so it's
just amazing how do actually migrate the
users between the my sequel databases so
obvious if somebody logs in from a
different location how'd you actually
pull their data and got you that's a
manual operation that gets performed by
the storage platform so for automatic
char migration you just mark well the
storage planner has in the structure
where it knows per bucket GID where is
the data and in which version of the
bucket so perversion of the bucket you
have a pool of connections that point to
different databases right so you can
mark that you want to migrate data back
ETA deeper to new version and that
version is stored separately so the
system will automatically read from the
old one right in New 11 since the new
one it marks it flip and then it always
goes to new one so it's not like we have
a script doing the migration of the data
we don't but the data gets migrated when
you start using it yeah more
so my question is you said that you're
replicating my secret but basically what
you're saying is that the data of users
in one data center which is defined path
GED so what is replicated on ok the data
of the user is usable in one data center
but given the replication thing that I
mentioned is actually in two data
centers so the data centers know a set
of gads that they own like I showed you
in that picture before so when a date
center receives a request first thing is
checks ok I do I own this if the answer
is no even if the data is there in the
physical storage that's not something
the software is going to check ok it's
like do I own according to these tables
that I have for controlling the data do
I own this thing no okay there it goes
it will ignore it but actually the data
will be in the storage because it's
being replicated from another data
center ok it's only one datacenter it is
using the data at a certain point in
time but that is always in two because
my sakura placated yeah another question
so you said that you have these
cross-functional teams yes but minus 10
is this is basically singer product so
how do you split up the teams so what
defines a team that they need to be
cross-functional how can they work
together and well I've nobody's going to
answer your question if not just correct
me and one of the teams we have for
example build is actually performing a
frontal migration where we used to work
with one technology and now we want to
do it completely different with a small
widgets approach compounding the whole
big page ok so for that we first
identify the needs that we have so well
we need people that is able to
understand the widget mechanisms do we
want to use we need the backing engineer
or two to provide a platform we're
developing those widgets with very few
back and knowledge is possible and then
in the end where we're building a front
and widgets we know we need people that
knows about front and stuff so you
select the people you actually need for
the task by identifying the task
you need to perform focusing first on
delivering our platform so the guys can
actually perform and that way you make
them independent and one last yeah so
how do you deploy this I deploying the
services individually or is that the
whole software's rolled out as one thing
we deploy airline notes individually to
the server because as I show in that
picture the moment you actually put it
on the life classroom for example that
other nodes that actually need the stuff
that you have deployed will look it up
so what we do is one by one if we have
three instances of one application model
airline node running in the cluster we
just that the approach that we're taking
now is this simple as possible it's like
we stop one we start a new one stop
another we start a new one stop another
restart anyone done and general searches
yes mostly they are stateless yeah more
thanks a lot thank you as I said on your
way out if you preach</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>