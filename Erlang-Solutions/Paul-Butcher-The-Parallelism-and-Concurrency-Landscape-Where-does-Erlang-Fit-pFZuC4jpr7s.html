<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Paul Butcher - The Parallelism and Concurrency Landscape: Where does Erlang Fit? | Coder Coacher - Coaching Coders</title><meta content="Paul Butcher - The Parallelism and Concurrency Landscape: Where does Erlang Fit? - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Paul Butcher - The Parallelism and Concurrency Landscape: Where does Erlang Fit?</b></h2><h5 class="post__date">2014-08-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/pFZuC4jpr7s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so hello everybody I'm going to do
something she's probably extremely
foolish it at an o-line conference and
that's mega talk that has absolutely no
Erlang in it of any variety whatsoever
and the reason for that is that when I
was speaking to Francesco about coming
to this this conference and presenting
and he asked that I made a point of
talking about the the wider concurrency
and landscape and now i don't i don't
know you guys half as well as maybe I
should but but one of the things he
asserted was that the erling community
as a group maybe sometimes gets a little
bit insular and there's wonderful things
going on within the airline community
but there are some pretty interesting
things going on outside the community as
well and he wanted me to talk about some
of those things that are going on
outside Erlang and just to provide a
little bit of context that that you guys
maybe we'll find useful in terms of just
just seeing what's going on elsewhere
and maybe stealing some of the some of
the better ideas that are out there and
so I think that the first thing which I
need to make very clear is I'm
absolutely not suggesting in anything
that I say here that I think that any of
these approaches are better than what
goes on in Erlang and I'm certainly not
suggesting that it wouldn't be possible
to do the same kinds of things in Erlang
in fact I guess part of my hope is that
you guys will see some things here that
maybe you think are interesting and that
will inspire things within the airline
community that maybe we'll we'll take
things in in new directions and so there
are really three key messages that i
want to i want to get over here and
first one is that there's a lot more to
parallelism than just multiple cause
multiple calls are really interesting
and I think multiple cores are
particularly interesting in terms of
some of the things that Bruce was
talking about earlier because everybody
in the in the programming world is
getting very excited about multiple
cause and very worried about multiple
cause that's the opportunity for
interesting languages like Erlang to
increase in popularity but
multiple cores are the complete picture
parallelism is about a lot more than
just multiple calls so that's message
one message to is the parallelism and
concurrency aren't the same thing
they're certainly related to each other
but they aren't the same thing and
sometimes if we come at a parallel
problem from a concurrent mindset or we
come at a concurrent problem for a
parallel mindset that can lead us down
blind alleys so that's message to and
the last the last message I want to
convey is that there's more to
concurrency than actors now it's
absolutely not me saying that actors
aren't a very good solution to
concurrency I believe actors are an
extremely good solution to concurrency
but there are other ways to tackle
concurrent problems and one of the
things I want to do today is just show
you some of those other approaches and
but I'm going to start by showing you
probably the most hackneyed graph ever
created in in talks about parallelism
which is good old Moore's Law so
everybody knows this number of
transistors is doubling every 18 months
this obviously is a logarithmic scale
we've got a nice straight line and right
at the moment that straight line looks
like it's going to carry on being a
straight line certainly for as long as
any of us can can predict with any
certainty and again you've seen this
many many times although the number of
transistors is increasing the
performance of a single core kind of
topped out sort of around two thousand
and five ish there are there abouts and
this is what's giving us that well-known
thing the multi-core crisis now like I
say this is a very hackneyed graph I
know you guys have seen this graph and
over and over again everybody
concentrates on this end of the graph
but actually there's something really
interesting going on here what the
hell's going on there why is it that
increasing the number of transistors
from you know somewhere around 10 to the
3 up to somewhere in the sort of 10 to
the 7 10 to the 8th mark why does that
make our computers go faster I mean if
you've only got a single core wider
throwing more transistors
it make it go any faster well there's a
bunch of reasons some of it is about
increasing clock speed as we make our
transistors smaller they get faster and
you know if you've got a clock speed in
the gigahertz instead of in the
megahertz obviously your programs are
going to run faster but that still
doesn't tell you why more transistors
are helping that just tells you why
smaller transistors are helping and so
part of it is bit level parallelism why
is an 8-bit computer slower than a
16-bit computer why is a 32-bit computer
faster than both of them well the answer
is because it does more stuff in
parallel if you want to add two 32-bit
numbers together on a 32-bit computer
that's one operation if you want to do
the same thing on an 8-bit computer
you've got a whole series of operations
in order to do your single edition and
but again that's still only part of the
part of the solution and the main thing
the main reason why to date or not to
date actually when main reason why up
until fairly recently things have been
getting faster is because down
underneath our feet these processes have
been getting more and more parallel and
they've been getting parallel at the
level of instructions so you might write
your code yeah i'm talking about
assembly level now you might right move
this value into that register add these
two registers together branch if this
register is less than zero and it looks
like what's going on is one thing at a
time what's actually going on under the
hood is a huge amount of parallelism
we've got branch prediction we've got
speculative execution we've got
pipelining there's all sorts of things
which the chips have been doing in
parallel the clever thing about all of
this is they've done it while
maintaining the illusion that
everything's running in series and but
that illusion well first off it's it's
not working anymore because the chip
designers have just run out of
opportunities to to apply this
parallelism in a way that seems to work
for us but it's also causing this grief
now as we move to multiple cores so now
that we now that the chip designers are
starting to cram more cause into these
and these CPUs
the fact that all this parallelism is
going on becomes a big deal and in fact
that's a large part of the reason why
multi-threaded programming is so hard
everybody talks about deadlock and live
lock and lock contention but actually
there's a great deal more going to going
on under the scene so I want to present
you guys with a hang on a second and
sorry we had all sorts of grief with the
computer before I started some having to
bring this up where I was hoped I heard
it before so i'm going to show you guys
a puzzle here is can you read that is
that large in a few guys to read cool
and so this is a really simple
multi-threaded Java program it's got two
variables we've got a boolean and we've
got an int and we have two threads first
thread simply writes a number to our
integer and then sets the flag and the
second thread checks the flag and
depending on the value of the flag
prints the result what can this program
do come on you guys are some of the best
computer scientists in the world what's
the possible outcome possible output
from this program really there's nobody
in the room is going to give this a go
okay well so we've got we've got two two
variables we've got a boolean and we've
got an int thread one is writing to the
int and setting the flag thread 2 is
checking the flag and either printing
out the value of the the int or saying
so you're so so what you're saying is
you think that one of two things is
going to happen here yes thank you one
of three things absolutely absolutely so
we can get the obvious two answers we
can get a program which says the meaning
of life is 42 we can also get the answer
I don't know the answer but the really
weird thing is we can get a third answer
and that is the meaning of life is 0 now
how the hell can that happen I mean the
only way that could happen is if the
order of the
line and that line gets flipped somehow
yeah well the thing is the order of
those two lines can get flipped because
we're dealing with concurrency here and
because the CPUs that we're running on
on doing all this clever stuff they're
pipelining they do expect the divx
acuson they're doing branch prediction
down at the level of the of the hardware
yeah these two things really can happen
in different orders and because this
this code doesn't have any
synchronization in it suddenly it's
exposed to all this stuff stuff we
haven't had to worry about in the good
old fashioned sequential days but really
bite us in the ass hard when we're
running in parallel so I'm going to make
another quick change to this because
it's not quite as simple as that let's
imagine instead of this code I've got
I'm going to do nothing in that loop and
then just it's what can that code do yep
and the two are ah no actually there's
another one as well here is even better
it could never terminate because there's
absolutely no again we've got no
synchronization here it's quite possible
for our compiler or our jet or our CPU
any of those things to take a look at
that and say well the thread that's
writing to this value never uses it so
why the hell would I bother sticking it
in memory there's no reason why it has
to be there yet so this stuff is really
complicated now you guys don't care
about this because you're using a same
language that the reason why I'm the
reason why I'm mentioning this is that
this is this is where some of this
clever stuff that's been going on under
our feet for all of these years actually
suddenly becomes noticeable we can no
longer pretend that our CPUs aren't
running stuff in parallel under our feet
so fair enough and let me go back to you
so this stuff is going on and it stopped
giving us value which is why we've now
got our multi-core Christ
but actually this stuff is only stopped
going on in one very specific area of
computing if we're talking about
general-purpose CPUs near the the Intel
CPUs and the and the arm CPUs that we
all use every day yeah this stuff is
stopping value but we use a lot of other
bits of computing hardware and in
particular we use graphics processing
units and in graphics processing units
this stuff's still going on they're
still seeing massive benefits from
exactly this kind of this kind of
parallelism so one thing that's going on
is we're getting pipelining yep so you
we tend to think of multiplying two
numbers together as being a single a
single step but if you're a hardware
designer if you're right if you're
creating chips what's actually going on
is there's a whole series of operations
here and we can pipeline them so that we
bring to offer ends in those operands
and run their way through the pipeline
and I can have one set of operations at
one stage in the pipeline a different
operations differently into the pipeline
so if we look here for example we've got
multiplying two vectors together and we
can have the multiplication of the
seventh item of the vector and the
multiplication of the six and the fifth
and the fourth and third all going on in
parallel so if I've gotta say a ten
element pipeline I can have ten things
happening in parallel the other thing
that can happen is we can deal with much
wider memory verses so if we've got
multiple al use within our within our
system and we've got nice white memory
buses so that we can we can bring
multiple items out of memory
simultaneously we can combine pipelining
inside the ALU with multiplayer use all
running in parallel and we get really
really good speed ups and the kind of
speed ups I'm talking about here aren't
just multiplying your performance by you
know three or four which is what you
might get by having three or four cores
within your system I'm talking about 10
20 30 times speed ups here the trick is
how do we program and if you're if
you're displaying graphics on your room
on your screen yeah you get graphics on
your screen that's great and that's all
being done for you but if you want to do
number crunching
how do you how do you program these
things well one of the ways of doing it
probably the most popular way of doing
it at the moment is to use an open CL
which is a language which is designed
specifically for solving these kinds of
problems so i just want to show you one
very simple example of this if we want
to multiply two arrays just
straightforward secret mm
one-dimensional arrays of data and in
opencl what I would do if I've got let's
say a thousand items in my array what I
would do is I would create a thousand
work items and the work items are
exactly as dumb as you imagine they
might be it simply takes input a input B
and rights to output and i'm going to
show you the code here so you can you
can see when i'm talking about so where
are we we've got x rays so this is our
code and these really as dumb as you
think it is now the interesting thing
about here is there's no there's no
parallelism here I'm not seeing if
there's no threads being created there's
no synchronization there's no nothing
we're literally just saying in order to
get from item I of input a and item I of
input B so item I of output this is what
we do now actually opencl has some
issues so the way in which either run it
is you have a driver program which i'm
not going to show you the time think
that there you go there it is so you
know we're not talking about the
simplest piece of code in the world but
the stuff that actually does the work
this stuff genuinely is incredibly
simple and we get very real performance
speed ups here so just to show you I'm
not making this up if I go into so here
we've got it we've got some this is this
is a really simple case so we're not
going to get our huge speed up here
because we're just doing simple
multiplications but that really simple
piece of code gives us a 2 / 2 / 2 times
performance increase and if we go to
something a little bit more interesting
likes a matrix multiplication which is a
bit more complicated so here's our
matrix multiplication
and I'm not going to go through this in
detail but basically this is this is
exactly what you would normally expect
for 2d matrix multiplication we're just
doing a loop within a loop so we've got
a we've got a single here of course
because this this work item is dealing
with one element of our output some kind
of that the outer loop has been hoisted
into our driver code here but if I run
this and time it I'm getting an eight
times feet up and we're getting an
eight-time speed up on a completely
commodity piece of hardware you start
running this on really serious bits of
hardware you know you go buy a mac pro
for example which has got I think for
GPUs in it something like that you know
you can get a hundred times speed ups
really easily without actually having to
write a thread without having to write
any synchronization yeah this this is
what these things are designed for now
there's a price to pay there's a really
serious price to pay because writing
opencl the code isn't a great deal of
fun but the point I'm trying to make is
there are different ways in which you
can exercise parallel ism now it gets
even more interesting potentially when
you start to look at some of the new
parallel accelerators that are coming
out so the parallel aboard for example
kind of sort of looks like a GPU but it
also kind of sort of looks like a whole
pile of CPUs and there's some really
interesting work going on at the moment
I know to get a line on to these systems
so what that means is you own line guys
are going to find yourself in direct
competition with the opencl guys you're
going to be trying to target exactly the
same pieces of hardware and they're
going to be cases where one approach is
going to work better and they're going
to be a cases where the other approach
works better I think it's going to be
really interesting actually seeing how
this how this plays out and which are
the kinds of problems where Erlang is
the right solution and which are the
kind of problems where opencl is the
right solution and it's there some way
in which we can start to use some of the
the much nicer Erlang development tools
instead of some of the frankly dreadful
opencl tools
so that's parallel ISM really in a very
different way but I accept not
everybody's going to be writing GPU code
any anytime soon and so I'm going to
take a look at another another type of
parallelism which this time is is aimed
much more at the sort of traditional
multiple CPUs and so is anybody here at
closure programmer i'm going to write
you some closure code is anybody here is
yeah we've got at least one okay so it's
going to be new to most of you but it's
pretty straightforward and so let's
imagine I want to do a similar kind of
thing I want to take a whole bunch of
integers and i want to add them all up
together and and i want to do it in
parallel what let's let's start by
seeing what the sequential version of
that would look like and so line is the
closure bill tool and line raffle just
starts me up there a command line rifle
which I can type things out and so I'm
going to start by just getting myself a
whole pile of numbers so this is just a
little tiny piece of of closure which is
going to give me an array that goes from
0 up to 10 million so it's dirty great
bigger rave full of numbers and if I
want to summit i'm going to use time
which is going to tell me how long this
is going to take reduce reduces i think
the equivalent to reduce an erlang as
fold is that right yep cool so this is
this is a fold operation with the plus
operator on numbers and that is the
result of summing all the numbers
between 0 and 10 million i'm going to
run it a couple of times because this is
running on the JVM so we ought to give
the legit a little bit of time to kick
in so we're getting down to a number of
around about 100 milliseconds to perform
this now closure provides this really
nice thing called reduces so what I'm
going to do is I'm going to bring that
into namespace which is
so all I've done is just bring that into
Interscope so that I can call it and I'm
going to do exactly the same thing again
except instead of reduce I'm going to do
r / fold now our / fold reduces fold is
basically just reduce am just to make
life interesting they decided to give it
a different name in the reducers
namespace and if I run this again I'll
run it a couple of times well now around
50 milliseconds so simply by using a
different version of of the effectively
the same library function I'm getting in
this case because again it's a really
simple example something around a 2 it's
about almost exactly a 2 times speedup
now this is that this is a two-time
speed up which is genuinely just using
the general purpose cause inside this
machine so how do we achieve that so go
back to my ear back to my diagrams
what's going on under the hood is it's
producing a tree like this it's a
reduction tree and the way that it's the
way that it's working is it's it's
dividing the input array into a set of
partitions and these partitions we can
control if we really want to exactly how
large they are but you know broadly
speaking is just choosing partitions
which are kind of about the right size
and then it runs our reduction function
over each one of those in parallel and
then what it does is it merges them now
the interesting thing about the example
we just saw is that this is an example
where the thing that we use to to reduce
and then the thing that we use to
combine actually is the same if you're
adding numbers they boil down to the
same thing but I want to show you a
slightly more complicated example of
this so that you can see how this might
work in practice and the the thing I'm
going to look at is a frequencies
function so I go back to our command
line I'm going to have a slightly
different type of numbers and so one of
the things just just so you know I don't
know what I'm doing one of the things
that closure provides is a function
called Rand int which as its name
suggests gives me a random integer so
each time I call this get a different
number in this
x between 0 and 10 and i'm going to do
that's just going to give me a whoops
that's going to run forever is what
that's going to do let's try that again
so that's going to give us 10 million
random numbers which you can see if I
just do so there we go that's given us a
dirty great big array of a random
numbers now let's say I want to know
what the frequency of each each number
is I mean I know it's going to be
roughly one-tenth of the size of the
array but hey they're random so probably
isn't exactly that now closure provides
us with a really nice function called
frequencies which we can pass to i'm
going to time that as well so there we
are we i can tell you that we have got
990 8930 20 we've got whatever it is 1
million one hundred fifty three
instances of 1 and so on and it took
just over a thousand milliseconds so
let's do that a couple of times just to
give the dish a chance to and to make
its mind up so who actually we're
getting slower that's interesting okay
so we've kind of settled it around one
and a half thousand milliseconds and in
order to do that and now i have in my
code a parallel version of that so this
again uses fold from the reducers
namespace but this time its passing two
functions through its passing and a
function which is used to do the
reduction and i'm not going to go
through that in detail because you guys
aren't massively worried about the the
innards of closure i'm sure but
effectively what this is doing is its
building an array using a sock and
incrementing it whenever it gets a new
instance of a number but then it's also
doing a merge and what it's doing is
it's it's merging the two array at sorry
the two maps using addition whenever
there's the same key present in both
maps so if we go back to our diagram
we've got a reduction at the top which
generates a map given an array of
numbers and we've got a merge which
takes two maps and merges them together
and if I try that so the first thing I
need to do is
so I'm just going to change it from
frequencies to parallel frequencies and
again we'll run this a couple of times
so we've gone from about one thousand
four hundred milliseconds to about 400
milliseconds so we've got ourselves a 33
in a bit times and speed up and now the
interesting thing about this is again
there's no there's no for King there's
no process creation there's no nothing
I'm just operating on perfectly ordinary
data structures I'm just doing so using
parallel operators instead of instead of
sequential operators the other
interesting thing about this and this is
kind of the the key point about this is
because there's no synchronization
because there's no messaging because
there's no concurrency what is no
concurrency in the sense that we
normally mean when we say concurrency
this is all completely deterministic one
of the one of the issues with concurrent
programming is it introduces non
determinism now sometimes that's exactly
what you would expect I mean if you're
dealing with a genuinely concur a
problem I mean you're you're looking at
bank accounts you know I really want a
different result if to debits arrive for
a thousand pounds I want the first one
to succeed and because I don't have more
than a thousand pounds in my account I
want the second one to fail if they
arrive in the other order I wanted to
succeed and fail in a different order i
mean that's that's real genuine
non-determinism which is a part of the
problem you're trying to solve but a lot
of problems particularly when we get
into parallelism you don't want to be
deterministic I mean the sum of all the
numbers between Norton and millet the 10
million is the same answer every time I
don't want non-determinism in that I
want it to be exactly the same and
because what I'm dealing with here is
parallelism not concurrency in this case
I'm getting I'm getting that determinism
that I that I want so I spoken quite a
bit about concurrency and parallelism so
what actually is the difference between
the two things well this is a quote from
from Rob Pike who said concurrency is
about dealing with lots of things at
months parallelism is about doing lots
of things at once and one of the ways
that that that I like to explain
is in terms of my wife my wife is a
teacher now because she's a teacher like
most teachers she's extremely good at
multitasking she can cope with being in
a classroom with 30 kids around her and
yes he might at any particular moment in
time be trying to do one particular
thing primarily so she's listening to
somebody read or she's talking to the
class but you know if there's an
argument breaks out on the other side of
the class she'll break off and deal with
that if some one of her children has a
particular question she'll answer that
question now that is a case of
concurrency not parallelism there's only
one of her but she's dealing with more
than one thing in concurrently if we
imagine a situation where she's joined
by another adults so she's got a
teaching assistant and comes into the
class and one of them's listening to
somebody read and the other one is
working with an individual child now
we've got something which is both
parallel and concurrent because there's
multiple things happening and they're
all happening concurrently but if you
imagine a different situation let's say
for example and the the classes decided
that it wants to make its own greetings
cards and they decide to set up an
assembly line to to manufacture those
well now we've got something which if
you view it from a high enough level
it's parallel but it's not concurrent
because there's really only one thing
going on the fact that that multiple
entities within the class of are acting
on it at any given moment in time
doesn't really make it a concurrent
problem and another way of thinking
about this a slightly different
different way of thinking about it is
that concurrency is an element of the
problem domain a concurrent problem is
an intrinsically concurrent problem I
mean the kind of problems that Erlang
typically is is used to solve you're
dealing with real concurrency I mean if
you're writing a network switch and
multiple things are arriving at the
network switch at unpredictable moments
you really need to be able to handle
them and can handle them concurrently
and parallelism though is an aspect of
the solution domain really all we're
trying to do when we write parallel code
is trying to execute faster we want a
problem that would normally take say a
second to operate in a tenth of a second
because we bring
cause to bear on it so they're related
in the sense that there and they're both
non-sequential down at the level of the
code but they're they're kind of
different things and if we come at a
parallel problem using concurrent tools
what we can do is we can bring a lot of
the issues associated with concurrency
into into parallelism without
necessarily wanting them there so the
big one being non determinism and so I
spoken quite a lot about about
parallelism concurrency what I now want
to do is go and look at specifically at
that concurrency and and I want to look
at two things I want to look at two
slightly different approaches to
concurrent programming which is which
are different from the Erlang approach
and again just as a as a compare and
contrast and so one of the one of the
things I want to look at is the the
closure approach now I think that's
interesting because it's fundamentally
based on shared memory which is
completely different from the way that
the Erlang typically works but it
doesn't have most of the problems which
we typically associate with shared
memory programming and the reason it
doesn't have those problems is because
closure is not a pure functional
language but is a mostly functional
language and so what I'm going to do is
I'm going to show you a piece of code
which is written in closure here it is
now this is an exceptionally simple web
service so this is the complete this is
every line of code of a close your web
server and what this close your web
server is doing is it's maintaining a
list of players in a tournament so it's
not a very it's probably not the most
useful website you've ever seen and and
before we go through it in any kind of
detail I'll just show you it in action
so I'm just running my server there and
when it started up oops we started up
yet okay no it started up
I'm going to talk to it using curl so
all this is doing is it's just doing
HTTP GET on the URL I've given it and
it's returning the empty and the empty
vector so it's returning that to say no
there aren't any players so let's add a
player so I'm going to curl this is
restful in the true sense of the word if
anybody who went to the talk yesterday
about what-what rest really was so I'm
going to add a player called Paul and
I'm going to add a player called George
and then if i get my players I can see
that I've now got two people in my list
so you know complex this isn't the
interesting thing is if we go back to
our code what we've got is a list of
players which is where are we so this is
this is the the function that's actually
doing it all we're doing is we're konjam
is closure for stick on the end of the
player name onto a vector this is a
perfectly ordinary vector in shared
memory and it's running in parallel I
mean I I can't show you the fact that
it'll cope if we if we hit it with
multiple multiple clients simultaneously
you'll have to take my word for it but
it's running in parallel because this is
running in a web server and the web
server handles multiple requests
concurrently so why is it that this
doesn't give us any kind of concurrency
issues whereas if I try to do the same
thing say in Java and I just stuck
things on the end of a of a vector we
would have those those same problems now
the answer is this little data structure
here this is an atom and at'em as its
name suggests is an atomic variable so
this is the equivalent of just an
ordinary variable in Java in closure but
it can be changed atomically so if two
different threads both try to change
this change this atom at the same time
one of them will succeed and the other
one will retry now that's all very well
in itself but why does this work I mean
you
you could create that kind of data
structure in Java but actually using it
would be a would be a complete nightmare
so what is it about closure that makes
this work really nicely whereas the same
kind of approach wouldn't work in a
language like Java the reason for that
is that closures data structures because
they're all functional are persistent so
I'm sure most guys in this room most of
you oh sorry I shouldn't have said guys
actually based on look the keynote but
actually it turns out I'm right I think
am I yeah that's that's a bit sad but
yes I am right but anyway most of you in
the room and I'm sure we'll know about
the persistent data structures but just
a quick recap if we've got a list in
this particular example and I start off
by adding something to that list even
though list v1 and list V to appear to
be different when I look at them in
memory there's an awful lot of structure
sharing going on similarly if I drop
something off the head and add something
on the bottom then again we still had a
lot of structure sharing going on so at
this point even though I've got three
different lists in memory I've got a lot
of structure sharing going on and if
I've got a pointer to any one of these
that pointer will remain valid for the
whole of the life of the app I'm never
going to have stuff changing under my
feet and that's the reason why in a
functional language like closure we can
make use of shared memory and but we can
do so in a in a very safe safe way
whereas trying to do the same kind of
thing in an imperative language like
like Java would would become problematic
so why am I telling you this I'm telling
you this because and it's very easy to
get into the mindset of thinking and
shared memory is bad yet there are lots
of issues with shared memory but it is
possible to make shared memory work
reasonably well and closure is a good
example of how it's possible to make
shared memory work really well by using
the power of functional programming in
conjunction with it the last thing i
want to show you and it's not what i
meant to do last thing i want to show
you is something which is maybe a little
bit closer to
to your hearts and closer to the way
that they're laying Erlang works so what
I've got here is a diagram of kind of
what an actor's program looks like so
each one of these little blobs is an
actor and a nectar intrinsically is two
things at the same time it's both the
thing that contains your code and the
thing that X cubes and it's a mailbox
which is the destination to which
addresses get to which messages get
written so this is typically what an
actor program looks like now there's
another message passing approach called
communicating sequential processes which
has a very very similar kind of approach
but one key difference and that key
difference is it separates out these two
things so instead of having an actor
which has a mailbox in CSP you have
processes and then you have what CSP
calls channels which boil down to being
mailboxes but these two things are
separate and that leads to a somewhat
different kind of design so instead of
getting that kind of shape program you
get this kind of shape program where
you've got you've got processes and
you've got mail boxes and the two things
are separate now what does this matter
you think well I just want to show you
an a CSP program just to give you a feel
for how it changes the the way that you
would go about programming this so it's
going back so I'm going to go
get myself a command line to know
eventually there we go and so how do we
actually talk to these things well i'm
going to create myself a channel and i'm
going to thread is actually going to
create me a separate thread and i was
going to read for my channel oops okay
so that's that's fought to thread and
its forked to thread which is going to
read from a channel you'll notice I
haven't had any output from that thread
and the reason why I haven't had any
output from that thread is because it's
trying to read from this channel and
there's nothing to read so let's write
to it and now I've written to it and now
I've got the output from my thread
really simple stuff and we can play with
channels in a much more interesting way
though because then their first their
first class entities so in the same way
as i can use functions like filter and
map and all the rest of it on say arrays
and streams i can do the same kind of
thing on channels so what does this look
like well let's do define another
channel i'm going to use a little
utility function called to chan so what
this does is it's defined a channel and
automatically written on to that channel
the result of range 0 10 which is going
to which are the numbers between 0 0 and
10 and just proved that what's going on
I can read from it and I get there zero
I read from it again I get one but
there's this other really nice little
function where I can do
so a sink into is a little utility
that's just going to read from the
channel that it's given put them into
that the day structure that's its first
argument if I run that I get what's left
on the channel and I can do interesting
things here so I can do something like
def go to find a new channel so same
thing again but this time I'm going to
define a new channel so filter less than
is a version of filter that takes
channels its argument and i'm going to
pass it a function which it's a
predicate which returns true if the
value true if its argument is even false
otherwise and give it that channel i
created before and now if i read from
the channel oops that's all I meant
sorry
so if i read from that i'm just going to
get the even numbers because my filter
that acted on channels is only going to
return the even numbers now this is a
this is a really different way of
thinking about dealing with dealing with
these things because their first class
i'm going to show you kind of a shuttle
and i was going to show you a fun little
thing done this one I'm not suggesting
that this is a this is a good way to
solve this problem but let's say we've
got we want to implement the servants
ever a toss the knees now this is an
implementation of the servo eratosthenes
that uses channels and actually this is
I think the purest implementation of the
sieve of Eratosthenes I've ever seen
mostly because the way that it's defined
is you remove all the numbers that are
multiples of your first Prime and then
you remove all the numbers that are
multiples of that and you remove all the
numbers which are multiples of that and
when you actually implement it what you
tend to do is you go through the numbers
one one after the other and knock them
out if they're in some list of primes
that you've already seen what this is
doing is this so actually it's creating
a channel which has got all the numbers
from two up to infinity and then it's
creating another channel which is that
channel with all of the numbers which
are factor of two removed and then it
creates another channel from that which
are all of the which have all the
numbers which are a factor of three
removed and then 5 and so on so this
this actually is a pure implementation
of the severity of Sinise and it's
completely concurrent using using
channels and and things were a little
bit shorter time I won't go through the
code in detail but I guess what i wanted
to what i wanted to demonstrate here is
that by taking the concept of a mailbox
and separating it out from the concept
of an actor you can create a very
different way of thinking about how you
compose the concurrent aspects of the
the problems that you're you're working
on and now i know that was a bit of a
whirlwind tour
and but you know hey it's 45 minutes and
we're trying to cover the whole
concurrency and parallelism landscape
and I want to come back to those those
three key messages which I wish I wanted
to talk about begin to at the beginning
of the the talk parallelism is more than
multiple calls multiple calls are
important absolutely but there's a lot
more parallelism than just multiple
cause parallelism and concurrency aren't
the same things and thinking about them
differently is a very useful exercise
when you're faced with a problem make
sure that you're thinking about
concurrent problems in a concurrent
mindset and thinking about parallel
problems in a parallel mindset and
finally actors are a great way to write
current programs concurrent program so
they're not the only way to write
concurrent programs there are other
well-motivated well thought out ways of
putting together concurrent programs and
with that we'll move to questions yes
sir oh of course
so I as I said at the beginning I'm not
suggesting in any of these these cases
that any of these are either better than
the airline way of thinking about it all
that you couldn't find a way of doing
the same kind of thing in inner like um
this is this is as with all these these
conversations that you have about one
language versus versus another language
it's far more to do with emphasis and
mindset because the way the way that you
tend to think about things if you come
at a problem from an actor kind of
mindset is these independent entities
that that kind of work on things in
their own right and have have their own
addresses that kind of shapes the the
the type of program that you come up
with and so actually I'm going to go
back and show you this this diagram
again one of the things that you can do
very easily with with CSP is as well as
having multiple things writing to a
channel you can also have multiple
things reading from a channel and
unfortunately i don't have an example
that i can show you of that but there
are there are ways in which that changes
the kind of programs that you typically
are likely to come up with so one of the
really nice things that's supported by
both corded async which is what i was
showing you there and also go is the
concept of a rarified timeout no real
fight timeout is a a channel that will
have a value written to it actually
doesn't matter what the value written to
it it's just a value written to it after
some period of time and because it's a
real thing it's a it's a channel which
you can pass around you can pass that
timeout around to multiple different
things so if you imagine you've got say
a sequence of ten things you want to
happen and you don't want each
individual one to time out you want a
timeout for the whole thing you create
this rarified timeout right at the
beginning of what you're doing and then
you pass it down to all of these things
and all of them whenever they read from
any channel they read from the channel
they're interested in and this time out
and if the time out fires then the whole
thing times out and could you put
something like that together in erlanger
I'm sure you could but would you think
of putting it together like that in
Erlang I'm not so sure and that I think
is the value of this stuff
just a it's a way of giving you
additional mental toolkits to tackle
problems yes yes oh sure ah so this
actually is one of yes so this actually
is one of the nice things about about
CSP which aren't fortunate again I
wasn't really able to show you is that
the flow control is built-in so actually
that that sieve is pretty much data flow
so the thing which is generating numbers
at the beginning will only generate
numbers as fast as all of the things
downstream from it can consume those
numbers now that doesn't mean that it
won't run out of memory because it's
kind of profligate in channels but it
certainly won't run out of memory for
that reason
seen it yes yes I guess so so so we're
getting kind of into the into the
monisha here but but in modern CSP
languages what you can create a two
different types of channels there are
synchronous channels and there are
asynchronous channels is the jargon I
mean they're all asynchronous really but
yeah well so this is the interesting
thing is that the asynchronous channels
have a limit so you'll say this channel
can hold up to 10 things or hundred
things or whatever it might be actually
if you do that you don't get this nice
feature that we were just talking about
so the majority of channels in in most
CSP programs actually have 0 elements in
them they have that there is no room in
the channel at all it isn't a buffer in
any shape or form it but at the time
that you write something to the channel
you unblock anything that's waiting on
that channel and if there's nothing
waiting on the channel the person is
writing into the channel blocks
it depends i mean is this different
sorry thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>