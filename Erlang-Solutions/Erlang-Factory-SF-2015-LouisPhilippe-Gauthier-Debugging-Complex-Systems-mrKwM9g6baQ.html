<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Erlang Factory SF 2015 - Louis-Philippe Gauthier - Debugging Complex Systems | Coder Coacher - Coaching Coders</title><meta content="Erlang Factory SF 2015 - Louis-Philippe Gauthier - Debugging Complex Systems - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Erlang Factory SF 2015 - Louis-Philippe Gauthier - Debugging Complex Systems</b></h2><h5 class="post__date">2015-03-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/mrKwM9g6baQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so initially I wanted to talk about
mostly tooling today but I kind of
realized the tooling wasn't really the
problem it's more of the process so I
found this tweet recently on Twitter it
was trending at what like 1,900 retweets
I think at basically anyone that's tried
debugging something at some point ended
up in this kind of situation where you
start wondering what the hell is going
on and eventually you wonder how could
it ever happen or ever work so instead
of talking about tools today I'm going
to talk mostly about the process so
instead of just guessing let's try to do
a systematic approach to the process so
first of all you need to understand the
system by this what I mean is you need
to understand where your application
worked runs I mean you need to
understand the vm you need to understand
the OS you need to understand the
protocols between the different parts if
there's any external services you should
try to understand how they also work if
you don't know all of these moving
blocks it's going to be really hard to
understand where the problem lies you
can make some some guesses and try to
figure out which part but if you don't
know all of it then you're probably
missing something second thing is you
need to know your tools and by this
there's two things one if you know the
tools then you'll probably pick the
right tool to find whatever problem
you're looking for and the other thing
which is more kind of a tip but you
should always experiment with the tool
before you end up in a situation where
it's 6am or 5am and
trying to find the bug play with the
tools in the production environment and
experiment another point is you need to
understand what were and what are the
requirements when the code was written
it's possible that it's not really a bug
in fact maybe that's what it was
supposed to do back in when it was
written and now it seems like above
because it's not the expected behavior
anymore a second step is to reproduce
the bug so how can you do that first of
all there's a couple of things to look
for is there a special function input is
there a state a special state that you
need to be in could be an invalid state
or some special environment variable or
some OS setting now if you're lucky
you'll be able to reproduce locally
which will help you debug more easily
and without having the risk of damaging
the production environment if you're
dealing with money if you're you start
tracing stuff it slows down request my
time out and you might be losing money
so if you're able to do it locally it's
always something that you should try
first if it doesn't work then try doing
it in production and like I was saying
if you're able to reproduce it's much
easier to test because you'll be able to
iterate way faster at to either to
either
figure out if your up disis is right or
not also having a test case or a way to
reproduce it will make it possible to
validate that your fix is correct at the
end third step is collecting data before
you start going crazy and tracing
everything take two seconds and go on
google and search often you'll find the
bug and someone's already going to have
like fix it for you then if you're not
lucky and it's not there don't jump to
conclusions too fast don't take guesses
use observations and by by using
observations I mean either use tools to
find more data or just use logic and
biddle build empirical decisions
collecting data can be sometimes
dangerous if you collect too much data
then the problem is going to become even
more complex so you have to really like
I was saying before pick the right tool
for the problem and limit the amount of
data you also kind of have to filter out
the noise and by this I mean you might
be timing something in the VM and
unfortunately there's another process on
the box that keeps making you contacts
which you can't assume that that timing
is right so sometimes you have to filter
out some of the data just because you're
not controlling the full environment
fourth step is to use process of
elimination so basically you have to
divide and conquer you start with macro
observations so are all the servers
affected is it just one data center all
data center
are you talking as an external service
is that external service running in
multiple data center on every box this
way you can correlate and figure out if
it's like a global program brought that
problem or just affecting one note from
there using data you you want to narrow
down your search and always try to pick
the one point where your observation
will reduce the search Pat the most you
have to watch out though because
sometimes you'll have multiple bugs and
one bug might be hiding the other one
and you might miss it next step which is
more of a tip you want to change one
thing at a time if you change multiple
things you won't be able to correlate
anything and one of the strategies we
usually use if we want to go quickly
about debugging and do multiple changes
is that we'll use different branches and
deploy one branch per box this way we
can still do many observation and test
many things at the same time but we
still have results that we can correlate
the next point keep an audit trail so
this one is actually kind of hard often
you won't take the time to start writing
stuff down but if the problem is hard
enough you might be working on it for a
week or and take a break start again you
want to keep an audit trail your memory
is not going to remember everything
you've tried and your memory is not
shared with your co-workers so sometimes
if you want to have someone else l pew
it's really helpful that they can just
read what you've tried and then they can
build on it it's also useful for post
martin's
it's it's good to be like transparent
about what happens in your
infrastructure usually so if you have
this audit trail you can write more
detail information about that and it's
also useful to coach teammates so it
might be easy for you to debug that kind
of problem but someone else on the team
might not be able to and with this this
trail they can learn how to do it in the
future verify your assumptions so this
is something that happens quite often
you'll deploy a new change and you think
you pushed it but it wasn't pushed so
you have to check if all your
assumptions are right is the code
deployed was it is it the same vm
version is it the same ol s the same
kernel is it the same system config if
you're comparing an apple and an orange
there's no way you can correlate
anything another assumption you have to
verify is your tools your tool might be
saying that it took 10 milliseconds to
do something but if the tool is broken
it doesn't mean anything so you want to
use multiple tools and make sure they
all say or all share the same picture
and because you have an audit trail you
can go back in time and look at what
you've done have you made all the right
decision it's possible that one step you
took you made an assumption and you made
a jump to another step where it wasn't
actually logical so go back in time and
look at your trail and start over next
point is take a step back so often
you'll be working on the same problem
for for a day or during the night trying
to fix it sometimes it's useful to just
take a step back sleep on it and the
next day you'll
in your shower and you'll just figure it
out you also depending if you have an
ego or not sometimes you want to step on
your ego and just ask help just you
there's other people on your team they
probably know or can help just ask for
help and then if it's really impossible
no one can figure out the problem asking
that an expert if you have timing issues
and the colonel or something not
everyone is familiar with that stuff
it's perfectly correct to ask for help
after all of this you finally found a
fix you need to validate your fix you
can't just go to bed and assume that it
works it might be working in in dev mode
on your laptop but it might not work in
production there might be another bug
that was hidden so you want to test and
validate the fix in production validated
on one node two notes different data
centers and make sure you're not adding
any kind of regression where your fix
another thing you want to make sure is
that maybe you're not really fixing the
problem but you're just kind of it's a
side effect maybe you added some some
logging to figure out what the problem
was and I caused a timing issue and
another problem disappears so you want
to make sure you really found the root
cause obviously if you were able to
reproduce the problem you're probably
able to write a regression test and you
should and after that if everything is
green and everything seems to be good I
go back to bed so these are the the
rules i just talked about and summary
and now i'm going to talk a little bit
about the tools and the vm we're pretty
lucky
the erlang interactive shell is super
powerful you can log in remotely to the
node and inspect the the state of the vm
there's tons of documentation for
lichelle there's tons of documentation
for our functions to get more
information about what's going on inside
here you can see I'm using like system
info just getting the release version
but there's so much what so much more
digging the the documentation it's a
gold mine honestly there there's so many
points where you can get information of
what's going on in the system another
thing you can do in the shell if you're
using X table you can go inside and see
what's going on that's also i think one
of the things i use the most to see if
the system is doing the right thing
another thing you can do in the shell if
if you add like functions in your module
to monitor stuff you can write like a
name fun just loop it in the shell print
out stuff and really see in real time
what's happening you don't need anything
fancy right now like this example is
just showing the number of message in
the code server so every second it loops
gives the number of the length of the
message queue and you can see what's
going on obviously the coach server
shouldn't be overloaded with messages
but it could be some other process where
you're missing back pressure and you you
could see it in real time what's
happening so another type of tools this
one's pretty obvious but it's loggers so
you want to be able to use
print messages sometimes obviously I
don't really recommend using i/o format
in production if you have a lot of
errors it's going to blow up in your
face I'd say same thing for error lager
lagger as much safer if you're in
production and I'd say also don't forget
the system logs if you're getting seg
faults or it's getting killed out of
memory or whatnot you'll find messages
from the colonel and there's also you
should check your external services if
you're sharing the service on the same
box it's possible that for example you
have cassandra running on the note it's
doing a compaction taking a lot of i/o
and at the same time starving the Erlang
vm so you want to make sure to be able
to check all the locks to correlate with
the events the other type of tools that
are good for debugging so metric
collectors I've talked about it quite a
bit last year am I talk you want to have
as many metrics as you can they're super
useful to debug after and in real time
my favorite is vmstat plus that's dr l
plus that's d I'm kind of biased because
I wrote them but for me they work well
because they have low overhead since we
don't collect all the metrics locally we
only sample some of them there's also
tons of other alternative folsom
statsman XO meter which are all
application and vm statistics for more
of a system statistics there's a couple
tools we use collecti collecti will post
directly to graphite all the system
metrics you want plus it supports also
ton of external services and obviously
you need to collect all this data
you can use carbon graphite so you to
store it there's also many many
alternative there's commercial
alternative alternative now too so do
some research but you should absolutely
have if you're running any kind of
system in production and you don't have
metrics or some sort of dashboard
showing you metrics you're doing it
wrong another type of tool so debuggers
so there's Erlang trace which is super
super powerful but also kind of
dangerous if you don't know what you're
doing you're going to receive tons of
messages and you're probably going to
run out of memory caught it fast to make
it a little bit easier there's dbg which
works well but is also kind of dangerous
there's no limit on the number of
messages you can receive so these two
tools although super powerful or kind of
dangerous and production a better
alternative at least for production
systems is red bug red bug will limit to
number a message and it gives you nice
little syntax to match messages other
debuggers which are more o s plus
application there's system tap dtrace LT
TNG those are super powerful they're a
bit more complex to understand but it's
worth spending the time to try to
understand how they work program dumps
the vm one crashes or when it gets a
signal can dump a crash dump the crash
dump contains a lot of information about
what's happening in the vm it's super
useful to to read it after it dumps
there's many ways to do it enough server
there's a crash down view ER that's
probably the most user-friendly there's
also and recon
application I guess from Fred there's a
script directory where there's a bash
trip which will give you some
information from the crash dump and if
you're a little bit more familiar with
UNIX tools there's it's mostly strings
so you can cat prep and usually find all
the information you're looking for in
there and then if you're doing the nips
I core dumps or if you're getting like
some sec faults in the BM it can be also
very useful to understand what's
happening profilers so profilers are
obviously good for performance bugs
there's included an OTP there's f Rauf
ypres off which are based on Erlang
trace their powerful but the output
might be a little hard to understand one
of my favorites these days is aflame I
talked about it last year in my talk now
what we do is actually since most of our
services are HTTP servers we have an end
point where I can go on it and I just
get the SVG in my browser and I can just
profile random requests happening and
then again for profiling you can use
system tab dtrace LT TNG which will
probably give you more realer results
since they won't include context
switching and yeah there's also perf if
you want to see really what's happening
inside the vm performance-wise we used
it to figure out why some of our some of
our boolean evaluation stuff was slow
and we found out that forty percent of
the time was spent just copying my
objects room Ed's to the process so
yeah it can be useful and there's many
other ones but I'm not going to talk
about them system utilities so in the
same sense that you need to understand
the full stack you need to know your OS
and the system utilities to observe
what's happening in the system here's
some of my favorites which i use
probably daily top h stopped to see the
cpu usage and other metrics and then if
you're doing anything networking related
i use and grep probably 10 times a day
just to see what's happening HTTP wise
and netstat to see what's up with the
sockets TCP dump if you need to dig a
little bit deeper and TCP and then a
tool that's really useful to just see
what's happening is estrace seeing the
syscalls you can really see if it's
blocking or something you'll figure out
a way quicker than just trying to guess
I guess what's what's really happening
and if you're using files or our socket
i/o talk will give you I Oh information
and else off will give you the files
open or the files those was just like a
small really small sample of utilities
you can use there's tons of performance
and Linux observability tools I can't
recommend enough Brendan Greg's book if
you want to really do anything
performance wise in linux or bsd you
should really read this book there's so
much information and yeah
next static analyzers those good for
typing bugs and the Erlang ecosystem
there's dialyzer if we were using a type
language and we wouldn't have this
problem but dialyzer is really really
useful there's a lot of documentation
online how to use it so if you're not
using it you should start using it tools
for the bug so this is a summary if what
kind of tools you should use or types of
bugs so for logic bugs if you didn't
understand the specs or you just did a
mistake in your logic debuggers loggers
and the shell can be useful if you did a
mistake in typing debuggers shell static
analyzers will help you for resource
bugs so if you're leaking memory leaking
files metric collectors program dumb
shell system utilities will show you for
performance again metric collector shell
system utilities will be super useful so
if a request crashes and no one is
around to monitor it does it trigger an
alert by this I mean you should really
be monitoring everything obviously you
don't want to trigger an alert for
everything because you'll never sleep
but if you don't monitor it you'll never
know it's happening so I'll go over some
examples of debugging I do or I've done
or someone in the team has done first
example this little chat excerpt so I
woke up the morning go on a chat and
read a little this little snippet where
my boss says the numbers and the
reporting you I are completely fucked so
where do we start from there so before I
go
deeply into the problem you have to
understand the full scope of the the
stack so the reporting this you
reporting you I as a web application
which stocks the reporting service which
stock Stan analytical DB which gets
filled up by an ETL which comes from
logs which comes from a back-end server
which comes from HTTP requests that
comes in so if you don't understand all
of this it's really hard to pinpoint
where the problems happening and it's
going to take more time you're going to
spend a lot of time just turning in
circles and not really finding so for
this example what I did I checked first
thing I did was checking numbers in the
database and the numbers in the database
were not good so I concluded that it
wasn't an application bug and it wasn't
a reporting service bug then I looked at
the logs were the locks good no those
missing events so it wasn't an ETL bug
so basically now we had it could have
been just the backend server bug or
there's just no requests coming in so
the numbers shouldn't be there in the
first place so the next step was to go
check the back end server and see if the
data was being aggregated so I validated
it was actually request coming in and
then I went in the shell got the the tid
of the table and then did it tap to list
on it and then I saw a table was full
data so obviously there was something
wrong with the data coming out of pets
and going to the the disk so at this
point it could have been either
serializing this data to JSON or it
could have been the way we lock the data
on disk I assumed that the way we saved
the data on disk was ok since
that's the same way we use everywhere
and we haven't changed the code in like
four years so let's look at the way we
serialize the data so for this I used
red bug i trace the one function which
is called and the function runs every
minute and you can see basically it ran
at 907 and then 908 but it should have
been called many times because it's a
recursive function over the list and in
this case there's two items in the list
so it should have called itself again
the fix was actually really simple and
it was dumb logic mistake we added
filtering for undefined in the metrics
so when the flight ID was undefined we
just returned an empty array or empty
list and then we had a case to not log
the empty list but we forgot to put the
map counters so it wasn't calling itself
in that one case and for some reason
usually the undefined event was one of
the first one in the list so it's always
just cut and only iterate over the the
head and then stop in this case I think
tracing was by far the easiest way to
see what was happening I think we had a
fix for this under like an hour in
production a second example at some
point we started getting these errors in
the log so cowboy protocol parse errors
and they were kind of just happening
randomly on some servers sometimes more
different times of the day so we were
kind of puzzled by them
so first of all since this is like live
HTTP traffic it wasn't that easy to just
reproduce especially because it's like
keep alive so first we we try just
googling the error we found a ticket
open in cowboy about something similar
there is no solution to the problem and
it was I think it's still open so we
added some extra logging in anch to
print out what the state of the request
was when it would crashed and from that
we kind of guessed it would be it's
something about the buffer has it keeps
but we couldn't really figure out what
was happening so we we well our first
guess was that the content length for
some of the request was wrong and that's
why sometimes it would read the body but
there's still be some data on the socket
and when the next request would come in
because it's keep alive the buffer would
be wrong so we used em group to validate
unfortunately all the requests were okay
so we have to move on to another
hypothesis so instead of just trying
endlessly we decided to capture TCP
stream and replay it and I build a
little tool called HTTP replay it's on
my github it's kind of alpha but if feel
free to send a pull wreck if you find a
bug so we replayed the traffic locally
and I was able to reproduce but not
determinists quickly so the problem
would happen sometime sometime not and
it would not be a the same pattern we'd
see in production so at that point I'd
spent way too much time writing this
this replay tool and I had no idea what
else I could do so I stepped on my ego
and told my teammate to try at it so he
did try he tried tracing no dice then he
well we both stopped thinking about it
and during the weekend while driving he
had a ruka moment we were passing around
the cowboy request between processes and
inside of it there's a socket but the
socket is mutable so if you do change or
if you do read from the socket that one
request object that was in the previous
process will not change its state but
the socket itself will change this kind
of the pattern we use so if you're not
familiar with cowboy there's the ranch
acceptor which accept connection and
spawns a cowboy protocol with the socket
this cowboy protocol will call the the
handler and in our case because we have
a pretty strict timing requirement we
spawn another process that will execute
and try to build a response and if the
response is not completed in time we
sent a default response back so in this
case we would read the body in the
internal request handler and whenever we
time out the previous process the
protocol boy protocol would reply with
the default response but using the
request that was not updated so that
request object or record would have the
state that the body wasn't read but in
fact it was red and the next request I
would come in on the keeper life would
be borked
so this this kind of happened not that
often anymore but so you'd get paged by
by die neck which is our DNS provider
saying this the service trouble and then
five minutes later get an email from
nagios saying servers down and then you
ssh to the server and you found out that
the vm is dumping so where do you start
so while we're waiting for the vm to
dump first thing I usually do is go
check graphite in this case it was
pretty obvious what was happening
there's a message queue that just went
crazy and they ran out of memory so
after waiting for the crash them to be
done I used a little tool I talked about
that's in recon to analyze the crash
dump one of the things that shows is a
different messages queue length so in
this case there's one process with
1,400,000 messages seven process with
two messages 22 process with one and
like 10,000 with zero so it's pretty
obvious that probably most of the memory
was in there and then if you want to
figure out which process that is you can
just grab for that number and you'll
find the information about that process
in this case the process was the anchor
server which is a client we use to talk
to memcache so I looked at the code
after and the code is pretty simple this
is obviously a simplified version but
it's just a proc live that loops and
handle messages there's really two
messages it handles most of the time
unless the connection goes down one that
sends and one that receives the
receiving one is non blocking so the
most light the most likely culprit is
jenn tcp sin
so what happens if jen tcp send box well
in this case you pile up messages so
next step was to validate that jen tcp
can actually block a quick Google search
will tell you that it can you can also
validate yourself by create creating a
little toy server that will accept and
keep its its buffer full so you can send
to it how do you fix that there's
couples of ways you can make it by using
the sentiment option but again if
there's too many and the timeout is not
low enough you can still grow your cue
the real solution is to add back
pressure yay unbound and queues are fun
so the pattern we usually use for
unbounded or for back pressure I think
it was discussed earlier another talk is
using ETS counters we increment a
counter and set a default max value when
this values reach it means you can
accept any more so basically you'd pass
the table ID the max value try to
increment if it returns that the
previous value in the new value are the
same that means they can increment more
if it's smaller than the max backlog
then yes you can do it and if there's an
error well it means the table is not
there and you didn't actually create
your back pressure ETS table after
adding that a quick way to test was to
actually block the socket on the other
side to make it fail on purpose and then
I was able to just check by using the
the terminal or the
the shell that it was doing what it's
supposed to do so the backlog grew to
whatever the limit was and the number of
messages on the cube never went over
that backlog limit so some tips this is
a sample from my rebar config and most
of the projects i use you want to have
your tools ready in production if it
fails at 3m you want to just log in in
your shell and be ready to trace right
away you don't want to start deploying
stuff and again don't hesitate to play
with them in production before there's a
real problem it's much easier to debug
when you know what you're doing second
tip is build your own tools if you end
up doing the same thing and in the shell
if you always print out some statistic
about queues or some statistic about
some process build a little wrapper call
that instead 1 i'll save you time to
co-workers will be able to use it and if
you're writing gen servers or anything
that has like a state add some functions
to actually query the state so you can
see what's happening add a function to
give you the table ID so you don't have
to guess and find which one it is so
that's it thank you
Russians in my audience questions in the
audience race tires end of the day I
have a question so other than Erling
what other production language
experience do you have I've done a lot
of Ruby some objective c some see some
java i was it so in terms of debugging
of complex erling application how does
it compare to some of those other
experiences and other languages in other
ecosystems I'd say it depends on the
type of bug but for anything running
like a system and production the tooling
is pretty good obviously some of the
more advanced like the gvm is a lot of
lot of like tuning possible and ways to
introspect the vm so and i'm not that
familiar with all the tools but compared
to most language i think it's it's up
there like there's no it's not missing
anything you didn't find the error
message is just generally appalling
interlink yeah well I think I think this
is one of the lessons you learn really
quickly yes it did crash quite a bit I
think the first three months we wrote
the service I was crashing maybe one no
today would crash with error lager stuff
but eventually lagger came out and we
also put some fixes in places but yeah
that's maybe one place that could be
improved well other questions sir the
front
about a audit trail you talked about and
then I think it's easy to forget once we
each pixel issues and then I'm
interested in how you guys manage the
rocks how you are kind of issues I've
happened and how did you guys fix it how
do you got money that kind of
information or future reference mmm well
we ever were pretty small team so
managing this hasn't been really of an
issue my favorite way is just having
Google Doc and that way it's shareable
with everyone in the team and accessible
for everyone but we're we're probably
like three working on the erlang back
end so we can we still talk when there's
a problem we talk to each other
obviously as the team grow we'll need
maybe a better better titles for
whatever we're using but currently yeah
it's just google docs questions I won't
follow up i log aggregation you guys are
sending metrics to graphite are you
doing it I mean how many nodes you have
in umbria yeah I didn't talk about that
but yeah we do we do have lock stash in
place there's also a lot of other
commercial alternatives out there there
are splunk is one of them look pretty
good but cost a lot but now for now yeah
we're just using log stash there's other
types of tools to that I didn't really
talk about there's like trend analysis
stuff nowadays that you can kind of
figure out when a metric goes bad or
whatnot we've tried playing with it we
weren't able to tune the algorithms
right to actually get anything sensible
out of it but it's definitely possible
cool all right do we feel it
thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>