<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Solving the Server Utilisation Crisis with OpenStack Cloud Infrastructure Automation | Coder Coacher - Coaching Coders</title><meta content="Solving the Server Utilisation Crisis with OpenStack Cloud Infrastructure Automation - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Solving the Server Utilisation Crisis with OpenStack Cloud Infrastructure Automation</b></h2><h5 class="post__date">2015-03-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/KBzS4NL_TZY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">you
hello and welcome to the air like
solutions monthly webinar my name is
Marty military's gon be VP for the ER
regional theatre Lang solutions today's
webinar represents a continuation of a
series of webinars we have been running
across topics of interest in the world
of erlang and dealing with solutions
based on the airline programming
language specifically today we will try
and tell you a bit about these server
utilization crisis a modern paradigm
that has an increasing impact on
operational costs and energy consumption
we regularly hear from people running
very large clusters of a numbering nodes
in their thousands and where server
utilization is typically between a
staggering six to twelve percent and
when a problem appears companies tend to
throw more more service at it rather
than increasing utilization of their
existing capabilities so all this leads
to increasing operational costs
increasing power consumption and
associative energy costs and finally
increasing costs of creating and running
large facilities the discussion that you
are about to take part in will present
an alternative groups one where server
utilization can be dramatically enhanced
leading to significant savings and less
complexity in operations management now
this with any live event please excuse
any technical issues that we may face
today but to start by telling you a bit
about airline solutions we are a
products and services orientated company
completely devoted to the airline
programming language since our starting
1999 we have worked with organisations
and individuals using Erlang helping
evolve the language and supporting
people and businesses using it today we
are just over 100 people across our
offices in London Stockholm Krakow
Budapest Seattle and Buenos Aires and
working on projects across the globe we
are very keen on creating value and
competitive advantage for our customers
across industries and truly unique
features and characteristics of language
language specifically we develop airline
based products and some of those include
our mongoose I am messaging platform
the react distributed data store and one
bato a.m. which is a monitoring and
management technology as well as many
other solutions applicable across
sectors and problem areas where our line
makes sense I'm really pleased to say
that our speaker today is Kenneth tan
the CTO of Sardinia systems Kenneth has
spent many years leading the technical
development of companies across
industries and helping businesses and
individuals benefit from improved
utilization of their existing
capabilities and today's talk Kenneth
will try and give us a tour of the
technology study no systems provide to
help companies substantially enhance
their server utilization whilst
decreasing the operational costs and
energy consumption now please allow me
to finish by saying you are welcome to
post questions throughout the duration
of the webinar for that purpose you can
use the chat facility on webinars
interface our speaker Kenneth will
answer as many questions as time allows
at the end of the webinar if any
questions do go unanswered you're
welcome to raise them by email using the
following address webinar at line hyphen
solutions com if you're interested in
learning more about Cedeno systems these
products in general or wish to establish
whether they could be a solution for the
challenges your own business may be
facing then please feel free to contact
me directly my email address will be
displayed in one of the final slides of
the presentation we will share with you
today the same goes for any other
questions you may have feel free to
contact us I would now like to hand over
to Kenneth tan who will be glad to start
us off it's taken Laden good afternoon
my name is Kenneth tan from Sardinia
systems I lead the technical solutions
operations at sardina systems
we are a cloud data center
infrastructure is V headquartered in
allen and estonia we are focused on the
infrastructure as a service data
sentence segment we have developed the
world's first energy saving and
utilization improving solution for
OpenStack we believe that by addressing
the energy saving aspect we can
massively decrease the other costs
amongst the top 3 kos items in data
center operations we've been selected by
a large site recently to met for
managing a 150,000 VMs operation in
production the software we can integrate
quite easily with thread head OpenStack
and through the cloud and we can look at
this look more later the team behind
this as always software is built by
teams we have a team that has been
working with OpenStack since the early
days of OpenStack with now nearly four
plus years of experience the senior team
or came from large-scale the essential
operations and in developing software to
target such environments
we are now partnering with Erlang
solutions to bring fish director to
customers operating large-scale
facilities which and this combines
Erlang solutions commercial strengths
with our technology and our know-how in
scalable data center operations with
this customers can rely on our language
Erlang solutions outstanding service now
backed by sorry no systems in data
center operations and automation
in conversation with clients we
typically find that clients who have
been who are tasked to solve data center
or the neck problems having to decide on
complex workload placements struggling
to keep data center balanced and
constantly tuning and refining to try to
cope with civilization problems overall
this tends to lead to quality of HR
utilization being relatively low and op
acts and capex being unnecessarily high
this is sound familiar to you
conversely by having an automation
system which automates workload
placements prevents all enak keeping
data center well balanced and hardware
utilization optimal one can not only
increase HR utilization to be higher
quality and more effective we can also
optimize our packs and capex within the
data center
what we do is to bring you from the
state of constant worry to then
now we know that the top three cost
items in data center operations are
hardware acquisition costs energy and
facilities
but ultimately is about what you do with
what you have perhaps we should look at
what is done with the resources
first reservation is about what is
requested utilization is what's actually
used as observed that some of the large
sites reservation could be up to eighty
percent which is high but utilization
remains consistently low at sub twenty
percent this under utilization servers
then leads to unnecessarily requiring
more so as the caterer workload to the
same workload more energy to operate and
cool more servers
large and facilities which costs more to
operate the house of the extra service
as we see utilization that some of the
operators could be in a seven to fifteen
percent range would you think of leaving
lights on at home for the whole week
while being home for a day that by the
way is less than fifteen percent
utilization
the solution to this problem from
certina systems is a product fish
director we have an analytics enabled
call for fast initial right placement of
workload driven by life resource
utilization and then as the workload
changes within the facility we
dynamically reallocate the workload
across the entire facility with this we
then drive down energy costs by much
better utilizing the servers which then
means that the workload could be catered
with less servers lower servers
translating to lower number of servers
translating to lower capex and lower
backs with less servers you then need a
smaller
print smaller facility footprint to
house the server's uniquely we can do
this without requiring data center
partitioning and we can also scale to
span multiple sites and it's a solution
which is work load application
independent now this is quite simple if
you have lower costs it leads to high
profits we can enable this with optimal
high density VMS packing using fish
director how much can you really save
well if you look at a 55 thousands over
facility running at circa fifteen
percent utilization the year
energy bill at ten cents per kilo hour
will come to approximately 50 million
dollars by raising utilization to just
forty five percent which by which is by
no means that the top we could save in
the order of 10 million dollars a year
in energy and we've seen this type of
multiples before on the capex side again
with a 50
five thousand facility catering to
workload growing at sixteen percent a
year also running at fifty
personalization across the facility
using a using models from a large
operator which spent in the order of
billion dollars over four years in
hardware infrastructure capex by raising
utilization 45% one would be able to
save in the order of 67 million in
hardware infrastructure capex it's quite
simple raised utilization you you could
do with last number of servers how do we
do it the software has a number of very
interesting parts ultimately to drive
decision-making one needs to collect
data we've built a super scalable state
data collector which then siphons data
into a scalable data warehouse and I'm
on the other end we siphoned data back
out to drive our decision engines
every single part of the software has
been designed to scale and to be
reliable so we could where necessary add
additional components to cater to the
workload without interrupting the other
parts of the software by doing this we
can also cater to resile to hire
resilience we can do this integrating
you exist Nick OpenStack masking away a
lot of the underlying complexities so
how do we drive decision that's
completely transparent to the end user
the end user being the data center
operator doesn't need to worry about
that as we see here the we have a
component here for optimization which is
integrated as a plug-in to OpenStack
horizon which is the OpenStack user
interface there are a number of use
cases that I can take you through one
it's the disaster recovery use case now
common in some large commercial operator
commercial organizations to have dr
environments which replicate either
partially or fully the production
environment in one or more locations
this is quite costly because it multiply
multiply steer capex and opex the
servers typically sit idle under normal
conditions
looking for disasters that happen really
and then when disaster happens you need
special situation work processes which
are error-prone and intent on the very
intent on training with fish director
what we can do is to have workload
contained in VMs running on physical
servers spanning the multiple locations
and then in the event of disaster we
then flow the workload over to whichever
one of the sites that is up and running
and this
lowing of the workload would be done
could be done automatically with this
you have no other engineering so you
don't have idle resources and there's no
hardware that's sitting there waiting
for disaster to happen
and the whole process could be done just
using normal work processes so you
remove training out of it dr test very
simple in the event of disaster response
could be in the range minutes what about
market disaster now today some
organizations having to cater to mark a
disaster situations would again over
engineer to try to handle the demand
spike and when disaster happens the
responses typically in the range of
hours again similar to the dr situation
you're also relying on special process
special situation work processes which
are again error-prone and intent on
training with fish director what we
do is to dynamically manage down a
workload designated to be low priority
so for example in the event of a market
disaster you may want to allocate your
compute resources to risk at that point
you don't care about compliance
departments workload so what you could
do is then manage down the workload of
compliance Department thereby freeing up
resources to make it available for risk
all this is based on an existing set of
resources so we're not talking about
adding hardware here so at every port
in time you have no over-engineering
just rely on optimal utilization of
resources again no training required
and when it comes to disaster handling
it's very easy to do response to market
disasters again measure the minutes not
hours what about testing development we
often hear about teton development to be
smaller than real production scale and
developers may not be able to test
scalability resources become
underutilized and often idol it's a cost
very very expensive way stage you know
to have servers sitting doing nothing
and when
to test certain capabilities it's also
very slow to respond to the to that
requirement which then means HR
utilization is suboptimal then to take
and after all that to take the software
from development into production you
again have the craft or processes which
are error-prone intensive on training
now conversely with fish director what
we could do is provision like the light
scale and we just manage a resource
allocation in the back end with this you
can seamlessly go from development test
into production and this can be done
with zero training require response to
developers requirements seconds
what we want to do is to take you from
the worried sup up the more inefficient
state into state of zen once the
resources required to run our software
it's actually quite minimal as you see
on this slide memory CPU requirement
it's negligible by today's standards to
be operational we recommend for
physicals to have full resilience which
is what we recommend if you are running
tens of thousands servers it require
20-plus servers basically to cater for
each and every single part of the
software to have a failover component
and that's it let's try to increase your
data center efficiency with fish
director thank you vladan can affect on
thank you thank you doc and I'm sure
everyone else being present at the
webinar will join me in thanking you for
you know very inspiring sort of detailed
insight into the server utilization
crisis and obviously studying US
response to that so can affirm I can't
say this very often but to me this was
particularly interesting as the use case
and the solution that you present it are
crystal clear in terms of you know how
the technology sort of acts against
server utilization and you know it's
sort of outcome only cost and savings
and you know simply you know increasing
profits we're obviously costs previously
stood so just to try and best use the
time that we still have available we've
received a number of questions as you
were as you were speaking so to try and
honor as many of those questions as we
can one of the questions is just
focusing on scalability and one of our
audience is asking where does fish
director max out in terms of scalability
so meaning how far can you actually go
in terms of numbers of nodes virtual
machines and so on okay let me try to
break that question down into several
parts just so that the audience could
try to understand how we come how we
approach the problem
scale in such an environment given that
we build on top of OpenStack we don't
build inside open stack so OpenStack is
a tool for us we collect data outside
OpenStack we make decisions outside
OpenStack post decision-making we then
instruct OpenStack to execute the
actions necessary so then let's look at
the data collection and decision-making
aspect on a data collection side we have
a data collection system which on each
node collects the data and subsequently
streams the data in to a time-series
database the data collector on host is
quite standard this is based on collecti
but the IP is in how do we stream data
from that node into time series data
base and there we have certain
technology which implements a concept
whereby you stream data to a destination
you don't care about the target at that
at that destination you could deploy and
number of targets as necessary in a
fashion that's not too dissimilar from
say when you're writing to writing a
letter to electricity company for
example you send a letter to electricity
company you don't care about actually
opens a letter on the other hand and on
the other side of the data warehouse
then siphoned data back out to make
decisions that decision engine can be
served that part we can operate it with
multiple copies of the decision engine
to again to cater to the workload
essence if necessary so that way as a
facility scales up it's a simple matter
of deploying more copies now if I want
to put a figure to where do we scale to
do our design criteria is hundred
thousand physicals okay so I hope that
answers the question I think it does
thank you kind of for a detailed answer
now we have another member of the
audience obviously coming from a larger
sort of organizational background and
effectively asking how complex is
implementation of fish director in terms
of multiple data sites so we're talking
about you know 56 data centers you know
scattered across the globe ok so the
complexity again let's break that
question of complexity down to several
parts so first off let's assume that the
facilities are able to see each other on
a on the network level and you have a
control plane that is able to access
croft sites once you have that with the
fish director component deploy it at
multiple sites recall that each one of
them can have multiple copies across my
both sides then in the event of a
disaster or in the event where you want
to flow workload from one side to
another this could be all managed as if
it is one large facility it becomes
transparent that way okay thank you can
if I think again that sort of gives us a
good answer now one other question that
and you know questions keep arriving so
again apologies for everyone as we won't
be able to cover all them but rock on is
asking how complex is the integration
process with fish director and do you
need to run you know an airline virtual
machine on the note where you're
implementing fish director so the short
answer is no we don't so we have I can
every further on on that now be on each
of the compute nodes where you actually
run VMs what we deployed there in
addition to standard OpenStack part is a
data collector so they just to get us
the necessary metrics and then the data
warehouse part that would be deployed on
this separate machine or separate sets
of machines and similarly the decision
engines
let me go back to this slide here just
for reference and then so we make we
make the decisions based on the data
coming out from the data warehouse and
then we talk to OpenStack using public
API is only recall we don't make any
changes inside OpenStack we talk to
OpenStack but we don't change OpenStack
is that answer the question yes it does
yeah thank you can affirm I kind of just
to jump straight on to the next question
as they're sort of pouring in that
Russell is asking could you give us a
bit of an insight as to how fish
director differs from systems such as
yarn and may sauce so just researchers
oh sure yeah you've heard the question
fine okay here what we're doing is we're
managing the infrastructure so from a an
infrastructure operator standpoint we're
providing an operating environment for
the other applications so whether your
applications end up to be say certain
how do type jobs or you're running some
web farms that's completely transparent
to us that's contained inside the VMS
and as far as we're concerned we provide
heart we provide virtualized hardware
and for the management of resources
there are some similar sorts of ideas
but our focus is to drive high
utilization across the facility in a
application independent manner
okay Thank You Kenneth now just to move
straight on to the next question this is
more of a more of an open question I
guess open for debate one of our
audience is asking you know high
utilization obviously you know healthy
and perfect but at the same time you
know with the current trend of
underutilization people at least have
that kind of buffer to protect them from
you know reaching the limits of that
utilization obviously you know we can
say that's a very expensive buffer to
have I guess the question is which is
you know which is costlier you know
underutilizing or utilizing sort of very
close to the limit and how does fish
director sort of cater for those Peaks
you know when when utilization is
already high and then you know that a
sort of resources needed ok so you
engineer your facility to whatever peak
levels that you want so whether you want
to use a workload multiple of two three
whatever that multiple maybe now the
bigger question is once you have put in
such a facility what do you do in your
down times in your in the times when you
are not maxing that amount of resources
in that situation what we would do is to
consolidate the work load onto a few
physicals as possible while still
meeting the performance requirements and
then ask your application which is
contained inside of ems starts to draw
on more resources it then triggers
another rebalancing across the facility
and then we pack again the we pack of
VMs again to best fit that requirement
across the facility so to take that
question a few steps further I guess
what what do you do when there is a
spike in resource consumption it then
depends on how long a spike last fall we
would rebound across the entire facility
to meet the higher resource requirement
but certainly it could happen that the
spike is lasting much shorter period
than your rebalancing cycle
Thank You Gareth now just to move
straight on to the next question because
we have them sort of coming in at a
faster pace and that we're answering
them here's a very specific query how do
you integrate with the Red Hat OpenStack
or su SE cloud okay and that's quite
straightforward now first off because
we're not making any changes to
OpenStack every single part of what we
do it outside OpenStack where we can't
do that we would provide a hundred
percent API compliant component which
then on the OpenStack side would just be
a simple configuration to point to our
component as opposed to the standard
component and then because we sit
entirely outside OpenStack integration
with whether it's rather OpenStack or
screw the cloud or any other OpenStack
distributions so long as it is a public
API compliant distribution that
integration is simple its
straightforward there's no complexity to
it
Thank You Kenneth now just to jump on to
the next question squared away and again
to apologize to our audience we can
probably accommodate no more than two
three questions from this point on in
line with the time available so here's a
very concrete query how do you gather
your server state's data okay so server
state data on the on the individual host
we deploy click e alongside collect d we
deploy certain plugins to collect the
metrics of interest the metrics of
interest would typically be wouldn't
typically tell us what the resource
consumption of the VMS what what the VMS
are doing on each of the house from a
resource consumption standpoint and that
data is then fed in to collect d again
using another plugin for streaming data
that plugin will take the data stream it
into a time series data base the time
series database is then backed by a new
SQL database it's cassandra now there
are some interesting aspects here as
some of you probably know collect the
implementing plugins to collect is very
very simple and so if you want to build
on our implementation to collect
whatever metrics there that could be of
interest to you it's very easy
one of the drivers for us when we select
what building blocks we may use to build
our solution is that we aim for zero
training in a facility so whatever a
typical sysadmin may have in his toolbox
that's what we aim to use which is which
drive the decision are behind collecting
but if you can affirm now because the
time seems to be running out of running
out on us it'll probably have to be the
last two questions but here's a really
interesting one one of our audience is
asking basically saying I understand
that the computations for optimal
packing is possibly very complex and the
question is would it be possible to
configure the rebalancing frequency to
fit their particular data center sure
sure sure so so now the short answer is
yes we can configure the rebalancing
frequency to fit and the rebalancing
frequency would then have to take into
account the size of facility and how
quickly we can power service back on
note that we power servers off to reduce
energy consumption and it also affect
the lead time for us to migrate VMs
across the facility the now for other
audiences on the line here the problem
of packing how is it
complex now for those of you who have
kids you probably observed that children
sometimes with the same number of toys
the toys may fit into the toy boxes at
some point in time and then other point
in time the same toys may not fit it's
all about how you pack what you put in
at which point and where and that
mathematically that packing problem is a
non-trivial problem especially when you
start considering that each variable
that one needs to take into account
becomes equivalent to a dimension so you
have a very high dimensional jigsaw
puzzle problem to solve okay thank you
kind of now I just like to apologize
again to everyone who's questions will
not be answered but there's only room
and time for a single question which we
have remaining and the question Kenneth
is what do you use for your own data
warehouse so the data warehouse part we
built on Cassandra for sites that prefer
say for example HBase building a Hadoop
we can do that as well we have that
option the and then layering on top of
this underlying new SQL database we have
a time series data base so as to be able
to
have a time associated aspects to each
of the metrics that we that we collect
kind of thank you for that and I think
that's all the questions that we can
accommodate in the time remaining so I'd
like to thank you for a very inspiring
talk on the server utilization prices
many thanks to all of you in the
audience who have joined us for the
webinar as well please join us again for
our next monthly webinar and just to say
that following today we will be sending
you a short survey to make sure we
capture your feedback from today's
webinar please also note that the
recording of today's webinar and the
presentation that we used today will
also be available for you to collect on
our airline solutions is corporate
website at www airline hyphen solutions
com thank you all once again and we look
forward to seeing you on our next
webinar
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>