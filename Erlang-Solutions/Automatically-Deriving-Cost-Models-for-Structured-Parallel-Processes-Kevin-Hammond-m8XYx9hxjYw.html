<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Automatically Deriving Cost Models for Structured Parallel Processes(...) - Kevin Hammond | Coder Coacher - Coaching Coders</title><meta content="Automatically Deriving Cost Models for Structured Parallel Processes(...) - Kevin Hammond - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Automatically Deriving Cost Models for Structured Parallel Processes(...) - Kevin Hammond</b></h2><h5 class="post__date">2017-03-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/m8XYx9hxjYw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good morning Lander days if they're say
good morning bank thank you so much
appreciated John's talk this morning I'd
appreciated it more his references few
of my papers but I did like the bit
about combining operations and about
there being fundamental laws because
that's what I'm going to tell you about
in my own presentation so quick Paul of
the audience for I start on who knows
about parallelism good oh well great who
knows about type excellent and who knows
about Hilah morphisms whoa and who knows
about parallelism types and hila
morphism aha Thank You Adam so this is
what I'm going to be presenting this
morning is the material is a bit
technical but i'll try to explain it and
in telling general terms and if anything
isn't clear please do ask me questions
probably at the end of the session so
what's the motivation for this work well
patterns are great in the parallel
programming community parallel patterns
are particularly great because they help
us with structuring our code however for
a given parallel pattern what we need to
do is need to choose best implementation
and that means that we need a way to
reason about the structure of our
parallel programs now in the previous
paper that my colleague de vivre Castro
presented as I cfp earlier this year
what we did was we showed you how to
deal with the kranks of transformations
I'm not even doing in work that I've
presented our to the special issue of s
GCS is then to reason also about the
performance of the programs these two
things are complementary first see
reason about the structure done that now
reason about the performance and put the
two things together now I've got a
complete system for reasoning about the
structure and performance of a parallel
program
using a type system so first I'm going
to show you a bit about and skeletons
there are a number of common patterns of
parallelism that we find crop up all the
time in the Paragon community one very
common one is a so-called task farm what
happens with the traffic farm is you
have a number of inputs and the inputs
are divided amongst a number of workers
each of the workers can work in parallel
and then the results are collected up
and passed on ok the very simple I kinda
skousen turns out to be very very common
this way you can replicate our operation
we've got some f you can replicate it
another type of common skeleton is a
pipeline and with a pipeline what
happens is you're familiar with these
things from undergraduate days I expect
are you have some input coming in some
operation s works on the input produces
some result in an intermediate form that
gets picked up by a second stage in the
pipeline a tree that does its stuff and
produce the result and what nice here is
that fmg can both work in parallel
provided you have some kind of buffering
in between the two pipeline stages so
they don't bump into each other and they
can operate at the same time so this is
another common scholarship I turns out
that there's a bit of theory from the
parallel community that says if you've
got our pipelines and farms are you can
do everything well actually we think the
theory says you can do almost everything
we showed you to do a bit more than they
could do in our previous paper so i'm
going to be example what we're trying to
do I've got a little function an image
merging function and what an image merge
dance is it works over a list of pairs
of images so here's the type here thinks
the left of the arrow is the input type
list of pairs of images and what we're
going to do is they're going to give as
a result a list of images
sorry John that's not Haskell it has go
like okay I hope you can forgive me for
that so let's say as ben is coming in
this student is coming out and what we
doing what we're doing is we're mapping
over all of the input list in one after
the other and we're applying first mark
operation and then emerge operations got
the sequential composition of a mark and
merge operation now it turns out that
even given this incredibly simple
definition there are many possible
parallel implementations just using
farms and pipelines I've listed a few of
them down here so image mode one you've
got to farm over some function of the
merger Lamarck vocalese it the merchant
mart together and some base
functionality and then i filmed over it
now got any copies okay so i can have
endings working on the inputs in
parallel really simple or i could have
our two farms working together I've got
farmers and fight and apologize Emma
they don't have to explain sighs they've
got n workers on the mark stage it
pipelined into n workers on the merge
stage and so on and so forth and the
question is which of these is the best
thing okay and that's the question we're
trying to answer but first a digression
why do we care about types are making
kijan to tell me something well
according to the types community we care
about falmouth well type programs cannot
go wrong whatever wrong means and
there's a whole PhD theses on that and
documentation which is the type thing to
provide valuable documentation to the
programmer but what we really care about
from my perspective is that well tight
programs can be parallelized as
described by the type there's an
operational interpretation of the type
that tells us about the execution of the
program
and moreover the type level parallel
structures clearly separate out what's
happening in the parallelism from what's
happening in functionality thing John
was saying earlier separates two things
specialism the types tell us about the
structure of the parallel program and
the code tells us what the program is
doing simple isn't it so why has nobody
done it before and finally the great
thing about types is are we don't have
to do it all ourselves because some very
very clever people who like solar and
other people have come up with really
great our unification algorithms and
type system so we can just pick that up
Edwin Brady with Idris they can pick up
all that clever stuff and you've it and
we don't have to build our own stuff
fantastic I'm always in favor of
stealing other people's work well i can
so i got my image merge function and
what i want to do is to paralyze it well
how am I going to do that well I'm going
to do is I'm going to decorate the type
the function type with some herb I am
n/m from caveat and thats going to tell
me a bit about the parallelization
there's nothing red is now the
parallelization a bit like a trike cloth
which is familiar with those in Haskell
so if i define i am NM to be that stuff
in red the farmers n over some function
of type A and farm of em over some
function with author of type a then what
I've done is to define one possible
parallel implementation simply by
specifying the type and now I can
automatically use the type system to
rewrite my source code to introduce the
parallel execution that I want all I
have to change the type click we've got
a parallel implementation that
corresponds exactly to that parallel
structure or you change it some other
time and and you get some other
implementation and we can guarantee from
the underlying law or functional program
but what we've got is functionally
equivalent the original parallel version
okay if you've done any parallel
programming you realize just how hard it
is to know pregnant you end up with is
the same as the one you started with
race conditions they're not all that
stuff but we know because we are
functional and we have laws so how do we
introduce our parallel patterns well
what we do start on the top left we've
got the original program structure or
you might think about being sequential
but it doesn't actually have to be
sequential does what you've started with
if we normalize it into some canonical
form we can then rewrite the canonical
form according to laws which are in the
ifp paper and the upcoming FG CS paper
and then we can go backwards and we can
end up on right hand side with a
parallel program that we want ok so the
type system is driving all of this
transformation the track system says you
go from here to here that means there's
a rule there the corresponding rewrite
rule and the type system can't put form
that rewrite for all for us using its
normal mechanism so question how do we
decide when two things are the same well
I mentioned hilar morphisms that start
we use high-low morphisms which have
strong laws and properties so our hila
morphism is a generalization of a divide
and conquer it's got a highly morphism
over a G and an H then that's defined as
doing a chi applying thumb f and then
doing the H what's happening is that the
G part is a split part in the divide and
conquer the f is the operation you're
applying and then the H part is doing
the combined part in your divide and
conquer so hina Morstan is simply
generalized divide and conquer
and it turns out that these skeletons I
described to you can be defined as
instances of Pilar morphism which is
very cool so here's a little example
quick thought if you start off with a
height key and the key is the three
parts that you get when dividing a quick
thought so you've got the pivot you've
got a left sub list and the right sub
list less public type list of a right to
uplift also a list today then we can
find a quick sort to be a hyna morphism
of emerged and of split where the merge
is going to take the results of the
thought and put things together and
split is going to start off with our
original list and split it down they've
got pivot and the to sublet now we
completely assigned a quick sort
algorithm just using a high-low morphism
cool if we now start with the streaming
sequential version of this so we're
mapping that over a list of inputs what
we can now do is if we want to attach
the version using task farms and
pipelines we change the type this
special bit on the type to say oh well
it's a parallel implementation with a
farm pipeline with something else no
sighs that's an underscore in type to
indicate there are holes there we don't
care in our type what they are will let
the type system figure it out that's
what time systems a good for good at
after all they're good for figuring out
stuff I don't want to have to think
about it's gone or if you want to create
a divide-and-conquer version all you
have to do is change the type again so
all we have to do is to say working on
two parallel implementation now you're
just divide and conquer algorithm rather
than the pipeline and farm version and
we can see trivially because hilum
orphan is really a try and conquer
generalized of course singing it divide
and conquer
unlike white with the farms infection we
define a base semantics for our programs
using some well-known recursion schemes
these are things that I hope everybody
uses our everyday in their programming
the mat fevers map good fault okay uncle
AHA of all I expected plus a sequential
composition the map folds unfold these
are very common pattern that we find in
our software it turns out that you can
find the semantics of the operations who
want in terms of these based things so
our DNA tional semantics I've abstracted
here the interesting things to show you
all that a pipeline simatic for pipeline
is just the semantics of composition to
the second line down my semantics there
which is composing two things get what
happens functionally these masks are
dragging conquer is simply the
composition of a cat and morphism that
is a unfall fourier fold or get was
wrong with an Animorph ism that is a
fault an armful operation the first are
unfolding then we're doing stuff from
them were folding that's the divide and
conquer as you'd expect and then to lift
these things up to the power level as I
shows you what you do well you put in a
map and now we're streaming over a
series of input it's not good parallel
so if you want now to work out our two
parallel programs the same all we have
to do is to erase the parallelism from
the original program work out what the
sequential structure is transform
sequential structure by saying well
we've got some form in terms of hilar
morphisms let's transform it into
another form of compositions of Hyla
morphisms reverse that by top to the
parallel
we want and now we can just
automatically transform one parallel
structure into another I turns out that
recursion schemes are highly more for
them so all these current schemes maps
Kaz morphisms animals seasons these are
all instances of generalized hina
morphism divide-and-conquer-type things
where you might put in an identity
operation for example in the animalism
because you don't really care about that
part of divide and conquer a Highland we
had a Eureka moment in my office when we
discovered this it's Turtles all the way
down so as I said we can now what we can
now do is to leave some holes in the
type underscores of holes that we want
to be filled in type inference mechanism
tight unification is going to replace
the underscore with any suitable
parallel structure but if we've
specified have to be unified with the
actual type then the underscores where
we can just fill those in or we can
actually write type level functions if
you're attentively types person you'll
see this is easy to do so we can for
example writer type level functions that
said well given this structure a
pipeline with something first and then a
psalm what is the least cost structure
you can infer for that and the type
system will go away it will do it stuff
and it will use a dependent type or or
other type inference mechanism and it
will say to it yeah okay here's the
operation that gives you the least cost
and all you have to do and this is the
topic of our new paper is come up with a
good cost model that job
and that's what our new paper does what
it does is to build a model for
skeletons in terms of Q structures and
explain how you can come up with
amortized costs for each of the
operations so here's an example while
I'm doing here I've got cost of the farm
and what we're doing basically is
replicate the farm and China the Q
capsule queues are the input and output
cube in each case of what we're doing is
we've got an operation p it started with
q0 q1 and basically replication enzymes
original input queue original output Q
merged somehow by the implementation
we've then got n copies we can work out
what is the cost of each PQ the base
it's time to do the operation pretty
over some DQ time takes thing off the
input in q time put the results on the
output queue we've got any of them then
the amortized cost is just that divided
by n okay import
but does it work okay now if I was
serious and doing my PhD on say yeah
okay back there Don problem solved I
don't need to do but hey I'm really I'm
really a practitioner pretending to be a
theorist so let's have a look at what of
this does we've got here is we've tried
some example this is a simple matrix
multiplication running on a 24 core rmd
option machine from our colleagues in
italy and the solid red line is the
predicted or is the actual performance
the dashed red line is we predicted
performance in terms of speed up on
these call you feed that we are getting
something like a 20 17 times V dot on 22
cord and we're projecting something very
great simulant and we tried this for a
number of examples it's not just one
flash in the pan there's another example
bit more complicated structure we've got
here the pipeline double bond pipeline
to form over function and we vary the
number of workers VN 1 and V n two in
each case and watching sees again you
the dark hot mole is a very very solid
prediction of all the comp we're
guessing if it's a very good predictor
are in this case moreover each time the
lumber we're predicting is below the
actual number so we have a high degree
of confidence that we're not over
predicting giving you something that's
realistic and you can rely on pretty
good or if you have different structure
the step state the structure radar
matter are on the same program well
again we can predict the performance and
the astral form for each leaf flutter
and you see that in this case the
prediction is almost one on one with the
actual performance we observe so we're
getting good prediction for different
structure that's great good with that
the same thing our cost model would be
pretty rubbish
and this another example and I'm send it
up here to 64 peers and what rats do is
an image convolution more else do is
click that forms about 56 times on a 64
core machine for our example and we were
able to say to you well this thing form
over a pipeline actually that isn't very
good clearly going get about 23 24 times
speedup and hey it's going to tail off
okay so we're getting some real usable
predictive power able to show you water
the best solution completion so driving
costs parallel structures from la
prismatic incredibly powerful you can
automatically derive cost equations from
an implementation in terms of our cues
etc now if we show in the paper and we
getting compile shine information about
runtime behavior based on a simple and
easy to understand model using types to
expose the structure and when you put
this together with the work that we
previously don what happens is you can
automatically rewrite programs to
minimize costs for a parallel system
under tight control just changing the
type we accurately predict lower bounds
on speed ups and we verified that
empirically and we can choose between
different alternative parallel
implementations that use this reason
about the parallelism in your program
work out which one is best in a provably
correct way so you can choose different
passions the implementation or you can
say well actually say I wanted some of
the professors to be CPUs some others to
be GPUs or say I had many many cores all
of them very lightweight and a few big
ones which are much heavier weight
what's the best implementation in that
situation it's research there's lots
more to do we've shown you some some
important common patterns but a lot of
other ones people are interested in to
things like stencil bulk synchronous
parallelism what we would like to do
show that essentially all of the
interesting parallel pattern
the people come up with over the last 40
years or so our can be described in
terms of pilot morphism that'll be a lot
of fun we might need to investigate even
more general recurring patterns from
hilar morphisms so maybe adjunct fold or
conjugates hilar morphisms of heaven's
sake don't ask me what they are to read
wrote that and what we really want to do
is to take this stuff which I've
described fairly abstract see in a
prototype and we want to apply this to
real language Haskell Erlang etc etc the
work is sponsored by an EU horizon 2020
project but we're running our website so
I'm coordinating this project are
halfway through it our websites on the
front page of slides child give the
audience and I'm very pleased to say
that we've received on 500,000 pounds it
look more impressive if students lotted
the two and a half million zlotys to
look into commercialization of these
things and what we're doing at the
moment initially if we're looking at
commercialization for c and c++ because
we think we can make a real difference
to those guys we think they really
really really need our help and we have
good functional technology that will
help them and they're coming in our
direction to c++ isn't an
object-oriented language it is a
multi-paradigm language with functional
features they're moving towards our job
of moving towards of scholars definitely
functional right this is future swiftest
optional the future is functional guide
thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>