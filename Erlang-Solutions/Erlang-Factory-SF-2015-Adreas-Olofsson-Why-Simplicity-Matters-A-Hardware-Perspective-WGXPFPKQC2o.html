<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Erlang Factory SF 2015 - Adreas Olofsson - Why Simplicity Matters: A Hardware Perspective | Coder Coacher - Coaching Coders</title><meta content="Erlang Factory SF 2015 - Adreas Olofsson - Why Simplicity Matters: A Hardware Perspective - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Erlang Factory SF 2015 - Adreas Olofsson - Why Simplicity Matters: A Hardware Perspective</b></h2><h5 class="post__date">2015-03-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/WGXPFPKQC2o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">this talk is part of my seemingly
endless crusade about simplicity
simplicity and program simplicity in
hardware and hardware is not free and I
think you know the greatest magic trick
of the last few decades has been chip
vendors and hardware engineers making
the hardware disappear it was always
there but it just felt like it
disappeared so now the the bad the bad
news is that the free lunch is is kind
of over and some people realize this ten
years ago in certain systems but for the
most part if you're looking at the cloud
or doing something is not super critical
in terms of power and area you may not
feel it there's still an abundance of
hardware but basically the effect is
that frequency stop scaling ten years
ago you know Intel started stop pushing
gigahertz there was an ITRs roadmap in
the late 90s that said that we were
gonna have like a 20 gigahertz processor
by 2015 that didn't happen frequencies
flattened out power if they would have
kept pushing power the way they were in
the 90s and the 80s we would have had a
you know two kilowatt chip today but of
course there's no way to cool that so
that's not gonna happen
ILP instruction level parallelism the
idea is that we can just have these
SuperDuper hyper-threading dynamic
schedulers and it'll just schedule
everything automatically like an auto
paralysing hardware that didn't happen
either and so what we see here is that
everything kind of flattened out now so
what are we left with how are we gonna
get more performance because 80s 90s
2000 you could you know you can set your
watch by Intel's progress basically you
know double the performance every 18 to
36 months and so if you had a program
wait for the new chip recompile or or
even run the same binaries it will run
faster and so this is now over so now
who would care I mean actually this is
the
is a very complicated question I figured
that when I started coming 2008 I
figured there's no such thing as too
much performance everybody always needs
more performance but what I've found is
that in seven years is it's become
harder and harder to find those special
applications that need that performance
because processors today on the desktop
are incredibly powerful I mean if you
have it let's say I have an INT 11
quad-core running at 2 or 3 gigahertz
that's you know the equivalent of
running you know 10 or 15 billion
instructions per clock cycle you know
you can probably do most programs
certainly email or web browsing or
something like that will work just fine
with that many clock cycles and if it
doesn't something's really wrong but you
know there are applications are needed
they just don't exist yet and so so you
know search kind of far and wide you
know what are the things that that we've
located and I've seen that the people
need more performance but not just more
performance more performance that less
energy that's the interesting problem
and so one is communication yeah it is
that communication is still exploding
right it still Expo at an exponential
pace when everybody wants their
individual stream right so it's not TV
or radio anymore where everybody are
casting everybody wants a Netflix stream
to their device right so now you have 9
billion people everybody demanding a
complete video stream I know that's kind
of the peak now so that's one stream but
you know what what as we get to the IOT
phase what if you now have 50 billion
sensors that are streaming data so so
that's you know every every
communication camp will tell you that
it's still exploding and yet the boxes
can't get any bigger if anything the
boxes should get smaller because you
need many more boxes to have that kind
of capacity so if you have a certain
size box you can only get so many watts
out of it that's physics right that's
thermal thermal laws so if you want to
do more work in a certain size box well
then you need to consume less energy
work same thing robotics there was
actually just an article came out this
morning about how we're gonna make
robots smarter well we can use the cloud
that's a really bad idea I mean back
then this actually came out of somebody
at iRobot that said that that yes the
solution is today the robots are stupid
because we can't cram enough processing
into the robots right it would require a
whole cluster and so what we're gonna do
is going to set up a radio communicate
to the data center and then get do the
crunching and get a response I don't
really want the robot that has that kind
of latency in terms of his response time
so it has to be local so we want to put
a cluster inside a tiny device whether
that's a you know a a robotic for
assistance that's on two legs on four
wheels or on the drones so drones let's
say you want to you know you it could be
anything I mean drones it's pretty hot
right now it could be a
search-and-rescue could be farming it
could be news reporting pretty much
anything and the idea is that you have
these very lightweight drones but of
course physics says that you you know
you have to liftoff right so you can
only have so much payload and let's say
we put the payload at 100 grams hundred
grams is not a lot of processing you
certainly can't fit a modern heat sink
and a you know 200 watt into the
processor on it you need something that
weighs nothing and so that's that's
another great one the last one I'm a
little bit ambivalent about I think
there should be a huge need for more
processing in data centers DARPA
stalking about exascale computing Google
Facebook everybody is you putting huge
data centers the truth is nobody really
cares about energy efficiency in those
domains they talk a lot about it but if
they really cared about it they would
they would change the whole architecture
now of course the problem is that the
software infrastructure today is so in
so intense and so big that there is no
way to do a clear-cut switch I mean for
example what's the difference between a
1 megawatt data center and 25 megawatt
data center that's the difference
between running a city right or having a
data center
and so but the the what you know again
seven years of trying to come up with a
more energy-efficient computing solution
it all comes back to the software and so
whereas the first three ones there is no
choice physics says you cannot build it
right you cannot build a robot as small
enough a a sensor a drone the data
center you can always build a bigger
data center if you have enough money and
build a nuclear reactor to power it it's
it's a solution it's a very poor
solution but it is a solution you cannot
you cannot cheat physics so so how many
people here have no hardware design
electro engineering ok cool so so this
is like engineering 101 for for high
school right but let's not let's not
forget it so the the most important
thing is power and this is just one one
part of power the top one but the idea
here is that you know how do you affect
power one you can bring the voltage down
but that's come tough beyond a certain
level because that's you know you're
getting down to things like thermal
noise and and thresholds and solid-state
physics where you cannot really bring
the voltage down too much or your
surgeon will start failing of course if
you can bring it closer to that edge
you'll have circuits that fails
sometimes and sometimes don't but then
you'll you'll gain a lot of course you
need a programming model to be able to
handle that so you can you can play with
the voltage you can bring the frequency
down but that's actually not that
interesting because if you bring the
frequency down you also bring the
performance down so it's you've got a no
net game and the last one is capacitance
right so if you look at the the
right-hand side CMOS circuit right which
is a staple of all of chips for the last
30 years when you wiggle capacitors you
basically burn power and so so what you
need to do is you need to reduce amount
of capacitance so how do you do that you
reduce the size of the devices so you
scale down in process but the most
important thing is you lose
less devices less transistors so the
less transistor you can use the less
power use and so the whole thing becomes
using less silicon that's the whole goal
and that's from storage right so
whenever you have a bit of storage this
is what you're using
whenever you're you know doing a
computation or control you're wiggling
some gates and the key here is the price
of hardware is not zero it's it's
getting smaller every year and has been
but that is also saturating because you
know Moore's law is ending and there's
we can't go down to zero nanometers
right there is gonna be a physical limit
and so at the end of the day every
transistor has a cost so this is a five
minute introduction to very log so I'm
gonna teach you everything you need to
know about very long in five minutes and
this is pretty much all the syntax
I used to design I can design a thousand
court shape with the syntax
I mean it's trivial right so first one
is is is module right this is the basic
level of hierarchy nice so what you do
is you you define a a module with an
interface there is no magical leaps here
everything that you want to interface to
this module has to be an input or output
it's a physical is based in physics so
you have pins on this device but now
once we define this this module we can
instantiate once twice or a thousand
times across the chip so this I do
modularity and reuse and and this is you
know again by having many many layers of
hierarchy with with combined with
symmetry and architecture you can
actually design a very very large chip
very very easily and there's a lot of
parallels here between software design
you have to declare your wires right so
because the wire is you know connecting
two points right so you should be
declaring it a register is a storage
point a piece of memory one bit and of
course you know in in Hardware and
boolean logic every bit has to be
specified there are no no fancy types
right there no integers no
floating-point values there are bits
right and so and all the assignments are
are explicit so you know and also it's
no such thing as byproducts or of you
know you say a is equal to B and C and
and that one of the coolest things about
writing relic code is that there is no
order to your code you can have ten
files with a sign statements and they
all happen at the same time right so you
can have when you look at your code you
can move code up or down the page
there's no top of the page at the bottom
of the page it just happens that
everything happens at the same time and
so how do you then order your design
will use this something like this always
at which always add event do something
that's how you do it so this is very
much event-driven design and the the
master event and most designs today is a
clock so you say always at paws edge of
clock do something and then of course
you can you know to make some things
simpler you can have you have case
statements if-else and other things and
of course you know a bunch of operators
to do all the boolean logic that you
would you would imagine but it's it's
not it's not as difficult as as some
people make it out to seem hardware
design is pretty easy and fact I would
encourage you I'll show you something
about the parallel board later I would
actually occurred all of you to if you
have a parallel board if you don't you
buy one but if you have one it has an
FPGA in it right field programmable gate
array so you can actually synthesize
your own Hardware wiggle some LEDs
wiggle some pins and and see it work
it's yeah I think it's liberating right
liberating you see what the others how
the you know how the other side lives so
so this I've been doing chip design for
for since 96 96 so almost 20 years now
and and especially this came in last
eight years things I've learned about
the future of hardware from building
hardware
and and number one is obvious massive
parallelism that comes from the first
slide I think it's indisputable fact
nobody's gonna argue about that the
second one am does law is the law of
diminishing returns is you know if you
really want a million times speed up or
a thousand times speed up there cannot
be any serial component new program zero
right and and this applies to so many
things right if you want a thousand x
improvement in energy efficiency right
you need to improve all components of
the system by a thousand X and so so
what that means is you know the biggest
lesson for for the ambulance law is that
this whole idea of having a threading
model right where you have a one master
right that then pushes things out and
then collects results coming back like a
fork/join model can never work it's just
it will never scale beyond a certain
number of threads so you need you can't
have this coprocessor domain either you
need something where you unlock that
Amdahl's is dragging around those
bottleneck the third one thermal density
the smaller would make the chips
the harder they gonna get per per square
millimeter and that that means that
we're gonna have a hard time running
them at the peak performance and that
just comes down to we run it using CMOS
logic and we saw the first equation
power depends on frequency and and
capacitance right so as we scaled the
devices down to 40 nanometer 10
nanometer capacitance goes down somewhat
but the you know not as much as we would
like and so things get per millimeter
hotter and hotter and it gets harder and
harder to extract the heat out of it so
we something called the the dark silicon
phenomena and and so what it means is
that we're going to have to start
turning on things selectively on the
chip and in software right or in an
operating system a runtime so as not to
make the chip burn up failure rate this
is very natural as we move down to small
and smaller geometries we get closer and
closer to quantum mechanics and we have
a harder time we're not in
kind of macro physics in where we have a
hard time controlling whether things
fail or don't fail so things will start
failing and at the moment people know
how to do robust systems at the macro
scale at the clusters by using air line
at the chip level people still assume
that every single bit will be perfect
and that needs to change drastically and
right now the we started seeing this a
28 nanometer where there was some
back-and-forth between silicon foundries
and chip companies doing microprocessors
and they were kind of pointing the
fingers say no no your your your process
is bad no no your design was bad that's
the way it wasn't yielding and the truth
is that we cannot expect a silicon
family to build perfect devices and so
so the idea is things will start failing
get used to inside the chip and at the
system level the number five the i/o
bandwidth chips for the foreseeable
futures are still gonna be planar and
i/o is mostly going to come out of the
sides and so things are gonna be our
limited you have to conserve how you
move data there will be 3d scalar
scaling we this is very clear the
economics of it have have said that it's
too costly to do it but now that Moore's
law is slowing down there's more
incentive and people are working on it
so this is this is a pretty easy one and
the systems that are going to be built
are gonna be amazing
because if you look at if you look at
today's systems and and you look at a
motherboard look at an iPhone or a
MacBook I mean they are very very dense
but still then the amount of silicon on
that board is less than 10% 90% is board
space and connectors so you can imagine
making even denser systems by stacking
energy efficiency if you were to take
just a thousand Intel processors x86 put
them on the chip well one you couldn't
do it today but let's say that you know
five years from now maybe you could but
it would be a monster it would be very
energy-efficient so the guys working on
the draw
and the robots they could not use that
system right they need something that is
much more efficient so what we start
seeing is heterogeneity right so we
start seeing specialized processors for
different functions and so that's how we
overcome the saturation of Moore's law
in that if process doesn't give us a
free lunch anymore
then we have to be smarter about the way
we do the work and and this is gonna
make programming them much more
difficult because you now you don't have
this homogeneous processor framework
maybe SP where you can just kind of
write your code and just works now
you're gonna have to program
accelerators big processors small
processors keep track of things with
special instructions just so we can
reach those performance targets and
productivity right so it's at least to
me it seems unlikely that there will be
one language that can that can rule them
all there will be a combination of the
main specific languages communication
languages computation languages that
needs to be master in the system and
number nine right so the program has
gotten easier and easier for four
decades now and now it's gonna get
harder I mean at least to me it seems
pretty clear now the good news is that
compared to the let's say the 80s when
program was very hard where you had to
watch how many bits were using in a
program maybe program and assembly but
everything was quite proprietary we
really there was an open source
collaboration today with with all these
concerns that are coming up in
programming things are going to get very
very difficult and but the good news is
that we now have a much more robust open
source collaboration model so that we
can move forward faster together
so what is the architecture of the
future so there's three programmable
architecture the futures the three main
contestants right now if you see around
the industry one is the kind of ESP
model right so I've drawn here a blue
box for memory red box for a
floating-point unit or computational
unit and
for memory just a kind of a cartoon
mystery illustration so SP right this is
the traditional Intel world for those of
you like Intel Xeon processors this is
it right you get four cores eight core
16 cores now Intel is working on an x86
with 64 cores I think or 80 cores but
the idea here is you have coherency
right so you always know that your your
memory hierarchy is intact and that's
the SMP world right then you have Cindy
Cindy is more like a GPU where you know
you have tons of computational units and
you only have one sequencer so everybody
works in lockstep so you write a program
like add up to vectors right so you add
up vector a and vector B and that vector
can be you know 32 wide a thousand wide
but the program model is everybody works
in lockstep and of course and then the
third one would be well let's have a
completely distributed system where you
have tons of processors connected by
some kind of network and each processor
has its own memory - its own prophet a
scheduler and its own computational
units and there is probably going to be
a some combination of all right so the
Cindy in GPU is not really completely
Cindy there's some independent scheduled
nadir of the threads the SP of course
every SP machine has some kind of Cindy
inside right maybe a neon or a VX and
and you can certainly combine some of
the Cindy concept and distributed
computing as well but those are those
are some of the kind of directions that
people are jealous ting with right now
and there's a lot of buzz and hype as
you could expect from from chip
companies so so this is the architecture
that that I came up with in 2008 and the
idea was just distributed architecture
and very very simple and this is you
know from a designer's perspective it
was all about designing
one small tile self-contained
encapsulated that contained all the
functionality including memory compute
and networking and then step-and-repeat
that tile to infinity and so the the
idea was there's nothing shared in the
system sound familiar
nothing shared so you the way you
communicate is bye-bye ideally by
messages so you would have a 1:1 process
running on this core maybe another
process running over here and to
communicate with two of them you would
you would pass messages the architecture
itself is actually a distributed shared
memory architecture and that the reason
for that was to not to to have a to
actually do loads and stores from other
cores at abandoned but to open up for
different programming models right to
say what's the easiest way to pass it a
piece of data from here to here well how
about a memory transaction and we can
build messages on top of that in
software so so we built a chip in 2011
with 64 cores this was a prototype and
it worked beautifully reached a hundred
gigaflops two watts of power consumption
and some pretty impressive skating specs
I mean 50 gigaflops per watt is still by
far the most efficient ship in the world
today and this is 2015 and that was 2011
and my only amazement is that it hasn't
I mean we're doing well right but that
it hasn't taken over the world yet and
I'll get to that part later why you know
why wasn't a more interest in something
like this but but the chip itself 64
cores in a mesh network with high-speed
i/o to connect to to a main main
processor or other chips so coming down
coming back to physics right so when you
look at the we look at the specs the
data sheets and the performance it's
it's easy to get lost but the the true
number that you should always be looking
for is how many square millimeters of
silicon is a chip using for a certain
amount of performance that's really hard
to cheat if you normalize it on the same
process node and so and this is not you
know this is not meant to say that one
core is better than the other but the
point is that when you look at a core a
CPU core you can have anywhere from a
monster like the Intel Haswell a very
powerful core but that burns a lot of
power down to a tiny Epiphany core which
is you know not as powerful the Haswell
but still pretty decent runs a 1
gigahertz and but is you know one
hundredth of the size so you know what
what can be done with that right and how
far can we go how simple can we get to
so just a couple of wake up calls
so Vannoy Ayman specified long long time
ago one of the many things he did that's
Richard Fineman up there who said this
plenty room at the bottom but why no one
specify that you know what is the energy
needed to move one electron right and so
it's it's you know 10 to the minus 20
joules a tiny tiny amount of energy what
did we use today to wiggle an inverter a
28 nanometer something like 10 to the
minus 15 joules so it is you know five
orders of magnitude difference between
what is the best-in-class limited by
physics energy and what we do today with
these kind of very robust let's flip a 1
or a 0 complimentary CMOS so that's a
that's a big that's a big difference so
we know there's still you know five
orders of magnitude difference so five
orders of magnitude that's you know that
could make my my iPhone last for longer
than 12 hours right so it's it's a big
deal
take another one like look at the the
biggest supercomputer in the world today
at TNA 2 in China 33 petaflop 24
megawatts
that's a pitiful number right
that's one gigaflop per watt for a
system right so they are now they have a
either you know cold plant or something
like that powering this computer and you
can do better right we can definitely do
better and when you look at that thing
how you know this is now a machine that
takes up a big room how small could you
get it that could you make it so if you
look at the actual silicon right 12 inch
wafers
how many wafers would you need to
produce that kind of performance and
it's it's sixteen twenty eight nanometer
Frisby wafers right so kind of like that
frisbee that's the size of the silicon
performance the raw size now of course
even at fifty gigaflops per watt that
would still consume a fair amount of
power right so you would you couldn't
possibly draw out hundreds of kilowatts
out of a frisbee
it's too thermally dense so you'd have
to make it bigger but just in terms of
silicon efficiency there are so many
orders of magnitude here and I'm not
talking about some kind of theoretical
math you know physical calculation I'm
talking about things we can build today
to what actually people build so there
it's an enormous advantage and just I
mean this actually works right so we've
had we had customers and partners that
have shown that looking at real work
real benchmarks that this many core
domain beats the traditional Intel
processors about 25 X on energy
efficiency things like FFT things like
password cracking and other compute
intensive things and kind of cool right
he can even be programmed by students so
a bunch of students at Australian
National University took it on and ran
benchmarks stencil benchmarks matrix
multiplication FFTs on on the 64 core
device and they got to 85 percent of
peak performance and so I you know this
is not this is not the experts at
Ericsson this is not me
who knows the assembly this is students
off you know off the street that the did
this so I thought that was quite it
quite a nice one but
so we have some good good good things
that happen but we weren't winning over
the massive software community and
really very few people knew how to do
the parallel programming so we started
this thing in 2012 called the parallel
board and it's I always walk around with
one in my pocket but it's a it's a
credit card sized computer kind of like
the raspberry pi and the idea was really
to do for parallel computing the
Raspberry Pi is trying to do for
computer programming in general for
everybody and before we did this we were
selling our evaluation kit for thousands
of dollars because that's what they cost
to us we couldn't give them away for
free we didn't have that much money and
we also slapped NDA's on them right so
anybody who had to work with us had to
sign an NDA so that was like a perfect
filter for keeping up everybody was
interesting and so so a 2012 which was a
scary step but we also we did this
because that's what everybody did
everybody every parallel processor
company we'd seen that came before us
that's what they did they sold their
kits for ten to twenty five thousand
dollars everything under NDA only worked
with the biggest companies and so you
know we just did what everybody else did
2012 realized that was dumb and we
should do something new so we bring the
price down so everybody can afford it
open up our documents or architecture
reference manuals open source all the
drivers all the libraries and and we
launch the singer Kickstarter and it was
it was it was pretty amazing so we
raised about nine hundred thousand
dollars in 30 days and we took us quite
a while longer to ship them that we
wanted to but in the end we shipped
10,000 units sixty three hundred to
Kickstarter and another thirty seven
hundred to two pre-orders all over the
world including two hundred universities
so now these things are out there so
it's basically one per person so you can
imagine roughly ten thousand developers
who want to do parallel programming on
the on this device so just to put that
in perspective right you look back 20
years right 1993
in Boston the cm5 at the time the
world's most powerful computer it cost
about 30 million dollars and 136
gigaflops at 100 kilowatts right so this
was a beautiful but massive computer
today 2014 the 64 core computer 199 and
about the same performance so in 20
years our our computational performance
has stay the same in these two cases but
the cost has gone down by many many
orders of magnitude which makes this
kind of performance accessible to
everybody but you know 64 cores is cute
right it's but it doesn't like joe said
he wants a thousand course and so we
have a big roadmap so this is the this
data points are anchored on on our
devices today so I talked about silicon
efficiency right so if you know how much
silicon you use a twenty eight nanometer
then and you know what the maximum size
that you can do then you can figure out
what's the biggest device we can make
and there's no doubt in my mind that
today it's possible to build a thousand
core device anybody could build that
today and in 2018 we're gonna be able to
build a 16,000 core device so that's
that's I consider that a fact we'll add
a tube you'll be able to build it I
don't know it depends on a lot of
economics right if we do well maybe we
go out of business
we don't survive but I think it's a it's
an imperative that somebody needs to
build this and you know if it's not us I
hope it's Intel or somebody else but
yeah so so when we had that many cores
you know what does that mean for the
programming model what does it mean for
for all of the software infrastructure
and so so this is something that I am
now working on I can't
I was hoping to have more details about
it when I came to present but this is
pretty much all I can give today but I
you know we are working on something so
it's a it's a thousand cores on a chip
and this is for this year
and one gigahertz operation
32 terabytes per second of local memory
bandwidth and yeah one terror messages
per second on chip that's that's a lot
of messages and yeah all of this in
about ten watts
so now what what are these specs mean
well it's you know this is not a GPU
right let's just be very clear because
the GPS today have specs that maybe get
close to this but this isn't again a
thousand twenty four independent RISC
processors they can all run a complete
task you can have a hundred thousand
independent and separate tasks here
right so I think if nothing else it it
shows what what can be done so so what
you know what is programming such such a
monster look like well there'll be some
new things one when we build this
thousand core chip not all cores will be
working there will be some cores that
will not be working on a manufacturing
because we can't reach the yield so
you'll have these blackouts box here and
so when you when you think about the
runtime environment of the programming
if you assume that that all cores are
working you're gonna be gonna be stuck
right because we won't be able to
produce it second one there will be
intermittent bit failures with this much
SRAM on chip it's likely that it will be
soft errors once in a while so what do
you do about that so in the scheduler
needs to be smart enough to never
schedule something to a core that's
never gonna work and smart enough to be
able to kill a process that got hung
right or the fact that that process that
gets stuck doesn't ruin everything else
on the chip so this is this is just a
fault tolerance robustness right so this
is this is the first thing second thing
clearly when you have that many cores on
a chip the traffic congestion becomes a
big deal
so in in the chip world in the 1980s and
90
we used to do everything by hand right
we did with the schematics we drew
little polygons to draw gates and
transistors and wires and then the that
was just so inefficient that at some
point somebody extrapolated that to
build the next Pentium processor after
the next one sometime in 90s you would
require every person on earth to do it
based on X number of transistors and
number of transistors per person per day
right it would require all people on
earth to build a chip which obviously
didn't happen because the tools and the
methodologies became better so the
productivity of a person went way up I
mean just for example I think that the
the the in the 90s the the rule was 25
transistors per day per engineer that
was the efficiency right kind of like a
lignes / lines of code this this this
thousand core chip
we're gonna do with three people and
it's gonna it's gonna have it's gonna
have about four billion transistors so
if you if you look at let's say six
months three people four billion that's
way more than 25 transistors per day so
so and and this is you know we've
already done this we did our 28
nanometer chip in 12 weeks with three
engineers and that was 200 million
transistors so so the productivity is
okay now the yeah so the one thing to
note here on the congestion right so
like I said the we start out with
schematics and transistors back in the
90s but then we the complexity gets so
big that we had to use machines tools to
do it for us right so people invented
EDA tools things that did automatic
translation from from RTL to gates or
the very log code that I showed there is
now tools incredible tools with millions
of lines of code that translate that to
super optimized logical gates CMOS gates
and no again the the parallel with
software right so in the beginning you
would have a camp that said there's no
way to build an EDA tool that can do
better at a better job than people
manually optimizing boolean logic right
that's kind of a weird statement but it
was absolutely you know the belief and
but the only problem was it was just
that the EDA tools hadn't caught up yet
right today
nobody would ever write you know logic
with gates you know you always
synthesize you're writing a high-level
language and you have a compiler that
compiles it same thing we lay out 10 15
years ago a lot of high end processor
companies would do manual layout to
compact the gates to get to make the
silicon size as small as possible
today the EDA tools the new place in
route are amazing and the complexity is
such that there's no way to manually do
it you need automation and so at 16
cores 64 cores you can maybe do the
manual threading manual communication
and manual placement of tasks at at a
thousand cores you need machine help and
so so you know there's at this scale
there's definitely need for a a place in
route methodology for placing tasks a
very very smart scheduling to minimize
congestion and and latency and energy
and so forth so today's chip design
tools all have these things built-in
right so that when you do you want to
run a chip design process you have knobs
that optimize for for power for timing
for area for integrity and so forth and
you're generally doing this
statistically over a certain range right
so you might be mid running Monte Carlo
analysis to figure out what's
statistically the best way to produce
this thing and we are gonna get the same
kind of uncertainties here which we've
never seen before because again until
today every chip was perfect and it's no
longer the case energy these these
energies are a little bit you know let's
take them with a grain of salt but
they're more or less correct it depends
on which process node you're in but
basically what it means is that there is
you know computation is one thing moving
data on chip is another thing
moving data off chip is a whole other
thing and so you need to minimize all of
them if you want to minimize energy and
the idea is don't move data unless you
have to keep it you know it means pretty
clear don't compute if you don't have to
but also don't move data unless you have
to and so if you want to minimize energy
you really want to communicate only with
your nearest neighbor not to the edge of
the chip and never off chip and and this
is this is not you know not something
probably most people think about today
if you look at a CPU based system you
have a cache hierarchy in there and the
cache controller is continuously
shuffling data around without anybody
really paying attention unless sometimes
you get cache thrashing and then people
realize there's a problem but it's
pushing data off chip all the time and
burning all kinds of energy outside your
control same thing with the GPU GPU has
such an incredible amount of DRAM
bandwidth to bring data in that you can
probably put all your data and a lot of
your data in DRAM and still get good
performance of course you're wasting
huge amounts of energy by moving data
all over the place but because there's
such a high level of bandwidth you can
get away with it at something like this
if you really want to minimize energy
you need to think about locality its
locality locality locality so future
programming is gonna be pretty rough in
my view it's gonna be really really hard
if if program was hard today cannot not
get easier right because now we're
bringing new concerns in that they're
gonna make it harder but I do generally
believe that that you know it's just a
matter of the tools catching up and if
you know again drawing a parallel from
hardware design every node that we did
write we start out at 0.18 micron point
one three ninety ninety meters 65
nanometers
every node we got a huge new concern
that we had to deal with and so at at
like 0.1 3 micron it was like voltage
real drops at 90 nanometer was signal
integrity at 28 nanometer it was
variability at 49 minutes
yield and so we have you know a DEA
companies out here in California sent
offices and Kate is in mentor they they
get it done alright so they you know
huge engineering teams that build the
tools that let the designers not worry
about these things and so to me there is
a enormous need here for the the tool
makers right the people working on the
programming languages and the SDKs and
and the tools themselves to figure out
how to make the programmer not worry
about the majority of these things so
the programmer can focus on the really
important stuff so I mean because
because this list is pretty enormous
right so if you look at most programmers
right one get it right get it correct
that's not that's not that easy
and then once you have it correctly
start mucking with the code and making
it faster and then you start breaking it
and spend a lot of time optimizing it
and that that's the whole thing now if
you have to add five or six other
optimization constraints psychologically
I know programs you can't can I won't be
able to deal with that right they're
gonna get stuck in an infinite loop of
optimization and for you know for things
that I soo complex a problem that it
really needs automatic tools oh so
I have one more slide
so so I this is my plea for help so we
are again I've been doing this for seven
years and I can and before I did it
there were you know tens of parallel
processor companies that tried this as
well
companies like the transputer right or
architectures like the transputer for 20
30 years ago
tell era in Boston recently and and many
many others and the success rate is zero
zero out of n have succeeded and I would
say that in the past there was a very
you know very good reason for it right
Moore's law was still marching along
Intel and others were producing
general-purpose processors every 18
months got better and better there was
there was no purpose for them now the
things are slowing down there is a
purpose but the software ecosystem is so
entrenched in the old way of doing
things that the kind of the the the the
energy needed to escape this this kind
of von neumann energy well is feels like
infinity right certainly a fini for a
startup company we can you know we can't
do it so so it's all about the software
and so we built the hardware we built
the boards it's out there last few
months we started focusing on right it
has to be the software we're not gonna
build hardware until the software
catches up to it
so the thousand core chip as sexy as it
may be it doesn't matter because today
we don't see that people know what to do
with 16 cores to 64 cores and so the
thousand core is just gonna sit there on
the desk and it's not gonna do anything
so we have a few things going on one is
this open source standard library built
for parallelism so when you think of
standard libraries and see they were you
know formulated in the 80s and they
don't have any parallelism built-in
right you've you compute on a scaler and
and and then you know as things evolved
you start getting POSIX calls from
mutexes and and threads and things like
that but nothing is is is built for many
core
the way you know you would do it if you
built it today so we started this effort
we're gonna keep working on our own but
we clearly need contribution there's
about 200 functions in there right now
pick a function contribute will honor
the pull request so please
VMs I I really hope there can be an
early on virtual machine running on the
Epiphany course the future course will
have 64 kilobytes which is a massive
improvement of the current 32 kilobytes
so I don't know what the VM is today a
couple of megabytes so you all you have
to do is reduce the size of the VM by a
factor of 100 or so no problem but if I
mean it would be incredible right so we
so I hope I hope there's some way to
either you know tweak the model the
usage model or the to get something
that's close one thing that that we do
have so we don't have Hardware caching
built-in because to be honest I mean I
didn't talk about that but when you
start having a thousand course let's say
you have a traditional thousand cores
with with a cache hierarchy you know a
kind of a multi-way cache miss cache hit
kind of thing what happens when you do
have a cache miss and you're going to
have that probably 1% of the time and
you have a thousand cores so you have a
thousand cores with a one 1% cache miss
ratio so they're all going somewhere
well where's the DRAM where is the
second level store here well somewhere
off chip and if if one percent of a
thousand core starts thrashing that DRAM
asynchronously your performance will go
to nothing and so there really isn't a
way to make it hard with caching
algorithm smart enough with asynchronous
process to make it work and so the only
way to do it is to have software caching
where it's under the control of of the
application and so so this is what we've
done and so we didn't put on harder
caching we put in software caching and
yeah so you know that may help some
programs that are larger than 64
kilobytes which there definitely are
some and and and the next thing is we've
had a very very hard time finding people
who speak our language meaning people
who know parallel programming when I go
into and talk to friends at Ericsson or
at government labs supercomputing
facilities absolutely no problem right
speak the same language if I go more
mainstream either you feel very
uncomfortable usually the question is
how we're going to program this thing
and usually within a couple of minutes
based on the questions I realize that
this is not gonna work out
right this is I we can't meet we can't
afford to take them through the learning
process of becoming parallel programmers
and so I think there that's where we
need a huge amount of help as a kind of
if we think of this as a parallel
programming community in in showing the
way I would say 99% of the program is
don't know how to do parallel
programming and yeah and and the the the
the last one I mean this is I don't know
what the answer here is but certainly it
would be fantastic if we could start
teaching parallel programming at the
base level before people pick up the bad
techniques right so so for example right
I program very log for 15 years and then
when we got the chip out I decided I'm
gonna do a parallel matrix
multiplication algorithm on our chip and
start writing in a C and I don't program
a lot in C but so the very log code
where any statement on the page happens
at the same time there's no order to it
and then I programmed this thing in C
and I was passing data around I
configure I was working and within five
minutes of going from barrel I see I was
reading the code like this right but all
the the code over here on this processor
was running in parallel to the code over
here so there was most no significance
to the order of it and so the idea is
that you know things happen
asynchronously is a tough concept to
teach and it's probably better to teach
it at the beginning so so can you know
can we at the high school level get
and and teach people the right way
instead of reteaching them after ten
years of bad habits or even five years
about happens if you go through college
so so yeah
so that's my that's my plea for help and
yeah by board okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>