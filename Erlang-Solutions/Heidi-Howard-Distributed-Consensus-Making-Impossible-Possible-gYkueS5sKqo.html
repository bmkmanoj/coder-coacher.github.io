<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Heidi Howard - Distributed Consensus: Making Impossible Possible | Coder Coacher - Coaching Coders</title><meta content="Heidi Howard - Distributed Consensus: Making Impossible Possible - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Heidi Howard - Distributed Consensus: Making Impossible Possible</b></h2><h5 class="post__date">2016-11-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/gYkueS5sKqo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">if you are looking for Dish treaty
consensus making the impossible possible
you are not in the wrong room I change
the title of the talk but the theme of
the talk is very much still how do we
achieve consensus something which you
know in theory we can show under certain
condition is impossible but in practice
we do manage to achieve it even if we
don't manage to do it in a very
performant way so yeah my name is Heidi
Howard I'm a PhD student in the systems
research group at the University of
Cambridge and I'm gonna be talking about
flexible paxos flexible Paxos is a
simple yet white very widely applicable
result about distributed consensus I did
this work at VMware research in
collaboration with Dalia and Sasha this
consensus is famously a really tricky
area something a lot of people have
difficulty with and I'm gonna be trying
to explain some of those subtleties
today so I'm really happy for people to
put their hands up and to interrupt me
with questions during the talk and I
made sure that the talk is short enough
so that we can do that so I'm going to
be talking about flexible Paxos there's
a website there if you want to look up
any of the resources the slides for this
talk are there as is the paper so when I
say dushman consensus these are the kind
of projects that often come to people's
minds and I recognize I think most of
you will recognize at least one or two
things on here we have things like
zookeeper which is a tried and tested
system that uses their to really connect
consensus underneath and then we've got
newer things like log cabin which is the
reference implementation for a raft we
have we heard from earlier today from
cockroach DB and people have been
talking about Cassandra which is some of
their components do use consensus
underneath then we've got container
orchestration solutions so these are
things like kubernetes docker swarm a
Miso's that also use consensus now
whilst this is a really broad space of
open source solutions the key underlying
idea of behind these things will come
from this paper so this is part time
Parliament this is Leslie Lamport
original paper is
it's an experience to read I'm not gonna
say it's particularly useful but it's
definitely it's definitely interesting
and it's definitely a fun insight into
something I'm not sure if that thing is
distributed consensus but to save you
reading it I will say this
basically what paxos says is that when
we want to reach agreement over
something and it's distributed system we
need to get the majority of people on
board and when you could do it twice for
the purpose of safety and I'll talk a
little bit about why we need to why we
need to do that why we need to have two
phases the key idea is basically we have
a first phase we often call it you know
leadership election or something like
that if we're talking about raft and a
second phase where we talk about what
value we're going to be agreeing over
and in both cases we need the majority
of people to get on board whilst this is
easy for us to think about in practice
we can see this isn't very scalable
right if you have to get the majority of
people to agree to ever anything it
doesn't work very well and the analogy
here is is government right we don't
have the whole population voting on
every single decision because that would
take a really long time instead we elect
someone to represent us and then they
make decisions for us and sometimes we
don't like them and they make decisions
for us British breaks it yeah so what
but what I'm going to tell you today is
something quite new and different this
is work that we've just done it's just
been accepted for publication and we'll
be seeing it coming out in the next few
months and the result is this we do not
need majorities to safely reach
distributed consensus the idea of
needing majorities introduced even
before Lamport introduced by Brian Brian
Oki in 88 has perpetuated this field and
has been absolutely foundational for
three decades but we have been able to
show formally verified and accepted at a
theoretical venue that we do not need
this this is a big claim I understand
and I'm going to talk you through the
details of it you know in many ways it's
a very simple result but I think it's a
really powerful result and why I'm here
today is to kind of share this with you
and I hope you know in future years we
may see people using this
in systems the other thing to point out
which has been a source of confusion
this isn't a new consensus algorithm
this isn't like a competitor to raft
this is actually all fog '''l to the
existing things that are there and you
can and I'm talking with people to apply
this in all sorts of systems whether
that be zookeeper console raft at CD
paxos variants etc you can use it in all
sorts of ways so the purpose of today
firstly is to understand the paxos to
actually understand the Paxos algorithm
it's not that hard
ish but do interrupt me and then we're
going to learn about what is flexible
paxos how are we generalizing it why can
I make this big claim that you don't
need to use majorities when like
everyone knows you need to use
majorities I'm going to introduce the
idea that now that we don't need
majorities there's a really broad
spectrum of ways that we can achieve
distributed consensus and from that we
can build linearizable and strongly
consistent systems and and thirdly most
controversially I'm going to argue that
majorities are not the best and they're
not the most practical solution for most
systems so this is a talk on distributed
consensus so what is consensus consensus
for the sake of this talk is the ability
to reach agreement in an asynchronous
dist distributed system in the face of
crush failures so asynchronous here
basically means real right you send
messages and they'll get there probably
at some point we can't put a bound on
how long that takes and the same with
the machine you ask the machine to do
something you ask it to write to disk
it'll probably go round to it at some
point we hope it will be quick about it
but we're not going to put any timing
assumptions on that we don't assume that
the clocks are synchronized either in a
relative way or an absolute way in fact
the only assumption we really make here
is that the participants in this system
do what the algorithm tells them to do
we can allow them for them to not do
that that's a field called Byzantine
fault tolerance and if you're interested
in that I can answer questions about
that later on but for this talk we're
assuming non Byzantine behavior so
machines act the way that we tell them
to I wish that was true
so people have been researching
distributed consensus Paxos zookeeper
various variants for decades and decades
and decades and yet I still decided that
I would pursue a PhD in this field so
why isn't it should be consensus to
solve problem well firstly dictionary
consensus isn't scalable typically when
we talk about consensus and we write it
in our papers we say we deployed a
system across three machines we deployed
it across five machines we really pushed
it we've deployed it across seven
machines these kind of numbers are at
odds with the practical reality of
scalable distributed systems secondly
consensus is slow people have been very
quick to rush into deploying raft in
their systems and people when it first
happened warned that this is gonna be
really slow guys this is this is gonna
take a long time to reach agreement but
people have deployed it and now people
are looking back and saying do we really
need the guarantees of some of consensus
like that that's taking a long time
you're talking to the majority of people
to reach every decision every single
time that's that's a lot of work when a
leader goes down it takes quite a while
for us to get get going again and that
leads into the third point here which is
distributed consensus isn't that
available so usually with these systems
we had a system that was on one one
machine and then we decided to replicate
it so that it would be more available
but in many of these systems we actually
seen worse availability when we
replicate than if we'd stuck with one
machine in the first place which is
pretty sad in fact many people many of
the people here will say consensus it's
just too high a price to pay we should
avoid it let's have weaker guarantees
let's have eventual consistency let's
use CRT T's let's go down that route
because this this is hopeless we've
reached the end we can't do any better
so what I'm going to do is I'm going to
start by showing you what actually is
the algorithm and why people believe
there are these limitations so back in
the day once upon a time we lived in a
world before distributed systems there
was a simple server and some clients I'm
trying to be really abstraction general
and explain something here that
used in many different algorithms so I'm
literally gonna use shapes in slots to
not get into the details of any
particular application so clients come
along and they won't know interact with
this server that's got five slots and
some shapes so they're gonna add some
shapes and they're gonna read some
shapes so he would come along append the
diamond for me okay cool I'll do that
for you tell me what's in slot 4 there
we go there's the diamond all as well at
some point however this show is gonna
fail failures are unfortunately
inevitable in systems and the client is
gonna be left waiting so distributed
systems to the rescue everyone so let's
replicate this let's have three servers
all with this lock and what we're gonna
do because we want to keep these in sync
and we want to keep them up to date is
we're gonna pick one of them and say
you're in charge of deciding what gets
done and the other guy is they're gonna
be your replicas so we're kind of have
one person will say you you're the the
leader the master the primary the
distinguish proposer you're in charge
and the rest of us will be your backups
will be your followers so in this
scenario I'm gonna take this middle note
and say you're the leader and the other
two you're just you're just service you
do as you're told so a client comes
along and says hey I want to append this
by minute again and the leader
replicates this to the other servers
gets the response that that's been done
okay and returns to the client all is
well with the world but the problem here
is that that leader waited for both the
service to respond so if you wait if you
require everyone to participate in your
decisions you've just got worse
availability because now you're three
times more likely for something to fail
so the question arises how many copies
of that request is enough if you're a
leader and you've said come on everyone
do this thing append the diamond for me
how many people need to come back and
get on board and say yeah we're on board
with you we've done this when can we
move on when is it safe to tell the
client that this has been done well we
call a group of nodes who have agreed
which is sufficient
a replication quorum and usually this is
majorities and I'll go on to kind of
generalize that in a bit so in this
system where you have three you would
need two nodes so that's the leader one
of the other servers to get on board
before you tell the client yep we've
done that we've appended the diamond for
you and we see a trade off here right
the more copies that you want to make
the more resilient you'll be because the
more copies of the information that you
have and the more failures you can
tolerate the fewer copies that have you
have the faster you'll be you could go
all the way down to having just one copy
just on the one node and then it would
be nice and quick you don't have to talk
to anybody so what happens when failures
occur let's say the third server here
has failed and we're using majority so
we need two out of three so we should
still be able to make progress and carry
on request comes along again to the
leader the leader will replicates it to
the server who responds and the leader
replies to the client to two nodes out
of three are up we've been able to
handle a failure all as well what
happens when the node that fails happens
to be that one that we said was the
leader the one that was deciding the
ordering of things and the one that the
client was speaking with this is where
things get a little tricky so we still
have two of the three nodes up so we
still want to be able to make progress
and carry on with the system and one of
these other guys they can step up and
say I'm gonna be in charge I'm gonna I'm
gonna take on this role I'm gonna take
this responsibility for you I'll do this
but the point is this leader this new
leader can't just start telling clients
everyone talk to me I'm in charge
because if you look at the logs you can
see because they previously failed
they're actually missing an entry there
so it's not safe for them to do this
before they take over they've got to do
a little bit of work first before they
can do this so it's important to recall
that in this class of systems what I'm
trying to do is ensure that we never
compromise safety and safety being when
we tell the client we did that thing for
you we did it we are never gonna we're
never going to go back on that promise
so the new leader before they can take
over has two jobs that they need to do
firstly they need to stop the old leader
so I showed that old leader is having
failed but in practice we don't know
whether that old leader is failed
generally what we do is we kind of poke
them and if they don't answer we assume
that they're dead
but as you know zombie movies and stuff
show us things maybe alive even if they
don't respond when you poke them so we
want to make sure they're dead so they
don't come back to haunt us in the
future and secondly we need to learn
about everything that's happened in the
system before we became the leader to
ensure that we don't contradict things
that happened in the past and the
ordering is important here you need to
get rid of the old leader and then take
a kind of a review of the state of the
system before you proceed if you did it
in the other order you would learn about
the state of the system and the old
leader may still be hanging around
messing with things so first thing first
how do we deal with this old leader that
might come back to haunt us well a naive
solution would be just to ask them to
stop and this is something in many of
the newer systems where you actively
choose to preempt leaders people forget
you can just tell them to stop doing
that although the problem is if we
believe the leaders failed telling them
to stop doing it probably isn't gonna
work as they're probably not going to
answer us another thing we could do is
use leases so the leader has some kind
of lease and they they're in charge for
the ten seconds and when that ten
seconds is up someone else can take over
the problem with theses is they rely on
clock synchronization and especially now
that people are moving more towards the
cloud into AWS in this world where
safety is paramount that's a dangerous
assumption to make
so thirdly what we can do is if we can't
talk to the leader but we want to stop
them we can talk to other people in the
system and say promise to stop talking
the promise to stop promise to ignore
them whatever they tell you don't listen
to them recognize me as being in charge
and if you can plant enough people in
the audience to to ignore the last
person and to recognize you as a new
leader
then you can take over control there may
still be someone there who thinks
they're in charge and things there the
leader but if you've got enough plants
there
won't listen to them they won't be able
to do any harm to you so say this is you
know say the old leader has died
I am the new leader I want to take over
the world and I ask you all promise that
you're going to listen to me and you're
gonna ignore the last guy the question
arises yet again
how many promises are enough how many
people do I need to get on board bearing
in mind that some of them might be slow
and some of them might be on their
laptops and some of them might have
failed how many of them do I need to get
on board before I can proceed and what
we do is we refer to this group as the
leader election quorum so this is the
group of people that you need to become
a leader basically and the idea here is
that I need one person from each of the
possible replication quorums to be on my
leadership election forums so that old
leader who might be replicating
something adding something to that log
they won't have access to a group of
people to do that because I've gone to
each of the possible groups and said no
ignore him talk to me no ignore him talk
to me so in the majority's case if we
were using majorities for replication we
now need to use majorities for
leadership election so I've got two
people out of three to promise to listen
to me and so the old leader won't be
able to do any harm because he has to
talk to at least one of those people
there's an overlap here the other
question arises if I'm going to get
people to go around and promise that
they're going to listen to me what
happens when I then fail and someone
else comes along we need a mechanism by
which people can make promises and also
safely break them and breaking promises
is difficult business so the way that we
do this is when you go around and ask
people back me ignore these previous
people we have some kind of unique value
and how you get that value depends on
which protocol you're using which you
use to say recognize me as being in
charge and promise that you will only
ever break that promise to me if someone
with a bigger integer a bigger number
comes along in that condition you can
break that promise to me but if they
have a smaller number like they're those
old leaders from the path
you can't break your promise to me the
other thing we need to do is learn about
what's been committed in the past so we
safety is paramount here we can't undo
the decisions if something's been
appended to the log
it so again we're going to go around and
we're going to ask people and we need to
talk to one person in each of the
replication quorums now difficulty
arises here there may be some requests
that are in progress or some cases where
we actually have conflicting requests in
a single position in the log now from a
safety perspective we know that only one
of these could ever have been successful
and this is where that promising scheme
becomes really useful because we know
that people made promises and they only
broke them for leaders with higher
numbers if we see two conflicting
requests in the same position in the log
we choose the one with the higher number
because we know that's the only one that
could possibly have succeeded and that
is paxos that is the core algorithm we
see a huge number of variants the most
popular of which being things such as
zookeeper raft and few Stamper
application but they all build on the
same underlying idea so I'm not here to
give you guys a history lesson what's
changed why am I here today well
traditionally it was believed that all
the quorums whether you use them for
leadership election or replication
needed to intersect and if you need all
groups to intersect majorities they're a
pretty reasonable scheme but the fact
the matter is that that isn't the case
we only need the group used for electing
a leader to interact in to intersect
with the great group used for
replication we do not need two groups
used for replication to overlap and in
most protocols you do not need two
groups used for leadership election to
either even overlap now I understand
that this is a bold claim and I'm going
to talk a little bit more about it in a
second so what are the implications of
this well firstly it helps us to
understand Paxos I try to teach pack
cells to undergrads at Cambridge and
mostly they just nod and go away and
complaints so it's hard to teach people
pack cells and this if it helps us to
understand the protocol maybe it will
help us to teach it to people and to get
it right in the future for the theory
crowd it's a generalization and a
weakening of the original requirements
which is to those kind of people it's
just neat in its own right and it's
orthogonal to any particular algorithm
and this is really where this rework
becomes interesting this isn't a
competitor to raft it's not a single
open source project or a single codebase
where I'd say everyone comes which to my
thing it's the newest the bestest thing
since sliced bread instead it's a
contribution that changes the way we
think about how we reach agreement in
distributed systems and something I
think we're going to see realized in a
whole host of systems in very different
and interesting ways in practice the way
that we construct quorums can now be
different for a long time we thought the
majorities were the only way but what
I'm going to show you is there are
plenty of other ways that we can do this
many of which are much better in
practice and this introduced a scalable
a new breed of scalable performant and
resilient consensus algorithms you don't
have to give up on consensus because we
can do better so this is what we used to
majorities we have a system of 5 nodes
and we require three of them to get on
board before we can make a decision and
it doesn't matter whether that decision
is electing a leader as shown here in
the kind of orange color or doing some
replications so adding something to a
log whatever it is we need three people
the majority to get on board and the
logic behind this is they're always
going to overlap so the problem with
this as you saw earlier when I was
talking about numbers of nodes they were
quite small and they were also all odd
so what happens when we have an even
number of nodes what happens if our boss
just happens to give us eight machines
to work on well one you need majorities
you would need to speak to a strict
strictly more than half of those machine
it's strictly half or more so you can't
speak before you would have to speak to
five to ensure that they overlap with
flexible packs offs we know that this is
no longer the
so here's an example here we have six
machines now traditional packs of logic
will tell us that we have to speak to
four of them in order to reach any
agreement and we do still decide to talk
to four of them when it comes to
leadership election but when it comes to
replication we just talk to three of
them at those two sets may not overlap
but that's okay that doesn't matter
and this is even though it's an
improvement of one this is really useful
in fact there's not actually a downside
to us doing this firstly it improves
fault tolerance because half of the load
nodes can fail and if we still have our
master we can still make progress
secondly it's faster we used to have to
speak to four nodes now we can just
speak to three i'm thirdly there's
potential here for us to greatly improve
the throughput because we have two
disjoint sets that we can now send
replication requests to so we could send
it to the left-hand color of the
left-hand side and then the right hand
side the left-hand side and then the
right-hand side essentially doubling our
throughput we can take this further so
as long as the replication quorum plus
the leadership election quorum is
greater than the number of nodes then
you're safe majority is a one trivial
way of satisfying this but there are
many other ways of satisfying this and
the important thing to remember here is
traditionally in a system you elect one
node as a leader and then you do loads
of work tens and thousands of requests
and then the leader fails and you elect
a new leader so leadership election is
quite rare and replication is quite
common so go ahead yeah yeah go for it
so um no I don't think I made that class
so basically what happens is you've got
a leader and they're doing they're
they're trying to replicate some state
in this system they will put something
into a log
they're not gonna then they can't change
their mind they can't send it out and
then tell some other people something
different so as long as it's you know
the leader is doing that it doesn't
matter because they know that it's the
same so it doesn't those two don't have
to overlap if that leader then fails
then you have to elect a new leader
which will see you speaking to four
notes so you will speak to each of those
groups and you'll safely learn what has
happened in the past any other questions
stunned faces so as I was saying we can
trade off the size of the group that we
use for ratification and the groups that
we use for leadership election because
leadership election is much rarer and
here's one example here we have six
nodes so previously we had to have four
of them on board for every single
decision then I showed that we could
just have three of them on board now
let's just have two of them on board so
when we're doing replication when we're
adding things to a log or doing a
consistent read to the log we're just
going to talk to two of them for each of
those four each time then when we do
have a failure and we need and that we
need to recover a new leader we're going
to talk to five of them so you're
increasing the cost of leadership
election here and decreasing the cost of
replication the reason why I argue that
this is something you would want to do
it's because replication is so common
and it's very much in the critical path
of your system and the leadership
election is far rarer but what about
fault tolerance right the reason why we
put something on the majority of nodes
was for fault tolerance well the reality
is now we can handle for face
if we still have the master we can
handle four failures in replication and
the system is predominantly in the
replication phase so if we do it
statistically over time we get a higher
availability with this scheme than we
would with majorities any questions at
that point it's a big claim the what
sorry is that because there's the most
load on them so I think oh yeah I can
see that argument it depends the hope
for the whole idea of flexible paxos is
I'm going to show you some other quorum
systems as well is if you're in a
situation where your master is most
likely to fail we can actually do some
other work that comes with other systems
that are better than majorities so it's
about depending on the needs of your
system when I've looked at really kind
of high performance consensus systems
they tend to separate them separate the
master from the rest of the system and
actually deploy that on their machine
that is more reliable than the rest of
the system so it's actually less likely
to fail than the rest of the nodes but
if if they that if masters in particular
are likely to fail then maybe one of the
other systems here might be best any
other questions at this stage yep
I am a PhD student and I feel unable to
comment on the realities of real world
systems from talking to people
leadership election does not seem to be
a particularly common particularly
common in comparison to replication
should not be I feel like many other
people in this room are more qualified
to answer your question and I this is
this is something that's really funny so
people tend to assume that the issue of
election is a fairly lightweight process
when they're using something and I spoke
to them that speak to people kind of at
the beginning and I was like it's gonna
be really expensive and they're like no
no no it's gonna be fine and then a
couple of months later that like it was
really expensive our system was down for
a while whilst it was like transferring
all the states like yeah I know I told
you so the claim and here's another
example with some slightly different
numbers so here we have a replication
quorum size of 3 with the total system
of 8 the claim here in these particular
examples is that if your leadership
election is rare that this is a
trade-off that you're probably willing
to make and we can do analysis on this
and show that the availability is
actually better that it was before but
let's take this result and start to do
some things that are a bit more
interesting with them so even though
when we're modeling this in theory we
say all nodes they're equally likely to
fail and will fail independently anyone
the worse with systems knows that
failures aren't independent right one
thing breaks all the things break
failures are very much dependent you
know if two machines are in the same
rack it's more likely these guys are
gonna fail together they don't
independently at all so this hand-wavy
thing of saying oh we'll assume that
only the minority fail and then it will
be fine
I don't think that's actually a fair
assumption in a lot of systems so how if
we know that failures aren't independent
let's not treat the machines themselves
as if the failures are independent
they're not equal so let's not treat
them equally so here's an example of a
system we see the system here has 13
notes which is big for consensus
standards this is this is really very
much so it has 13 notes and we've got
two examples one here and one here and
what I've done here is draw these groups
which basically represent for example a
rack so a group of nodes where they're
likely to fail they're more likely to
fail together so what we can say is when
we're doing replication let's get one
machine on each rack to have a copy of
what's going on so let's spread that
across the system first thing to know
here is this is good in terms of fault
tolerance if we think that machines in
the same record more likely to fail
together secondly this is nice because
we're just talking to four nodes in a
system of 13 if we were using majorities
we need to speak to many many more so
then when your master does fail and you
need to elect a new one we just need to
get one rack which will be easy to talk
to you because they're going to be close
so low-latency just need one rack to get
on board and they will have a copy of
all of the requests and they will be
safely be able to become elected as a
leader and take the system forward and
again you know we're still now here
we're talking to three notes maybe two
notes or four notes there's still far
fewer than majorities so in actually
both cases here we're talking to fewer
nodes and the majorities are the
majority of nodes and it's still
perfectly safe and as I say 30 nodes is
big bye bye papers and consensus
standards so another example scheme than
this one is mostly just for fun is grids
so let's take a system of nine nodes
and split it into a 3x3 grid we can say
that any column could be a replication
quorum so if we want to replicate a
request we just need column 1 2 or 3 to
get on board with us and then when it
comes to leadership election we just
need a row to get on board with us now
clearly any row and column are going to
intersect so we're going to meet the
requirement that we needed but this is
quite a fun little system because we
never accidentally overlap in the sense
that to replication quorums will
definitely not intersect at normal to
leadership election chorim's so when for
this paper we were doing model checking
this was the scheme that we
predominantly used and there are huge
host of other things so the fields of
databases to some extent file systems
and data replication had this idea of
quorums particularly they called it
readwrite forums a long long time before
we did and they've done lots of research
into different systems and we can
combine these systems so we can start to
say let's use majorities within the data
center and then when we're looking
across data centers let's use a smaller
group so we only need two data centers
out of a possible 8 for example and we
can construct grids and we can do hybrid
schemes where we have majorities at the
top maybe and then you have some
underlying thing that's using groups or
something like that the fact of the
matter of this the fact of the matter is
majorities are not your only option the
point of this work is not to sell you
something else and say this is the best
whatever system you're using but instead
to tell you that you have the
flexibility to use whatever scheme you
like as long as you meet the requirement
that a leadership election quorum and a
replication quorum will intersect and it
doesn't matter what quorum scheme you
use the reason why this I think this is
interesting
to the community is because the
requirements of real world systems
matter now when we talk about this in
theory we say you want to tolerate our
failures and we want to minimize the
number of nodes so we use to F plus 1
but in practice people have actual
constraints for example I want the
average request it takes
long I only have so much money to spend
on Azure I want this failure recovery to
take no more than this amount of time
for this percentage of recoveries so
it's up to systems and communities to
decide what trade-offs they want to make
they don't have to take this one
trade-off that was offered by Lamport
three decades ago so in summary the only
quorum intersection requirement is that
between leadership election and
replication however sir so we never need
to have two replication quorums overlap
and for some protocols they do require
that leadership election quorums overlap
but some don't it depends on the
protocol I can talk a bit more about
that in questions majorities are not the
only option you have a whole host of
options available to you and that you
can choose whichever is the best for
your system you don't have to give up on
on strong consistency guarantees you can
have a system that is twenty thirty
nodes thirty nodes and still run Paxos
on it and this is actually scalable and
performant if you need to talk to a
couple of notes each time you're doing
replication so what's next for this work
where I'm talking with various people in
various communities to both adopt it
into existing systems so for example
when you're talking when you have an
even number of nodes even number of
nodes reducing the quorum size by one to
exactly half for replication is
something we can really easily add to
things like raft and zookeeper with very
little work it's literally changing an
integer deep inside the protocol and
then the second part of that is building
new algorithms that really really take
advantage how can you really build very
scalable very performant consensus
algorithms thirdly I'm working with some
people to do a practical analysis of the
different quorum systems and hopefully
what I have what I would hope to produce
in the end is this kind of a document
with the maths and practical analysis
that says you know given you're running
this kind of system this is the best
quorum system for you in terms of
availability and performance and also to
give people numbers right if this is a
configuration option people should have
access to a table that basically says
you know do you want 99.9% availability
and this average latency or do you want
99.9999% availability and this average
latency and we're looking at extending
this to other algorithms so lamb pulls
original formulation formulation has
been built upon and built upon and built
upon and branched out into leaderless
consensus algorithms fast paxos
generalized Paxos businessing fault
tolerance this idea has absolutely
perpetuated into all sorts of areas now
that we know that this isn't necessarily
the case we can now go and do
interesting things in all sorts of
different areas that's it
I'm happy to take questions otherwise
you can reach out to me by email or on
Twitter thank you
questions stunned faces no one can leave
must be questions so I have a question
what so what was it
that's flexible pack so see did you why
not just create a new name just start
afresh
with a new there was a lot of discussion
over this naming things is very hard the
one of the reasons I didn't want to give
it a new name is because it's already a
problem we've begin to see where people
assume this is another algorithm they
assume that it's a competitor to
multipacks also competitors you shall be
a competitor to Raft
instead it's theoretical results that
works kind of at the foundation to take
Paxos and say you don't have to have it
like this you have the flexibility to do
whatever you want with it
so by naming it flexible paxos I hoped
that people would recognize it as being
kind of orthogonal to the existing
algorithms one of my biggest regrets
about naming your flexible paxos is that
since I did since we like initially did
this work it's become apparent to me
that a lot of people are using basically
consensus and they don't call it Paxos
for example they use train replication
which is which is an instance of Paxos
or they use for you stamp replication
which is a knit which is a in fact came
before
paxos hope your practice is always an
instance of check overview stamp
replication so this result actually
affects these guys as well where
majorities is much more pervasive but we
call it a flexible Paxos can't go back
now
I just got a a specific kind of
implementation question in the great
example you showed with the columns and
rows so your columns as replication
chorim's and and the rows as leaders if
there's a partition between one of the
rows and the other two so that you've
got you know one group that has six then
theory still has a majority of the nodes
but there's now an entire leader
election quorum on the other partition
does that prevent any of them from
actually replicating until they connect
again so sorry the partition is between
one column and the other collar between
one row when they're in theory that both
sides then have a leader election quorum
so they can presumably they can both
elect leaders but they can't replicate
because there's no replication quorum so
is it then yeah not available until that
partition so like in this in this slide
that would be unavailable as you say one
of the things to note here is generally
speaking I'm not going to the details of
dynamic membership and how you actually
go about changing corns and kicking
nodes out and bringing them in but
generally if we can form a leash a
leader election quorum that means we can
recover the state of the system and
we're then in a position to reconfigure
the system and basically kick that kick
that row out if we want to continue to
make progress if we're in a position
where we can own as if but if by
contrast we lost a column so we were
able to replicate we could keep going
for a long time until the leader fails
but then when the leader fails we would
then be unavailable
so in this great example the since
you've got non-intersecting replication
courtrooms if you have a leader election
and it decides to go from the left to
the middle so that that means the new
leader is not part of the original set
of up-to-date replicas how does the new
leader find out what the state of the
log should have been before then if it's
not part of the old replica set so when
the the new person takes over and they
get a row on board there will there will
be by a necessity at least one person
from each column there and since we know
how to deal with conflicts etc when we
can guarantee that at least one person
will be able to give us the correct
answer anyone else
all right thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>