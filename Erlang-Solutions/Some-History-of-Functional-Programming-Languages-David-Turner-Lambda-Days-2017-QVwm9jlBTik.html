<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Some History of Functional Programming Languages - David Turner (Lambda Days 2017) | Coder Coacher - Coaching Coders</title><meta content="Some History of Functional Programming Languages - David Turner (Lambda Days 2017) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Some History of Functional Programming Languages - David Turner (Lambda Days 2017)</b></h2><h5 class="post__date">2017-03-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/QVwm9jlBTik" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay this talk is called some history of
functional programming languages it
follows on in a way from the excellent
talk that John Hughes gave yesterday
morning what I want to do is look at
some milestones in the development of
lazy strongly-typed polymorphic
higher-order purely functional languages
of which Haskell is currently the
leading example and the talk is biased
towards stages that I was aware of at
the time that they were happening so
it's one particular cross-section
through the history is that does that
look all right I assume people can see
what I can see on this screen which is a
list of milestones yes so these are the
milestones I'm going to talk through
let's start with the lambda calculus as
as John did there was in the 1920s a
project involving both church and curry
to create a new foundation for
mathematics based on functions rather
than sets and the project ran into
paradoxes very similar to the ones that
are had afflicted naive set theory so
they threw away the propositional
apparatus and what church discovered he
was left with was a very simple notation
which describes all possible computable
functions and this is the lambda
calculus so functional programming came
into existence quite a few years in fact
about 10 years before the first computer
I think the earliest published version
of the lambda calculus was 1934 so the
lambda calculus as you all know has
variables application and lambda
abstraction there are three rules which
you will know how the conversion says
that you can make systematic change of
bound variables b2 conversion says that
you can substitute an actual parameter
into the body of the function
substituting the actual parameter
for the formal parameter and each of
conversion is a simplification rule
which says that Lander X dos FX is the
same as s and that turns out to be quite
important in some of the measure
theorems what are these functions
functions of and this is a tight list
theory the lambda calculus in its
original form is a tightness theory of
pure functions by pure functions I mean
that there are functions of functions or
functions or functions and its functions
all the way down there are no ground
types a bit like Turtles all the way
down in the story about what the earth
is standing on you can of course add
constants like numerals and and the plus
symbol and have reduction rules but you
don't need to as John showed yesterday
there are functional representations of
all the data types that you could want
and other data types you haven't even
imagined there are three really
important theorems about the lambda
calculus the first is the church-rosser
theorem which says if you reduce a in
two different ways to get B and be
dashed
there's a way you can reduce them
further to get to a common destination
so the rules are convergent like that
and this immediately implies that normal
forms where they exist are unique the
second church-rosser theorem says that
if you repeatedly reduce the outermost
read X you will get to the normal form
if there is one and the third theorem
which is due to corrado bone a very
important theorem says that if a and B
have distinct beta e to normal forms
there is a context that will strongly
separate them see if one if the same
context e will reduce the first lambda
expression for K and the second lambda
expression to ki those are the selectors
for first and second so it shows that
they're strongly separated
so what this means is that the lambda
calculus with these three rules alpha
beta and eita
is the strongest possible equation or
theory of pure computable functions
because if you added a single extra
equation between normalizing terms the
theory would become inconsistent you
would get x equals y the second transfer
of the theorem says that to find the
normal form in general you must
substitute the actual parameter into the
function body without evaluating it
first it corresponds to lazy evaluation
and so late it was this was known from
the earliest days normal order reduction
which corresponds essentially to lazy
evaluation is a normalizing strategy it
will find the normal form if there is
one okay
so lazy that the development of lazy
evaluation very quickly when went
through this sequence of events
call-by-value which is where you
evaluate the arguments before you stick
it into the function applicable in other
words applicative order reduction is not
a normalizing strategy you will
sometimes miss the normal form if there
is one and go into an endless loop but
it's more efficient because you evaluate
the actual parameter only once and from
the earliest days from lift in 1960
functional language for implementations
of functional languages used applicative
order reduction they did call-by-value
it's not a correct strategy but it works
most of the time and it's more efficient
in 1971 wordsworth
who was a PhD student in the same
building as me in oxford in 1971 because
that showed that you could overcome the
efficiency disadvantages of normal order
reduction by doing reduction on graphs
rather than on trees so then you have
one shared copy of the actual parameter
when you substitute it into the body of
the function you get several pointers to
it in several copies of it so you still
only reduce it once and this is what we
now call lazy evaluation in 1979 I had
the idea of doing this to combinators
which gives you a much simpler abstract
machine you know ask sk combinators
anyone not know about Combinator's oh
okay
kuri who was a colleague of churches
showed that you could make an even
simpler calculus that had a small number
of atoms sk and i is enough each of
which carries out a very simple
transformation and any lambda expression
can be translated into how should I put
it
Oh an SK word something that just has s
is in Kaizen isin it applied together so
you can think of this as a kind of
functional machine code
it's very unreadable for human beings
but there is a mechanical process for
translating lambda expressions into SK
expressions the SK expressions look
quite a lot bigger but the machine for
reducing them is very very simple
you need the s rule and the K rule and
the I rule and if you add a Y rule you
get a more efficient way of doing
recursion so I had the idea of doing
normal order reduction on Combinator
graphs and that works quite neatly and
then lots of things happened after that
Thomas Johnson and then at augustson at
Chalmers University in Sweden had the
idea of extracting specific combinators
from the source code instead of using
these very fine-grained combinators SK
and I and this is essentially using John
Hughes's idea of super combinators and
that way you get something that works in
bigger steps and can be more efficiently
mapped onto a conventional or Neyman
machine and this was further developed
by son Peyton
Jones and others to produce the
spineless tagless G machine which
underlies the Glasgow Haskell Haskell
compiler so over a period of about a
decade and a half we got to efficient
implementations of lazy evaluation now
back into the history we go back to 1960
to list John McCarthy developed list
starting in 1958 there is an S language
which is a very simple basis for
representing non numerical data you have
atoms which are just words or they can
be numbers and they're combined by a
pairing operation so all you have is
pairs of pairs of pairs of atoms at the
bottom and that way you can represent
any any shape of tree and then he
defines an M language which is a very
simple recursive language it has some
basic functions for manipulating s
expressions and it has first-order
functions and conditional expressions
and recursion and that you can easily
show that Universal the M language is
first-order you cannot pass a function
as a parameter however what McCarthy did
was he found the way very simple way of
encoding M language programs as s
expressions and then you have a function
called eval which takes an S expression
there's one called eval among called
apply so you can get rid of the M
language you just have the s language
which when you wish to you you interpret
the s expressions as functions ok and
this all works rather nicely through an
awful lot of things and this was an
absolute revelation and one of the most
important things that the mccarthy did
was to decide that he did not want to
clutter his programs with allocate and
dispose instructions he said I'm going
to have the runtime system figure out
when I'm not holding on to an S
expression so it can garbage collector
so McCarthy created a need for garbage
collection and created the first garbage
collector or one of his students or
coworkers did and that was incredibly
farsighted decision because computers
were much smaller much slower in those
days so it was a very expensive decision
but it was the right one okay a couple
of myths about this people talk about
peerless peerless never existed this
actually began as something like Fortran
it had assignments and go to before it
had conditional expressions and
recursion and Lisp programmers made
frequent use of assignment of update in
place to get more efficient algorithms
important when you have computers which
by modern standards had tiny tiny
memories just a few thousand words of
memory in those days second point is
list was not based on the lambda
calculus notwithstanding McCarthy used
the word lambda as he explained later
for example at the the conference in
Pittsburgh in 1982 he was thinking about
first order recursion equations and he
came across the lambda symbol and used
it but at that stage he hadn't read
churches 1940 paper about the lambda
calculus so the scope rules come out
wrong what McCarthy was doing was
passing the s expression which
represents the function instead of
passing the function itself and the
problem with that is you get the wrong
binding rules you'll get dynamic binding
instead of static binding and not until
scheme in 1975 did you get versions of
Lisp with lambda lambda calculus based
scope rules meanwhile back in on the
other side of the Atlantic Algol 60 was
happening so we're back to 1960 again
Algol 60 is the great of the grandfather
or great-grandfather the whole sequence
of languages how will W Pascal modular 2
what have you
now Algol 60 had textually nested
procedures and it did allow you to pass
procedures as parameters although not to
return them as results so it was higher
order in a sort of halfway house you
were actually doing higher order
programming in continuation passing
style although nobody was thinking that
in those ways at that time Randall and
Russel were amongst the first people to
do a full implementation of our little
sixty I think their book his dates from
about 1964 it's called the
implementation of Algol 60 and they
discussed how to implement free
variables and they have what they call
the static chain on the stack the stack
has all the procedure incarnations that
you've been through that you've got into
in time order but the static chain goes
backwards through what your textually
inside it's all explained quite clearly
by diagrams in their book so because of
the curious discipline of Algol that you
could pass functions as parameters but
not return the most results you can use
stack based storage allocation and
everything works out okay however when
you go one step further in power and so
I'm going to allow myself to return
procedures as results then you may be
referring to some free variable that is
no longer on the stack and Landin solved
the problem of how to implement that
with his secd machine which keeps
closures a closure is the code for a
function together with its free
variables and closures live in the heap
which was the big step that landon made
landon invented a family of languages
called i swim which john talked about
which were sugared landers at one point
he calls it church without lambda so you
have let and rec and where and so on and
you
and say anything's been saying the
lambda calculus but it looks much more
readable and then landed added to this
into further layers if you add an
assignment statement you get lambda
calculus plus assignment and then you
can add a transfer of control operation
Landin had something called the j
operator which is essentially captured
current continuation for those of you
who into that list stuff the j operator
was rather complicated to use but very
very powerful you could do things like
co-routines and backtracking and so on
but it's all a bit difficult to decipher
right I swim was never implemented but
it inspired pal at MIT which was
pedagogical pedagogical applicative
language and good Duncan at the Argonne
National Laboratory it's quite similar
to each other I got given the Powell
tape from MIT when I started my PhD
studies in 1968 and I you know spent a
long time
puzzling over this thing and figuring
out how to implement it more efficiently
Powell was a sugared lambda calculus
very very I swim like it added
dynamically typed like this it added
mutable variables and assignments and it
added first-class labels which was a
much easier thing to understand the
landings J operator
you could set labels you know L :
and then jump back to that label later
and using that you could jump back jump
back into a procedure that you'd already
let a procedure incarnation you already
left so you could do backtracking it was
great fun and you could confuse yourself
enormous Lee doing Co routines and
backtracking and so on then I I left
Oxford having not quite finished my PhD
and went to some tendrils to take up a
lectureship and I I got given a course
in functional programming
coming to teach started trying to use
lists discovered that list is not
actually based on the lambda calculus
all my examples went wrong because the
binding rules were wrong so then I
remembered what I'd done with pal and I
took just the applicated subset of PAL
and showed this to the students and they
thought this was great and started
programming well first of all it was
just a blackboard notation but then my
colleague Tony Daley surprised me by
implementing it over the weekend he
implemented it in list and so then as it
was implemented we had to give it a name
so I called its and Andrew static
language so it was the applicative core
of I swear and this first came to
existence in 1973 I think January 1973 I
made two improvements from Powell as I
simplified it by getting rid of the
assignment and the go-to it had one
level pattern matching I changed that to
multi-level pattern matching and I got
rid of tight string and just had
characters as a basic type because lists
of characters are strings so you don't
need strings as a separate type okay so
that's a bit more about subtle how long
what times I got to stop oh that's fine
okay so I've got until half-past okay
right what the advantages facile over
list or the advantages for me as a
teacher was that sattell was based on
the lambda calculus so I didn't keep
getting caught out with examples that
don't work it's a pure showing of the
lambda calculus with no imperative
features and no is our quote
distractions a problem with lists if
you're using it to teach lambda calculus
type ideas is that Lisp has this eval
quotes business that you're moving
you're interpreting data as program and
then quoting programmers data which is
you might call that meta programming
it's a very complex game you can do all
sorts of clever tricks with
but it's completely orthogonal to what's
going on in the lambda calculus so if
what you want to teach is functional
programming is better not to have the
bell and quotes around it has the
correct scope rules because it really
you know it was based consciously based
on the lambda calculus and multi-level
pattern matching makes for a huge
improvement in readability so you've got
there a list of expression with loads of
comps and cars and cutters and what it's
actually saying is in this it's led it's
let a b c d with some brackets in equal
the set x can you see what's going on
there
it's just simple pattern matching so
these impenetrable cascades of cars and
cutters car means head and cut them into
town okay and when you read this code
you scratch your heads or all the cars
and cutters and have to draw pictures to
satisfy yourself about what's going on
and if you just replace it by pattern
matching and sessile did becomes much
easier to see so SATA was probably
unique in 1973 in being a pure
functional language with less and rec
rather than lambdas and multi-level
pattern matching and the students really
liked it and other universities are
started asking if they could have a copy
right it had runtime typing because i
wanted to do polymorphic things like
right you want to write a program to
take the tree and turn it into its
mirror image and you don't care if it's
the tree of numbers or or a tree of
lists or or a tree of words and in the
mid-1970s nobody knew about polymorphic
type discipline or well I certainly
didn't anyway and people did what list
did was to delay it which was to delay
the type map the type checking until run
time and that enables you to do
polymorphic things like writing a
general reflection program or list
reverse program
and you can do quite curious things if
you have run time typing look at this
sattell program I've got a function a
boolean function it takes some number of
boolean arguments you know true or false
and returns true or false as an answer
and you want to know if it's a tautology
and it's a curried function this
definition here of taught taught is a
tool tullum a to taught a tautology
tester for a function of an unknown
number of arguments so what you do is
you take air you ask if it's the truth
value logical is a type testing function
if it's a truth value that's the answer
it's a tautology if it only if it is the
value true and otherwise you apply it to
true and apply it to false and check
that both of those things at autologous
so that's a very very neat tautology
tester compared with ones that you there
was one in the list manual with the
several pages of code right run fine
typing still has devoted followers this
is still around and does run time typing
and of course Earl I suppose successful
language which does run time typing okay
little bit more about facile I moved
from I moved to Kent in 1977 but sat all
sort of came with me and was evolving
all the time at a certain stage while it
was still some Andrews I got rid of rec
saying why do I have to announce every
time I'm going to do recursion let's
just make recursive scope rules the
default so you just say less and a bunch
of functions a bunch of function
definitions and then in and whatever it
is you want to do I decided to switch
from left to where because it's more
natural to say solve problem where
problem equals and solve equals then
split the definition sir say what you
want to do first and then
the definitions off after so I switched
to where and then very important change
in 1976
sattell became lazy I decided to switch
to lazy evaluation I had known all along
about normal order reduction being a
normalizing strategy but I didn't know
until I read Burgess book which came out
in 1975 how to modify the secd machine
which was Landon's machine for reducing
Lander expressions to make it lazy and
in his book is called doing what it's
called recursive programming technique
that's right it's a white book I have a
hard white hardback have a copy on my
shelf and he showed lots and lots of
interesting stuff in there published in
1975 and one of the things in there is a
lazy secd machine so since I saw that I
modified my sattell implemented to be
lazy and so in 1976 I had a lazy
language and that is exactly the same
year that the two papers came out that
John that John mentioned so it was
actually three people doing the same
thing simultaneously and possibly four
because Mary pointed out that John
voluma did a lazy implementation system
somewhere around the same time I have to
check the year but clearly lazy
evaluation was an idea whose time had
come in 1976 and you have the two papers
that John mentioned and my lazy saffle
implementation I also got from John
Darlington the idea of using pattern
matching not just to break down the
argument but to do case analysis and
that way you get multi equation function
definitions so that made cessful much
much nicer to read and much easier to do
proving
because each equation is is one rule in
one proof rule for that function and
then sattell carried on evolving gently
as I moved and Andrews and the major
change I reimplemented it using common
translations of Combinator's and
Combinator graph reduction that was the
first thing I did when I got to Kent in
1977 okay and then other people picked
up black reduction and worked out ways
of doing it more efficiently rather than
using SK Combinator's what are the
advantages of laziness well first of all
consistency with the lambda calculus in
the lambda calculus normalizing
expressions have a normal form and it
the meaning of a lambda expression is in
some sense its normal form if it has one
and normal order reduction that is doing
the alpha most reduction first not one
of the innermost reductions is a
normalizing strategy so you really need
lazy evaluation if you want consistency
with the lambda calculus it's better for
equation already don't have a lot of
exceptions to do with non termination it
allows you to do things with lazy lists
can be used instead of streams so you
can program things involving
communicating processes because the
channels between the processes can be
represented by lazy lists so there's a
big increase in power and you don't need
exotic control structures and this was
actually one of my main motivations that
realizing that with lazy evaluation all
the amazing things that you could do
with Landon's J operator like Co
routines and backtracking you didn't
need you don't need Co routines because
the typical situation for which you need
a Co routine is you have a producer and
a consumer that are recursing over
structures of different shape so they
have to
each take a step and then talk to the
other one well if you have lazy lists
you just make one of the processes
create a list of whatever it wants to
extract from the data structure that
will actually happen lazily as the
consumer eats it so lazy lists get rid
of the need for co-routines
and the list of successes methods the
list of successes method in in fill
waters famous phrase replacing a failure
by list of successes shows you how to do
backtracking using lazy lists who don't
do backtracking you do forward tracking
and you rely on the fact that you're
working with all the possibilities but
only the ones you look at actually come
into existence and I already had that in
my 1976's or manual I had an example of
the list of successes method and it was
clear from the wording that I gave that
I understood it was a general method but
I didn't have a name for it and it's
very important to invent names for
things like lazy evaluation as the name
for normal order reduction and calling
it list of lists of successes made
people understand it and start using it
okay so sattell was quite successful in
a mall in a modest way the Combinator
implementation was efficient for its day
we can do things much more efficiently
now but it was really quite fast
compared with the secd machine and these
are all the places that gasa copy of
sass when we're using it as about 30
place altogether including three
companies and I had a long cooperation
with Burroughs corporation who designed
a Combinator reduction machine in TTL
logic and built three three model
computers of this and they wrote all the
software the operating system the
compilers and everything
in fact in sattell but in a in a lazy
functional language and then
Sperry UNIVAC bought up burrows and
closed down the project which was sad
that was about 1986 had the project come
to completion that would have been quite
exciting they were planned they were
planning to make 10 or 20 copies of
these machines and give them to
universities so okay meanwhile let's go
back to 1969 1970 in Edinburgh Burstall
rod bursal wrote a very important paper
in 1969 called proving properties of
programs by structural induction and he
took an I swim like notation actually
quite similar to my satchel and he added
he made it tight and he added algebraic
types you know what is a tree well the
tree is either a nil tree which is a
tight tree or it's a node which has an a
command to trees to make another tree
and what can you do once you've got
algebraic type definitions you can do
case expressions so this is very slick
and then John Darlington Versalles PhD
students instead of using case
statements he used multi clause pattern
matching so there's his definition of
Fibonacci and the language was called n
here for new programming language it was
func purely functional it was
first-order and strongly typed and he
was using it for working program
transformation John Darlington was in
Edinburgh I was in some Andrews and I
went to see him and we swapped ideas
I showed him higher-order functions and
laziness and he showed me these this
pattern matching stuff which I
immediately stole the idea of and put
into SAS all okay he'll John Darlington
also had what he called set expressions
and they were written in the way you see
here set of even X you write this had
funny brackets
settle it all X in X all little X in big
X such that even that alexia these were
not lazy they were set expression so
they eliminated you placate and they and
they computed the whole answer but it
was a very powerful expressive medium
because you had some of the power of set
theory here you could say also and so
such that so and so and he was using
this in program transformation he write
a specification using set expressions
and then using transformation rules turn
that into an efficient functional
program and that was John Van Winkle's
PhD thesis ok NPL evolved into hope
which became higher order was strongly
typed with explicit types had
polymorphic typing with explicit type
variables and there's a famous paper of
1980 at one of the list conferences
about hope meanwhile also at Edinburgh
ml was developed which had a Anam
polymorphic type system with type
inference who didn't need to declare the
types and this was Milnes famous
polymorphic type discipline and standard
ml which was developed in the mid 80s I
think and well it was it became the
standard came out in 1990 and then a
revised and in 94 I think is the merger
of the hope and the ml streams okay now
going back to me in Kent in around
around 1979 to 1980
I developed a simplified version of sass
all which was just single equations so
the whole program just consisted of one
line equations to do that I changed from
conditional expressions
to guarded equations as you see in that
example okay now if you combine guards
with pattern matching there's quite a
big increase in expressive power and I
think think that I'll see was the first
language to do that
it had only top-level equations so I
conceived of it as I say this is about
1980 it was in 1980 as a sort of
functional answer to basic so it had a
built-in line editor the lines were
notionally numbered and you had little
commands you could type into swap lines
3 &amp;amp; 4 also do a substitution in line 6
so it didn't need a screen editors were
not common in those days right so okay
so that was que hace que hace also had
list comprehensions I took John
Darlington's idea of set expressions and
applied the idea to lazy list and you
could the idea becomes much more
powerful when you're doing this
component with lazy lists makes enables
you to think quite complex thoughts in
it in a single expression and then what
how Myranda happens which I developed
after chaos II was I decided that
programming with only one level of
function definition is entirely possible
and people wrote medium-sized programs
in chaos fee but it is so much more
convenient if you put the where back in
and have local definitions so I put the
where back in that raises the puzzle
about the scope rules if you combine
guards and where you have to make where
modify the whole right hand side not
just one expression and you see that in
Haskell also another change I had to
introduce was because I brought in
algebraic type has had been invented by
burst or and the polymorphic type system
of milner which I got from the original
ml
so Miranda began to look quite like
Haskell is now or those several more
important steps to get Haskell if that
looks some simple a simple Haskell
program often looks almost exactly the
same as the corresponding Miranda
program apart from a few trivial lexical
changes I introduced uppercase letters
for initially for the first letter of
constructors to distinguish them from
variables and functions because that
enables this distinguished pattern
matching as in this first equation node
XY equals stuff there you are naming
parts of stuff from a definition of a
function node XY or stuff the function
will have a lowercase name and again
that's passed into Haskell okay so
Miranda I say is rather than was because
it's still there and you can download it
from Miranda's oqk it's lazy purely
functional has list comprehensions is
polymorphic with type inference and
optional type specifications and there
are papers describing it as well as the
code at Miranda wk and so now I've got
five minutes left so so Miranda was in
place by about 1980 my first release was
in 1985 but that was called a
pre-release the first real release was
in 1987 and the second release in 1989
and it was quite popular I think about
200 places took copies of Miranda and it
was used for teaching as well as for
about 20 companies we using it for
research projects and then Haskell got
born in the second half of the 80s about
half of the original Haskell Committee
were people who had been Miranda users
so the influence of Miranda on Haskell
is you know is quite clear and none has
ever tried to hide it one very
noticeable change is that Haskell had
the idea
of putting the guards on the left-hand
side of the equation so they are next to
the patterns of the pattern matching
which makes it clearer how they fit
together but it's really just a lexical
transformation of what Miranda had
Haskell extended the upper case lower
case distinction I had that to
distinguish functions from constructors
and Haskell applied that also to title
expressions to dis to distinguish type
variables from type constants and that's
very nice you get much nicer looking
types I had lots of sequences of stars
which I coverage from the original
versions of ML almost everything that
was in Miranda is in Haskell often with
the same syntax but Haskell adds major
new features of which the most important
was type classes flight classes are
amazingly powerful and open up a whole
new world of possibilities so I in no
way resent that Haskell has replaced
Miranda it is it has a very powerful
additional feature as well as quite a
lot of other things okay so that gets us
more or less to where we are because of
the type of languages I've been talking
about lazy purely functional
higher-order polymorphic and so on
I think Haskell is still the main state
of the art so that is where I've got to
I've got three pages of references for
people I will put these slides up on the
conference website so people can dig
into all the references so that is a
quick canter through the history I I
think I've got about one minute for
questions haven't I sorry through summer
first of all thanks for a great talk it
was really enlightening I heard that
Simon Peyton Jones said that he doesn't
want too many people using Haskell
because then that limits what you can do
with the language in terms of
experimenting do you share this view or
do you want to see a language like
Haskell in the commercial sector
infusion no that's a very interesting
point because I wanted Miranda to be
used
I kept it stable so something whereas
when some danger and set out to do to do
what became Haskell isn't have a name at
the beginning he said exactly that this
is an experimental language anyone is
welcome to use it but be aware that it
might change next week and that enabled
them to do exciting things like
developed type classes whereas I
I wanted Miranda to be a sort of
standard but it was too early to
standardize so Simon Peyton Jones
approach was right at that stage now I
think it isn't we've got enough people
doing functional programming it's
important to have standards we've got
what is it called Haskell what's the
current standard I can't remember but
there is a standard Haskell so if you
want your programs to carry on running
next week and next year you must write
to the current Haskell standard I mean
that is it's the problem there you know
you either keep you either keep things
still and lots of people will use it
yeah thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>