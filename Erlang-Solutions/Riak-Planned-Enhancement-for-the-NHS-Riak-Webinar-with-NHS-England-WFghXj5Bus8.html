<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Riak   Planned Enhancement for the NHS | Riak Webinar with NHS England | Coder Coacher - Coaching Coders</title><meta content="Riak   Planned Enhancement for the NHS | Riak Webinar with NHS England - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Riak   Planned Enhancement for the NHS | Riak Webinar with NHS England</b></h2><h5 class="post__date">2017-10-03</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/WFghXj5Bus8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello and welcome to the airline
solutions monthly webinar my name is
Martin Milliken I'm the VP for the AMIA
region here at erlang solutions today's
webinar represents a continuation of a
series of webinars we have been running
across topics of interest in the world
of Erlang and elixir and dealing with
solutions generally based on the beam
now today we will be talking about how
the NHS is directly investing in a
number of improvements to its use of the
react database we will detail some of
those enhancements such as improvements
to anti entropy and replication
mechanisms other improved efficiencies
with the level back end and the new ring
configuration parameters this is a live
event so please do excuse us should any
technical issues crop up but to start by
telling you a bit about erlang solutions
we are a products and services
orientated company we are completely
devoted to the Erlang and elixir
programming languages we work with
organisations and individuals using
things based on the beam and we help
them evolve aligned and elixir and we
support then in their use of these
technologies now we are headquartered in
London we have offices in Stockholm
Krakow Budapest San Francisco we work on
projects across the globe and also
develop Erlang based products some of
whom include our Mongoose I am instant
messaging platform and wombat OEM which
is a monitoring and management
technology for Erlang and elixir systems
now I'm really pleased to say that our
speaker today is Martin Sumner a
technical consultant at the National
Health Service
Martin specifically focuses on designing
and building systems for NHS and
provides the technical leadership for
the large scale spine core system which
is the central messaging and database
solution for the NHS we are indeed
deeply privileged to have Martin
speaking at the
Bernard today I'm telling us a bit about
the latest technical accomplishments
with Indian HS now please allow me to
finish by saying you are very welcome to
post questions throughout the duration
of the webinar you can use the chat
facility on the webinars interface for
that purpose our speaker
Martin will answer as many questions as
time allows at the end of the webinar
we'll have Q&amp;amp;A session if any questions
nouveau on answers you're welcome to
raise them by email my email address
will be displayed in one of the final
slides of the webinar I would now like
to hand over to Martin who will be glad
to start us off okay thank you for that
introduction Robin so I'm going to talk
a bit about react our use of react in
DHS and particularly around the new
features that we're looking to add to
react and over the course of the the
next six to twelve months and so for
those unfamiliar with the the NHS in
England where the Public Health Service
and the NHS is is a relatively large
institution it's reputed to be the third
largest employer in the world after the
Indian Railways and the Chinese army so
when we do centralized IT systems for
the NHS that they're often of a
significant scale for enterprises and
and we have a number of systems of
currently using react that the primary
one being the spying database that was
talked about earlier and across those
systems we have about a hundred compute
nodes running react in production plus
various of a react nodes in reference
environments and of attack test
environments and this kind of stuff so
our largest clusters have around about a
billion objects in them and and they
tend to come with fairly stringent
requirements around always on
availability so you know zero downtime
for change 24/7 permanent availability
and you know people do use these systems
and throughout the night and you know
it's important to be available and to
you know to be able to inform someone of
a commission of what an adverse reaction
the patient's had in the past regardless
of whether they're doing the patient at
nine o'clock in the morning or two
o'clock in the Sun on a Sunday morning
the reason for choosing react originally
in the NHS was for that spire system and
it was very much focused on its
availability capability to support our
availability needs and but also on its
the its durability of data and for the
systems we run and run react on
generally speaking and there's a real
importance that we never lose data that
any changed it given to us is always
eventually made available through to the
users and we don't lose any history of
access to data or anything like that so
gerbils is really important as well as
availability and and and since we've
been using react in production I think
probably the primary benefit we gained
from it it's been around and the
long-term low cost of ownership so and
though aspects of using react probably
increased our development time at the
time it took us to get to production and
it's run it runs for as a very low
operational cost so there we are
hundreds of nodes in production we don't
have a large team of database
administrators and they were running the
system 24/7 we don't have to have react
experts on hand at all times to deal
with failure and failure is generally
dealt with automatically and can be
handled in simple ways so it's worked
out very well for us so far but now we
have the change of circumstances with
reacts crime developer basho no longer
been available and so we're in the NHS
we have to take some more responsibility
for our rehab system going forward and
for those that aren't familiar react I'm
going to try and give like a to slide
overview which I'm sure will infuriate
everyone who isn't familiar and leave
anyone who isn't familiar fairly
confused and so yeah let's see how this
goes and it's inspired by the arms of
dynamo paper I recommend you have a look
at the paper and it interprets that rice
in terms of paper and produces the key
value store written in their land that
is able to scale out nicely
but one of the primary difference
between react and other things that
implement the diamo paper is that it
protects from data loss on conflict so
it doesn't use last ride wins and you
know in the worst case scenario it will
provide back to the the application both
options it doesn't make a an automating
choice on conflict unless you're using a
specific data type which can't make that
ultimate choice for you
so given that we run things are quite
large scale the scale that was very
attracted to this given that it handles
failure quite efficiently that very
attracted to us but it was also very
truster that all that all that came
without risk of losing data that would
receive and so it has a concept of a
ring size so the ring size represents
how many individual databases exists
within the cluster so this is an example
drawn up with a ring size of 16 and
vinos we call them so effectively the
clusters were in 16 separate databases
and those 16 separate databases are
split around separate physical nodes so
he was showing six separate physical
nodes each of capable of running any one
of those 16 v nodes and but the react
core claim algorithm is distribute if
the actively knows around so that each
the knows only after running two or
three of those three nodes and when we
take a key that will and half that key
you'll be hashed a given partition and
freaking consecutive v knows you never
numerically will be responsible for that
partition so
highlighted the Vino's freeform 5 which
will handle a key this house ash to
partition free and those free nose is
split across three separate physical
nodes so that we've got diversity in
protection of that data and should know
if I fail and no longer will still be
still sodding envy no free on no.4 and
no.5 and no.6 but it needs a replacement
for V node for that was on on no 5 and
it can't and so what it does it asks the
ask the physical note that holds the
next available V node in the ring so in
this case that's the node 6 and physical
node 1 it asked that that physical node
to star an instance of vino for to
replace the instance of vino for that's
been lost from node 5 and so now things
will be stored still in vino 3 4 and 5
or something has to partition 3 but that
will now be on those 4 node 1 and node 6
and when node 6 notes when no.5 recovers
node warm will hand off into the context
of that vino for back into node 5 so it
picks up any data that's missing so in
its or shorts time as possible that's a
sort of rough idea of how react works
and this distribution of actively nodes
around the physical three nodes and
while maintaining separation physical
separation is is you know what
contributes to the availability and
durability and then as we add more nodes
and those those virtual knows V nodes
and can be distributed further around
giving us the scale outs and
characteristics so this has worked very
well for us and and where are the broad
areas where we think things can be
improved well you know the big focus
isn't available in durability and
there's some aspects of that what we
will feel we pay as the slightest to
higher cost of that so I'll discuss the
change we're looking for there and
although almost all I use of the Rio
databases is focused on gets and
so he gets and puts systemic key-value
operations we do in some clusters make
significant important use of secondary
indexes and things like and you know
tracing patience when we don't know the
full details of the patient and
particularly their identifier number and
so that's an important feature for us
and the nature of react means that there
are some issues sometimes windows when
an inconsistent can occur in secondary
index queries and one of the areas where
we're looking to improve is to look at
the course of those windows and see if
we can narrow those windows down and the
number of situations in which such sort
of inconsistency windows occur the third
broad area is that it's really gets and
puts a very very important sores and and
must never fail and we don't want the
guests and pots to fail because
something running secondary indexes has
gotten out of hand or some kind of
background database process has gotten
out of hand so you know we have a script
idea of how we want our workflows to be
prioritized and what we're willing to
sacrifice and should there be contention
and so when the areas of improvement
we're looking for is how we can sort of
enshrine that prioritization in the way
that react works the fourth area is that
you know react is used by many people
for different purposes with with
different object sizes and most our
objects are relatively speaking on the
larger side so you know our average
object size is probably somewhere
between 8 and 12 kilobytes but
increasingly as we use react and the
most popular object are growing in size
and so we have this kind of
exponentially prone problem where we're
the more like the bigger an object is
the more likely it is to be requested
and the more likely is to subsequently
get bigger so I haven't I deal with
bigger and bigger objects all the time
and we don't want to end up to go beyond
limits of scale or because of that so we
won't find ways of handling larger
objects more efficiently the other
observability and how do we know what is
in our react posture what is going on
very a cluster and especially is you
know with no basho being around there's
no kind of basho support an operational
staff to lean on so it's important that
we understand how to observe react
directly and the final issue is
replication we found a number of issues
with the Batchelor provided previously
closed source enterprise replication
features so we've been looking at how we
can prove those to make them and more
reliable in in our use case so cost
availability and durability so when and
when the original spying program was
started so this was one that wasn't
using react yet and using a sort of
old-style relational database backing
and 2003-2004 and there was a lot of
focus within the program and even the
get the NHS as a customer on on making
this the service resilient to all cancer
disasters so the various public
statements made about how the service
would survive planes landing on date
sensors and this kind of stuff and and
that created and you know made sort of
push various design decisions this kind
of stuff and when we started to on the
spine to to replace that service is that
one thing is we sort of set out and said
well you know the good news is the
planes do generally speaking land on
from ways and not on top of datacenters
and that's not actually you know a very
likely scenario and it's probably not
one should be focused on that's not to
say that data centers aren't fully
reliable and you know what we do know
from a pattern of failures across
datacenters is that you know things can
make data tend to go dark so and you
know even if power is resilient power
can go offline data centers can get get
partition from each of their can be
catastrophic network failures and data
centers and there can be over where
there are other shared components
particularly storage area networks we've
seen those things go down across
they attend those so you know we have to
deal with datacenter Vania and but yeah
the one that we're probably not going to
have to deal with is the kind of
disastrous the day attend has blown up
time scenario there are over more likely
data center for failure scenarios that
we should be focused on and of course
individual servers fail and you know all
the time that's a constant thing and
they fail and come back without as know
seeing and they fail and never come back
and fail to the point of destruction so
so we have to be able to do that so one
of the things that we want to make sure
was that in all these common dates and
that all these common fate of scenarios
you know servers failing both temporary
and permanently data centers going down
power going off in the data center data
centers partitioning from each other
that not only do we maintain
availability of service that we may
maintain the durability of datas that
the data is stored in more than one
physical node under all of these failure
condition conditions and that's
something that wasn't occasion in the
previous system is that it could you
know kind of not necessarily
automatically but eventually recover
from some sort of some of these failure
scenarios but after that recovery you
had a loss of data resilience for a
period and and we want to make sure that
we were still duplicating data even
under all of these common failure
scenarios and so this this actually
works an out of the box with react but
we what we want to do is try and do this
at a slightly lower cost so at the
moment you can confirm that a right has
definitely gone to to physical nodes in
a cluster but only through saying
something called a PW value which
confirms that it's gone to two primary V
nodes well in many circumstances when a
right hasn't gone to two primary vinos
it has actually gone to to physical
nodes and the pasta so if what we really
care about is that the fact that it's
gone to to physical Dino's not whether
it's gone to sort of where it's got a
true physical nodes rather than two
primary V nodes
we would prefer that to make that the
configurable parameter so as we will say
a write successful regardless of how
many primary vinhos it's on as long as
it's certainly on two separate physical
nodes so we want to add a write put
parameter that provides that no confirms
and the other thing is because we know
that datacenters can go dark is that we
may have sent a right to multiple nodes
but all those nodes could lose power at
the same time so if that right is only
memory on on those nodes then we could
still possibly lose and that data and so
to get around that in our current react
configuration we use a configuration at
the back end to flush every right to
disk so it means that when we write to
our react lustre it gets rid of three
nodes and there are three flushes that
have to occur in order to complete that
right so that's fairly expensive and and
we kind of get round that through some
hardware magic using flashback
flashbacks right caches on the on the
raid controllers and we'd like to make
that slightly cheaper so we'd like to be
able to be selective about how many
nodes the write actually has to be
flushed on and so you know it's probably
enough from our safety perspective that
it's been flushed on one node and not
three and if it's been confirmed that
it's also been in memory of another node
so a combination no confirms and
selective sync we hope we'll be able to
reduce the available TeamViewer
abilities and the cost of that so so
there's their configuration parameters
so right configuration parameters were
working on at the moment and then the
other thing is is that there are some
circumstances when you build and expand
a react lustre when it can choose an
inefficient allocation of resources
around the cluster and that can lead to
more scenarios when multiple node
failures can lead to a situation where
where individual drives aren't actually
going to enough
diverse nodes and so we've got some
improvements to that claim algorithm
that distributes those V nodes around
the physical nodes to try and deal with
that situation and better so that we
have that that's far less likely to
occur as you expand and contract your
your your cluster so that's the first
set of solutions that we've got and and
most of those things is stuff that we've
we've now got in the case that the
change to call claim we've got some
significant testing around it to prove
that words in the other two we've got
some basic testing done or not yet ready
not yet released ready so index
inconsistency is the next area of
problems and so it's a particular
problem when we're doing kind of fourth
replaced by cooperation so when one
individual node has has gone beyond the
state repair let's say it's a you know
each rate controller has died and that's
corrupted in loaded disks and this kind
of stuff and so we need to force replace
that in the cluster we can't do a neat
replace whereby it slowly hands off its
data we have to kind of force replace so
elect a new node to be a member of the
Ring and getting that to begin to learn
about the old data from from of a note
from lovely nodes across the across the
physical cluster and during that
operation we believe in the moment that
two I queries will potentially be
inconsistent and won't give reliable
results so we currently have to perform
that operation whilst the site is shut
down and to avoid that occurring and and
that's fine but it leaves us in a
nervous situation that you know what
would happen if we had a catastrophic
failure on the site that what that was
there was active and whilst that
operation is taking place so we wouldn't
be able to deal with that scenario the
other thing is that is that within AV
node on a physical node is that
internally can have entered the issues
so you know
when the leveldb that back-end writes a
key value change and some index changes
of it that's written transactionally as
one right operation but then over time
the index values and the original object
value will migrate to different areas of
disk so if possible that we can lose the
index values about losing the object
value and therefore we get incomplete
results return from a to I query and we
don't really find out about it through
throughout Comanche entry
entropy which is focused on
inconsistencies between object file news
and then the final issue is with the
short term window that exists whereby
because we will accept to write once
it's been sent to vidos there's then a
very short window whereby if secondary
index query happens to choose a coverage
plan that went to the thirdly no not to
the original to be notes then we'll get
an incomplete query and you know that's
kind of fine at the moment because it's
a sort of arbitrary race between the
query and the write anyway in our system
so and and it's kind of arbitrary
whether you get the answer or not but
still it'd be nice to sort of find ways
to narrow that the potential for that
window occurring so so we can't
necessarily spot these thinkin system
and O's but potentially we can reduce
the length of the windows and as they
occur so the solutions that we're
looking at here is we're trying to look
for a way in which we can actually
convert through configuration the status
of a given node so that won't
participate in coverage queries so if we
know that a node is in a bad state
either because of the kind of internal
entropy issue or because it's part of
the fourth replace it can be state
itself to be outside of coverage plan
and therefore coverage plans will avoid
it so we'll avoid those inconsistency
windows and the other thing that we're
looking at is ways in which we can do
more more flexible queries to support an
entropy so rather than have a static
database around
entropy which exists in react at the
moment we can actually dynamically query
react so we can do things like a query
react to say is a coverage plan on one
offset giving the same answer has the
same Merkle tree for an index as a
coverage plan on a liver offset and
perhaps does the coverage plan on a
given offset if you do a murkily that
indexes I agree what the indexes should
look like according to a coverage plan
that was run across the objects so
finding ways in which we can more
flexibly compare and detect issues of
inconsistency between indexes and
objects is another area that we're
working on and then the other thing is
when we when we want more consistent
results currently we use G sets rather
than 2i queries and you know also we
make some use of sets as well so the
actual CR DT sets within react and so we
want some ways in which we can actually
prove that there's no entropy that's
been introduced whereby the content of
the G set or the set has drifted from
the content which it should be based on
the attributes of the objects so we're
looking at some fast little and entropy
queries there and the general mechanism
were using these a metric queries and
we're calling and tick-tack trees and
this is something that's implemented in
my level ed backend which is an L and
key value saw designed as a react
back-end and this does local trees just
as the existing react database does
Merkle trees but they're special Merkle
trees that they don't have the
cryptographic strength of the normal
Merkle tree but they're measurable and
you can grow them incrementally without
needing to know the full key space in
order to trade them so it makes them
much cheaper cheaper to produce in folds
and to and make
it possible to merge across phones merge
trees across phones as well I think it
gives anti entropy trees that can be a
fixed sizes and and this fixed size need
to be transferred across the wire so
they have some interesting
characteristics and we want to try
building those and directly into react
and so we can use folds for those ticked
at trees in lots of interest in
different ways
the other thing is prioritization so
secondary index is other things use
thing called Li node workers and in it's
kind of normally configured react list
over normal ring size and a normal
distribution V note you have about a
hundred Vinodh workers per node well you
know a hundred Vinodh workers you know
we have probably we only have six this
on every node we only have twelve CPU
cores so they're more than capable of
saturating all the load on the on the
know so all the all the capacity of the
node so so at the moment and you know
background jobs and and to I queries
tend to use these Vinodh workers and and
you know we have no way of stopping that
from from bringing the whole cluster to
a whole and some of the other more
expensive jobs have some additional
locking and front link schemes on top of
that but you know kind of every job
can't instantly sent block confronting
scheme within in react and and so we
want to make sure that in the context of
these background jobs and the contours
of to our queries using V node workers
we can maintain priority for get some
pots because you know they're the
primary purpose of our react stores and
so there's two things that we're trying
to do here one of them is the again the
level ed back-end which the prime
distinction between level ed and level
DB is it separates out keys and values
and so that rather than having the key
store together with the value the keys
stored with the metadata and the value
store separately and so the merge tree
that's created the stuff that's ordered
by key
contained only the key in metadata so
that when we scan that merge tree we can
scan we can perform a scan that's you
know equivalent to the size of the
combination of all the keys in the
metadata not current size of all the
keys and the values and for us with
large objects that's a big difference so
it's much much easier for us to have our
key store that contains the index
entries and the key and metadata in
level head you know fully covered by
memory and yeah we are prioritizing the
page cache that kind of stuff so we can
actually make the job of scanning over
that much much cheaper and because the
metadata tends to contain all the
information that we're interested in or
the other two are key information for
all these types of jobs we can actually
make these jobs fundamentally much
cheaper so so the risk associated with
one of those jobs overloading the
service is reduced and then the other
thing is we're looking at is is
introducing one or more node worker
palms so rather than having a separate
worker pool for each V node have a
second worker pool for each physical
node so the initial information Tom
we've done for that is one where we have
a we have a pool of only one worker per
node and then all the shant kind of
deferrable background drops such as you
know a tree rebuilds and all such things
are sent the node worker pool rather
than the V node worker pool and then we
know that well that's only running you
know on process and so should only
consume one CPU call and effectively
want disk and this kind of stuff so we
can make sure the majority of our work
is protected from that and we're
thinking about how we can potentially
have something like you know a very
constrained pool for low priority work a
small node worker pool for kind of you
know mid priority work that we don't
want to be slowed down by the very low
priority work and then reserve the V
node worker pool only for stuff that
definitely has to happen
immediately so we have level ed is is
kind of it's working and available now
but still needs improved testing
particularly around sort of fuzzy
testing quick check type testing before
it's ready for production environments
and there's some initial rough code from
no worker Paul stuff that we've done and
and some initial volume testing of that
to show that that has potentially you
know some smaller positives and
difference there so that's still very
much a work in progress
that work so bigger objects as we said
you know you know more popular objects
are getting bigger all the time and
bigger of the objects is the more likely
to access it and that causes some
secondary issues as well and
particularly leveldb because the key
store the value and we found that
leveldb
were closed can get dominated by right
amplification as the as those values and
merge down the merge tree and constantly
have to be rewritten that becomes a
dominant factor in this business and the
fact that the value has to be read three
times for every fetch even though in
when the value is the same across all
three V knows creates extra dis load and
extra network load and some interesting
problems and like TCP in casts for where
were you see as he sets we're having to
manually shard those sets at the moment
and we find that large objects has
commonly been an issue with replication
as well so often we've seen replication
failing and often when we've seen it
fail it's been associated with large
objects so really our requirement is is
that we know large objects you can make
things harder how do we mitigate the
risk of a performance decay and
increasing performance decay as objects
continue to get again and so part the
answer is going to come free the level
eight backend again so where the merge
trees are going to be keys method a and
not values that right application is
reduced dramatically the other thing as
part of that is we're looking at
changing the gate
so that rather than asking each back-end
to give a get response comparing all
free seeing that by and large that all
free at the same and then discarding two
of those responses what would change the
get FSM now it can act send three head
requests first the only in effect
returned the keep metadata they can do
the vector clock comparison and in a
normal scenario they then only read one
get request or furl so rather than
reading the object free times from this
we only really wanted this rather than
transferring in freedoms over the
network and getting the in task risk we
transfer it only once over the network
and then potentially also we can open up
those head requests so that they're and
they're accessible from the attention as
well so situations where the application
is perhaps only interested in whether or
not an object's updated since it last
read it it can do a head request rather
than a get request and avoid pulling the
whole object off again and avoid the
very nasty situations that can occur and
particularly have sibling explosion when
objects become unreadable and then
become undeletable because you can't
find vector plot because you can't get
it if we have a genuine head request
support where it's when we can resume
risk there so to resolve the issue
having a shard sets at the moment we
need some of the word that have been
worked on a show with russell brown
around big sets which brings into
operations on the pen that occur subkey
and rather than require the fetching and
updating of the whole set object and so
that stuff that we've would not made
much progress on yet but we do think
that with a pure line back-end
we may well be a to such a level aid we
may we'll be able to make faster
progress with some of that big set work
and going forward and and then the you
think around replication and one of the
beautiful of areas where we believe
replication has a problem with big
object is you know as it tries to manage
its multiple queues for distributing the
sending of those objects across the
network
and you know the current replication
code rights has its own tube mechanism
so one of the looking at is to say well
you know there is already a queue system
written in a land called rabbitmq and
will that actually handle larger objects
better and will that make it easier for
us to manage multiple queues and
workaround situations by replication
gets blocked by by big objects and so so
Russell Brown has done some work on that
and he now has a fully working prototype
for replacing react Enterprise real-time
replication with a rabbit mq based
replication of code with multiple
RabbitMQ instances one and every
physical mode observability so and then
when you want to know what was in your
react database the kind of traditional
react ancestors we say well you know you
put it in there
so you know you tell us what's in there
and yeah the reality is is that again we
don't feel that's a particularly
satisfactory answer and so you know we
want to be able to actually make
observations about what's in I
reactivate base what sequence on its
side actually precisely how many objects
could be got in there you know what we
histogram object size is how it
currently has things this kind of stuff
and there's no way of telling that at
the moment so we want to we want to we
want to provide an answer to that thing
there's there's a lot of processes in
reactivity around and for example
expansion variables that have a habit of
logging to tell us they finished but not
logging to tell us that they've started
so you can be in situations whereby
you're sitting poor performance in your
cluster and the the thing that's causing
the performers didn't log at the start
so you don't know that it's going on a
presence so more of a observability or
what's going on sixty with background
processes then the only thing as well is
that you know react is you know is is
written in Erlang and so the various
things we can do in airline to observe
what's going on and it to it and
until that goes into the backend and if
we're using the leveldb back-end that's
you entirely written in C++ so we need a
different set of reserved ability bills
to what's going on in the backend and so
we'd like to sort of break down that
barrier and even the big kiss bits back
end which is you know allegedly pure
airline a large part of logic is still
written in seed sniff now particularly
the main part of that so we still don't
necessarily a full observer see from
from an airline perspective about what's
going on and and then the other thing is
that you know sometimes we want to know
something
fortunately about all our objects in the
cluster and there was a MapReduce
capability in react that was intended to
help you if that kind of thing but that
MapReduce you know wasn't very well
controlled so it was a ding with back
pressure to stop you from overloading
the MapReduce query but not from
consuming all the resources in your
cluster and stopping of all that from
going on
so we want to nice away kind of writing
reports of more functional reports
around the content of objects across the
cluster as well and so I basically crime
and here's some very informative
contests and activity so one of things
that were looking at the moment is
optimize falls over object metadata so
same mechanism that we're putting in for
doing different false transient reverses
is we realize you can do those use those
same faults for observing contents in
the database both non functionally in
terms of Sealand counts and size and
objects and council objects and
discounts that and also functionally if
we if we can load some key information
into an object better data we can we can
use those forms we're also looking at
sort of sampling folds
whereby because we know the the load is
evenly partitioned
perhaps we don't need to query all the
load we just need to query the contents
of one veno or we can extrapolate out
from there and you know to do this kind
of stuff we want folds that have
accumulators that have things other than
lists so you know if the if the
the if the foal can only produce a list
then that means we still maybe just had
to stream a lot of data back to the
application and if actually are
interested in account you know then the
foal should be able to produce just
account and if what we want is a you
know a bloom filter to produced or a
Merkle tree to produced from the farms
that's what should introduce it also
looking at doing that and really the
answer to some of our observable
problems with backends is again level ed
store given this written in Erlang
allows us to have more joined
observability and end-to-end and it
makes it easy for us to understand
what's going on end to end and because
you know there's a common language
across across all the code so so that
work i've mentioned as the station level
ed before and these different type of
falls and that kind of stuff that's very
much working progress at the moment i
was hoping to have something ready and
publicly available on that roundabout
now it's probably a week or two away but
hopefully if we can get a slot at the
bet365 and workshop in stone and may
have something that can present working
on these different types of folds there
and finally replication originally
replication of being in our big focus
because that was seen as the Perea so is
going so having a fully open source
react and with great thanks to various
in five we no longer have to make that
our focus and because soon you know
replication should be available to
everyone open source but there are some
problems with replication and our
biggest issue with replication is that
and is that the current enterprise
replication coned
really only works if you've got common
ring sizes between your two clusters and
you know at sub stage and on the state
you're going to need to change your ring
size and the internal ring might change
code in in react was never something
that was recommended for you about a lot
of hand-holding from Basho and with no
basho being here it's not something we
want to look at again
so the easy way we think and changing
being size is to have replication that
works independent of supply so if you do
change them in science you just start a
new cluster and a synchronous
replication flips that custom which has
a different resize and get it to in sync
and then for cash alone owns that new
cluster and so your replication works
like that
ideally replication doesn't pend on this
key doesn't have band concrete and the
new thing is when we look to compare the
state of two clusters reaction only ever
compares the whole state at the moment
through its Enterprise replication
process where actually what we're
probably most interested in and what's
most likely to occur as a gap and is
recently sent rights so we want to be
able to make things more efficient by
separating the job of comparing recently
received rights from comparing and all
the rights that we received and so
that's another area that we're looking
at and generally speaking you know
replication code is quite complicated
and think potentially that could be made
simpler potentially by you see
introducing ferment party and products
and for most of the NHS
easy go we're really really focused on
bi-directional replication and an
ideally we want it currently to be to be
controllable externally so that you know
we don't have to rebuild react if we
want to change how we schedule
replication and and that kind of thing
and so as mentioned Russell Brown's work
on rubble and that's out there and I
think provides a less complex
replication mechanism and potentially
using things like topic-based queues and
multiple subscribers and this kind of
stuff
it may well open up in a simpler way
different replication approaches I've
talked a bit about the attack Merkle
tree folds and we see that it's being a
big part of how we compare and state of
clusters and we're looking at how we can
get that to work so we can compare the
state of to react clusters but also
potentially the state of react lustre
and the same data stored in a different
database as well so rather than making
the job of synchronizing and two
databases require a whole load of
specific code written in to react as is
at the moment with comparison with solar
is that we can make that a more generic
externally available feature and people
can potentially compare state of react
database with any open database of their
choice as long as that database has got
that sort of excellent clocks in it as
well and then the other thing we're
looking at is is its objects giving
objects a self expiring index or an
index that will that will disappear
after a few hours and after that index
was written but an index there's ordered
like by last modified day and that we
can run these Merkle tree falls over so
we can do and very quick comparisons of
can you tell me have you both sides
received with all the changes that were
made and you know in the in the past a
Dora much quicker than comparing the
whole state so that's what we're working
on moment and all that stuff is kind of
a lot of stuff in theory and some kind
of prototype code and that nothing yet
ready for production
so this rather lot that I've gone
through there and hopefully it kind of
wasn't too confusing just hearing we
talked about it and and you know this
presentation is available online and you
can contact either myself or Russell
that we worked on this also dr. Amin Sen
that you you make those in from the real
world may not be familiar with he's the
lead from the NHS internal team that's
been working on this and he's also been
writing a number of these features as
well and so we should be available and
going forward and I do hope the for
those are able to attend the workshop
and stoke that we'll get chance to
present some of these ideas and
solutions in more detail and so you can
get a ask more detailed questions and
get a more detailed view of them and
everything that we do do is that we are
going to you know put out and long reads
on github
x-play and will advertise to them and on
the slack channel and the make
it's part of the definition of the work
packages between the NHS and the people
working at work that everything we do
the available open source so there's
nothing to do that we won't publish
sighs so there will be more information
becoming painful on these thicker or
what so okay okay
Martin thank you for a very inspiring
talk and react which I thoroughly
enjoyed and I'm sure the members of the
audience did as well
I'd like to formally open the question
and answer session which we promised we
have a small amount of time available we
will answer your questions in the order
in which they were received and we will
try and answer as many questions as we
can so Martin to dive straight straight
into that one of our audience members is
interested in knowing a bit more about
what features specifically would you
personally like to see added to react
going forward so for me personally
rather than did sort of NHS of the whole
I think that the big thing that I've
always wanted
and in react is more flexibility around
to my queries in terms of the the
functions that can be passed in and the
accumulators that can be used so I kind
of contributed towards a feature that
exists at the moment which is the
ability to pass regular expressions into
to my queries and I think this is an
underused thing and I do think as a
general rule there's some there's some
very interesting things in in react the
in the past and they had side effects in
production and the kind of the answer
was sort of not to use them so you know
- I was kind of discouraged usually it
was discouraged to a certain extent
MapReduce was discouraged quite a lot so
I do think that this there's some stuff
that we can achieve with perhaps you
know fairly
limited work by actually making those
features more usable in production and
slightly more flexible you know we can
you know yeah the work I'm a producer in
particular from Brian thing with really
really good work and it's kind of sat
there unused because people are
recommended not to use it and and rather
than have to sort of create a whole new
interesting and complex world it would
be great if things like that we could
actually begin to make make use of by
trying to focus on the reasons why it
was recommended not to not to use them
in production in the past so I think
there's some there's some quick wins
there and but you know we've built some
really powerful things using second to
AI queries we've written our own query
engine and this kind of stuff and I
think this problem stuff probably some
other people could do there and that
would perhaps avoid the complexity of
doing things like integrating solar and
I think that's that's the area that I'm
most personally interested in Martin
thank you for that we'll move straight
on to the next question just to try and
honor as many as many of these queries
as we can one of our audience members is
interested in your comparison between
react and other no sequel options that
you may have experienced with obviously
this is the assumption is you have some
experience with other no sequel options
how would you compare them yeah I mean
most of the experience came from the
point when we were kind of evaluating
between them and obviously we've
refreshed that evaluation recently to
make sure that you know the right thing
for the NHS is to continue with react
and I guess the the primary areas where
we felt react stood out was the stuff
that I mentioned before about you know
the focus on data protection on conflict
and the flexibility about how to handle
conflict and the you know the automatic
data types and this kind of stuff it
really really really worked well for us
because you know we are focused not just
on availability but on on long-term
durability of the data
that need never to lose any history of
what's gone on so so that tends to work
better for us and that tends to be a
feature that's missing from other no
sequel databases that you know may scale
out better and be better
the other thing is is the the automatic
recovery from failure and so what we
found a you know no sequel products is
that you know they they handle failure
well but then require an admin process
to get nodes back online and this kind
of stuff and don't necessarily deal with
no it's flapping up and down as well and
this kind of stuff so kind of
intermittent failures and this kind of
software we found in operations
you know reacts you know it's very very
efficient prism requires very little
attention and and that saves us a lot of
money and it you know it may well be
that you know of other things that have
other operational features but certainly
our experience of react is that from an
operator's perspective and aside from
the things we've highlight around
observability it's extremely low
maintenance and it also and I don't know
how this compares to of an OC clusters
an open a single day basically runs very
very effectively on very low cost
hardware so very for so low speed CPUs
and slow this and this kind of stuff
which can still get significant fruit
per react databases and and because you
know we don't have to recover from
failure quickly we can deal with
multiple nodes failing we can have low
cost our hardware that doesn't have fast
response time on failures so we don't
have to you know get a man and run out
quickly so our hardware and new to you
crisis and I've just crashed and we make
enormous savings on hardware and
compared to what using previous
databases that tried wonderful Martin
thank you for that I think that's a very
comprehensive answer and in fact you
kind of tied into the following question
that we received and I'll just ask you
to sort of expand on that's a bit one of
our audience members is basically asking
in terms of operational ease of life how
would you compare react with the kind of
more mains
traditional databases could you give us
almost an order of size or a kind of
concrete practically numerical
comparison in terms of savings and
ease-of-use yeah I mean and if we
compared to the previous incarnation of
spine and you know we had a there was a
team of full-time DBA is required to
keep the system going and that was
without sufficient rehearsal of various
disaster events and this kind of stuff
and I know I've you know we have a log
of activity of who's logged on to the
database and this kind of stuff a week
and often go four or five weeks without
anyone connecting to a database let
alone anyone having their full-time job
to look after the database so you know
we we build systems for government that
last a long time you know that the time
in development is relatively low
compared to the time in operations we'd
expect this system to last at least a
decade so saving in manpower is huge and
it's a cost that really increments for
us and the hardware annuity charge
difference is probably two orders of
magnitude that we've saved there and
it's it's an enormous difference and
probably we're already at the stage
where we've accrued a saving of over ten
million pounds and compared to the
previous solution just in database
infrastructure and operation costs and
so yeah it has made a it has made a big
difference compared to having and what
was at the time the the the the best
practice and enterprise sequel solution
Martin thank you for that I think that
sounds fantastic and you know I think
we'll all agree the NHS can you know
redirect the savings and use them in a
different area to our benefit to the
benefit of everyone using the system
really so we have the following question
which i think is quite interesting but I
think we'll need to post it over to you
because it involves a couple of formulas
so we'll just do that now you should be
able to see the question in your chat
window and perhaps you can kind of
quickly summarize it and then give us
your answer okay so this is about the
the combination of guests and heads so
yeah so I I did look at that so whether
we could do one get and n minus one
heads rather than n heads and one get
and I'll look it the answer is quite
complicated so it does make it is right
by by doing and in mark I'm sorry n
heads and one get was we increased
latency in the system and the one of the
problems was I think if I recall
correctly was how we would deal with how
in part how we were going to deal with a
mixed back end clusters so clusters
where we'd only rolled in some nodes on
level aired and some nodes on level DB
and so so that was in the consideration
and I probably can't recall the whole
argument that I had with myself at the
time that's definitely something I
considered and if that's from hinds I
think what I'll do is I'll try and
answer that in a kind of written blog
post in a bit more detail and and take
some time to think about the precise
reasons and because I did have think
about maybe changing the FSM so it was
more like the put FSM so he actually
went to one of the one of the nodes that
was one of the owners and that would do
a local get and then the two heads to
cut there to two to compare it with and
and and I decided not to do that and so
I'll try and give some more detail on
that
thank you and we'd also like to thank
our friend Hines who asked a question
and as you probably heard Hines Marting
will try and provide a wider answer
perhaps even in form of a blog which is
fantastic so Martin
I guess you know with paschal stopping
effectively the running of their
business and stopping stopping trading
the big question in everyone's mind is
the longevity of react and how react
sort of moves forward you mentioned that
the NHS believes in react and the people
like there 365 and other large
corporates using react believe in the
database on it in its future can I just
ask you is the commitment to using react
in the NHS a long-term commitment I mean
I think it's definitely a medium 7
commitment so it's definitely a
commitment we've made in terms a number
of years you know we were only moving
across soon when the sort of news first
came out about basho we did a
reassessment and we said ok you know
events have changed should we be truly
moving and the answer was no there was a
good reason why we chose react in the
first place and that good reason remains
true and also given our experience could
be work closely we borrowed because we
had access to source code and that kind
of stuff is yeah we have a degree of
confidence that within the NHS and the
partners that it's worked the NHS works
with we have enough capability to
actually and maintain it going forward
even if you know there's there's no
support from from other third parties
and if the community doesn't form and
this kind of stuff so I think for now
there's there's there's there's
certainly no ongoing project to consider
changing it and so it's you know and
we're unlikely to reconsider again you
know in the next year or two but every
technical decision we have is something
that we subject to review sort of ence
occur so you know I can't guarantee you
will still be running react in ten years
time but there's currently no intentions
and not using react and given you know
that there is a certain amount of of
drag when you've got a billion odd
objects in a database it's it's risky to
change so we will need good reason to
move away from react
and the moment we've got good reason to
stay with react so it's probably the
opposite end of the spectrum troffer is
moving Thank You Martin the time has
almost run out on us but I would like to
ask one final question that we received
from Andy and he was asking have you
investigated Delta based CRD teas at all
and so I haven't but I'm pretty sure
Russell Brown will have investigated
that so and I'm probably not in a
position to answer around that at the
moment we're just focus on CID teas on
using the kind of simpler forms and and
but yeah we delve a co-teacher always
something we'll look at I guess I'm a
bit because I'm not littering myself and
maybe unclear um the terminology and
whether some of the things that we're
doing with kind of and single key based
operations are equivalent to those Delta
based the oddities or whether there's
something altogether but Russell's our
man on that and so I was just popping
him a question on slack around that and
he will no doubt be informing much
better than I can
wonderful Martin thank you for that I
just like to mention in regards to the
penultimate question I asked you so in
regards to the longevity of react I
think it's evident that NHS is obviously
investing a lot of trust and a lot of
work in react bed 365 is doing the same
other companies are doing the same I
just like to say that with the
unfortunate demise of Basho
we here at erlang solutions believe in
react to we believe in the product we
believe in its future and as we have
worked with the show very closely over
the last several years we are in a
position of deep expertise poking react
and obviously in terms of Erlang and the
being in general so what we now do is
provide very extensive react support if
you're using react if you feel you need
support please talk to us we can
provide anything from working hours to
24/7 coverage in other words you've not
on your own and react is continuing to
be supportive and will hopefully
continue to be developed as a product
and that's something that we will be
working on with our partners and
customers so just to say first of all
our team thank you so much for an
incredible webinar that I really enjoyed
and from from reactions and feedback I
can see others have as well I just like
to say and finish by saying everyone is
welcome to obviously look at our website
following this webinar we will be
posting the recording of the webinar on
our web pages so you'll be able to take
the entire session there and obviously
see that again if you'd like to many
thanks to our audience for joining us
today we will send you a very brief
survey to make sure we capture your
feedback from today's webinar if you
have any questions remaining then
obviously feel free to email myself
martin has left these details as well
we'll be happy to answer any queries you
may have so thank you all once again and
we very much look forward to seeing you
on our next monthly webinar cute</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>