<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Erlang Micro-Services with all the Buzzwords - Chad Gibbons - EUC17 | Coder Coacher - Coaching Coders</title><meta content="Erlang Micro-Services with all the Buzzwords - Chad Gibbons - EUC17 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Erlang Micro-Services with all the Buzzwords - Chad Gibbons - EUC17</b></h2><h5 class="post__date">2017-07-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/9o-2K-5SaGw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">mics on can hear me back there alright
um so I'm Chad Gibbons I'm a director of
engineering at alert logic and I'm going
to tell you a story
it's the story about our journey to
using micro services in Erlang how we
got there and how it worked out for us
you know I talked about the buzzwords
and the title of the talk because we
really touch upon every single one of
these instances of buzzword e stuff when
we created our system really add a
necessity let me give you some context
about who we are and why we got here
there's a little bit of history involved
here but it makes a lot of sense so a
logic as a company where security as a
service company and really what we do is
we monitor our customers networks you
know we're ingesting so much data from
these customers and we figure out if
there's a security vulnerability or if
there's an active security attack and
then we notify them right so it's kind
of the enterprise monitoring problem but
focused on security part of that is
actually we have also a security
operation center filled with hackers
that actually knows how to hack things
and what hackers would do and we use
that as an expertise bridge to our
customers we talk about data I love this
graph I made this the other day this is
actually real so right now we ingest 1.6
petabytes of data per month and we're
planning for 100 petabytes per month in
just a few years and our growth rate has
been like that since the inception of
the company sometimes bigger sometimes
slower but generally it comes like this
this is not only from adding new
customers we're at 4,000 today this is
from customers adding more data as you
would expect and for us adding more data
sources so you can imagine the problem
space that we're in is truly big and
there's a lot to worry about on a lot of
distributed systems to manage that so
our engineering history and this is
really important to figure out how we
got on this path that we are on
company's been around since 2002 and
early on it was really about taking open
source tools and making them work for a
service you know that's really what was
built there's a lot of Perl code and all
that all the integration between these
pieces of code was done at the database
layer and we had a lot of my sequel
based integrationists
and years later was proven to be very
problematic that changed pretty
significantly around 2005 they got a
bunch of funding the company grew quite
a bit we hired some world-class
engineers this is when we started using
Erlang
what came out of that period was a new
product within the company to ingest
customers log files which you can
imagine is a lot of data you know
because we ingest all of them what we
built out of that was a cluster a grid
cluster and it's almost like a real-time
version of Hadoop if you want to think
about it that's our big earlene
infrastructure that we still use today
very powerful for certain kind of
computing model few years later this is
about the time that I started the
company 2011 we grew again had a bunch
of new engineers new ideas new products
that model of computing with Erlang
didn't really apply to what was being
built at that time so we kind of went
off on our own and did different things
and there were different implementations
of erling modules we had like two or
three versions of Erlang logging and to
entry versions of Erlang web servers and
it was kind of a mess and when I came in
in my role and I was an architect at the
time one thing I would point out is that
we didn't really have a company culture
in engineering and it was real apparent
I mean it was just little silos even
within engineering so about that time so
2013 we wanted the new approach not only
from the business but also from
engineering the business really wanted
to make another big leap in technology
but we couldn't really do it on the
foundation we had built before so I did
a proposal and we were like let's make a
new platform from scratch let's go off
do a skunkworks project figure out how
to do something new see how we can apply
it to our current business and a future
business and we really wanted to define
an engineering culture out of that so
like six or seven of us went off and we
did exactly that I had the chief
architect role at that time and I had
six peers plus a business guy that were
all about the same level so we had you
know a lot of Chiefs in the room a lot
of discussions about what we should do a
lot of disagreement but we came down to
a set of principles
and I'll talking about talking through
all those principles and that really is
the heart of what we did here the first
was we really wanted to decide you know
what we were going to build on you know
we could build a cluster computing
system again or we could do Web Services
or something completely different and we
decided you know distributed micro
services made a lot of sense you know I
have been doing those and micros to my
career in one way or the other and seen
them work in a production context
especially for software as a service we
really wanted to focus on the software
interfaces not database interfaces or
anything like that so we just said from
the get-go everything is going to be
either a REST API or publish/subscribe
messaging and we'll deal with that as it
was before then there wasn't a standard
within the company about how components
should talk to each other we also wanted
to recognize Conway's law and if you're
not familiar with this one this is the
one that basically says software
architecture and organizational
structures match and if they don't match
they will eventually whether you want to
or not so you might as well recognize
and embrace that and what that really
meant for us was let the team's really
own components and focus on them not
have components shared across teams or
anything like that because that winds up
not working that well this one was my
favorite you know chief architect this
one is one we debated the most mandate
as little as possible some of us wanted
to mandate everything like that we shall
use Erlang you should do it this way and
it was a lot of back-and-forth and we
kind of came down to how would we want
to be treated if you know an
architecture team came to us and said do
it this way so we really wanted to focus
on the interfaces not the particular
implementation you know document the
standards of Health things should talk
what's funny about this one is years
later the one guy that was the most
adamant about us being open was the one
that wanted to think make things more
strict as the company grew so people can
change their mind over time so we came
up with a set of design principles I'll
go through them now and a set of best
practices and it's really interesting to
even say use best practices and yet I
think all of you have probably had the
experience where the industry best
practice is known and engineers go off
and do something different kind of
wanted to change that right let's go to
the design principle so this is really
what we decide to do from the get-go
first one is is everything as an API
there's no exception of that doesn't
matter if you're making a micro service
that's designed to be an API or
something that just processes data
everything has an API we use that not
only for the software interface itself
but also for our monitoring and our
control like any simple example of that
would be a health check of the services
done through a REST API so you always
know how to manipulate these things a
follow-up from that to was you know
everything should have a standard API
pass again it should be obvious but it
wasn't done that way before
for us as a SAS company we deal with
lots of customers and it was an
important to have a standard way of
identifying customers inside our URL so
you can imagine a lot of the servers
deal with data that's specific to a
customer sometimes they deal with data
for everything we wanted it to be
consistent and you'll see layer later
this actually applies toward network
routing layer fairly well this one is my
favorite most controversial what I have
to explain it usually people get it over
time every API is public as a security
company you can imagine an immediate
reaction of this was no way it turns out
this is real important for a lot of
reasons and I'll share the reason why
before this we would make an internal
service that did something useful and
inevitably a partner because we have a
lot of service provider partners like
Rackspace that use our service would
want to talk to it and we would make a
proxy API that was secure and specific
to them or maybe a few other customers
if you've got two different systems like
that there is drift between them always
you know you can't keep up there's two
separate teams and the team that makes
the service really can't be bothered
with the public API part of it so you
inevitably have that problem you also
have the problem internally of how to
other development teams within your
company talk to your service and there's
doc maybe if they got around to it right
you know so you can only really do it
when you have time if you go down the
route of this though you're pretty much
up front saying we want every service to
be documented tested reliable secure as
if it was exposed to the Internet which
they are there's lots of ways to make
that secure but that was the premise
now we don't make everything actually
available through authorization but it's
all there
we don't make all the documentation
available to every end customer but it's
all there for us internally and a good
example of that we have a service that
does billing and it's purely internal it
processes data it uploads it and I can
control it through an API you know we
don't document that one but I have no
qualms about it being actually reachable
from the internet because we have
authorization around it we also put this
in actually a screenshot of our webpage
we put the documentation in our console
so every customer can go see it and
actually non-customers too and when you
click into one of those services you get
documentation like this and if you've
seen that format before its API doc it's
a nodejs project that does great
documentation fantastic for REST API so
every single project has this another
thing that was important was a follow up
from that is we wanted opposite authen
or excuse me
authentication authorization and
auditing to be built-in and pervasive
everywhere no excuses
so we just did that we made it fast we
solved all the problems about you know
caching and building a framework to make
it easy but it's there every single
service has this built in for every
single API path there's not a special
service that has a backdoor it's just
there because the service framework
won't actually let you specify a method
without that we use kind of a very
common way of defining these permissions
just like Amazon does at other places do
it's a colon separated list and you can
have optional account identifiers but
the general idea is that it's a service
and an operation and you know you can
figure out what things need that
permission the other thing is that every
user in our system every service in our
system has its own identity so if one
back-end service wants to talk to
another service they can either do it on
behalf of an inducer or it can do it on
behalf of itself it's all treated the
same I'll actually show you some Erlang
code this is how permissions are
actually done in our code I've got two
examples here this one is from a service
called Ticketmaster and this is a fairly
typical example of I want to create
something and I want to do it only for
this account so if the UI is calling
based on my user credential it will only
allow me to create
for that user ID I can't create it for
marks as an example compare that to this
other service called Auto as marks that
naming things are hard so names are
interesting this one doesn't have
account level permissions this is a
global thing and this is really an
internal service for us to manage
infrastructure so anybody that has these
permissions is doing it for all users
that's fairly typical of how these
things work next one it was fun because
it actually was a great result we have
no web server our UI is done all in
JavaScript and there's absolutely no web
application server it's hosted by a CDN
it talks directly to the back-end
services securely and that's it the cool
thing about that is it really helps
avoid have any business rules in the UI
obviously some creep in but generally
not and the UI team tries not to but it
also means that that the UI can do
something in end-user can do it through
a script they can code it we can code it
very very powerful and you know also
good for security company we eliminate
the largest vector of attacks by doing
this on our own system this is where we
get into DevOps so 100% of our
environment is automated 100% of the
time there's no shortcuts around that we
are I'd mentioned it before we deploy
fully in AWS and we're using Abe us as
CloudFormation to script everything it
was a little bit of work at front but
once we had kind of the framework for
doing this there it just became easy
we've added tools on top of that for
continuous deployment stimulus
integration and now it is one of the
biggest single positive things I've seen
in my career and I'll get into more
details if you're not familiar with
cloud formation this snippet right here
is an entire service this is all it
takes for someone to create a new micro
service this is again Ticketmaster and
there's a there's a little DSL we put on
top of this just generates a little bit
more JSON behind the scenes to make it
easy for us but generally since all it
takes you can kind of see generally what
goes on here we have an ami we're
actually building a full system image
for every service for now we're changing
that to use containers right now
actually which dependencies it has in
which security groups in it
the cool thing about that if you don't
know much about Amazon is you can
precisely control exactly how this
services talked to and by whom in
addition to these authorization things
and every services like this and if
things are not that simple like I had a
few we have third-party services that we
use you still have the raw information
you can do anything to give me an idea I
scripted a third-party Windows
application on Windows Server to boot up
and auto provision its software license
from that provider in this format it
wasn't fun but it was doable
all right continuous deployment so I've
been doing this thirty two years now and
I can say this is the biggest single
improvement in my career that I've seen
what we really do is that we release
components into production that are
small and loosely coupled just like you
would want to do with software modules
but in production we can do this dozens
of times a day and soon we'll be able to
do it thousands of times a day Amazon
does this it actually makes your life so
much easier and think about what it
actually enables if you've ever worked
on the software project that maybe took
six months or two years and then you
ship you know the chances of integration
bugs is 100% and you finding them out
will take months right that just never
works that well we can we try and solve
that in the industry by making the
releases smaller
six weeks is good six weeks is still a
lot of change what if you can make one
change submit a pull request in github
and have it online in 20 minutes we've
had interns do that within two weeks
enjoying our company now suddenly
imagine a migration let's say you want
to change an API and deprecated an old
one you can do that change a queue up
all the other clients and system to
change at their leisure say over a
couple of weeks and then deprecated the
old software change you can do that
level of change rapidly it's not a big
project anymore yes you have to worry
about things like you know database up
race and all that but it winds up being
such a different problem to solve when
you have that ability to change things I
also have one of my teams in the UK
actually runs that in grave computing
software project that I talked about
earlier they do releases about once
every six weeks and there
high-risk with all the testing in the
world or high risk because it is a big
piece of software and it's not built
using this deployment pipeline this is
actually a screenshot of what it looks
like Muse Jenkins to kind of host the
tool and kind of walk through the
process briefly you know you do a change
in github what you're actually changing
at this point is the version of the
service their service is actually
already been compiled tested and built
and tagged version 1.5 or whatever it is
and then we release it as a change to
the infrastructure that JSON
confirmation I showed you earlier that
goes through it gets its own integration
test if that works it actually goes to a
staging environment which is exactly
like production and it has production
workflows if that passes then we roll it
out to production and for us that means
UK and US based data centers in Amazon
if any of that fails they all get rolled
back to the previous state without issue
and this works consistently let me show
you how we actually upgrade a service
because it's actually fun the box here
in the icon that's a Amazon auto scale
group and if you're you can guess from
the name what they're really designed
for is originally it was like scaling up
fleets of web servers but they're much
more powerful than that so in this
example we have in version 1.0 of a
service it's stable there's two
instances those there's always at least
two instances of anything when we're
doing an upgrade we change the
configuration of the auto scale group
and now we have the new instances
running and the old at the same time and
depending on the policy of the service
it'll either start taking traffic
immediately or it'll wait to test the
data or some other thing we have new
services where we'll actually migrate
data processing over in bulks by
customer ID because they're a bigger
idea supports all of that effectively
what happens is once a health check
passes they come up and they're stable
we need to roll back or we mark this one
as good and we move forward and moving
forward we just ask Amazon to shrink the
auto scale group back to its original
size and depending on how you have that
configured Amazon will just turn off the
old ones or if it failed it turns off
the new ones or rings in there whatever
you want very consistent
yes and if there's a failure we have a
manual mode but basically we do hundreds
of these all the time and it works
another principle that was super
important was the ideas that we
shouldn't run our own infrastructure
this one's an easy to misunderstand so
let me elaborate a little bit on that
very specifically if there's an Amazon
service already around trying to use it
don't go use a third party database
unless you need to part of that is it's
just expensive you know the Amazon's
already solved the problem and even if
it's not a 100% fit for your application
you start looking at the actual costs
you probably decide the Amazon approach
is easier and as an example of that we
almost exclusively use DynamoDB for our
databases now we do have a proprietary
graph database that we use in Amazon and
just to summarize the cost of that
besides the license fees it's you know
five or six experts that know the system
that maintain it it's a lot of code they
got built by someone in the room here to
actually keep the thing alive and do
backups and all that that's engineering
time we could have spent on something
else
now it was the right choice for us at
that time but it makes you think about
it and if you're left with the choice of
I could run this database and it cost me
$1,000 a month from Amazon or I'm going
to spend two million dollars on this
third-party tool but it's really easy to
code you might change your mind about
what you get on kodaline I think
developers in general don't think about
the operational costs of their software
because they're usually divorced from
that this one's real fun minimize
configuration this is what enables the
adage about treating your cloud
instances like cattle not pets if you've
heard that cloud instances are not
special they get special by having
configuration on them and state on them
and you should be able to go and delete
your servers at any time and have them
recreated you do that by minimizing
configuration and we looked at years of
our history before we were in Amazon
most of the failures we had in
production or because someone made a
typo to state one way to fix that is to
use something like chef right you can
then script logger configuration and
that works but you want something even
better you want to minimize that and the
most important thing we did there was
use service discovery
when a services comes up it registers
with service discovery and anyone else
can find it dynamically you just take
that problem away because most of
configuration is what database should I
talk to whether the credentials where's
the URL for this service just make that
dynamic let me show you real quick what
that kind of looks like from a command
line tool we have a normal user Earling
calls that does this but we also have
little scripts that do the same thing so
in this case I asked asked our
production environment for the
Ticketmaster service you get back to
your RLS that's where it is there and
you can kind of tell they're on
different subnets which is fairly
typical the same thing happens
internally and then the library we have
just picks one it's really that simple
but there's a lot of complexity that
goes on behind the scenes I'll cover
that a little bit later another
principle that was turned out to be
awesome we want to log all the data
mutations we do this in Kinesis which if
you're not familiar with is Amazon's
implementation of Kafka it's a
persistent distributed log file and the
idea is that anytime we change anything
in the system we just make a record of
it and if no one looks at it that's fine
but what we use it for primarily is
business intelligence and analytics we
do an ETL on that and put it in a
database and write reports but we can
also do other things with that what's
interesting about that model is that if
you think about what alert logic does is
we're collecting that same sort of data
from our customers so we sort are
treating ourselves the same way
now we also log those data mutations to
a message bus in this case rabbit in
queue and there was a lot of debate
internally if those should be the same
thing or different right now they're
different mostly because we haven't had
a lot of experience with Kinesis at the
time so it made sense to use rabbit do I
name it scalability here in the cloud
this one is the must we did two things
here to really enable that one is if you
go back to the URLs I've pointed out
earlier we can actually not only you
know Auto scale a set of instances if
they get busy we can actually use that
routing mechanism that we have for
customers and pull out customers into
their own infrastructure automatically
so if we have a real noisy customer we
want to give them their own private set
of servers we can do that from the
architecture level
and we will I mean we see that's quite a
bit actually we're you know maybe 10% of
the customers are 90% of the volume if
not more and very pertinent to Jonas's
talk this morning in the keynote we want
shared-nothing services as much as
possible if you start coding services
that way they are a trivial to replace
in the scale horizontally what this
really does is by actually putting this
in the architecture it gets the
discussion at the developers every
single time they write a service they
have to think about it and actually they
don't have a choice because the first
thing that happens with your services
you get to over no matter what you do so
you have to think about it right away if
you mess that up it'll be really really
obvious but it takes the choice and puts
it front and center because you are in
fact dealing with the distributed system
you should recognize that all right
DevOps portion of this so metrics and
monitoring if you've heard about what
Amazon does they review metrics every
day every week pretty randomly about who
gets picked and apparently it's not a
good thing to be picked but you know the
idea is that you're always monitoring
your system we do that as well and the
biggest success for me for this was
actually watching the dev teams on their
own the engineer is having a meeting
once a week and going over metrics and
finding problems that weren't obvious
they're actually looking how this thing
runs in production they're not waiting
for an outage they're actually looking
ahead and seen that looks funny we
should go look at that to give you an
idea this is one of our dashboards and a
tool called data dog which is our
current favorite monitoring tool we have
dozens of TVs in our office with things
like this running this is probably the
least interesting one but it's one for
my team so I thought I should show it
this by the way is a real-time sequel
injection machine learning algorithm
dashboard and the idea is that if that
exception box is anything other than
green that's probably a bad but you can
kind of tell from looking at those
graphs if any of those graphs start to
look at a nominal there's something
going on and you can see real consistent
behavior just by eyeballing these things
even before you would put an alert in
like that and that's just part of what
we do now with that comes an ownership
culture you know we didn't really have
an ownership culture as a company
for that which sounds odd but it made
sense in a kind of a startup mindset
where you don't have enough engineers
and there's a hot project and when
you're done with that you go to the next
hot project we really wanted to change
that and so the main thing is is that
whenever we create services they're
owned by the team that wrote them for a
really long time and they there's some
number of services that make sense for a
team to own say team of six people
they'll care for that they'll continue
to improvement because none of the
software we wrote has never needed
improvements right it's constantly
changing and the things that haven't had
that because we have a lot of them from
the old days we actually do business
decisions based on the fact we couldn't
change the software which is a terrible
thing to do really so we wanted to fix
that and it works out really well
downside of this is a course that means
developers are responsible for
operational production at some level
that's a really big topic of both
concern for the developers and for the
business because we don't want to wait
take too much time away from them but it
winds up being if you get that balance
right I think we've got it pretty close
developers will not leave bugs in
production for long sometimes it's hard
I have one product in particular that's
really hard to do this with but
generally you get the right behavior
then which is very rapid system fixes
let me show you how this actually turned
out for us and then we'll cover lessons
learned um this is what a canonical
service looks like in Amazon and then a
very simplified version but effectively
everything looks like this
the boxes are in amazon availability
zone uh if you're not familiar with
those those are effectively physical
data centers that are separated by a
high-speed link so you know Amazon has a
US East region or region and Ireland for
example we're in both of those they have
multiple data centers in the city so we
always run at least two but they're
effectively treated transparently the
same traffic comes in from our API
endpoints it gets routed through a load
balancer to an internal routing proxy
who then looks at those URLs and goes
okay it's this service that we should
talk to you right now and that service
will talk to service discovery it'll
talk to rabbit and queue and we'll
figure out what else it needs to talk to
you know because usually it's not just
one service that's doing a job and usual
we'll talk to dynamodb and I will talk
to Kinesis sometimes there's a sequel
database involved but the cool thing
about this is that while our system is
not that simple and in fact if you threw
up our system it would look very similar
to want to be honest as diagrams from
this morning out of the spiderweb but
every service could be understand like
understood like this they are all that
simple and it makes it really easy to
train people and want to service should
look like and how it should behave
because they're effectively the same all
right so some of the lessons heard
learned is a service discovery is really
hard if you don't have to do it yourself
don't one of our best engineers and
architects has been still working on
this and he mostly makes the problems go
away for us except when it doesn't and
then it breaks many fixes it but it's a
pretty hard problem in this distributed
state it really is and there's some
toolkits like Amazon or Netflix tarika
if you can use that use that there's
also a ways to do it without any of that
like one classic example in Amazon would
be why not have a load balancer in front
of every single service and just use DNS
names that's also an approach and for
our use case it would actually work
fairly well Amazon itself so just from
like the keynote every service has to
think about high availability and
disaster recovery and that should just
be part of every engineers job they
should understand it they shouldn't have
to reinvent the wheel but they should
understand it and then make sure they
code systems and test systems the right
way this is not something you want to
add on later so you do it from the start
it's actually fairly easy fun story
there so if you happen to make a
continuous integration and deployment
pipeline like we did make sure you test
that too because when Amazon's region
goes down that's got that and you only
did it in one place that means you can't
deployed any of the other reasons to do
your backup plan so game day testing is
important the other thing that a
engineers don't like but it actually I
found I find it fascinating as a
software engineer is the cost management
so you know you probably have been
taught that memory is not free and CPU
is not free and bandwidth certainly
isn't free but we don't really get the
chance to deal with that when you're
doing with Amazon you actually get
charged for everything you do it's not a
lot but it adds up so if you start
thinking about implementation choices
and one implement education choice might
be a thousand dollars a month and
another
might be $100 or you could tweak your
program to drop that to $2 that's
actually fairly fascinating and what
that leads to is actually better
implementations it's really really cool
the other important side effect of that
is that you don't want developers to go
crazy I mean we constantly have to tell
developers to shut down stuff that
they're not using and you know are
giving an idea our production bill is
probably one tenth the entire bill of
our Amazon infrastructure because of
stuff at that part of that cost cutting
is to use containers you know like we
did our initial implementation with
Amazon images a.m. eyes and it was great
got us there very fast containers are
really the way to go now to manage costs
and they're just as trivial to use they
make things faster to build and super
easy to deploy in Amazon now that they
have ECS and they're I wouldn't say
they're transparent we've had a lot of
work to do to make them work that way
but I mean it's going to lower our bill
by 50% just by doing that a good
Microsoft Michel service topic is how
big should services be and the answer is
you know if there's not a good answer
except make them simple not too simple
right that's an art form what we settled
on was services should own wholly the
data they are responsible for and
nothing else it's really like one piece
of data one piece of object something
like that and the service that we has
worked out really well now what's
happened over time and we knew this
going in you know there's a micro
service pattern called a composition of
services or composite services where
especially for you eyes you might want
to have a service that does a bunch of
other back-end operations so that UI
doesn't have to know about ten services
that have to know about one and we're
about to point now with that source to
make sense just kind of go into it
knowing that that's a normal healthy
thing to do if you're making micro
services culture so it was actually my
favorite topic of the whole thing and if
you might want to talk about this I'll
do this for days culture doesn't happen
with that effort and you know a lot of
Engineers types don't like this topic
and they don't like politics let me tell
you they're inescapable and it's
actually not that hard once you just
embrace it I avoided this for so long
even though I've been an architect
relationship roles most of my career I
can only say that if you actually
confront this front end you can actually
make huge changes I mean we changed our
corporate and engineering culture in
about a year it wasn't easy
I mean say I had six peers that disagree
with me on every single topic but in the
end I started changing my expectations
and I changed how I went about
convincing them of things it was more
influence instead of you should do this
right in the end I got about 90% of what
I wanted out of the architecture and the
things that I didn't get are a
relatively minor or we've changed to
later it's really powerful to do that I
had to change first before anyone else
could change that's really the lesson
there you know politics one thing I'll
add with that engineers hate it there's
something to think on there's politics
even with yourself if you actually
embrace that dealing with other people
and getting them to do what you need to
do and vice versa is actually relatively
easy but you can't avoid this in
engineering okay
early it's early in conference what
about early we love that we've been
using it heavily now for over ten years
it has been a great choice for us you
know the thing that wasn't a great
choice has really been the community
support around libraries we need in
particular Amazon support there's an
earl cloud library and i think there's
one other one now that has amazon
support we had to go and add a ton of
work to our cloud to get it to work
there's still more needed you know and
that's kind of intrude for a lot of the
stuff we do - give me an example one of
the other projects we're working on
right now is high-volume machine
learning for all that data and we're not
doing it in Erlang because of the lack
of library support and it's just it's
the right choice in that case but if it
was there we would serve my user life
and what we wind up doing is building
architectures around the parts that have
to be in Erlang because we like it in
parts that just can't be underlying
because of all this stuff I've been
asked by the rest of my team if anybody
wants to contribute to our cloud we're
looking for contributors because there's
a lot more work to be done and you know
kind of run out of time on it
okay for a quick wrap-up in order to
show this is actually Fisher from a
website we are hiring pretty much
non-stop we have like four locations in
the US for engineering and two in the
United Kingdom as well as in Colombia so
feel free to talk to me if you want
more about us and then that's my contact
information and I got time for questions
anybody hasn't okay thank you questions
well one up in there do you run all this
stuff on yourself I mean you use your
own system to monitor yourself yes okay
so you're the ultimate test is yeah and
sometimes that's annoying because well
you walk off and break our stuff by
monitoring and I will throw out - I am
the company's official chaos monkey I
seem to break things more than anyone
else just because I could touch not
supposed to
you said you should we use as much as
possible and take existing solutions if
you see this dependency as a risk now
that you're kind of bound to like one
system or not
you mean like specifically Amazon yeah
that is a risk you know he think about
Amazon today is basically roughly the
equivalent of IBM in the 1970s and 80s
right there's probably a good way to
look at them certainly there is a risk
there what we did for us and this was
actually orders we had from our CTO at
the time was if we thought about using
an Amazon service how long would it take
us to redo it and say is your or Google
cloud if we had to do it from scratch
all hands on deck you know in the
situation is imagine we got bought by
Microsoft and they wanted us to move to
that and if we could do it within six
months reasonably that was kind of the
goal we gave ourselves before we could
use that technology that worked for a
while and then there are things like
Kinesis right which really doesn't have
an equivalent in as your unless you kind
of did your own Kafka deployment so we
kind of satisfy ourselves for that I
think the reality is the cloud platforms
have diverged a lot now in Amazon I mean
if you go to reinvent or watch what they
come out with it's some remarkable how
much they're producing and you can use
so it is a risk you know and there will
be issues with that I think we're pretty
comfortable with it knowing how long it
would really take of something dramatic
happened you know we could reimplemented
certainly from a design perspective
there's nothing in there that
specific to Amazon even some of the
newer stuff we're using would be like
lambda functions there's not an
equivalent to that anywhere else but you
can do your own without too much trouble
so yeah but certainly the business asked
us that question to make sure we just
didn't decide to use you know candy for
fun okay so more questions
thank you that was a nice talk and you
mentioned early out yeah and that
there's still some some work could you
maybe elaborate a bit on what what is
still missing and and how much is
supported I think the one example I just
heard is that we got a request from the
community for something with ECS which
is Amazon's docker service I don't think
there's any support for that there's
probably a lot of that we typically in a
world logic we have a dozen or so
services that we use and we don't care
about the rest you know we're not
actively maintaining it as a
general-purpose API and it wasn't
written by us originally we just sort of
had been adding a lot to it so it's
mostly somebody that needs to go on and
take it and kind of love it make it
equivalent to the Amazon API is because
they change constantly
you know and if the community is waiting
for us to do that I mean it's going to
be hit and miss on how fast we'll get to
it I know ECS is one of those okay so if
you are making changes to it then we'll
be things that are specific to what what
interests you rather the general sense
yeah unless somebody wants to take over
hang up everyone
charlie sounds fun it sounds fun to me
actually but you know in my spare time
so we are currently in a similar
situation with migrate with regards to
migration to containers and we've been
migrating our services to actually
kubernetes cluster that we are hosting
on AWS and from what I've seen with your
diagram with the service discovery and
open source and all that stuff like
services and keeping a track of
deployments and so on so forth it seems
like you're describing kubernetes pretty
much yeah ever consider it's your sole
using it yourself we did a little bit
you know and honestly I don't think we
ever did an exhaustive evaluation of it
probably honestly most things in the
system we tended to have some
preconceived notions about what we
should use and certainly for us there
was a tendency to want to use docker
based solution so when Amazon came out
with ECS it was just there
same thing with Netflix is Eureka I
could never get the guy looking at
services I've discovered you ever look
at it and that's okay like it didn't
actually impact us but yeah that's a
great example I think if you had come in
today the different set of people that
would have been an obvious choice to
make I just don't know if it would have
really mattered you know like how you
know it's a choice of how you do it does
it save you anything maybe maybe not you
know it's really interesting the one
thing I'll add to that that's just
interesting you might be interested in
the only thing we really had to worry
about with ECS compared to running
machine images was the network identity
doesn't exist support management and all
that had to be transformed a little bit
so there was a little bit of coding to
do as well and then that was pretty much
it
okay but just otherwise if you haven't
looked at kubernetes or busts half a
year for a year check it out yeah they
they've moving forward so fast there are
the features constantly and I think like
from what I've heard here I think it
might it might be a really good game for
your company yeah I think I think a lot
of places that if you're doing this sort
of thing from scratch unless you know
your scale was insane which now the
scale of what qualifies as a saying is
really high yeah use something like that
I mean we could build a lot of our
system that way very quickly what's
interesting about us and I think a lot
of places is that those tool kits may
not have been where we needed them to be
when we started and so then they'll get
discarded and then you go and you go
down a path and it's hard to change you
know yeah thank you thank you I'm for
one more here
Thanks thanks for great job I have a
question about culture which you had a
sort of a red thread throughout the
presentation culture is always a little
bit hard to assess very more like the
holistic part of development but
obviously coming from a company that's
that does micro service itself and a lot
of interesting things there's a lot of
things to recognize here and I was
wondering if you could sort of like give
some concrete examples of how culture
has had a profound change or affect both
in both directions actually yeah there's
an incision yeah probably the biggest
thing that changed for us like
externally like if you were outside in a
business looking at engineering is now
engineering talks to one another you
know we're actually trying to make
software for the rest of engineering to
use more than anyone we treat them as
users before that it wasn't the case and
that meant all the little things like
doing good doc or talking to your client
didn't happen and I wouldn't say it's
perfect like this is a hard problem to
get going but what I really wanted to
and I actually talked to mark Alyn about
this quite a bit was we wanted to
started to treat our internal code like
open source what makes a successful open
source project good doc listening to
your customers making rapid changes why
not do that internally so we started
asking the engineers especially those
early engineers who were really the
smart productive ones you know the ones
that were crazy coders the ten Xers you
guys don't need that but how would you
make everyone else want to use your code
and that was usually the answer so we
got people thinking about that and that
had all these little behavioral changes
besides Docs and testing little things
like we had read these in our github
projects for our internal code modules
that were actually useful they tell you
how to check out the project and use it
you know which before didn't exist
before when we wrote doc it was just
Earl doc for the api's that's probably
the best example and I think everything
else is a isn't a fallout from that kind
of behavior of being responsive to your
Kirsten which the internal customers
okay any more questions then we are done
and great thank y'all very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>