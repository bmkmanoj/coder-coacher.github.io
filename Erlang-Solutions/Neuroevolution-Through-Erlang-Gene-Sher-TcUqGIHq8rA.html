<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Neuroevolution Through Erlang - Gene Sher | Coder Coacher - Coaching Coders</title><meta content="Neuroevolution Through Erlang - Gene Sher - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Neuroevolution Through Erlang - Gene Sher</b></h2><h5 class="post__date">2013-04-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/TcUqGIHq8rA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everybody my name is Jean Cher so
basically my work is in
Ellucian through Erlang I am a PhD
student at University of Central Florida
now today we're gonna cover in this
particular topic I'm gonna introduce
biological neural networks and
artificial neural networks then we're
gonna discuss what is necessary for our
programming language to be perfect for
this particular research area or
particular domain in general then I'm
gonna elaborate on exactly why Erling is
such a perfect fit for this and then I'm
gonna give you a case study of the
system I developed called the xnn and
then discuss the future projects within
the area so the objectives of this is of
course elaborate on why erling is a
great fit for this type of research and
discuss the first use of such
development of such a system using your
link and a promoter link within the
science if none of you are if some of
you are not for example using Erlang but
doing research in this area then this is
me promoting early for you to use it
alright so biological neural networks
there are chemical signal integrators
the brain is just a vast network of 100
billions of neurons created together
processing signals standard neuron
accepts inputs in dendritic areas and a
soma cell body processes them and then
creates an output and forwards it to
other neurons now standard neuron
performs a spatiotemporal signal
integration and the signal is encoded
naturally in the frequency in the face
this actually means that the neuron when
it accepts signals they arrive the
voltage charges arrive at the axon
hillock at different times even the
particular time there is enough charge
then the axon hillock generates an
action potential and for was the signal
to other neurons this means that the
general it's actually a rather complex
processing element because not only the
frequency is important but also the the
face of the whole spiking Network so the
whole system is this sequence encoded
and phase encoded
process now also one of the most
important then
part that of the numeron that makes it
so powerful let's integrate it is its
ability to learn and to makes us gives
us the ability to learn in general the
plasticity of the neuron is what allows
it to learn it can produce new acts and
extensions new dendrite branches and of
course increase or decrease the number
of receptors therefore changing the way
process signals and gives you the
ability to memorize and learn new things
artificial neural networks are
mathematical abstractions of this
biological system there there is
simplified of course you can simplify
them at different levels of precision
but the most basic one is as follows is
just a node that accepts input signal
sort of like from other exons and it
weighs them and this is analogous to
receptors the way of a particular signal
specifies how important that signal is
then you advocate everything together
and put them through some form of
activation function usually it is a
sigmoid or hyperbolic tangent activation
function and that output is forwarded to
other neurons here is an input the input
is basically a simple vector right
because each signal we can accumulate
into a list form a vector send it to the
new round the Newton has weights which
are themselves vector and then dot
together put a threshold function and
forward to other neurons now when you
put this together we are starting to
simulate a neural network this is the
simplest one it's composed of three
neurons feed-forward connected the
signal here coming from some receptor
forward to neuron a and B which process
the signal forward to see this
particularly neuron is this particular
neural circuit actually is a soar logic
operator you can see it here with this
particular set of weights it can emulate
the sole operator continuing to extend
this type of neural network system we
have larger neural systems again
feed-forward connected or recurrent now
recurrency in new networks allows them
to have a memory for example if a neuron
out with a signal that feeds back into
itself that means that will second later
time step it
the input something that already output
beforehand which means it can remember
signals of itself so in this way we can
form networks that sort of form
rudimentary levels of memory attaching
sensors and actuators to the new network
gives a renewal based agent which you
can then use for your problems now of
course all of these things you know that
we already can see they're concurrent
and everything else but how do you set
up those particular topologies or
weights or all the parameters that are
available in these functions well there
are two approaches the supervised and
unsupervised one of the most common and
costly uses of course the back radiation
approach supervised learning approach
where you have to have beforehand a set
of a data set where you really know what
the answers are right you have the
inputs and already expected outputs and
through that you can use gradient
descent to send the neuron those inputs
for which already have the output and
then see what the new network outputs
and basin's output calculate the error
between the expected data and its output
and using gradient descent update all
the synaptic weights but again this
doesn't really solve the problem of
topology now unsupervised learning is
our ability to give these neurons their
ability to change as they process data
the simplest one is of course a heavy on
learning rule coding help says neurons
that fire together wire together this is
basically an internal algorithm within a
neuron that gives it the ability to
modify its synaptic weights based on the
signals it processed before okay but how
do we set all these parameters right we
still even when we have a system that
can update its own synaptic weights we
still don't know how to set the
parameters that specify how to update
those weights more in topology in the
first place so how do we solve it well
another also distributed approach to
computation is of course Ellucian
computation and unit is rarely seen
they're basically concurrent processes
right evolution works on
and current engines and it's also again
an approach that we we know works right
and we were the result of billions of
years of evolution and here it's very
simple you know we have a huge
population they're all competing
performing some of the agents were
forced be better at some particular
problem for which they are competing
those agents will be selected and would
greater probability than their
competitors and we allow them the chance
to create offspring basically some
variations of themselves in this manner
we can loop through the algorithm
creating better better agents one of the
particular very simple and coatings but
you know a Bush repetition it's applied
to a particular genotype in biological
chemistry that's the DNA but here for
example we have a string of ones and
zeros that's the genotype and the
phenotype what they're representing is
some kind of a four-block structure for
example in this case let's assume that
the whole goal is to be able to better
blend in into the environment right that
is green so obviously the more green you
have a new the better you are able to
blend in so a simple mutation in a
genetic algorithm can be as follows we
start off with four agents we calculate
on my screen they have in them three of
them have two greens and one does not so
we choose have the population with two
greens and then remove the other two
then we perturb them just randomly
choosing which bits are going to be
perturbed to one or zero to create
offspring one of the offspring is three
greens obviously it's more fit and the
next time we choose that one eventually
we get a very fit agent that's
completely agreements able to blend into
this environment of course biological
organisms and simulated evolution also
use crossover in which case instead of
simply perturbing these genotypes we
take two agents or more and then we
split them at some point combined in
genetic material to produce new ones in
this case again the new agent combined
from these two parts
has three greens and this one has also
has three greens and eventually same way
we can produce ones with all four so
these are just ways you can perform
genetic algorithms on different
genotypes genetic program is basically
further extension instead of using a
string of zeros and ones you're applying
you're also just you can come up with
ways what the mutation operators can
represent and before it was just
perturbing a bit from zero to one in
this case we decided genotype is not
gonna be a string it's gonna be a tree
and with the phenotype being the
function again we decided if it's a tree
then our particular mutation errors are
going to be adding you know to the tree
right and if it's cross over then we can
swap subtrees the all these things that
they've been here for a long time and
they're actually quite kind of different
sides of the same coin there you can
choose any genotype you can choose any
problem and then create an encoding for
the genotype encoding and then come up
with a few ways to just change it the
mutations so for example here we have
genetic programming system so it's a
tree in coded genotype but it's really
just a graph encode write is the same
thing and a graph encoder system is
basically a neural network so we can
move directly from here all the way to
using the same kind of algorithm to
neural networks to perturb a mutate
networks in the same way we can of
course continue extending this you can
evolve and mutate circuits so then we
found that neural networks are highly
distributed system right completely
concurrent all elements work in
biological systems in concurrent matter
and there is a an algorithm that we can
use to mutate and change and produce new
agents you know the best agents that are
better for for solving some kind of
problem and that algorithm itself is a
lucien algorithm itself also concurrent
so what would be if we wanted to create
some kind of programming language that
is perfect for these type of problems
what should it be I mean where II have
Hardware right that's improving just
recently for example the Xeon Phi
coprocessor has been released it's 60
cores
wanted 1 teraflop of performance all x86
so general course and you know we're
scaling outwards it's just gonna
continue in this direction but the
programming languages that we have at
our disposal to do research in this area
are did not have it they have a
conceptual gap when we're trying to
build this very concurrent very
distributed your network based agents so
if you had a chance from the very
beginning to develop a programming
language just for this domain what would
it be right what would you want it to
have well obviously you want the system
to be able to handle enormous number of
neurons right and of course they have to
be perform at a certain time or within a
certain time system should also be able
to be distributed among multiple
machines clusters even because both for
example if you're doing ablution then
you have thousands of agents right large
populations and it's in the in the
noumic itself you have multiple
processes multiple neurons working
together so both of these require
scaling that you can distribute or
remove your machines of course you
should be able to interact with hardware
that's a very important thing right
evolutionary robotics afterwards what
are using the new networks for if to
apply them to something and robotics is
one of the very important fields so
interacting with Hardware controlling
hardware is something that you want your
language to have of course the systems
have to be very large but if you're
thinking ahead if you're for example
they cannot just applying it to you know
currency trading or robotics which if
you're thinking like you know maybe it's
possible do the singularity right then
you want systems that are incredibly
advanced right you want them to be able
to rewrite themselves I mean that's kind
of the Holy Grail right you want the
system to be able to not just think but
then have the ability read some source
code and so on so you want for your
problems to be a support code hot
swapping as well if you go into that
direction and of course you want it to
be highly robust
faltering tolerant and you know so you
can operate for many years you don't
want your
and to just crash from one little bug
interesting enough interesting enough
doctor wasn't talking about a numeral
program language he was actually talking
about the language for telecom switching
systems and that's early early ends up
having pretty much all the requirements
we want it has a perfect mapping perfect
mapping for the processes are new runs
it has concurrency full detection
primitives so your system doesn't crash
you know from a single bug it even
supports Hardware interaction it has its
own fault tolerant database it's
actually Erlang seems to be the perfect
language because that could have been
looking for it has a conceptual
one-to-one mapping here for example we
have a neural network censor censor a
bunch of neurons interconnected and
actuators and represent an earthing it's
direct it's a process it's sending each
other messages interesting to each other
and controlling Hardware through
actuators having this kind of perfect
conceptual mapping is I think is very
important because it allows you to think
about problem directly instead of
switching between you know thinking
about okay how am I gonna build this in
assembly language and how I'm gonna what
is exactly is it doing what is the heart
you know how is it working so this
allows you to tackle much larger
problems without doing this context
switching so I mean if this gives us an
advantage and I'll discuss I mean many
of you probably thinking yeah but you
know you have thousands of neurons
there's gonna be a huge number of
messages it might not be very efficient
I'll I'll actually discuss that
particularly towards the end so the
Axman is a such a new ocean system it's
a platform that evolves highly robust
advanced the neural network based
systems that I've developed a few years
ago
so you know it's a numerical algorithm
based topology and weight evolving
artificial neural network which means
instead of it's a generic but ematic
means you simply separate the local
search and global search phase which has
shown in the research and benchmarking
that has certain advantages
when you're evolving systems so it's a
simple approach of application of
evolution to neural networks you start
with a seed neural network population
which is very simple give me just one
neuron each and then you apply to some
problem that you have doesn't matter
which one you calculate the fitness how
it performs you select the most fit
algorithm within the population you
create offspring from them permutations
and different mutations they applied the
problem again go through the cycle again
again in a metric algorithm though you
don't simply do this you also you apply
to the same problem multiple times while
the same time tuning the synaptic
weights and other local parameters so
that's the difference
so the Primerica mutations can be as
follows you know we have a whether it
uses whether the neuron uses plasticity
or does not so you can permute between
non or hab or or just or some other
algorithm or function then of course
activation functions yeah usually it's a
herb aleck tangent or sigmoid but it can
also be nothing to be linear or sine or
Gaussian so you can permute between when
you're applying these local search a
mutation operators and of course you can
permute the different synaptic weights
by adding and subtracting random weights
so here's for example when you trust the
local search is basically applying
stochastic hillclimber
to random set of neurons in the neural
net based agent in this case we started
with some particular synaptic weight and
the the whole unit of agent is based out
of a single neuron we apply the perturb
it's a synaptic weight check how it
performs how the whole new level
performs with this new synaptic weight
if it doesn't perform as well we retract
it to the previous synaptic weight
apply a new perturbation and try again
and this way we can sort of optimize the
weights for the neural network until the
new network has a pro prosthetic ways to
function properly
now Global search in this numeric
algorithm is the mutation topological
mutation of the neural network again
just like in three encoded systems in
graphical systems we come up with their
own mutation operators that evolve a
mutated
apology in this case we can start with a
single for numeral based agent and one
mutation operator can be added in
neurons so we randomly create a new new
run added here and connected randomly to
and from some random selection of
synaptic weights here and here we have
splice which basically it takes two
neurons that are connected to each other
this connects them and we connect them
through the newly created neuron here
just adding new connections and from and
two sensors and from and two neurons so
we just come up with their own mutation
operators and of course we have to apply
this to the whole populations right so
we have we start off with a a population
of neural networks and we're trying to
evolve again XOR Alijah calculator so we
check how close each one approximates
the logic appear X or this one is the
best we choose this one to create new
offspring through the application of
their assimilation operators to create
better ones until finally apparently to
use the cost it only takes a single
neuron now what you can see already here
probably is it requires very little
explanation we it's pretty much process
process you can pretty much think of it
as early rather that's that's I think
it's you know one of the beauties over
using early because it's just so simple
the mapping you don't don't have to sort
of think about anything it's it's right
there in the xn and the whole new based
agent is a relatively complex type of
setup it has all the distributed neurons
sensors and actuators but they're
synchronized by another element that
oversees them called the cortex and then
the whole system is again monitored by
EXO self in this way we also have a sort
of a hierarchy that you can use to
monitor the different neurons so if
something crashes we can recuperate from
the crash using this type of setup this
is another type of agent you that use a
substrate encoding
now this particular encoding was the
popularized and you know she essential
Florida by Kenneth Stanley here the
neural network is sort of embedded in
this movie dimensional substrate it's a
single process that has neurons located
different coordinates now and use a
neural network the one is distributing
the one that you're evolving you send it
the coordinates of the different neurons
and its output would be the connection
strength and well there is a connection
between all these different new roads so
if you were using this we only need to
use this neural network at the very
beginning of our experiment because as
soon as it sets up the synaptic weights
for all the neurons right then it's
basically a single process you don't
need to reference this anymore until the
next time you're mutating something so
this particular approach for example
allows us to change from using neurons
as the concurrent process to using an
entire agent a single concurrent process
so then our distribution is on the level
of agents rather than neurons in the
population this is what the substrate
looks like it's basically like I said a
multi-dimensional system you can feed it
some information the nodes are embedded
in a particular hyperplane and they're
connected to each other which is defined
how they're connected and their synaptic
weights by the neural network in this
case we have an example of an image
being fed to it this image is encoded as
a pixels located at the coordinates Z
negative one each pixels has it's also Y
and next coordinate nervous black or
gray it's one or zero respectively and
there is nothing then it's negative one
this one of the way it works and of
course this particular approach has been
shown to work very well when your
problem requires geometrical analysis or
where the agents are using the in
themselves already have geometry because
you're actually using the neural network
to process the coordinates rather than
the actual signals and it defines the
actual structure of the whole thing in
some sense if you think about it you can
help two dimensional space with billions
of neurons and it's defined the way it
looks how it's connected is defined by a
neural network which is self a you know
as a function approximator
so in some sense it can
you can evolve very complex yet regular
connections the distribution continues
at this level in my system the level of
population so each agent is working
concurrently whatever with every other
agent once again monitored by a process
called population monitor which performs
this selection and the creation we new
offspring so distribution is a level of
agents as well and this is the
architecture of the whole thing so we
have just a background it's just a
platform you when you start this
particular process it starts the media
database and everything else that means
to the simulation environments that have
to that support this whole evolution
process and then you have populations
working concurrently monitoring their
own particular species with monitoring
their own agents and the agents are
interacting again completely decoupled
with the escapes now escapes it's an
interesting it's a it's the way you
present in my system the the particular
problem to the new mixed agent you call
it escape which is a self-contained
simulation environments often dead
process or can be on a different machine
the whole thing that escape has to be
able to do is accept sensor sensor
request from the new metal agents from
the sensors which are programs and then
accept actuator signals from the new
metal sent by the actuator to put some
kind of you know controlling and using
that command to do something within the
environment
similarly for example if we have a
artificial life experiment then the
sensors send a sensory request and the
scape first of all it creates the avatar
the simulated robot and the sensory
requests are basically what the robot is
seeing with its sensors that's available
to it sending it to Metro can you not
with does some processing and since the
actions through the actuators that the
scape can then look at and say okay so
this is you know this is to control the
differential drive of the agent in the
same way the sensitivity requests from
the database for new data and the actors
can control whether to write to the
database or move to the next position in
the day
bass this is very decoupled approach so
allowing you to separate and design your
escapes and problems separately from the
whole system without having to touch it
or even put the whole escapes and
simulations at different machines if
you're doing ocean robotics for example
and they're using gazebo or rowboat
operating system ross if you used it now
finally you when you're creating an
agent applying it to a new problem you
also have to specify the the morphology
and morphology that's where you specify
what kind of sensors your system will
use what kind of batteries we'll use and
the scape it will communicate with so
you actually you can leave the whole
system on its own all you have to do is
just define sensors and actuators and
the actual problem and everything else
we can stay the same in this case we're
talking about Flatlanders which are the
similar robots here's a neuron and
here's beginning to actually or Erlang
finally it's incredibly simple this this
this whole thing for example in you run
is in standard system it will be you
know just a standard single neuron but
here this this these two lines define
pretty much every possible type of
neuron you might want to have which is
quite incredible for example here we
have a the output of the neuron which is
based on postprocessors apply to
activation function applied to signal
integration apply to P processors and
you have modules with a list of
pre-processors and post processors and
activation functions so if you for
example using want to have a single
standard neuron you can have a hard
buttock tangent apply to a dot product
of the input and the weights right but
just as easily you can mutate it by
using the mutation appears to function
completely different like for example
adaptive resonance adaptive resonance
theory based neuron in which case you
have a threshold applied to an octave
ation function with a difference between
the synaptic weights and the normalized
input here again a single neuron in
under 80 lines of our link code now this
is not just the just
basically when first of erling allows us
to so easily read this it's actually
readable you you know exactly what it
does but more importantly this this
neuron supports plasticity and Static
based computation
it supports Lamarckian and they're
winning based evolution and can accept
requests from the extra soft to update
us to write itself back into the
database in each database I mean all
under 80 lines because it's so simple
and because it's gonna be any neuron as
we discussed in the previous slide
because we can emulate any neuron it's
you can tackle problems that are
significantly more complex and develop
systems they're completely more complex
and yet still be able to understand them
and read them that's a significant
advantage I believe in this particular
research area for my system I use
Manisha as a storage for the genotypes
again it's very friendly for this type
of research because it's both the
genotypes are very easily can be tuple
encoded and you can support that not to
mention when you perform mutations and
something goes wrong ninja it can take
care of it
something goes wrong it breaks there's
an error you don't produce a false here
neural network based agent it simply
retracts the whole mutation I predict
you apply a new one so it takes care of
a number of problems on its own here's
the for example the genotype it's
composed the other of the population
which keeps track of all the species
which keep track all the agents and the
agent is a container that keeps track of
all the core-tex sensor actuator and
neuron elements which all belong to
their own tables and here's of come fool
genotype again I think it allows us to
produce systems and gentiles that are
very readable so here for example we
have a complete genotype almost with
cortex sensor neurons and actuator and
you can actually read it which means you
can work on it and once again try to
create create a better system more
easily the system I have recently
updated that started off using stranger
mutation appears but right now I have
updated to use the Hall of Fame and
archiving which have significant
improved its performance it's used now
multi objective optimization it also
incorporates and loss for the
position emoji search which would become
recently very popular and it has also
starts using most my most recent
research in the is in the use of neural
micro circuits and the utilization of
complex numbers in neurons and also now
supports adaptive resonance theory now
we come we're now coming back to the
question ok so you have a whole bunch of
messages and these neurons are not
really very complex right so at some
point there is just it's just not
efficient that's true if the neuron is
very basic but in more complex problems
like artificial life Ellucian robotics
each neuron is not just does not simply
produce an activation function apply to
a dot product right you have huge
amounts of algorithms support plasticity
not just haben you can use a self neuro
modulation which increases the amount of
computation performed by each neuron by
two to ten times depending on how
complex the algorithm is so that's skews
the messages the amount of competition
we do in our favor furthermore I'm
recently started using nodes instead of
a standard neurons you can now also use
a node composed out of a two layer
feed-forward neural micro circuit now
feed-forward networks are universal
functional proximity eyes which means
they can be anything but a single neuron
right it can't even perform nonlinear
mapping these are much more powerful
elements which are still manageable
biomimetic algorithm so the simplest one
would be just two neurons and this
further increases the complexity of the
node at which you're distributing your
system by by three times roughly so
further put it in our advantage and I've
I'll show you some benchmarks in just a
second here's the code for a neuro micro
circuit based node again it's very
simple just how how you can increase and
change the complexity of the systems
which is so little code furthermore you
can this is a new area of research I'm
undertaking is the using of our suction
coded systems but you embed
smaller substrates inside a larger one
and so the distribution at this is at
the levels of these substrates which can
themselves be composed out of a couple
of hundred neurons but because their
synaptic weights and their topology and
everything else is set up by the
external unit that you're actually
bowling it's completely manageable
because your actual operating and this
one the search space is still the small
neural network where the phenotype is
much larger and distribution occurs were
each processing with where each process
is about a hundred neurons so again this
improves your scaling and again this is
now also support art map based agents
another approach to improving your
amount of a computation to the number of
messages is through what's called
automatically defined functions it's a
an old approach from genetic programming
if you're applying a bunch of mutation
operators and you're mutating if you
noticed and you can't still keep a track
a counter each noodle has a counter that
keeps track of when was the last time
that something that it was either
mutated or something that it's connected
from has been mutated if these counters
don't change for a long amount of time
because you know the only ones surviving
the only agents that are surviving are
the ones who have not who have this
particular section not mutated then you
simply transform this small sub circuit
into its own a process so once again we
can leverage this and of course the
grand goal is creating very large
systems where you have different types
of modules interconnected so the
benchmarks are for the system it's
actually excellent so here's a standard
double pole balancing benchmark
classification new revolutionary system
not particularly good at classification
the best approaches we know is sort of
support vector machines but the more
Guard based on the art system are very
competitive now and even they require a
less amount of computation but the
double pole balancing benchmark is a
very it's commonly used benchmark for
also generation systems pretty much
every single others
has done it the Dixon and the old one
was already superseding others in it
including a bunch of other more other
ones the more recent one the one is
using archiving and Hall of Fame has
further improved the performance we can
see the the neuro micro circuit approach
is not quite as fast at the original one
but it has advantages when you're
applying it to deceptive problems and
nevertheless it is when the problem
becomes more complex it does have the
advantage where as was more simple than
smaller approaches are better another
benchmark is artificial life that the
system has been applied for example food
gathering where the agents are
interfaced with the simulated robots
within some kind of flat land and they
have to run around and try to eat
particles using sensors such as color
and range and there are traders are
basically controlling the differential
drive on the simulated robots so again
the system very easily was applied to it
and excellent performance there
dangerous food gathering benchmarks is
where there's food particles but also
you sort of put some poison around it so
they have to use the color based sensor
to avoid the poison particles and only
try to eat the food particles so there
is a pretty much no some change here but
performance excellently
and another benchmark is of course
protein versus spray when you have now
two populations competing against each
other where the prey population survives
by eating food and the predator
population survive by hunting the prey
actually very interesting results you
can find them on YouTube I uploaded them
is the behaviors that the both were
interesting very complex the Predators
learned how to sort of push around all
the food particles into a single zone
and then they would hide behind the food
particle because raycasting is used for
the range sensor the color sensors so
that the prey would eventually they
wouldn't see the the Predators hiding
behind the food they were only see the
food particles eventually they get very
hungry obviously because are they losing
energy they would go for the food
particle and as soon as they do that
they consume the food particle on the
to confuse them so this ambushing it's
very interesting evolve behavior I don't
think it is popular you know it's
because it's a flatland it's I doubt
anything more TP evolved out of that and
of course forex trading I've also played
it too that results obviously results
were any good I won't be sharing them
good but you know you can you can apply
them to a forex trading you can either
use directly the sliding window of our
closing prices or the actual image now
what I one of the methods that I'm was
exploring last year is the actually
using the actual image the pattern the
geometrical pattern now technical
analysts they they usually look at the
patterns I'd even have these names you
know the the head and shoulder pattern
the cop and handle pattern the whole
bunch of these things that they trade
based on how the geometry looks so
instead of using just numbers I fed the
substrate and coded system the actual
image of the the currency and the
geometry of the graph and actually
performed better than than the one
because you know even if there is
actually no meaning in any of the
technical analysis the fact that so many
people believe that it works they behave
appropriately to i'm actually make the
market behave in that way so we can use
them sort of because they believe in
these images to leverage it here's some
organizational results not very good but
making some profits but nothing I would
hide obviously
and finally epitope prediction it's kind
of similar to sequence analysis but
you're applying your neural network to
predicting where on approaching you have
epitopes and this is a platform for this
type of bioinformatics research so in
conclusion I think that Erling is can
substantially leverage this particular
research field it's it makes things so
simple it allows us to actually utilize
all this available hardware and without
actually having to worry about the
distribution itself it takes care of so
much
the mapping like I said there is no
conceptual gap which I believe is a it's
very important because it allows you to
create more complex systems even a
single student starting from scratch can
create an entire platform which can
impressive future applications is in
cyber warfare evolution of circuits in
cyber warfare you basically have
artificial life but where the
environment is not 2d or 3d it's it's a
network and the agents are programs and
the sensors are coming from the port the
signals coming from the pores are the
actual sensory information and the
actuators send commands to use for
example parameterize Metasploit commands
to attack other elements on the network
so you can use network simulators like
NS to to this kind of work this is
something we're exploring illusion of
circuits is another approach instead of
neurons you have logic gates and other
larger components so this would be
something like you know the way cyber
warfare system would look like the end
this is escape which is just a network
and you have the same approach for
everything else evolutionary robotics is
another feel that I'm exploring using
the Ross and gazebo and here we evolving
unmanned combat aerial vehicles which in
this case the neurons are controlling
simulated in 3d environment quadcopters
if you're using raw system they actually
provide you with models so you can
already have access to most of us and
they constantly competing so you're
trying to create arms race and the
agents are competing trying for so
they'd evolving to be able to actually
fly and navigate and to to try to
outmaneuver each other and kill each
other
so it's the same as predator and prey
but three-dimensional space and it has
application because after you're done
you can actually apply it to actual
robotic systems so that's pretty much
concludes it thank you for your time and
if you have any questions all I'll take
them
yes sure sure yes desk we're exploring
we're not having good results yet but
it's being exposed right now yeah so
there's a function for that but nobody
results yet yes now the the in the book
you basically build the entire system
the second generation of my original
system so you're doing from scratch and
you're applying it to financial analysis
and artificial life yes yes only you do
not link your system you do not link the
system to the gazebo or Ross in the book
so you don't do that but you do
artificial life and currency trading and
build the entire system from scratch so
you'll know how exactly how it works and
you have a code online yeah yeah there's
it's full of references
right so you know there is another new
model base spite in neural networks they
take into account frequency encoding but
there is actually another approach that
also takes into account phase and
frequency that does not require spiking
neural networks that actually work has
been pioneered by Nam Eisenberg and 71
in spy in Soviet Union he used complex
numbers now complex number is represents
both phase and frequency and his neurons
well but he does not do new revolution
this is what I'm exploring now his
neurons are able to are significantly
more powerful because of that so that's
one of the ways in handle you just use
complex a quaternion system instead of
the standard one and you get all that
without actually having to do frequency
encoding were actually but using spiking
you on that was directly and his system
also works much better classification
which standard neural networks I'm not
very good at new Lucian approaches
it's well in gazebo your player stage
project right
the thing is controlling in a simulation
can be directly transferred to actual
robots in my approach I'm still working
with the simulation zone but you can but
it's the same drivers we should which is
why they made the whole project so you
can go directly from simulation to extra
robotic systems
yes so you're talking about analyzing
the actual neural network for how
exactly it works
that's an old problem neural networks
are pretty much to great extent black
boxes when you get the results you don't
exactly know how it works so no at least
not very well yes
show you
well of course you can you can analyze
the final initial topologies in fact the
new system keeps track of the whole
village in native trees so you can keep
track of how and what mutations were
added you have this entire database of
the whole fledged native tree which you
can then later delete or data mine so
you can perform all of that yes and for
generalization if you're trying to
evolve robust systems with an artificial
life you the starting position are all
different for different ages at
different times so you can sort of try
to push forward for a highly generalized
that project is still very early stages
please elaborate
right that definitely have to be done</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>