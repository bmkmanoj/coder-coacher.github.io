<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Genomu: A Concurrency-Oriented Database - Yurii Rashkovskii | Coder Coacher - Coaching Coders</title><meta content="Genomu: A Concurrency-Oriented Database - Yurii Rashkovskii - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Genomu: A Concurrency-Oriented Database - Yurii Rashkovskii</b></h2><h5 class="post__date">2013-05-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ujaoprIKY9M" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I think that that all that research
that had today basically led to this
project but the project wasn't started
just to write a database after who wants
to write a database in his own st. mind
that's probably not a very good idea of
how you should spend next year couple
next year's because it's it's an
interesting experience but don't do it
my story is very simple we just had a
customer's project and that we were
working on and there were certain
requirements they are basically a
non-stop pay-to-play a success system so
basically provide certain games that
customers pay for two other basically
distributors and whenever end customers
cannot pay basically cannot register
their their their entries to the game
the clients of the company there that
we're working with they got really
annoyed because they can't make money
that's terrible so they needed to accept
those payments at any cost any time if
the cluster is split in five pieces they
still need to do that and they need to
understand what was happening so they
had interesting conditions like race
conditions for example they needed to be
able to pay out but only once because
you don't want to collect your winnings
twice well you do want they don't want
you to do that and refunds as well you
don't want to refund twice you don't
want to refund after the you know the
game is over obviously you don't want
well again you do they don't and they
obviously since it's all related to two
money they wanted to have some sort of
audit trail what was happening why and
if we lost money and expect why and how
do we fix that in the future so those
were problems that we're trying to
attack and we were obviously trying to
use existing databases we actually try
to use react but we just fell that it's
just not the right way for this
particular application just doesn't feel
right although the you know the base is
solid and it's great and it's seemingly
what we need but every time we run into
those specific requirements by the
customer we ended up writing some sort
of workarounds and
like that so we decided well how about
we take what's really important in react
which is real core which is the
foundation of the platform and play with
it so let me just give you a brief idea
of what kind of database is this is that
you have an understanding of what we're
talking about Sonia tradition
traditional kv database and by
traditional I mean Ryoga and and similar
databases and most of key value
databases anyway let's assume you have a
key that has a value of a certain list
let's say one two three four and let's
have two clients and be one of them will
try to append the list one of them will
prevent the list with certain value in
this case 5 and 0 so what's the result
well depending on the database you're
using either the latest right will win
or you'll have a conflict and you'll
have to figure out pretty much on your
own what is the right behavior so if the
actual behavior that you want that you
have the whole list well you have to
resolve the conflict which means your
application has to deal with that it has
to understand by looking at the
conflicts it has to understand what the
hell happened how do i reconcile that so
what does ginamit oh well it has a
well-defined behavior in genomic that
list will be 0 1 2 3 4 5 and that's it
no conflicts and everybody wins so how
do we do that we do a variation of
events Orson I idea how many of you have
heard about the idea of event sourcing
two three four about four okay so well
the the way I understand of insertion is
very simple you basically describe
events and basically have a data
representation for an event and you send
it to a certain end points and it uses
that to aggregate the final results of
the computation so for example if you
think about a counter as an actor for
example it receives a message increment
and that's an event
and it increments the value in the state
and that's the aggregation of that
single event if you have multiple
increments you have another aggregation
so in a nutshell that's a very very
simple representation of event sourcing
so every command engine amo is basically
getting appended to the list of of
operations so in this case we have the
starting list of 1 2 3 4 and then for
example depending on the order of events
of clients a and B we have either one
two three four append five and prepare
zero or we have a vice versa prepend 0
and append 5 and the end result is again
the list of 0 1 2 3 4 5 but again why so
let's dig deeper into motivation so one
of the important things that we had in
that project specifically was that we
were doing a lot of concurrent updates
to different objects so for example we
were doing a lot of counters so how many
how many bets are in the system and for
our for example the list of things in
certain containers and obviously all
those operations were happen happen
concurrently and when you're trying to
update for example counter if your model
is to read the value then increment the
counter and send it back in then again
you have a conflict or latest one wins
it it don't have a correct correct
counter in our case you actually save
one round trip because you don't need to
read anything you just send an event
outs because you don't really care
what's the value of the counter at that
moment anyway you say increment the
counter and the system takes care of
ordering those events that get sent out
out of your application so in the very
end it has the correct contra value and
the same thing with accumulators you can
just keep sending events like a pen to
the list and saying okay this is a new
element this is new Ellen
this is new element and you never really
need to know what is the content of that
list it might be a huge list but as long
as you know transmitted in it every time
you actually don't care what was the
length of that list and you might never
actually transmit the whole list anyway
and I'll show why is that the case later
on is that clear so far like any
questions anything that is uncertain
nothing okay good another one we haven't
done much of that yet but we're getting
to this area these days so the
motivation is that well lots of web
applications that are being written
these days they're using the term
real-time web application and what they
normally mean is that they continuously
receive some streams of information and
they update the visual representation
and the interaction with the user
dependent on that data problem is with
many databases yes in many cases you can
watch the changes to certain keys or
certain structures the problem is how do
you know what actually happened so for
example you got that the update that the
key a have changed from value want to
value to what actually happened do we
know in many cases yes we can sort of
figure that out by knowing exactly how
are our application works and hope that
it actually is being written well and we
are not misinterpreting some facts in
our case you can actually have a stream
or a feed of operations that happen to
the key here watching so you know what
is the exact operation that happened to
that key so you don't have that
ambiguity you know well that's what
happened that's what what it means all
right so we can build those applications
without having an extra logical layer to
interpret stuff we just know what's
happening and you can you know interact
with the database in the same way so
you're keeping everything in one
communication channel which is
interesting and the third one is to
enable highly available clusters
in presence of net splits and in terms
of data flow recovery and what I mean by
that so let's assume that the cluster
that we have for that for those games
split up but we still need to to work
with with those gamers and for example
they keep buying those bets and we keep
writing them down to the to the database
and we keep updating counters in those
split parts of the cluster but by the
time we actually need to compute who
wins you know and who wins how much and
how many bets we have in the system all
the kind of stuff we can wait until we
have the recovery event in in the
cluster but what we can do now since we
have the whole log of the updates for
every key in the system and for every
transaction effectively we can recover
this stream of events that happened and
get the right aggregate values as well
and get the the whole data set and then
when when the you know the clustering is
healthy we can actually do the draw and
figure out what's the right jackpot
value and all these kind of parameters
so that was probably the most important
part of the initial project with that
customer so in a nutshell what is this
database it's a key value database it's
an event source in engine it's based on
log merging well in case of failures
it's it has regice like operations and
i'll show you what i mean by that and
it's built on a dynamo architecture how
many of you know what's a dynamo
architecture fair amount i would say
roughly half of you so yeah and we're
using real cord technology from bath
show to enable this in a nutshell what
you can get from deinem architecture is
basically a highly available cluster
that has multiple replicas of the data
that you're writing and it's rebalancing
the data across the cluster when you
grow the cluster or shrink the cluster
obviously their own
more properties to it but just as a very
very high level overview for example if
you look at this we're writing data it's
in presentation mode can show this but
anyway you can see the similar colors to
getting distributed across the ring so
next one how can you actually use the
database like what's the way of
interaction with the database so your
entry point is a message pack interface
how many of you know what message pack
is but how so message back is actually
an efficient a binary serialization
format think of JSON but a binary that
is actually cheap to encode cheap to
decode compared to JSON caswell json is
a fairly complicated format to parse and
encode so it's really efficient really
tiny and what we have on top of that we
have a very simple multiplexing protocol
so you can have multiple conversations
with our server in the same TCP channel
now we're getting to actual operations
so that's the cornerstone of of the de
nom de nom with technology so what that
means is that unlike in react and some
other databases instead of manipulating
is effectively binary data when you read
a piece of binary and you know what to
do with it and then your modified and
write it back to say react we're doing
something that is actually closer to
well traditional SQL databases in SQL
they know what the data type of your
values are and what are the operations
that you can do on them for example you
can compare them you can increment
things you know they actually know what
to do with that data we're sort of the
same way we have a server-side library
of those small operations that operate
on message back data types in a way like
in red is those who are familiar with
red is probably know they have
operations on on strings on binaries on
lists on on on hashes and tons of others
it's fairly reached library that you
have it read is so that it basically
taken operation and apply to its
remember
value that is normally stored by certain
key and we're the same way so we have
numeric types so you can do increments
decrements and that kind of things we
have collections such as list sticks
with actually wait support even
operations like mapreduce which is photo
and others like filter all any
substandard functional programming
language kind of operations on
collections we also have support for
binaries and we expect to have bit
strings whenever the message back guys
will finalize their next enter that
actually distinguishes well actually
allows to define application specific
types so we'll be able to define a bit
string type so you can start building
things like planes for a bitmap kind of
indexing for your own application we
have a very interesting thing called
assertions and assertion is a very
interesting mechanism to fight race
conditions and similar situations so
consider your except in certain records
into your system and you're also
incrementing a concert so that you know
how many elements of that kind you have
in the system problem is you actually
will receive some duplicates once in a
while for whatever reason the
communication channels are not robust
enough from some remote terminals so
you're getting resubmissions how do you
deal with that so we have this special
well almost special assertion operation
which means proceed only if this
assertion passes and if it does not
aboard the transaction so you assert
that data element that you're trying to
well going well that key that you're
going to write two is equal to a nil
which means well nobody have written to
it yet and if the assertion passes your
transaction goes forward and then you
write the actual object and and then
your increment the counter and the
reason why this works is we have
transactions that I will be talking
about a little bit later and you'll see
what kind of
interesting guarantees do they have that
enable assertions and i'll come back to
the topic and we have many more features
in the pipeline utf-8 strings so you can
actually ask for the length of the of
the utf-8 string and you know capitalize
uppercase down K so do normal things
that you can do with strings except that
you can do that remotely without sending
data back and forth sets possibly crdt
module as well so for those who need
very strict guarantees about ordering
events so many things we're still trying
to nail down the you know the best set
of the standard library what should it
be and the other thing that I was
mentioning before for real-time web
applications we have this Watchers API
and it's very simple you're basically
subscribing to a key or a list of keys
and it delivers you a messages that are
effectively tuples of two elements one
is well what was the key affected in
this transaction and then the whole
transaction object and what that means
you can actually know all of the keys
that have changed in that particular
transaction so you can know the scope of
the change which is oftentimes very
important to know so now that you know
about operations there are this moment
at least four commands and they're very
simple get set a plan commit let's just
go through them the first one is get so
basically what do you do you pick an
operation that you want to apply to the
value stored in the key get the results
but actually did not change anything in
the storage so basically the database
remains intact just to give an example
consider that in that in that certain
key you're storing a binary maybe even a
huge binary and only want to know is its
length you don't want to get the binary
especially from all the replicas because
it might be like 10 megabytes each and
that's going to take time but you just
want to know the length of that binary
so you pick the binary size operation
and you you do get key are binary size
and all you get is the size of the
binary
the key but it does not change anything
the database it remains the same way it
was the opposite of that is set it does
apply the same of the same kind of
operation to a value it returns you the
results in that way it's exactly like
get but it also saves that result into
the database simple as that apply is
just the variation of set but instead of
sending you the the results back to you
it just sends an okay use when you want
to set the value but you know that the
result value is either of no interest to
you or say too large to transmit you
just say apply but i don't i'm not
interested in the results just save it
and let it be there and the most
important after all these operations is
commit the reason why we have committed
because we actually have transactions in
engine amo so we well you're doing
commands like said get or apply you're
actually working with in an isolation
layer so until you do it can meet or
discard this whole transaction nobody
will see the changes that you're making
they're completely isolated from you
even though the are stored on the server
and eventually we might actually do
something like persistent transactions
they're like long-running transactions
but we don't know anything about that
yet until you don't commit they're
completely unknown to anybody else once
you can meet all those values get
written to the database coron by quorum
and then everybody else can access them
and the transactions that we have
they're not really a seat transactions
but they do provide enough of guarantee
is that to be useful so one is isolation
and the other one is causal consistency
so they're not consistent as in our DBMS
kind of sense but I'll show you an
example in a few minutes that shows how
you can build consistent views of data
without basic sacrifice and scalability
and distribution of your data
across the cluster so I'll start with
simple examples can you actually see
there I hope so so basically just says
set counter number increment commits
this all you do to work with a simple
contrary the Nexus example is actually
about causal dependencies or causal
consistency so yeah I really want
everybody to be able to see it is there
anybody here that can't see that okay
okay well I'll be vocalizing this then
ah so let's start with this simple
scenario so we have two actors in this
system one is Ellis and the other is Bob
so what Ellis tries to work with here is
a photo album of the kind that has a
privacy setting so when she supposedly
creates the album she sets the privacy
to a value of public so she defines that
her album is public and then she commits
that transaction then she says okay so
I'm a pendant a nice picture myself to
the list of photos in that album and
also commun said and then she realizes
well I actually want to upload some
pictures but I don't really feel like I
want to share them with everybody
they're not so good and she says privacy
of her of her photo album to private and
Bob being an independent agent in the
system all he wants to do is to look at
the pictures that Alice uploaded so he
goes to the database and asks for photos
so he says get photos and he gets photos
which is at this point is still just a
nice picture and notice here that we're
getting a certain version back so we
know that's a version let's go vs one so
after this and so this means the that
that Alice has updated the privacy
setting and Bob got the pictures that
were uploaded before and what happens
next after this Alice says ok so now I'm
pendant
pratley like I'm at in the private
picture to the set of pictures and I'm
committing this and now Bob well he's a
little bit slow maybe he wants you know
to grab a beer and now he's actually
ready to to see the list and he asks
this isn't okay what's the privacy at
the version one and it says well it's
public you can show it and he shows well
the list of pictures that does not
include a private picture but now he
hits refresh he wants to see more and he
goes to the same steps so he requests
the list of photos and notice that we
actually get a nice picture and a
private picture and then he asked okay
what is the privacy at version 2 that we
just got from that object and it says
it's private so it knows that but at
this time it cannot display the list of
those pictures because it's private
within this consistent severe and
basically what the question here is what
enables this kind of dependencies so
what we are using here is we were using
another generalization of the vector
clogs mechanism how many of you are
aware of the clogs about a half again so
the clog the simplest way probably to
explain is it's it's it's a way to do
partial ordering of events in the
distributed system and to detect
violations of causality so you know
which event happened after which
basically and ITC is a similar system
but it generalizes it to the point when
you can have well when you don't need to
have global unique identifiers for
actors in this system so you can grow
and shrink as you need so with this
technology we can actually have global
versioning of every object so we can
compare the causality of any object and
this is not just one which is different
so the and this is the reason why asking
for a privacy setting for a specific
version of
of a photo album actually works because
it's it's the global space for for
versioning so whatever available which
we just open sourced it a couple days
ago about a week or so it's available in
github and as an alpha beta version
already works it has a fairly nice UI
that you can use to monitor it and to in
the in the next tab it's called
instances you can actually assembler
cluster right from the web UI so it's
it's not tedious it's actually fairly
simple and we do things like DNS SD so
for those who don't know what it is it's
a dns service discovery so if you launch
a couple of instances in your local
network the system will see them and you
can just click add and add them to the
cluster without action knowing where
they are just you can can find them in
the list because they will they will
identify themselves using multicast so
another part of of the talk is what is
implemented in and how many of you came
from the Alex your talk that was just so
again roughly a half so as you know of
blazing for those who weren't at the
elixir talk Alex here is a made-up
programming aware language in top of
early so basically takes Erlang and
applies some interesting ideas from Lisp
and related languages to it and tries to
create a more productive and extensible
environment for early developers such as
myself and I just want to show a couple
of things that really helped implement
and implement in genomic and explain at
least try to explain it why so one of
the things here is since we were
operating with message pack data types
we want to be able to work with the
message back efficiently the reason for
this when we operate with things
especially things like collections in
our operations for example here is the
example how we extract the head of the
list so instead of constructing a list
from message back representation stored
the database and then actually get in
the element out of it and what we do
since we store message back we basically
work with banners directly we do not
allocate our long terms at all well for
ya just for a few exceptions that are
actually not related to collections but
specifically for collections like
binaries lists and dictionaries we do
not allocating anything in Erlang we
operate directly on binaries which means
we need to match on message pack data
structures in binaries but here are two
examples so if you look at the bottom
one you'll see that we would have to
match on a specific part of the binary
saying it's in binary like 101 and then
next four bits are the length of it and
then the rest is basically the rest of
the list representation in in message
back but since we have macros in Alex
hear the message back library that we
wrote it actually contains macros for
every data type in in message back so
instead of that line that well I can
never remember those numbers obviously
so I have to look them up and I don't
know what what they mean really we have
Mackers that generate exactly the same
code but we know how to write them
because only do I write the head
function definition and I right message
back fix array and ask for the you know
for the remainder of the binary so that
I can work with that list specifically
but I know that it's a fixed size array
which means it's up to 16 elements so i
can imagine specific types of message
pectin and don't i don't need to care
what the prefixes are how long they are
how long the length is so I just been
taken care of by the library that
provides couple of macros and and thanks
to that the operations on those special
complex data types like lists and
binaries are very efficient engine amo
so no new memory allocation really and
so we don't need to spend time of that
either which makes it fairly efficient
other things that are worth mentioning
code generation so for example all those
operations that you have so like the
commands plus operations they get in
coded into an efficient binary
representation as well so for example an
operation called core identity which is
an operation that would just return the
value stored in the in the key it's
encoded as 00 so it's two bytes the
first one is for module the second one
is basically for the number of the
identifier of the of the operation in
that module and the way we do dispatch
in we actually compile time because
we're building the whole library in Alex
you're as well compile time we produce
enough function clauses in a central
dispatch in module that just matches on
the beginning of the binary so it looks
it knows all the combinations of module
operation modulo operation she's matches
to them and dispatches right away where
it needs to go so it's very fast again
so and there's at this one we don't have
many operations maybe 50 30 I don't know
something like that so it's not that big
of a deal but when the library will grow
it will show tremendous improvement
because we don't need to do any ets
tables or anything it's just compiled in
its baked into the central dispatch in
module and obviously it doesn't have any
real bottlenecks because it can be
executed by multiple cores at the same
time without any problem the end another
one are when we write modules and you
can look this up on github others a link
at the end of the presentation we write
documentation for those operations for
example in the module and during the
compilation time every time we compile
the operations module we extract the
data about every operation how many
arguments does it that doesn't declare
what is the documentation what is the
public name for it and we actually
generate both detail documentation for
it so you can actually have a nice
documentation and we also produce JSON
files for client libraries to know what
kind of operations are available in this
specific version of genomic so they can
just basically use that to generate code
themselves or just dispatch from the
client libraries but this is all part of
the compilation process it's not an
extra step in the make file or
anything like that it's just baked in
again into the compilation process and
if you're interested at the end of the
talk I can show parts of the code there
are actually very small and they deliver
all of that and another thing worth
mentioning is protocols so how many of
you have worked with closure or maybe a
small talk a lot so basically protocol
is similar to interface in Java I would
say not exactly the same thing but when
you define how that thing should like to
what kind of messages this thing should
respond and then you can dispatch to
those particular implementations
depending on the of the type that you're
using and in in our case it has been
very useful for basically two things one
thing is storages so we can implement a
storage as a record and then define an
implementation of a prodigal so we have
implementations for say ETS bit cask and
other storages that operate through the
same protocol hey the other one since we
do different kind of coordination well
we have commands like said get and apply
and we have a commute operation that
operates on multiple quorums instead of
a single quorum at the time so that
means it's a different kind of
coordination but the actual coordinator
is is only one module that we have there
but for specific logic for either
operation coordination or transaction
coordination we have implementations
defined for the important parts like get
the quorums how we handle the responses
from from the back hand they're
implemented through protocols and if
we'll ever need anything else that acts
as a coordinator with respect to time
outs and everything like that and
coordinates actual quorums but the logic
is different we just write another
implementation of this protocol and
we're done we don't need to change much
in the system which is really really
useful that's pretty much it how are we
with time
left or 40 minutes left okay so
questions I can show some stuff if
anybody is interested sure I basically
oh sure the question is so when you send
a commute operation is that when the
when the log get up gets appended or is
is that happening before that is that
the question so yes the this is the time
when the actual log gets appended before
this it's basically what we call a
staging area so it just rides data by a
version and then the committee is
basically linking that stage data into
into the log and updating are
aggressively cached data into the log is
itself so at the top of the log you
always have the computed value so you
that you don't have to recompute you
know the whole transaction log every
time so it's always cashed at the top so
you know you know you know the values
but that's what happening at the commute
time yeah so yeah the question is I'm
just trying to put it shortly so the
question is does the committee operation
try to negotiate with all members of oh
yeah yes so with all the members of the
of the quorums included in the in that
particular transactions a transaction
since you're working with multiple keys
right so the answer is basically what it
does it operates with multiple corns so
it figures out chorim's sufficient
number of quorums for for the food for
the whole transaction to complete and it
sends out a repackaged operation for
operations sets for
every quorum so we know these three
operations belong to that forum and
these two operations belong to this
quorum so we sent out to quorums and we
wait until all quorums are finalized and
that's when you know the committee is
considered successful well it is growing
but actually the way itcs grow it's more
like of this kind of stick so that
actually the thing is every time we
created transaction with fork the tree
of the of the clocks but every time the
transaction ends it joins back so in our
experiments we we try to model the
growth of this clock for a couple of
years worth of operations like with
constant transactions and I think it
grew it about 29 bytes per per version
which is still acceptable to us yeah so
about i think it was 29 bytes in a
binary representation so we don't handle
anything like we don't have any specific
handling for pruning that you know part
of the clock yet we might consider that
in the future just hasn't been an issue
so far but that concern is always on the
back of my head so I'm trying to think
maybe we'll have something like epochs
in the system so we can basically you
know say at this point everything is
fine like the cluster is healthy we can
dump that part and start dumping it at
this point we don't have that but it's
part of the plan how much any other
questions
mm-hmm so the question is how do we deal
with operations order for those
operations where order actually matters
so when you want to append one thing and
then another thing and you don't want to
append them in the other order so at
this moment we don't have a good idea
for this but the plan is very simple
basically for every transaction to be
considered in a specific order you need
to basically join their clogs together
and and and them again so basically
what we'll have is a synchronization
primitive for transactions so you know
that these transactions at this part of
the system they can be they they need to
be order it so basically every time
before they do that they need to do this
short sinkhole and get the new clock
which will make every every transaction
in that part order it correctly so
you'll have the logical clock adjusted
so that's one way to deal with this and
the other way we're thinking about Aiden
s crdt module to do that conflict
resolution data type I think that's
that's it what the Quran M is for to do
certain operations like for growing sets
and for counters that also might be
interesting for that but yeah the basic
idea is basically sink the clogs and
that will ensure the ordering when you
need it so yeah it's clock
synchronization we don't have this as an
API yet but we're planning to that
pretty soon is yeah we didn't really
need it so far but it was part of the
original design that just didn't make in
the code yet
okay so the question is of what rimmel
what continues to work and what does not
continue to work during the cluster
split correct so to answer this question
so basically what is not going to work
what is not going to work when you're
trying to write well to operate with
with certain keys when specifying when
you only need primary nodes or primary
notes and in the real core terminology
so when all of the well when those
minnows are not available to you
basically you're like you're turning the
knob right and basically you're saying
well I only want this kind of nodes not
a fallback kind of notes to respond to
me and then you'll have failures but
when you have a net split in terms of
genomic itself basically what it will do
it will keep collecting data into those
logs and when you have an event
basically that is opposite to net split
so like net join right that's the time
when it will start reconciling the the
keys that have participated in so
basically repair mode so it will find
out the you know the most common common
denominator in the transaction so when
when they're all equal and from that
point it will basically start ordering
them by HEC clocks and time stamps
depending on where those times stems
come from and figure out the right
ordering based on those two elements and
rebuild the values so at the top of the
log yard gain have a new are cached
value that is representing of all
operations applied to that key does that
answer your question okay cool any other
questions
think we're out of questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>