<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>WebSockets, RabbitMQ &amp; Erlang @ the Huffington Post - Adam Denenberg | Coder Coacher - Coaching Coders</title><meta content="WebSockets, RabbitMQ &amp; Erlang @ the Huffington Post - Adam Denenberg - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>WebSockets, RabbitMQ &amp; Erlang @ the Huffington Post - Adam Denenberg</b></h2><h5 class="post__date">2013-04-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/lOyG6ZMq738" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">uh all right so thank everyone I know
this is tough being the last speak on
the Friday everyone's tired and can
probably smell beer so I'll do my best
to kind of get everyone to drinking as
quickly as possible I guess I'll just
start real quick everyone always says
you're supposed to start with a joke I
don't have a joke so I will just sort of
reenact my first experience with Joe
Armstrong yesterday so really quickly I
was in the lobby and I see Joe Armstrong
and he's by himself and he's got this
big red suitcase and I say this is great
this is my opportunity to meet the
infamous Joe Armstrong and I go
introduce myself so I go up and my first
thought is to walk up and go hello Joe
but I can that idea so I go up to Joe
and I introduced myself and I say hello
Joe says Adam Dunn root from The
Huffington Post just wanted to introduce
myself and he looks at me with his red
suitcase kind of like half shakes my
hand and he goes the airline lost my
 luggage is that ok not going how
I planned so we get into the elevator
and I'm like all right let me kind of
bring this back a little bit me buddy
maybe you settle down so we get into the
elevator and I say Joe just want to
thank you for all your work and erlang
you've been an inspiration and your
decades of hard work and his response to
me was and what the was I gonna do
for socks if I don't have any socks I'd
have to go shopping do you like going
shopping and he walked out of the
elevator so that was my first and
probably last experience with Joe
Armstrong so that's my replacement for a
joke
so I'm Adam denaburg I'm the VP of
engineering at Huffington Post I
apologize I'm losing my voice I'm and
speak it I'm gonna talk a little bit
about our journey with WebSockets sauk
Jas and RabbitMQ so a little background
huffington post is a now Pulitzer Prize
winning social news online newspaper we
do about 2,000 articles a day doing
about 500 million pageviews a week about
twelve million unique visitors a week
and we're at I think this is actually
law updated prob about 250 million
comments now across all of our articles
and we're averaging about two million
comments per week across our articles
and you know a very strong engaged
community around our content so HuffPost
live so when I was originally hired it
was to build a new platform called
HuffPost live so HuffPost live is a
12-hour live streaming network and the
whole concept was to essentially tap
into this community you know these
people leaving 200 million comments
across all of our stories the idea was
to you know take a 12 hour live
conversation and bring it to the masses
and you know the mantra we've always
been using is that people don't want to
be talked at they want to be talked with
so we really wanted to bring that
community into this conversation so you
know part of what we were doing was
having real-time commenting which was a
big part of it and a lot of what we're
talking about today across all of our
segments so we do about 8 hours from our
New York studio four hours from LA
segments are about 20 30 minutes each
and we do about 30 per day you know and
there was a lot of real-time activity
happening on the platform we didn't
really have the opportunity to do ajax
refreshing we had to sort of react and
be very real time for both commenting
and sort of as segments we're
transitioning throughout the day we
didn't really have the you know luxury
of just sorting like doing it an ajax
poll because at that point it's too late
you know for producer wants to
transition a segment it happens all
users need to be in sync at that moment
so browser refresh was not really an
option and we couldn't just say you need
to be on an html5 browser to use our
application you know it's such a large
user base our browsers are all over the
place so we couldn't just say sorry I
ten and above or ie9 Chrome and Firefox
26 whatever version we're at right now
so we needed flexibility in terms of
reaching the users and being able to do
real-time so just to kind of show you
real quick what things look like so this
is HuffPost live right now so across the
top are our segments and I'm just gonna
pause because the bandwidth is probably
no good so across the top are all of our
real-time segments and these are being
controlled by producers so you know we
have traditional production studios in
New York and LA and there's a production
team that's controlling what all the end
users see so as soon as all the
rage is over there's a producer in a
studio that's gonna trigger the diable
mia segment to come up next and all
users need to react at the same time now
we also do some tricks because obviously
there's buffering with video so we do
some tricks on the JavaScript side that
we actually queue the event so we see
based on the time stamp of the event and
we wait until you've actually reached
that time stamp on the video stream so
we don't prematurely transition you but
the events thing to happen in real time
and we need to be able to control that
all the users seeing that event at the
same time the comments which should be
flowing down the write again it's not
doing polling using WebSockets these
coming in real time people can post text
or video comments and below the video
here is kind of what we call our
resource well so again this is something
that could be pushed in real time from
the production studio so the concept
here is that if someone says something
completely ridiculous and one of the
producers says I'm gonna go prove them
wrong with this Wikipedia article they
can go onto our CMS pull up a Wikipedia
article publish it to the site and say
you know and they will talk to the host
and the host will say hey we just posted
the Wikipedia article that actually
proves you wrong so click below so a lot
of real-time integration to the platform
commenting resource well below the fold
and the video transitions across the top
so I'm just gonna close this to save my
browser all right a quick overview of
our tech stack so we have CMS and a
bunch of JSON API which is now powered
by a Ruby and rails for both those
applications we're a very heavy
client-side application so a lot of our
logic is actually done in backbone j/s
which is a sort of light weight you know
JavaScript framework so basically it
happens is backbone controls a lot of
the view logic all on the JavaScript and
the client-side the Ruby API exposes a
bunch of JSON API which really happens
essentially onload a large majority of
what happens after page load is
happening via WebSockets and then we
have our CMS application that our
production team uses to kind of control
all the metadata that's a ruby app as
well
Erlang obviously is doing a lot with
WebSockets and sock j/s and also the
AMQP bridge i'm not going to talk too
much about this but something else we
had a build
was almost the equivalent of shovel so
we have a Erlang app that is a consumer
and a producer and the only reason we
don't really use shovel is we had to do
some kind of wacky JSON transformation
between the two rabbitmq endpoints but
actually end up working pretty well so I
won't talk too much about that but there
is another place where we use or lying
in our infrastructure
MongoDB on the backend document-oriented
database sued us very well for CMS like
data memcache traditional key v key
values caching varnish for some api edge
caching and elastic search not sure if
anyone's familiar with elastic search
it's sort of a distributed version of
leucine and Solar has a nice very
restful interface for doing search and
it's really elegant so if you're using
leucine or solar definitely take a look
at elastic it's it's really nice alright
so what kind of real-time messages do we
have on the platform so we have comments
that are being adjusted from our central
commenting platform I'll talk about that
in a second but you know we have a whole
comment infrastructure that was kind of
outside HuffPost live that deals with
the massive comment volume that we have
again video transitions being initiated
by our production team from our internal
CMS so one of the really innovative
things we did was bring a web platform
into a traditional new studio so we have
producers associate producers executive
producers literally controlling what end
users and users experience from a web
portal which is really has been very
unique to what we've been doing so there
are producers in a traditional studio
with our CMS open on a laptop kind of
controlling what everybody's seeing
and again resources below the video
player you know is what I mentioned
before you know there's a whole team of
people and if someone says something or
new information comes in a tweet a
comment we can push it right to the app
in real-time we don't have to worry
about people refreshing the other thing
was you know we wanted to go to support
various inputs to publish a real-time
message we didn't have one quote-unquote
producer a message can come from an
early app directly from RabbitMQ an HTTP
consumer a ruby application there are
various inputs to being a producer so we
needed something that was gonna be very
generic allowed any any mechanism to
publish a message
as long as it was compliant to what I'm
gonna be talking about
so again needed something very generic
all right so not to get too contentious
into all this stuff cuz we can probably
go on forever but some things that we
looked at the obvious thing that I think
people default to no js' socket IO sock
Jas which is why we're here eeehm web
socket committee and you know real-time
and web sockets is exploding this list
can be 50 pages long to be completely
honest so just kind of a quick overview
of some of our results so nodejs socket
IO we didn't want a flash fallback for
web sockets and I won't get too much
into it but you know I believe sucka do
actually up creates the web sockets and
uses a flash fallback which requires
port 843 and is like a default
three-second timeout for flash it's
pretty messy node itself didn't really
have a great story at least not when we
looked at it you know Evan did a really
great job with the last side showing
some of the madness with deferred
callback some JavaScript that stuff is
crazy
the concurrency story was not great it's
JavaScript it's single threaded you know
at that time the way to get concurrency
and nodejs was one to run one instance
of node per core to me was asinine I
think they're fixing some of these
issues but again it wasn't a great story
yet there I believe for horizontal
scaling we need a persistent back-end
and I believe the only persistent
back-end that's currently supported is
Redis and then the final nail on the
coffin for us there was that the whole
focus of socket IO was actually
transitioning to engine IO so I think
they realized that socket IO was kind of
too big they wanted to you know kind of
move towards a more generic transport
mechanisms so things were evolving to
engine IO I don't know we kind of felt
like they had gotta get their stuff
sorted out and we just didn't really
have a lot of faith there
eeehm WebSocket obviously we're using a
lot of Ruby so we took a look at some
Ruby solutions we quite honestly didn't
give it a lot of investment we just
didn't think that Ruby out the gate was
gonna be the right choice for a
concurrency and scale I love Ruby we use
Ruby for real-time and concur
see probably not the best choice we
spent a little time looking at comedy at
the time there's really only a long
polling option WebSocket seemed a little
bit buggy and not fully supported I
think it was disabled by default again
we weren't getting a lot of warm and
fuzzy feelings as we kind of dove into
comedy so then we came across sock j/s
not sure if anyone knows about sock j/s
so the things that we liked number one
no flash fallback absolutely no flash
required the other things we like auto
fallback to other protocols we didn't
have to worry about what browsers
supported right we didn't have to get
into this game of if I'm on Chrome and I
support WebSockets do X if I only
support accept to our polling do this if
there's a bug and this browser disabled
JSON P didn't have to get into that so
that enabled our code to be very generic
we didn't really care about how the
transport was happening
we had our backbone models tied into
callback events from sock J s and all
the plumbing you know we've been talking
about plumbing for two days now was
handled by sock J s it dealt with
figuring out what the browser supported
we didn't have to get worried about it
and that was a real nice thing for us
another nice and he was just native
WebSocket client support so one of
things about sock J s is it actually
implements the WebSocket protocol as its
transport so even if you're not using a
sock J's client you can actually pour a
point a standard I figured which RFC it
is 64 or 55 I think client add a sock J
a server and it will work which is nice
and I'll talk about kind of how we use
that in a minute from mobile load
balancers support no shared state this
was another nicety if you need to scale
suck J s you add more notes there's some
built-in supports for load balancing and
horizontal scalability we didn't have to
worry about kind of getting it's a
back-end shared State and kind of you
know how to scale another back-end you
can kind of scale just like you do a
load balancer a web server put up on a
load balancer add more nodes and you're
kind of good to go and it was written in
our language well it was a plus you guys
have questions just jump in you don't
the way to the end so what was our
decision surprise sock J s like I said
before basically there's a sock j s
client JavaScript API we wired that into
backbone applications you know has some
very basic sort of you know on message
on clothes type events in the JavaScript
library we wired that into our backbone
models nice seamless integration no
clunky browser detection anywhere
happening all we really had is a really
nice transport javascript library really
seamlessly integrated into our backbone
models so everything was really pure
simple consistent the other thing that
was really good is that we were tested
and then now we're currently using a
production native WebSocket client so
our iOS right now with an iPad app out
we did some POCs with flash and Android
and that worked as well but native
website native WebSocket clients work
out of the box no problems we're using
them today in our iPad app in production
and like I mentioned before you know not
a lot of scaling concerns we treat it
like a web server but put it behind a
load balancer we need to support more
connections add more boxes this was a
really nice thing for us
what is sock J s sock j s and I'm just
gonna read it to you it's a browser
JavaScript library that provides a
WebSocket like object gives you a
coherent cross-browser javascript api
which creates a low latency full duplex
cross-domain communicate communication
channel between the browser and the web
server under the hood sock j s tries to
use native web sockets first if that
fails it can use a variety of browser
specific transports xhr polling json p
all this other stuff so basically sock j
s is a transport so it implements the
WebSocket framing and protocol but it
uses its own transport right this is
very nice this just makes things very
consistent very predictable and we
didn't really have to get too bogged
down into how things are getting
communicated right sock J s implements
its own native protocol the other thing
that is nice on the backend if you want
to write your own sock J a server so the
sock J s servers in Erlang which is what
we use obviously there is one for node
for Python there's a whole set of
protocol tests so since sock j s is a
very well defined protocol if you want
to write your own sock j s server the
protocol is well-defined and there's a
whole suite of i think they're written
in python tests to validate that your
server is implementing the protocol
according to the spec so it's really
well-defined and really very nice
load-balancing sake is so basically
there's a predefined URL structure here
so from the sock JS protocol the session
between the client and the server is
always initialized by the client the
client chooses the server ID which is
just a three-digit random number so
basically when you put your nodes behind
your load balancer you could just use
URL hashing the load balancer persist
the URL to the same node we don't have
to worry about you know what happens if
my next request goes on to node B when I
was on node a but doesn't happen right
so the URL hashing just ensures
consistency that the same user is
talking to the same node and then the
load balancer by doing URL hashing could
just sort of horizontally scale as we
add more nodes really simple our
comments workflow so I'll talk a little
bit about this so comments at the
huffington post are actually all
moderated we have a whole infrastructure
around machine learning technology for
moderating comments so we can detect if
comments are abusive you know using
certain language that's inappropriate
so the workflow at a high level is a
comment comes in we submit a comment to
our comment infrastructure the machine
learning technology analyzes it and
basically makes three decisions all to
approve auto reject or I'm not sure the
Omniture column basically goes to a team
of moderators that can do something
ridiculous like 20 comments a minute per
person and they literally are manually
approving comments in this gray area so
yeah auto rejected Auto approved so you
know we're constantly feeding back into
the model it's kind of hard to say a
success rate you know we have a we have
some some dials we can turn in terms of
how sensitive the system's called Julia
we have some dials that we can turn up
and down on how sensitive we want to
make it so over the last few years we've
gotten the dial to a point where you
know we're comfortable that what's
getting auto approved is safe
what's rejected is indeed not something
we want and the manual queue is
definitely in this gray area so that's
you know a constant refeeding of you
know the machine learning technology
oh that's good questions so what percent
is getting auto approved actually don't
know it's good question I want to say I
don't know I'm gonna try and guess I
won't try and guess so real-time
comments was one of our primary use
cases for WebSockets and that common
infrastructure this is where that AMQP
bridge came into place so comments get
posted to this whole machine learning
workflow when they publish the approved
messages either from the manual or the
automated machine learning technology it
just got posted to a RabbitMQ topic we
had an AMQP consumer just sitting there
listening saying let me know when you
got comments and then we would just kind
of shoveled them over into the real-time
infrastructure so that's where the AMQP
bridge was being used
CMS workflow this is another input for
us so again we had producers in control
rooms managing the real-time web portal
so they were literally deciding you know
when videos were in transition and the
thing that's tricky about this is you
know videos buffer right so what people
see on the web is about 30 to 40 seconds
on average behind what's happening in a
studio so there's some really tricky
timing issues so a producer sees what's
happening in real time because people
are there and they say ok we're on the
next segment they push a button so then
we have to say okay I got an event but
I'm actually buffering 30 seconds if I
transition you now I'm actually gonna
prematurely transition you and you
haven't finished watching the previous
segment so we had to do some analysis by
looking at the timecode on the video
stream the timestamp of the WebSocket
event and put it in like an arbitrary
buffer on the JavaScript side to ensure
that things were in sync and that's been
working really well but that was
definitely a challenge but you know
again one of the things that we really
did was put the power of literally
controlling with end users C in the
hands of our production team you know it
wasn't scripts that were on or anything
like that it was producers traditional
news producers with a CMS pushing
buttons that were publishing you know
real-time messages into the
infrastructure so on the Ruby side we
were using AMQP event machine which
basically just throws an event machine
loop on the Ruby side and there's a
really nice AMQP client that we wired in
to the
at work flow so whenever someone would
hit a button Ajax call on the UI on the
backend we would just do sort of a rabid
MQ publish through the event machine
library on the Ruby side so just a
couple of things you know we're figuring
this all out well number one no one knew
a lick of Erlang which I don't know
maybe wasn't the best choice but
something we wanted to embrace and we'd
have a lot of time so everything got
built in you know between the CMS the
client-side UI the aim could be bridge
the WebSocket infrastructure everything
was about three to four months from
first line of code basically in
production the native support from
WebSockets and the load balancers we're
using citrus necks Geller's was beta
that wasn't very comforting before that
was in there it the net scalers didn't
know what to do and they basically Auto
default it every one down to polling so
luckily Citrix had support for
WebSockets but it was beta fortunately
we didn't have any issues there and
message latency you know we really
needed to ensure we don't have a very
high throughput but we wanted very low
latency so when a producer in a control
room says change the segment and if
there's 50,000 people watching we needed
very high guarantees at all 50,000 we're
gonna get the message in less than a
second right so that was our really big
focus was not so much on throughput but
latency and we didn't know if it would
work you know you know a lot of time and
you're kind of taking some Gamble's so
let's talk about what it is so we called
it outbreak and outbreak is basically a
set of infrastructure middleware
components that allow a generic
mechanism to pub and subscribe and the
idea was basically build this with the
mindset of being reused more broadly as
time went on we didn't want built too
specific what I mean by that is when we
do pub/sub we didn't want to have to
modify our code and say okay we have
this big case statement it says okay
case when channel is chat and then have
to like push a new version of the app
because we added a new channel type or
new things that the app how to be aware
of we wanted something very generic
right plumbing if you publish a message
and you subscribe to that message we
will deliver it we don't care really
what's in it you just subscribe to the
common format that we define and we will
push the message to everyone listening
for that so we didn't want anything
anything it was really too specific that
just said okay if you're a comment we
know what to do if you're not a comment
well we got to push a new version of the
app so that was very important to us
because there was a lot of different
types of messages we were pushing again
very simple and generic consumers wait
for messages for the channels they're
subscribed to produces send a message to
a predefined RabbitMQ topic so I'll talk
about this in second but we use the
concept of RabbitMQ routing to map the
subscription model an outbreak basically
bridges the till right so outbreaks it's
in the middle uses the RabbitMQ topic
structure to determine what the message
is where it should go and then who would
you go to so this is kind of a workflow
of what happens so a producer a producer
can be anything a producer is
essentially in this case anything that
can generate an AMQP message into a
rabbitmq topic so when I publish a
message I use a very specific routing
key structure if you're not familiar
with rabbit and Q rabbitmq has this
concept of routing keys that determines
how you listen and how you publish and
you can sort of partition off consumers
that way so we use topics and routing
keys very heavily in this model so the
routing key structure is basically a
prefix which in this case is just
outbreak dot channel ID very generic so
our lang server for all intensive
purposes consists of two main gen
servers our pub sub server that was
basically a RabbitMQ consumer and the
WebSocket server so this was the Sauk
Jas component listening for end users
for subscriptions so what happens here a
user comes in and we're gonna use a chat
room with an ID of 102 in this example a
user comes in from the JavaScript side
and sends a JSON payload of subscription
chat room 102 the WebSocket server
receives it it parses that out as the
subscription stores that subscription in
an ETS table on the production side we
produce a message say whatever the
payload of the message is we use the
routing key of out break channel that
chat room 102 which matches the
subscription the pub sub server consumes
the message
parsa's the routing key chops off the
prefix the second element after the dot
is the channel the third element after
the dot is the ID so that allows us to
convert the routing key to a channel an
ID which matches the subscription so now
we have a radically subscription of
chatroom 102
so we basically query the ets table when
this message comes in and says give me
all the connection objects that have
this tuple we go into here into a
broadcast and then every user that came
in with that same subscription now gets
broadcast that message so imagine this
Erlang server box
just imagine essentially n of those
those are all the nodes that sit behind
the load balancer so now a topic by
design broadcasts the same message to
every node so that's how we sort of
scale horizontally so every Erlang
server here is all each one's going to
get a copy of the same message the
reason why that's okay is because the
user is only bound to one Erlang server
so there's no risk of duplication so
that's how we get sort of our horizontal
scalability and that's how we do some of
the mapping I'm gonna go to a little bit
more detail than example but that's kind
of the topic the routing key from the
topic mapping to the subscription so
subscribing basically we built and then
I think about sock J s is it's just a
transport so you can do whatever you
want
over that transport you build your own
application logic on how messages get
sent back and forth so we built a very
simple JSON structure on top of that and
a very simple format action action name
channel channel name ID ID name very
simple so subscribers give the channel
and the idea they're interested in
we basically support three actions sorry
sub unsub and query very simple sub is
subscribe unsub is unsubscribed query is
just give me all the list of things I'm
subscribed to and that's something we
would more use on the backend of our
JavaScript side that's the that is it
for subscription one little small JSON
payload
yeah that's specific to application
logic so HuffPost live all those
segments come back with an ID we use
that same ID for the subscription and
then our CMS knows the same ID so when
we publish they admit they batch but it
could be anything basically as I
mentioned before when a subscription or
a non subscription is received we
basically just store that with the sock
J s connection object in ETS table the
sock J s object is a little bit special
which I'll talk about in a second but
really straightforward we take the JSON
payload we parse it which is not the
most fun thing to do in our Lang but we
parse it and we start with a connection
object we also it's an ordered ETS table
so we anchor on channel and ID an
ordered ETS select is faster when the
thing is anchored to the front something
I learned the connection object from
sock J s is special because it allows us
to simply extract it from the ETS table
and just call a send method on it it's
one of those special tuples where it's I
think sock J s session is the first
element and the rest of the parameters
to the the send method so it's really
easy for us to do an ETS select query
all the connection objects that match
and just do a connection send on all
those objects and then the broadcast
happens the connection object only lives
on one node again we don't have to worry
about sending a message to a user on
another node each node is responsible
for the users that are connected to it
there is no communication between sock
jeaious nodes they know absolutely
nothing about each other and will show
in the publishing slides how we use this
to basically just loop through all
matching connections so publishing
currently we leverage RabbitMQ as our
publishing queue we rely very heavily on
the concept of routing keys in topics as
I mentioned and we don't require any
sock j s node to be aware of any other
node and again this was a really
important architectural decision for us
topics are leveraged that all nodes
receive a copy as I mentioned as we add
more nodes the the Erlang a rabbit's
consumer fires up rabbit just knows as
another list
for that topic and message get broadcast
of that new node no config necessary we
just literally turn it on when a message
is published is published to a single
topic using the format that I mentioned
prefix is just a namespace for the
messages and then channel nid and then
all the consumers basically listen a
prefix top pound so in RabbitMQ when you
have a hierarchy of routing key here a
pound means any level deep of a routing
key an asterisk I believe is just one
level so basically what this means is
I'm going to listen for every routing
key that starts with prefix dot anything
afterwards and that's how the consumers
basically know to listen for anything on
that and not have to worry about again
what that structure is I don't have to
program the consumers to say ok you're
listening for outbreak that comments
outbreak dot live transitions it's very
generic so any channel and I'd
accommodation will flow through without
any of the applications really having to
care or know why it's flowing through
that workflow there and again the
routing key is critical this is what
Maps both sides of the equation so just
to talk through how this works do a
quick pub/sub example so two users want
to listen to a chatroom user a user B
and each are bound to a different sock J
s node so they both send the JSON
payload to the server action sub channel
chatroom ID 103 totally arbitrary
you know 103 would come from probably
some JSON API that determines what
channel you're in and then your your
JavaScript client would you know send
the subscription event each sock J s
node and serves one record to the ETS
table on each node with a sock J a
session object and the subscription so
now we have this tuple with the sock JS
connection on two different nodes one
for user a one for user B either know
anything about each other
so we have a publish so a moderator in
the backend stay wants to broadcast the
message to all the users in chat room
102 102 so a moderator in the backend
say there's some web UI decides to
publish a message the chatroom 102 102
103 maybe I changed number so I'm not
sure it should be consistent he
publishes a message to a rabid mq topic
using the routing key out break that
chat room that 103 the consumer on both
the sock JS nodes receives a message
on the topic with a routing key outbreak
chat room 103
the server parses that converts at the
channel chatroom ID 103 that forces an
ETS select for the tuple chat room 103
the connection object comes back from
that tubule we literally throw them to a
loop and we just call connection send on
the payload that consent from that
message so some challenges so the model
suits us but we are bound by the
performance of a single rabbit server
not even close to an issue for us but
architectural II we are bound to the
throughput of a single rabbit server you
know it's not distributed pub/sub that
way so just something to be aware of you
know I think you can probably easily do
twenty thirty thousand messages a second
on a rabbit and Q node so unless you're
doing like market data probably not an
issue
monitoring rabbitmq was a little bit of
a pain but we actually got into a really
really great place we've done extensive
testing with just randomly shutting down
rabbit know it's turning them back on
and our reconnection logic has been
flawless we've had network interrupts in
the data center
you know just random upgrades of rabbit
servers the app doesn't even hitch
native mobile clients that need to use
the native WebSockets so saachi's has
some heartbeats built into it when you
use the native WebSocket client you do
have to implement your own heartbeats
that was one thing that we came to
figure out and you know making a release
I don't know maybe this was just me and
my team but we battled rebar and rel
tool for a lot yeah I mean it's fine now
it's great it just I don't know we had a
lot of problems just getting things to a
really happy place but now everything
actually works great we have everything
wired into our CI server we do a build
things get zipped up are synced over to
the servers everything's nice and happy
which part
Oh rel tool oh it's just um you know I
just didn't remember like the
integration with rebar and rel tool I
just wasn't very well documented it took
a lot of looking at sample apps to
figure out like directory structure and
when you make a release the parent
directory has to be named the same name
as your release just these little quirks
that we certainly didn't find anywhere
on the internet but we learned it and
now we create very happy tar balls we
could do that I have to go back now and
remember all the pain points but yeah we
could do that sorry what's that yeah we
have rebar and rel tool and I forget the
whole workflow you know there's a rebar
release workflow but then I believe we
got to make a bunch of changes to the
rel to config I don't know maybe rel
core I haven't heard of rel core
Oh real cool sorry no I'm not known as
real cool again it was just really a
matter of just small configuration
tweaks about where dependencies lived
and some directory structure layout it
really wasn't anything crazy
it was just I needed another bullet yeah
I mean you know in hindsight it's easy
as heck now so we create new apps it's
two seconds like I said it was just kind
of getting there was a little bit
painful but I guess that's subjective a
little bit of performance so we got sock
J asked about a hundred thousand
connections pretty easily on a single
node which was more than we needed with
about sub-second latency a lot of tuning
plus pieces good ol one thing I'll say
about sock J s and something my team's
trying to work on we do have a fork of
sock J s right now that we're in
progress on that's integrating cowboy
point eight
but really this last two bullets are
really where Sauk Jas Erlang really
needs the most help from the community
basically Sauk just relies a lot on JSON
encoding to standardize how the browser
in the backends communicate and it's
just a little bit of I'll just say
design flaw I don't mean that with any
disrespect because Merrick is a genius
but basically when you do a broadcast it
actually JSON encodes to every
subscriber of a message and really what
it should be doing is JSON encoding once
and then sending that JSON encoding
mesh's message out to all the consumers
right now it's JSON encoding every
message out so it's really just a matter
of just reworking that workflow so it's
something we've been looking at
contributing back but you guys were
interested in getting involved in an
open source project feel for anything a
look at that I think that would be a
massive performance game and there's you
know I spoke to America a little bit
there are some optimizations as well
there's a lot of message passing that
goes on when all these message messages
go through the the real-time
infrastructure and I think a lot of this
can be minimized to improve some of the
performance as well but the JSON
encoding is certainly one of the biggest
ones some kernel Tooting this is pretty
boring this is kind of some stock you
know AOL Huff posts kernel Tooting stuff
but you do have to tune your kernel to
get concurrency up you know Mac's ports
again something we found this by default
I think it's 4096 and we were
bottlenecking it like 4096 connections
which seems really odd for an Erlang
server we have like an 8 core box with
16 gigs on RAM and I'm like this can't
be possible right you know Erling is
passing and winning all these he 10k
tests I'm like 4000 connections so we
did a bunch of research and you know we
played a lot with these two parameters
and that kind of stuff - about a hundred
thousand connections before you know CPU
and latency started to dive off but
again if we fix some of the JSON
encoding issues I believe that the
number will go even higher staying
within the sub-second latency number
time frame so what's next we want to
open source outbreak right now it's not
open source we are working on open
source in the project we also talked
about kind of making thing at the
outbreak more configurable by not
relying explicitly on RabbitMQ but
hovering this concept of basically
afters you know kind of how react has
like these pluggable back-end concepts
so if you don't want to use rabbit use
whatever queueing system you want and
basically making those more generic you
know we are tied in to the whole routing
key concept so we'd have to figure out
how that mapped and that worked but we
would like some flexibility in terms of
making out break a bit more generic in
that regard exposing an HTTP interface
you know sometimes clients don't really
want to go you know even on the Ruby
side you know getting a vent machine
running if you're not using an event
machine based Ruby server is not always
the most fun sometimes it's just simple
if you want to just do an HTTP publish
so something else we've been looking at
just exposing an HTTP interface you know
fixing some of the Sox GS performance
issues and last but not least we're
hiring so if you want to live in New
York and be an engineer email me that's
it thank you guys right on time any
other questions
I agree
so the sake is choy specifically was you
know we really liked the sort of
agnostic nature in which the browser's
wouldn't have to worry about browsers so
that part was really nice about Sachi I
yes Erlang was just something as a team
we discussed you know we really wanted
something that can have a concurrency
and you know it's just where we ended up
you know there was a risk but you know
no risk no reward
yeah so there were some early developers
in AOL Alex knows so we did have some
reference points so we weren't totally
in the dark but it was definitely new
from a programming standpoint so yeah I
mean look there was risk but it's just
after evaluating everything it just felt
like the right choice even with the risk
of not knowing the language so so it
worked out but you know you never know
but yeah if you guys are not familiar
with stock Jas
mark is a really great guy actually the
opportunity to meet him he he's in New
York for the hacker school program for a
few months you know really smart guy
he's done a great job and you know I
think what everything happening with
WebSockets I think the stuff he's doing
is actually really innovative in terms
of you know giving you a platform to
really build consistent web applications
because browsers are a nightmare so any
other questions
um any other pain points you know not
really you know outside of just having
to learn you know standard OTP
principles and you know just doing
things the right way things went
surprisingly smooth you know the
community was extremely helpful
you know we'd post once in a while so we
got through with really without a lot of
pain I mean really making the release
was probably one of the worst pain
points and some of the tuning stuff you
know that took a little bit of research
in terms of Colonel tuning and VM tuning
but outside of that you know everything
worked about as good as we couldn't have
hoped yeah I mean you know which is why
it was great to see Evan talk about
Chicago boss I think personally one of
the weaker points of Erlang is you know
building web api's so I think that's
probably one of the only places where we
evaluated other decisions but I think
anything that involves you know
real-time messaging you know everyone's
been extremely happy with Erlang I mean
our servers and our apps have been just
humming along rabid servers they don't
crash but we take them down network
outages all kinds of stuff and you know
we've haven't had one issue with any of
our apps they just run and so we've been
extremely happy so wherever we can use
it I think we would for sure so
Huffington Post we have PHP we have Ruby
we have Erlang we are making a little
bit of an investment in scala a little
bit of python ton of javascript it's
kind of a big polyglot of languages it's
kind of an opinionated question I mean I
think most people in this room would say
javascript sucks you know I like Ruby
but it's a little bit slow so I think
it's you know right tool for the right
problem you know I mean like for CMS the
Ruby stuff has worked out great abled us
to develop very quickly you know for
WebSockets
you know Ruby's probably a bit more of a
head
choice I think our line was certainly
the right choice for us so I think it's
a right tool for the job kind of thing
yeah yeah
yeah we have our own data centers we
only need one so yeah I mean we don't
really share how many nodes right now we
probably don't we have like a handful of
nodes nothing too crazy but you know I
mean based on the numbers I showed you
don't need there's no minimum you can
run on one if you want to do something
like that but it's horizontally scalable
like a web server so based on your
concurrency you add as many nodes as you
need I mean it's a latency concurrency
you know game right you could probably
support more than a hundred thousand
connections on a sock jeaious no but
your latency is gonna go up so that's
just kind of balance you got to play
with why do we need back-end persistence
oh we don't there's none well we're out
the the user to the same server so we
don't have to worry about sending
messages and figuring out where they are
in the cluster right so since one user
can only ever be mapped to one server we
can comfortably broadcast every message
to every server and no it's only ever
gonna be destined for one user for the
so it just prevents having to have like
a back-end shared state and say oh hey
user a your on node two user B you're on
node three and then you have all this
routing logic and I give another single
point not a single point of failure
about another back-end to worry about so
there's some there's some timeout logic
on the JavaScript side so basically you
know I mean there are some extreme
conditions but you know we've tested
shutting down a node there are some
callbacks that happen on the JavaScript
side that have a timeout and we just you
know issue a reconnect and the user
recent scribes so we've done some pretty
some pretty thorough testing on the
JavaScript side with that
Mongo's fine I feel like that's a
question for beers my honest answer
because we're going through this now I
think the global right lock on Mongo is
idiotic it's my personal opinion and
they've gotten it better it's down to
database level but the Mongo do so we
have one database so we're fine
we have other scenarios where we have we
have to basically roll out multiple
databases because right now on to to and
above Mongo locks at the database level
which is insane so I don't know it's
fine I would use it with caution the
out-of-the-box defaults for Mongo are
completely unsafe alright there's
there's no guarantees out of the box so
you know if you're writing like a CMS
and you need guaranteed trans not even
transactions but you to ensure your data
is written to disk you need to change
the defaults so it's been fine for us
you know but I think it needs to be used
with caution
anything else all right thank you guys</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>