<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Erlang Factory SF 2016 - Brett Cameron - Effective Parallelization of Monte Carlo Calculations | Coder Coacher - Coaching Coders</title><meta content="Erlang Factory SF 2016 - Brett Cameron - Effective Parallelization of Monte Carlo Calculations - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Erlang Factory SF 2016 - Brett Cameron - Effective Parallelization of Monte Carlo Calculations</b></h2><h5 class="post__date">2016-03-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/xj3smNjGLaE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">yeah so this is kind of one of those
talks where I fired out the title it
just came to me in a burst of
inspiration followed by perspiration and
quickly whipped up an abstract and seen
it off to monitor and then said oh hell
what am I actually going to present so
it goes off on a few tangents and
probably spends more time talking about
the virtues of Erlang for doing
applications on SMP multi-core
environments and so forth but hopefully
there'll be some bits and pieces in
there that that are of interest to you a
little bit frustrating having to stand
here behind the lectern because you see
I chose this t-shirt to try and distract
people from the talk and things like
that but you can't really see it
properly although maybe I'm tall enough
but anyway so I'm not going to read the
title again you've read the abstract and
things like that and that's why you're
here so my name Brett Cameron as we said
in the introduction I've been around for
a disturbingly long time now I started
out in the IT industry back in about
nineteen ninety two I'd finished my PhD
and didn't have any money and seemed to
be able to write code and ended up
getting a job with what was the in
digital equipment corporation writing
software for them and I got stuck there
pretty much ever since and we went
through from digital to a compact to HP
and I spent a lot of that time as was
mentioned traveling the world working
with HP's customers helping them to
modernize legacy environments mostly
around the openvms side of things which
some of you may know as a legacy
operating system that came out of
Digital Equipment Corporation originally
and now ended up with with HP for the
past few years I've kind of varied that
a little bit and spent a lot of time
working for the HP Cloud team mostly is
the sort of RabbitMQ guru rabbit of
course is used extensively with an
OpenStack which is what HP used for
their cloud offering around about a year
ago a little over a year ago I left HP
and joined this company VMs software
incorporated which is a kind of a
strange sort of startup company based
just out of Boston I'll be up there next
week I'm from New Zealand by the way so
in addition to the longest title I
possibly had the longest flight but
that's beside the point and VMS software
inc we've licensed all of that old
legacy operating system off of HP and we
have the right to create new versions
and things like that and my role within
the company is around open source
figuring out our open source strategy
getting products open source software
onto the platform and promoting that so
how does all this relate to billing well
my introduction to LA and probably
started around about 2008 time frame
when I was looking to try and get an
amqp implementation advanced message
queuing protocol implementation working
on VMS and I kind of like to look at
RabbitMQ it had a catchy name and good
documentation and things like that but
there was a problem it was all written
in this weird language called early so
having had a few beers with a friend one
day at the airport this friend basically
dared me to try and plot Erlang to the
openvms operating system which I Julie
did so that was kind of my involvement
so you know I don't really consider
myself a particularly crash out early
and programmer but I like to think I
maybe there's a consequence of doing
some of that porting work actually have
a reasonably good understanding of
what's going on inside thing all
completely irrelevant to what we're
talking about today but there you go
anyway okay you read the abstract you
know about me so what I'm going to do in
the talk is breaking it down like this
the introduction and background is
really just going to talk about how I
came to be involved in doing these monte
carlo simulations and things like that
how many people here are familiar with
monte carlo techniques or know anything
about them we got one or two so that's
good I'm actually going to do a little
bit of an overview it's very very high
level and fairly simplistic but just do
a little bit of an overview or
introduction to Monte Carlo methods give
you some feel for how they're used and
what they're all about terminology
that's just simple stuff that i'll raise
three because you all know it and then
just sort of start looking at some of
the characteristics that i think maker
laying really good for doing monte carlo
simulations now one of the things you
know that you may be thinking is hey but
a lot of these monte carlo simulations
team to be number crunching elings not
necessarily so great at number crunching
but what it is really good at is
stringing everything together so you
might have some poor
so whatever the written in C or some
other language that are doing all the
heavy lifting as far as that number
crunching is concerned and you're using
Erlang to string it all together within
your SMP environment within your
distributed computing environment and so
forth so we'll talk a bit about that
I've also kind of got a bit of a random
section in there when I was playing
around with some ideas here and I'm
still working on some ideas in this
space you know I realized just how many
options there were with them being for
tuning the the Erlang vm for multi-core
so I just wanted to talk about some of
those briefly so from a Monte Carlo
perspective this is me many many years
ago before I had a job and before I had
gray hair and things like that back in
1988 when I embarked upon a PhD in
chemical physics which also had a nice
long title but I didn't come up with
that title my professor did and weird
kind of managed to score some funding to
create a thing called a molecular beam
machine which were these things roll the
rage back in those days for studying
chemical reaction dynamics this is quite
a coup for little old New Zealand down
the bottom of the world my professor
managed to get this equipment and we'd
put it all together and built this
machine to study some of these rather
interesting chemical processes you know
people sort of think oh yeah you know
chemicals react molecules hit each other
reaction happens you know you put two
things in a test tube and shake it up
and there's a color change or heat
evolved or some smelly gas comes out or
whatever but in order for those
reactions to occur the molecules
actually have to collide quite often at
specific orientations to one another
it's not just any random collision and
what we were looking at studying with
this apparatus was those sorts of
collision dynamics how do these things
actually have to collide with one
another but there's a problem with these
machines they break down a lot they're
ridiculously complicated there's lots of
wires and lots of pipes and all sorts of
fancy bits and pieces and we would spend
a lot of time at the start of my project
anyway designing components and then
having to wait for them to be built or
components to be shipped from overseas
or just general downtime and as I think
I've commented here even even when the
machine was behaving itself and working
it would actually take several days to
get down to an acceptable vacuum before
we could do experiments and my professor
really did not like me just sitting
around reading the newspaper doing
nothing and I didn't really like doing
that much either so I came up with the
idea that well what I've got all this
dead time with the experiment why don't
I start looking at the theory and
figuring out how I can simulate some of
this stuff and in those days compute
resources for fairly lean as I've said
dear we had an old vax VMs system at the
University with basically with 26
megabytes of RAM serving most of the
campus of what was then i suppose six or
eight thousand students obviously not
all using machine at once or anything
like that but we had limited compute
resources and trying to solve some of
these molecular dynamics problems via
conventional numerical means just wasn't
going to happen it just couldn't be done
with the computing resources we had
available and so feeling a little bit
depressed about this I wandered off to
the library which you know the older
people here will know as a thing that
has books in it and I stumbled across
some books on these things called Monte
Carlo methods thought okay well that
sounds pretty cool so let's let's get
stuck into that it actually bypassed a
lot of these constraints that I was
faced in terms of the compute resources
it did have a consequence that
calculations would take an awfully long
time but whatever I could actually kind
of see stuff happening and it was
working nailed the PhD eventually
totally broke ended up at digital nth
much then I got bored around about 2000
and went back to the University again
and we did some more research the
machine had evolved considerably since
this time and there's a new technique on
the horizon for studying chemical
reactions which was called iron imaging
and I on imaging was kind of quite cold
if one of the products of your reaction
is an iron you can use electrostatic
lenses and so forth to focus that
essentially up onto a phosphor and
basically take a pile of photos of the
thing and if you gather enough enough
images you know you get enough of these
things you can take take it create an
average or whatever image or an overall
summed up image and that actually gives
you some data about where when those
molecules collide where the reactants
are scattering to
and you can use that information to
recover a lot of information about the
reaction dynamics but true to form
despite the fact that it had been maybe
a decade or eight years at least since I
played with the bean machine true to
form it still broke down and we had to
wait for parts and things like that so I
wrote some code since I've been out in
the workforce for a few years I'd
learned other languages like C and and
and the evil C++ but we still had
relatively limited computing power we
were however able to produce much
prettier graphs so I said about
simulating again what should happen with
this iron imaging technique we never got
images that actually look quite as good
as those ones over there on your left
and they certainly weren't in color but
the whole experiment did work pretty
well and the simulations were actually
very very useful in terms of guiding the
experiment so this was essentially my
introduction to Monte Carlo techniques
as a method of simulating physical
processes in this case chemical
reactions and things like that so what
are these sorry sorry CWA no to I mean I
could talk about the mathematics of
analyzing those images and things like
that but it's terribly boring and
there's lots of squiggly Greek symbols
and things like that and I probably
forgotten most of it in you so putting
it really simply Monte Carlo methods
randomized algorithms computational
experiments okay they can be used as
approximations where analytical
computation is not tractable I'll talk a
little bit more about the history of
Monte Carlo methods very soon but you
know where normal numerical methods just
aren't track aren't aren't feasible you
know your equations might just have four
or five integral signs and all sorts of
nasty variables and things that you need
to deal with and trying to use
conventional numerical methods is for
whatever reason just just not feasible
all the rest is sort of more formal
words but you know just to take a really
trivial example over there in the yellow
so you know just to illustrate what I
mean by this technique
what's the probability that team dice
throws are going to add up to exactly 32
now you know we're all reasonably smart
people here you guys more so than me and
I'm sure you could easily calculate the
exact solution not a problem the Monte
Carlo approach is to do an experiment
let's go and throw 10 dice some large
number of times and see how many times
they add up to 32 mean / divide that by
in and we can work out the probability
ok so we've replaced theory essentially
with a perfectly valid and in this case
very very simple experiment this ends up
being a fairly Universal technique for
the solution of many mathematical and
physical problems natural phenomena like
the reactions and so forth that i was
looking at simulations of your
experimental apparatus and even just
numerical analysis that can be used as a
fairly general technique for solving
very complex integrals and things like
that so it has long been recognized to
know as a very useful technique and over
the years it's it's found application in
a broad number of areas I've listed some
of them there I mentioned integrals
before molecular dynamics yes quite
popular in the finance industry and
things like that interestingly enough
you know whilst you talk about random
numbers and probability distributions
and things like that Monte Carlo methods
are applicable to both deterministic and
probabilistic problems what I mean there
is probabilistic well okay so as a
probability distribution of your
variables or whatever deterministic
these are things that can mess ified
mathematically exactly and involve
absolutely no chance whatsoever
sometimes it's actually possible to
restructure those problems in such a way
that you can solve them using some
probabilistic approach so you can spin a
deterministic problem round into a
probabilistic problem and solve it using
monte carlo methods the history of Monte
Carlo methods was probably been around
also all since since the dawn of time
but probably one of the first documented
cases was some French mathematician with
a very long and difficult to pronounce
name who obviously had a lot of spare
time on his hands because
tell you this way of calculating pi is
not good maybe he just had a whole lot
of people doing it at once or something
i really don't know but you know I can't
imagine that he would have got much past
a value of three as his approximation
for pi using this technique by hand I
won't read it all there but basically
this was one of the first documented
examples of what by today's reckoning
would be a Monte Carlo simulation where
Monte Carlo methods in their modern form
come from as around about 1994-95 1945
during the Second World War Los Alamos
where you had people like John von
Neumann and and many other famous
scientists working on the Manhattan
Project in developing the atomic bomb
and they were faced with a large number
of mathematically complex problems to
solve along the way one of those was to
do with as I've said they're random
neutron diffusion and fissile material
now this in part related to actually
building the bomb but also impart
related to them not frying themselves so
they were very keen to actually make
sure they had things right when it came
to some of this data and John von
Neumann can probably be credited mostly
with pinning down Monte Carlo techniques
and formalizing them and indeed a lot of
the refinements and so forth that he and
other mathematicians on the project
developed are still in usage today in
those days as I recall seeing a picture
a lot of the calculations were actually
done by a large group of women with the
sort of you know deputy tap-tap pull the
lever and a piece of paper comes out and
hand it on to the next person they
didn't have modern-day computing
facilities but with the advent of modern
modern day computing facilities and as
computers get more and more powerful the
sorts of problems you can solve by this
technique just get bigger and bigger and
more interesting I've also mentioned a
couple of other guys Maxwell and
Boltzmann if you know your chemistry and
physics they've got equations and so
forth named after them they however used
Monte Carlo methods to solve these
problems but they never really thought
about thought about it in those terms
they
more interested in the answers than the
techniques von Neumann and his
colleagues appreciated the value of the
techniques I've talked about this
already where you can spin deterministic
problems round into probabilistic ones
by replacing theory with experiment
whenever you can't use the theory for
some reason probabilistic is fairly
self-explanatory observed random values
chosen in such a way that they directly
simulate the physical process of the
original problem ok so you're choosing
values with appropriate probability so
for say just a trivial thing in some of
the some of the work I was doing with
the bean machine the molecules with that
were flying through the bean machine
conform to a certain velocity
distribution that's a probability
distribution so one of the things I was
doing for example was selecting
velocities worth appropriate probability
for the collisions that I was examining
all of this thing of course spins round
to a need for a good source of random
numbers and random numbers of course are
essential to any Monte Carlo simulation
you've got to be able to calculate them
efficiently you also do want them to be
random just a point of note there's no
such thing as a single random number
that is just a number it has to be a
sequence of numbers where there is no
box not a sequence there's no defined
sequence a set of numbers that have no
no known correlation now the best we can
do on computers is what are referred to
as sudam random pseudo-random numbers
and they do have a period which is
essentially determined by the largest
integer that you can represent on the
computer in question but to all extents
and purposes these the pseudo-random
number generators today more than
adequate for pretty much any any Monte
Carlo simulation you know there are of
course truly random physical processes
and I've listed some of them they're
radioactive decay thermal noise cosmic
rays I suppose hypothetically you could
you know interface a Geiger counter to
your computer and put a piece of
plutonium beside it but that really
doesn't sound like a good way of
generating random numbers to me when
people when you talk about random
numbers a lot of people think that these
are just random numbers distributed
uniformly
between two values or something like
that there are of course many other
types of distribution you know I
mentioned the velocity of molecules
flying around in my bean machine or even
flying around in this room they conform
to something that's more approximates a
Gaussian distribution and I've just
tried to illustrate that here where the
there are two noise spectra the top one
is sampled from a uniform distribution
so every value has the same probability
the bottom one is noise sampled from a
Gaussian distribution not the same
probability I mean obviously the
resultant histograms are a little bit
rough but they're definitely not the
same shape right so what you tend to
find is most of those random number
generation algorithms that languages
like early anglesey or whatever provide
will be uniform and you will then need
to provide some sort of transformation
to convert those uniformly distributed
random numbers into a probability
distribution that is applicable to the
problem you're trying to solve
interesting point here who knows who
knows that you know there's this dude
called a ka lang who actually has a
probability distribution named after him
so this is obviously where the name of
the Erlang language comes from and he
was one of the first to start examining
things like I've got in the little
yellow box down there around
interactions at people making calls with
operators at switching stations and so
forth but there are many many non
uniform probability distributions I've
listed a few there gaussian would be the
most common i suppose but i'm sure
you're all familiar with the others and
as i say normally in any of these
simulations you'll use your computer or
your languages uniform random number
generator pseudo-random number generator
and you'll apply a transformation to get
a value which conforms to the
probability distribution that you're
working with one problem that you faced
with with the sort of thing and you are
essentially with Monte Carlo you're
doing an experiment and you do it many
many times with many many different sets
of values and you basically end up
taking an average now in order to refine
your answer and to improve the accuracy
by some factor of n you basically need
to do
squared more samples right so it's
computationally intensive as far as
improving the accuracy of the
calculation is concerned what what
people have discovered and this even
dates back to the likes of von Neumann
and so forth back in the 1940s they
realize things and he never smart guys
of course they realize things like hey
there are these values out in the wings
of these distributions that really
contribute nothing to the overall answer
so why waste their time even looking at
them so they came up with that that was
referred to as important sampling why
waste your time looking at things that
are completely irrelevant to the final
answer they're just not going to count
so they actually devised a whole lot of
techniques which are known as variance
reduction techniques that don't distort
the problem in any way they don't
introduce some sort of unnecessary bias
but they do allow you to calculate and
answer more quickly or potentially get a
slightly more accurate answer more
quickly there are books on this stuff
and it goes on and on and on yeah but
another way that you can improve
accuracy or speed up these calculations
is of course parallel ISM and this is
the biggie here where you know talking
about SMP and things like that is that
most Monte Carlo simulations are easy to
parallel eyes you could run the same
simulation with different random numbers
on multiple cores combine the results
and you've got it answer ok so like I
say here say we want to find an
instrument around about n times faster
we are in as the number of cores or CPUs
we've got it out espousal yeah so
running these things in parallel they're
very amenable to running in parallel be
that multi-core or distributed so just
very very quickly some terminology just
like people need to know it
concurrency we all know what concurrency
is we all know what parallel processing
is but just for the sake of it
concurrency property of systems in which
several computations are executing
simultaneously and potentially
interacting with not one another
parallel simultaneous use of one or more
cpus or processes etc etc we won't talk
about bottlenecks because nobody likes
them so I mentioned with concurrency
there you know sometimes there will be
in direct interaction the idea of
parallel ISM hopefully there is no
interaction although you know in theory
and practice often two very different
things and trying to avoid any
interaction in either space is just
about impossible in many cases now
here's a picture of Joe doing something
really nasty to a cupcake with a
soldering iron i I really don't
understand but I thought it was a cool
picture so having defined concurrency
and parallelism I thought well how does
Erlang to find them ok so as erlanger
finds them concurrency refers to having
many actors running independently but
not necessarily at the same time it
defines parallel parallel ISM as many
actors running at exactly the same time
and i cut and pasted some text out of
wikipedia that that kind of made me
laugh a little bit and i suspect that
some of the object oriented program is
might get a little upset with but that's
that's life multiprocessor multi-core
hall standard stuff multiprocessor more
than one processor sharing bus and
memory multi-core more than one
processor on a chip each with its local
memory and access to shared memory just
so you know ok so with all the
formalities out of the way I thought
look at a simple experiment i mentioned
to work that that french guy did back in
1777 to estimate pie and i had a little
bit of a search of the net and i found
that this this piece of code with the
link there which is a nice little Erling
example of Buffon's needle problem ok
it's an early implementation of
calculating pi for our Monte Carlo means
as I say written completely in early and
it's kind of you know it's obviously a
trivial example
does illustrate very nicely the ability
for this to scale across multiple cause
as far as this program goes all you need
to do is specify the number of illing
processes and the number of trials that
you want to do it goes away and comes
back with an answer the good thing here
is because this is actually a
computationally intensive thing each of
those early in processes actually
completely obliterates one hundred
percent of the core so if you've got 30
of these processes running you're
basically going to be using up one
hundred percent of 30 cores on your
system so I ran some tests on a 64 core
server running centos with a length 17
and I did that 41 to 60 processes so
essentially chewing up a hundred percent
of between 1 and 60 cords on that
machine I also thought just for
comparative purposes I would do it with
one to 60-year LAN processes with only
32 cores enabled and also this
particular system supported
hyper-threading so I thought I'd see
what effect that might have did like a
hundred million samples in each case and
this is what I was saying if Byfuglien
and the streams were calculating pi this
way it would have taken them a heck of a
long time to do 100 million and the best
answer I got out of this for 100 million
was 3.14 one ok so there was the result
it's basically pretty much as you'd
expect the blue line which unfortunately
is somewhat obscured by the red line but
the blue line is the good line with with
one to 60 processes all 64 cores
available and it goes pretty much as I'd
expect I mean basically doubling the
number of processes roughly halves the
overall time taken to complete the
calculation there's a bit of a glitch
around 57 cause which i think is just
just some syncing issue I haven't really
looked into it I wasn't too concerned I
suspect it's actually the operating
system as opposed to an erlang issue the
other two lines red and purple lines
show with just 32 cores enabled or with
hyper-threading enabled and you can see
things start glitching a bit around
about 32 process mark which is kind of
what you'd expect but I was pleasantly
surprised that the glitching the
a going back up it wasn't as bad as I
thought it was going to be I was kind of
actually surprised about that but i
would say mileage might vary in that
respect because this is a totally
trivial calculation if you had a much
more complex workload I daresay that
with only 32 cause enabled once you hit
that barrier you would probably start
noticing some serious degradation it
made virtually no difference I have you
know I talk about it later on but I
found that mixed results with
hyper-threading yeah yeah just very very
quickly I mean when it comes to just how
much scalability you can get out how
much speed up you can get with the
program I mean it all basically comes
down to arm Dale's law and it's limited
by the serial part of the program I mean
it's kind of common sense but as I say a
lot of these monte carlo simulations
like obviously the pie calculation is
totally trivial a lot of these
simulations are basically trib you leap
aryl I zabal that this really doesn't
need to be any interaction between the
threads running on separate cause at all
so more complicated problems I was
intrigued I've been reading a paper that
was written by Google back in 2013 where
they were looking at applying monte
carlo methods to the solution of certain
big data problems and i thought well
okay that's kind of interesting how can
we hook Erlang in with that and as i say
this is this is something i'm looking
into no no solid results or anything yet
if you look in general terms though at
the use of early and for Monte Carlo
calculations it's fairly limited a few
trivial examples and and test programs
and things like that and I think a lot
of that does come back to my comment
earlier about the fact that you know
Erlang's not it's not a number crunching
language it wasn't designed to be it
wasn't intended to be but what it is
good at is doing the SMP thing doing the
distributed thing and you can use it as
the vehicle for stringing together other
code that's written in language more
amenable to doing those calculations if
you look
programs like react for example leveldb
C++ code bit cask C C++ code level DB is
evil C++ code it's not pure Erlang okay
so I think as I say where Ln comes in is
acting as the glue acting as the
mechanism to string all the stuff
together but as a general comment as I
say that the use of Monte Carlo for big
data analysis type work seems to be
growing and I've cited a few examples
their genome research marketing research
medical imaging things like that it's
starting to find application in these
areas and it would be nice if we could
string Erlang into the equation here
this is the the paper I was referring to
that was written by Google back in 2013
and it uses a special Monte Carlo
technique based around the use of
bayesian statistics it's a good paper to
read if you're having trouble sleeping
but it's it's it's it's pretty good but
it's it's hard going it's it's heavy
reading and it's an interesting
technique and you know I've kind of been
really rude I think there and said that
basically it boils down to a form of
sharding but it's it's kind of not too
far from the truth it's an interesting
read as I say if you if you feel like
getting into that sort of thing and I'm
currently just doing some research
looking looking at what i can do with
relaying and the stuff the paper itself
talks more about distributed as opposed
to multi-core but from an early in
perspective doesn't matter this is just
just amusement for my sake this this
whole big data thing just seems a little
bit crazy so I'll just have a small rent
I also like the fact here that you know
they're saying by 2018 the United States
alone could face a shortage of 140
290,000 people with deep analytical
skills and 1.5 million managers hang on
hang on the ratio seems wrong to me
but i'm not sure what this was trying to
say it sort of seemed to be saying to me
that that software engineers these days
all they knew how to do is to write code
and they didn't actually understand data
why that seems a little bit harsh to me
and I would kind of question I just
found it rather amusing anyway it's good
to know that we probably all have jobs
it's cool just going to go off on a
little bit of a tangent here and talk
about their legs approach to SMP and and
the role of Erlang potentially the role
of early and these Monte Carlo
calculations doing SMP by hand is hard
you know if you have to sit down with a
C compiler and write code that runs on
multiple cores deals with all this the
shared memory threads locking all that
sort of stuff you seriously get a
headache and you'll probably soon start
developing liver damage as well for
other reasons you know even with
languages like Java Blissett threading
in Java was a real pain in the butt as
far as I recall you know languages like
Scala have helped to address that to
some degree quite nicely to be to be
fair to scala and you've got things like
the akka toolkit to help you with the
construction of distributed and
concurrent applications but it's nowhere
near as transparent as it is with Erlang
you sit down you write a piece of la
encode you barely even have to think
about the SMP or whatever it's their
magic happens you know simile when I was
doing my PhD they were there were these
api's pvm and MPI were kind of available
but and very good pieces of code that
would allow you to distribute
applications across multiple generally
across multiple machines but once again
this specialist knowledge required you
need to understand those api's you need
specific knowledge on how to incorporate
that into your C code or whatever in
contrast Erlang magic happens you know
in general it's transparent to the
programmer that there's all this stuff
going on around SMP it's got all the
good bits you need in there to avoid all
those headaches around
you know coordinating access to shared
memory it doesn't even let you do it
it's you know got all the asynchronous
message passing distribution
transparency all that stuff that most
other languages you'll have to craft
something I mean I've seen a disturbing
number of my travels over the years I've
seen a disturbingly large volume of code
that does all this sort of stuff and
these are applications that might have
been in use for 20 or so years written
in languages like Pascal and things like
that and when they break they break in a
major way and it's a nightmare to try
and figure out what the heck happened
early and does so much good stuff for
you it's a blessing I would however say
that you know obviously the earlier
chill machine beam is written in C and
it's a pretty hairy piece of C code but
you don't have to worry about that it's
taken care of all this good stuff for
you okay so I commented before that you
know ur llings not such a great language
for number crunching it's not that's not
what Erling was designed for in the
first place you might be able to get a
little bit more using something like
hype in some situations for some sort of
number-crunching type application that
was interesting hearing the guys talked
yesterday about how they're looking to
use the LLVM compiler project you know
so maybe early and we'll get to the
point of being just-in-time compiled
much like like some of the JVM based
languages or something like that i I
don't know there's certainly a lot of
work going on in that space point I made
and something that Garrett Smith did a
good talk about in Chicago back in 2014
is the calculations not conducive to
early and can be offloaded and
implemented in a port or something like
that or a driver that you would write in
C C++ i mentioned bit cask and leveldb
as being a couple of good examples of
something like that you know
heterogeneous applications of the norm
homogeneous applications not really the
enormous it's a fact of life I think
obviously if you're going to start
spinning off ports like this you need to
be very careful to make sure that they
operate at an asynchronous manner
otherwise she'll end up blocking
schedulers and bad things will happen
and you'll end up essentially
synchronizing all your code but
interesting talk from Garrett as I say
back in 20
14 and he's got a little bit of code up
there and github some of the ideas that
he was playing with around this approach
of having early basically coordinating
the whole thing but you may have other
applications in the mix these ports or
drivers that are implemented in 3gl code
just very quickly I thought I'd put up a
comparison with scalar for dealing with
this NP as I say it's not transparent
it's got some nice stuff in there but
it's not totally transparent let's say
we've got a really simple example here
we've got some tail recursive method
called do it which is going to do some
number of trials what's that number 100
million or something like that trials
and return an answer we can parallel
eyes that really simply as I've shown
here basically is paralyzer a map over a
collection so we're going to do 16
separate computations but of a map but
of a reduced got our answer that's quite
neat not too hard to change if we then
want to make that take advantage of
multiple cores again very simple we just
drop in the dot power method and it
converts it to running in parallel but
it's not transparent and it's not
distributed it's multi-core so
Scarlett's got some really good stuff in
for this kind of thing but it's still
not as nice i think as a yearling
approach you know to be fair though i do
quite like some of these languages i
quite like rust it's obviously still got
a way to go and and all of these
languages that I've listed there I think
have their virtues and it as usual it's
a case of using the right tool for the
right job what is really good to see
though is that I think there's there's
now general appreciation of the value of
some of these functional languages like
Erlang rust and so forth when it comes
to implementing these massively scalable
and fault-tolerant a highly concurrent
applications okay it's really good to
see so
just very quickly just talked about a
few things me most of you I'm sure are
familiar with tuning the vm for this
sort of thing and you'll know that by
default when you run beam or when you
run the l command beam will try and
figure out well it will try l will try
and figure out whether you're on a
system with one or more cause and it
will start either beam will beam Dottie
SMP accordingly you can obviously
override that using the minus SMP
enable/disable flag by default it it
takes the auto approach and it will
basically try and use whatever cause are
available to be used it will start up a
scheduler thread for each and those
schedulers have their own little run
queues which as I say they're a list of
Erlang processes on which the thread is
going to spend a slice of time and early
and got some quite nifty code with these
schedulers where if a shed EULA has too
many tasks or whatever Erlang the vm can
try and balance the load across the
available schedulers this was introduced
oh I can't me but must have been around
fitting maybe a little later anyway so
the pus capital s option is used to
specify a number of scheduler threads
and as I say unless instructed otherwise
it's going to try and spin up the vm is
going to try and spin up as many it's
going to try and determine how many
cores are available and can be used and
it's going to spin up that number of
scheduler threads if it can't make any
determination it's going to default to
one which could might be a bad thing in
slow down your application so hopefully
on whatever platform you're using LA it
can figure out what your CPU topology
looks like now plus s is you know the
simple option there are a whole range of
other flags that you can use to tune
scheduler threads and I just wanted to
go through just a couple of them really
should you the wake up intervals an
interesting one that started off life
actually is a bit of an experiment and
if you read the documentation it kind of
looks like it may disappear at some
stage so this option this option is SF
WI when a scheduler thread runs out of
stuff to do
will fall asleep waking it up again
requires a system call system calls take
time so this flag was built in and
included to try and limit the the impact
of this kind of thing happening as I say
I'm not sure it may go away it may not
but you know generally speaking you want
to try and keep all those see dealer 32
doing something and try and avoid them
from going to sleep because if they go
to sleep it takes time to wake them up
much like getting my son out of bed in
the morning is just about impossible
that's that's kind of more than a you
know a system call that's a beating
around the head I mentioned before how
the the the schedulers have it have a
run queue and so forth an erlang will
try and that the vm will try and balance
the workload there are two approaches to
that one is to try it is what's called
compaction of load with a vm will
attempt to fully load as many scheduler
threads as possible the issue you
potentially have there is that you may
end up with some scheduler threads with
absolutely nothing to do and they will
fall asleep I should however note that
that compaction of load is the default
scheduling model the other model is
utilization balancing of load where
basically the vm will try and balance
the workload across all threads all
scheduler threads five minutes to go
okay that'll be right and you can you
can tune that and that's something to
watch out for if you have applications
in this space so in general if you have
an SMP application you've got
performance issues there are a lot of
knobs you can play with but chances are
what's optimal for your configuration
may not be optimal for another
application so you need to play around
and experiment in general bind your
schedulers to cause that can have a big
performance improvement turn off load
compacting don't let your schedule
asleep bad stuff will happen
you could you could have that issue I
mean these are things you'd need to
watch out for of course yeah if you're
if you're locking she jealous reads to a
core and and and you then start up
another another instance of beam and you
do likewise you're probably going to
shoot yourself in the foot somewhere
right just very very quickly then some
some cpu topology examples you can do
early on system info CPU topology and it
will tell you good stuff like in this
case this is my laptop it's a it's a
single dual-core processor a 1-1 process
of two cause it's got hyper threading
enabled which are the logical things you
can see out there a somewhat bigger
example 8 core processor with
hyper-threading again the same deal just
two CPUs eight cores on each and hyper
threading enabled I won't go through
this because we're running out of time
but basically this just illustrates when
you start up by default by default
schedulers are not bound to cause okay
you can use the SBT option to specify
various ways in which they get bound to
cause and it's generally quite a good
idea to to do that binding in one way or
another but read the documentation as to
which option is best potentially for you
we talked about hyper threading I you
know as I say I've had mixed results I
mean give it a spin see if it works for
you it's it's it's potentially useful as
I say I've just had some things where it
seems to make no difference in other
cases it's worked quite well okay so
very quickly summary and conclusions
Erlang has many traits that make it
ideal for for multi-core and certain
types of problems problems that conform
to naturally concurrent patterns and
that includes many problems that can be
many many sorts of Monte Carlo type
simulations which are trivially
paralyzer ball you can you can run those
problems on multiple cores using
different sets of values and merge the
results very very easily Monte Carlo
calculations generally involve number
crunching erling is not very good at
that you can spin that out to a port
written in another language that will do
the job better and use Erlang to string
the program together and act as the glue
that coordinates all the activity
we'll just finish up with a quote from
Joe this time without him toasting a
cupcake and just taken from his book
which i think is poignant and relevant
to the whole concurrency thing okay and
then finally any questions and a nice
picture yeah that's actually a very good
question the question for those who
didn't hear how to approach paralyzing
the random pseudo random number
generation that's I knew somebody would
ask me that and it is always a problem
so if you yeah it is actually and if you
look for example I don't have a
particularly good answer for you at the
moment but it's interesting to see how
if you look over the years how Java
seems to come up with some new
pseudo-random number generator just
about every release right it's it's an
ongoing issue and that can be the real
killer yeah very good question sorry
glossed over it yeah
the
I
and it's on thank you excellent alrighty
any more questions okay tea and coffee
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>