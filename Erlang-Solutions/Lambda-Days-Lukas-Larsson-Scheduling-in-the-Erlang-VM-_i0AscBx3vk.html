<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Lambda Days - Lukas Larsson - Scheduling in the Erlang VM | Coder Coacher - Coaching Coders</title><meta content="Lambda Days - Lukas Larsson - Scheduling in the Erlang VM - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Lambda Days - Lukas Larsson - Scheduling in the Erlang VM</b></h2><h5 class="post__date">2014-03-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_i0AscBx3vk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everybody my name is Lucas I am
consultant working at our institutions
and I be spending the last 45 years or
something at ericsson doing development
on the a language machine so doing c
programming mostly in there this talk is
going to be very airline eccentric the
closets I line I'm going to try to
explain the algorithms and ideas we use
to distribute them to come in then scale
on different hardware so C trying to
explain and see a bit about how what's
up reach their magnificent figures but
not and it's a kind of a difficult
problem to do and it's something that
not many other virtual machines try to
do we try to be an operating system for
you basically so when you write on your
linux kernel they scheduled processors
and they know it's scheduler hundred and
fifty processors running at the same
time possibly up to a thousand we try to
manage somewhere and I think we've maxed
out it's the biggest one I've seen is
somewhere in the region of 8 to 10
million running at the same time people
keep pushing this boundary and really
the boundaries the amount of ram memory
you have on your computer as such a
scene there's a couple of machines with
about two three terabytes of RAM that
are running a line that have quite a few
processes in them so I'm going to try to
explain what we do and how I decide
where things are supposed to be running
so we're going to talk about processes
which is a line code executing insid a
language machine so that your sequential
a line code port is a another concurrent
entity you have in Ireland which runs C
code so which interacts with your
operating system beneath you an example
of a port is an open file so you have a
file that you're reading or writing for
that support another thing is a socket
connecting to a TCP connection that's
another instance of a port so these are
concurrent entities in ireland's you
send messages to and from ports and
processes in a similar way that so you
community
by message passing between processes and
you can also do a process port
communication the third thing that we're
going to talk about this timer so timers
is an integral part of most software
development you want something to happen
at a time out later on so if something
doesn't happen for four seconds you want
something to happen if you want to wait
for a response for one minute scheduling
when these types of timers happen is
quite complex when you start to have a
couple of billions of timers in your
system own wits and then we're going to
go on to the scene deciding where things
should be run so if you're in a
multi-core system this fits the thing
with having a couple of million
processes in the same system fits quite
nicely with guidelines so we try to load
balance and do different things there so
load balancing these things and then I'm
talking about about profiling so a
process when you create the processes
and there's a built-in primitive in our
line that you spawn another process when
you spawn on the process it's created in
the same thread so I'm going to use the
word scheduler here but basically that's
an operating system thread in your work
in the virtual machine in a normal
Evelyn virtual machine you have one
scheduler park or execution unit in your
hardware so a process when it's created
it spawned on the same scheduler as the
the originating processes on a process
runs on that scheduler until it runs out
of reductions it blocks into receive
when there's no messages or no other
processes and messages to it's at boxing
you receive a big strap is an internal
thing that also can cause this not
really important or we're trying to send
something to ports that's blocked for
some reason so the TCP connection might
have gone down so then it might stop
running when we're talking to that port
all airline processes are pre-emptive so
they can be scheduled out at any point
into a function call so whenever a
function call is being done you can be
scheduled out it or not you're not going
out in the middle of a function but
whenever you do a function call and send
serving is a function program
we do recursion into a lot of function
calls so looking at the different
processes and how their schedule
basically we have what we call run
queues one for each scheduler which is
basically linked lists of all the
different processes that are active to
be run at the moment so on a normal
system you would have scheduler one two
three and then all the way up to the
number of course you have I've seen a
lung systems run on up to 256 core
machines they're trying to get it to
work on a 512 I believe but there's some
issues so quite a few of these a normal
system that I will deploy would have
maybe eight eight course was pretty
standard set up and it thinks
unfortunately they're having just one
run queue it's not really enough to map
the island paradigm because in erlanger
also be on our processes with priorities
so we actually have three different
types of run queues for the different
processes we have a max a high and
combined normal and low Q and and these
are hard priority so if there is
something in a maximum one it will
starve things in the high one and in the
known one if it just hopes CPU so
normally only system critical processes
are putting these things and the normal
Allen program and never uses these
priorities investors very special
circumstances so in order to figure out
how long a process is supposed to be
running before it gets scheduled out if
it receives messages and this consumes
consume consumes we want to have some
kind of progress in the entire system we
do not want one process to take all of
the CPU power so in order to count this
we allow one process to run what we call
10,000 reductions which is basically a
counter that starts at two thousand and
we decrement it until we get 20 each
documentation is done when you do a
function calls every time you do
function call is decremented by one you
also do documentation of this when you
have garbage collection process so if
you have a garbage collection of this
process depending on the size of the
data that needs to be collected you will
decrement a certain amount of
these reductions and for beef course
which is built in function course that's
see calls as well for different things
you will also decrement these reductions
and when you reach zero you will
schedule out you will be put at the end
of the run queue and the next one gets
its own this way even though a system is
flooded with connections or if you're
under a denial of service attack or
something like this you're creating one
process participe connection is just
flooding in you will still have a
measurable progress in your system you
won't get things that are starving
things out but you will have some kind
of progress that seems to be important
in order to yep what is the reduction
call this well if you call it if you get
right over something with 1 million each
time you iterate it would be one
reduction so it will do two thousand
operations we put at the end of the
queue and then two thousand against it
consumes two thousand elements each time
it's scheduled ins and i'm scheduled out
again and every other things are allowed
to run in the meantime so processes are
not really interesting i think in when
it comes to scheduling because they're
quite predictable they're on Alan code
and so on well the things that are
interesting is the way when you interact
with operating system because the
operating system is much more
unpredictable than just running your
normal Aaron code because you don't
really know how what the response times
of the operating system is if you do the
file read you have no way of knowing how
long that fine read is going to take it
might be on NFS that needs to do a
lookup into some server somewhere you
could take seconds for it to run so
managing to the port's that that operate
on the operating system is a much more
challenging task this does making them
more complex they have a lot of things
similar though with the process portis
again created on the same scheduler as
the process that is creating a port or
the port that is creating the port only
some of the activities of a porter
actually scheduled and put in
they run queue some of them are executed
immediately in order to bring down
latency and things so this is an
optimization detail again ports as well
run month in two thousand reductions are
consumed or it's out of tasks here we
have tossed instead of messages that on
their own the key thing about ports is
that they are not pre-emptive in any way
because the running sequel and see it's
not really a pre-emptive language at all
so we have to do a lot of tricks and
things in order to make them not do this
so for instance if you're running a port
that's a TCP socket you will always try
to do it you wouldn't always do a
non-blocking right on the socket and
then do a select on it later on the
yelling distribution uses ports almost
right off the bat so it's say if you if
you're talking to another know they say
you can actually see that there's a
action portal anything on so expanding
on my previous picture we also now have
port in the run queue mix of things that
are being run so I'm going to go a
little bit deeper into exactly what the
different port things done doesn't worry
it is about in order to give you some
understanding of what it needs to
schedule so we have two categories of
things that a port has to take care of
things that come from the island
programmers so say you want to send
something over TCP socket you want to
send some data over TCP socket this is
what we call a command you send to a
port so this is a some kind of payload
that you want to push on your TCP socket
we also have a control function that is
done this is similar to what we do in
ioctl to if I socket so you're changing
the state of the port somehow but you're
not really consuming pushing data to the
port and you can stop it as well and we
also have things coming from the virtual
machine so things that are triggered
from the mock turtle machine this is
things like okay so if something on the
TCP socket comes in and I need to read
this then I have the file descriptor in
a select and this will trigger these
kinds of code back so for both input and
output callbacks and some other things
let's see
so the way that processes and Allen
processes and ports interacts depends
victim was how many skills you have and
in trying to scale these things while
keeping the latency down is quite
difficult and there's a lot of things
when we talk about language designed so
on mistakes that have been made in
airline that makes it difficult to do
this thing I'm going to describe one
example of what I would call a the sign
flow of a line that really rich havoc on
scaling these things but since you have
to be Coco we have so much code or out
running in the world depending on these
things you cannot remove it or we change
the language so you just have to live
with it and make solutions that work
around it so when you're talking to a
port you do a command you're delivering
some kind of payload that needs to be
written to file or sent over TCP socket
what happens then is that you take
immediately you take a look on that port
making sure that you are the only one
executing on that port at the moment and
the way that this port command called
works is that it's a it's a function
code that you make on a port and this
returns true if the command was
successful so this is a synchronous
command and this is a problem because we
have to know because we have multiple
entities sending things to this port at
the same time so we don't know if one of
the things that could come back here is
saying the fort diet for some reason
somebody did a stop before I do the Sun
but since you have parallel things
executing at the same time you have a
problem so there's an other syntax
that's not used as much that does that's
not synchronous is that you can just
send a message to the port using the
normal I like message passing things and
this scales much better because we don't
need to have a reply saying this port
was alive when the message was delivered
and given that TCP you can't really
guarantee anything along the line anyway
we can guarantee that the TCP message
was sent to the buffer of the other
machine but you cannot really guarantee
that it was read by the actual
application on the other side
you need to have some kind of
synchronous mechanism on top of this
anyway it makes sense to use these
synchronous ap ice instead but locked
and lot of code find it very useful to
be able to say what's the port alive
when I sent this data so the scaling
problem comes into play when we do
something like this so we do a port
command and then we have another process
running on a different schedule er
trying to do the same thing and this
process blocks so it cannot continue
executing its command because it's
already executing something else and
then you have more and more and more and
that's we're running up in course we
have more and more course and more of
them start waiting so we gotta get a not
a deadlock but we get a lie book on this
thing then yes backward compatibility
presenter yeah yeah if which means you
start changing things that are integral
parts of the language the way it is many
things will break in his people depend
on these things work and it's not just
TCP because it's unreliable it but if
you have things like a CT pier and these
things where you can actually act on
things or if your integral interacting
with something else that is actually a
reliable thing then these primitives
become useful and just because it's like
one of the good things were like if
princess if you're working with UDP you
have absolutely no guarantee that the
things work actually so I was playing
with the idea of making the airline UDP
driver just send everything to drive
knob because you don't have any
guarantees at all and it doesn't it's
the same thing you have there so you but
you can do some guarantees to get it
along the way and this small things are
helpful for the program so it's a yeah
but we know with some clever ideas about
how to do these things we can make them
scale but if you really want to have
your software skin and then this one
takes this one and so on so if you do
the porch sending thing again we can
optimize things by saying ok so we
create a small item that needs to be run
after this look
been released by the original command we
still want to do the original command
because this brings down latency a lot
instead of automatically scheduling and
putting it into run queue and then going
back I will increase latency by a lot
but if we can go directly into the port
and mediately execute the command and
then return that brings down vacancy and
then all of the things we know that this
will hold the system we can add more and
more and more processes and we put these
files and the processes can continue
executing not caring about this thing
but you lose the fact that you don't
know if the data was actually delivered
out on the TCP socket at your end and
then you release a lot there and these
tasks and take the look and you consume
these thoughts on the other side so
that's how we try to scale these things
and there's many small minded things in
the language that were very good because
when Alan was first developed it was
developed for one Corp these concurs
these things were just a nice
abstraction it was not there in order
because you have many cores that you
were running on was one core and then
this didn't matter because it you always
were able to take the lock the Warner
locks we were just operating on your
same data and then the code has grown
and grown and grown and we added S&amp;amp;P
support somewhere in 2005 something like
this 2004 and then these small details
start to pop up that the original
language designers didn't really think
about let's see yeah so things coming
from the virtual machine from their part
is basically things coming from k QE
pole or select or whatever your
operating system fancies wait for
multiple objects if you're running
Windows and there's lots of things to be
said about if you want to use k q or e
pole and the different ways I have some
things that one important thing for the
airline programmers in the crowd to
realize is that there's a when consuming
al one of the things that what's acted
in order to scan their thing is that
he wrote this last statement here to
scale on more than one scheduler so one
scheduler at the time can only check for
all of the input-output events that are
happening in the entire system and this
becomes a bottleneck if you have 23
million connections and packages being
sent back and forth and whatnot so it's
a scaling thing that we intend to fix in
the future whenever we get the time so
that's that's talking about ports and
talking about timers so timers a is
basically you have three primitives that
you can use to send the timer you can do
a long send after which means I want to
get this message after five seconds or
something like that and lots of
different other things things that
update time in the virtual machine is a
core two now which is basically telling
you ok what's the clock now what clock
is another way set timer and we also
check the time when we turn from check I
also progressing time in the system and
keeping time coherent between the
different schedulers is quite hard and
quite expensive I on a normal dialing
system you do all this gets time of day
function somewhere during your four or
five times per millisecond so it's
keeping track of these things is very
expensive we the implementation behind
the timers is a wheel with which we put
all of the things so basically an array
where you put the timers and then we
have a non number of times that time has
to increment so if you say that you have
a time with us five seconds and you want
to put something in 15 seconds in the
future they put it in at the five second
slot and with a number three and then
when it reaches zero we triggered a
timer so deciding what to do is
scheduling in these things so this is a
basic pseudocode of the main scheduling
loop of the island virtual machine so
the first thing we do is check if any
time we should be triggered then we
might check if we need to rebalance
something so if we want to move some
processes from one scheduler to another
these things and so the maybe check
balance there is the decision if we
should move stuff not the action moving
of the stuff later on there in momentum
in the third day and today we do
actually the migration of the processes
between different schedulers we execute
any auxiliary work so this is emulator
internal stuff we might check if this
new iron were available if all of the
i/o has been consumed before we execute
one port for two thousand reductions and
then we execute a process that is those
reductions and then we look on and on
and on and on until we run out of work
and when we run out of work we go to
sneak now load balancing here is a
tricky issue because load balancing what
you want to achieve is that you want to
have a good load on your processing
while not doing any processing to figure
it out most of the fancy cool algorithms
that we can think of oh but if I know
these things it's not be too expensive
to actually collect the data than to
what you gain from doing it and so the
main mechanisms for along is task
stealing so toss feeling basically works
like this we put a lock on ourselves
preventing ourselves from being tossed
stolen from before from something else
are we move we take the toss from the
scheduler that's one number of others
and here we see one job is consumed and
then it tries to take that one and says
no not possible because we have a lock
on ourselves it tries to make take the
next one works with that that's fine and
then we put a look at ourselves that
work is consumed there's no work there
so we take the high-priority work of the
first scheduler and move it over to our
on cue and so on this look very split is
in place because we don't want things
being this is the 13th animations say
they're so these looks being put in
place there because we don't want thing
to schedules to be stealing from each
other all the time this will be moving
the processors way too much in between
the lock is removed when some kind of
work happens on the scheduler that has
not been stolen so if somebody sends a
message to a process on that scheduler
it gets put back into run queue and then
executed over them let's see so the task
stealing is basically in the basic
variation and then we have something
we've got load balancing so migration
logic as well it looks at it lots of
different factors and we try to move
loads so that we compact it to as few
schedules as possible when we do that
because of cash constraints and also
because we want to be green and nice and
be able to shut down the course that we
are not using so if you're loading
loading your system at twenty-five
percent capacity and you have eight
course then we strive to make it so that
your work lands on only the two cores
that it actually needs running out of
time here so I have to you can influence
this migration of compacting things you
can shut it off if you want to if you
want to have a nice talk we're all of
the things are equally balanced then it
just does task stealing and toss
stealing is amazing at the distributing
load let's see I'm going to keep the
sink threads profiling is inherently
extremely hard I don't think we'll
figure it out exactly how to do it
properly you need to benchmark
applications and see what happens and
have lots and lots of text printouts to
figure out what's happening and the
virtual machine program uses something
provides some alternatives like we can
do not counting to see how how the new
Texas and we'd right clocks and searches
in the emulator behaves and you also can
use operating system tools like car a
low profile g prof tamara things for
that and yeah too much slides
so this is supposed to be a 40 minute
presentation but it's not so we have
questions here so that's my presentation
about these times it's very basic the if
you want to check the code it's not that
big and it's not that complicated so you
basically go and really those functions
there in the Allen code i think the
check balance thing is like 150 c9's
schedule is about 200 c lines so not
that much and we strive to write nice
code any questions about I always prefer
gathering it together because you get
things like Cassius being in the same
place you get memory of locality
especially if you're working your place
where you have a new market Ector so
non-uniform memory access where you have
two processors that have some kind of
interconnects and it's always better to
compact these things like a very only
eight or yeah if it runs alone doesn't
communicate with everything else then
that would be a problem but normally
another process always communicates with
the outside with something else somehow
and then it makes sense to group them
together the performance difference is
not all that big the main difference is
the possibility to shut down course so
that you actually save the power
consumption of having those online you
know my opinion performance wise it's
not that big of a difference unless you
start really doing complex Numa stuff
like the tie layer awards and so on they
then it becomes very expensive as each
controller try to try to steal us well
yeah so whenever a run queue becomes
empty it tries to steal something and if
it fails to steal something it will go
to sleep can you do if you were what
happens to performance I did not know
that much memory becomes a bigger
problem you have to configure your
system so that you know exactly which
because when you have a hundred course
you have lots of different normally you
have at least eight or even more
processors and you have lots of
different memory architecture so
localization and different things
becomes very much more difficult but it
also affects the online programming
quite a bit there because you have to
realize that if you have a single point
which everybody is accessing you would
have a problem no matter what magic
solutions we come up with so if you have
a single for the anatomy of ETS tables
where you store things which is global
term storage and if you have a single
point where everybody's accessing that
then you would have problems no matter
what you do scheduling do this girl just
literally yeah I haven't there of course
there's a small drop off but there's
nothing major that I've seen in the
different parts it depends a lot about
the memory architecture on the telly
airports we don't have that much success
because of the way that their caches a
bit done they have like really really
tiny even l3 caches which is a problem
for us we like our cash space this
context which it is not about context
switching is not very expensive now
minutes when we when you switch context
basically you get the airline runs in
something called process name so you
have the alum code running interpreting
interpreting interpreting and then once
it realizes that now I'm out of work or
I'm out of this thing then you call a
function and then you check for the next
one and that gets swapped in so it's not
very expensive but of course if you if
you want to do benchmark something and
you have a single process running it
would be
much faster if you don't have to care
about these things but in a real world
scenario you have money many multiple
entities running in the same place so
the kardashians years ago the other was
not so fast faster and this wednesday
yeah we've tried playing with it but it
doesn't really matter all that much what
the value is and such you want the
responsiveness and ask the hardware
gross you want the latency to your
hardware becomes faster you expect your
latency to decrease and in order for
your latest decrease we have to run
shorter time slices as we go along we've
been trying to see if we can do some
kind of time based scheduling instead
but that becomes so extremely hardware
dependent rather than having these
balances you have to really really
implement these mechanisms for each kind
of cpu and chipset and so on and that's
too much work for us at the moment to
help make degrees with options if you
run this you automatically know but the
dinner is the NIF codes called something
eel if consumed time slice something
something but if you're talking about
nifs and these things you should really
check to work that Steve in Oscars doing
with Dirty schedulers that's in the
latest release candidate that should
have been released yesterday I don't I
haven't seen the email yet but it should
have been push the gate club yesterday
there's 17.0 release kind of it two of
our line so let's say it would be part
of the 17.0 release these things and it
will hopefully help some of the problems
people have had with running naked code
inside of the emulator loop right thank
you thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>