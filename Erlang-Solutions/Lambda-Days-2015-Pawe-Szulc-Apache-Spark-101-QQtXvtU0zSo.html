<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Lambda Days 2015 - Paweł Szulc - Apache Spark 101 | Coder Coacher - Coaching Coders</title><meta content="Lambda Days 2015 - Paweł Szulc - Apache Spark 101 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Lambda Days 2015 - Paweł Szulc - Apache Spark 101</b></h2><h5 class="post__date">2015-04-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/QQtXvtU0zSo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so hello everyone I don't know how many
of you guys have been in the other room
with Neil and John presentation yeah so
he stole o of my jokes not many in this
presentation but there are gone so sorry
for that this will be a set presentation
alright so yeah Apache spark 101 and the
clicker is not working awesome my email
if you fear to ask questions and just
write a mean email if you if you guys
tweet then please include me so I know
your opinions about this presentation
the thing with this presentation is that
it was originally designed for one one
hour and a half and I have to squeeze it
into 50 minutes so I'll ask you guys
some questions like whether you know
whether you understand my videos or not
I will not but anticipate answers just
like yes or no so I know what I can skip
through some plus and slides or not
alright so the very first question here
why are we here what's the point of
being in this room and you guys will
tell me spark but I know spark as a tool
why do I need this tool and the answer
is one word Big Data yeah one of those
jokes because can you guys tell me what
actually Big Data is the very definition
password yeah but the one I truly find
well I find one on Twitter and I really
like it and basically the the definition
is like big data this actually like
teenage sucks
everyone talks about it nobody really
knows how to do it everyone thinks
everyone else is doing it
so everybody claims they're doing it as
well
so that yeah so so yeah that's actually
problem of big data because if you want
to be a cool kid on the block you have
to be doing big data but eventually we
are doing big data in our industry and
if you're doing big data then you need
some tools to actually run it and not go
insane and four but but if you want to
find the very definition of big data and
while doing tooling for it then I think
the definition of Big Data is basically
Big Data that's it that's all about data
if we all right now we are currently
creating mass amount of information for
me I don't even understand this number I
mean like come on what doesn't mean ah I
like this guy who said that we that
right now we every two days we create
more information that we've created from
the beginning of humankind till the year
2003 that's like insane how is that even
possible
how is like within two days we just
created more information that the whole
humankind created from the very
beginning all the answers about this
right but nevertheless it doesn't matter
how do we create big data the thing as
we have to somehow deal with the big
data because the challenges that we will
have our new when when you have when you
have a cluster of computers and you have
data that you are possibly kind of fit
into one computer you have to switch
somehow your mindset to deal with those
problems it there's no such situation
right now that for example your computer
will well my computer my databases and
working sorry so it's a death of problem
now right now you have classes of like
hundreds of computer at just a normal
situation like one of the notes is
rebooting or not working or there's a
latency problem in the network whatever
so the tool said that you need has to
deal with all those kinds of challenges
but it's not a new problem big data it's
not suddenly that we have to go reactive
but since yesterday it's not a new
concept and there were tools before ten
years ago we were introduced to
MapReduce our
research paper from guides by Google so
how many of you guys are familiar with
MapReduce they have to explain all right
most of you guys that's cool because
we'll be able and fortunately my
computer isn't isn't compatible with
this with this thing here so my video
with my demo is not working on this Mac
so I'll have to probably improvise but
we'll see nevertheless MapReduce the
basic the finish I'll just quickly go
through the basic definition but you
guys are pretty much aware of it so we
have two phases of sequences of map and
release I probably don't have to explain
what map and radius is on the lam
today's conference hopefully right all
right I hope yes the problems just you
know it's a slide for people who
actually didn't know what buy produce is
well actually with with implementation
in Hadoop so its runs on key value pairs
you know oh you know you don't have
collection of values it's always a pair
like a key and a value and your operator
on those alright so the work count which
is the very hello world of big data we
have our input files with lines of text
and we want to count the occurrences of
each word within the our input input
file I feel like a star thank ya alright
so how it works in my videos you have
your input data here then what you will
be doing is splitting them data
partitioning it throughout the cluster
so be you know data goes through all to
all the notes within your cluster then
there will go the mapping phase in our
example what we essentially doing is
we're taking each line we split that
line into our words and we are emitting
a path where the key is
our word is a key and the value simply 1
there comes another phase called
shuffling this is the most expensive
part of the whole cycle because what
essentially happens is that all data
travels through all the network around
the cluster putting all
values with the same key into the same
note so that does that the more if
you're running like petabytes of data
that's the most important crucial part
which will well if not done properly
will you have performance issues and at
the very end you have this reduced face
and our example is like simply taking
all values and signing it up and at the
end we have a final result is that
comprehensible yeah so quite simple and
this is a code taken from Wikipedia so
as you can see I'm taking document for
each word in that document I omit a pair
with the word and the value one and I
reduce probably not the best example for
guys who are doing functional
programming but it works right it works
we have we just simply add all those
values 1 and and we have our final
result so so the question again is why
are we here it works right yeah we have
heard of like nine years MapReduce was
introduced sense years ago so it's a
decade of working make mature framework
so white house Park alright so
apparently that problems with MapReduce
and we've had it in particular the very
first problem is difficult programming
model and you should be like what now
this is easy right I mean I know it's
not the best functional curve that could
be probably there could be some
optimization here but it's easy to read
the only problem is let's observe the
code from Wikipedia but if you google
word example in Hadoop you will run with
this and that's not the whole code it
actually finished somewhere somewhere
here all right
that's not writable at all second thing
is so sorry
so how to try to address that problem
but I said well let's take a declarative
language sequel and we just give our
programmers the declarative language
that they understand they will just
simply tell us what they want to achieve
and we will give them the equivalent
running MapReduce faces and everything
will should work just fine the only
problem is that even the simplest query
that you will do in hive which is like
count distinct occurrences within your
data set will give you two phases of
MapReduce and if you know anything about
it you know that shuffling operation is
very expensive each space switching to
another phase is the most important time
of a most expensive part in the whole
computing so but I say okay that's not a
problem right because it's a tank
nine years of Hadoop running in our
production code so we have already books
like best practices and cookbooks and
things like that and you will learn that
that's that action at a problem because
you can do something like that and you
end up with like just one face of map
and reuse but at this point do we still
have the declarative declarative 'ti
because if I have to run my sequel query
and every time I have to see what
actually is running within my head of
cluster that I would just rather right
ahead look where I had a program and
that's it all right so another problem
there's that actually Hadoop and
MapReduce suffers performance issues and
you guys should be like wow that's an
oxymoron right
Hadoop performance issue no it's blamed
fast apparently it isn't so there are
many problems with with Hadoop the main
the main problem is Hadoop each time you
do you will do MapReduce face all your
output data will be saved on the disk it
goes all to i/o even though even if you
go to even if you go and run the another
computation with the data it will be
saved within your cluster that's slow if
you have if you have a batch processing
algorithm like go you read your data you
create some report for your business and
that's it then you'll be fine but if you
have algorithms that have to go on and
on with data and you have to read them
every time that's also a pain and also
the API the key value pairs makes the
simplest queries like joints are pain as
well
for in point of the performance and the
last but not least we are at the
reactive de reacted with conference
right we know that everything now has to
be real-time almost real time with with
bad it was bad processing with Hadoop
well you can you can for example choose
let's say Apache stone but essentially
you will have your batch processing done
in Hadoop you'll have your real-time
processing in a fatty storm and backs
can happen in this code and then that
code and maintenance might be a help so
you have to be aware of that alright so
I just gave you some set of problems
with my previous now spark to the rescue
alright so the very first problem was
not intuitive programming language so I
introduced you see an intuitive
programming model all right so word
count one more time we will look at work
out right now from the point of Scala
developer and I come from the Java load
I'm actually developing Scala for only
half the year of my professional
experience of only half the year so like
that oh my god this is a Java devil okay
throw tomatoes happy no sorry so this
example might not might be the best so
if you guys see any any optimizations in
that goal please tell me but actually
that's it that's not the point of this
slide you will see in a moment why so
let's do a word count in Scala so I'll
take my iterator of lines
I will map that into lower cases I will
split each word to have collection of
those words I will group by back my word
and at the very where's mine oh no oh no
please do work yeah so at the very end I
will I will map that into like my word
and occurrences of that word now let's
see how it how it works in spark so so
that over here was just simply a code
code in Scala what we'll be doing right
now will be seeing an API of spark in
scar just that right right right now you
don't have to go into details then don't
try to understand everything just try to
feel the mode of programming in this
in this API so the same thing happens we
collect our text file the correctly some
kind of interface to that some kind of
abstraction to the file we will come on
we will we will lower case that each
line we will split each word we will
group by by this word and at the end we
will return a pair of work and and the
size and just a quick quick info for
guys who are actually developing stuff
in in spark they know that using group
is not the best thing to do I could tell
you later on why but but then again you
have an API you can it basically looks
exactly the same as the code you'd
rewrite in Scala without using spark so
I hope well this is easier than what I
showed you in Java I hope yeah all right
so I introduced problems with Hadoop
I'll try to show you how spy actually
try to over work around those problems
so performance boost imagine imagine
that we have a cluster with with free
notes that let that be our presentation
for our cluster and we would like to do
some calculations on the data that we
have it will be like some kind of bat
for testing right now so I want to do
some mapping maybe later on I will group
by my data by some key then I will do
mapping into one more time I'll reduce
by key and I will have my my output my
calculation so what part actually does
it sees that well we have some
calculations on our data that data is
already partitioned within the cluster
so it can take that calculations over
here and just start pipelining running
them on that very first node without
actually other nodes even starting up
and it will deal with you know each but
it will actually run round the same the
same curve over here with each note in
parallel because at this point those
guys do not need to talk to each other
at this point they are just run in
parallel and they it doesn't matter if
the other guy is winning or not I don't
care but there is a moment before group
by that we didn't need to have all the
data mapped to actually group by you
have to wait here at this point to all
the data to finish the calculation on
each of the note and then again you have
a another set of tasks those guys can
run in parallel as well so in spark
those phases are called stages you have
stage one and we have stage two and you
can essentially think about stage as the
super calculation so it takes you a set
of your calculations spark will spark
scheduler will see ok I can actually run
it in parallel within the cluster and
and I will just run those calculations
wait for results and then I either push
the results to another stage or just
return results to your program and you
guys might think well that's MapReduce
right I mean phases it's exactly the
same well it is it first of all you have
to remember that you you can do whatever
programming thing you want here as long
as you are able to pipe and so map
filter all that stuff you can do as long
as you get as you can as long you can
pipeline those things within the one
node and there's the one another issue
that you should be aware of imagine
again our example we we have some our
initial input we did some mapping some
filtering and at this point we group by
some key our data is partition ours and
spark knows by which key it holds that
information over here if we introduce
another set of data and we will do draw
between those two data sets what will
happen well first of all it will be
joined by the same key it will be joined
by the same key that we did actually
grew by over here so well at that point
shuffling mechanism has to start it's
essentially to to have all the data
within the same no you need to shuffle
or data from other other nodes to you
know all keys from those go here here
and here but but that data over here is
already partitioned SPARC is aware of
how the how the how the grouping by went
so it will be not doing shuffling it
will work just like this actually it
will work other way around this guy over
here will be doing the joint will see ok
this is already partitioned so I'll just
put all the data according to how the
this data say this partition and that's
actually is one of the biggest boost
performance in spark that what kicks the
performance in spark you might hear that
caching mechanism is that one is
actually giving spark it its boost
that's why spark is so fast well it
isn't it's one of the mechanisms but
it's not the most important one did you
get just give me a second I have a sore
throat and I was drinking beer yesterday
that was so stupid so sorry for that I
blame some of guys who sitting here it's
their fault alright so let's imagine a
simple algorithm
I like PageRank the guys know PageRank
that's a that's this picture is taken
from Wikipedia I like this picture I
like this one
I will not go into the algorithm because
I hope most of you guys know but for
those guys who don't know how pension
works basically it gives you ability to
rank whatever graph you can think of of
the graph you know notes addresses we
say that like pages and it works in a
way that I'm more important
within the graph if more guys I'm
pointing at me and if some really
important guy is pointing at me then at
the same time I'm also important so to
actually calculate PageRank you have to
go through each of the note within the
cluster do some calculations with when
with using some calculations defined in
imperfect pay track algorithm but you
have to do it sequentially some number
of times there's a like to stop stop
point where you stop but eventually have
to do some few few loops within the
algorithm so you have to read the data
do some calculations then again read the
data do the same calculations and
eventually stop so spark introduce a
caching algorithm which allows you so if
you if you load your data within your
cluster from HDFS files or whatever from
your database doesn't matter you have
ability to actually persist that data
within your memory so you eat so you
load your data for the very first time
in the PageRank algorithm you persist
that data within within the memory of
your cluster when the memories of each
node of your cluster working notes and
then if you rerun the calculations one
more time this time they will be not
read from the input source like database
or HDFS file they will be already within
your cluster and already in the memory
so this gives a kick that's actually
taken from a bunch of spark web page and
it's running PageRank algorithm so you
see that you know hello is like okay I
will just start this algorithm and it's
like I'm dumb you know I'm going for a
beer so this this really kicks off if
you have those kinds of algorithms but
if you go if you go to forums like
Hadoop forum posts like which are right
now maybe one year old you will see
those guys asking but on the Hadoop
forum listen guys there's this Apache
spark suddenly everything is cool about
about to spark there guys know
everything
thing about it and those guys will tell
you how patches spark is only when you
have data that will fit into memory if
you not fit into the memory it will not
work well this isn't true the caching
mechanism first of all it can be can
cache it might be configured to cache
some amount of data into your memory and
also go away England
it might cash some part of your of your
data into the memory and also it only
gives you bush if you're doing recurring
algorithms if you're doing patching that
you're not even using this
dysfunctionality at all but just to
prove a point guys from apache spark
they started they run into this sort
benchmark it's a district benchmark that
basically you have to sort data it's 100
Telep 100 terabytes of data one trillion
records it doesn't state how many
machines you have to use it doesn't
state what kind of machines basically
the result is the fastest the better and
in 2000 I believe in 2015 the record was
in Yahoo cluster 2100 notes and 72
minutes so the guys also started within
the smelt remark and the results were
like this 200 notes and 23 minutes so 3
times faster and 10 times lower the
machines but the most important part
without the cache mechanism working they
were simply turned off because when you
so date and you can't just have to run
once for your three-hour go a few for
your data the cache mechanism is
actually not important the thing that
works the thing that actually kicked the
algorithm was the example the ones I
described you before also the cool thing
about this about this benchmark is that
actually there were the very first guys
who actually ran it in the cloud so
you can you can go to apache spark
webpage you can actually take the code
if you have enough money for Amazon you
can run a cluster of 206 notes and rerun
the benchmark and see okay those guys
were right because I can do the same
thing all right how much time they have
and the last part I told you about well
being reactive right so with a batch of
spark as you probably some of you guys
so in the previous in the previous talk
there's a separate module called stream
a box for streaming it allows you to do
almost real-time calculations because
it's not real time in terms of Apache
storm but it's almost real time meaning
that it will it will get values from any
kind of stream that you will have you
will build a small window while you try
to read data from that input and you
will do a micro batching so it gives you
a notion of real time but the cool thing
about it is that I can actually take the
code like 90% code that you had for your
batch processing and reuse it for stream
processing so you have one single
codebase the things that will change is
actually how you will acquire the data
and what we will be doing with the data
once the computation is finished but the
very main core is the same stays the
same and I'll fight hi if I have time I
will I will show you the details in a
second alright so I finished
quarter-past sorry quadruple right so
all right so how it how it works so the
very big picture you imagine that we
have a cluster and apache spark will
give you its own cluster which is
actually very cool if you start having
fun with it because instantiating a
cluster is like simply easy it's it's
not the same pain as you had with hadoo
but you can also write if you have
already a Hadoop young young cluster or
if you have messes you lucky bastard
then then you are able also to to run
spark on on that
but let's imagine it's a cluster and it
simply doesn't matter which one all
right we have another driver program
driver program is you know yo you're a
common line tool you're your web
application whatever the thing that you
are developing for your clients this is
where you actually be running
calculations sending them to Apache
spark cluster to do them for you we have
an API which gives us API in three
different languages Scala and Python as
a Java developer previous Java developer
I might say oh my god this is ugly I
mean really ugly but enough doesn't
matter I was complaining of leg morris
yesterday and now I'm building flame
soon but nevertheless you have an API
for three different languages so it
gives you ability to choose whatever
language you like alright within the
class so you have a master node master
node was the note that you will be
talking to if we if what to actually
connect in the cluster we need to have a
URI which is like essentially a host and
port we need to pass that URI to a
configuration which takes different
other some other stuff but the set
master method is the most important one
and then you will create something
called spark context which represent you
probably see it but there's a line over
here represent this line it's a
connection with a cluster with with
master and you also have other notes
which are called executives those are
actually those guys who will be doing
the work for you so what I will do right
now is I will read some file locks txt
and the important thing is that this
file needs to be present at each of the
executors it doesn't matter if you use
HDFS or Glasser FS so any distributed
file system you might be using local
local file system but at that point you
are responsible for having file on each
of those notes and and then we'll be
doing our
relations so the simplest calculations
is count what I want to do is count my
locks so what will spark will do will
create those tasks I show you so it will
partition our my data the log files at
this point it will be partitioned into
three partitions it will take my
calculations which is simply at this
point just count the lines and it will
run it so it goes to executor
each receive this task which telling him
this is the calculation and run it on
this part of the file and the other one
will have the same calculation but on
the other set on the other part of the
file so the calculation runs we have our
partial results they go back to master
and the master will just basically
finalize the result or rerun rerun the
calculations for another stage whatever
the cool thing is that if you have this
scenario and you run your tasks and they
go into executives and one of your
executives dies but you had already some
calculations that we're building we
calculated then master will actually
realize okay this guy is dead I don't
have t3 and it will essentially
recalculate only t3 it will go back to
your your your result it will not rerun
the whole calculations there are other
optimization that you can do for example
master might eventually see that one of
the executives is really slow so the
first two were like running like sprint
fast and the second act was like well I
have time right so it will for example
stop sending tasks to that note seeing
that well this one is performing really
really slowly alright so if you got if
you're going to go to the SPARC webpage
you will read all about LED so just
quickly just do you guys know what led
is think of RTD as a as obstruction of
collection of data in your cluster so
you have a cluster of all those nodes
running your computations and you have
your data in that cluster
okay so
sensation of that data the abstraction
is the LED so just a quick example if I
read some file from HDFS using that
method to spark context text file the
thing I have here this Lux is LED this
is editing
so it's an abstraction of file that is
already partitioned within the HDFS is
that okay you guys get it
cool awesome so as I said you can you
can read from local files as well you
can actually paralyze your collection so
if you already have some data in your
driver program you know some collection
that you read whatever you can also put
it into your cluster using paralyze
method alright so let's do something
with our locks so if I refrain right now
if I lower case all lines within my
locks operation over here that this
operation map here will return yet
another LED so that's that already is
immutable it will be recalculated every
time you some blue something on it but
if you do something good it will return
you a new LED so if I do a filter with
one for example look only for for errors
within my locks then you will have yet
another LED then again somebody might
ask what about performance well
basically all those operations as you
see over here are lazy so at this very
point my driver program is not talking
to a bunch of spark at all
there's no point right there's actually
two kinds of operations transformations
would give you lazy you just you're just
saying what you want to do without
actually doing it and actual actions who
actually run those calculations so as
you will see in this example if Iran for
example count then this will trigger
spark and give me give me the number
that I won't have demo I will not show
you sorry because for some reasons my my
computer set go away and this does
because I
had demo actually actually I had demo
recorded so I thought I would be able to
play it on Mac but no for some reason so
if you guys interested in the demo I
will try to put some somewhere on the
side mystic maybe you can download the
demo but essentially what I try to show
you so maybe then I will just tell you
guys two important things from the demo
all right that you will actually see in
the demo the very first thing if you
what I can encourage you guys to do if
you're really interested in spark the
entering point is really low because
yeah all you have to do if you is to
download spark the very latest release
extract the file on your file system and
that's it
because within that within the bin
folder you will find a script called
spark shell which is simply repo but
it's already with with important
imported files from apache spark api and
will also a does it instantiate a
cluster for you a local cluster so as
you guys remember I showed you actually
you connect to a cluster you have to
provide the URI host and port what you
can actually do is put localhost instead
of the URI and spark will instantiate um
like it will not be a cluster it will be
a mutation of a classic because it will
run in one JVM process that's actually
our fingers not documented it's spark
it's called local local cluster local
slash cluster which emulate real cluster
but so for some reasons they didn't put
it in the documentation I read they said
we didn't want to confuse developers I
was like I'm confused that you actually
didn't put that in documentation but
whatever the thing is that you can
download spark you can open repo and and
you can start doing all the stuff I just
showed you right now so entering point
is really really small you can you can
later on then create your very first on
cluster or just download docker file so
I'll sick files or whatever quantization
or virtualization that you want and have
fun on the real environment but
for the very you know just to see how
how api's are working
that's simply yes yeah yep
alright so I had um I have like ten more
minutes but for those guys who who are
not on the previous example I'll just
simply show because there's a what are
you actually talking about right now
where's apache spark core this module
over here at the top of that module
grants set of different modules sparks
equal streaming amoled and rocks this so
simply no time to evaluate all of them I
thought I'll be doing spark streaming
that was a mistake
but the most important part is each of
those if you for example have a spark
sequel module and you will you will read
some data from your structure input
source like database for JSON files or
perkier files and and you will do
whatever queries using sequel that you
would like to do the result of those
calculations will be RDD so you can you
can read some data from the database you
can do some query on it but the result
will be abstraction of data within the
cluster so you can take that led just
some stuff using your normal API over
here and for example come back to
whatever know this module because this
is what will also understand LED so
that's really awesome and the other
awesome part is that if they optimize
this stuff this stuff also is optimized
out of the box so so yeah so for guys
who haven't seen spike streaming in a
previous session I'll just quickly show
you that yeah also without them or just
some slides so I I will not be telling
you why you need streaming because you
guys probably know why how it works you
receive input data from from your from
your stream spark will collect the data
but it actually
open a window within there like one one
second window it will read that data
that is collected from the stream and
we'll be doing the very same operations
that you see in the spark or module but
just on this very small amount of data
so this is the that's what I'm saying
this microbe etching because if each of
those small batch will be essentially
calculated the very with all the same
algorithms that you saw in the in the
core module and once the calculations
are done then results will be also sent
back as a stream so just to show you an
example what you have to do is you
create and create something called
string context you also provide windows
so for example read within one second
there's there's a set of operations that
allow you to read data from the stream
one of those operations among many
soccer text stream so it will reach from
from a server on a given port whatever
comes from that server as a result you
just seem use you receive something
called this stream which is like like
led in this spark streaming module and
you can do all the calculations that you
see before you have every almost every
transformation that you saw on the LED
almost every transformation is on there
on this this stream as well so I can
flat map give you a word with with one
and reduced by key and I have my word
example running I think within the
streaming API and as you guys see before
you can I have to actually instant
instantiate this thing this thing here
to have this actually working because
without it you just simply said what do
you like to do you have to start it to
just start reading the stream yes so I
have like five more minutes I can take
questions
well how do I test my application
depends what you want to test is it like
we'd like to test the result where the
calculations are correct well no you run
into the same problems that you haven't
you know any distributed system so what
you can do if you want to test your
logic then you can simply write unit
tests that will run on this local
cluster and you know you can do it on a
small version of data if you want to see
whether your your solution scales then
you have to create a cluster and see how
it performs when you add more data to it
or mote more notes to it
SPARC actually gives a lot of tooling to
see how it's performing you can see how
the data was petitioned you can see how
much time it took to actually how much
it spent in the GC garbage collection if
you can see how much data each stage
took in and how much data was actually
written to i/o so you have all those
metrics metrics that you can look at
when developing a solution but you know
the problems like what will happen if I
have more load of data you have to deal
with them essentially the same way you
deal with any distributed system
Vanakkam production
okay yeah so you have this caching
mechanism which allows you to cache the
data within the ad but default
configuration is to actually say that
all that stuff in memory but you can use
for example you can divide it and save
all the data on disk you might just say
save that data but please save it on the
desk and it will be saved on each note
that that part of data that each node
was actually doing a calculation son so
you have a ability to choose whether to
be instead of being forced to the thing
is that if you have a problem on on on
led it will be if you're editing fails
then you don't have to worry about it
because spark will recalculate all that
stuff for it that's why it is called
resilient but yeah so that's why you
have this caching mechanism but the cool
thing is that actually you don't have to
save it to disk because if it's data is
small enough to be saved on your in your
memory instead of i/o then it will be
blaze fast yeah there's a method called
distinct yeah exactly so but you still
have to read all the data right
essentially when you run when you run
your calculations the theta has to be
divided into the inter cluster they has
to run through and and and look for
distinct values
yeah so essentially that I'll just give
one advantage of you guys playing with
spark
read the API there's like 30 methods
it's actually nice to know all those
methods because don't do my mistakes
I read 10 and I thought that's enough
what happened is that other 20 where the
actually ones I should read about so
yeah if you guys go into if you guys
like to play with spark or try to go
with reduction know your API because
there are some methods that will do some
stuff better for you the thing is sorry
the thing is I I don't have experience
so I cannot give you the full answer I
don't believe in golden hammers I think
for example if you have like vast amount
of data flying in and your your issue is
your how fast you are and you don't care
about batch batch computing then maybe I
would switch into a party store instead
of a purchase park because it you have
to remember it not always give you the
best performance it gives you the
unified API and you can connect with em
lab with machine learning with the spark
or things like that so that's it there's
always a trade-off right the spark gives
you this unification and it's actually a
good thing if for some set of problems
maybe for majority of problems but not
for all so I would say answer your
questions like I don't know I don't know
if Hadoop but for example Apache stone I
would consider actually thinking about
it if if if you have many streams
actually connected many times to you to
your to your system and you have to be
very performant
with hive which is essentially
functional again this one's the same
problem with no because it's done better
now actually if you if you guys they did
something they did sound a research it's
actually it's a it's another good thing
because it's tells something about our
industry but they did some research and
they were able to do better calculations
within Apache sequel aspects equal then
when they gave core API to developers to
around the to run the calculation
without using without using SPICE ago so
they say they they rethink because okay
so Apache spark at the very first moment
when they introduced the tool to
industry they need like ninety percent
of companies that they're saying that
they're using Hadoop then actually using
Hadoop using hive so they in you if they
need to show the performance boost they
have to have some ability to actually
run - where it's honest box system so
what they introduced was a spark shark
which was like they did some workarounds
around Taif gave like a small
abstraction around it and called it
shark and it performed better than hive
but still it was crappy
so after initial phase they rewritten
the whole thing taking into
consideration that they are doing this
for spark and they put some optimization
into that code it performs better simply
as
now the problem is not the problem of
MapReduce is essentially you are stuck
into a map and reduce those phases and
that's it with with with spark you can
actually you can think a little bit
about it I will give you an example for
example imagine like group by method
what you would like to do for example is
you have your data you want to group by
some key whatever you have your you have
your users they have some ID and you
want to maybe that some users maybe some
logs or whatever and you want to group
the collection of data with the given
key having that group you just simply
want to reduce okay so flatten all the
data so at the end you will have your
key and reduce the value all right
and you have the API to actually do
goodbyes and then reduce okay but what
will essentially do to group by you have
to shuffle because all the data will
flow through the cluster and on each
cluster you have each data for each node
and it will be reduced locally but this
face over here has to put all the data
all your 1 million 1 trillion values
through the cluster just to be reduced
and and output to the client there's
also another method called aggregate by
key
what will it what it will do it will
first reduce data on each node in
parallel so you have reducing being done
on each node and which will create a
partial results and then it will sell
partial results who to the notes so the
suffering will also happen but with a
lot of less data in it all right and
then the performance boost between this
so the first solution and the second is
enormous but so it happens that you have
a richer API with MapReduce you can you
can't do this you have to go with the
very first solution right so so that's
the probably that's why they can
outperform hive in the that's actually I
don't have it in slides I had previous
previous presentations but there's like
the same queries the same sequel queries
done in hive
sorry yeah so some of the methods within
the IP I gives the ability to
repartition to the data and that's also
a method called
with partition which basically
reproduces the data so yeah so now your
API and you'll be you'll be fine but
yeah you have to remember that we
partitioning data is again a shuffling
you can actually do it if you there's
another method I cannot remember how
it's called which will actually will not
will repartition without doing the
shuffling but only if you will be making
your partitions bigger not smaller if
you're doing a partition smaller than
the data will flow throughout the
cluster but sometimes it's better I saw
like a long spark Samak last year that's
just showing an example that they were
had they had this computation that was
really real slow and they did some
optimizations eventually they introduced
more stages because essentially there
were two stages at the end they had I
think four but the four were optimized
one of the very first stage was
repartition the thing that you are
asking and repartition allowed them to
actually speed the things up
more stages but it was quicker just
don't be shy ask me questions outside
I'll be happy to answer
a living thing so in the last row and</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>