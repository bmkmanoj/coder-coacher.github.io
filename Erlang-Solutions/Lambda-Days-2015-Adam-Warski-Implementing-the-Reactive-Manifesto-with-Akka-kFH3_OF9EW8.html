<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Lambda Days 2015 - Adam Warski - Implementing the Reactive Manifesto with Akka | Coder Coacher - Coaching Coders</title><meta content="Lambda Days 2015 - Adam Warski - Implementing the Reactive Manifesto with Akka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Lambda Days 2015 - Adam Warski - Implementing the Reactive Manifesto with Akka</b></h2><h5 class="post__date">2015-03-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kFH3_OF9EW8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so therefore trade is defined in
directed manifesto the first one and the
most important one is that your
application should be responsive
responsive meaning that it should
respond in a timely manner to use a
request if it is at all possible like if
not all of the servers are dead right so
this kind of builds user confidence in
your system it provides some quiet of
service and then there are three other
traits with which help you realize the
responsive trade right so if want to
have if we want our system to be
responsive we should build it so that it
is resilient resilient meaning if
there's an error it is properly
contained in two in one component data
is replicated so that we you if we lose
one replica of data we can use another
one errors should be isolated and
components should fail should be able to
fail independently without breaking down
the whole system okay another trade is
that a system should be elastic so if we
have a sudden increase in the number of
users and we should probably we should
react that accordingly so probably
increase the resources allocated toilet
system scale up our machines add new
hardware a hopeful automatically but
also on the other hand when the number
of users goes down we should decrease
the number of resources consumed and
finally it as the trade that our
applications should be message driven
and that's the most technical the most
technical side just start my timer and
so message driven the main point here is
that it components in our system should
communicate as synchronously okay and
and like if they communicate a
simplicity and they've then they
probably communicate using some form of
messaging okay and so when we have a
system which is message driven which
uses messaging between components to
provide commit to provide communication
then there are high hopes that or higher
hopes let's say that our components are
loosely coupled and that the exact
location of each component is
transparent to other components right so
these are all things that are like a
good to have in a system and it is kind
of easier to do when our system is
message driven and when we have
messaging there are some other things
that come into play like load management
flow control right when we have two
systems exchanging messages we don't
want to drown one system with messages
so that it died because it has to it has
to handle too many messages or it can't
fit the messages into memory right so we
need to somehow control how the messages
flow between the components and here we
also have the notion of back pressure so
we want systems which are overloaded to
somehow signal the other systems that
please stop sending me messages right so
these are the traits that a reactive
systems should have and that's like a
bit of it's well it's a bit of marketing
it's a bit of theory but let's see how
we can actually realize these these
concepts and so a small note like
indirect manifesto 10 it wasn't messages
revealed about event-driven so what's
the difference here and the difference
is that events are very often a business
a business concept business domain
concept right and also if you have
events in your system it doesn't imply
that they are processed are synchronous
right you can process event synchronous
lenders fine and with messaging you
usually have the awesome close
communication and that's what the
authors of the manifesto wanted to
emphasize that you have the synchronous
communication
and okay so let's direct manifesto in
like two minutes and just a short note
about me so that you know who's working
two day today I'm a scholar holder for
four years now I'm also the co-founder
of software well we are software house
spread around Poland and we do software
in Java in Scala and Java and I did a
couple of open source projects they are
all very interesting of course so you
should go check them out at all a plus i
have a blog and twitter so that's that's
about it and ok so to implement the
reactive a manifesto we are going to use
our car so if you go to the archives
website you can read that it's a toolkit
and runtime for building highly
concurrent distributed resilient message
driven hubs on the JVM right so that's
exactly what we need right it has all
the trades enumerated here yeah not
really not really no and so like the
core acha the core of vodka it contains
an implementation of lightweight actors
actors our entities which encapsulate
some possibly mutable state and we have
a guarantee actors basically the only
way of an actor could to communicate
with the outside world is by exchanging
messages so we have the assignments
message passing right out of the box
right and so the only way to communicate
is using messages and we have a
guarantee that each actor processes at
most one message at a time so the state
that is encapsulated by an actor is
protected from any concurrent access
right so let's like the basis of actors
but akka is much more there's like a lot
of other modules some of them like in
various degree of experiment charity
let's say and so there's the remoting
module it's not experimental so we are
not going to talk about it they were in
the remoting module a we can connect
actors which live on different nodes
over the network on top of that there's
the clustering module which we are going
to use
in it provides a cluster membership so I
cannot a note on which I currents can
form a cluster and then and they may
maintain you a uniform cluster view then
we have persistence that's a module
which is experimental but close to being
released I think and and using
persistence you can implement things
like event sourcing you can persist the
state of an actor into into on into
journals either on disk or in the
database like like Cassandra then
there's acta streams which is the newest
addition to the to the family it's used
for defining data processing pipelines
with builtin back pressure and we are
going to use that one as well and
finally this acha HTTP which is the
rebranded spray i oh if you know spray
it's a DSL for building west end point
seven eyes dsl passes it's an
implementation of the lower level of
actually parsing all the establishing
HTTP connections crossing all the
headers and so on okay so that's like an
over vodka and I with me Mallis
Morris okay so what are we going to do
so we are going to build a very simple
application the application is going to
process some data so i have a flight
data from the US from 2008 and it's like
all the takes off and landings from US
airports it's about like a couple of
hundred megs of data so it's not
something you process in a second but
also not like big data not like a
reasonable for a demo I hope and so we
have like all the text of an end and the
landings and what we want to calculate
is we want to look at flight so that's
like you can see that there's a lot of
data about each flight here right you
can see that there is a lot of fields
and so among other data we have a the
origin airport the destination airport
distance between the airports and and
the delay and we want to see what is the
flight which had the biggest labor mile
flow so like the proportionally biggest
delay okay and so we are going to have a
server and which will calculate which
will calculate that biggest delay and it
will calculate basing on data received
from remote clients which connect to the
server over TCP and stream stream the
CSV file streaming the data to the
server and the server will keep track of
what's the current a worst flight to be
on if you want to have like when
comparing the delay per mile and okay so
that's like the input file of course is
like 10 lines I'm not going to open a
500 mega fail in IntelliJ even though
it's a great ideas on 3rd grade and okay
so that's like what we are going to
parse so I have the model ready because
it's not very interesting
and so we have like a flight data case
class right which has the basic data and
from two different smiles delay minutes
and I have a way of if I have a post
line as CSV line right that's the
Parsees v-line I can turn it into a
flight data if it's if it contains the
fuse I'm interested in and then we have
a class which actually calculates from a
flight data what is the delay per mile
ok so let's like that's the calculation
over here it's a simple division so not
very complicated if there's no delay we
get enough so that we are going to use
that throughout throughout the demo and
ok so let's start writing some code and
i will start with the server so that's
going to be our server the receiver and
so the receiver is going to need some
stuff it's going to need the address to
on which we are going to listen for
client connections ok it's going to be a
circuit address and it's going to be is
going to need an implicit actor system
so that we can create some actors for
processing the data and ok we need the
method to actually run the thing and ok
so what we will do now is we want to
bind to the socket to to the address
right we want to listen on to client
connections and we want to receive a
stream of data from the clients ok but
we don't want to distress it nh3 we want
to receive error active stream of data
now what is their active stream it's
actually quite a simple thing are active
stream is like a data processing
pipeline okay in a pipe let me have a
number of stages each stage does
something to the data okay and and it is
reactive because data doesn't flow like
as it is available it only flows on
demand right so if if the a component
number 10 decides that it wants data its
it signals demands to the previous
components and the previous components
then signal them out to the previous
components and all the way to the data
source and the data source spits out
data only when it actually only when
there is actually demand for the data
okay and if it does so not one by one
but it can do like small batches of data
so i can say i want 100 elements so the
data then we'll travel in batches of up
to 100 elements okay and so each each
such a stream has three basic components
there is a data source which has no
inputs and one output of a data of a
given type there's a sink which has one
input and outputs okay and there is a
flaw which has exactly one input and one
output right and to create a stream we
of course in the source and any number
of flow elements in the middle anything
ok so i guess that's less quite regional
ok so what we are going to do is we are
going to so we want to stream over tcp
so we are going to use this stream tcp
extension for Marcus dreams so that is
going to create as a TCP stream of data
coming from TCP right in a reactive way
so we are only going to read from the
socket when there's actual demand ok
when there is no demand the socket
buffers will fill up and on the other
side is reactive as well the other side
will stop sending data so we have the
stream TCP we bind to the address given
if the other side is not reactive and it
keeps writing to the socket eventually
it it will out of memory right if it
treats from a file and tries to write on
the socket but the socket Buffett is the
socket buffer is full so the circle will
stop accepting in what mod a time that
will accumulate the data memory and run
out of memory and same here right if any
component is not reactive and we block
some some somewhere downstream we risk
an old memory and here if we have the
explicit demand bust we don't run into
such risks okay so and we bind we bind
our stream TCP extension and we get a
stream of connections again that's a
reactive stream of no no no data yet so
far we only have a rack to stream of of
connections why why is attractive well
if our server is overloaded we don't
want to accept no connections as well
right we only want to accept new
connections when there is actual
computing power to serve those requests
okay and so we want to that components
to be a back pressure as well okay so
now for each connection and we are going
to take a connection here okay we are
going to us to print line that we bound
to the progress bound to
address okay so now we have a connection
and from the connection we can obtain a
flow a flow is like the middle part of
the stream right and it represents the
client flow right and like the de client
if we you where you normally think of a
flow like from output from in apple
style but here we have to think the
other way if we receive something from
the client when we usually need to give
something back right the client sends
out some data requests and we respond
with some data okay so we take that flow
let's say flow from bites two bikes a
byte string is like a better byte array
so the client spits out bites okay so
what the first thing we need to do is we
want to transform that into actual
strings because we have raw bytes so we
added transform stage to a pipeline
power lines stage and we add a stage
which will actually parse the bites into
lines that's a buffer okay so now we
have a stream which spits out strings
okay the delimited by new lines so once
we have those strings we can now filter
out anything that doesn't start with 20
because there is there are some headers
here which we want to skip right we only
want the raw data right so we filter out
any strings that starts that do not
start with 20 and then we transform dead
strings into byte arrays right so we do
a simple map on the streets we have like
a lot of operations that you probably
know from scholar collections and so we
take each string and we split it by
commas so we get an area of strings now
we can use our
map concord we can use our model that we
have prepared earlier and right so we
create our data class which work which
represents a single flight so this
returns an option because the data can
be incorrect though so we turn the
option into a list that list you can
have 0 1 elements and there is no flat
map in streams but instead there is a
map conquered so it's not the name is
different because it's not like a
monadic flood map or it's a it is just a
map which con cuts all the elements of
the given list ok but if it behaves a
bit like a flat back and right so for
some elements we are going to have no
flight data for some we have we are we
are going to have one and okay so we
have a stream which outputs flight data
right now right so but we still have a
flow right we don't have a complete
stream we need a sink so let's let's
provide a thing for now we are just
going to do it for each thing so for
each flight data received we are going
to print line that file data and we are
going to present that we are really slow
in processing the data so we are going
to the extra sleep for now okay so that
I will flow and that's always saying
sorry why is it the thing okay so here
we have a sink right so we have the end
of the stream that's the thing that's
the flow right so if you connect a flow
to a sink we get something which has one
input and not output so that that's a
thing right so okay so we have this
thing we need a source so we need to
somehow respond to the client here we
never send any response to the client we
just accept data and process it so we
are going to use and lazy empty source
so that's a source which has no elements
we are going to connect it to our
receive sink okay and now we have a full
data processing pipeline and actually if
we do that there's no data is going to
be to be processed just yet because what
we have is just like a blueprint a
description of how want to process data
we still have to run it and to run it we
have to somehow materialize this
description of the our data processing
pipeline right we have a number of steps
k like we have a transform step we have
a further step we have a map step but
there's only like a description of how
our pipeline looks we have to material
materialize it so that the actual data
processing starts okay so we have to do
run okay and when we do run things
actually going to start being processed
now run has an implicit argument which
is a flow materializer which we have to
provide actor flow matter and there is
currently one a default implementation
of the flow materializer and it's actor
based so this flow materializer is being
used over here when we actually using
our blueprint when we actually run this
three okay and this of course is going
to use actors like each stage like this
over here is going to be mapped to an
actor and the actor is going to do the
actual work okay and it's all going to
be back pressured so since we are slowly
processing data here data from the
socket will be read only when there is
like sufficient demand in the hole in
the whole stream right so we are going
to read from the second twice every
second in effect and the materializer is
also used here that's like a shortcut
because here we have a stream of
connections right and here we have like
a so here we create a forage sink and a
run in in one in one method okay so
that's just a shortcut method but it
uses the same mechanism of materializing
the streams okay
so and so that's the that's the receiver
part and lets me just write an app which
I can actually run receiver extends up
implicit valve system we create a new
actor systems of it that we can actually
create actors new receiver you know we
need to fight the address local host to
182 dot run ok so let's run our server
hopefully it will run ok it's running
and now we need to the client we don't
have time to write the client from
scratch but it is like the same
principle over here in the client we
create a source which reads data from a
file okay and then we connect that
source so we don't we don't we don't do
a bind and we do an outgoing connection
here ok so we connect to the server in
the client but like all the concepts are
the same as in the server right so we
read data from a file it is read in a
reactive way so it's only we only read
from the file when there is demand and
that the men flows all the way from our
sink through the TCP connection through
these processing stages in the client to
the to the source that is in the in the
client ok so let's let's try running our
client and let's see if the server
receives anything over here yeah we are
we're seeing some data ok we are
receiving it a one every half second and
the sender didn't out of memory so even
though the file is huge it didn't get
read into memory all at once though you
can't see it right here ok so let's
that's our first step
so now now now we will continue to
actually doing something more
interesting with the data and yeah
mm-hmm what you don't see it but if you
are using the actor from at arise oh
yeah it's going to be mapped an actor
which were concurrently probably with
the other stages process the data okay
so now we have the receiver let's let's
create an actor which will process our
data and actually calculate the biggest
delay okay largest delay actor okay so
we are going to have an actor alright
and now we want our actor to be reactive
aware so we want our actor like normal
when you send messages an actor it
doesn't signal any demand give me more
idle I have enough right we need to make
our actor somehow aware of the fact that
whenever active system so either actor
to produce demand to give me more
messages right so we are going to use we
are going to mix in the actor subscriber
trade okay and okay so we will have a
variable that's our protected internal
actual state largest delay which is
going to be an option right with the
labor mile initially we don't have any
data so its nun and and we have a
receive method which we are going to use
to receive right depth method which is
involved when an actor receives a
message right so now when we receive a
new flight data
ok so the on the next is a rapper not a
difference in the on case yeah the on
the next is a rapper which is provided
by the octo subscriber so that's like if
you if we send a normal flight data to
our actor if we have the actor
subscriber mixed in our data is going to
be wrapped into an honor next right on
next data element that is coming from
the reactive street okay so when we get
new data we calculate the biggest delay
so that's a great data so if there is if
if the flight was delayed at all that's
why we do the for each we are going to
process the delay data and the
processing is going to be quite simple
Wow
now I idea is trying to be too smart the
processing is going to be quite simple
so we are going to update our largest
delay to be some you're just going to
take the maximum so the our data class
implements a comparable interface so it
can take the maximum okay so we just
take the maximum out of one or two
elements from from from from a list so
nothing complicated so we have a method
which actually updates our internal
actors thing ok so still we need to
somehow provide the notion of how many
day how much elements are we currently
processing and how much elements we want
to signal demand for ok so to do that we
need to provide an implementation of a
request stress our request strategy so
that's a thing from the actor subscriber
so here we will use the maximum
in-flight request strategy yeah ok so
let's say that we want to specify that
we can only handle at most 10 messages
at a time ok we are going to have an
internal counter of how many messages we
are currently processing so here so far
the processing is very simple right
because we only update our internal
counter so this 3 doesn't make sense
just yet but wait a second so you know
processing a message and an actor can be
anything from updating state to actually
communicating with an external system
right so if it if it is communicating
with an external system for example so
this in a fight counter can be nonzero
quite often right so this means how many
requests how many messages are like in
the process of being still well
processed right so far we have very
simple processing so we are just
incrementing and decrementing encounter
effectively effectively we are not doing
anything special ok but let's leave it
at that for now
and now we can connect our largest delay
actor we need to we need to connect it
to our data processing pipeline so let's
go back to our receiver and we are going
to substitute the sink so let's create
an instance of our actor largest delay
actor is going to be system actor of
folks now just lay actor yep okay
imported okay so we just create an actor
right of the of that class that's our
actor so instead of a for each thing we
are going to send data to a sink which
is an actor subscriber we need to
explicitly specify what kind of data it
accepts because actors untyped at least
as for now and so we are creating a sink
which sends data to an actor kind that
actor is what we have created over here
ok and ok one more thing so that we can
actually see so that you can actually
see what kind of data were like what
what's the current state of the
processing each second we are going to
send
and a message to the octo please log was
the current largest delay system
scheduler schedule 0 seconds every
second send to the largest delay actor a
message log largest delay ok we need to
import two things we need to import the
dispatcher so that can actually do that
and we need to import that nice syntax
over here 0 the second so we import
syntax imports color concurrent duration
ok and in our actor we need to handle
the loge level a message and we are just
going to print line largest delay so far
larger delay so we know what's happening
ok so yeah
yes yes so here like the flight data it
goes through the actor subscribe so it
goes through the sink decoration right
so that is when it's being enriched with
the with the on next right so that here
we know that it's coming from the
reactive stream and here is just a
normal message you know once per second
so it's not being back pressure or
anything
no no I like here we can do that any
pattern matching right so whatever
whatever our stream is producing we can
yeah we you need to have some common
type yes yes it can be neither made for
example and okay so let's try running
that so first we run the server and then
we try right so we have no kinds
connected right so we have no largest
delay computed now we run the sender ok
so the sender send some data we have a
new client connected and you can see
that there are some flights coming in
and there are some fights which head
like 522 seconds delay per per mile so
that's very bad and what it happens as
as you can ok so that works likely and
so okay so we have a system which is
like we have the back pressure from we
have the back pressure from from from
the actor which consumes the data all
the way to the client which produces the
data breach from from disk over the
network right so it's it is a bit
reactive but still like it still do this
the server might crash and if its
precious we lose all the data so that's
bad right so let's do something about it
and okay so some source also nothing
here so now we are going to use a capper
systems to actually store the state that
we are computing in our actor okay so
here we have some state which we are
computing as the data comes in and want
to persist that state so that it is safe
from crashes so instead of actor we are
going to extend persistent actor okay
persistent doctor yet so we need to
provide that persistence ID which is
like a key in a database using which and
this state of our actor is going to be
stored so anything and any string as
long as its unique will work and okay so
now when we actually receive we don't
and we need to rest implement receive
command now that's a minor thing okay so
what what what do we know now when we
receive data before we process it we are
going to save it so we're going to save
an event like it behaves a bit like an
event source system so we are saving
events ok so we received an event we
have the flight with the Labour my we
are going to save it into into our
journal so we call persist async on the
data received right and only once the
data is is persisted we are going to
actually process the data and decrement
the in-flight counter ok and now the
inside counter makes a bit more sense
right because it may happen that the
disk is busy and it's going to
accumulate 10 pending rights right of
out of the data and like not complete
them for two seconds or so and during
that time like the actor will stop it
will stop generating the man right and
the data was stopped flowing because
here we cannot process animal data right
because the disk does some is something
else right so here like a quite often
the in-flight counter is going to be non
zero because we're only decrementing is
once the data is true truly processed
meaning here at least it means that it
is written tower to our journal okay so
that's like the writing case now what
about reading data so it sometimes
happens that our actor crushes so when
it starts up it's going to have its
state restored from whatever is written
to the to the disk and then we'll have
the receive recover call
okay and here we are only going to
receive the events that we've actually
written we have written events of this
type right because here we write D so
that's that's of this type so if d is of
type fight with dry per mile we are just
updating the internal state with that
with that event right the events are
going to be replayed in order as they
were persisted so we are going to end up
with the same state as if we receive
them all like from from from this dream
okay so by default a there's no drama
period so by default this is going to be
written on disk and that's like a
default journal implementation which is
leveldb I think but you can easily swap
the journals and you can use for example
Cassandra go to the rapid replicated
database and then your data is safe like
replicated all over the world and so on
well you have to go okay so let's write
actually running that and let's see if
it works so let's run the server the
server shouldn't have any data right no
no data yet we run the client which
streams the data so yeah we have some
data stream from the client now I will
receive a crush is okay and you can stop
the Sanders were now let's restart the
receiver and let's see if it has any
data or not it has some data right so it
recovered the data properly
okay so so let's persistence which we
can use to save the state okay and one
thing that is left I don't have of
course a lot of time left so so so and
so now we are safe against crashes of a
single server right but still like if
the server results we are safe but if it
doesn't restart we are not safe like we
have lost our functionality we are not
responsive anymore right so the question
of course here is that we need a cluster
of these things running on different
machines and when one dies we want the
others to to take over so I'm not going
to code everything here because we are
running out of time so I'm just going to
copy a bit of code and explain what it
does and sorry for that okay so to
create a cluster in a car it's like not
a lot of code that's all they call we
need to actually create the cluster okay
so the first thing you need is you need
to provide alternative config
configuration that's the configuration
the main points of the configuration is
that we are using a clustered actor
provider okay so that's one thing in the
configuration and we need to provide
some seed nodes so how the a cluster
should look for initial members okay so
that they find each other so we are
going to run three nodes on one machine
here and they have different ports 1 1 2
3 right okay so let's like the base the
the minimum configuration you actually
need to run a cluster so what we want to
do here is we want to form a cluster of
three machines and on that class 31 hey
we want to have exactly one of that
machines running our receiver server
okay and that exactly one server is
going to accept connections from client
and then they will be able to stream
data to to that server so what we need
is we need a cluster of singleton okay
so there's an Akha a module for that
it's called the cluster of singleton
manager over here so on each member of
the cluster we start a cluster singleton
a plus a singleton manager and that
manager makes sure that it will run the
provided actor so here's the provided
actor it will run the provided actor it
will start the provided actor on exactly
one note in the classroom okay okay so
once again here we have the
configuration right which I have shown
you with the cluster actor provider we
override the configuration with our port
right because we need to specify what
ports to bind on the cluster
communication and here we start the
cluster using the configuration right we
use the cluster configuration the
cluster is formed then we start a
cluster singleton manager which is going
to run the provided actor on exactly one
node okay in that actor so the doctor is
implemented over here so what it does it
does only one thing in fact when it
starts it runs our receiver okay so that
receiver is exactly the same call to no
modifications as we have written before
okay so we have an actor which starts
our receiver server and that receiver
binds wants important accepts
connections from clients okay so let's
create a three nodes and clustered
receiver one extends up okay new
receiver cluster node and we are going
to use port once 1192 1713 we are going
to run it so we are using two poles here
one is the port bow plastic
communication right so all three nodes
are going to bind on three different
ports for faster communication and
another port is where our a server
actually listens for client connections
and it's going to be a dead plus 10
right so here we do the blastin ok so
let's
creates three of those okay so i have
three notes i will just remove whatever
state is stored current in the actor so
that we have a clean system i remove the
journal so that's like the aqha
persistence thing from the previous
examples okay so let's run this three
cluster nodes okay and
yeah I don't need to set them on
different ports thank you so two and
three
and one
tour
and three okay so it's going to take a
couple of seconds to actually form the
cluster but we should see that on a one
exactly one note should start reporting
that it doesn't have and the largest
delay yet right so no cluster yet what
is actually doing that's the younger
node oh here we have it right so on the
first node we have information that we
have no data yet so now let's stream
some data to that a two node number one
right so I would just change the port
here so it connects the right word okay
we start the sender and it streams some
data yeah we have some data here on the
first node right okay so now what now we
of course kill the first know right what
else can we do so let's kill it and
hopefully now the data will be because a
the journal is short across because it's
on my machine right the journal is short
so the data should be recovered on
another node so let's see number two
nothing yet it is like a 10-second
window through in which they try to
detect if the novel is really dead or
not and onions on the node number two we
have the data recovered as we wanted
okay so that's disgusting and yeah I
have 10 minutes left so that's a good
timing so just to sum up and just to sum
up and I have four more slides for the
end and let's start with with with right
so a we have implemented at least part
of the rack to manifest or and right we
have a responsive system we have private
resilience is through clustering and
persistence we have provide elasticity
so we react to varying component loads
using back pressure and akka streams and
everything is message living thanks to
actor actor communication
and there's a couple of things you could
do more here right you could discover
where on which note the plan our server
is actually running to edit add to
address report and address and you can
do that with a cluster client that's
also pride the Bayaka and we should also
provide some reconnect behavior on the
client so when the one server dies the
client should try to recollect went to
another server and we could also use
snapshots and persistence so that we
don't always recover so that we don't
always read all data from the beginning
of time when we rate when we recover
actor state so there's like a bit more
that we could do it to akron enhance
that system but that's not enough for
for 45 minutes so if you're interested
in that stuff and this code for that
presentation is on github if you would
like to see a bit more complete version
a like a reactive message queue written
in a very small way but with the
enhancements I showed on the previous
slides is also on a github and the
reactive streams is like a broader
initiative akka is only one
implementation of the reactive streams
standard and also others and they
hopefully interoperate well they should
interoperate that's the idea but we see
how it is in the rally and and I have 11
last invitation and so I have stickers
as well I have I'm proud of my code
stickers and I have scarlet I'm stickers
if you're interested in Scala a squad I
missed our newsletter and also there's
going to be a free to attend one day
conference in the war so on the 11th of
April so if you would like to come feel
invited it's on a saturday there's a
party afterwards and so sign up for the
newsletter we are going to start
registration soon and if you have any
questions i'm not sure if we have any
time but they're true
what's the advantage of using just a
kind of spark so with acha a source park
is built on top of a car right it's not
it's not built on top of extremes so I'm
not sure how back pressure looks in in
spark and but here you have like it is
maybe a bit a bit low a bit lower level
but also you can you have more
elasticity in what you can actually do
right you have well it if you have just
a data processing pipeline then maybe
spark will be better if you have a data
processing pipeline which needs to
connect to some other systems provide
some persistence provide some failover
clustering so maybe a Kaiser is a better
choice than
yes messages here in that example could
be lost yes yes that's true that's true
that we didn't we didn't build any like
a message acknowledgments right so and
the end it's not provided out of the box
yeah
you only testing for
and unit testing for resilience well
well testing is a very good question
there is a these are testing framework
so you could test actors in many ways
there is an architect kit and if I guess
you could build you could build a
testing for like some testing
infrastructure around that and you could
also we know start a cluster your tests
just as we did here and but I don't
think that's like anything specifically
in our cutoff for testing resilience but
I think using test kit and like the fact
that you can run multiple nodes quite
easily right that shouldn't be a big
problem oh that
thanks
yep
yeah there's an extension also like
singlet enters a Chardonnay extension so
you can have like so the typical use
case I guess of the first use case was
to implement aggregate roots which are
present on different nodes like each I
burga truth has some note on which it
lives and yet so there's an extension
you can implement charging with it this
way
okay thanks a lot I'm here for like the
next to the end of the conference thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>