<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Fredrik Linder - Realtime performance at scale (...) - Erlang User Conference 2015 | Coder Coacher - Coaching Coders</title><meta content="Fredrik Linder - Realtime performance at scale (...) - Erlang User Conference 2015 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Fredrik Linder - Realtime performance at scale (...) - Erlang User Conference 2015</b></h2><h5 class="post__date">2015-07-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/2Llc_k28a-U" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello welcome as I said my name is Fred
Linder I have more than 15 years
experience on service side development
mostly are long and C++ currently
working machines own I have does it work
better better okay good I'll have pity
previously in a corner at ericsson among
other things places i have a masters
from stockin's university and how many
of you have heard about machines own you
get a hand raised a few of those that
nice when i joined almost 33 years ago
we were about 40 people in this one
building inside palo alto and nowadays
we are a multinational company with
offices in a couple of countries all
around the world and we would like to
think of ourselves as a tech company and
when I say tech company in our case that
means that we need we have what we build
and get money for we need to build some
technology to make that happen because
as far as we know we don't there's
nothing available for us that would
serve that kind of scale that we need so
we're taking planted build games and we
build primarily free-to-play games on
the mobile market and so there's it
there's a difference for on the mobile
market compared to the PC market for
instance the device of wherein on the pc
market the devices that you use they
typically have a lot of energy I mean
they don't run all the batteries right
they had typically have quite good as
stable network connection and they
usually have quite a lot of CP
you and graphics computation power and
on the mobile market those things are
not necessarily there I don't know if I
know you remember original gangster I'm
mobile I night for instance they were at
at that time that they were big they
were top grossing games a most recent
success which is being on the market for
years at like two years i think is game
of war so another Henry's how many of
you have played game of war so a few how
many of you have heard about game of war
before this a despair so I game away is
the world's largest single world
interactive game what I mean by that is
for everyone who's playing the game they
are in the same world typically when
game companies in games they can have a
lot of players but they are typically is
isolated into lowers smaller worlds
because that's the kind of scale and
capacities those companies are capable
of producing in our case all players are
in the same world even though yeah jump
that yeah so we have players all over
the world we start out with the testing
in New Zealand to see that everything
worked fine and then it moved forward
over to the Australia and some other
other countries in that region has to
root out all the difficulties and stuff
and then we moved over to the big
companies there are big countries like
Russia and France and the u.s. and
Mexico to see that that was a big launch
and nowadays where we launched worldwide
I will have players in over 40 countries
last I checked I think so what do you do
inside this game well it's a game of war
so and it's the setting is
medieval fantasy game so you build your
city you build your army you haven't
hear that you want to improve so that he
or she will bring benefits to the two
you're fighting or to use strength you
have inside your big city you can build
your forms or queries or the place to
build weapons or to teach how to teach
warfare yeah and then with this army
that you have you can either attack
other people or defend when they are
attacking you you can also go out on the
nearby Reese neighborhood nearby place
in the map together resources so one so
going back to technology now so one of
those things that the game of war needs
is a map and well this is not the
graphics of the map of course but it's
showing how the map is built so in our
case the map is bit of a large grid of
tiles and some set of those times is
part of the kingdom and all these
kingdoms share the same is on the same
world within the kingdom we can have
cities it placed out on the map
different places and we have resources
and monsters roaming around over there
yeah so go back to more things about the
game so one of the things that you start
off doing in the beginning is typically
join an alliance to help get some
friends do to protect you to teach you
how to play or to send your resources
help up on here in trouble and when you
grow in this game we get more and more
involved and learn how to do this better
better maybe you can climb up on those
higher keys or maybe you can create it
your define start your own Alliance to
be the king of data lines or the leader
and another thing that you do in this
game is that you meet people I think it
took half a year or something for the
first marriage to happen in the game
with two people who met by playing this
game where you get married so it's a
typically a social game right it's a war
game but the main thing here is that
it's a social game we have these people
all over the world playing against each
other and join teaming up or team
against each other and they need some
way of communicating because the not
everybody here speak the same language
right so and one of the things that we
need in all this is a way to break that
language barrier and there's we come
back to technology again so game of war
need to chat with real-time translations
and yeah you could think that
translation would be an easy thing you
just send it to Google or Bing or
something but those translation systems
are based on text with larger volumes
like a book or necessary or something so
the language detection there or the
translation things there are based on
that they have a lot of information
about the text that you're sending in a
chat it's typically have a few words so
you need to be able to detect the
language just using three or four words
which is a very difficult task and you
need to define to know the language that
you're translating from in order to
split up because
we also do things like chat speak like
Laura Laura and your witch which is
slowly in French for instance I would
always make sure that those things are
also translated to the equivalent thing
on the language that you are speaking
weird that your device is set on
typically and always have to be since
it's a chat this translation has to
happen in real time right you can't have
a chat with you send something and then
the other person receives that after
three seconds that wouldn't do that
wouldn't be that wouldn't feel natural
when it play this you have to have
enough performance in this and an
upscale to handle these low latency
translations so I'm not going to go into
how we did the translation hard thing
but I am going to talk about something
else later on so you can say that our
company is like in two parts one part is
doing the game figuring out how the game
should work I didn't have the arch
should be how the interaction should be
how to make people want to stay in the
game what to play more how do how to
make them want to win things then you
have the tech side to enable these
things so for instance there's an app
service is shut servers translation
service and we have a couple of other
ones as well so that would be on the
tech side so all these things on the
tech side we can reuse them in other
products if you want to and today I'm
going to primary focus on the map
service in the chat service okay so it
will be fairly rare to be honest with
you we don't have a map service the game
the what I call a map services actually
split in two parts one part is how the
game uses the technology that we provide
another thing is a
it's a pub sub service and chad is also
pub/sub service ok now another handler
is how many knows about pops up I almost
everybody that's good sub pump some was
defined I think like 20 years ago or
something or maybe 30 and with it hasn't
this it's about half of you knew about
it it's not been that widespread I guess
so pops up is typically you have one or
more publisher that sends things over
from the topic or a channel or chat room
mrs. and then you have zero or more
subscribers to get all of those events
and in our yeah so the chat service that
we currently have no the pub sub service
that we currently have it's fast enough
so that you when you scroll the map or
do the chat there is no lag you don't
experience that it's that you scroll and
then you have to reload the page and or
things like that so how does the map
does game people make a map out of the
pubs observers remember I said that the
map was a grid system so more less every
grid in this system every square in this
is a separate channel let you post
events too so for instance if you are on
the position 33 you would see you on
your map you would see the things around
you we'd put maybe it's like 1 1 2 3 3
the green area that means you are
subscribed to events that happen on
those channels and then you send your
army where you scroll around which means
you move around on the map right so now
you are interested in the other nine
tiles around you
closest to you so then since this is a
mobile device we don't want to stay
connected to advance that we don't no
longer need to know know anything about
because we don't want because that would
do two things it would eat up some of
the bandwidth and it would also eat up
some of their battery in the device and
we don't want that because you want we
want you to be able to pay for a long
time right and we want you to not spend
your money on on the mobile operator so
we need to insist subscribe and
unsubscribe to these channels so you
subscribe to the blue ones and they all
subscribed on the red ones and this
subscription and our subscription needs
to be pretty fast so that when you
scroll if you continue scrolling you
should wigan others square up you should
be able to do that in real time so you
still get all those notification that
you are interested in it can cancel the
other ones in any case yeah yes so
that's how we map how it had the map is
implemented using pub/sub for the chat
system it's kind of late to be
understood I guess each chat room would
be a separate topic or channel that you
subscribe to or publish to so in our
case we have each kingdom is
automatically your whenever you start up
the game you are joined in 11 chat and
that's the kingdom chance you can see
what everything is everyone says around
it in the kingdom and when you join an
alliance you have access to that
alliance chat but then we also have you
can send one-on-one chats if you want to
or you can have discussions with other
people in the game the
that are not related to an alliance or
anything so that means you can have
different modes all kind of sizes of
chat system of chapter ohms yeah so I'm
talking about this when you scrolling it
there's no lag that means that there's
low latency on all these things the
latest is typically sub-second latency
or some even better than that and
apparently there is in Japan there's
this one this there's an underground
movement that when a certain popular
show is sent is sent and is aired on the
TV everybody who's watching that is
sending a tweet at exactly the same time
during that episode and for the first
couple of times that happened Twitter
went down and after whatever after a
couple of times they were like yeah we
divided one of those bursts if I am have
an asst to do right we are having that
peak capacity every day but we have
gained team who wants to let new things
right so they have figured out that this
pub sub is really cool thing and they
want to have more features based on it
and our current system cannot handle all
of those things so we need to improve
and they come back and say okay how much
do you want us to improve is it like 10x
or 100 x or three see the thousand eggs
so basically if there is a 10 X that
means that you can they can select the
top priority features and implement them
and see that that would happen
and to get 10x you could probably just
start up a couple of new clusters right
so reaching hundred eggs that would
probably not be economical anymore I
would hundred X you could probably get
all of those feature that they are
thinking about and with the salsa night
we could get all those things that
haven't thought of yet would probably
will eventually so our question was how
fast can we make it what is the
theoretical limit limit well the
theoretical limit would be you check on
the network connections since you have
big they aren't chunk of the size and
say okay this can be the rate right so
say to help one megabit 10 megabit
network for instance and the typical
paler would 10 be 100 bytes that means
quite a lot of messages per second and
then you need to have the assistant
tackle hunger to be to to be able to
perform a pub sub at that speed and yeah
that would be great if we could do that
so when it comes to performance what you
typically talk about is throughput and
latency there's the cost of operation as
well but that one also boils down to
throughput and legacy because you can
have if you are able to maximize your
network usage and you haven't that with
it low enough latency to for the game to
accept you could have fewer hard less
hardware right so that pulls down to
throughput and latency so how many of
you have heard of a little slow a few of
you okay there okay that's a good so
little slower is it says that the on the
unstable system the average throughput
of that system equals the average number
less
this that is in the system / the queuing
delay imposed on those messages so what
does that mean so now we're messages
that's wanting right s'okay okay I
picked up listen latency the time
experience from the client side is it's
the sum of two things it's a processing
time for those events plus the queue
that the q onda later is imposed by the
system so naturally we want to maximize
the number of medicine system and
minimize the queuing delay and also
minimize the processing times and there
are some techniques that typically can
do to meet that to maximize nonetheless
it is what you typically do is yeah are
there two things you can do the do
things in parallel but that would scale
only up to number of course because
after number of course you would induced
you can that's the only thing you can do
it one on one time so what do we do is
if you add more things in parallel that
means you had queuing delay so in our
case with the postal system we have an
ordering guarantees as well for instance
if you send a message to me I'm supposed
to receive those in the order that you
sent them just as in Ireland so in a
line I think that what the delivery is
also guaranteed as long as nothing fails
and crashes and stuff but in our system
we do not guarantee that because that's
a little bit too costly for us instead
we make sure that any messages that you
send it if it is acknowledged by us
saying that yeah we have received it
then it will eventually be sent if it's
not acknowledge that means it might have
decent you want you don't know unless
you subscribe and see that you get it
the other approach that you would
normally take is to patch up messages
you can do this if the processing if you
can reduce the processing time to handle
Otis messages for instance say that
you're posting will be a truck if you
can load with one thing or we can load
it in 10 things or a hundred things that
would be the same cost of driving the
truck from A to B yeah okay so I'm done
minimizing queuing delay if you look at
on a network perspective that means that
you would ideally be reading in a faster
pace than things are written to that TCP
buffer that would mean that whenever you
take something out of it that means that
it's the sender would always be able to
send which is that's when you reach the
high capacity right in so we are using
gently CP which I guess most of you have
heard of and the NTSB has it both
handles the TCP layer or vm notice I
think and it also has an internal buffer
that you can control out by the buffer
argument that's a it was a killer thing
for us you also need to have some kind
of fast lap parsing and dispatching of
those messages so that you can read the
next chunk before it gets filled up in
the other TCP buffer so on there so that
this piece at anyone and it fixed time
and a specific time it has a fixed size
buffer for reading writing and also the
buffer on Indian TCP for on the process
inbox is that there it's not limited by
size that only you can crash there vm x
loading too many messages in the name
box right so the ideal there looks a
little bit different so when you ideally
if you want if everything is really
really super fast so whenever you get
scheduled out from the scheduler so you
run time is up yeah that means whenever
you have done two thousand reductions
that's when you get scaled out ideally
the your inbox should be empty at that
time you should have low enough
processing to allow that to happen and
yeah and another thing you can do so if
you have like 40 course for instance and
you have 200 processes active process is
started doing things 160 of those would
be waiting to forget the execution times
that that waiting for execution time is
this is queuing delay that's how they
need to wait for the getting things done
so to minimize the queuing delay you
should minimize the number of active
processes that does not always map well
to the thing that you want to do right
so for our case if we have say thousand
or or 20-minute 20,000 connected clients
all of those might be active at the same
time and you can't really do something
about it but it's something to think
about so when you're testing your system
there's one thing you should have a
watchful eye for and thats processes
with big large in boxes because if there
is a large in box that means the queuing
delay the the time it takes from that
you put that the message in the inbox
until it has been processed that's
large name box the lodge at the time it
takes to get that thing handled and that
means the larger the queuing delays you
don't want two large queuing delay when
you have a high throughput system and
another thing you should also avoid that
sink has blocking course because that
would while you're blocked waiting for
the reply for something that means you
can't eat anything from your in anything
else from your Inbox so that is done in
such a situation that doesn't really
matter but for the critical path that
might be crucial ok so then minimize
processing time that's normal hardcore
stuff right you would figure out the
right algorithm oh the right data
structures you have you know exactly how
many reductions your language constructs
take one thing that I believe is not
that common that you're thinking about
is that you shouldn't generate any
garbage even though the garbage
collection is kind of great in airline
it still generates data that's not
needed in your system I creating data
does not need it that's just kind of
waste and if you can get away from it
that will be a great speed up I will
show some example that later on okay so
back to a current and new pops up so the
current pubs that we use is it jeopardy
based it has a single node type so
whenever you want to scale out just add
a node and if I remember right there is
a session table in Lisa session table
that collects so you can know where all
the rope and the rooms are
and since it's misha at means you can't
really scale it out in that madam nodes
and also means that all nodes are
interconnected they need to know about
each other and we have an XMPP protocol
that is kind of heavy through the pores
so in the new pops up that we were have
been built bill building it's everything
is built in the house or the core part
of the system is built in-house it has
more than one node type so that we can i
optimize certain behavior and also means
we can scale things differently tippy
hasn't I was really sure how to name
this but like an m2m like connection
mesh mesh we have some notes connecting
to two other set of nodes but those
loads internal do not connect to each
other and for sim for to make the world
a little bit easier on the on the users
of this system we have using JSON
instead of XMPP with a in-house protocol
so Jason isn't like most efficient
protocol out there but we chose it
because that was we can already do
better after that right so we want to
scale more okay so what we did was we we
split out the multiplexing part on the
system that's the connection the mapping
between connected connected clients and
the rooms
and the Q which is where we put all the
topics and channels we place them in
separate nodes and that means that we
can if we need to add more people we can
add more MX notes and if we need more
data traffic more the volumes of the
data become bigger we can add more q
naught then that can be done seamlessly
without any affecting anything or if one
of them died we can just add another one
the connection would die of course but
but then when they reconnected with that
will do work again and when we see that
the traffic is is shrinking we can
reduce the system and still have a
working system and everything would be
great from from the cost perspective
okay so a very simplistic data path
would be like you have have a client
that is connecting to be the socket to
connection process and then and then
whenever someone is posting or
subscribing to a certain topic that will
connect to process on the q nada so now
we're going to take you by the path
we'll see how to apply those maximize
number of messages minimize late cuban
late delay or minimize about 10 minutes
only okay now i'm going to be fast
ok so we connect the client with tcp
using ranch and we were thinking about
we should use should we use active
passive mode or active once or active n
so how many think that passive mode
would be the best no one how many things
active once would be the best a few how
many think active n will be the best
that's more ok yeah so I'll get this it
turns out that active ones is actually
the fastest
and it also has a nice feature it has
it's a nice clean way to apply back
pressure which is how we control in flow
data but this wasn't didn't give us good
enough throughput but if you add the
buffer thing which would then maximize
the number cities in the system so we
have like twenty thousand as a buffer
size then that would be a good
throughput number I think it's hundred
thousand or app it's actually good
however if you have a big buffer that
means the queuing delay gets increased
because the first part will have to wait
for the last pipe in order to be handled
so let's have a closer look on the
connection process so one of the
problems with the connection process
with this a simplistic thing is that we
get to the data flow and put through
Direction are competing under the same
inbox and the same processing power
processing capacity and so you need to
spin it that means so that then we have
two processes one taking traffic from
one direction than the other prosthetic
in traffic from the other direction and
they won't be waiting for each other
with stealing each other's computation
time it also has the benefit that since
is a network since your mobile phone is
sometimes the radio shadow so standing
could possibly take over a minute and
that would not block your connection
process so when it comes to message
passing will go between nodes
we measure that on entire inside nvm you
can do at least a million messages a
second tip you more but between two
nodes we could only get 250 thousand
degrees per second thats eyeliner cities
burning terms being sent so the way to
lie should do does this since we don't
want them to share the same network we
need to buffer them together so that
each message each airline message going
between those contain more of the pub's
of messages that person if you look at
the Q side but yeah we were trying to
maximize normal message by paralyzing
things and we had an idea that s would
be great for this it turns out that s
was actually too slow so we had to write
an if to address this done good the next
thing so let's say when they go from q2
MX so what we did was initially was when
you publish something you push the
message when you want to subscribe
things you pull in an air for every loop
to get more messages however there are
some problems with this when you with a
push and pull for the same process that
means that they compete over the same
resources and same inbox same as we saw
earlier and another thing when it comes
to pull is the approach the topic is
unable to send anything while between
pull cords which is which would reduce
the throughput of the system and I also
wanna the way we design the pooling is
that we said okay whenever you pulley
get all the data that's available that
is also bad because that will get an
irregular amount of traffic through the
system through the network and that's
powerful performance
and another thing that is also bad is
since each individual puller put
poreless for them unique set of data
unique a range of data data that as
processing time because you need to
figure out where to start and the NICUs
and rest so our solution was to create a
subscription subscription-based instant
so whenever you get data whenever you
push over data the subscriber would have
their own subscription process that will
send data to it so you would have we
push over to the the topic process the
topic we will send it to a process that
knows where you are where this guy is on
this path so it wouldn't have to
calculate anything and it can also do
this sending in parallel and they have
been smooth things out I already talked
about limited bandwidth and to maximize
the messages we buffer them we don't
want to send the same message more than
once between two nodes so we are
exploding them on them excited however
that also had an issue see if that's
next yeah so sending messages to a lot
of processes takes a little bit time and
how many think it takes this would take
more than five milliseconds okay if you
how many think is would take more than
10 milliseconds okay I'll let them wave
okay so yes we're sending binaries now
it doesn't matter okay there's a very
big banners so then it doesn't matter
because if big banner when you send that
within the node your won't send a
reference to the pointer so that's
pretty small and especially
so it turns out that sending from one to
20 kms cities the quentin 20k processes
take about 80 milliseconds and it's a
it's more or less a straight line for
number of did the time it takes to send
to them and the number of most processes
it takes so that that's a good thing
though so it would be great if this
could be optimized of course so in our
case we had to write an if to address
this this knife is a very special
purpose so it's not that we can share I
think I think so how many how much more
time to have that sounds good two
minutes all right okay so the base so
the big part here is of course you know
that you should find the right algorithm
use across tracks and data certain
structure stuff but it's something that
is often not seen or understood this too
would generate garbage like i said
earlier so decayed like that so this is
great places have a look efficient skate
so for instance in this case no let's
not take that one let's take this one
okay i can do it this part of the list
so if you have a folder with that
actually generates it creates a list
that is thrown away compared to a for
each going on the over the same list
that doesn't create a garbage and you've
used a comparator to list comprehension
doing the same thing as the for each
loop there is a quite a big of a
difference in them in how long time it
takes so choose list comprehensions if
you can
so in this case you have record going to
show difference in garbage collection in
the coastal garbage so for these
compressions if you haven't something
after the list compression and you don't
use that value that this return it the
list is never generated which i think is
a great thing so in the second example
we have the list that we returned it to
it means the compiler can't optimize it
out or the preprocessors can't optimize
it out so we measure it and there's
quite a difference in the test case i'm
not using that at least anything else
through the right away immediately so
the difference is like three times just
because of Corbridge that the garbage is
built and thrown away another thing is
you should prefer doing things in line
sometimes you count because sequence of
stuff is required to not do it but if
you can that's a great thing so in this
case we ate regenerating garbage by
accumulating list in the first step and
then doing something new on the second
step and the difference is almost almost
four times waste another thing you can
see it also has this is the number
number of reductions that are used so
when I generate a list and when we react
reiterate on the result the resulting
list you get more reductions which is
not good for processing time ok I guess
I could quit thing thank you
and I think we can stretch</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>