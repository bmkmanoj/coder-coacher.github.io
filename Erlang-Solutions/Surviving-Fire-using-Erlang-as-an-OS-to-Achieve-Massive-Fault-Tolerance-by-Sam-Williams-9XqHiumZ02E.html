<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Surviving Fire: using Erlang as an OS to Achieve Massive Fault Tolerance by Sam Williams | Coder Coacher - Coaching Coders</title><meta content="Surviving Fire: using Erlang as an OS to Achieve Massive Fault Tolerance by Sam Williams - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Surviving Fire: using Erlang as an OS to Achieve Massive Fault Tolerance by Sam Williams</b></h2><h5 class="post__date">2016-09-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/9XqHiumZ02E" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">and Sam I'm a PhD student at the
University of Kent working with Simon
Thompson and Fred bonds so sly this is
gonna be a little disorientating because
I can't see what you can see here or
there on my screen but there we go um
right so my operating system is called
hydros after a Hydra single animal with
many heads as the idea and this that
you're looking at now is Hydra so this
is an airline program that is the
presentation running on the operating
system yeah the point of the operating
system is to withstand hardware and
software faults that's for the general
principle and and we do this by having
one beam per core and allowing those
cause to talk to each other but
generally operate independently all the
operating system services are written in
Erlang as well as the drivers and it's
built using a multi kernel design yeah
so first I'm going to go through what
the operating system is how you can use
it how you probably shouldn't use it and
what multi kernels are and how it works
under the hood so that sort of details
of the system and then we're going to go
through some of these sort of other
interesting features that it has
so the general project aim is to build
an operating system that will survive
complete failure of the software as well
as partial failures that of the hardware
so that's things like CPU cores failing
to respond RAM failure and peripherals
which is kind of easier but still yeah
um and the general idea here is that if
you have what would normally cause like
a blue screen of death on Windows then
that'll only take down one of your nodes
in your system so if you have n cause
that'll take down one over n of your of
your system's capabilities
so potential usage environments are
basically anywhere that you can't get to
a system easily to repair it so weather
stations through a good example or if
someone's going to have to go in a canoe
for four days to try and repair the
system that sort of environment and also
space is another obvious one so in space
you're lots of space dust flying around
incredibly quickly which might hit part
of your system and knock out small parts
but not all of it
yeah and generally just arenas where
your hardware is likely to be damaged or
or areas where you want your system to
run without stopping for longer than the
expected life of parts of the hardware
in the system yep so multi kernels an
idea that was sort of created by the
barrel fish and factored OS teams who
are building research operating systems
that were trying to run on systems that
had tens or hundreds or even thousands
of cores they were motivated by trying
to get rid of locks essentially that's
the main principle yeah and this came at
a time when people were expecting that
we would have you know thousands of
cores within our computers within like
10 years or so back in 2007 they didn't
really work out but yeah we found
another interesting use for the
architecture so the general principle is
that you treat a modern computer as
analogous to a network of older
computers so each of the cause runs
individually and separately and there's
a message passing layer which is
implemented in shared memory that allows
the course to talk to each other you
have one core sorry one kernel per core
and the the memory address spaces are
disjoint so they can't access each
other's memory
yeah so generally we try and avoid locks
by duplicating OS state and then
negotiating when required so the whole
idea is that each node would essentially
run as if it were alone and talking over
a network to other computers and sharing
important information about that yep we
use message passing rather than shared
memory again to try and get rid of locks
and we use yeah solutions from the
distributed systems world to try and
solve operating system projects at Skate
sorry operating system problems that
occur at scale that's the right so the
project as you can see it works this is
an airline program you're looking at yep
it can run arbitrary Erlang programs
without much problem we have pretty good
compatibility there's a solid amount of
device drivers which do things like
beeping and you know console cursor that
sort of stuff and a couple of other
features that are going to more in more
detail later but a multi-unit kernel
system which essentially allows you to
host unique kernels in your system
without a hypervisor and also a
capability system which allows you to
stop processes doing things that you
don't want to do so you can run things
in sort of quasi sandbox mode okay so
the system is designed essentially with
the virtual network of course each core
runs a beam and we do message passing by
each core sorry each each node exposing
a small yeah a reasonably sized message
buffer that you can other cores can
write into and then it'll take those
messages up and distribute them to the
correct process within the
note yeah an all program code is
executed in the beam environment there
is no there are no bits available to
users of the system and there's a reason
for that which we'll get into later okay
so why do we use earning well Erlang
Maps really well on to what we were
trying to build Erlang uses or has first
pass its own first-class message passing
that's the sort of general principle
which Maps perfectly to our model yeah
and also the beam itself kind of
resembles an operating system it does
process management scheduling memory
management this sort of stuff so now
we're going to look at how it works
under the hood so there are basically
six major components to the system the
bootloader the C standard library which
provides us with a very minimal UNIX in
UNIX it environment that just allows us
to run the beam and then some bits that
allow us to interact with hardware so
port IO this sort of stuff and then OS
subsystems and drivers on top of that
and those last who are all Erlang okay
so the bootloader basically we found
that we needed a new bootloader when we
were building the system because we
needed to load not one kernel into
memory but in kernels where n is the
number of course you have and also to
distribute them correctly through memory
and set up page tables so that they are
or they believe that they are running in
their own single address space and that
led us to to an interesting problem
which was basically that bios's come
with good support for reading from
whatever medium the system is booted
from
in this case on this machine that's from
a USB stick and if we wanted to well you
know spend a second and in 16-bit mode
you can only access the first two
megabytes of RAM and our kernel is eight
megabytes and we would like to avoid
duplicating work as possible and also
because all of our drivers are written
in Erlang we don't want to write a C
driver to access USB sticks and then
have to rewrite that later in Erlang so
the the solution to this essentially
came from embracing the multi-core
nature of our target machines and
basically what we do is we boot a the
first core into 64-bit mode where it can
access all memory but not the BIOS
routines and then we boot a second core
which we keep in 16-bit mode and we get
those two cores to communicate and
cooperatively load the system into
memory so the firts core will copy parts
of the system into high memory and the
second core will read parts of the disk
into low memory and it's sort of
cooperatively loads the whole system
which kind of interesting so the C
standard library that we're using is
bespoke as well well that's not quite
true basically what we did was we got
the beam and we yeah we try to get it to
compile in a system that had no standard
library at all and then went through
adding the headers that were required
from canoed live C just to make it
compile then we looked at the the
functions that are required and
implemented stubs or a macro that made
stubs for us then we went through and
made yeah 180 stubs so that the system
would compile then we ran the system and
every time we encountered a yeah stub
being called we were going we would
implement that stub function and then
yeah so then we had
system that would be and it turned out
that actually you only need about twenty
four functions in order to get the beam
to load and start executing code which
is kind of interesting so the entire
standard library which essentially
represents the unix-like part of our
system is 84 83 kilobytes which is
interesting and yeah we were also trying
to keep this absolutely minimal because
it's much harder to recover from errors
in the C part of the system than in the
Erlang publi system so that was one of
the main principles we developed which
is basically pushing as much of the
functionality up into earning where we
can easily monitor things as we can so
next up is the beam and essentially we
try to leave it as much as possible
vanilla so yeah most of the deviations
that we had to create word just edits to
the config dot H file which is nice
because that allows you to get rid of
huge parts of the system that aren't
necessary yet we had to add a few things
for interrupt handling for example so
there have to be entry points in the
system for interrupts to go to when
they're called and these essentially all
point back to a little bit of C code
which sends off a message to an ER lang
server which is yeah which then
distributes the message or the
interrupts out to the appropriate
listener and that's how for example
keyboard works so when I press space
yeah right
so the OS services are written
completely in Erlang and they
communicate with the hardware through a
very small set of bits yeah I think it's
for in fact which really just do raw
memory reading and writing and import
i/o it's not it's not very large yeah
and we do device driver management and
also topology management
we try and work out what the state of
the machine is at all times and keep
that locally and that's um yeah that's a
that's a per node thing so each node has
its own vision of what the system really
looks like yeah and we also monitor
other nodes and obviously when they fail
they can't restart themselves so or most
of the time they can't restart
themselves so we yeah we restart them
from others and we do something kind of
interesting there which is we detect
node failure not by just pinging it and
if we don't get a pong or a few times
then then we'll restart it instead what
we do is send it a function which it can
then execute and then return the
response to the sender and this is kind
of useful because in that function we
can do arbitrary computation that
stretches and stresses the operating
system so that we can detect whether
it's in the same state or not so for
example we spawn a process we do some
floating-point arithmetic or not yes you
know I think we do and then we send the
message back so if you get a response to
one of these sanity check messages it
essentially means that the operating
system is not just running something but
it also schedules and it can also do
arithmetic and those sorts of things
that you really want an operating system
to be able to do yep
we also have a concurrent init system
which uses an interesting algorithm that
essentially pushes all the planning on
to the scheduler so we just spawn a
bunch of processes and the yeah the the
process or the the init goals that don't
require don't have any sub requirements
execute because they can't and then
messages are sent to other processes
that are listening for that goal
completing and then the sort of tree
collapses on itself which is kind of
nicer and there's also a basic Erlang
shelf which you might have seen me
playing around with earlier if you we're
in the room yet so the driving there's
drivers in the system
handle all sorts of timers and yeah the
pics which other programmable interrupt
controllers and these are quite
important in our system because the i/o
a pic or global a pic which is an
advanced programmable interrupt
controller controls routing of
interrupts so we use this to send
signals to the correct node in the
system or all the nodes in the system we
can set it up as we like yep do keyboard
cursor control consoles as well and we
can also read tables like the system
management BIOS tables PCI configuration
space those sorts of things we have
reasonable good reasonably good support
for a next stuff is an Ethernet driver
so that we can expand the system out
onto a network and see see what it's
like when it can talk to other hydros
nodes right so there are essentially
three layers in our system the kernel
layer which is written in C the local
Erlang note or local beam OS node layer
which is written in Erlang and the
global layer which is written in Erlang
- and the general idea is that we push
as much work as we can from the kernel
layer which it's much harder to recover
from from failures in to the Erlang
layers and so we keep the code minimal
and terse with the kernel and the
bootloader we try and yeah minimize that
and then the local OS layer we found
something kind of interesting which was
that oh these are these are processes
that have to be or systems that have to
be duplicated for every node in the
operating system or within the machine
so this is stuff like capabilities
management there's a server that looks
for that and those sorts of things and
also inter-process communication servers
that handle the mailboxes for each
and in these cases we found that you
actually have to embrace serial
execution and shared memory because
otherwise you get huge latency so if
every time you spawn a node sorry a
process in our system an entry is made
into the capabilities system that says
you know it can do these things can't do
those things and if we do this by
message passing then you end up with you
know potentially multiple round trips of
all the processes in your sister or on
your node being scheduled which is very
long and it's latency that we can't
afford
when spawning processes so the we ended
up having to use ets tables to store
this kind of data but it but it works
fine because there's no concurrent
execution on a single node which is kind
of interesting so the final layer is the
global layer this is for services that
are exposed to all programs from all
nodes and you don't care where the
processes are placed that's down to the
this Pawnee utility and they don't have
any shared memory this is sort of your
normal Erlang model of programming and
this is also where almost all user
programs would go right so one of the
things we were looking at is because we
because we run only bean code in our or
bean code from users we we have this
opportunity to have much finer grained
control over the execution of programs
in our environment because every single
instruction has to go through the beam
VM which we can interrupt and and check
whether certain things are allowed or
not and sorry
yeah and this this is slightly unrelated
but kind of interesting I think we use
this system to essentially create
resilience to bugs within the beam
itself and basically if you if you were
running if a process was running a bit
that is accessible from the system layer
and that Biff happened to cause a
general protection fault and try and
access some memory that it didn't have
access to normally on a Linux computer
this would cause the beam to die and you
would have to you know restart and that
sort of thing but because of this system
we're able to catch the interrupts and
kill the process that caused that a bit
of the beam code to be run and then
continue execution with the other
processes which is rather nice right so
the capability system in the operating
system allows you to provide
fine-grained controls over what a
process and it's sub press it or
children processes can and cannot
execute so you start the the first the
root node in the system starts with an
empty list for its capabilities and it's
an exclusion list so this music can do
everything and then when you spawn extra
processes you can choose to provide some
capabilities that that process should
not be able to do or you can keep with
the default which is just the processes
the capabilities of the parent process
and this essentially allows you to start
trees of processes which have fewer
capabilities than their parents so if
you had a program that you weren't sure
whether it was malicious or not you
could spawn it with none of the
capabilities that allow it to talk to
the outside world or write to disk or
even message processes outside the node
this other stuff and you could be sure
that it would run and it wouldn't be
able to harm your computer which is nice
and we also yeah we also use this for
drivers
and things and you can essentially
provide a sort of defensive layer if you
want to if you have a driver you're not
sure of the quality you can work out
what what capabilities that driver
requires and then cut down all of the
other accesses in the system so that it
can only access the two i/o ports it
needs in order to do its job and this
means that if it if it goes AWOL for
errands then it can't harm the rest of
the system which is nice yeah the
control scheme for this is deliberately
very very minimal at the moment you
can't add processes higher up the tree
can't give capabilities back to sub
processes at the moment and there are a
few other things like this but the idea
here generally is to keep the attack
surface small so the neater and smaller
it is the less likely you are you're
going to find a way into or around a
capability system um so another feature
we have in the operating system is sorry
just checking the time
Yuni kernels or multi unique handles
rather but I'll first explain quickly
what a unique ml is so unique annals are
essentially compiled programs or
programs that are compiled into
operating system images that can run
without without aid of another operating
system underneath it so they're normally
used in hypervisor environments to cut
down the size of your deployable image
and also cut out unnecessary OS services
that might expose you to security holes
so this is quite nice because you know
if a hacker was able to get Holt in to
say Mirage OS oh camel program they have
no shell with which they can attack the
rest of the system so in our system we
built a thing called multi unique URLs
which fits very well with the multi
kernel model in that essentially each
kernel on each core is independent so we
can replace some would be Erlang or beam
nodes with user made uni kernels and
this allows and these kernels run
outside of a hypervisor unlike normal
unicameral environments means there's
less interruption you get entire access
to the CPU and you're never interrupted
so it can be very very fast yeah and and
another kind of interesting thing we're
looking at doing with this sorry and
again is potentially creating what we
call a sub OS model where you can load
an operating system on its own onto one
of these nodes and it runs side by side
with your Erlang nodes and then you can
use the device drivers provided by this
operating system within your hydro
system and this is you know for systems
in the future perhaps where there's
hundreds of course so giving away one
core to run you know net B BSD or
whatever is no problem and so these
multi uni kernels can communicate with
each other through the normal message
passing system that the rest of the yet
the rest of the hydro system uses and
they can talk to one another rather than
through an Erlang no necessarily so you
can make interesting systems where for
example maybe a unique kernel has direct
access to the network interface card and
handles requests for you filtering some
that need a lot of data processing that
and sending those to a unique kernel
which does that processing and sending
others to Erlang notes that might do
HTTP handling and that sort of stuff
right so next up we're going to yeah
find a way of emulating
or at least triggering realistic CPU
core failure this is this is a big
problem for us because we're we're quite
sure that our system will handle this
but without a good way of testing it in
realistic conditions it's hard to be
sure yep
and another thing is expanding the model
out onto the network so making sort of
meta hydro systems which allow sub nodes
to communicate with others in in the
system and yet we also have two
undergrad students this year one of whom
is going to look at building a
distributed file system for the
operating system another that's going to
look at making a concurrent GUI so that
I don't have to present with this
interesting text-based interface yeah
and one interesting thing we're looking
at doing with the networking is getting
programmable network interface cards
that that we give kernels or programs
that can access the message out boxes
for each node in the system and then
Hoover up these messages send them out
across the network without the core
having to do any any sort of interaction
with the network interface card itself
and we can do this because PCI devices
have DMA or direct memory access so yeah
so the operating system is available
from hydros - project org you can run it
it runs in box qmu KVM and Xen and
probably a lot of other things it
doesn't run in VirtualBox at the moment
you can also use it on real hardware
like I'm doing here it runs on all the
systems refused but you might have a
problem with getting it to boot off a
USB device like I did today which you
can just use grub and chained load for
if that's the problem
yeah and you can also download the
source and compile it yourself and edit
the interest in see work or people or
people do that yeah so thank you yes
time for questions how's your interrupt
queue implemented how do you do that
become an airline message yes it does at
the moment and there are major problems
there message queue bounded or yeah we
recite well of course it's not infinite
and also another problem that we're
looking at is level triggered interrupts
rather than edge triggered so our
interrupt handlers run at the moment and
they'll send a message and then they'll
do cert the interrupt and and wait and
wait for another and allow execution to
happen which is where the actually
interrupt handling happens in the Erlang
part of the system but with some sorts
of interrupts called the level asserted
they won't do cert when you tell them to
so they'll just trigger another
interrupt again and again and again
until it actually has you know whatever
the device driver wanted to have or
whatever the device needs to happen to
happen so so that's something we're
gonna look
so I'm wondering if there's any like
potential upstream changes to I like
based on your experiences in this no and
that's a good thing we we are trying to
keep compatibility with the beam so we
try and keep as many changes as we can
outside of the system so that
essentially at some point you should be
able to plug a desktop Linux machine
into a hydro system and they will work
together and they can distribute work
and those sorts of things so we're
actively trying to avoid editing the
beam as much as possible but I suppose
one thing we are doing is implementing
some things that were written in bifs in
Erlang back into Erlang so there's a
sort of weird standard library the of
Erlang implemented things that used to
be in bit so if anyone is interested in
that they can let me know and I'll give
them a coffee can can you sort of
migrate applications between courtrand
or do that applications duplicate
themselves well do strange things like
that so processor mobility is that your
yeah there is no process mobility at the
moment but there was some work by a
master student from I forget where I'm
afraid but they implemented a process
mobility system that we're likely to
look at when we're implementing ours
generally the idea at the moment is
generate short-lived processes and then
they're scheduled or their position
sensibly and yeah node utilization stays
sensible but that is definitely a
problem because we don't have the beam
on multi-core so yeah we can't just
shift them from run key to another from
one run queue to another
anything else more questions okay let's
thank the speaker</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>