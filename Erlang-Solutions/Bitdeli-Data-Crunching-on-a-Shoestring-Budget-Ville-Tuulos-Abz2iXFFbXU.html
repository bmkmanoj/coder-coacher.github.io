<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Bitdeli - Data Crunching on a Shoestring Budget: Ville Tuulos | Coder Coacher - Coaching Coders</title><meta content="Bitdeli - Data Crunching on a Shoestring Budget: Ville Tuulos - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Bitdeli - Data Crunching on a Shoestring Budget: Ville Tuulos</b></h2><h5 class="post__date">2012-06-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Abz2iXFFbXU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">a couple of years I've been giving talks
about disco in a Lang factor again I was
with Nokia research on the last
September hacking disco and and the guys
are still there and well I left dude I
found a new start up start up is called
bit Delhi and it's the old related to
data and like making data processing
especially real-time data processing
really easy and basically the thing that
I want to talk about today was the kind
of the observation that i made that I
had to make when I when I left Nokia and
like we kind of well I mean like the
nice thing about startups is that like
you get this awesome opportunity to
start like doing things from scratch
which is always very awesome I mean
that's they're always the best part like
six months of the company that actually
was just hacking intensively and like
you have no limitations and like well of
course after that you've realized that
like all the kind of the heart quiet
points that after that but anyway so I
started packing a bit Delhi with my
co-founder six months ago and the idea
was to be able to kind of this like a
scalable platform for for processing
data and I will talk about more about
big deal later but the the basic
challenge that we faced compared
desperately kind of to the previous
situation that I had with disco at nokia
research was the fact that well nokia
research has this like very nice big
petabyte scale cluster running disco and
and like if you happen to have
infrastructure like that available and
like if you have it like your own data
center and your own servers I mean
things are quite well i mean it's it's
very very easy to kind of build systems
have a having that infrastructure in
mind but now given that like we are a
small bootstrapping start up with no
like not not like millions millions on
bank account so basic what we had to
start thinking is that ok so definitely
of course we want to leverage like the
public clouds amazon and an N Rackspace
and so forth but if we just take the old
mindset and like just let's say that
locate okay so we take like 100 notes
from from amazon amazon cloud and keep
them running 24 hours a day and then we
did the math and we found out that
that's absolutely absolutely kind of
beyond our budget so we had to come up
with it with a different idea and we
kind of started by thinking that ok so
what are those things that that like one
has to think about when when actually
kind of starting to build us like system
from scratch for the cloud did
a and and really kind of this is this is
the question that like you see often
with existing systems let it be disco
react or a MongoDB or whatever system
and like people send an email to the
mailing list asking that ok so how do I
run run these systems using Amazon and
like the funny thing is that well I mean
you could like mean that in the best
case I guess what would happen is that
well I mean you do exactly the same as
you you have been always doing you just
like launch instances and you have linux
servers and use the doctor machines and
so forth but of course in practice what
you see people answering and what people
have noticed is that it's actually quite
different environment so be especially
when it comes to i/o when it comes to
like the dist ability of the instances
and then various other things I mean
actually you can't just do things in the
same way so so basically the question
that then like with like asked ourselves
when we started building bit Delhi must
is that ok so kind of turning the
question upside down so that like if I
mean given that like we want to use one
of these clouds well I mean we have no
other option we can't afford like having
our dedicated machines dedicated
petabyte scale cluster from day one how
should I build my system so knowing all
these issues that kind of the cloud has
and especially now in there ok so we
want to handle big data of course the
kind of tea the constraints are a bit
different if you are let's say you are
building like a messaging application
yet like then well I mean it might not
be about big data and so forth I mean
it's it's different set of constraints
but I mean today I will people talking
about this specially kind of a
cloud-like with respect to kind of a
building really data-intensive systems I
don't know I mean many of the lessons
might be applicable to other things as
well and really kind of what we know
this is that there is this like
environmental pressure like for doing
things in the different way so I mean if
if this is kind of T the old way of
doing things I mean like I mean you have
like a certain environmental pressure
the kind of the thirsty kind of the drug
things are changing the environment is
changing so basically what you need to
do is that you need to evolve you have
to do things differently so basically
kind of what I want kind of talk today
is like actually for for like a main
observation move forward like main
constraints that we said to ourselves in
the beginning then I will like talk
shortly about like how we actually
realize those constraints and how it
ended up being in the end and of course
like well I mean coast
saying since I'm here that actually the
whole back end is written in her lung
and I will mention like a couple of good
reasons at least in our point of view
why I still think after like having the
system now in production you can go to
bed Delhi calm it's it's a live system
you can actually sign up for free and
hopefully the site is alive so that it
actually I think that that's really a
good way of building systems of the
cloud but let me start with the with
before four observations that we made so
the so the first one is maybe maybe
quite obvious so well I mean maybe then
depart not the part about like having
having short living instances but the
fact that well I mean like the nice
opportunity that you get with the cloud
is that you can actually launch instance
it's really fast so you can as well have
many of them and they're a couple of
reasons for it is so well I mean the
first obvious reason is that well I mean
that the instance is made end up being
short living even if you don't want want
that to be the case so I mean like this
is what a mess and ECT promises well I
mean it might look good on the paper
that well I mean you have like quite a
mini nines there and then 15 but
actually if you do the math it like ends
up being like 20 minutes down time per
month per region and that's like one
good thing to notice that it's not per
instance but it's per region so
basically it means that like if you are
running running instances in like let's
say they kind of did the the Oregon data
center in a mess and it might be that
actually I mean like the region may be
down for 20 minutes and and then like
still they are kind of within their SLA
the service level agreement so I mean
that's something that like well I mean
we were thinking that of course we want
to build robust system has anyone else
we are providing a platform so it has to
be robust so knowing that there is the
kind of the issue with the downtime I
mean we have to kind of take this into
account right from the beginning so
that's one good reason for what we think
you thought that ok so we want to kind
of date this approach that instead of
having something that we think that it's
like like live and up i'm in every
instance is up and live like 24 hours
like and like have like nine nines
availability whatever I mean we take the
approach that actually any single
instance can go down another interesting
observation related to these small
instances is the fact that of course
well I mean like if you have a very
dealt with infrastructure and data
centers you know that maintaining the
illusion
of stability is of course very expensive
so I mean we know that in practice the
many reasons why white servers go down
and and of course like if you are an
infrastructure provider like like AWS or
Rackspace and so forth you guys know
that that's of course like really an
expensive part of the business to kind
of people on call and like kind of its
switching discs and whatever it takes to
kind of keep the systems up and running
so the interesting thing that like what
ms and provides like nowadays is that
they provide this thing called spot
pricing so actually what the thing shows
what the craft shows here is the kind of
the last month and the price per hour
per instance and like then like
different several three different like
pricing models that they provide so the
kind of the topmost line here which is
something like I guess like it's a zero
point like it's a 38 cents per hour HD
on are the only many instances the next
line here is our a reserved instances so
if you are going to be ready to make an
upfront payment of a couple of hundreds
of dollars you can of course like if
like get like a cheaper per hour price
so that's cheaper but the cheapest price
by far is actually for spot instances
meaning that basically you are bidding
for the amount of money that you are
ready to pay for the instance to stay up
and and like for most of the time the
price is pretty stable but occasionally
that there is a spike maybe somebody
kind of its bidding more than you are or
maybe the machines are going down so
they have fewer resources available and
that's basically why why why there are
these bikes another thing is that like
if you are if you basically if you want
to minimize your cost like basically you
are kind of the trying to beat the
lowest amount or a little bit about that
the lowest amount like looking at the
historical average maybe and then of
course what happens here is that like
since you are not like bidding you know
your bid is not high enough your
machines towie I mean like basically
amazon research try to kill your
instance without any prior notice which
is of course now if you imagine that
like you run some like like like classic
system which doesn't handle this like a
sudden crash is like very creation fully
of course that's really bad but on the
other hand like if you if you take that
as one design criteria then like things
can get pretty interesting with it with
the protein pricing point of view but
this is something that I I find very
very interesting nowadays that actually
there is this like a very interesting
direct connection between the pricing
and the kind of
the price per hour for running your
system and the actual architecture so
it's not so that like there's this you
have architects like just designing the
system and then it's very hard to see
that okay so what's the kind of the
long-term kind of a cost effect of
certain design decisions but nowadays
actually there it can be very direct
like like like correspondence between
the kind of the design of your system
and the price you end up paying for
running it so that's pretty interesting
another kind of interesting point about
like having many small instances is the
fact that lets say that like you want to
get some amount of resources so let's
say that like you you know that like you
want to get like 17 gigs of ram so what
do you do so of course you can actually
go to msn and like you can get like one
high memory instance that ends up
costing something like almost like four
hundred dollars a month and that's how
you get 17 gigs and of course like now
many of you think that ok that's like an
outrageous price for a 17 geek so you
can just like to get the dedicated
machine and that's all good and fine and
much cheaper and of course that's the
case but what like where it gets really
interesting is that like if again I mean
they're like many considerations to be
taken into account but if you can
actually kind of instead take 29 micra
instance is that a cost well I'm a
little bit more but I mean still on the
aggregate level you have many more
resources available I mean you get the
17 it's a little bit more even than that
and and and now you have instead of
having like one fractionally instance
you have 29 instance instance is running
and well I mean then the thing is that
like then like depending what you want
to do you might actually want to take
like like then small or three large and
so forth and that's that's kind of
interesting that like you really have
this like a different different workload
like different server types available
but I really the interesting fact is
that actually kind of having many more
instances 29 instances versus one
instance actually doesn't make a big
difference in price but of course in the
architecture point of view again it
makes a big difference a huge difference
so well I mean the next point the next
kind of observation that we made like if
you look at the react mailing list or
like well I guess not so many people
have been running this code in Amazon
but like basically any system that
people have been running especially kind
of no sequel systems that people try to
run in a mess and usually the first
thing that people complain about this
day is the block
the unreliability of disk access and the
kind of random Layton sees that you see
and then all that stuff so I mean like
well given that like we are not in
control of the even infrastructure it's
of course hard to kind of fix those
issues per se so we decided to kind of
start thinking that ok so how would our
system look like if instead of like
using this guy oh it we would like
totally avoid using it and now of course
like if you have ever used like AWS you
know that there are several different
options for using local disk you have
the really the local local disk physical
disk on the box but um which of course
like very unpredictable I mean like it
can go down at any point i mean this
this crash and so forth and like well
I'm there like strange features that
like do that like the lazy
initialization and formatting that they
perform it actually benefits from being
initialized and that kind of complicates
things and and so forth then I mustn't
provide something that they called
elastic block story which is basically a
network attached storage which kind of
supposedly fixes the differently like
the the reliability issue because well I
mean its supposedly more reliable but
they have the the fact that it's
actually attached to you it's actually
network at that storage and like it's a
multi-tenant system many people are
using it at the same time it's very
unpredictable when it comes to like a
single like volume so what people end up
doing often and if you actually they're
like many good benchmarks like about
this and like if you run three liquid
performance what you have to do is to
take many volumes and then have
something like raid over it evolved umes
which basically means that given that
like the cost is of course per volume
and the more volumes you have the more
you end up paying so I mean that's
oftentimes very expensive and again of
course that adds complexity because now
suddenly you have this like a like a
software raid running in your stack as
well and and really kind of the thing
that's really kind of annoying in the
end is that even though I mean like it
would be really nice to think that ok so
the something like EBS actually provides
you like really good robustness that you
don't have to care about disks crashing
of course things like this happen so
this is actually an actual email that I
got luckily we didn't like lose any data
because of that but I mean given that
like you both the service and you
thought ok so now I'm using some awesome
like a sand device that never crashes
it's not
nice to get this email that just
casually mentions that well I mean like
there happened to be a disk crash and we
lost all your data and there's nothing
we can do and like good luck with your
next volume so but yeah that that
happens so basically if you want to
avoid like losing any day that
especially if you want to avoid losing
any customer data you might want to
consider different approaches so like
third approach like the third constraint
that we thought is that well I mean like
I mentioned already before it like there
are many different instance types and
the prices are actually interestingly
nonlinear so it really makes sense to
actually separate different workloads so
basically what I mean is that now if you
consider like typical application it
doesn't have to be data-driven
application per se but any other
application as well basically have like
different types of things that he needs
to do so I mean like typical application
nowadays is of course a web front-end
meaning the the engine X or Apache
server that you are running on some
server somewhere and that's of course
like something that like handles some
decent amount of data let's assume that
you are not doing crazy cashing on that
but I mean of course you need some some
amount of RAM and and then like like
resources like do to kind of handle the
incoming data the latency requirements
of course pretty pretty strict since of
course you want your service to be
really responsive so I mean when
somebody loads your front page it needs
to respond in in like basically it's
just like tens of milliseconds and of
course like given that like of course
you are creating a popular service you
can get like thousands of requests per
second and first like what's really
important is that lack of course if you
have this like a one web front end and
it fails and like suddenly your site
becomes totally unavailable that's of
course totally horrible and that's the
last thing that you want to happen of
course in practice you want to have like
many fail overs and so forth but that's
really kind if you don't want to kind of
a crash every day the web front-end so
that's that's one layer that's one type
of workload then interestingly in many
applications you have this kind of a
second layer which is still low latency
like I mean we want to kind of give
results in less than a couple of
hundreds of milliseconds basically
meaning anything that like should
respond to a single HTTP request now if
you think of a popular service like
github for instance so and they have
this like visualizations where you can
go to your repository and see like oh
in working it and so forth I imagine
that this could fall into that category
of course they might be cached most of
the time but in practice I mean if you
have to generate something like that
visualization on the fly you want wanted
to be done in a couple of hundreds of
milliseconds not quite as fast it's
loading a single resource over HTTP but
still it should be pretty fast so people
don't have to wait for for a long time
and and again I mean that's not like you
are not probably not dealing with huge
amounts of data and like in that case
like like failing that well I mean like
you can have like redundancy I mean
given its back-end service totally I
mean it's kind of easier to handle and
in many cases like one feature failing
on your website is not as bad as the
whole website failing so I mean that's
not quite as bad then like especially
data driven applications first there's
the whole layer of actually doing the
big data crunching and like taking all
the data that you have received and like
doing some awesome stuff for it so like
mapreduce definitely falls into that
category and then of course basically
the amount of data if you handle can be
unlimited in our case well I mean our
service happens to be such that most
likely we won't be handling more than
like a one terabyte per one customer say
but but still I mean the kind of
basically it would be could be anything
but in that case of course the latency
isn't an issue that's pretty much the
definition of batch processing that
nobody is waiting there and like I mean
people go to have coffee when they are
running the MapReduce shop and so forth
so it's it's not so sensitive to latency
which is of course kind of totally again
like fxdd the design and again I mean I
first light most likely you are not like
running many of that requests per second
so at least not that of that scale but
then like an interesting fact is that
even that like you are crunching a lot
of data and like and then like you you
have potentially you are kind of
consuming CPU resources and so forth at
MIT might actually end up costing a lot
of money and like I mean like resource
why so it's just like simple needs but
of resources and then there is the kind
of the interesting last year which is
the fact that well given that like if
like your if your service is made of
those like a three main components then
of course you have the kind of system
running so I mean oftentimes what you
want to do is that like you have this
kind of like a testing or a best-effort
here basically that you can use to kind
of test things and then and so forth on
the background even that there are any
available resources but if some customer
happens to be doing something then of
course you don't want to steal the
resources from them so that's the best
fourth layer and now of course this is
like very much dependent on your
application how you want to map these
two different different instance types
but like for instance like one
interesting observation what we made
with our services that well I mean we
have like definitely like low latency
requirements on the back end side so we
have to perform certain operations in
less than 500 milliseconds but the
operations that tend to be really spiky
so I mean like basically you kind of run
like like some piece of code for a small
amount of data and and Pedroia mean
really doesn't last for more than a
couple of hundreds of milliseconds which
actually happens to be a huge match for
the micro instance that Amazon is
providing for really cheap price because
this is something that like of course
like not many people can actually
guarantee on their like a like a design
of their system that actually the load
will be really spiky but again that's
the point that if you are designing your
system from scratch and if you can
actually take into a car take into
account right from the beginning that
actually well they are providing these
instances that actually make it very
affordable to write run spike workloads
well I mean what if we actually design
it so that we can really separate those
spiky workloads and like take advantage
of the packet you can do it in an
affordable way and then like it the same
way I mean like in the in batch
processing well I mean if you are really
like really betrayed latency do it do
you kind of do to kind of the money that
you end up paying again I mean you can
actually maybe deal with like not huge
cluster instances that cost like a lot
of money but actually maybe some smaller
instances so that's like really
important in cost perspective that
actually you really can design your
system hand in hand in hand with the
fact that ok so what kind of resources
what kind of infrastructure like there
is available then it also dating the
cost into account now the oh ok so right
so this is like actually well I mean
this is like an interesting thing that
like I know this when we started of
course like it was like very much common
knowledge that actually something like
AWS is actually kind of can end up being
pretty pricey and like it's looking at
like single instance citizen and so for
then of course we did the math many
different versions and life like many
different scenarios and but I mean just
like looking at like for instance like
at this basic question that like if you
want the single instance with one
gigabyte of RAM so what do you do so so
basically I mean
this up like just a few popular popular
providers I mean I'm sure that like you
can find one like like a hosting
provider that like provides you one gig
gig of ram for less than like 31 dollars
per month but I mean this happened to be
the popular choice is enough first the
other considerations you might not want
to go to the most obscure provider in
the end of the day and like basically
the interesting fact about this is that
like especially for the past year or so
that the amazon has been pushing the
prices down and they have been
introducing new like layers of pricing
like different layers of like reserved
instances and so forth they might
actually end up being so that like that
like if you design your system careful
actually might be a very cost efficient
solution in the end of the day as well
and of course well I mean you should do
your math and like of course like amount
of RAM is only one consideration but but
it can be cost effective so now actually
this is this my like favorite point and
this like very much I twice in two Erlin
so of course the kind of the main point
of like building a system like this well
I mean like let's say that like if you
are a research environment maybe what
you want to do is just focus on like the
massive scale of data crunching but even
that like if you are providing a service
to your customers really the most
important thing that like you want to
provide is the high availability of
robustness of the book of the system so
you absolutely don't want it to crash
and that's the first like why people
like Ireland so that's a very nice
language for doing that and and that
makes life very interesting in light
cloud point of view we're like so I
showed before I mean many things can
crash and many things can go wrong so
now a very interesting question in like
system design point of view is that like
even dislike very fragile and very
heterogeneous environment how do you
actually design your system so that it's
super robust and actually an interesting
concept that goes beyond like robustness
is this concept of and the fragility and
basically it's something that I stole
from this like famous famous author who
wrote the book about Black Swan and so
forth and he defines the kind of the ND
fragility by saying that well I mean
it's something that actually it's not
only robust in its environment but it's
actually something that's that's like
designed to be mishandled and designed
to be handle
carelessly so that's kind of a very nice
definition for a system that runs in the
cloud where we know that actually kind
of like the cloud provider can be really
mean and kill your heel your instance it
at any point so basically your system is
not only robust but it's something that
like actively copes against against
hostile environment and this is a first
like like nice connections to do the
kind of the biological systems that kind
of very much operate in the same way and
well this is like like I mean they're
like many ways to approach this question
but but like one way of looking at this
is like a very operational so now
thinking that like you have a system
already operational and you're running
something and let's say that you are
running it on a dedicated machine now
let's say that like instead of just like
machine crashing and like like there is
a hardware failure and like it the
machine totally died so well that's kind
of an easy case but I mean like kind of
am Ness staircase is that what
oftentimes happens in more complex
systems is that actually the system
doesn't crash per se but but it's just
like becomes more sluggish and like slow
and like somehow doesn't like work why
does it shoot and so forth so another
question is what do you do so well I
mean if you are running like things and
dedicated machines and like let's assume
that there might be some let's assume
that the problem is not in your software
per se but it's some like environmental
issue you like something is going wrong
wrong on the switch level our let's say
that let's say that like somebody
actually you have like like like kind of
like a backbone in your switch and
somebody kind of the cough the
connection or something like that so
that it kind of doesn't work quite as it
should so basically kind of if you
happen to be lucky enough that actually
you don't have to fix the machine by
yourself you call the data center and
there's like some guy with remote hands
and like tries to do something but I
mean in the worst case especially if
it's weakened I mean it might take days
depending on your support contract so
and it's it's very manual process and
like very not very nice especially if
your environment is very fracture now if
you have a if you are running a virtual
server let's say that you are using
lion-o door or many other like like of
this like a standard like virtual server
providers again I mean if you have this
issue I mean most of the time it's
actually due to the fact that like it's
a multi-tenant system and somebody else
is on the same
physical box doing something crazy and
like eating up your resources and that's
why they let's say that the disk i/o is
sluggish so again I mean you call the
support and say that well I mean my
instances is acting funny Lee and like
then they will move your instance maybe
on the fly to another box and like in
the best case that can be done in hours
but again now well that's of course
something that it takes resources and
it's not nice especially when it when
you happen to have many instances
running but the interesting fact about
the cloud is that given that it's so
fast it kind of launched new instances
you can take a totally different
approach which is that instead of like
trying to fix the problem you just
accept the fact then like you see I mean
like let's say that you are using Erlang
and light you have this nice concurrent
heartbeat process that checks the
Machine and checks the like different
like health factors in the in the in the
system and notice is that okay something
is going wrong so what do you do well
I'm you just kill the instance so I mean
you do the same thing that the provider
would do instead like well I mean you
can do it automatically that like you
just get rid of the instance and you can
be extremely opportunistic that you just
launched a new instance on another box
and like if you don't happen to be lucky
it might be up for something that's
region-wide and I mean doesn't fix the
issue and you might be even clever
enough to try a different region but the
thing is that like in the end of the day
most likely eventually the kind of the
situation gets fixed and like like the
kind of DD you didn't have to do
anything manually and that's that's like
very interesting like changing the
mindset that like you can actually build
your system so that it's self healing
your decree just as an artifact of the
fact that like if you consider it right
from the start that actually the whole
environment is hostile well then you can
retreat the environment as hostile so
whatever bad happens you it's just kind
of killing and that kind of course like
really kind of place nicely with this
idea of crash oriented programming which
is like very well known in the Erlang
space so next let me just quickly go
through the kind of the the actual
architecture that we ended up end up
using I mean based on this observation
so so I mean just quickly before i go to
details so well actually that's a bit
delete there in the middle so like how
the our system works is that like you
can actually send events either from
your web application at the back end or
wherever you like simple chase and
messages power race
end point we received the event we have
this layer that scales nice we can take
thousands of events per second and like
what's really kind of a nice about our
service is that you can actually write
the script in Python that we keep
running in the cloud again that takes
your data like in real time and just
some processing may be something simple
like a simple format conversion or
something more complicated like let's
say like even like sentiment analysis
using let's say natural language toolkit
for a Python and then it like produces
the outputs that then you can access
over another API and then get to access
to the chase and results so basically
what it means is that like you want to
build a custom dashboard for your
internal analytics or where you want to
build a let's say some like a fancy
solution for tracking sentiments of of
what people are saying about you in
Twitter I mean this is really the
easiest way to do it but I mean like as
you can imagine of course we have a
couple of issues here so we have to make
sure that like in every possible
situation we can receive the events
whatever happens to the machines we can
keep running the scripts that the user
has provided also taking into account
that it can be arbitrary Python code so
it can do anything crazy I mean II like
it like as much memory ESS there happens
to be on the instance and anything can
happen and and also of course that we
want to make sure that like if you
create like if you put your
visualization on the front page of New
York Times weekend we can serve it so
they are like interesting architecture
questions and so this pretty much the
kind of the decline of the architecture
so it's it's it's quite simple so the
kind of the thing is that like we have
the kind of the coordinator process in
the middle which is like written in
Ireland and then we have this like small
processes and like well I mean they're
not exactly three of them but are but
many of them and it really depends i
mean the ole is everything is very
dynamic and that's that kind of ties
back did what i mentioned about having
in many small instances so so instead of
having like kind of dedicated server to
do this thing and dedicated server to do
that did actually the kind of the the
coordinator guy here basically kind of
launches every possible instance only on
demand so if the instance goes down it
notices it and like it launches a new
instance so in a way it treats treats
the MSM cloud almost SS like one big
Erlang vm that were the kind of things
are linked so that like these are almost
individual Erlang processes but actually
in this case they happen to be
individual instances in the in the cloud
so if anything goes down it's just like
a launch is a new one or if there's more
demand for for certain resources it's
just like launches more instances and of
course that's exactly what what the kind
of plows were made for so if you are
just using the mast as like as a simple
hosting provider I mean well that's
that's not the best way but really the
nice thing is that you can you can do
things in this way nowadays the second
thing about like the disk i/o what was
really something that we wanted to avoid
and well I mean I first kind of if you
want to persist something in the end of
the day you want to put the stuff
somewhere and like well i mean amazon
happens to provide history the simple
storage service which is an interesting
interesting abstraction gives you
instruction of persistent storage but of
course it's only eventually consistent
and there are many tricky things about
that too but i mean basically what we
ended up doing is that we try to keep as
much data in memory as possible of
course that that's also something that
like you need to be careful about when
you know that like instances may go up
and down at any minute but then every
now and then we flush flush the data to
s3 they not in a custom structure that
that can be accessed easily but this way
we can like to a high degree we can
avoid like using any local disk which
actually makes things like very very
nice since it's kind of funny thing that
like using usually the network interface
is much faster than using the local disk
so if you just keep basically you have
this more instances to basically act as
buffers or or cash is forever incoming
or outgoing day that then well I mean
you have just this like small caches and
then like you fetch data like on demand
basis from history so so then like the
question about like the separate
workload so well I mean like like
different classes of of instances and
actually there is the kind of the
coordinator guy here in the middle takes
care of of scheduling so there is a very
simple schedulers that like knows that
ok so I mean now we need resources of
this type and like do we have any
available resources so or not and if not
if there aren't enough resources then it
launches new Munson and so forth so of
course an interesting question is that
well I mean this is this something that
we really kind of also wanted to make
sure that well the very annoying fact
about the AWS DMS on a lease that
actually you can't set a credit limit
for yourself so they just take your
credit card number
and then like well of you go and and
then like if anything happens to go
wrong in your code and like you end up
launching like really many instances
like you might like wake up next morning
seeing that actually a bill runs in like
thousands of dollars so it's very
annoying that they they haven't fixed
this issue but I hope or first it's an
obvious that like why probably they
don't want to do it so I mean but you
have to careful about it so I mean like
don't put like don't like launch the
instances in an infinite loop or
anything like that now of course the
question about like the ante fragility
and robustness well I mean like if you
look at the look at the previous slide
and then like you see that okay well I
mean step pretty kind of a typical thing
you have the coordinator process the
master process and of course now you
think that if the master goes down
everything goes down but actually the
beauty of this approach is that like
actually all these machines are
independent so even if the coordinator
process goes down these guys continue
doing their job quite as usual so that's
that's really nice about the fact that
actually they have different physical
boxes even in different physical
geographical regions that are that
actually they can really operate
independently and then of course
eventually the coordinator comes back
and sister like no this is the situation
and light then fixes the thing but I
mean that that's really part of the the
ante for chili that not only you can
like a kick-out any of the single
instance is here but you can even like I
click away that the big guy in the
middle and then still the kind of the
team keeps operating and that's that's
really that makes the system really
robust software is another thing is that
well there is the simple storage service
that's kind of like I mean like the
older all the kind of the components
access its of course we assume that
that's something that like I messin
takes care of like keeping running and
it worked pretty well this far but
that's another of course assumption so
now like that was the architecture and
those were the observation now just like
couple of points like based on the kind
of the recent experiences we had about
Erlang when when building the system so
by the way how much I have time left so
just a couple of minutes 12 minutes okay
should be enough so so well I mean there
of course like many many like answers
that like why did you sir lung and and
like actually well when I started to use
Erlang of course the kind of the
official answer and the kind of thing
that was everybody was saying is that
along is really awesome ability
concurrent systems and like if you want
to do many things in parallel and then
and then so forth and you want to send
messages back and forth you should be
using early well more and more
especially like due to the fact that
like that now there are many other
languages like golang from google and
then scala and well even rust from
Mozilla and like mid language cluster
that actually provides the same
functionality I mean of course well I
mean probably many of us have been
asking ourselves that ok so why am I
using our line so should i be using
scholars should i be using golang or
should i be using posture and actually
well so the answer is not so much said
well I mean you can do concurrent stuff
with Ireland but they actually a couple
of reasons why again I like after
building the system and after seeing how
it works I mean I very convinced that
actually Erlang is the way to go and and
like really the first reason is simply
the failure models so the kind of the
fact that actually if you look at the
the architecture here it's just it very
much like fits the mindset here and
that's that's a feature that like of
course you can most definitely simulate
the same idea and the same same same
same fourth model in different languages
but it just happens to be so very
convenient that it's so much like built
in the Erlang culture and the antebellum
components that actually that's that's
the way how you build systems so that's
definitely kind of one of the biggest
reasons why why I think like Oh long is
a good match for building systems for
the cloud and for handling big data
which by definition almost nowadays
means that actually you have to do like
horizontal scaling and cope with the
fact that like things go program and so
forth and and the second thing maybe
even more importantly importantly its
simplicity and that's simply because of
the fact that of course well I mean
doing something like this would be of
course very awesome and I especially
having this like very biological
antifreeze oil system that just doesn't
go down ever this of course very nice if
it was really easy to do but of course
it's not easy to do it's it's super hard
and and like at least I think that it's
very hard to do complex things with
complex tools so I mean like really
that's that's really the reason why I
think that erling is really kind of
exceptional in the sense that you can do
stuff like this with a simple language
and look really if you look at any other
language in the same space you don't you
really don't get the same same level of
simplicity to model complex problems
that's really great so now some lessons
learned so
that that's actually something like i
mentioned i mean like each of the small
boxes that you saw there are actually
running along BMS when we were like
working with disco and what discos still
does the disco MapReduce is that it uses
like distributed ireland in this case
well i mean that the whole system is so
so kind of dynamic actually kind of what
we ended up doing is to is to actually
use simple HTTP between different
different instances and and that
actually kind of seems to work very well
it not only do the fact that well I mean
it's easier maybe to handle like a
really large networks of these things
but also the do the fact that well we
have noticed that like I mean like the
Erlang virtual machine can crash not due
to the fact that along might be buggy I
mean that occasionally happens as well
but like really really but even like
more often due to the fact that actually
there's something funny going on in the
environment that like just kind of makes
life very hard for the Erlang virtual
machine so one thing is that well I mean
would like this is a first not specific
to Ellen but any system that like
basically on a single instance you I
mean if you're running like the system
and your local laptop and especially on
linux I mean Linux is very nice like
overcommit feature so you can just keep
allocating memory as much as you like
and like things work or at and the worst
case I mean you start swapping and
things get slow but often times in these
like cloud environments actually you
have a hard limit for the amount of
memory you can handle so basically if
you try to allocate if you mal look more
than like certain amount of memory then
just crash so so we have seen that
happening another thing is that like
it's very easy to run out of like all
kinds of resources not only five the
Scriptures but like that's that's
something that happens and especially
like well the fact is that like still be
kind of the Erlang environment sits on
the disk and like anyway I mean like the
disks are very unreliable so if anything
goes wrong I mean like the disk and
hanging and like the latest is going to
be huge and the whole system can get
really unreliable so it would be just it
is easy to kind of have this concept
that even the early in an individual
learning machine along virtual machine
can crash and of course then when you
are doing many things simultaneously and
the same machine it might be that it's
just the other like the rogue process
that actually eats up the resources and
like makes makes again life hard for the
on virtual machine so that's that's like
one thing well this is more specific
doing like really
like big data stuff and then and really
heavily kind of the data oriented stuff
when you potentially handle like lots of
stuff in your allen burgee of machine
well it goes without saying that
binaries are awesome so I mean it's very
nice way to kind of not just the fact
that well you can do binaries but
actually the fact that you can actually
use binaries in the pretty clever ways
to pack your data if you happen to be
very very kind of memory memory
intensive and we want to be efficient
efficient with that of course that goes
without saying that well that's of
course an optimization that you should
don't you should do only after you have
noticed that that's actually an issue
but it's really nice that you have that
tool at hand when you need it and also
actually another thing that I have
learned to appreciate the fact that like
when you need it actually you can create
like a few you can write a few like very
tactical nifs I don't think that kind of
the right approach is to kind of start
creating like large portions of your
system as an if even though it might be
and if it might seem as a performance
bottleneck but I mean doing very small
like library functions I guess that's
what nifs are for and like basically
offloading certain things that would be
very hard to do in a long in an
efficient way you can just use take an
off-the-shelf let's say some like a
super efficient library from google that
last something and you can package it as
an if and like wrap it in a changeover
and and then like suddenly kind of many
things many many tricky like like let's
say data structures become become much
much simpler to implement so this is
this is just I don't know I mean this
might be something that like does not
apply so much generally but actually
kind of a one thing that we noticed that
really helped in in designing the system
is that actually well I mean of course
like we have like a huge number of
ginger versus everybody does but so but
what also seemed to be a good idea is
that like they are pretty independent so
we tried to minimize I mean it kind of
goes back to the kind of the whole
mindset of having this many small
instances many small small short living
instances and now you know what happens
if you have like this like it many
linked like st. gen server instances and
if one of the one of them goes down of
course you have this like a domino
effect and that's exactly of course what
should happen and then many things can
go down and so for it and of course in
the worst case if you're if you haven't
like designed your system careful it
might be that like like like Indian like
basically kind of went is like a
Domino's go go down and down the path I
mean like eventually or
whole system is down so and even if it
comes up after after sometime it would
be nice to kind of have this like a
circuit breakers in the middle to make
sure that like basically kind of things
they reasonably independent another kind
of a very kind of a nice feature that we
found out to be really helpful
especially with respect to vote hot code
upgrades is the fact that actually
change servers when they launch and when
they start they actually can reconstruct
their own state based on some let's say
that like some external resource like
the simple storage service so instead of
us having to kind of somehow we really
carefully manage the state in a chain
server actually we can just kill the
chain server and like the changeover
Aris capable of just like a fetching
it's like that whatever it happens to
need for each state and and then it
basically kind of can can like like like
become a live without any external help
so that's that's that's really super
convenient then that kind of saves you
from the trouble of like writing
functions the kind of support from one
state to another state if you do hot cop
code upgrades so now let me kind of this
is kind of a fun thing that I that I
kind of wanted to kind of a challenge
you both so I mean it's kind of called
airline and differentially to challenge
so you might know they take the decayers
monk if you have been reading the kind
of the blog articles by Netflix where
they're going to introduce the idea of
of chaos monkey this little process that
actually kills we saw gills like servers
in your in your cluster like every now
and then randomly and you have to build
your system so that it actually survives
the chaos monkey doing its job actually
another interesting version of chaos
monkey for Ireland which I call chaos
monkey on LSD is is the kind of this
chaos monkey that actually goes you're
like a long process and actually
randomly corrupts the state of a random
like change server and now an
interesting question is that well I mean
you might we might like discuss like
whether this happens in practice or not
well I think that it really well might
happen and and the question the
interesting question is that of course
well I mean the changeover in question
that happen to kind of be a victim of
chaos monkey I mean like of course
probably goes down eventually but now
the interesting question is that does
your whole system like slowly
disintegrate over time if this happens
so I mean like the typical example might
be that you have this change server that
lets say handles like database
collection database connection
and and now if the KS monkey hits you
and and like corrupts your state and
like then well I'm in the stage somehow
kind of the maps do the database that's
persistent and and then like kind of the
database it's corrupted and then other
processes are using the database and
like over time the whole system
disintegrates and like falls apart so
that's of course something that you
don't want to happen so but I mean the
question is that can you actually build
your system so that even if this happens
and even if whatever happens whatever
random happens and and like the kind of
that you end up having a like corrupted
chain server your system somehow kind if
it might actually kind of have it like a
some part that like it doesn't work or
maybe one customer like it's not able to
access their data or something like that
but the idea would be that actually the
whole system to didn't go down and they
really kind of the kind of the effect
shouldn't shouldn't kind of a percolate
through the system but it should like
stay contained within a within a certain
area and it's kind of an interesting
interesting idea if you think about it
and think I in terms of your service
that like like like how does it actually
react to a situation like this so that's
pretty much all i had to say today so
yeah i'm happy to answer any questions
thanks yeah well i mean well they are
like number of things so basically like
in our case we the data eater game so
the state teacher comes from the simple
storage service that it's actually kind
of just like the chain server knows that
ok so i should be accessing resources
resource and it goes to history and
light gets whatever they that needs to
get from there another cases that
actually let's say in the case of the
coordinator during the middle it
actually can access the dimsm api's and
ask that ok so kind of tell me the state
of the environment and what instances
are running and then it kind of
reconstruct its state based on that so
of course it depends depends on your
environment where you get the data
and so basically we are using VMs on
history as asteroids back answer well I
mean basically well I mean like they're
like two of us working for a bit Delhi
and like like my co-founder is working
on death on the other interesting
problems that are on the on the client
side like javascript stuff and I was
like responsible for the back end so it
was like six months full time works six
yeah two people yeah but I mean it's a
start up so I mean like we're sitting in
the garage hacking 24 hours yeah well of
course the thing is that like well I
mean like their offers different ways to
kind of like tried to deal with
corrupted data but actually what we do
is that we don't we don't deal with
corrupted data per se what we do is that
we make sure that like the corrupted
data stays as isolated as possible so
even like a personal customer we want to
make sure that even if there happens to
be like a local corruption for data I
mean like we minimize the effect of that
corruption so we can access all data
before and all data after that but I'm
there might be some limited amount of
data that's corrupted well you could
basically we rely on like the ds3 to
kind of keep data somewhat like not
corrupt it but I mean like yeah you
could be doing more but really what we
wanted to do is do this to kind of
isolate and like make the effect that's
as minimal as possible and not kind of a
make sure that like we can fix every
possible corruptions
yeah yeah so I mean like a first well I
mean you know I mean when you are
building this like like highly available
systems you kind of go through all
possible things that can go wrong let me
think that what was the specific I mean
basically in every kind of signal
component we were considering that like
I mean like especially when you when you
kind of a design your system with the
idea in mind that like anything can like
kind of fail at any possible time so
then like basically for every change
over you have to think that ok so let's
say that like if this thing happens to
die at this point so and how would it
actually affect other things and and I
don't know if there was any kind of a
spell at least there wasn't any kind of
a horrible incident like memory getting
corrupted and the beach split so yes
there
yeah oh yeah yeah okay so sorry I mean I
well I should repeat the question so
what is the first question again so I
guess okay so what kind of algorithm i'm
using for scheduling yeah that's a good
question so basically it's a rather
simple algorithm that actually i mean
they did it's a modification of like a
first in first out but like taking the
account like a number of constraints so
there are some like locality constraints
owed some things need to always run or
would pin it would be beneficial to run
on the same box and and then the other
thing is to kind of like taking into
account the country sources that are
available so i don't know if your second
question so what was the second question
again here oh yeah yeah right yeah well
I mean it's okay so let me go back to
the conversed question still so I mean
like you kind of it's very true what you
said about the kind of thick that it's
of course like a very very kind of
import and in cost efficiency point of
view and that's also another thing and
especially it gets interesting when you
do spot pricing that actually since I
mean I mean like you saw the graph and
there are certain spikes of course if
you if you know that certain types of
instance is at certain point of time are
very expensive so of course your
scheduler shouldn't start using those
instances so you should avoid doing that
so basically we have a time in this of
course somebody who has done like some
like who has been in the financial
industry public can come up with a
really efficient algorithm for doing
like really good spot pricing but we
have pretty simple model that just like
make sure that we don't like spend like
huge amounts of money on like stupid
things but basically that's also
something that the scheduler takes
account and then like going back to your
second question about the is it single
point of failure well I mean it's not
single point of failure in the sense
that like the whole thing can actually
survive the kind of the machine going
down actually how it works currently is
that like like I mentioned in the other
slide about Li like different different
workloads so actually our like web
front-end currently actually runs online
also for the fact that actually did it
that kind of the resources that we need
I mean it's it's cheaper to kind of get
needs to be up 24 hours and anyway I
mean that's not the part that like we
keep kind of digging up or up and down
all the time so basically couple of
times it has happened that likely kind
of the central process has gone down and
actually it doesn't affect the systems I
mean kind of like within like I guess
currently it's not like 12 over a period
or so so if there's somebody who can
like fix the situation in 12 hours then
it's okay I mean like you could possibly
increase that but that seems to be
working about the walls yeah yeah
like like this like okay so the question
is that like why we are using HDX elites
HTTPS and not like distributed Erlin I
guess that's the question well I mean we
we were using and disco is still the
disco mapreduce is using distributed
along and it works rather well when your
system is relatively stable when when
you have like let's say you have your
own dedicated boxes and so forth but
what we know is that like even in an
environment like that it can kind of get
tricky at times especially if you have
anything kind of resembling network
partitioning I mean timeouts happen and
like I mean it's kind of hard to rely on
let's say something like just like
monitoring the kind of deep denote
monitor so that like you can like you've
basically assumed that you would get a
notification when an old goes down so
basically kind of we we ended up using
and well I mean then there's the other
issue of course is that I don't know our
guest actually that might be fixed by
now in the air long virtual machine but
at least it used to be so that they of
course the traffic between the nodes in
this stability along is not secure so I
mean it's just using plain TCP and of
course that's that's an issue in in a
public cloud so we wanted to use HTTPS
and it seemed easier than like trying to
build something like disability over SSL
which actually nice now might be working
I don't know that that was like one
consideration at the times yeah thanks
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>