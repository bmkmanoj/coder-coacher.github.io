<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Lambda Days 2018 - Wojciech Gawroński - Functional Programming in Serverless World | Coder Coacher - Coaching Coders</title><meta content="Lambda Days 2018 - Wojciech Gawroński - Functional Programming in Serverless World - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Lambda Days 2018 - Wojciech Gawroński - Functional Programming in Serverless World</b></h2><h5 class="post__date">2018-03-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/RTX0hrt5sFw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone I hope you are hearing
you well at the back give a sign okay I
hope you are also enjoying conference so
far I would like to thank you for being
here this will be a personal story how I
try to onboard a couple of functional
programming languages when it comes to
server less computing how the landscape
look like looks like a rut right now and
it will be in a form of a story of
personal experience with some evidences
I did some measurements test scenarios I
can get some tips I made the talk as
much beginner friendly as possible so if
you don't know anything about serverless
you are also good good here a little bit
about me
sorry for technical problems yeah so for
those of you who don't know me or didn't
read the conference page my name is
Wojtek I'm also available with under
nickname of France came over Internet
I'm a software and operational engineer
in up the scale we are a small
consulting company focused on real-time
bidding systems real time scaling cloud
computing and so on I'm majorly
responsible for writing code in Erlang
in the next year but also doing DevOps
and working with AWS cloud as well
obviously there must be a shameless plug
at the bottom of the slide you can see
that we are hiring if you are interested
in such topics let's have a talk
afterwards I would love to answer your
questions I would like to start with
providing common ground for everyone and
discussing what is serverless in the
first place
and I will start with a little bit of
ranting because as IT industry I think
we suck at naming there is very famous
joke about there are too hard problems
in computer science cache invalidation
naming things and off-by-one errors so
basically with several as it also looks
like this like the picture you can see
here it's a noisy overhyped word
at the moment it simplifies the reality
and many things are actually confused
what it really means so probably nothing
unclear for you but at the end we've
helped you have servers down there
we need servers we need hard work to
execute our code that's obvious we also
need operations people operational
people
someone has to monitor manage that
infrastructure configure those services
for us and I think that the better
description the service several as buzz
word is when we look into the brief
history of computing cloud computing and
that's national evolution basically so
we started with renting Hardware and
constructing our old data centers then
we moved into the more consuming aware
of consuming way of for the
infrastructure when it comes to
infrastructure as a service then we've
got platform as a service and finally
several S which is the let's hope it all
teammate way of executing programs on
someone else's infrastructure basically
it's a transition from owning to
consuming and server less is hopefully
the last step as well in the current
landscape we can distinguish two flavors
of server less computing one is called
back-end as a service and if you use
parse or firebase you basically used
that particular flavor also there's very
important second branch of that server
less computing thing called function of
the service and it's basically way of
running our code on demand on someone
else's infrastructure and do not worry
about operational hassle related with it
and the first word from the acronym
suggests that we as a functional
programmers are in a privileged
situation because we means that we can
think in a unit of execution which is a
function which is not a real thing for
us and is that true is that really true
we will address that in the in the
presentation and that's also my concern
that
it's not exactly that way I think that
there is a plenty space for optimization
and doing stuff better let's start with
also describing how the server less work
and how it affects the pricing model
because that's very interesting for us
so at the beginning we are preparing
source code and bundling it into a
package is it is either zip file or jar
file and that particular package
combined with the configuration in the
in that sense I mean the amount of
settings that we need to provide to the
provider about for example how much
rummy we'd like to allocate package and
the configuration together are uploaded
to the service and from that particular
step we are basically off everything
else is handled by the provider so it
means that scaling infrastructure up and
operational hustle is done on their site
it means also that we are giving up
control because in exchange for
receiving such nice capabilities like do
not worry about operations we are
basically skipping that particular bit
about choosing the operating system or
choosing the hardware on which we will
run that it is either a benefit or a
problem for us depending on the context
and that also affects the pricing model
as I said so I hope that is visible on
the diagram that we allocated and
created a execution unit that consumes
half gig of ram if our build time if our
real-time is basically 120 milliseconds
we will be rounded up to the bucket of
100 milliseconds and that to 200
milliseconds is basically our build time
so will be built by the execution time
and the amount of RAM that we allocated
for that lambda additionally we are
paying for how many requests we are
actually consuming with that lambda as
well but those amount that that amount
of requests is but it is batched into
very huge amount like million requests
and so on that's obviously being tricky
when it comes to combining your lambdas
and executing them but the the major
problem with cost-effectiveness in that
scenario is that we need to optimize for
execution time as well and also for
memory footprint so knowing how it works
and what is it let's talk about why we
bother with well why we should bother
with this approach and I wanted to
prepare a couple of slides here but
there is very very fantastic paper
prepared by go cottage and Robert jatlee
about serverless computing and how it
effects three areas basically
cost-effectiveness architecture and
operational factor if someone would
would like to have this paper as a video
or talk there is a link also to the go
to go to call from last year performed
by Geico
it's very nice talk I recommend if you
are interested in the service computing
to watch it I read the paper
I analyzed it I prepared three quotes
from each area that it covers which
basically provides enough information
and builds up the context if you are
interested in that topic and
cost-effectiveness is basically an
essential aspect of that paper and it's
even reflected in its structure the most
critical element is that paper is
basically the table that compares
several as computing options in
comparison to platform as a service or
infrastructure as a service Elevens and
as you may see here our reference price
is basically lambda with half gig and
everything else below is basically
either platform as a service solution or
infrastructure as a service solution
that it's much more expensive but it's
also very important from the tail
is that Lamba includes failover costs
when it comes to the other platform
other solutions we do not we need to pay
it separately so it means that by giving
up some control over neat nitty-gritty
details we are also buying in something
else which is in that sense a fail and
fail over operational costs and
theoretically smaller amount of money
that we need to pay for execution
additional thing is related with
architectural impact so this is very
extensively covered by this paper as
well and the most predominant one and
the the most important statement from it
is that it allows better modularity for
our programs if you squint this posted
quote may look like a microservice
architectural definition so it's I know
that it differs but basically it's it is
very similar to the premise that
micro-services architecture are giving
and wouldn't it be useful to have such
elasticity and less operational burden I
would buy in last but not least I would
like to invest emphasis that it's not
reflected in the title but this topic
screams from between the lines
operational context and operational
costs are very important for that paper
as well and there are such statements
like this
supported by evidence and measurements
that are saying that for some cases they
are basically saved ninety five percent
of their cost operational cost after
moving to server less so that's very
important as well a key thing to
remember is that applicability and
context is the key so if those
definitions sounds familiar for your use
case maybe surveillance will be a good
choice I will show another example that
choosing server less will be a very bad
choice you may say that the
it's only one paper and you are right I
would like to support it by real-world
examples those companies are using
server less computing it's very
important to notice that use cases are
pretty much different and they are
ranging from fully functional
subscription-based MOOC platform or
online collaborative collaborative mind
mapping tool as well knowing what and
why I would like to talk a little bit
about tech and finally go to the meet so
to the programming and functional
languages so at the beginning when i
interested in started to interest the
the getting interest into the server
less computing i made the list of
functional languages that are available
in the landscape and i focused only on
languages that are forcing me to use a
functional paradigm i deliberately
skipped languages that are
multi-paradigm because i wanted to focus
on those that are forcing me to to do
functional programming i compiled the
list and i started to wonder which ones
i can run on the lambda in that
particular session on AWS
and I started to graph and and look how
many options do we have
provided by the platform and it looks
like that in most cases they are sharing
the same runtimes like no js' dotnet JVM
but the key thing here is that we are
basically limited from the start when it
comes to languages that we can use on
server less and that's that's bummer
in my opinion another thing is that from
those runtimes that are available here
only dotnet is basically treating a
functional paradigms and having
optimizations implemented inside the
Virtual Machine JVM does not support
tail call optimization yet no J's
supports them but in many cases causes
the the optimization if you are for
example using exceptions or something
else it will basically depth
and that's the v8 fault not the nodejs
itself so it means that we've got
limited choice and limited runtimes but
we can cheat we can cheat a little bit
additional think that I have to mention
is the shame approach which is a little
bit as I said cheering it's a well-known
workaround when you are able to run any
executable that is compiled to the Linux
infrastructure and then wrap it around
with some small program written in
node.js or Python and redirect input
outputs in order to interact with it
many people use that for using and
dropping go executables before they were
supported I used it in order to support
Haskell and that's the only exception
that I did for platforms that not
directly supported during my tests so
limited by the runtimes and limited by
the amount of cheating that I can use I
chose those languages to test and
compare between you may see that the
Haskell as I said is the only exception
from from the rule that runtime is not
supported directly and I used shims for
that I choose also pure script buccal
script closure script from the node.js
runtime f-sharp from dotnet and skull
enclosure from JVM I have to drop the
the Erlang and the leaks here I did that
with aching heart but I did the
exception for Haskell I dropped down
because right now it's inherently a
front-end language it might change in
the future there is a movement into that
but it will be really hard hard to adopt
other workflow and I couldn't force the
JVM runtimes to run Frigga or ETA
unfortunately I also skipped a camel but
buckle script is basically a proudly
represents the family I think so
I said a little bit about tested
runtimes and languages that I choose
let's go to the measurements and how I
compared them each lab that I prepared
each implementation is basically has two
handlers both accessible via HTTP
endpoint and I used post and points in
order to cheat a little bit and avoid
caching as much as as I can so I didn't
use get in order to avoid caching on the
API gateway level first endpoint
basically does nothing no work at all so
basically it copies the request body and
the content type to the response and
basically echo everything and the second
handler is a naive implementation of an
algorithm that gives us our private
Rimes number list to a given point
so we basically I needed an endpoint
that will trigger it with one argument
we'll do a do a different job and
different complexity depending on the
size of the argument in both cases
lambda does not perform any additional
work so the it does not log and they
think and so on I have four scenarios
that I would like to test I started with
project package size and compared them
between the platforms I then measured
the memory usage and memory fund print
execution time and those two are
actually dependent on the input and I
measured the cold start time which is
very important when it comes to server
less and we will talk about it
in scenarios three and four I use the
prime numbers endpoint for the fifth
scenario I use the Eco and when it comes
to fourth point I would like to emphasis
that it's not a performance test is the
cost-effectiveness test because I
murdered the execution time how long
till of the runs not how late and is the
response in order to infer how much
we'll pay for it when it comes to
pricing so let's start with the with
with the easiest example to compare I
hope that it will be legible for you if
not bear with me I will just
transcribe so I would like to compare
package sizes at the beginning before I
will upload to the service and that was
pretty easy to get right but there are
some tricks as well with it so you can
see that bars mark with red color are
basically jvm completely compliant
platforms green ones are no js' and this
one is dotnet this one is actually a
Haskell footprint yeah and basically as
you may see JVM runtimes are the biggest
one also buckle script is pretty high
because of the standard library and
dependencies bundled with the package
for me but that's the only my personal
choice f-sharp is kind of a winner
because it provides a very nice runtime
and also has very small footprint when
it comes to package size one thing worth
mentioning is that you need to know your
tool chain I attached here deliberately
this huge bar which is closure script
without optimizations and that's not the
fault of closure script what I would
like to show you by that bar is if you
will not enable enough agressively
optimizations you will end up with
bloated package because in closure
script if you will disable or
optimizations you will basically bundled
that code and so on so here is the real
value for closure script and it's much
smaller if you don't know your tool
chain or mase the optimization you'll
basically have a bloated package so far
so good
let's analyze analyze the memory
footprint this time winner is not
visible at the first sight green bars
are basically no J's runtimes and they
are at the on the first side they are
having the smallest footprint also
Haskell has pretty low footprint which
is the this bar call it a teal color
however it is related with how we
bundled the package and how we prepared
the package because we use the GHC to
produce the executable which
aggressively optimizes everything and
drops off another unnecessary stuff it
also means that without it we basically
if something goes wrong with the
executable we are on our own so that's a
workaround and trade-off I'm a little
bit surprised by high position of
closure script but maybe it's something
related with the implementation I would
like to point out that no J's no J's
based runtimes are allocating the
smallest amount of memory however pretty
steadily we f-sharp behaves also pretty
steadily as well so it's predictable and
we can also optimize for smaller lambdas
knowing that it will behave more
predictably this scenario is probably
the most controversial measurement so I
would like to spend here a little bit we
once again we do not measure here
efficiency or performance we are
measuring cost-effectiveness so how long
you lambda will run and by that number
we will be able to know how much we'll
pay for it for smaller numbers nothing
exciting is happening but I skipped it
deliberately because we pay for 100
milliseconds anyway so if our lambda
runs 8 milliseconds we are still paying
for the 100 millisecond slides exciting
stuff starts to happen with bigger input
and basically Scala and closures clips
are consuming a lot of time the
execution and they are significantly off
they are even hitting the hard limit at
the end which I choose 20 seconds for me
clear winner is again the F sharp
because it basically it's steady and
predictable and knowing that you can
provide the most significant savings as
well
very important thing that we like to
mention here is that due to
fire-and-forget nature of serverless
the containers are reused in a slightly
and intuitive fashion so if you would
like to leverage JIT compilers or very
fancy generational ggc's you are
basically need to provide a steady
workload in order to warm up those
things if your workload will be spiky
and there will be no steady workload
after a certain amount of time provider
will actually tear down your container
and you will start with the cold start
as well and that is also related and
perfectly described by rich Hickey who
called those kind of programs by
situated programs and described it on
his closure conch talk which is link it
and for such programs that are need to
live for a long time they are loading a
lot of stuff into the memory choosing
serverless approach will be a great
mistake and yeah last but not least cold
startup in other words how painful it we
thought will be for the end-user to hit
our cold infrastructure in and would
then wait for container to warm up and
load and here there are no surprises the
smaller runtime you have it's the faster
it will boot so no J's but no J's based
runtimes are winner here as I said the
smaller one time it will faster it will
load much faster another thing that I a
little bit talked about on the previous
slide was related with how AWS or how
providers are actually recycling and
reusing the containers and there is very
great article about this and
unfortunately that's the empirical study
only so there is
guarantee from the provider side that
containers will live for that amount of
time unfortunately it's basically
optimization on their side that they
will may handle or not and from the
empirical studies is it look like if
your containers are idle for an hour
provider will basically tear them now it
means that as I said if you don't have
steady work for work load you need to
either provide some health checks that
will keep that instance not idling or
you will deal with cold start and you
have to incorporate that more over the
biggest allocated execution units are
teared down a little bit faster so it
means that the more memory you will
allocate for your execution you need the
fast this one hour it'll time will be
much slower so it's a sensible
optimization on their site as well and
one thing that I need to stressed out
here is that every time we do a
deployment or recreating our functions
we are basically dealing with this
effect so it means that if you are doing
a deployment you should warm up and then
switch in order to have it properly
warmed up and do not affect your users
by by this cold start and one additional
thing that actually appeared during the
test
so all concurrent requests that are
hitting the cold server less your
lambdas will basically pay the price so
it means that it's not about queuing
it's about all first request that our
contract concurrently hitting your
architect infrastructure so we talked
about capabilities with the we do
measurements I would like to talk about
pain points and problems that are
basically an open opening to the
discussion about how we can make things
better and I would like to emphasize
again that it's always a tray
we gave up a control and we basically
received in exchange a less operational
burden but we need to apply additional
rules and constraints to our code and
solutions so starting with the first
thing we did we do not know anything
about containers and how they are reused
and so on and as I said it might be a
benefit or a problem for us depending on
how we how close to the hard one we need
to be and we don't know anything about
hardware operating system because
knowing that it will be a linux is like
knowing that you you are dealing with a
reptile instead of animal so it's
basically the still a problem and
unknown we don't know how it will be
reused we do have on the empirical
studies about that so Vegas is pretty
new approach and that requires a set of
new practices in order to do it well and
that includes the bug debugging
observability logging and so on also
monitoring and maintenance and luckily
some work is done in that space but
there is definitely a lot of
improvements that we can introduce here
as well additional thing is about limits
so as I said we gave up control we need
to deal with the constraints and limits
and there's plenty of them
so concurrency limit space limits disk
access package size because we can
upload at most 50 Meg's of jar file to
the to the service that's why I measured
package size most of these solutions are
advert advertised as infinitely scalable
and in practice maybe they are but you
need to fulfilling their rules in order
to achieve that and the pressure points
are put in a different place when it
comes to designing such systems in my
opinion so that's that's one thing that
means we need to consider and choosing
that approach we are also at the mercy
of the platform providers that they will
add a particular feature or support for
that particular runtime I can
example would be AWS support for dotnet
core up until the January this year they
didn't support any version above 1.0
they didn't support even 1.1 which for
people that are dealing with f-sharp and
net they you and you know what it means
you are basically on a very old compiler
and so on I did another one if you are
choosing the thicker runtime you will
need to be prepared to optimize your VM
configuration heavily in order to reduce
the cold starts or fiddle with the
garbage collection and so on so that's
additional knowledge you need to be
aware of in order to do it properly and
again last but not least you need to
know your tool chain you need to know
your tools in order to support this very
well and optimize and I filled with the
oldest languages and I use several
frameworks for that and in many places I
need to fight the build pipeline for the
particular language in order to output
some stuff in proper directories and so
on and so on a lot of yak shaving is not
the hard stuff but you need to do it it
means also that you need to do it on
your own
instead of rely on some plugins or
additional tools that's definitely a
place that we can improve on and the
same remark can be applied to the
optimizations as I shown in the closure
script example that's not a fault of
closure script but you need you need to
know watch which optimizations you need
to apply in order to be thin and lean so
probably it's much information so I will
recap for you because we are coming to
the end the safest choice would be to
use f-sharp on as your functions or AWS
I'm not saying that you have to use it
but that will be the safest and sane as
choice the the best support out of the
box and I'm not surprised at all because
majeure is also a Microsoft stuff if you
have to use AWS you are dealing with
star
that this introduced this year a month
ago so there might be quirks I didn't
run into any but there might be quakes
when it comes to bigger implementations
and so on for me a choice would be
either f-sharp or closure script or
something that is going on there no
jurist on time but mostly f-sharp if you
are seriously considering this approach
I strongly recommend to read that paper
and watch that talk
definitely worth time spending enjoyable
and also very important information
about context and about applications and
last but not least again applying those
rules requires knowing everything about
your use case without such attitude you
will basically apply maybe good ideas in
the wrong environment which will cause
more harm than good and that's all what
I have thank you folks for being here if
you have any questions I would love to
address them one side note here are the
list of the references and at the bottom
you've got the repository link with all
examples and implementations thank you
very much
okay so you mentioned some vendors like
yes and uh sure and you have some
experience or idea how to round the same
packages on local machines or lock our
servers if you try to compare to it
that's a very good question so there are
a couple of alternatives so for example
serverless framework provides plug-in
called serverless offline you are able
to simulate that on your local machine
however it it would it won't work with
every single example that I provided
here so for example i I couldn't force
it to work properly with closure script
maybe there is an improvement that we
can made so not all tools are working
out of the box with those examples as
well so yeah there are tools for that
it's not like simulating 100% compliant
infrastructure on your local machine but
you can do it locally as well it's not a
problem
server unless offline if you are losing
several US framework and there is stuff
called local stock if I recall correctly
for AWS in particular I don't know
anything about Asia truth maybe someone
else know so I was trying to see how I
screwed it on your slide said as far as
I could tell it was often among the best
at idle rate not far off so could you
say something about hascomb yeah so that
does vary right conclusion why I didn't
emphasize on top of that is that you
need to deal with the shame so we're
basically compiling and executable and
then dealing with standard input and
output which might be a problem or
advantage depending on the context again
however it went very well so memory
footprint was the best one the lowest
one the same went with the execution as
well so if you are fine with having work
around in your in your system that's
totally fine and very very good choice
as well hi there
do you think that the application that
you
to ruin her skill can be employed to I
don't know Ireland for example yeah
definitely so you can you can use any
executable for example people are using
those shims to use image magic tool for
converting images as well so you can use
any executable that is compiled into the
Linux infrastructure the one thing that
you need to consider is that you are
compiling to a very general use case
because you know that it's Linux and
that's it
so there is no optimization for the
hardware and so on but that's things to
consider any more questions
okay so let's thank again our speaker
for the presentation</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>