<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Erlang Factory SF 2016 - Peter Hamilton - Reactive Programming With Elixir And RethinkDB | Coder Coacher - Coaching Coders</title><meta content="Erlang Factory SF 2016 - Peter Hamilton - Reactive Programming With Elixir And RethinkDB - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Erlang Factory SF 2016 - Peter Hamilton - Reactive Programming With Elixir And RethinkDB</b></h2><h5 class="post__date">2016-03-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/6K0Tjvn6PV8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right um turns out the new here yeah
we got a seat this is a little bit more
of a smaller room so I have a tendency
to pace around so we'll see what my
nervous tic ends up being here but yes
thanks for the introduction
introduce myself further i am i think
when starting to call a weekend
open-source warrior this you know
everything i'm talking about here today
my experience in the elixir world is
outside of my employment this has been
something i've personally found
interesting and just kind of dove in
headfirst on and Here I am today so in
coming up this talk title I decided
throw as many buzzwords into my title as
possible I figure reactive programming
elixir and rethink VB would get the talk
accepted seems I've worked but my intent
here isn't to really isn't to go through
and describe each of those individual
terms I think we can read up on that
what I'd like to do is present a way of
solving a problem that is different
that's unique I'm not sure it's the best
way to solve a problem but in working
with these tools and and applying kind
of these some of these newer ideas I
think it's definitely novel and I think
it at the very least will help you to
rethink the way you've solved problems
in the past
so to go through this let's build
something you'll notice in these slides
I have this wonderful mascot this is the
rethink DB thinker he's their mascot
they have an in-house artist on the team
there and they've provided me with some
wonderful artwork little shout-out and
thanks to them but we're gonna build
something so what do we like to build
you know one of the things we all find
fascinating is data it's always fun to
take a large amount of data and try to
extrapolate some interesting insights
one of the very useful sources for that
as Twitter provides essentially an
infinite amount of data for us to
process and to play around with and so
we're gonna start with a somewhat dumb
question but we'll call it we know it's
simple enough what character do people
tweet most so here's the first tweet
ever just setting up my Twitter and it
used the character T six times far more
than any other letter there
and so what would this look like if we
analyze the whole stream of tweets if we
looked at you know the big pipeline
coming from Twitter what do people
actually get when they tweet I'm
guessing you know it's the old wheel of
fortune you know and tal are those
letters are pretty common so how do we
actually solve this we thinking
procedurally what we would do is we'd
get access to the tweets for each tweet
we'd go through analyze it come
characterize it as this particular type
of tweet we might then take all that
data roll it up into some pretty report
of some sort and show here's the
breakdown this is what it looks like we
have 20% of all our tweets are
characterized as a tea 15% are an A and
so on and so forth alternatively I'd
propose that you could define that
problem less in terms of here the things
we do but here's how we transform our
data and so the flow I'd like to propose
is we start with the raw tweet that's
what we're getting from Twitter we
analyze it and it is now graduated to
the magical class of the analyzed tweets
at that point we can aggregate these
together we have our dashboard data and
then we can publish our data out to our
dashboards if we look at this flow here
these different boxes and arrows that
are drawn we can see some interesting
patterns that we've all likely seen
before if we look at this analyzer we
have n raw tweets coming in and and
analyze tweets coming out this is the
very familiar map function that we'll
find so for every tweet we perform some
analysis on it and it outputs a new
analyzed tweet at this point so then our
next box is our aggregator so we look at
that box there's two inputs one output
you know kind of to me at least when I
first see that my first thought is this
is a reducing operation we're taking
data coming in and we're and with a
feedback loop we're applying a reducer
now in this particular case for every
single tweet that comes in we're
probably going to want to update our
dashboard and output
the intermittent intermediate value and
so instead of a reduced this becomes a
scan function which is very similar
concept but where every intermediate
value is output and then from our
dashboard data goes into our publisher
and we publish that out you know this
could be another map possibly this is in
each where we're just causing side
effects for every value we get and so if
we were to write this up in elixir we
have our wonderful stream library and it
ends up being fairly simple
we have our tweet stream coming from
Twitter we map it by analyzing it we
scan it by updating the dashboard and
then we broadcast each resulting
dashboard and then we run the stream
pretty fantastic then just four or five
lines of code there we can capture the
entire flow of what's going on now
granted I have these magical analyzed
tweet update dashboard and broadcast
functions that I'm referencing but the
high level of what we're doing is simple
easy to follow this is not without its
problems now if you look at each of
these steps there's a few interesting
characteristics in performance in
robustness of the system to highlight a
few of them the way that back pressure
works on this system is that you will
not accept the next tweet from the tweet
stream until you've broadcasted the
previous dashboard and so in this
particular case we do not have
concurrent pipeline we are not analyzing
tweets concurrent with updating
dashboard we get a tweet we take step
one step two step three and then we get
the next tweet in this case if our
analysis may not be very expensive it
may not be a big deal but if that
analysis was expensive let's say we
wanted to do instead of counting
characters who wanted to get some tone
analysis we had to call out to some you
know FML library were using or even an
API like Watson I actually considered
building this on top of Watson just to
kind of see what it would be like to get
the sentiment analysis ran out of time
didn't get to actually go that path but
having that parallel parallel
computation that currency between stage
of the pipeline would really be helpful
here so that's one part second
part what happens we fail this
particular code here the scan function
is very problematic in that we provide
an initial dashboard we update that and
it maintains its state but if we crash
in broadcasting for some reason this
whole this whole stream would drop and
we'd have to restart it and we lose that
whatever state we had maintained
assuming we can find some way to
checkpoint that state and restore it we
still have the problem of we analyze the
tweet and then now we know we're
updating our dashboard if we crash we
have to analyze that tweet again or
maybe we lost it and Twitter's not going
to give us a new feed and so we have
some problems of durability here and we
could probably go through and you know
add checkpoints and when we do crash we
go and we have to check what the latest
checkpoint was and see is there any work
that we have to repeat since the
checkpoint
and we lose this very compact view of
our system that we're just doing these
four steps these four functional
transformations of our data so that's an
interesting problem and so I'm going to
take a quick detour away from that and
we're going to talk about rethink DB one
of the wonderful buzz words and the
title of the talk here I didn't mention
this in my introduction but one of the
areas that I've spoke I focused on an
open source has been you know dataflow
programming and somewhere along the way
somebody recommended rethink DB to me
and so I set out one weekend and said oh
I'm gonna play around with rethink DB I
want to do an elixir where's the driver
and there was not a working driver at
the time there was an old driver that
was outdated and didn't work with the
latest version of rethink DB a lot of
changes that happened the meantime and
so I spent an afternoon hacking away and
wrote the basic network communication
piece and if I did the breakdown of my
query myself I could actually go and get
data from the database and I thought wow
this is really cool this is a success I
implemented one or two parts of the
query language it's a very long query
language and then you know like any good
weekend hacker threw it up on github
with you know a little readme saying
this is just an experiment and a week
later had and
our stars had some issues open people
were asking why I didn't support this
query or that query and the fact was I
didn't support 99% of the queries but
I'm glad they found you know that there
were two two that I hadn't explicitly
listed and so this coincided as well
with a personal goal I had of last have
for the month of April in 2015 I wanted
to work on open source everyday and so
Monday came around you know I kind of
hacked the other the basic piece on
Saturday and Sunday a few people start
it and we're commenting on it and so
Monday came around I said I am I'll
start you know it's letting the query
language and you know slowly but surely
over that month in the next few months
the k' became more and more fully baked
and and now it's you know a year later
it's I've given some time to season but
we're just about ready to go to our
fully stable version I'm waiting for
feedback and a few other little tweaks
so that's about that's why I'm here
talking about rethink VB that's my
exposure I sit here as the odd
intersection of these two technologies
rethink DB and elixir and apparently the
world-renowned expert on those two in
combination but what does it do what
does rethink DB why is this interesting
why was i drawn to it so we'll get to
that in a second so it's a document
store you know anyone familiar with
Mongo or and many of the other databases
out there are familiar with these
document stores you provide a JSON
document you can through various
mechanisms index into it you can search
based on certain parameters it does that
you know a lot of a lot of different
databases do that what's special here
there's a few things I found that we're
very interesting one is the query
language so the query language is not a
string based query language like you
might see with sequel or with Mongo it
is actually a under the covers it is a
s-expression ast you know it looks a lot
like a lisp and it's designed to be
baked into the client language and so
you should use native the guidance from
the company behind rethink DB is that
you should use native language
constructs native language idioms
and it should feel native in your
application and so that was kind of
interesting the query language itself is
very functional in nature and so it
seemed to kind of fit with my paradigm
in view of the world the other aspects
of the database or is you know fairly
easy to get operationally up and running
they have provides some great tooling on
that front
it does clustering sharding all the
wonderful things we expect from a no
sequel database so I'll go in a little
more detail here but I'll talk about the
query language and so I've got many
examples here and hopefully as we go
through enough examples you'll get kind
of a look at a feel for what you can
can't do with the query language so here
we have the first query there it's you
know table people we're gonna filter on
the name John this you know is fairly
similar to you know the type of function
the functions we might have in a numeral
numerable library and then here and in
the first example we're filtering on the
key value pair of name and john second
example we have the ability to pass an
anonymous function and so I've bolded
this keyword lambda that's a macro that
is part of the driver and if you using
the keyword lambda you can pass an
anonymous function and it will transform
that anonymous function into a rethink D
be compatible and on this function so
here you can filter you receive the
person you can do some nonsense math
like only except the people whose age is
greater than the length of their name
that would produce an interesting
partition split among a population to
say the least
you can go farther there's the ability
within a anonymous function to have
branching logic that we can actually say
if the person's under the age of 13
let's mask their name and so we would
merge name private over the document
merges have a press that have a
preference for the you know the document
on the right and so it would overwrite
any field name name with name private
and that's actually you know a fairly
common thing that's done we can go a
little deeper the query language
supports joins and so we have our posts
we join against author
one pattern that I've noticed in many of
the rethinkdb queries is that it returns
a very explicit set of data and so when
you join it returns an object with a
left and a right so you can see the two
documents that matched and then if you
call zip it will actually merge them
together that's what we call this up
there you can do sub queries you know so
get we can get out you know from the
table of people for each person we can
fetch all the posts that they are the
author of we have to explicitly coerce
it into an array but we return that we
can merge a new field of posts into the
author we can do grouping we can do
mapping reduce you know some is a
simplified form or reduce but you know
reduce is a if anyone's familiar with
haskell it's much more similar to a mono
introduced in which you don't have
control over the order of operations
that so it can run effectively across
the cluster and so I decided like what
is the most rikuo that I could come up
with and this is actually fairly
complicated query it given a group of
people or I guess given a table of
authors and posts how many people are
responsible for the post that are like
the authors that post more than 20 times
was what it is set here so we have kind
of our classification of frequent and
occasional authors and so we go through
and we can group our authors by whether
or not they have posted at least 20
posts there's a few you know nitpicks
here coerced a string because later we
use it as the key in J's in a JSON
document and you can't use boolean's as
keys so there's a few things you kind of
have to rules you have to play by but
the idea from the group there is we're
splitting it into those who've had more
than 20 posts those who've had less and
then our next function there is we're
going to grab the count the author and
any of their staff they might have a
list of staff out of those people who
helped them we're letting a sum that and
then we call ungroup which one of the
interesting things about group is any
map or filter things after that apply to
each group individually when you ungroup
it takes that all those groups and
just gives you a full array and then you
can filter out individual groups and map
the data in that way
we then take all the values of these
groups that's you know a quirk with how
the data is returned we can coerce to an
object and then method object so at the
end of all this we get frequent here's a
count occasional here's another count
yeah this is very very complicated query
but it's actually pretty expressive and
it's you know the the data flows in a
similar way as how you would write this
on the client-side so what are the
limitations of the database I think we
whenever we talk about a new database
that comes up and clustering we love to
mention the Jepson tests we love the
mention Kyle and what he's worked on
he's done some very interesting tests
recently and he did a full suite in in
collaboration with the rethink DB team
they reached out and actually hired him
as a consultant to validate that they're
clustering solution their automatic
failover based on raft was actually
doing what it's supposed to do and so he
did his full suite of tests and it
passed with flying colors and a pass in
this case it's not you never lose data
it's the documentation is correct so if
you write your data with the correct
settings and if you read your data with
the correct settings you will never have
unacknowledged rights or lost
acknowledged rights and there are ways
to soften those guarantees and those
meet you know the documentation as well
it's not was really interesting and
really kind of a good a great sign from
the team that they're really thinking
about these problems and not focusing on
marketing material in any case the they
did a follow-up run where they did a
similar set of tests while reconfiguring
the database so while adding new notes
the cluster and restarting tables and
that was the first time Kyle had
actually run that test against the
database no database had really gotten
sufficient I own past the flying colors
on the first run to warrant a second run
and in that they actually did find a
quarter bug on a corner case doing some
weird things and they've you know since
suggested that and fix it you know it's
I don't expect the database to be
absolutely perfect I think we'd like it
to be
the early days I know I do understand
that bugs will happen but the fact that
they are proactively trying to hunt
these down they have a very
sophisticated test suite they fork
Jepson themselves and use it as part of
their own team it's pretty impressive
acid like most no sequel databases we
don't have full acid our ability to have
atomic rights is limited to a single
document so you cannot have cross
document transactions in any way you
know that lends itself to some
constraints on the way you can use it
cap theorem it's a CP system with
automatic failover if you master for a
given shard dicen it uses a raft
election to to elect a new leader for
that shard so now we have a little
background and rethink DB we're familiar
and you know what it does and how we can
use it there's one key piece that I've
left out here and this is sorry I left
this out and kind of you know the
suspense building up to it all those
wonderful queries that you saw on that
wonderful query language I left out a
fantastic use which is called a change
feed and so for many queries probably
not that more complex when I had but
many of those earlier ones you were able
to register that query as a change feed
in which case you will be returned to
cursor and that cursor will represent
the given set of documents that match
that query as well as any future
documents and so as you enumerate over
it as you consume it it will block and
wait for more data coming from the
server and this is really where the
paradigm shifts this is really where you
start to think that was going to change
the way I program and so I would like to
revisit using rethink DB I'd like to
revisit our initial problem in our
initial solution specifically with these
change feeds so here's our original once
again we are we have a tweet stream
coming in we're mapping you know while
we analyze them we're scanning while we
update the dashboard and then for each
new dashboard that occurs we broadcast
it wherever that broadcast could easily
be a phoenix channel broadcast for
example but you know we're just gonna
call our broadcast and then we run the
stream so let's restructure this with
we think TV so what if each of these
four stages was entirely independent
rather than patty then consuming data
directly from the previous what if
they'd merely interact with our database
so the tweet streamer will write data to
the database the analyzer will have a
feed of data coming that this tweet
stream rewrote it will analyze it and
then write it back to the database the
tweet aggregator will get the data
coming from the database write it back
to the database then the dashboard
publisher every time a new dashboard is
updated it will receive an update and
then publish that out so to do this we
have in addition to the rethink DB
driver there's another package that's
available and that I maintain which is a
particular which is specifically around
change feeds and so you can register a
change feed with the query language at
any point in time and you get a cursor
the cursor would have similar problems
to our original model that restarting it
as a problem you can yeah you can plug
it in it implements enumerable you could
you know use all the stream map
functions on it directly but as I was as
I was working with that as I was you
know trying intimate solutions I found
that to have you know a lot of
limitations and so I kind of took a step
back and thought well what's what would
you know what's the OTP approach what's
the way to really you know embrace let
it crash and the last supervision and so
came up with the rethink DB change feed
behavior it is basically a superset of
gen server there's two main editions
here and I outlined them here in the
init function instead of returning you
know what you normally turn you return
subscribe and you provide the query the
database you'd like to use for that
query and any initial state you need in
your application and then there's a
handle update call where every time it
updates received this gets called and it
passes in whatever state you're
maintaining and then handle call handle
cast handle info terminate those all are
available as well
so using that let's go through and think
about these different stages so our
tweet streamer this is consuming from
Twitter by the way this is all this code
here is actually from I built this
application and ran this
so I pulled it out and added some
formatting to you know get this to look
nice but you know the canonical way
right now in the electric community to
run a stream is the stream runners start
link you see at the bottom and so you
build whatever stream you want and then
you pass it a start link and that gives
you the ability to it will restart it
with the crashes and things like that
that was done by I believe that was
James fish who did that and so we're
gonna lean on that for for this initial
piece here but we set up a filter we're
tracking some topic we for every tweet
we get we're gonna insert in our
database
two things one the text that from that
tweet and then two and this is a key
part here is this state field and so we
kind of talked about the state
transformation we're going from raw to
analyze to then to the dashboard so that
could be recorded and so we'll start out
we'll enter them in in the state of raw
and then we insert it and then we get
the next tweet from Twitter you could
optimize this in many ways you could
batch this and do all sort of things
we're keeping it simple here and just
processing them one by one so that goes
in your supervision tree off it goes
it's consuming something goes wrong it
crashes it restarts gets a new feed from
Twitter isolated just does its own thing
so our tweet analyzer so we had those
two callbacks in it and handle info and
so we're gonna go through the two of
them so here in a knit we establish our
query we have our tweets we're gonna
filter on state raw and so every tweet
that we've got is gonna be inserted it's
gonna have States set to raw we only
want those tweets at this stage and then
we call changes and we have the flag
include initial which means any tweets
any documents that match this filter
that are already there please return
them and then send me and then send me
anything new that comes so we subscribe
that runs and then here we have our
handle update so handle update comes in
data is an array of changes that have
happened it batches them together if
many happened at time instead in and and
you can tune that with additional
parameters to changes but for simplicity
we get an array coming in for each new
piece of data we come in
it looks like this form we have new Val
and old Val and so if you kind of draw
out the boolean truth table here if old
Val is nil and new Val is not nil then
we have an insert into that collection
there's a new element that was not
previously part of that query if vice
versa happens new Val is nil and old Val
is something then we had an element that
left that that query set and if we have
both set an element in there was updated
and so here were I simplified this a
little bit not dealing with the other
cases the other cases are basically
ignore and the way we're building this
but here if we have it a new Val and not
an old Val it means we have a new entry
in this data set we're gonna go
calculate the most-used character and
then we insert back into this terror we
update that element and so table tweets
we get the ID of that element we update
it and then this is a key part here so
in our updates we have the ability to do
an atomic update on that particular
document in a single document so here
we're going to confirm that the state of
the tweet is still raw you know this is
a classic kind of compare and swap
situation if it is indeed raw then we
are going to add the fields or add the
field most-used character and we're
going to update the state to process
otherwise we're going to do nothing and
let this run and this is some important
failure characteristics and I'm gonna
talk a bit more about how these things
fail a little bit but you know largely
the idea here is that this is resilient
against one failing midway through and
two if you end up with a situation where
you have this server running twice for
some reason you're still going to have
safe updates you could have two
different tweet analyzers processing the
same tweet at the same time and only one
will succeed the other will fail it will
crash when it reloads the initial
changes flag that we provided before
means it's going to get any new tweets
since we succeeded and updating it this
tweet won't be processed a second time
so either a processed or it didn't and
if it didn't process correctly just then
we're gonna process it again so that's a
tweet analyzer so this can run this can
crash
all you know it is entirely responsible
for one thing and that is making sure
tweets can move from the raw state to
the process state so now we have a tweet
aggregator so here's our knit again and
so here we're filtering on state
processed and we include you know
include initial true is again the flag
we're shooting for so this one ends up
being a little more complicated because
our goal here is to update a dashboard
and to also update the tweet that we
received and mark it as recorded just to
move it out of the state of processed
and into the recorded state so we don't
process it again and this is where acid
would have been nice would make a big
difference because we can't atomically
do those things we cannot update the
tweet and update the dashboard in a
single go and so we're forced to choose
between what is usually referred to as
at most once delivery and at least once
delivery and so either we can make sure
we process it zero times or one time or
we can process it one or more times and
to make that decision is pretty easy in
this case here I'm going for at most
once delivery so when we receive the
tweet we update the state of the tweet
to recorded if we then crash before we
update the dashboard here then we failed
to count that tweet and it just goes
uncounted alternatively we could update
the dashboard and then update the tweet
and if we failed in between we would
count that tweet twice or three times
and it would keep coming back for the
system there's a couple ways to adjust
this that we can talk about later but
this is one of those kind of realities
you generally need to need to deal with
in distributed systems and because we
are not an acid compliant database it
often is a distributed system and in and
of itself some solutions here are you
could instead do with CR DT where and
I'll come back to that a second but if
you look at the actual update dashboard
query I wanted to highlight this as well
it for our you know our dashboard table
we have a character dashboard a single
document we're going to update it and
then we basically go through and set
whatever our most used character is - we
end
permanent order said at the zero and
then increment if it's not there already
instead of having just a counter for
each of these you could safely do this
for example by keeping a set of all
tweets that had that character you know
set insertion is a very natural CR DT
and so if you process the tweet two
three four times and every time you just
inserted it into that set you'd have the
same result that's if you've processed
it exactly once and so that is a
solution here going for simplicity I did
not go with that I also wanted the
chance to talk about the trade-off that
has to occur here so we're updating the
dashboard we update you know we
increment that value and then we take
the next element coming and we do it
again and we just keep going if we crash
we either will have a one you know a
tweet at the very least there depends
where we crashed but we may have tweets
that get processed again or you may have
tweets that we don't end up processing
that's a trade-off that has kind of
application semantics that's necessary
and then we have our dashboard publisher
and here we get our dashboard we you
know request the initial changes and
then we have this squash flag you're
saying you know throw multiple changes
just squash them together I just care
about what the latest is and then our
handle update here this one's really
easy we get an update coming in we
process it and we publish the dashboard
and that's the application and so it to
me it was very interesting to go through
that process to build this out and
thinking about what you know what does
this actually give me what does this
change and so some of the things that
does that cited earlier is each of these
act independently they're their own
process and so the performance
characteristics can be isolated if you
have a huge stream coming from Twitter
you can do some things like have
multiple analyzers you can add multiple
analyzers in parallel you can carve up
your ID space on that filter and so you
filter on I only want the bottom half of
IDs and the other nodes gonna get the
top half and now you can process this
twice as fast you can
oh sorry laughs I traded off for a
second additionally you can have
multiple pipelines going on and so in
here we had a state flag and that flag
had a single value imagine that instead
of a single value we had an object and
that object had character analyzer raw
but it also had tone analyzer raw and it
also had image recognition analyzer raw
and we had different pipelines for each
of those different states and so at any
point in time that tweet might be fully
processed for the character analyzer it
might be just analyzed for the tone the
sentiment analyzer and each of these can
be happening independently so we have
one ingestion process and then fan out
to multiple pipelines we have the
ability for air handling one one problem
might be if we crash because of a bad
tweet coming in the way that I've laid
that already is we will try to process
that tweet again the next time we come
up include initial means we will get any
that we haven't fully processed and then
we'll fail again and we just keep
crashing and it stalls the pipeline
maybe we add an additional flag for a
number of attempts and in our filter we
then say only give me tweets that have
number of attempts less than five and
right when we get it we increment that
value and so we try maybe four or five
times and then by incrementing it it's
now out of our result set we crash we
come back up and the result set no
longer has that tweet maybe we add
another stage of the pipeline that only
looks for tweets with with that number
of tries over five and we have an air
handling pipe system we have a email it
off or record it somewhere as I started
thinking more and more about this I've
seen that by having this simple backbone
of each of these being independent and
not coupled together that you can extend
this you can you know edit the code you
know change the code for any individual
piece you can add additional parallel
processing happening there are a lot of
very very interesting directions this
can go
on the other hand it's pretty
complicated there's a lot more code than
the four lines I started with here it's
bottleneck by a database and you're only
going to be as fast as you can read and
write to this database I was talking to
Joe Armstrong yesterday and I asked him
a question specifically about databases
and you know whether he thinks people
have a tendency to use them too often
and one of his biggest points was
exactly that that you are bottleneck in
your system and inherently concurrent
and parallel system you are serializing
it through a database and ball in the
bottleneck in your system so that might
be concerned and perhaps that concern is
alleviated as you have a bigger cluster
and you have proper sharding and
replicas throughout or you relax your
reading of data and allow you know soft
reads and various things other problems
that might that might occur the fact
that we had to drop a few tweets you
know and aggregating the fact that we
had to choose between those semantics
you know that could be a little
problematic one solution there that
would actually built this was an
additional server additional service
running or just a stream running
essentially with a stream interval and
every so often every five minutes or so
or every thirty seconds it would go
through and look at all the recorded
tweets and re a greedy at them together
and update the dashboard with the
correction and so you get kind of this
hybrid you have your correct path and
then you have a fast update to keep it
in sync so you can see roughly what's
happening in real-time but at the end of
day you can have this as close to
accurate as possible so is it better I'm
not personally sure I think it's one of
those situations where it's in the
opinion I can think of many applications
I would build that are gonna be a little
lower volume there may be some job
processing queue or various things where
this would be a fantastic way to design
it overall I think rethink DB in general
has
giving me a lot of tooling a lot of
ability to solve problems differently
and overall and I think it's been a very
positive experience so that's what I'm
sharing today thank you for listening
I'm cutting this a little bit short
because I know a lot of people I've
talked to have had a lot of questions
and wanted to get a chance to you know
we can discuss things in more detail or
you can come and ask I've also got a
pile of t-shirts up here that rethink DV
was kind enough to send me with so
please come by and grab a t-shirt there
were some people who tried to get one
before and they seem to have stayed for
the whole meeting so they're allowed to
hang the t-shirt yeah thank you very
much for having me is there any
questions directly anyone have a
question they'd like to me you know
anything you like me to discuss in more
detail all right thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>