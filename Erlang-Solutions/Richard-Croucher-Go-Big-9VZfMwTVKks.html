<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Richard Croucher - Go Big | Coder Coacher - Coaching Coders</title><meta content="Richard Croucher - Go Big - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Richard Croucher - Go Big</b></h2><h5 class="post__date">2014-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/9VZfMwTVKks" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">yes so welcome along to this suspicious
so I'm gonna be talking really about
kind of big computers big data and
really then how does it apply to 22
Erlang and and trying to trying to bring
the two together there so first of all
something about myself so I'm I'm a
platform architect and as many years at
Sun really help whilst there I really
helped design the original kind of comm
systems we put out at that time but very
large-scale systems that in those days
as we powered up most of the big comp
sites that kind of some of that evolved
into the bigger grid clusters we were
building and I I personally did a
cluster around that time of over a
thousand servers huge challenges in
doing that in terms of the the network
connecting nose together I went on to
Microsoft and was a principal DevOps
architect they're really on their whole
internet facing side of the business so
what is now known as a zoo but also had
had passport xbox live all those
properties there while I was there we
were adding something like 4,000 serve
as a month into the data center so
massive challenge Erinn in how how we
could build those how we could manage
those the other interesting thing there
was it was very obvious that nobody knew
how to write a cloud app in those days
people were developing an app may be
running it with six or seven servers
maybe they had a sequel server back-end
they were they'd come to us and say
write this works fine on six service can
you now give us five hundred and their
kind of expectation was it would
miraculously just work and scale on
these and and really we set about at
that time lead with in Michaels of
looking at
well how do we write an application
which actually scales horizontally
because uh and nobody really knew I
probably spent most of the last ten
years over working in banks on high
frequency trading systems and that's
partly because that's been really
interesting technology and that's been
pushing technology to hardest but
there's a certain amount of scale and
things we've been doing there as well in
terms of running running running large
computers there I've been involved in a
couple of cloud startups and and that's
really where a lot of my or Lang workers
as has been and that's really kind of
what I'm going to lean towards in these
discussions so really I guess it was
probably when intel kind of really
stopped delivering us faster processors
and gave us more cause that it really
became apparent to us all that we were
going to have to write concurrent
programs and i was writing things in
java and those in those days and it was
just too hard and the thought of of
trying to write programs with hundreds
thousands of threads and debug them and
get them to work really let's do led me
to start to look elsewhere and led me
down the functional path and i probably
would have gone haskell had it not been
for for the whole OTP side on on Erlang
and that's really what what pushed me
towards there as a platform architect
I've spent far too long of my lifetime
building the the basic infrastructure to
run things and and just to find that it
was all there and did the basics I
actually made it make made a huge
difference to me so I started on my on
my own anger knee and like with most
things it starts good and then you start
to hit hit some of the problems
you know what's wrong with hyphens guys
you know the amount of feed people who
who must get caught on on you use using
a hyphen in a module night why isn't it
documented anywhere and why doesn't it
give it a decent error message it's it's
it's it's silly silly stuff and of
course some of the other bits difficult
to get used to you know the mutable
variables having been forced to do
recursion because there's no wall and
those kind of things and I OTP look was
good on paper but it was so difficult to
use course low maths and things like
that for chattiness and things dying
because they're out of memory so so as
usual you you start well excited and
then you said you hit all the problems
thank you guys you saved my life we do
need more books on an OTP I know
francisco's ones out soon that should
help but certainly that that helps and
really kind of what I'm going to be talk
talking on is really how we evolved
lying now how we leverage some of these
other things so let's kind of talk about
big clusters and big data so really what
we talked about there are computers with
fill a room so if we look at kind of
state of the art at the moment so
there's a continual race to build the
biggest computer if you go on to WWF and
org you'll see the top 500 computers in
the world they get this did there is a
there's an annual bake off where they
ran a benchmark and see who's who's what
what position you are in that ranking
and these are preg primarily used to do
kind of the big science challenges the
global warming weather forecasting
finding the elusive verb Higgs boson so
really a whole range of things there but
also things like web search so I was
building massive clusters Microsoft
20,000 servers to do to do search a real
big systems their picture on the right
there is is is is the blue jean computer
at Lawrence Livermore so it fills the
room and that's that's really what you
got to think about on these things is
that they do fill a room it's kind of
three generations of those I'm going to
be talking about those so the
distributed resource managers MPI and
then and then MapReduce which is kind of
where all the excitement's around now
and that's kind of that whole area is
what people really talked about as being
big date oh and the the first two were
more about computing for third one is
more about handling large amounts of
data and and that's really the focus on
that there I think all three methods are
are still viable and i still use and
i'll be talking more about those so what
can I caused it in the first place a lot
of it was just you know until until then
people had brought very expensive big
mainframe style computers or they were
the create XPS but really vers
dissatisfaction with the cost of those
plus they couldn't get any bigger and
and people started to to connect
together lots of low-cost commodity
computers the original ones used
distributed resource managers we then
got on to what no known as MPI grids
those then evolved with respect i we're
into vector grids and then finally we've
got this whole kind of MapReduce area
the big data with her do so I'm going to
drill into each of these boxes in a bit
more detail so we look at at distributed
resource managers
this was really about submitting jobs so
you define a job you'd submit it into
something which managed to cue the the
scheduler would then take it from that
queue when the right resources were
available which it would dispatch that
on to some worker nodes when they'd
finished executing their results would
come back for jobs in those days and I'd
still today are really not really much
more than shell scripts they partition
the data manually typically use a NFS to
actually share data between those nodes
so relatively straightforward but this
was this was hell and a lot of the
computers done still carried on today so
examples a condor some good NGM platform
tibco data synapse they're all examples
of the distributed resource managers the
the next phase around that evolution was
really the message passing interface and
it really became a programmatic way in
which these jobs could in to communicate
with one another the basic scripts on
the drm really couldn't communicate this
viscous gaver an asynchronous message
api so that those nodes could
communicate they could do things like
rendezvous they could share data they
may anything's like multi cars to give
forgive efficient broadcasting of
information they evolved from NFS to
cluster based file systems at much
higher performance allowed much much
greater data transfer rates between the
system so you could no transfer
gigabytes terabytes of data into those
systems the the top500 rating is done
using a limb pack benchmark and that's
actually run under MPI it's actually a
Fortran benchmark so you'll see this
supports c and fortran the AP
I foot for this the benchmark itself
does a lot of array manipulation so very
much focused towards that if you look at
the sensitivity of MPI performance so
this chart shows a number of different
benchmarks then side here along the
bottom are different MPI implementations
and then along the top here are
different network latency so the
internode latency and really what this
shows is that when there's the
performance drop a true put of that
cluster as you start to add network
latency between those nodes and that's
something which is really important for
these body for the for these large
clusters that the throughput is very
dependent on the network the single
instruction multiple data is is really a
way of using specialized hardware to
execute the same program on different
slices of the data it's been used in
GPUs for some time and really what
happened was people I saw that they
could leverage that same commodity
hardware to actually execute that their
programs as well so people started to
actually program using the graphics
primitives that was that was difficult
and messy and NVIDIA commercialized a
set of api's known as CUDA it's a sea
level API and allowed people then denta
to program that directly other hardware
vendors who ati you later acquired by
AMD did something similar there was
industry standardization around these
api's known as opencl and then finally
intel
came out with their own a solution which
known as Phi so these really provide a
large numbers of vectors and that and
that that's primarily what we's the bees
are used for if you look at a typical
CUDA server or so so these are the kind
of things we would deploy this is the to
you sir over it has on it to Intel x86
processors here you would then plug four
of these Tesla's so to each side into
this system each of these Tesla's has
512 cause so when you when you combine
the system together we end up with weave
weave weave 20 x86 cores and over 2000
Tesla cause if you build a rack of those
you've got 41 tails and cores in a
single rack so a lot of compute power
programming model is more much more
complex so this is really what we talked
about hybrid computing so you have you
have your main program running running
on there on the system over here and
you've got your sim d'architecture
running on these on the GPU cores and
part of the challenge when building
these computers is trying to determine
the right balance between how many x86
cores do I need for to keep the workload
busy on on on factory these vector cause
if you look at a a program example you
can see that a lot of what it's doing is
is is it is it's executed on the main
program it then has to transfer the data
across 22 22 to the cuda ng once it's
once it's done that it loads the code
across the same code runs on every call
so so each cause running a different
slice of that data but they will running
in the same
mode with their own threat so you end up
with lots and lots of these threads you
start the program you waited to that's
finished and you copy it back again many
times you find that because of the
overheating transferring across it's
often quicker just to execute something
locally than it is to transfer across so
all the time you're trying to determine
davai run locally do i do I take the hit
and transfer across so that's one of the
big challenges in this basis is trying
to decide what what to run locally what
what to transfer across and because in
teleporting more more cause on the local
side you can easily end and end up with
20 or 30 cause running on the x86 side
maybe 200 on vector engines now they
keep adding more and more there so you
end up end and can end up with a lot of
vector engines locally as well so what
were the top ten so this was the last
positioning last November the top 10
computers China has got the biggest one
at the moment so you can see that's
that's Intel's eons with with Intel
Phi's running alongside that there's a
physical as a Cray running at running
AMD x86 that's got nvidia you got some
some you got an IBM power you're a spark
and I've arrived in power another x86
here one thing you'll notice here as
well is that none of them run ethernet
they all over run custom interconnects
or they run InfiniBand you don't get
into the top 20 now running Ethernet I
it's a theme which are mentioned as well
it's a really we've kind of hit the
scalability wall on Ethernet we've hit
the scalability all on tcp/ip and it's
so these guys have already made the move
if we look at that that number one
computer so it's Betty Ann in China so
it's three million cause in the in that
one computer I've given to a 33 a
petaflop of compute power so massive
compute power but when building these
and I was involved with several of more
than what would happen is somebody would
come to us and say well my budget is X
what position in the table can you buy
me or can you deliver me for that amount
of money and we'd wind up working with a
lot of people who would everybody would
bid and whoever got them highest at the
table would be that would would be who
they place the order with the other
constraint we often found was people
would say well I've got 10 megawatts in
my datacenter that's all I can have how
much power can you give me how much
compute power can you give me for my 10
10 megawatt capacity and we'd go through
great exercises trying to do the
trade-offs here trying to design that
the maximum either for the money or or
for the power rating week we could get
there the software is designed to cope
with failure so so you avoid any single
points of failure in that software but
you'd build it with we've commodity
servers and and really what you find is
they once your software can survive
failures you're better to to take costs
either those servers and put more
servers in and accept the fact that a
bigger amount will fail so so ninety
eight eight percent of 1500 servers it's
better than ninety-nine point nine
percent of a thousand servers and that
we would see making small savings on the
server's leaving quite small things off
meant we could put on
Maureen the whole network design is a
challenge once you've got more servers
and you can plug into a single Ethernet
switch it becomes really challenging
designed in the network and a lot of
work goes on in that area we see a lot
of performance impact on on that the
often we would find that to get the
similar same amount of throughput we'd
have to put a much higher number of
servers on that then if we were able to
lower the network at lower latency on
the network we could get away with a
much smaller number of service so I'm
fortunate it's a complex area if anybody
wants to know much more about the
network come see me later it's an area
I've done a huge amount of work on but
we don't have time to cover that in too
much detail today so some of the
applications which which go on there
I've done a lot of work in the financial
services based on things like option
pricey market valuations value at risk
but there's all these other big science
stuff which which which goes on on on
these as well now let's let's go go
across the MapReduce so this was really
popularized in a paper published by
Google and really Matt reduces is really
about taking a large chunk of data
partitioning that across executing that
in parallel combining those results back
again to the reduction and then
iterating round that if need be that
you'd go through another another mapping
another reduction the original paper
really only describes the mapping so you
so so so you've got you've got a key
you've got a map of a key and a value
and you from that who you produce a list
with another key in another value and
that's really the mapping function and
then you apply a reduction function on
that so it's so you've got a other key
value and you create a list of those and
you combine those those lists together
so the paper doesn't really describe any
real implementation it really only only
describes the the mapping Hamlet
reduction at this function level however
I did attract a lot of attention and
Doug cutting who was at that time search
director of the Internet Archive had
built his own search indexer for the
Internet Archive known as much and
really what he saw when he looked at the
Google papers of MapReduce and Google
file system which is a cluster file
system he saw that the layered approach
they were taken to engineering was prob
was going to be a much more reliable way
than it than the monolithic solution
he'd built with notch so he actually
reverse engineered both Google file
system and MapReduce implemented them in
Java and then and then added his sees
internet search on top of that in 2006
EG joined yahoo and that they'd all
simile been impressed by google's papers
and effectively backed him to make that
an open source project gave him the
resources to do that and really as a
result of that the apache hadoop project
was was was created they kept much
internally so so it was really only the
basic infrastructure which went into her
dupe initially I only really scaled to
20 servers but yahoo put a lot of effort
in over the years they created a big
research grid internally for that so
thousand and eleven they were reported
as having 42,000 nodes running on a dupe
hundreds of petabytes of data there so
you using it for search now our yahoo
pulled out a search and I've loaded at
all to being now and I would suppose a
Facebook are now probably the largest
Hadoop user now that in 2012 they were
reported to be over 100 petabytes
growing at about half a pen apart a day
the important thing is but people get
confused about is google actually don't
use a dupe they have their own MapReduce
implementation which is it totally
internal made they don't just not open
source that's all written in C++ so
Hadoop woods is reverse engineered I've
MapReduce it's it's it it's not google
code a dupe is actually much more of an
ecosystem rather than just just just a
single piece of software you've got a
cluster file system the Hadoop file
system which take takes for data it
replicates it across multiple nodes by
default three copies and makes that
available to 2020 every node within the
cluster there's a columnar database as
well known as age base that's part of
that you've then got all the the
MapReduce processing side at the cluster
management then there's a whole host of
of data access components and then
really the workflow in the management
side of that a dupe itself though it is
written in java and i'm really only
contains really basic get set and
scaring primitives you really can't do a
huge amount with it you've got a right
pretty complex java programs to actually
make use of that and really what's
happened over the last few years is i
would say most of the energy has gone
around
rear-engine work and that's really what
people think about with a Duke now is I
using some of these query engines which
are available in the community so how
was the original one which was a sequel
like I would say the one which is
probably getting the most attention at
the moment is presto that is full ansi
sequel that's that's come out of
facebook they use it enormously they've
got thousands and thousands of people in
facebook who are running presto so huge
amount of effort and resources going
into that you've gotten parlor and
you've got another reverse and you need
a google one so dremel is is reverse
engineered implementation of Google's
that are bigquery API so we're going
there custom query engine but it's it's
a reverse implementation of that lots of
Hadoop clones out there so you can take
the original there are other people who
had value to it and I'd other things
sometimes they're they're based on the
open source code other times that
they're completely reverse engineered
themselves so you've got it I BM and
Intel doing things in particularly
there's a lot of derivative projects as
well Apache shark is one one of the
things we found about about Duke jobs is
it's optimized for big data for taking a
long period of time so think minutes
hours days weeks for a typical Hadoop
job if you want something done in
seconds it's not a good environment it
takes a long time to start up takes a
long time to get your data there between
each MapReduce cycle there actually
writes to disk so it's oh so it actually
saves it to disk then the next I picks
it up again so there's a lot of of dis
guy up activity going on between all
these steps some of the the derivative
projects such as a shark and act as a
spark as well goes with that
I'm much more aimed at shorter term it
more interactive kind of use cases where
you want want to send a query and get a
response back in in around 20 seconds so
a much smaller data set to execute on so
a lot of work going on and I was in
recent years so I would say when people
talk about her do nowadays they probably
mean the query languages and utilizing
those to actually query large data sets
they're running on top of a Hadoop
infrastructure cluster but most what
people are interacting with are there
are the actual query languages
themselves although they weren't
actually part of the original a dupe
system so if we if we if we go back now
and look at where we have with these
grids we could see we've got distributed
resource managers we've got MPI grades
we've got these matrix calculations of
vector intensive workloads and we've got
the kind of the big data and analytics
with a dupe but what do we do with
distributed applications what do we do
with cloud scalability if how do we
build the next Facebook you know a
billion you a billion users and I can't
write on a dupe and I can't run it run
it run it on on on these SMD grits or
not they're not they're not the right
things and to me that's a real
interesting challenges is is how do we
build the next Facebook and of course I
think we should use Erlang an OTP and
I'm sure lots of lots of you do butter
and of course you know it's scalable we
can spawn you can build clusters really
easily OTP he's got a lot of the basics
in there so so that really helps us
build reliable systems and I think so
Erlang an OTP together solve the
difficult parts for us of getting
concurrency built into your programs and
and and doing the reliability but
airline cluster scalability is limited
we've got you to chattiness I OT piece
of supervision trees are really just
designed to work on local nodes when you
look at at at the distributed
application controller you find it's
it's all about reliability it's it's all
about letting you pick from from other
nodes to to run on cinema leave Jenna
gen server cluster behavior murder it
allows you to have a single service
runnable and multiple nodes so you know
my feeling was was surely somebody must
have solved this somewhere I started to
look around a found a computer so that
was like a dynamic resource manager but
no commits that node finders a neat
little piece of code so allows you to
use multicast discover nodes in the
cluster saves you out adding them
manually there's even a separate version
for AWS because there's no multicast
support in AWS well disco project looks
quite interesting so that's the best
done at nokia research in palo alto they
were running on an 800 call cluster
there that's much more around a
distributed resource manager they got
their own distributed file system in
there bit of a dependency on Python
baked in there but but summer so some
interesting stuff there react core
course that pulled out by the bachelor
guys from from from react and made
separately available so there's some
there's some useful things there in
terms of note node watch sure to do your
cluster membership you can you can even
advertise different kinds of services
there it is implementing the master
worker pattern using V notes so it when
it talks about V notes but various
workers allows you to maintain global
stay in the ring
by gossiping there so it's got that
built-in it allows you to to send
workload to two different nodes that
didn't mean that based based on hashing
but but you have done a good job of
trying to make it generic but it's it's
still very much got a very strong react
bias into it so it's it's kind of part
of the way there but there's still it
it's still a challenge to use that for
more generic things the whole release
project looks really interesting i'm
looking forward to finding out more
about that here I know Simon's got a
session tomorrow are on that so EU
funded program I think the goal of
100,000 cause is a bit modest given that
biggest computer in the world is already
at three million better but it's a it's
a it's a start beds fit but there's a
lot of interesting papers i'm looking
forward to actually finding out what we
can access and download i think the
whole s group side look looks really
interesting in terms of being able to
partition a cluster better there's not a
lot of software available yet i'm hoping
that that that's bet that's going to
increase on there but it's certainly a
promising move in there in the right
direction i think there's a lot of
people looking to have to solve these
problems now so in conclusion there so i
thinka Erlang OTP really helped solve
the most difficult part so that's
writing scalable code but can run
reliably there's no doubt that is the
much more difficult thing so i think
along an OTP gets us there but there but
there's a there's a lot which neat neat
needs to be done to help that scale some
of the work we do in the large clusters
we use technologies like RDMA to
overcome the tcp/ip bottleneck that
would be a great technology to actually
bed inside a line to get some of the
scalability there be good to see things
like multicast used to reduce chattiness
again as a standard collect cluster
technology to use these all solutions
turned to known problems we ought to be
leveraging so i think the real
opportunity now for four Erlang and OTPs
really to take this next step and become
the premier cloud platform there's too
many of us out there doing this
ourselves solving the problem the heart
by its I think if we if we can do it as
a community then then we really do start
to get to to damasus sky on this okay
with that with take any questions thank
you
so if you have any questions I will walk
to you so that you can say your question
and the camera will get it yeah you
mentioned a little bit about like where
yeah or the typical thing is maybe where
a lung has reaches the computation
bottleneck but I wonder what what is
this date for Erlang when we come to the
data data bottleneck like moving data
around and so on I'm thinking about next
generation well that's right that's
right so that's where I think we get hit
by the tcp/ip bottleneck we we have to
copy everything at the moment and that
means the tcp/ip will it becomes a
fundamental bottleneck in moving that
data around so that's where technologies
such as our DM I could actually help
help they're immensely so but it's the
network that will be in a bottleneck
rather than along wrath of it on modern
on the modern compute clusters the
network is faster than the computers the
network isn't the bottleneck is the
computers are the bottleneck butter but
you have to get TCP out the way before
you get to that point any other
questions
in your opinion what is like the
low-hanging fruit in terms of airlock
tooling that rank is currently lacking
and like what should be delivered first
in order to you know improve either
cluster management or Erlang
distribution I think I think simple
things like being able to to just bone a
job to run on the on the lease loaded
node in a cluster so that kind of thing
you know they're all problems which
everybody's writing their own code to
solve and it's silly that there's 50 of
us writing our own code to solve the
same problem that that's that's a really
trivial thing I think things like our DM
I am more complicated that week that
requires a lot a lot of building better
tools like node manager we're good help
in terms of it helps form a cluster
without you having to go off and ping
every node yourself don't you think that
this should be part of OTP yeah
absolutely absolutely and and it you
know Oh tepees is kind of almost there
isn't it you know it teases you by the
by letting you put node in there okay
earlier like some different languages or
frameworks which can be used as a no
learning learning point to you know copy
some of the best practices or solutions
well I think I think some of the
practices we can copy so so so so that's
where thing things like using multicast
to share information some some multi-car
solel allows a 101 too many publications
so so what one person can push out its
stateside this is this versus this is
how busy I am at the moment everybody
can receive that without having to send
a message to
every other node in the cluster so it's
it we use it a lot in large clusters to
broadcast cluster state you can use that
information n to determine for example
which which is the least the least
loaded node which is which is what
what's which one's got available
resources though they could it could I
could spawn the next job on to it
because all these things are trying to
get balance is trying to deploy how do i
do it without adding too much overhead
in itself otherwise it can be more it
can't can cost more to determine where
to run the job than it does to run the
job itself so it's so is so so it is
trying to get the balance right there I
first grade talk I wholeheartedly agree
and but one thing you didn't touch on I
would guess with your experience
building large internet properties for
microsoft security must have come up so
I just wondered if you had any you know
just thoughts framed by the whole
discussion around security and this in
the state is currently with erling OTP
so yeah primarily we would be running a
certainly where I've been running OTP
we've been running it behind the
firewall so so so been been using the
DMZ there and I'm running it behind the
firewall and that's probably true for
most of the cluster state that we
haven't have an access layer and access
internet I probably wouldn't want to run
internal OTP produÃ§Ãµes across the
internet and that's probably true for
most of the clusters to attend tend to
use have separate clusters at different
locations and then and then communicate
with them at a higher level I've done a
lot of work there with RabbitMQ using
that to to interconnect different
clusters together
across the internet that that I think is
probably the savory you really got to
have kind of a high-performance low
latency internal network you you want to
use use for use for OTP because you need
that reliability you've got to have a
very reliable Network there to to to to
to run something like OTP on top of it
you've got to have no single point of
failure you want all those
characteristics I really can't guarantee
that once you go across or cross onto
the internet but I can have reliable
clusters dotted around the internet and
I can have them sharing state between
them and that that that that's the kind
of approach I've been leaning towards a
question on the airline know does a
concept so one cloud Ellen cloud
environment could be that I deploy an
airline node and as I add processes that
node that node spreads across several
more Hardware notes seamless seamlessly
another way if that's not possible maybe
one should have the airline node as
something that runs on one hardware
entity it maybe could migrate away maybe
it's deployed automatically on the least
loaded and so on but which one of those
paths do you think is more viable I
think they're both viable attending I
don't think there's a right or wrong way
to to do this I mean once you get a
reasonable number of nodes in the system
you've got to cater for that they will
some of them will die they you've
constantly got this scenario of nodes
appearing and nodes disappearing and
you're having to write the software to
coat to cater for that scenario that and
that's fairly challenging in terms of in
terms of doing that once you've done
that you've then got a very scalable
system but should we have a airline
cloud environment where we behind the
scenes to replicate the airline node so
that they will never fail or should we
should you as a programmer know that my
nail elling notes will potentially fail
so therefore I have to cope with it in
on the application layer
so the way we've done that elsewhere is
is is is that you you do it on on each
job so that the job gets gets gets run
on on a note if that job doesn't if that
job fails then something has to rerun
that somewhere else and that that tends
to be been the approach which has been
used there now that's a bit different to
to a standard supervising ballroom
there's there's going to be a cases
where you need that constant service
running and then the supervisors got to
be able to to restart it on another note
that if it sees that I it's failed on
this node it's got to be able to restart
that on another node so so you know
there's different compute patterns air
which which we gotta cater for some
summer our job intensive some summer
service intensive so so so I think we
need to cater for both well we have done
for just one more question before we go
the next hi is this which is ash table
will help Oh gp2 to scale so it is
distributed hash table it's all the
peer-to-peer protocol and and so on so
so so sorry a core kind of gives you a
means to try to scale that so so you
know they use they use the the ring
cluster there too to do that and I
leverage a lot of technologies I use
them easier a lot that that that that
that helps to deliver some of that
global state but you've gotta got some
scalability constraints on that so
there's no one right answer i think i
think that that that's the key thing
that we need a number or different OTP
and infrastructure building blocks to
help build these skip these scalable
applications thank you thank you thank
you everyone for a very interesting
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>