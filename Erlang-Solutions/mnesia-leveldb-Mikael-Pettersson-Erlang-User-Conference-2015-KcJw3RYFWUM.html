<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>mnesia + leveldb (...) - Mikael Pettersson - Erlang User Conference 2015 | Coder Coacher - Coaching Coders</title><meta content="mnesia + leveldb (...) - Mikael Pettersson - Erlang User Conference 2015 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>mnesia + leveldb (...) - Mikael Pettersson - Erlang User Conference 2015</b></h2><h5 class="post__date">2015-07-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/KcJw3RYFWUM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so yeah so topical talk nice applause
leveldb sort of match made in heaven if
you like and it's not like you want to
get rid of the limitations of deaths
which are horrible but it also brings us
some other improvements so how's the
more myself I've been involved with
airline since the mid 90s I took a
detour for about seven years where I
worked on performance analysis tools in
the HPC that's high-performance
computing market lots of C and the 70
programming some C++ but since late 2013
I'm back in the fold
working for cloner on the transaction
and getting written in argon I'm
basically taking care of the database so
why do you want to do this so here is a
summary of the benefits we get from this
combination we get faster node restock
terms which is a huge deal for us
because that's the limiting factor for
us we get less RAM pressure I don't know
if you know about Toronto systems but
they are reputed to be among the largest
in terms of RAM capacity that run
airline and that's also a problem for us
so reducing the RAM pressure would
either let us put more stuff in there or
possibly buy smaller and actually foster
machines
neasha currently has a number of a
priori limits on table sizes and
depending on how you represent your
table you also get limits on the actual
DB sizes by putting the tables in little
DB you avoid those limitations
now this talk mentions leveldb quite a
lot but the
that's been done Tunisia is completely
generic so if you're sort of interested
enough in a different local storage and
 that is not lovely it could be
anything then all you need to do is to
write the appropriate plug-in module
there's a driver register that Whitney's
yeah and voila we are up and running are
and in conclusion leveldb works quite
well for us we still have some ways to
go in rolling it out but we are using it
live we have been using the VDP live for
a year now and we haven't seen a single
data corruption or a single core down
from the levity be driver so it's
actually works well this is a picture I
apologize for the poor visual formatting
but whatever so to the left here you can
see sorry what Nisha is today is you
have the database interface the
signification you have a couple of
available local storage types typically
you choose between these copies and
discount copies there are two more but
they don't really count those that map
to died for instance this going copies
Maps deaths we then goes to file and if
you choose this copies it maps to ads
that's in RAM and this cloud that's on
the desk and believe it or not this
figure is actually what Google slides
suggested denotes direct access or core
memory and the real old timers would
recognize why this might be related to
core it's a drum right what we've done
is we have nice text which extends media
so you can have different back-end than
the ones that are predefined to this we
have the actual backyard which is grown
11 DB
we ignore that for a while
and in what we have 11 DB which then
talks to this this guy here the leveldb
manager is something will be introduced
in order to be able to handle backups
online without taking tables on down or
disturbing the application why we do
backups over here you can see who's
written what I will repeat this as we go
along
oh yes so the application in our case
will sometimes the payroll data set
either go to the Asia and then down to
for instance level DB but we can also go
straight to leaven DB through this
lengthy my manager so some of our data
are us do not use Misha but it goes
straight to the DB so this is a brief
now this is a brief overview what gonna
happen it sent the sub rice Misha and
his problems summarized never DB talk
little about the glue talked about some
of the migrations jobs we've done what
we learned what we did and then I'm
going to finish up with how we solve the
backup problem and some pending
improvements ok so nature is the starter
that database application you get with
of the B it's not today its type of
database is that the table is a
collection of tuples but all those
tuples have to have the same shape so
basically you declare a record in our
lock that gives you the energy of the
tuple at the tag of the tuple you select
one of the positions in the record to be
your primary key and those piece of
information you put in the schema and
then you can get the table and nissa
with us
so you can't have tables with
differently shaped shaped tuples for
instance or tables with keys in
different positions depending on the tag
in the to consult you have to have
different tables for most tables are
sets but you can also have order sets or
bags and accessing a table you would be
depending on your program start and your
requirements you can either do just key
based lookups and insertions or while
the insertions you put give them in an
entire tuple but you can do key based
lookups you can do party based lookups
you can have these high-level queries
the co so-called Cuban sees what nice
iya really gives you is persistence
replication if you choose to use it
transactions with you shall teach you
guys from that and internally locking to
make sure everything goes right so that
that's basically turns it into the date
proper database and it also handles
things like schema changes online say
about that now neasha the thing we care
about the corner is the available
parents so that's the storage of options
you have one of them one of the two main
ones are discovery copies and when you
say that the table is of this type it's
stored it was known as a debts
you don't know what an axis that's an in
rap table debts is sort of mirroring
that but it's stored on the skinny file
and it's actually a big hash table
identified the thing to know about that
says is that the data itself is always
on disk
there is no internal caching of the data
whatever internal caching it uses is
only for metadata like for instance if
you repeatedly add a delete record
you're going to get on you space in the
in the deaths file that need to manage
and reuse and that metadata to to do
that it is in RAM but but the actual
payload is not around ever that's has a
number of problems since it's the data
is on disk you have to eye on the time
obviously this is the the the operating
system page cache can help you a bit but
it's never going to be as fast as having
it in RAM immediately the real killer is
that the table size is limited to
gigabytes this has been a problem people
have known about it for at least 50
years maybe longer two gigabytes was
certainly for probably considered large
in the 90s but it hasn't been launched
since the advent of commodity 64-bit
machines and that's like 2002 three so
which means that if you have a large
table of this type then you it can't
actually be a single physical table it
has to be split we call that fragmenting
and this was a means that you need some
administrative administration to monitor
table sizes so you have you introducing
and risk here if your tables grow and
you don't have a strategy for
fragmenting them then you may end up
with a system that blows up in your face
if you don't do something also if you
have an unclean shut down for instance
if the system crashes which for a system
that's supposed to be up 24/7 that's the
normal reason why would ever resort it
had a crash so you get an unclean
shutdown down all these dead State
also long to be marked dirty well most
of them weekly
and so when you restart Mesa will go
through them and say oh this is thought
you have to repair it you can't do
anything about that
well a little bit but not much but the
end result is that you're restored is
going to take a lot longer because it is
shaft downs are also slow I've been
trying to figure out exactly why I don't
quite know why but they are slow as well
the other main storage option you have a
Mesa is dis copies those are completely
different so first of all they call they
keep a complete copy of the current
table contents in RAM in a net stable
and all reads and writes etc to this
table or directed directly to the
Earth's table so that's basically as
fast as as it gets in our so that's good
now for persistence this cop is used to
so-called disk lock files
those are sequential log files that
contain terms formatted slightly funny
way one of those the TCD file contains a
complete image of the table from a
previous point in time the previous
point in time could be five seconds ago
or five weeks ago the it departs the
other disk lock file the TCL file
contains depending updates that is the
updates that Mesa has accepted a perform
for you so they are in the table notably
speaking but they haven't made it into
the image file yet but they are on disk
so they are persistent
occasionally when Manisha thinks that
log filed with pending up as it grown
too large it will dump the ink or or in
RAM image to disk produce a new the
defy those dumps I will talk about them
in a minute
one nice nice the nice thing about this
cop is is that unlike the discounted
coffees tables there is no a precise
limit on so essentially they could be
arbitrarily large but they have their
progress as well they do slow down
restarts but for a different reason
and that's because by definition the
entire table contents has to be in RAM
which means that if you have like let's
say that let's say you have one terabyte
of data in these coffee stables and you
know three stops then you have to read
that one terabyte the data interim
before you come before the application
is out that takes time a lot of time the
other thing is that if you have a large
table and it is performance critical
that is you want it accesses to to be as
fast as possible then you declare this
these copies but since it's large then
we also consume lots around grab is not
used as a cache of the hot data it
actually copies all of it whether it's
hot or not so it's actually poor or
abusive share and these table dumps to
produce fresh snapshot files they cause
spikes and system which disturb the
system as a whole there are I mentioned
this more as in parentheses there are
other local storage options one is
called Ram copies these are not
persistent they can be stored in file
you can ask me sir to dump the table to
file but there is no automatic
synchronization between what you have in
memory in the X file
and what's on disk you have to ask in
Asia for to make that synchronization
which means that they're not really
database tables
in an array of cells we don't use them I
don't know if anyone uses them
the other is that there is a remote type
which is not a local storage option but
it really says to me is that hey I don't
have it you don't have a copy it's over
there
so I'll start to cry for it we don't use
those either now so in summary sit mini
xia suffers from high rare consumption
because if you want speed they have to
use these copies tables but since you
have lot of data you have the circle
that ear has high rank consumption both
distract and repair of deaths tables if
you have unclean shut down means that
the restart time so slow or long the
discount copies tables have this two
gigabyte size limitation now 11 DB is
something completely different or it's a
key value store there are many of those
around
so what it provides semantically is no
different from anyone else really what's
unique about level DB or broader not
unique but since for many of those not
know this but it's a thing is that it
uses these long structured merge trees
it's a special way of organizing date on
disk traditionally like a couple of
decades ago everything was stored in be
trees these structures are very
different data is automatically
compressed which is good and it's stored
in chunks and files and these files are
then grouped according to age so there
is a sequence of levels from 0 to M 2 P
2 n is 6 so you have seven levels the
old estate without being level
and youngest a thin level zero it's
based on a library from Google pushes
open source this library is actually the
sort of the open source reimplementation
of Google's big table but they couldn't
open sources back down because it had
like big and complex and had lots of
dependencies on Google internal stuff so
we rewrote it simplified it and that's
leveldb now our friends at Paschal I
don't know I've talked to them before so
they use the leveldb as a back-end for
react and in doing so they've done a lot
of work on improving level to be fixing
robustness issues fixing performance
issues they importantly for us they
provide 11 DB which is the NIF which
allows us to import level DB into our
area without that we couldn't access it
directly we have to use some form of
network a connection to a C note or C++
note that word came performance all this
is open source and the API is or what
you would expect get them put and delete
by key and the narrator API which allows
you to iterate through the table or
iterate from a given starting point to
the given key this is what the top level
of a actual live live and DB instance
might look like so at the top we have
the log file this is memory after file I
will talk about it in a minute there is
the manifest file this one lists all the
SSD far as the small data chunk files
that you have contains made metadata
button I will see how that's used and
then we have number of directories with
SSDs files according to the different
levels now you see you can also see that
the SSD files are
of varying sizes that particles or
compression a particles of how they're
produced you may actually when he
produced them you may I should throw
away obsolete data but you're already
committed to producing justice the facts
so you may not produce it as a large SST
for us you might otherwise do now
writing 11 DB you have this fixed size
right buffer the size is configurable
but when you create your instance that
determines the size there's a log file
this is M mapped into the levy DB
library and all the rights are simply
memory stores they do not do explicit IO
not because the file is M mapped things
actually work out ok anyway so rights
are actually placing the faust when this
buffer is full so semantically we have a
key value store so a set of keys and
corresponding values so the contents
that are in the file but in the buffer
one is full is sorted and put in what's
known as a sorted string table on SST
and then treated as a file in level 0 so
if you look back here so one of these
files in level 0 is what you will get
when the right buffer fills out and you
exported the manifest file lists all the
SSD files and metadata including the key
ranges that are contained in that
specific SSD father and I was used for
searches now as we produce data so we
write to write buffer it feels ugly
pretty trees a new SSD file goes to this
we continue writing obviously after a
while was going to have a lot of these
as T files there's going to be
redundancy there
instance we may have deleted keys we
have may have rewritten case that we
rewrote we wrote a long time ago so we
have to do something to not just fill
the disk and that's the compaction phase
basic what it does is that it's focused
on a particular level and then it takes
the SSD files at that level and
knowledge system into the next level the
next level those are the older files as
a result of compaction leveldb produces
new SSD files and the deletes over
months it does not edit the SSD files
once created and that's critical you'll
see why this compaction thing runs
asynchronous in in the backgrounds you
don't have to worry about it it's also
one of the things that Patrick has
worked on to make smoother and more
effective
so the original everly because sometimes
sort of stall due to this but pastures
done some work to alleviate that so this
is what could happen to the left we have
two levels we have SSD 1 and 2 which are
sort of semi old container called keys
or record for keys a and C and D and E
and from a right path every single code
test III with bindings for keys B and E
and the deletion of killing D and let's
say now that the compaction kicks in Oh
again we also have the manifest file
which describes all these files
appropriately so as a result of this we
want to produce new files SSD four or
five so when we merge SSD 3 and 1 we get
a file containing bindings for keys a
and B just a sorter
and when we merged SC 3 and 2 we get sd5
with - wiki's C and E so E is now
rewritten and the old entry for key D is
gone and also the manifest is sorted now
reads they basically go in age order so
you start looking in the right buffer if
it's not there they down level leave it
and looks in the manifest to identify
the candidate ssds using the key ranges
and then it basically searches those
from the youngest to the oldest until it
gets a hit the bloom filters I'm not
sure if that's a bash or invention or -
you just improve them but there is the
Bloomfield desti files can also contain
that bloom filters which are in an
additional way of sort of getting quick
rejection of searches there is no
explicit caching here so to get
performed read performance leather diva
relies on two things having an ample
supply of file descriptors which allows
it to keep many SST files open and if an
SSD file is open and it's memory mapped
then it only has to go to memory to do
such if it has to access a file which is
not open then it has to close some other
file open them on it needed map map it
and then through the search so that's a
bit slower so uses file descriptors and
a mapping for caching and then they
actually date the caching itself it
relies on the operating system so called
page cache and this is a deliberate
design choice by leveldb implementers
okay no me sixth how do we tie these
together so me next is a big patch
Denise yeah I know
9,500 lines sounds scary but mostly it
is boilerplate so it's not that scary
and it's actually heavily tested by now
so it works what it does is that it adds
an interface to meet a schema which lets
you declare a new table type or storage
type so in addition to disk copies and
disk only copies and the other two will
not use you can declare new type for
instance 11 DB copies and then you say
that tables of that type are handled by
back-end which is named by this module
here and you are a total control above
the the name of the type and of course
the not the module once you have that
you can then create tables as usual you
just in the table definition you just
use this type name the storage type name
you invented the way
neasha discovers that this table is of
this newfangled type is that the table
info call for storage type returns a
tuple X and then the name of the type
and then the name of the module that
handles that type now you as a pro
application program it don't usually
don't need to worry about this nice
internally and this is the reason why
the patches are big nice internally
calls table name for storage type all
the time to figure out okay what is this
table represented like given the
particular operation you want to use do
at this particular point within this
patch on the table type and they do
different things and this is all in line
code emanation it's not pretty
so what the patch does is basically
every time Lisa does one of those these
patches there is a fourth
or there's a new clause which matches on
X text tuple and then it simply
redirects to this to this back-end
module and cause a specific function
there and of course the function is
called is depends on the context in
which this operation occurs so basically
this is what Nisha should have been
implementing like from the start rather
than having these hardcore decoded cases
next has been in development by all
eager for I don't know how many years
about many years long before we started
thinking about using it at Rona about a
couple of years ago Donna decided that
let's try to make this reality so we
contract it off to finish it up and also
to interface it with leveldb and this I
guess finished more or less a year and a
half ago some fix ups like a year ago
but you know so basically finished now
Rica Coulson sits over there has been
working with us on tightening up the
patch and integrating into the version
of OTP use at Rona I've been working on
testing the damn thing fixing corner
cases that wolf missed all this all the
things you need to do to get it into
production and actually converting
tables and so on and the entire process
of converting tables and so on all the
tickets and review requests you have to
have to submit this is not quite open
source you can find it on github if you
know where to look but I don't think
it's on on a sort of under clone unable
yet they all know I think it looks a bit
staying but yeah so
there are copies in the wild but not
quite the latest one but it will be open
source trust me
no no me text itself doesn't know about
leveldb it's totally generic so you need
to have a back-end module and that's
crawl on 11 DB in this case so it
Bridgeton bridges the gap between nice
text and 11 DB again it was written
written by wolf
I now maintain it it's about 3,000 lines
of our line not too scary a big gun
server basically the reason it's big is
because you have these two sets to two
worlds it needs that you have a table is
a set of tuples and the set might be an
order set or a bag 11 DB you have a key
value store one key than one value
period so you need to bridge that gap
somehow and that's what basically what
this module does so it Maps between two
sets of so onto a key value module it
Maps keys tuples with keys to key value
pairs in this case what it does is take
the tuple it erases the key replacing it
with the empty list because that's a
nice small term and then it has the
written key and the value which locks
the key that's the stores and then when
you retrieve it it puts the key back in
and houses over Tunisia
that's just to avoid storing the key
twice bags or handled by basically
inventing a new key space where keys are
also numbered it's a bit ugly but sore
bags
also it implements high-level operations
like select using the iterator API and
for reasons I don't really understand it
also handles secondary indexes I think
this is the design flaw but we haven't
done anything about it basically there's
no reason why secondary index should
have to have the same type as the base
table it's just a table so it should
have the same flexibility as the main
table but right now
index tables have to be level DVD if the
main table is let level it also it
maintains metadata for some operations
like for instance the table in for size
call this is problematic I will talk
about it near the end so let's see what
time is whoops okay we have done some
some migrations the first one did not
use nice at all
it's it was a big bunch of the text
files stored straight down in a file
system one XML block per file and it
used a hashing algorithm to spare these
across a thousand directories one of the
problems with this datastore was that
the number of files and the large sizes
of the directories meant that backups
was basic couldn't do and things like an
operator was not allowed to go into the
machine and type ns in one of these
directories because that would stall the
machine so that's a risk at this time
this was this data is from a year ago at
this time these files contain on a 60
gigs of actual data if you just opened
them to see how many bytes order in the
file but in the file system they consume
almost 300 gigs
that's because of fragmentation once we
put it in level DB it consumes
sixty-eight cakes that's largely due to
the automatic compression so there's a
nice gain there from almost 300 gigs -
not quite 70 gigs I did some performance
measurements on axises and we could
easily write in the order of say 40 to
100 gigabytes per second reads were in a
small sort of more narrow bound but
totally acceptable performance numbers
so what's one thing you will notice is
that level DB is faster brightest and
reads the spec of the machine is to
generation of HP 580 with fourth course
I did try / qdv
now I don't know if how many of you try
to use Berkeley DB with Ireland but
that's problematic because there is no
good open source by name for it there's
supposed to be good one that is locked
up in synapse but that doesn't help us
so I wrote a simple one it should
probably really bad what we saw there
was that Berkeley DB thus write read and
foster 11 DB not a whole lot foster buzz
foster but the rights to Berkeley DB was
totally terrible like two orders of
magnitude slower so this just confirmed
to us that no we are not going to use
this the migration itself since the
files were stored in the file system
there was both an internal API and a lot
of code that just forgot about the API
and went straight to the file system
that had to be cleaned up because
everything has to go through the API
once we put it in the real back-end
testing we did side by side for
verification and also food to be about
compatible for instance level DB will
return okay when you de need
called whether or not that record exists
or not whereas when you do it in a file
system you will get different return
rates depending on whether the file
actually existed or not we didn't really
want to play games through the
application so we made the delivery even
back in mailer or what the file system
do also some other things that the file
system did like for instance is
maintained modification time so files
turn out to the code useless so we had
to adjust the back end to emanate this
this behavior and actually keep fighting
modification stamps with the record
migration was very careful we did it
first on a single note did it
non-destructively so we ran it in a
side-by-side we monitored the hell out
of it to make sure that nothing broke
and we will submit kept an option since
with did is not destructively to disable
11 maybe if it had acted it up
fortunately it never gave us any
problems so we completed migration and
did it on all the other notes as well
pack files are similar they are error
terms not to XML blobs lots of forests
lots of directories difficult to do
backups they had a huge size shrink when
we put it in level DB from 600 gigs to
40 the process we went through which
side by side and all of that was
basically the same as will you pay cash
the main difference is that the
background or sampler because we didn't
have to emulate so much of the file
system semantics once we took this live
we have a job which is really heavy on
reads to his pack these pack files now
we were afraid that this Michel slower
they turn out to the yoke itself was
something like you to experience was a
big deal
we have converted the discounted copies
tables are not going to talk about that
and we have not we have converted no we
have not converted the disk copies
tables yet we are in the process of
doing that right now I did a trial
conversion of a smallish table yesterday
so it was 15 gigs in the disk lock files
when we convert when I converted to
leveldb it's like five gigs so on your
third the beam process shrunk in size by
roughly the size of the disk all files
and the start time just with this table
an empty database but this table only
non-empty was four minutes 40 seconds
faster
now this is one table not a very big one
so imagine we have hundreds of tables so
yeah we think that smooth level D we
will do wonders for restore times now
the problem we have is backups the
reason is that our backup strategy wants
to backup so the semi online what we do
is that we basically block writes during
the backup face so that Mesa will not do
any rights and then we can sort of ask
tor basically to archival files while
this is going on reads or committed
obviously however leveldb emphatically
cannot be backed up wide open
don't even try ver the reason is that
the this background compaction thing can
run at any time so even if you're not
actively writing to the database that
can be internal rights and
rearrangements going on and you have no
control of that the only way to avoid
those is
to close the database or close your
handle to the database but that of
course prevents reads which doesn't
isn't quite compatible with our model so
the trick is to do or call quick
snapshots since the SSD files are
immutable what we can do is that we can
close the database we create the shadow
directory we do a hard link to hold the
SSD files and even if there are hundreds
maybe a thousand files this is quick you
copied the log file the manifest file
then you're done you reopen the database
you're back in business
and then you can offline do it archive
of your snapshot quickly the leveldb
manager implements necessary logic to
allow this to happen without disturbing
me see all the rest of the application
it basically implements a shared
exclusive look exactly like multiple
readers single writer lock the API is
called compatible level they 11 DB so
it's a drop-in replacement or a token
wrapper it's quite small like four
hundred lines of airline plus a small
supervisor oh this is how you do a
backup for a hundred gig table is so
flying less than a second so you count
the backups this way two stars left some
improvements so maintaining data
metadata for this table info coca-cola
that is hugely problematic because the
the nice thing about Neven DB is that
it's fostered rights slightly slower
than reads but Fawcett writes what
happens now is that every time you do it
right for a delete to meet the
background from Nisha to level baby has
to do read you know a shake if they
actually
the record existed or not in order to
know how it's going to adjust the cache
size so unfortunately right now reads
writes implement implied reads so we're
not getting the performance benefit from
false writes we are working on we've
eliminated all the frivolous use of
tabling for size from the code we've
been prevented sort of alarms in the
back end if it gets asked for this for a
large table and basically that's I'll
applause we know that we got rid of it
and the next step is to change the
background so that it doesn't do this
anymore
the other problem is this level DB
manageable whose only reason to exist is
to be able to do online backups before
so for that we wouldn't have it in the
stack so it causes latency through
operations so we think that this
functionality ought to be in 11 DB
itself that was all about those problems
nicely that's it for me
think we can afford some quick questions
thank you
did you try to develop debts to overcome
its limitation and if so how is that
I've been working a couple of few years
ago now we did some work on a 64-bit
version of debts we had a master master
student doing most of the groundwork we
got it mostly working but it's basically
not a very good idea the the properties
of debts how its structured on disk the
way it's dirty flags and tells you that
you need to do a repair on this because
you did an unclean shutdown that sort of
works if the datum if it's just two gigs
in the file if you have a hundred gig
file that doesn't scale so it's probably
not really a good idea anyway
so we basically abandon it nobody really
really wanted it anything else yeah
they work yeah Kuehl sees don't work
right now but they might work if we do a
bit they require a bit of additional
interfacing to the background that's not
in place yet I mean with respect to the
transactions in in a museum this offers
a transactional API yeah so what
happened there when when you use it so
in this case when we combine nisha with
a VD you will get actually atomicity
redundantly it was nice itself will will
serialize with respect to this table and
then the back the level leave itself is
all the operations there are atomic so
any reason why mini-z axed should not be
in the mesial code not really just it's
been under active development and we
yeah I can give a shorter answer so it
as he said wolf has been working on this
on and off during many years and it's
it's only recently that it's been that
this big pretty huge patch has been
cleaned up enough that OTP might
consider actually including it so we we
hope we'll be able to push it to OTP
relatively soon now that we've verified
in production that this stuff works
nothing else thank you Michael</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>