<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Sargun Dhillon - Checmate: Lying, Cheating, and Winning with Containers in Networking | Coder Coacher - Coaching Coders</title><meta content="Sargun Dhillon - Checmate: Lying, Cheating, and Winning with Containers in Networking - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Sargun Dhillon - Checmate: Lying, Cheating, and Winning with Containers in Networking</b></h2><h5 class="post__date">2016-11-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/OWCeB9DdLDw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I'm we talking about projects
checkmate and container networking to
start the name checkmate is named after
a hypervelocity whale gun that was part
of the strategic defense initiative that
was meant to stop bad things before it
happened so the drop k's intentional but
let's talk a little bit about Who I am
I've been in the industry for about 10
years and I started in operations and I
moved in to networking and network
engineering and eventually into software
engineering and what I realized as I
moved through the industry from the
bottom of the stack up is that there's
an incredibly large amount of
inefficiencies because we've stratified
these components of the organization and
I have a mission to try to make this
better they didn't containers or a
technology that allows this to work
together and I've been using them since
uh about twenty eleven before they were
cool like they are today and I think
they still have a ways to go until they
can become ubiquitous in industry um but
a little bit of an agenda I want to talk
about a brief history of how we got here
how were trying to make it better and
how some of this might look in the
future but we need to start with a
shared history of networking in the data
center in why it works today so I
started about 10 years ago 2007 2007 was
a radically different time it was before
the term DevOps was ever heard and read
a clear differentiation of ownership in
the organization the data center was
done by a NOC systems were owned by sis
admin's that were responsible for
deploying software and developers were
responsible for features in QA was a
gate between all of this and although
this adversarial relationship between
these organizations had benefits for
stability it made it so that software
was incredibly slow to deploy to
production and it made it so that
developers were typically oblivious to
what was happening in production making
complex systems nearly impossible to run
software was also very different back in
two thousand seven even though hpc had
had advanced in the SAS industry and in
enterprise people
typically statically partitioning
systems and people were typically
statically scheduling workloads own
systems in a given system would only do
one thing and do that one thing very
very well and this this stratification
of the organization led to every one in
the organization being responsible for
one task and one task only in that way
they didn't have a bigger picture of how
the entire organization worked and how
the entire stack worked again leading to
inefficiencies but applications were
changing in SAS was growing at an
incredible rate and they became erased
to ship faster in a year later in 2008
the industry started to change we began
to see the idea of DevOps come about and
developed took took off only a year
later in 2009 there was the first DevOps
day and suddenly that became the de
facto job as opposed to a system
administrator we all saw this idea to
cloud become adopted and it became
saying to run you start up on the cloud
but enterprises realized that they
needed have the elasticity in the
responsiveness the agility that you
could get from the cloud and in software
in 2008 changed as well we saw a
popularization of open-source to
learning to be would run systems at
scale to run data centers we saw the
likes of Jenkinson in cin CD tools to go
ahead and remove that QA barrier and to
automate that QA barrier we saw the
tools a cup-like capistrano so people
could deploy systems to production
without having to have intense knowledge
of system administration tools we saw
tools like puppet and chef take off so
system administrators could control
underlying systems programmatically
versus having to do everything by hand
but we also saw a popularization of
stacks that had more complex operational
requirements the Hadoop's of the world
the no sequels of the world and these
stacks could no longer be run by
administrators that were application
oblivious and in 2011 we finally started
to see the industry change and that was
due to some new enabling technologies
although hardware-assisted
virtualization had existed had been
there for a number of years we're
finally getting to the point where it
was very very low overhead and there was
an explosion the number of applications
in hardware was growing faster than the
rate if the applications were growing in
this meant that we have to partition
systems in order to get better
utilization in the data center an idiot
faster time to deployment because we had
greater applications and that led to the
rise of the the private cloud inside of
individuals data centers and the the
organizational stratification changed we
blended this idea of release engineering
operations in QA into this this is
DevOps thing and we were able to gain
efficiencies but networking remain
separate and network can try to have an
answer to this and that answer was a
software-defined networking the idea of
software-defined networking was to
introduce program ability to these
proprietary segregated middleboxes it
was the idea that we will take these
devices that were built by the likes of
cisco and juniper that we're using
chipsets there were proprietary and
software that was proprietary in being
able to have a common element or a
common mechanism to program it and it
was the idea of endpoint of lobbyists
networking networking never had access
to what was actually going on in the box
because these were vm is this was
software it was deployed bye bye sis
admin's that was independent from the
folks that were actually running the DML
network in the way that this actually II
what was done was was using a technology
called open flow and it was a mechanism
that came out of Stanford it was this
idea this this programming language of
kinds for / Sdn and it was a mechanism
to go ahead and in control flows in
control his intermediate elements that
we're doing forwarding so if an
originator had a connection that it
wanted to make it could go ahead and
send that connection throughout the
system and if an intermediate device
didn't know about that connection it
could ask an intelligent flow programmer
element and it could go ahead and give
that route back to the intermediate
device and take that input and slow
table and send this off but
unfortunately open flow was never never
successful the vendors continue to keep
their systems closed in due to scale
reasons this idea didn't take off
traffic matrices were exploding in the
data center and they were so
unpredictable that we realized that this
wasn't really going to work very well in
the idea of SDN never really took off
but then something happened
and that something was OpenStack in
OpenStack was this magic bullet to allow
the consolidation of workloads by a
virtualization and the belief was the
many many open software-defined
networking vendors decided to finally
make offerings here they took those
existing boxes as existing proprietary
boxes and finally open them up to
programmability music OpenStack had this
idea that it was a bunch of building
blocks and people could go ahead and
swap in components as they required but
they still did the same thing they
replicated the state they replicated
endpoint state and they didn't take
advantage of the fact that these
endpoints were right next to these
middle boxes and after a couple years we
saw another change we saw a doctor come
about and in docker was what made
containers popular but generic
containers existed before this the idea
of jail is the idea of software
segregation but what are containers
actually all about their about user
experience they're about having the
ability to deploy an application to a
machine and having that entire system or
the idea of the system dedicated to you
in your application and this comes
through a set of abstractions this comes
through a set of abstractions at sis
calls mount namespaces etc and we get
isolation an isolation means that I can
deploy a container onto machine I can
deploy multiple containers on to a
machine and not worry about them
interfering with each other and finally
it gives us system oblivious software
and this comes from the idea that I can
build something on my laptop take it to
development take it to staging and
deploy to production without having to
change that artifact but what is the
container where does the rubber meet the
road well there's two primary
technologies that that are predominant
today the first is namespaces in there
what gives us these abstractions there's
these give us the ability to split up a
system into multiple subsystems and then
we have C groups that do this this
resource isolation and I do this task
marking so we can go ahead and take a
set of processes and say that they're in
this hierarchy and and isolate them as
necessary if you look at these
namespaces they tend to
pattern so the mountain namespace was
one of the earliest namespaces and it
had the idea that you could take a given
file system and you can split that file
system up you could take given
directories and remap them into
containers or namespaces as you wanted
and from the the containers perspective
it had the entire file system where it
controlled the mount table paid
namespaces acted in much the same way
where the the host had one continuous
pin namespace but given containers were
able to control pin namespaces
themselves and they got their own pit
one there was a clear mapping in the
clear level of delegation in laughlin
TGS namespaces hostnames postnatal
namespaces work similarly where everyone
was acting on behalf of the host
everything bubbled up to the host but
network name spaces were a little bit
different if we were to take the same
idea for network name spaces and and
continue we would say if there's a
device like eth0 that's connected to the
network we will just mirror that into
the container itself but we didn't go
with this method we didn't go with the
idea of mirroring we didn't go over the
idea of slicing up these devices instead
we decided to use this technology called
virtual Ethernet pairs of vets and we
relied on some magic to get traffic into
the the container but this wasn't magic
at all this resistor standard netfilter
contract in routing subsystems that you
get from the Linux kernel but this was
kind of odd because these these
subsystems were replicated inside of the
container itself visas vet abstraction
only gave us a pseudowire between the
container in the hostname space and we
were again replicating all of the state
we were replicating the state that
existed elsewhere in the kernel and we
were relying on heuristics to reassemble
TCP connection tracking we're relying on
heuristics to reassemble all the state
but why do we need this abstraction why
do we need network name purchases at all
it seems like this shouldn't be
important most software doesn't stress
the network in any particular way well
let's talk about how applications are
deployed typically when you have a
container you have some set of base
components and these base components
probably want to run on the same court
and in previous environments the way
that this was done is that we could give
each one of these basic input each one
of these systems different IP addresses
and give these basic components the same
port and everything would be fine but in
the new world instead when we had a
large number of applications or a large
containers running on the same host we
have to dynamically choose these ports
because we only had one IP address due
to the limitation of these cloud
environments these the inflexibility of
the network need to loop the scheduler
choose the ports if we appear inside one
of these containers it actually gets
quite complicated so first we have to
use that that iptables dean apt to get
into the container and then we have a
bunch of services to assist in running
these containers to make it so that we
can be container oblivious in some ways
in one of the most common things here is
this idea of a request proxy where your
client app that's making upstream
requests is oblivious to the rest of the
system and it's the responsibility this
request proxy to talk to some sort of
registry and undo that mapping but is
there an actual cost to this and it
turns out there's a massive performance
cost so IBM did a study a number of
years ago where they took Redis in Redis
is standard benchmarking tool and tested
dockers default networking versus kvm is
networking and they actually found that
docker networking was ten percent slower
in certain cases as compared to kvm
networking / Kona earlier this year did
a test where they measured transaction
performance on containers versus non
containers and in in Dockers bridge mode
its default mode and they found that
there was a twenty percent decrease in
transactional performance in these
containerized modes in this starts to
make sense when you look at the
networking solutions that are given in
containers it looks like we just took
all of OpenStack took all the solutions
that were designed for virtualization
and carried them over to containers and
somehow we brought the blight of
virtualization with us and this is what
checkmate is here to solve chef me is
meant to be a better abstraction but
stepping back let's talk about use cases
let's talk about what where this
actually is is handy um service
discovery how we actually find the tasks
that are in the system how do you build
one of the service proxies well we need
some kind of directly in that slide
earlier i had the registry and one of
those implementations of the registry
one of the implementations that we have
in our system is called mesas dns and
the purpose of mesas dns is to take
state from the mesas master the system
that knows everything and take its HTTP
API in serve it over dns and it uses the
standard mechanism called dns SRV
records and these SRV records have all
the metadata you'd want a port
information priority information weights
etc and I thought was well everyone has
DNS right in even G Lib C has a bug open
for it if G lipsy supports everyone has
to support it right but this bug was
opened in 2005 and it looked like the
authors of G Lib C had no intention of
supporting this and we saw this
throughout the industry we saw this
throughout the industry where no one
wanted to change how DNS worked these it
was such a complex system that people
were afraid of changing in modifying it
and what people really wanted was this
abstraction of a load balancer a single
point that abstract 'add all of their
services a single IP address and port or
single hostname and port that was all of
their services and they didn't want to
worry about how this all worked Tyler
gave a really good talk yesterday about
load balancing and in load balancing
isn't really about how to distribute
load it's it's people want simplicity in
our first try if this was why don't use
LD preload and for this we have to step
back we have to talk about how how these
socket calls work if any of you have
written some code before you know the
connexxus call is one of the first calls
that you have to make in order to wire
up your tcp connection and when your
task makes this call it doesn't
typically make it directly to the
colonel it goes ahead and uses a Lib C
that converts this down into a colonel
cisco and that Lib C is knows these
thesis call numbers and it passes down
the data structures transparently but if
we introduce LD preload we look then
introduce a shim and instead of people
having to connect to specific IP
addresses and forsett these services
they can pass the canonical name like
Redis port 8080 and we kind of a shim
that can consult an external service
like mace SDNS a direct
and rewrite that address when it gets
passed down to G Lib C and then gets
passed down to the colonel in what we
want is this what we want is the ability
to live to the container and connect to
a given IP address and forget pier name
to work we want to be able to have a
preservation of the standard api's that
we know how to use for software
programming but it turns out this
doesn't work and because of go and in
other languages there's been this
resurgence in static linking and static
linking is basically taking that
conversion component and putting it into
the actual binary itself so we can no
longer insert that shim and it also
opens us up to the to time of check time
of use issues so if we go ahead and look
at this for a given buffer and memory it
could get passed down to the shim and
the shim could go ahead and rewrite it
to a good value to a safe value but that
process is still running in the
background it could happen under thread
running in the background and when that
buffer gets passed down to G lib see the
program can rewrite it to some
nonsensical value or to some evil value
we're trying to do something like
security and although they're just
mechanisms exceed group freezers in sig
stop to prevent this they have
incredibly high performance cost and
they're incredibly complex so what else
is there what exists below that disco
layer well there's the TCP stack it is
there anywhere in the middle that we can
easily hook in in a trend that there is
there's these Linux security modules
that already exists and there's a number
of thing that you might have heard of
already a parmer selinux smack but these
are all programmed using restrictive
dsls restrictive policy languages that
typically lack extensibility they were
created in a time before containers for
example selinux descends from the early
2000s where the NSA was trying to build
a hardened Linux project smack came from
similar origins in this idea of
continuing to introduce new policy
abstractions makes these dsl's unwieldy
so you thought well what if we change
the way this is done what if instead of
having these policies be in dsl's
we introduce the ability to turn
programs within the kernel itself what
if we can run these BPF programs inside
of kernel space instead of kernel space
safely and intercept that sis call and
manipulate it and what if we're able to
take these filters but if we're able to
take these BPF filters and use the
classification that we got from see
groups in order to only affect certain
processes but I mentioned this idea BPF
these internal programs what is BPF many
of you have probably heard of BPF the
bsd packet filter or berkeley packet
filter and it's it's a technologist over
20 years old it's a tried and true
provement technology and it's basically
just an internal virtual machine and
most of you have probably used it in
likes of TCP dump or Wireshark and this
was classical BPF classical BPF was a
very very limited virtual machine where
we had two registers a very tiny stack
only access to integer arithmetic but we
saw a massive number of use cases come
about initially it was only designed to
do socket filtering but we saw it become
popular and other networking subsystems
it turns out that the PPP subsystem
inside of Linux is largely implemented
in BPF even though it's actually in
kernel c code and the one of the biggest
turning points was in 2012 google in
their chrome project announced society
of set comp sandboxing where they were
using this this BPF filters to classify
sis calls and prevent people from making
malicious calls into the system and they
became an obvious need to extend this
and that's what led to this idea of EBP
F and E BPF is a massively more capable
virtual machine we have 10 64-bit
registers a much larger stack but that's
not the interesting part of it the
interesting part of it is the
performance gains that were able to
achieve and that's because it's jaded to
x86 64 code it's also jittered to arm 64
and s390 and then a few other
architectures and this is done because
it's a restrictive vm that has type
safety and it has runtime safety and
this is done because we are able to
prove that there's no unsafe accesses
and this is
that we're able to attribute types to
pointers since we know how the the
programs work instance we know the data
structures that are being passed around
we're able to ensure termination because
we prevent people from jumping backwards
so we don't need to worry about programs
that run forever in the way that this
actually works is that you have a user
space controller and this user space
controller assemble some high-level code
this high level compote compiles down
via llvm into BPF byte code and that BPF
byte code gets passed into the kernel
through the verifier the verifier goes
head and jits that and turns out into
x86 64 executable code so we're not
constantly having to take this BPF code
and interpret it the user space
controller is able to influence the
actions of this BPF program by using
these these BPF maps which are basically
just share data structures between the
kernel and user space there's several
BPF maps that already exist like arrays
hash maps and wing buffers and we're now
able to take this policy and implemented
in NC or implement it in any programming
language that we want so this for
example says that we don't want people
to bind to a specific range of ports
outside of 31,000 200-230 1300 and it
gives the user and immediate piece of
feedback that these ports are not a lot
behind it too but what are the actual
benefits of this this is a lot of
complexity in in this a lot of work to
say that you can't bind to a giving port
well the first thing is that it's
impossible to avoid being in kernel
prevents us this time of check time a
fuse problem and all those data
structures are already copied into the
kernel before we check them and before
we verify them to be safe in before we
verify that there they're accessing
valid values if we rewrite them again
it's in kernel memory so there's no way
that a user can maliciously change them
and it brings back that that standard BS
dapi that I was talking about earlier it
allows things that get pure name to work
and it allows things to receive from and
send to to actually tell you who you're
communicating with in this finally makes
it so that we can use smart clients with
container networking without having to
worry about application layer gateways
that are able to go ahead and under
stand how these protocols work and use
heuristics to rewrite the traffic so
actually is able to connect a ligament
hosts and we're able to introduce new
cisco extensions we're able to add new
kohls you might want for container
specific use cases so what are some
advanced use cases for this the port
problem that I talked about earlier it's
possible to do using traditional
iptables let's talk about one of these
use cases which is an isolation use case
how do i prevent my container from
exhausting my ephemeral ports this is
impossible to do with likes of IP tables
because they don't have this idea of
group by they don't have this idea of
classification by container and it turns
out we can express this in a sea program
you can express this in a sea program in
the code doesn't actually matter but
it's succinct enough where someone can
wrap their head around it and it only
compiles down into several hundred
instructions so it's not taking a given
hit every time there's a packet it's
only taking a given hit every time
there's a cisco and we can ensure things
like we capture every free call and undo
the work that that initial program did
another use case is load balancing so
this this given program here is able to
go ahead and take the sock ladders that
are passed into the kernel do a BPF map
look up and rewrite the socket buffers
so we connect to a new host analytics
security module hooks are pervasive
throughout the colonel they're
everywhere they exist in auditing the
existing file system the existing quotas
but what about performance does this
actually have much of a benefit in what
is that benefit well it turns out that
we ran a similar test to the the IBM
test we ran the Redis benchmark tool
against dr. bridge networking versus
checkmate and we found that we were
about three times the number of
operations per second when we were
putting a new connection / requests in
the requests were very very tiny we
found that in many environments this is
actually a typical pattern whether we're
their applications aren't keeping these
requests pools open because they don't
want to preserve state across requests
since there's inherent cost to it we
found that latency was a third using
checkmate as compared to
the iptables technique and some of this
makes sense some of this makes sense
because we're able to do this once
persist call as opposed to having to do
on every packet and these data
structures are persistent but you might
ask cool what about debugging how do I
tell what my programs are actually doing
if this is all happening in the colonel
I can't use my tools like t-shirt I
can't use my tools like TCP dump well we
have a date we have a way to exfiltrate
data from the colonel and that's the
ring buffers that I was talking about so
these BPF programs can go ahead and
write to these shared ring buffers and
the user space controller can fetch
these and expose these to the user and
what about a control plane that's the
most interesting part of this we already
have one that we're using inside of D
cos or commercial product called Navstar
Navstar is a control plane that goes
ahead and talks to all of the Mesa
agents and builds global state and then
is able to push this data into the
kernel and it's able to store this in a
backing crdt key value store that's
replicated all the agents but how long
until this ends up in your data center
how long until you can use this well
Colonel patches are still in development
but the big thing that's preventing this
right now is we're interested in
building a higher-level language and
we're looking at many tools from
functional programming to do this so
that really complicated c program that i
showed you earlier to go ahead and count
ephemeral ports we went into Erlang is
an incredibly succinct program and this
this functional programming is a lot
more natural means we already have the
idea of types in these languages in the
type systems are quite healthy in these
languages we already have the idea that
we're not supposed to jump backwards
because we don't use loops we can
leverage the internal tail call
instruction to go ahead and in build
these programs up in the way that we can
take an erlang program and compile it
into this is that we can take every sub
stanza of that erling program in turn it
into an individual c function transpile
it to see and then rely on the clang
auto and liner to determine which
components can be have to be in line in
which components can be tale called and
then put those into the kernel has
different programs in tail call between
them but here's a challenge that we
don't yet know how to Saul
how do we continue to cater to the
stratification how do we continue to
cater to the fact that networking and
network engineering and systems in some
environments is still a separate
organization in still a separate set of
concerns and how do we push people to
upgrade the colonel it turns out this is
one of the funniest things that we found
is that people are more than happy to
run five year old Colonels yet they want
to be able to run things at containers
they want to run the newest technologies
alongside their their existing
containers in existing kernels so what
did we learn when we were doing all of
this the first thing that we learned is
that we're probably doing it wrong today
we're probably doing containers wrong in
the abstractions that exist today maybe
at the wrong level in programmability is
probably the only way to fix this the
other thing is that we realize it if we
want performance endpoint integration is
key without having control over the
entire stack there's no way to squeeze
out every bit of performance from the
system that we can but fortunately the
future looks bright and there are
primitives that are coming to make this
easier as the question with programmable
filtering what would you do Thanks do we
have questions nobody
kind of on a side they're linked what
was the crdt key value store you using
so that's something I built it's called
lash-up okay it's an open source project
okay we'll just say was cool lascia last
show yeah we're looking for contributors
oh hi Hey so we've done various things
at fastly using iptables and then ended
up converting them into kernel modules
later in order to like avoid I potato
iptables for exactly the reasons that
you mentioned be so you talked about
this like primarily from like a like a
container networking like point of view
right but like what else can you see
that like this could be useful for so it
turns out that the the actual use cases
for this aren't primarily for containers
they're primarily for people who want to
run secure systems anytime that you can
manipulate the assist calls and do
things like change the interface that a
given socket is bound to its beneficial
but it turns out that there's not very
much funding in high security systems
but there's lots of money in with in
containers so yeah that's why the talks
about containers
as far as the the internal thing goes
kernel modules are really hard to
develop people are really afraid of the
colonel it's friendly it's nice it's
it's C code it's great but but for some
reason as soon as you say the word you
have half the programmers in the room
leave so BPF and in being able to
compile Erlang and run it inside the
colonel is meant to make this somewhat
easier and more approachable so the cost
of a kernel developer isn't you know
hundreds of thousands of dollars and you
know there aren't super rare so I have
another question and would this be
relevant for using raw sockets from with
an erlang in what sense well it's not
possible to directly use raw sockets
from within Erlang but I believe there
are libraries which use in order to use
it on non non Linux systems to use
birthday packet filters in order to
access raw sockets just wondering if
this could be a kind of a cross-platform
solution for that using erling so the
the Erlang here isn't actually erling at
all it's not running in beam it's
compiling into BPF and running inside of
the colonel so it uses a restricted
version of her leg so we have the
ability to do things like lightweight
processes in the kernel using vpf but we
don't have that all the typical
facilities like hot code reloading an
OTP available to us but if you want to
run a web server right a web server in
the colonel you totally can using using
EVP f in Erlang use don't have OTP or
beam what anybody else okay let's think
I'll speaking</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>