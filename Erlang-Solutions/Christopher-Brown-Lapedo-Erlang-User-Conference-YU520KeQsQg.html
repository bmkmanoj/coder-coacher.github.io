<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Christopher Brown - Lapedo(...) - Erlang User Conference | Coder Coacher - Coaching Coders</title><meta content="Christopher Brown - Lapedo(...) - Erlang User Conference - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Christopher Brown - Lapedo(...) - Erlang User Conference</b></h2><h5 class="post__date">2015-07-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/YU520KeQsQg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so we're all very excited I think
at the moment because we're kind of
embracing a new age of computing it's
kind of it's kind of a new parallel age
here I've just got an example of a chip
that you can buy off the shelf this was
released about four weeks ago it's
called an easy chip tile MX 100 and it
has a hundred cause on one physical chip
and this thing is available to buy I
think it costs something in the range of
about two to three hundred dollars so
it's actually very cheap as you can see
here it's got kind of the physical chip
if you like it supports 100 up to 100
cords and there's various different
models of this you can get kind of 50
cause 70 cause 100 cause whatever you
like it has and also the cause are quite
I mean that the x86 calls so they're not
lightweight cause at all that they're
real they're real cause and it has huge
amounts of cash so the whole the total
chip has something like over forty
megabytes of cash and it also has quite
a few a number of and dedicated and
devices such as accelerators network
control units and it supports quite a
substantial amount of memory and there's
something about a terabyte of arm so
these things available to buy the cheap
that they support huge amounts of
parallelism and so what's the problem
well there's kind of another dimension
to this problem so this is kind of the
the the CPU only this is the x86
challenge so there's another challenge
on top of this that's this increasing
kind of awareness of these accelerator
units here I've got some examples this
one here is a geforce graphics
processing units kind of things that are
very good for games ten years ago but
they've kind of developed quite a lot
from that now they used and generally
for lightweight processing they're very
good at
floating-point arithmetic and things
like this another one for example he is
the ultra stratix it has some to make 10
flow at ten teraflops of power is
absolutely enormous amount of power you
can get from these general-purpose units
they're also very cheap so they cost
somewhere in the same region as azzi
azzi easy chip that i showed on the
previous slide so kind of in the range
of hundreds of dollars so for hundreds
of dollars you can get teller flops of
power it's really really quite
frightening when you think about it and
you know ten years ago this kind of
thing was almost unimaginable and now we
have so much computing power we don't
know what to do with it this one is
quite an interesting it's quite an
interesting arrangement here can anybody
see from the slide what these devices
are it's probably not that clear the
Playstations yes yes so this is a I
think this is this was the largest
supercomputer five years ago something
greater than 500 teraflops and it's
comprised exclusively of PlayStation 3's
like 1700 PlayStation fees however this
is it's kind of not a free lunch so
these these hardware accelerated ships
have been around for a long time now but
the problem is programming these things
incredibly difficult mostly that in most
cases you need to be quite an expert so
you need to have lots of low-level
expertise I'm talking things like
low-level programming models opencl CUDA
possibly pthreads on top of that so a
lot of see lot of hacking involved you
need to know quite a lot about the
architecture so what does the
architecture do how do things fit onto
the architecture how does it execute
these things on top of that you've got
how do you paralyze your application so
assuming you've mastered these
techniques how do you go about actually
paralyzing the thing you're trying to
paralyzed in the first place it's quite
difficult and then on top of that how do
you know that when you paralyzed it it's
the most efficient paralyzation for your
algorithm it's almost impossible to tell
I'd say and then what happens if you
change so you go from a GPU to a
parallel ER or something like that you
have to completely redesign your
application from scratch almost to deal
with that so this is incredibly costly
just getting people who are appropriate
experts in this domain but also training
people it's extremely time-consuming and
costly process I think just doing this
in a trial and error approach
exclusively is just not going to work so
just randomly hacking systems to get
them to work in parallel is not the way
to do it that's what I'm going to talk
about today so one thing we've been
working on in st. Andrews is a concept
called a parallel pattern I'll show you
this this is a concept is quite widely
known to a lot of people it's been
around for a long time it's essentially
the idea of it it's kind of like a
function so another line you can think
of this as like a nice higher-order
function but what it does is it
implements a parallel behavior so here
I've just got two examples of these
types of parallel patterns the one on
the left T is called the farm I think of
this as like a data parallel operation
so the idea is you've got some pieces of
work and you've got some computations or
functions that you want to compute over
that work and you want to do them in
parallel so that function is replicated
it could be replicated over an
architecture or the number of CPUs or
whatever the one on the right ear is
called a pipeline it's slightly
different type of parallel pattern and
the things in the boxes here are what's
performed in parallel so for example you
can think of this as like a conveyor
belt with some robots so here you've got
you've got a factory that's making cars
the first robot puts a door on a car a
second one the engine third one the
gearbox and so on so the parallelism
comes from this that each robot
can work on a separate car at the same
time so you get kind of parallelism and
a streamlined where you are than being
data parallel and of course you can mess
these so you can put a pipeline inside a
farm so you can have each work of a farm
being a pipeline and so on you can have
farms nested inside pipelines and and so
on the great advantage of these this
approach is it's extremely high level so
most application programmers they don't
have to think about all the technical
details all the implementation of the
parallelism is hidden away it's provided
and by the by the pattern they just need
to think about how they're going to make
their application fit there's the
particular pattern now this is a nice
functional way of thinking about power
ilysm okay there's no low-level details
no opencl nor spawns and all of that
kind of stuff so what we've done is
we've implemented a parallel pattern
library inner line so you can everybody
can go away and download this it's it's
free we have a website here called scale
we bleed calm so the name of the library
is called skel short for skeleton very
imaginative name it's an it's a
basically a library of skeletons for
Erlang it has quite a number of
different skeletons there and they're
growing all the time so as people are
providing us with new types of
applications we're adding new different
types of patterns to that library the
general API of it is very simple so this
is kind of the top level API of the
scale library and what it does is it as
a top level function called do that
takes a skeleton tree so that so the
programmer simply just describes what
type of skeleton he wants to use whether
it's a farm whether it's a pipeline
whether it's a nesting of farms and
pipelines so he basically just simply
expresses this in terms of an erlang
tuple with some atoms very simple the
input items basically the tasks that he
wants to work on in the pattern so these
could come from the list they could come
from a messages being sent in and so
one and then the result of the of the
parallel computation if you like that
the parallel pattern is the output and
so again this could be a list or it
could be streamed so it could be
messages sent out from the pattern and
so it's very simple to use so what we're
going to do today is I'm going to talk
about using these these patterns for
programming heterogeneous systems so
that's architectures comprising of a cpu
in an accelerator some kind so we have
our our our our patterns we need a
couple of little more tools and
techniques of our sleeve if you like to
do this so one of those techniques is
we're going to use some opencl bindings
for Erlang and these basically provide
functions that executes opencl from
Erlang so they're basically just wrap
opencl so you can call them from Earl
and they provide things like marshalling
so you can you can give it an airline
binary and so on and it will it will do
all the marshaling between the Erlang
side and the opencl side we'll just
convert everything for you things like
functions to copy to and from the
accelerator memory and it enables Erlang
programmers generally to write code for
accelerators and but one important point
here is this alone is not enough this
doesn't simplify programming this is
incredibly complicated to use so what
we're going to do is are just simply
going to use this and but we're not
going to use it directly explain that in
a short while and using the slider you
basically need to write as much cold as
you do with C so you miss all this you
see this offers absolutely no benefit to
an airline programmer using this
directly another tool that we're going
to use as a refactoring tool I think
everybody probably is heard of
refactoring by now one example of
refactoring tool for Erlang is Wrangler
what it does here we've got a screenshot
of the tool it allows people to
basically restructure their code in an
automatic way so if you have a text
editor an IDE refactoring to allows you
to
some transformations on the code things
like renaming moving functions around
adding parameters to functions this kind
of thing but the transformation step is
done automatically so the code is
transformed automatically and the tool
will check all the conditions all the
side conditions for you so we'll check
whether the transformation can be
applied and whether it's going to break
the code whether for example if you're
renaming whether if you're renaming it
to a new function whether the new
function is going to change and the
semantics if it's going to conflict with
any other names and all this kind of
stuff so the refactoring too will do all
not checking for you and then it will
automatically transform the code so as
an example of refactoring tool in Erlang
is Wrangler this was developed by Simon
Thompson I don't think he's sitting you
at the moment but he's around in the EU
see if you want to talk to him about it
it handles the full airline language so
it's not a toy tool it's a real tool you
can use very important in these tools
it's faithful to things like the layout
in the comments so refactoring tool it
doesn't introduce code that suddenly
looks weird or suddenly looks incredibly
complicated it tries it's best to match
the idiomatic style of the programmer as
best it can it has an undo feature which
is very nice so if you make a mistake if
you choose the wrong with factoring you
just go back and it will just reverse to
change its built-in Erlang which is nice
makes it very easy to apply to itself
it's kind of a technical detail but it's
quite nice and this is just a screenshot
here so here is an example it's just
emacs runs in a variety of different
editors think there's an eclipse version
a VI version and possibly several others
now this is the Emacs version the
program has simply just has cold here
like you would normally an emacs he
might select a bit of code of interest
and then there's a menu here refactoring
menu that provides quite a number of
different things he can do he can't
quite see them from the slide but things
like renaming and function arguments and
things like that
so we've built on top of these we built
on top of the patterns and the
refactoring tool and we've created a new
framework called the pedo this this
picture here I'm guessing probably
nobody has seen this picture before
every time I assist in the talk
everybody always looks with blank
expressions does anybody know who this
is but from Kevin of course sorry no
it's not related to anyone in the room I
couldn't shoot so this is a skeleton
that they found in the ice at a team of
italian scientists found that founder
kind of almost completely preserved
skeleton in the ice what was interesting
about him was that he was a mixture of
human modern human and neanderthal so he
was a hybrid skeleton and he was called
the pedo so we thought it was it was a
nice name for our framework of hybrid
skeletons we called it libido so what
the pedo does it extends our skeletons
our scale library with hybrid skeletons
and what I mean by hybrid is skeletons
that allow CPU GPU accelerated
computations to be mixed together so you
can do offloading and things like that
it builds on top of all these libraries
so it just builds it just sits on top of
the of the bindings opencl CUDA
interfaces to open CL code and then on
top of the pedo we have refactorings new
refactorings that do several things
firstly they allow the introduction of
these skeletons automatically so you can
use a refactoring tool to basically
introduce all of this code into your
airline program will do it completely
automatically you can do things that you
can switch between a CPU in a GPU
operation and things like code
generation so all of that stuff I talked
about before about marshalling calling
the opencl using these bindings
refactoring to will just generate all of
that for you so you don't even need to
touch it and and we have an
implementation of this inner line it's
on github so
if you want to go ahead and download
this please do and tell us all the books
at it that exists we love to hear from
you about that so talking about these
hybrid skeletons what are they all about
so I talked briefly before about about
these patterns like farms and pipelines
well hybrid skeleton is quite simple it
basically extends the idea of a pattern
but it adds in a new kind of component
so this is component in pink here that's
what we call a hybrid component so this
is something I can that can operate on a
GPU or some kind of accelerator and what
the hybrid pattern does is allows you to
mix operations that run on a cpu and
operations that run on a GPU together
okay so it allows you to allows you to
mix these operations together in one
single pattern okay and then it works in
exactly the same way as it did before so
here the farm you've got some tasks
coming in you simply replicate and how
many CPU and GPU operations you have and
then the parallelism is handled for you
automatically so there's one one slight
thing i want to add here this is part of
it of a general framework called party
that Melinda will be talking about
tomorrow part of part of that work it's
kind of like a one-stop shop for
parallel programming inner line contains
absolutely everything you need to like
parallel programs inner line and
contains things like program shaping I'm
going to talk about this very briefly in
a moment so don't worry too much about
that but generally it means it means
refactorings to change the structure of
your program so that you can start to
paralyze it so it's like a pre
paralyzation step then it has patent
discovery which Melinda will talk about
in a lot of detail tomorrow it's like a
Google of parallelism for airline kind
of finds all the parts of your airline
code where you can then introduce
skeletons or powers and parts and then
it's got the refactoring which
introduces the patterns then on top of
that we've got lo pido which then allows
you to do the GPU and CPU offloading on
top
that so it contains absolutely
everything you need in one package so
this is what the pedo looks like so here
we've got kind of a diagram here with
with the user Hugh on the left hand side
up the top here this is what the the
user has to provide to the framework so
he provides his Erlang sequential Erlang
application and then the pinky has to
also provide a kernel and this is just
at the moment he has to do this so he
provides an open CL kernel and then what
the pedo does is he uses Wrangler to
firstly shape his cold a bit to make it
so he can introduce these skeletons then
once he's done that he simply introduces
the appropriately fracturing he wants
there sorry he introduces a skeleton he
wants why the refactoring tool and this
gives you a CPU applic application and
then he uses the GPU called generative
and libido which introduces all the
marshalling code to get the colonel up
and running and once he's done that he
simply basically we factors it again to
make it hybrid so once he's got a shaped
CPU application with the GPU cold he
then there we further zwi factors it to
get a cpu and GPU application so it's
just a few clicks of a button and go
from a completely sequential align
program to one run on a cpu in a GPU
almost completely automatically okay
then once he's got that refactored
program this hybrid thing he can then
run it on his architecture at the bottom
the architecture could be a mixture of
whatever he likes so let's just talk a
little bit and in depth about each of
these each of these stages so the
program shaping step is the first thing
he needs to do these are things like it
shapes code so you can introduce parl
ism this can be things like converting
data structures to make them more
efficient it can be things like removing
dependencies from code those you can't
have dependencies when you run things in
parallel doesn't work and it could be
things like breaking up ets tables to
avoid
collision of read and write accesses to
things like that it's a whole host of
different types of things so this is the
first step he has to do then once he's
done that the Hat if he provides a
kernel the Lapita framework will then
generate all of the wrapping called as
the things I all the boilerplate opencl
wrapping calling the colonel and
marshalling the data to and from the
buffers etc so that so the lipid o
framework will completely generate this
and it'll also generate all the cords
offload the computations to the GPU then
after he's done that he can then simply
start introducing the patterns so once
he's got this code generated he can
easily factoring tool to introduce these
hybrid skeletons into his shaped Erlang
program so he does this fine refactoring
tool again and this results in a in a
hybrid application that uses accelerated
components from the previous step so the
code that was generated from the
previous step the Lapita framework will
basically pick that up and link it into
the pattern automatically via the tool
this process is is what we call semi
automatic semi-automatic in the sense
that the programmer has to do a little
bit of guidance so he has to choose
which refactoring he wants he has to
choose which pattern he wants but it's
automatic because all of the conditions
are checked by the tool automatically
the cord has transformed automatically
so he doesn't really have to do that
much so i'm going to show you just an
example of how this works in practice
now this example is called n body and
it's a application from from physics
basically it's used to work out how
particles interact with each other under
certain forces and things like that so
it's a simulation of how particles
interact under an influence of some-some
force and we have a sequential version
of this inner line that we take and at
the moment that the programmer has to
provide opencl colonel we're kind of
working on on Jenna
waiting that automatically but at the
moment he just has to provide it so this
is what kind of the main part of the
court looks like it's very simple so
it's basically some kind of recursive
function that does some mapping of some
computation n-body one over some
particles so it's quite that the high
level structure is quite simple if we
delve into it the bit looks a little bit
more complicated but the thing we're
interested in is this this embody one
because this is going to become our
worker this is going to be the thing
that we're going to execute in parallel
when we're going to introduce the hybrid
patterns so the user basically takes
this he write an open CL version
probably quite simple to translate that
directly to open CL there's nothing
complicated about it he then does some
program shaping so firstly he does some
things like he changes some data
structures in his program so he changes
lists into binaries this is just to make
it so he removes all all the copying
overhead when he's going to do the do
the hybrid offloading this is so this is
a refactoring step he basically just
selects the high level function and says
make this binary basically so it cuts do
question is why would like why would I
do that those are number of reasons why
you'd want to do that firstly you can't
send list items to a GPU that have to be
in a binary form the second one is the
overheads of lists another language
massive because you you're copying all
the time when you send them to devices
so you want to eliminate that so by
introducing binaries you first you
introduce the copying overhead and also
allow them you also allow the data
structure to be marshaled across to the
device makes sense you still look
puzzled okay yep
because this this particular application
it's a list so the programmer provides
it as a list you could miss this step
out if you wanted to doesn't have to
just have to perform this step I'm just
showing an example of one of these
programs shaping steps that's all I'm
doing so the user could could also could
have a firm to binary version that
that's absolutely fine then you can just
completely skip this step altogether
it's not a problem the second step after
the program shaping after he's done some
kind of restructuring of data and so on
is to generate all of this opencl
marshalling code so this is just an
example quite cut down actually if i was
to fit all of this code onto a slide it
would be about 15 slides of code so i
decided not to do that so this code has
completely generated so all he does is
provide the colonel it looks at the
colonel looks at the arguments looks at
the types of the arguments and then it
just generates all of this code you see
it's quite quite involved it's got
things like setting up has to load the
kernel things like it creates a load of
buffers with certain types whether the
read or write only all this kind of
stuff and then it calls the colonel here
and so on and so on so it's a lot of
boilerplate code you really don't to be
writing this stuff yourself it's it
becomes very messy and you can all the
amount of errors you can introduce in
this code it's absolutely astronomical
you really don't want to be doing this
yourself then once he's got that he
basically simply reef actors his code so
he takes that function n body par and he
introduces a hybrid farm then the
refactoring till introduces this code
you see here it's got things like number
of CPU workers number GPU workers I'll
tell you a little bit about how this
information is derived in a moment and
then at things so this is what the scale
pattern looks like by the way so
you've got and an atom here that
describes what pattern you were using
here we're using a hybrid farm and the
farm takes two types of workers that
takes a cpu worker and it takes a GPU
worker so the first one is the CPU and
body one the second one is the hybrid
version that the GPU version so this
this is generated automatically from
from the previous step then you have to
do some other things you have to do some
tagging of the data to to know which
tasks are going to be sent to a cpu in
which are going to be sent to a GPU so
one of the things there's no point just
sending all your data to a GPU or all of
your data to CPU or completely randomly
distributing it that's not going to do
you any favors at all I'll explain that
shortly why so what we do first is we we
profile our data so if the skeleton will
do this automatically when you when you
when you run it so it will take your
data it will create a profile of what
that data looks like so whether the data
is CPU bound whether its GPU bound and
it does this basically by some profiling
running running samples of the data on
the devices and calculating performance
memory usage this type of thing once
it's got a profile of your data it will
then create an optimal scheduling of
that data onto the device so this is an
example of how that schedule looks like
and so the things in the red here or GPU
cores the things in the blue our CPU
cores I've just put in here the brown is
basically other accelerated devices so
cost is give a very good talk before
about using parallel er sorry I didn't I
didn't know your name but Marcus Magnus
sorry Magnus and Kostas both give a talk
Magnus mostly a very good talk I just
wanted to show that our our framework
could sit on top of that so you really
not can constrain to a particular type
of device and if you have a way of
running cold offloading code you can
simply plug it into our framework
so this year just shows an example of a
distribution so the vet is it is the GPU
core the green squares are tasks that is
that the scheduler has decided or
optimal to run these tasks on a GPU to
get the best performance and the rest
it's going to to optimize them on on the
remaining CPU cores dividing them up
based on some profiling or on some
metric so this is done statically so
this is done before the the parallel
pattern is running so it does it up
front it's just an example what happens
if something changes so an application
changes an instance of it changes your
data changes your architecture changes
well you simply all you have to do is
read profile or vivan the skeleton and
it will come up with the new optimal
configuration new schedule for those
tasks so you can see it'll change so
let's say I've added two more cpus to my
architecture it'll work out that the
best way to distribute them possibly
could be as follows so this is an
optimal or n near optimal configuration
so here's some here's just some results
we have obtained from this Lapita
framework this is an example of the end
body like I showed you before this graph
here basically shows the CPU only cold
so what the way to read this is along
the bottom here is number of CPU and
number CPUs we have working in the in
the pattern so one here up to 24 the
left hand side going up is the speed up
so how many times faster it's running
against the sequential version and as
you can see there's various different
versions of the code and this might help
address Kostas point about the program
shaping we've got three different lines
here the red line is a list version it's
actually the slowest and the fastest
version is if we do some program shaping
to make it a binary we actually get a
much better speed up so we go from what
17 to 20
speed up it's actually quite decent on
24 cause it's actually an okay result
and when we use the pedo to offload we
got something like over 200 speed up
when we added a GPU so something like
from going from 20 to 200 it's
absolutely massive speed up just by
introducing a hybrid skeleton and then
allowing it to do the offloading for me
somewhat an order of magnitude better
here's another example and so here the
dotted line here is the GPU and CPU
version this is the hybrid version and
the solid lines of the CPU only version
so you can see here the list version is
actually pretty poor so if I wasn't to
do program shaping and make introduce my
binaries I'd be kind of set at just over
one speed up which is absolutely rubbish
if I make things binary it gets a bit
better but not really that much better
something like a six speed up it starts
to tail off a bit and then if I if I do
an ETS so if I introduce an ETS instead
it's actually a lot better do get quite
decent scalability about speed up of 10
on 24 cause what's interesting about
this is that if we add a GPU it's
actually a lot better it lower down the
there at the end here so if I just look
up to 12 cpu cores and if i add a GPU it
goes from something like seven speed up
to about a 12 speed up and if i look at
the higher end like I think the best
case is about the 13 speed up versus a
10 speed up just by adding a GPU so you
get that slightly better performance and
it could be that the best case might be
to use this point to your GPU and then
turn off a couple of the CPUs because
you don't need them so there's no point
having them switched on we could we
could then use them for something else
and save energy or use them to compute
some other other part of the problem and
then the final version this is slightly
different so that the hybrid version
here is the brown line and we've got to
CPU only versions to read in the
blue the hybrid version actually
performs quite considerably better
there's something like from a ten-speed
up to a 18 speed up just by adding a GPU
so just to start concluding so what I've
introduced is a framework called the
pedo so this is a framework that allows
it helps you to write applications for a
heterogenous systems or systems
comprising CPUs and GPUs or CPUs and
accelerators and we have refactorings
that automatically introduce the
skeleton and the the accelerator code
into the Erlang file automatically I
have automatic generation of all of the
offloading computations and data
transfer boilerplate codes or generate
all of that for you and we've got pretty
good speed ups for a number of use cases
we've got at the moment we're currently
writing paper about this so if we want
to know a lot more detail about this
work happy to provide you with copies of
that so that in the future we've got
quite a few things we need to start
thinking about one thing is is this
generating these kernels so the moment
I've kind of said well user has to
provide this we're not thinking that
this is a long-term thing and once we
figured out how to generate these things
we will add that in and also integrate
more complex mapping mechanism so things
like how do we know how many CPUs to use
and how many GPUs to use and how do we
know which pattern that was supposed to
use should we use a farm or should we
use a different type of farm and so
we're currently looking at heuristics
and machine learning mechanisms to deal
with that so the idea is a machine
learning mechanism will tell you the
most optimal configuration those optimal
mapping was optimal architecture the
best pattern to use if you give it an
application we're looking at a lot more
realistic use cases so we're looking
into this and we've got a lot more
patterns to consider synopsis farms and
pipelines which are kind of them
simplest cases of pattern and just a
kind of a Thor ways slide at the end we
need people to help us with this so if
you want to help us please contact us
the libido and rephrase which is an EU
project that's supporting this work you
can join us on a mailing list which is
provided here you can get things like
news access to all of our software you
can get updates you can chat to us you
can come to our workshops we have tools
for Erlang and some other languages as
well so feel free to help us in any way
you can but we'd really love to hear
from you and that's all I wanted to say
so thank you very much for listening and
we have time for a few questions but the
you look at the speed up you sure yes so
you need to compare to get ne ne ne so
let's take the previous one I think it's
more interested so this one if I
understand correctly what's happening
and I think I do is that you are
comparing something which is entirely in
Erlang which something that is also has
some opencl code yes okay so this is
starting to comparing apples and oranges
because you have to show what the opencl
code alone will be doing so also if you
look closely here the speed up you get a
speed-up curve that you get in your line
is pretty good while the other one
already starts with something that is up
in the nine range so what would happen
for example if you had the here machine
with 64 course perhaps the online
version could have been better I don't
know or part about the girls down life I
all perhaps goes down I don't know
ok can I guess I guess the point is you
know why would an airline program in
this particular bill perhaps in the
other ones it might be a point but in
this particular one it looks like
airline is pretty good actually ok can I
answer your point ok this to basically
two questions so for the first question
is why are we comparing opencl to
airline because opencl is obviously much
more trusted inner line so it doesn't
make any sense to compare that i'll
actually argue that that's not what
we're doing the kind of the point of
what we're doing is to use Erlang
because it's nice and high level so
writing called another line is much
simpler than writing called an open CL
and at the moment the programmer has to
provide a colonel we understand that
that's not a very good solution so we're
looking into ways of generating that
automatically from another line function
so the idea is programmer mode user
language is nice and high level he would
express his algorithm and then it would
generate an open CL colonel then will
allow you to offload ok so the
programmer doesn't have to learn NEC
doesn't have to learn any open CL you
just he's kind of in a nice high level
language which is one huge advantage I
think and the second point not quite
sure exactly what your point was and
what you're saying if we have if we can
add an infinite number of cos oh ok 64
that it might go up well it might go
down also the points on this graph is if
you look here ok that's not quite as
good but let's say I just picked this
point I can turn off all of these cause
I can i can almost turn off half the
amount of calls that i'm using on my
system and use them for something else
so why would i use why would I use that
point when I can use that point does it
make any sense
well okay yeah which saving huge amounts
of energy
so I was just going to follow up on
custis's point which is obviously we
haven't tried this on 64 calls you can't
really project up from these grants 64
maybe we can do that but no matter where
you are in this you're getting benefit
from using the hybrid approach the GPU
is always adding and it will it is
extremely unlikely in fact almost
impossible but you will never get some
benefit the reason why you're not
getting as much benefit from the GPU as
the scaling up a lot of course is the
workers diminishing Costas so actually
what's happening is more of the work is
being mapped onto CPUs less it's being
done by the GPS there's a bit balancing
dynamic balancing happening between
systems but if you have a larger
application when the GPU is just going
to keep adding out of the performance
I'm actually disagree with your previous
point that erling is a nice language for
technical computing or numerical
computation because in my opinion it was
much better to use some kind of dsl
matlab like Julia like styled yourself
or for the poor describing actual
computing tasks and then make ohmic
scheduling this okay so I mentioned we
have this framework implemented for
other languages so if you don't like
girl I'm just use it for different
language that the phone working is
completely generic it's not tied to
Erlang in any way so we don't like girl
I'm just don't use it so it's my answer
final in question I've got some existing
CUDA kernels could I use them be the
stronger yes yes so you yes you can use
them in a number of ways so you can
either provide the CUDA kernel code is a
bit different because we don't have
bindings yet for that and but it
wouldn't be a very difficult thing to
add bindings for CUDA
so you can either just provide the
Colonel's directly and then use Erlang
to do all the coordination at a high
level you can also then if you want to
you can provide airline implementations
of your kernels as well and then try and
get a mixture of using all the
architecture so you can either just
provide the kernels and use your GPUs
and use the data hybrid skeletons to do
the coordination or you can try and use
both of the things and yeah so yes</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>