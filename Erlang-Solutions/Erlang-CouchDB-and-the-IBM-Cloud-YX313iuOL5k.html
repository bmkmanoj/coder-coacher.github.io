<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Erlang, CouchDB and the IBM Cloud | Coder Coacher - Coaching Coders</title><meta content="Erlang, CouchDB and the IBM Cloud - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Erlang, CouchDB and the IBM Cloud</b></h2><h5 class="post__date">2015-07-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/YX313iuOL5k" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay thank you mud and that introduction
and welcome everyone to today's webcast
so I'm going to kind of go through this
presentation in three parts right the
first part is loud and said it's going
to be about you know why we chose couch
to be and and subsequently why we chose
our line to power this Vlad database
service and not only discuss you know
some of the architectural implications
but also the implications for the DevOps
life cycle right and the second part of
the presentation is going to be able to
preview about CouchDB 20 some of the new
features some features which may be
familiar to users of clouds but
unfamiliar to users of couch TV itself
and kind of run them through some of the
ones that I think are more impressive
this release has been a long time coming
in there's lots of really great new
capability showing up and you know
available in the master branch today and
I want to kind of share a few of those
with you and then the final piece I want
to talk about is just you know the few
reflections on what we've learned as a
community as this project has evolved
you know over the past seven eight nine
years and you know things that we have
grown to be sort of staunch believers in
as essential to the establishment and
growth of successful healthy happy open
source communities so with that let's
kind of take a step way back to the
beginning of the founding of cloud you
know I want to kind of talk about the
story in terms of a personal journey so
when my co-founders and I were you know
working in you know a research
experiment in physics and we wanted to
start this company the first thing we
said we wanted was to build a
horizontally scalable distributed
database system this was 2008 and such
database systems were you know sort of
the purview of the big web companies but
there weren't any open source offerings
out there that played in this space
successfully right and of course we
weren't the only ones with this idea in
the year after we founded the company
the the no sequel term was born we
helped organize one first conferences
for and of course now there are dozens
and dozens and dozens of databases that
have cropped up and really spawned a
king brain explosion of systems that
have done some
very innovative things and you know have
begun to learn from one another and
really sort of mature the ecosystem into
a set of products that are useful for
running production applications with
demanding scalability requirements
demanding availability requirements but
in the beginning these things weren't
out there and we said we should put one
we think there's an opportunity for that
we wanted to kind of look around and see
what prior it was out there and see if
there was anything that we could evolve
and we bumped into couch TV and it's you
know at this point in time and in couch
TV was an Apache Incubator project and
you know was saying all the right things
right a couch TV was claiming to be
built for scale right to be making
conscious design decisions that would
facilitate you know the running of
highly scalable systems the interfaces
looked great night as researchers we
didn't have a strong background in
traditional relational database systems
and this kind of dynamic developer
friendly interface you know database
exposed as a web surface with HTTP
interface and JSON payloads it's
something that was very easy to kind of
get up and running with compared to the
other alternatives that were out there
in the final piece that really impressed
us about Cubs TV was this kind of nifty
take on replication you know most
database management systems do
replication using some sort of log
statement replays and you write any
updates to the database into a commit
log and then that commit log gets
replayed on another replica in the
database cluster CouchDB and other
systems like it take a different
approach where they actually rely on the
ability to synchronize changes to each
individual piece of data across these
systems rather than replaying logs
they're actually comparing data element
by data element the contents of two
different databases and executing the
operations that are necessary to
ultimately synchronize these systems and
it's a fantastically flexible system and
one that opens up a lot of new
possibilities for what it means to have
databases that replicate with one
another including you know databases
that live on mobile devices and you know
interesting things like that whether
device might be disconnected for
arbitrary periods of time
so the more we looked at couchdb the
most it okay this this seems cool we
hadn't thought our way through all of
the implications in the replication
stuff but we said you know the
scalability piece looks like the right
thing you know the interface looks like
what we would have espoused right and
then we sort of got more familiar with
the code and we bumped into early and
you know my co-founders and I were
coming to this venture from a world of
C++ for the heavy duty numerical
processing and analytic routines of used
in experiments and Python for a lot of
the scripting and you know kind of
management facilities and that was just
kind of kicking off hey so we looked at
our line the first thing was well you
know you get concurrency out of the box
right no more kind of blowing your foot
off with C++ or working around the
global interpreter lock this is the
seems great and then we said well we
wanted to build a distributed database
how does you know there's are lying help
us there and it's like oh this looks
like free distribution right you know I
can send a message to a process on
another node in my database cluster just
with the same syntax that I send a
message to processes on the local
machine this sounds fantastic so we were
really smitten with those
characteristics of the language right
and you know said okay great the airline
seems like it's it's on a good path here
it seems like the right kind of language
for building cloud servers cloud
services and we should go teach
ourselves are lying in order to evolve
the system the way we want it to for our
business so how do we do you know what
six six and a half years later we're
still here so I think that's one measure
of success right you know couchdb is is
so strong and growing Loudoun is still
delivering its service to you know
thousands of customers every day so
something went right and i think you
know we've we've absolutely found that
you know the use of CouchDB and Erlang
underneath have been strategic
advantages for us as we've delivered
this service I think the reasons that
they've turned out to be so important
and maybe been slightly different than
what we initially expected right so one
of the things that we didn't think about
at first but turned out to be really
really important is you know couchdb is
a database that really cared about
getting the durability story right it's
an
silly but lots of database systems make
trade-offs in this space in order to
kind of move quickly right or don't sort
of fully think about the ramifications
of what it means to durably store data
on disk and all sort of variance of
Linux systems and cloud servers I know
our infrastructure team at cloud in jizz
got an open ticket with a provider right
now that shows an exit fest filesystem
sort of randomly unmounting right and
having a database system that has
thought ahead about handle the how to
handle those kinds of crazy situations
is really really helpful it's you know
it's kind of saved us time and time
again the the sort of very well thought
out very altima tlie very simple and
elegant approach that couchdb another
kind of purely a pendulum e-systems take
to storing data has proven to be a real
sort of help for us so the durability
piece was great replication true enough
that remains the killer feature and I
think we found that time together this
is this is something just really
differentiating about this project and
something that we've built more and more
of an ecosystem around as time went on
on the airline side of things one of the
things that we learned right is that
there actually is no such thing as a
free lunch when it comes to data
distribution the the syntax and
semantics are great but you know I think
as most developers of distributed or
allowing applications can tell you or
should be able to tell you you know that
you do have to still think very
carefully about what the control plane
looks like what the data plane looks
like what is the distribution of
messages that you're sending across the
different channels you know these these
things you don't get to sign step to
kind of practical realities of
distributed systems just because you
have a language that embraced the
concept of distributed application in in
its own you know framework but that
being said you know we continue to
leverage the Erlang distribution heavily
in all of the database clusters that we
build and we've done a number of
optimizations to ensure that we're able
to provide a high throughput low latency
database service that makes the most of
this distribution bird call when you
know when it makes sense on the other
hand one of the things that we didn't
even consider at all
when we were starting out and turned out
to be a huge huge help is just the
ability to dive into these virtual
machines as they're running you know I'm
hardly the first person to mention this
characteristic of the virtual machine
but I do want to reiterate the comments
and made by others in the community you
know going back to a system that doesn't
have this kind of live introspect
ability is it's kind of a scary thought
in some respects for us now you know
certainly you have to be a heck of a lot
more careful about figuring out what
sorts of instrumentation add with what's
bugging dad and those things continue to
be very very important for people
building or lying applications but
there's nothing like finding a
Heisenberg of sorts right the kind of
thing that doesn't lend itself to easy
logging and debug and instrumentation
information and post hoc analysis of a
situation something like finding one of
those and being able to hook up to a
running system where the bug is
manifesting and diving in and inspecting
the live state of running processes and
how those processes are interacting
right that's you know an incredibly
powerful capability and one that is is
mostly unequal throughout the space and
certainly as you know more and more
applications move to you know
software-as-a-service move to cloud
applications where you know the
operations team development team are
kind of co-located I feel that this is a
fruitful ground for erlang in general
it's been certainly a very very powerful
asset for us in cloud so I think that's
the kind of quick rundown in it and we
can certainly dive into more details
about how you know what operational
talking we've built into into clouds and
how kind of Erlang fits into our overall
development lifecycle on those sorts of
things but I do want to make sure that
we reserve enough time to talk about
some of the interesting things that are
happening in the couch TV community and
so I want to kind of switch to the
second portion of this talk now just
really what's new in CouchDB to fidel
right and there's four things that I
want to focus on the first is the
database clustering piece this is
something that we developed
internally Cloudant and open source a
few years ago but because it was a
fairly substantial change and actually a
fork of the code base at the time we had
to do a lot of work to kind of get it
into a place where the two auto release
was really ready to go I'm happy to say
that that's you know moving along really
really well now all this code exists in
master and so on and so forth so what
does that really look like okay well the
way that we've chosen to do this is to
utilize consistent hashing for the
database so the idea here is we're
taking a single logical database couchdb
database and distributing it redundant
me across a cluster of servers right so
a database clustering is governed by two
parameters the partition count of the
shard count needs these times
anonymously and the number of replicas
of each shard right this is relatively
standard stuff we use consistent hashing
to route the documents to the shards and
for those of you for whom that's not a
familiar concept it's actually very
simple you simply take the primary key
of the document you hash it that gives
you a value in the key space of the hash
function or the output space the hash
function you can represent that as a you
know continuous ring and then just
simply carve out equal partitions of
that ring right so this maps you know
potentially very large space of primary
keys numbering in the millions for some
of our larger database is into a very
much more finite number of partitions
all right so the amount of cluster state
that needs to be maintained is really
just what other petitions of each of the
database and where the replicas of those
partitions hosted across the cluster
right so many you know managing that
amount of cluster state is is relatively
straightforward that in that cluster
state has to be gossiped throughout the
cluster so that each node in a couchdb
cluster is able to independently know
how to route requests for particular
documents and you know we have a
straightforward solution to that which
is just to simply use you know CouchDB
replication vertical so that's like a
little no global database it's gossip in
between the servers in in the airline
cluster right the other thing that you
know that we're able to do in this case
is because we're using a nice database
to support this management of the
partition table we can afford to do the
partitioning for each individual
database separately which is awfully
useful if you have databases that are
growing at different rates databases
that you know have different needs in
terms of their you know resources that
are being consumed in the cluster and
also useful for situations where new
databases are continuously being created
and you'd like to continuously grow the
cluster you can sort of define placement
strategies so that new databases land on
less loaded nodes and just sort of fill
those up as you go along so there's a
lot of flexibility basically in the in
the use of databases to model this
partition table for each larger
clustered system so that's how we you
know route the documents to disk so
let's talk a little bit about the way
that queries work couch jeebies got of
course the basic crud operations on
individual documents and we also have a
number of systems for building indexes
on you know attributes of these JSON
documents other than the primary key and
so the document operations in a
clustered to auto CouchDB those document
operations are subject to corn protocols
and Anna Coren protocol the system is
actually going to ask each of the hosts
that's responsible for storing a
particular document in the cluster and
to you know acknowledge the operation
and it will by default wait until a
majority of those replicas acknowledge
the operation before acknowledging back
to the client you know the operation was
completed and so when you go to write a
document that write request lands on any
node in the couch TV cluster that node
knows how to compute the right hosts for
the document using consistent hashing
logic that we talked about in the
earlier slide and it sends the request
it forward the request on using
distributed airline to those individual
hosts those hosts write the document to
disk in the appropriate shard file and
then send an acknowledgement back and
similarly on a document read request the
coordinator node we call it which could
be any note than cluster right there not
special each node can function as a
coordinator for any request the
coordinator looks up the right
information and asks the replica
is to retrieve the document okay and you
know there's a bit of subtlety in these
systems that has to do with reconciling
any divergences it's an eventually
consistent system and it's certainly
theoretically possible and something
that we see in practice that the
individual replicas don't always have
the exact same version of the document
and in this case we rely on the same
version tracking information that couch
TV has always used to understand the
Edit history of documents that are
replicated between systems we use that
same revision tracking information to
reconcile replicas of a document within
a couch TV cluster okay and so that's
where we're able to execute a you know a
read repair function but those of you
who might be familiar with these kind of
dynamo based clustering systems where
the coordinator can detect that there's
an inconsistency in the versions of the
documents that are stored in the cluster
and if it detects that inconsistency it
also knows how to repair it because the
lineage information is tracked and
stored with each document so it can
execute that repair function and then
return the result of that repair to the
client in addition there is an active
anti entropy mechanism that's always
running in the cluster which is sort of
proactively reconciling any divergences
that might have existed between charred
replicas so that covers all the document
operations right for the views or the
query indexes that we'll get to in a
little bit this system is a little bit
different right there's no forms in this
case in part because views can have
aggregations associated with them and
you're not any longer talking about
results that are comprised of you know
information from one document but
they're actually an aggregation of both
you know input from multiple documents
so what we do in this case is we
actually have to ask every you know at
least one replica of every shard of the
database to give its contribution back
because the other thing that you know
happens with these secondary indexes is
that we don't have priority information
about which shards actually contain
relevant entries for the query right the
the indexes are built local to each day
to day shard
now that you know they're indexing on
things that are not the primary key we
don't get to benefit from that
consistent hashing routing information
so we actually have to have the
coordinator forward that request on to
every partition and then you know we've
respond once we've received results from
one copy of each partition and so it's a
good system in that you get high
throughput for indexing if you need to
scale out the indexing power associated
with couchdb cluster it's
straightforward to build a database with
the larger number of shards and
distribute those throughout the cluster
however there's a fixed overhead
associated with each query and then you
have to hear from each you know on at
least one copy of each shard so there's
trade-offs which i think is what
distributed systems programming is all
about so that's the quick run through of
the clustering system the next feature
that I'm really excited about income
cd20 is all of the new work that we've
gone in the user interface system those
of you have seen CouchDB in the past
we've had a management interface called
futon that shipped with dodgy beings
delivered his web application and it's
certainly served as well over the years
but it's also has been getting a bit
long in the tooth futon ship with you
know releases of CouchDB dating all the
way back to two thousand eight and so
what we've sort of a bit of work that
we've let it cloud engine if have you
know really pushed forward in the open
source community is a rewrite of that
user interface picking up some you know
newer client-side JavaScript frameworks
mix of backbone and react and building
something that was architected in a
cleaner way to allow people to come in
and contribute more easily and to add
plugins to add modules and do you know
give us a foundation on which we can
really evolve the the kind of portal to
the database over time so this is a you
know codename photon it's shipped as an
experimental user interface and the
latest one dot x releases of kashi bien
is the management interface user
interface for the 20 release so I'm very
excited about all the work that's gone
on there and really about the ability of
that team as well to to iterate and you
know build things quickly and also to
sort of simultaneously served
the open source community as well as the
cloud and user interface which is you
know downstream from this that's been
very successful kind of open source
collaboration going on for a while let's
vote on next I want to talk about how
you query documents in couchdb right
couchdb has always had the the MapReduce
view engine right and this is you know
MapReduce shouldn't be confused with
sort of Hadoop flavored MapReduce right
this is really a system to allow you to
upload some code to define indexes on
JSON documents okay instead of using a
create index statement there was a
function the function accepted JSON
document is an input parameter and could
emit 0 or more key value pairs from that
function those key value pairs would be
maintained in a sorted index and you
could use HTTP requests and query string
parameters to slice you know regions of
that index that you had built then you
can optionally add a reduced function to
build some aggregations it's a flexible
system and you know it allows you to
craft but we also recognized that there
was a learning curve associated with it
MapReduce sounds like a scary concept
and you know it's perhaps a bit
unfamiliar right and so what we've done
is recognized that you know other
document databases like Mongo have
provided ways to define indexes and
execute queries that have proven to have
a less steep learning curve and so you
know your credit where credit is due
we've you know kind of worked with that
team to adopt significant portions of
the mongo DB query language and expose
those in apache HTTP so as a you know
the 200 release one of the things
couchdb you can do is support a fine end
point that will accept selectors that
look an awful lot like the selectors you
would use if you're using MongoDB it's
possible to use most of the kind of
special dollar operators then long going
to be supports it's possible
field selections and the system will you
know it allows you to define one more
indexes on different fields the system
has you know a query optimizer that
tries to select the best possible index
to satisfy the query that you submit and
so it's a you know it's it's clearly a
move to try to make the system easier to
use and the other thing that it does for
couch TV that I'm excited about is
because it's a declarative language
because we have this optimizer in front
we can also start to unify different
indexing systems that are available
under the hood right you can think about
a world where the full text indexes get
surface to do that saying unified query
system and ultimately you know make it
possible for people to build compound
queries that are served from a mix of
full text indexes and traditional be
tree based indexes on a single query I
think this is again you know it kind of
really solid foundation for us to
improve the way that people find
documents in their couch TV databases
and you know improve the kind of use of
the overall ease of use factor of the
system then the final thing I want to
talk about is a bit of inside baseball
on the internals right but I think it's
a really nice one for anybody who's
trying to use couch TV with very large
databases that skin off a lot of Rights
so you know I've mentioned a couple of
times the couch TV has it's very
bulletproof durability model right the
way that durability model is achieved is
by refusing to ever update anything in
place everything in the couch TV
database is a purely a pendulum index
structure and you updated documents that
new version of that document is written
at the end of the file and all of the
edits to the individual indexes that
allow you to find that document are also
appended to the end of the file and then
those updates are only made visible once
a header that points to the new version
of the index is written at the end and
you know durably committed right so it's
a very nice clean system that results in
a very crash friendly behavior there is
no such thing as an unclean shut down in
the couch TV in today's but the
trade-off is you know wasted space right
all
those old index pointers are no longer
valid and the amount of state you know
sort of garbage that gets generated with
each document update is you know it's
measurable it's substantial we've always
had a compaction process if you use
postgres you know that you need to
periodically vacuum your databases this
the the concept is fairly similar here
right but in one dot actually had you
know some some deficiencies in the way
the compactor worked and you know those
resulted in behavior that was a little
bit hard for users to you know to know
about a priori right turns out that
databases you know the degree to which
we could compact them depended on the
way that you chose your primary keys it
depended on you know whether you had
stabbed random IDs or not and if you did
have a so-called poor choice of ID
scheme then the best we could do was a
compacted file that still had wasted
space and we spend a lot of resources to
get you there too so it wasn't great and
the solution for this is quite nice
right it's you know it sounds simple but
working you know in the database
internals in a system like this is is
never really you know truly simple so
what we do now is when a database gets
compacted there's a temporary file that
maintains sort of the live entries from
the you know the primary key index
extracts those all out of the main file
and then allows us to write those things
in the you know in the optimal water
into the compacted file right so that
ordering system has you know a whole
host of benefits right it means that the
final file size is a lot smaller because
the temporary file pretty much sort of
swallows all of that wasted space that
used to be delivered into the compacted
file it uses fewer resources overall
because the temporary file gets written
in you know a much simpler format that
doesn't have to care about the final
index format and we're able to achieve
higher throughput to in part because
we're trading away a lot of random reads
that used to be used to update databases
that used you know random choices if I
eat right so it's a it's a win-win-win
as far as the compactor is concerned and
you know aside from being sort of much
more performant overall it also
you know makes the system more
predictable in terms of the resources
that it consumes regardless of the way
your application chooses to assign IDs
in the overall space right and so I
think you know this is something that
we've seen sort of quite often in heavy
duty production usage of cloud CouchDB
certainly within clubs and also within
you know reports that have come in
through the open source community in
general and the new compactor i think is
getting to be a very welcome addition in
that front so database clustering new
photon user interface the mango query
language and this rewritten compaction I
think are you know four key highlights
that I'm really excited to kind of see
go out the door with to that I'll so
yeah I think it's a it's really an it's
an exciting time for for couch TV in
general that release coming out I think
it's an opportunity to reflect a little
bit on you know where we've been and you
know what's worked how to really drive
you know a successful and exciting open
source community we've come a long way
since this original C++ an xml code base
I the apache software foundation has
this saying this model the community /
code right and you know we it's it's
easy to kind of say that and walk away
but I think we've from over the years
come to appreciate the benefits of
taking it seriously and you know have
become from believers in the value of
that concept last year we put together a
code of conduct for the project and you
know junctures a really led the effort
here and you know is a rousing success
the code of conduct was adopted by the
apache kob-tv project and subsequently
adopted basically unmodified that the
apache software foundation as a whole
and one of the things it did was you
know it takes a chance to introspect a
limiter to reflect on how we were
running the project and what we were
prioritizing and what we were what
behaviors we were encouraging in our
community and whether those were the
behaviors that we wanted couraging right
and it it also you know as we looked
through it
as we kind of wrote down what kind of
community we wanted to have right it
naturally caused us to kick off some
initiatives that helped us get to that
community and I'll go into those in a
little bit you know more detail but I
think that that was um you know there
was a lot of a lot of interest in
establishing these codes of conduct
around that time but i will say overall
i think was very positive experience for
our project kind of go through that
introspection the second thing that we
learned is that you can never do enough
to eliminate barriers to adoption right
you know it's easy when you're sort of
deep into your own projects to forget
about those little quirks those little
errors and the documentation for getting
started right the kind of five
additional steps you had to do when
setting up your development environment
you know the thing that you had to
google for one stack overflow to find
the workaround for right because it's a
one-time thing and after that you're off
and it's you know working on your
project right and the trouble is that
every single person who you know if
thinks about joining you in this project
is going to bump into a very similar you
know situation right and you know it's
easy to it's easy to miss this kind of
work when you're thinking about okay you
know what are the blockers for us to get
this release out the door what are the
things that you know are prioritized in
the backlog right but I think what we
found is that every time what we've
invested in eliminating barriers to
contribution eliminating barriers to
adoption we've been paid back in spades
right maybe one of the most sort of
stark examples of that is the work that
we get to bring the source code
management system up to get right for
those of you who've been around Apache
for a long time all of the apache
software foundation projects were hosted
in subversion for many many years make
sense right subversions an Apache
project itself but you know we also
recognize that these you know by using a
distributed version control system we
enabled a sort of easier collaboration
with members of the community to work
committers right and so we sat down and
really kind of worked closely
the apache software foundation infra
infrastructure team to first prototype
and kind of beta test the the
integration first Emmure of subversion
repositories in to get and then
ultimately allowing for right axis and
you know that that certainly was
something that you know we could measure
very very clearly the rate and paste and
increase of contributions that happened
when we were able to first provide
access to get and then also mirror those
repositories to github may then the
whole apache you know a SF has really
come around on this to the point where
now you know we've got a very nice tight
integration with a pull request novelty
asf and you see many projects especially
newer projects at apache who you know
are happily kind of transcending the
boundaries between you know a SF hosted
infra and other third-party applications
with the goal of just providing a you
know a welcoming community if you want
to submit a patch on github we can you
know figure out how to incorporate that
into work flow if you want to submit a
package here we can figure out how to
work that into right asking people to
conform to the ideal process that you
developed because you think it's a more
efficient model of working is you know
sometimes necessary but you really have
to ask yourself if it is right and
whether there are ways that you can kind
of meet people where they choose to work
and and bring them in that way no and i
think the last thing we learned and this
is one that you know was a a sort of
significant evolution for the project
management committee over the years is
kind of when do you choose to bring on a
new committer right since a you know
it's it's a significant step you're
saying you know I trust this person with
access to the source code repository and
you know the the wrong commit in the
wrong place in sort of seriously damaged
you know the project's reputation can
damage the businesses that are built on
that technology right it's a you know
it's a it's a significant step but what
we learned is that you know in fact
people are generally trustworthy and
just because you've given access you
know commit access to someone doesn't
mean that that person's going to go
start refactoring you know the core
database internals in a way that doesn't
make sense right
so rather than treating it as kind of a
a big gatekeeper staff right thinking
about the commitments in an open source
project as a you know a public
declaration of trust is I think really
powerful and what we've seen is that you
know by making that public declaration
of trust we have time and again sort of
notice that that actually incentivizes
significant you know kind of deep and
commitment to the project in an
increased level of contribution and so
we you know went on this kind of very
conscious decision to foster did you
know really grow the committer base
quickly and you know it by all accounts
I think it was a very sort of rousing
success right I think you know that also
kind of goes hand in hand with the
notion that contributions come in all
shapes and sizes I too many open-source
projects are kind of think of committers
as the folks who know how to evolve the
core code base when in fact you know a
successful project depending on a lot
more than just differentiated
intellectual property right there's you
know is just as important is kind of the
what are the getting started you know
experience of working with the software
right is it easy to you know obtain the
software is it easy to build it is it
easy to figure out how to do the kind of
hello world example right and you know a
dedicated focus on that you know first
experience of the software and a
dedicated focus on getting kind of
advocacy materials out there is you know
something that is pays back in spades
and our experience and so you know I
think that the that the project has made
great strides and sort of increased
focused on that with things like you
know weekly news emails that go out and
blog posts to go out and sort of curated
discussions from Stack Overflow that you
know people have found really insightful
and you know those little things they
add up in a big way so it's been you
know a long strange trip and one that I
would happily do over again and I'm
really you know sort of proud of the
community that we built and you know the
code that communities producing
so with that you know I think this
basically concludes the key part of
presentation here and you know thank you
again for your time and happy dance or
any questions that might have come up in
the chat so first of all Adam please
allow me to thank you for a very
inspiring talk and one I can tell you
has gathered a very large number of
attendees and I'm glad to say an equally
significant number of questions have
been flowing in so as to try and honor
as many questions as we can we'll start
asking them straight away I would like
to say that any questions we do not
answer here and now at the webinar we
will follow up on so you will get a
response inviting from us to the
question that you asked but to start us
off here's a comment and a question from
one of our audience Adrian and I sort of
tend to agree and I've often sort of
thought of this myself so Adrian is
basically asking why isn't CouchDB more
popular it's quite hard to find positive
case studies or developer and kuzey Adam
whereas we're all aware of these
qualities and you know you've just sort
of went into the detail you don't hear
as much about it so to speak could you
comment on this place sure I mean I
think that you know one of the things
that we we know we prioritized early on
was a significant investment is in the
core right a lot of the work that we
went through was ensuring you know the
viability and stability and overall you
know kind of performance of that core
code base and you know we neglected a
bit of the ecosystem that surrounded it
and we neglected the kind of first
experience right and I think what we
learned is that you know Wow well it's
hard to finish the importance of having
that core foundation on which to build
you know the ecosystem are avid is is at
least as important I know I you know
I've spoken with you know people from
the MongoDB development team before and
they said yeah sure I mean we
well the development teams that we have
on our client libraries are you know
just as large if not larger than the
groups that we have on the core code
base in many respects right and you know
for a very long time CouchDB hasn't had
client libraries to kind of ensure
idiomatic use of the database right it
became a database that if you knew how
to efficiently work with a web service
could pay back you know a great
dividends right but if you didn't there
were some bling blocks that we didn't do
a good enough job of helping you avoid
right and I think we've recognized that
you know or taking significant steps to
you know rectify it right whether that's
kind of documenting the best practices
documenting the right way to use the
service or whether that's actually
documenting it in the form of client
libraries and I think that ensuring you
know a the good getting started
experience and be that the that the
easiest path to take is the correct one
I think is really really important thank
you for that and again I'd like to thank
everyone for asking a number of
questions now we're getting more
questions than we can possibly answer
now during the webinar so again our
apologies but we'll try to answer as
many as we can we will answer all
questions in the order they being
received so I'll just move straight on
to the next question Richard is asking
can we add or remove notes from a
running cluster yes you can what doesn't
happen automatically is a rebalancing of
existing databases across the cluster
right but we do have command line tools
that are being included too you know
well the database will generate a
rebalancing plan and then there's a set
of command line tools they act on that
be balancing plan so full online
automated rebalancing of a cluster as is
expanded or contracted is not part of
the duo released it's still a human
assistant process and there are tools
that assist thank you for that just a
sort of move on straight to the
question and try and answer as many as
we can we have another question
basically asking is there a Erlang API
to plugins it doesn't seem to be listed
on the cloud ENCOM web pages so one of
our audience is asking is there an
erlang api specifically through Cloudant
not too cloudy now Clarence delivered is
an HTTP service right so you have to you
know I mean that as part of a cloud
service we can't afford to open up the
Erlang API right because it's a the
system is multi-tenant at various tiers
and you know if we add an erlang API
then that would allow a level of access
to the users that wouldn't be safe right
couchdb is you know the to doto release
has a much cleaner Erlang API than 1 dot
x had and so yeah I'm curious to see if
there are more you know your Erlang
usage is that it and it will kind of
fast thank you for that Adam moving on
strips to the next question one of our
audience filter is specifically asking
which problems should I expect when
running CouchDB in production I could
have a broad question man for me no I
think I think we're probably the world's
experts on it at this point right so you
know for a very long time the database
space management was an issue depending
on the right load that you encounter the
new compactor goes a long way towards
making compaction much higher root point
however it's still important to kind of
schedule the compaction zuv the right
databases at the right time and so space
management is important in the system
that you know it's really a pen only
like this contention on individual
document updates is always something
that I think is some needs to be
carefully managed CouchDB will always
get to a correct situation right it
never throws away data it's not like
it's using server time stands to
reconcile conflicts but if you were to
build an application that had heavily
contended updates on a single document
you'd find that that document would
prove to be a performance bottleneck and
that the conflict management
else would be sort of an annoying part
okay no in the application fantastic
thank you for that sir we have a very
limited time remaining but i'll try and
ask as many questions as we've done so
christopher is asking how exactly does
couch TV 2 point 0 benchmark against
couch pace and mongodb etc haha I don't
have solid numbers to deliver to you in
this I know we've kicked off some
efforts to you know get numbers that I
think we trust I compared to you know
touch base I mean couchbase has done a
lot of work to ensure that read
operations flow purely out of Koree like
using the memcache d protocol couchdb
continues to have an HTTP interface and
go back to disk so I would not expect
you know that the the you know the kind
of low latency operations that you get
from from basically having memcache to
you there that's not going to be
something that couch TV is going to you
know start to achieve right I think the
comparisons with MongoDB will be more
interesting right thank you for that sir
that's much appreciated again to try and
sort of answer as many questions as we
possibly can here's an interesting one
so Christopher is asking when exactly
would you steer clear from couch TV and
sort of not recommend using it well what
are the kind of use cases that you would
absolutely you know run away from so to
speak sure sure I you know I think that
there are certainly systems that
analytically focused ones right I think
is one we're doing analytics inside of
CouchDB is generally speaking not going
to beat your best bet right you know we
built CouchDB to be an operational
database to be something that you know
stayed highly available nemeth of all
kinds of crazy server failures and was
able to deliver response this too
predictable queries with low latency if
you're in an analytics environment we're
asking a lot of ad hoc questions of the
data and you know queer you're changing
from from execution to execution I think
the better bet is to use the change
capture feed the chips
every couch database and dump that data
into a Hadoop system into a spark system
you know transform an inch of in
relational shape and put it in a data
warehouse right doing the analytics
right inside couch i think is not
something that we're targeting now or
okay thank you for that now here's a
couple of questions I'm really curious
about myself so firstly in regards to
the release of couch TV version 2.0 when
would you expect that to be happening
yeah I mean it's ready when it's ready
but you know the the headline of the IRC
channel is burning down the list of
blockers for the to do auto release the
master branches you know in my
experience quite stable and useful and
you know I feel like the the database is
sort of very clearly converging on a
path that I would expect to result in a
to data released this quarter fantastic
thank you for that and finally you know
we can entertain I think one one more
question perhaps a bit of a broad one
for the end how do you see the evolution
of couchdb following version 2.0 where
do you see couchdb specifically going
after that yeah so I think one of the
things that at least from the Cloudant
perspective were really keen on is
ensuring the couch TV participates in a
larger ecosystem of other data services
right there are you know lots of ways
that using CouchDB in a complementary
fashion with other data stores makes a
ton of sense right and so whether it's
kind of getting you know the binaries
that you can attach to documents out
into an object storage system or whether
it's connecting you know providing that
connectivity to analytics systems and so
that you can analyze the day that you
have in HD be in a very kind of low
overhead fashion I think those kinds of
integrations or something that you're
gonna find at least you know my team at
Cloudant pursuing actively thank you for
that um because we're getting so many
questions I can't resist it's just
asking one final query so Julius is
asking which online version exactly will
be used for two
Joe so I don't believe that were you
know stipulating a particular choice of
our lying version we have been doing an
awful lot of investigations with our
line 17 dot five and you know are
looking at sort of broad or roll out of
that throughout the cloud service so you
know we're not sort of I don't know that
we're going to be on the bleeding edge
of 18 but I would expect that I need to
be the kind of you know preferred thing
that we go for at this point excellent
bill Adam a big thank you again on
behalf of us and on behalf of fun show
everyone who's attended the webinar so
first of all thank you for speaking and
obviously thank you to our audience for
attending please do join us again for
our next monthly webinar just to say
that falling today we will be sending
you a short survey to make sure we
capture your feedback of today's webinar
please also note that the recording of
the webinar and the presentation that
Adam sure today will also become
available for you to collect on our
corporate website which is www.hyken.com
once again thank you everyone thank you
adam and we look forward to seeing you
all on our next webinar</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>