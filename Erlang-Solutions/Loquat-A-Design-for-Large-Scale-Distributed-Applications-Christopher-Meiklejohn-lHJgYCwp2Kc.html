<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Loquat A Design for Large Scale Distributed Applications - Christopher Meiklejohn | Coder Coacher - Coaching Coders</title><meta content="Loquat A Design for Large Scale Distributed Applications - Christopher Meiklejohn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Loquat A Design for Large Scale Distributed Applications - Christopher Meiklejohn</b></h2><h5 class="post__date">2017-03-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/lHJgYCwp2Kc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">um ok so I'm Christopher Mikkel down I'm
going to talk about this work in
progress it's all research on a system
that we're extracting out of some other
work that we've done if you've been here
in the past you've seen we talked about
last probably which is like my PhD
thesis so this is work that's done in
the context of that yeah so I'm a PhD
student at two universities université
catholique de live on in Belgium and
instituto superior tech nicola in
portugal and so this is joint work under
a EU funded grant on two projects sync
free which just ended in january which
they were a bunch of sync free people at
Erlang factories over the years this was
on CR DTS and then our new project which
just started which is this represented
by this icon here called light cone
which is a horizon 2020 project which
pair is also a member of so you should
go see his talk tomorrow somewhat around
the same spot so normally I don't have
to do this kind of thing academic events
but I will do this because we're the
industry event so I worked on you know
here's a list of things I've worked on
I've worked on variety of Erlang systems
over the years primarily I was a senior
engineer at bachelor technologies
working on react supporting people in
production and doing CRT peas and then I
did some crdt work at Mays exterior and
comcast in Erlang an elixir and I did
some protocol verification work at
machine zone so so I've spent a lot of
time in Erlang and not one the first
year PhD student in Belgium and all of
our work has been primarily done in
Erlang with the exception of some stuff
that was done in Isabel whole and
various things like that ok so the talk
is a little bit different than what the
abstract says I'm going to talk a lot
about distributed computing I'm going to
talk a lot about scaling distributed
Erlang but i want to put things in
context of where we're coming from from
the academic side of things ok so we're
coming from this place that is new is a
very neat area the first conference reg
computing just appeared there's been
about three or four workshops that
they're in their first versions as well
and so this is a really cool area and so
the way we look at things is that
basically for the most part the way
we've developed applications can
now and basically many applications
today are focused around the data center
so whether you're building like a mobile
application where you have state on your
device and you kind of read it from the
server you may be cash it locally
operate on it send your events back to
the centralized data center all of this
is happening at the server so we
leverage the compute that's available at
the data center and then we kind of just
like cash you know cash out to the edge
put state on mobile devices and things
like this so what's interesting is with
the proliferation of like mobile devices
and sensor data and I oten all those IOT
stuff you might have heard about this
morning observation that's made by IBM
IBM's research group that's working on
analysis of health data coming off
sensors in Watson is that basically
sixty percent of data that's coming off
the device at sixty to sixty-five
percent of data coming off devices and
sensors is basically useless after a few
milliseconds we just want to get rid of
it and we can't really make any
decisions based on that and so the
challenge here is that while few
milliseconds well it takes definitely a
lot longer than that to get to the data
center and so what we want to do is we
want to start extending applications
providing programming languages and
networking infrastructure so that we can
start moving these computations to the
devices right so having a language that
allows us to write programs both that
operate within the data center and
programs that also operate at the edge
ideally this is the same programming
language but there's a bunch of
challenges around that right and so
general idea we want to do local
computation on the devices we want to
augment that with information from the
data center right so what's involved in
that so from the academic perspective
and we start looking at how to provide
these edge models there are two kind of
critical challenges so the first is that
we need a general-purpose programming
language ideally this programming
language needs to mitigate and or rather
not mitigate but minimize the usage of
shared state because concurrency control
across a million sensors or two thousand
mobile devices is very very difficult
and so ideally we want this to be the
same language if we look at the IOT
offerings that exist today what you
normally see is that you see you have
like a lure component that's run on the
device for
filtering data and then it went back to
the data center and then in the data
center we use something like to do for
spark or spark streaming or something
like that to process the data so kind of
a standard model that we have today with
the notable exception of a few companies
and a few academic prototypes coming
from the pervasive computing world is
that we have two different languages
that we write in and we write for the
data center and we write for the devices
okay and so we'd like to provide a
general purpose language will use the
same language across everything and no
see is not the answer even though it
runs on everything okay I mean assembly
runs on everything I mean and so the
second component here is the second
challenge is that we need to deal with
the dynamic nature of these kind of
pervasive networks right these networks
have high churn and they have high
Network partitioning right you're going
to go into a subway you're going to lose
service probably depending on if you're
in a light rail or if you're in the tube
whatever right you're going to have
nodes that come and go and so we need to
have a networking infrastructure that
can handle this dinama city that happens
in these real-life sensor networks and
edge networks last week I was at a
conference in Hawaii they are doing they
have a nice general-purpose programming
model built on Java that compiles to
Android phones and data center nodes and
they have data they have they use old
smartphones to place them around
volcanoes to detect like seismic
activity right so this is super cool we
want to see more of this right this is
the pervasive computing world that's
very cool all right and so basically i'm
going to plug it out the system today
which is called low quad and it's a
lightweight publish-subscribe protocol
for edge networks and we layer an
actor/model on top of that and this can
be seen as kind of a generalized model
from the Erlang model and we'll talk
about the differences there okay so
everybody here probably knows what
actors are does anybody not know what an
actor is okay that's good otherwise you
probably at the wrong conference and so
actors are you know these are
lightweight processes usually
preemptively scheduled they act
sequentially there's no shared state we
pass everything to asynchronous messages
things are copied for the most part
depending on the implementation and if
we need to simulate the notion of shared
state we have a single actor that has
some states and a bunch of process is
kind of rendezvous with that and we
sequentially and this is how we kind of
get shared today we simulate shared
state okay traditionally probably
everybody's here familiar with Erlang
there's this thing called echo which is
pretty famous you probably heard of it
and then there's these lesser-known one
so we have like toy implementations that
exist in LS ml if you come from the ml
world and if you come from the scheme
world then you have systems like
termites game but these all provide an
actor model for concurrent programming
okay now what do we care do we care
about concurrent programming absolutely
but what we really care about its
distribution what do we have in terms of
distribution we have distributed
Erlang's we have Microsoft Dorian's we
have Alex a cluster and we have cloud
Haskell I won't talk about cloud Haskell
in this talk but you can come talk to me
about cloud Haskell if you're interested
in that and so basically actor/model you
all know this this is summer lang it's
very nice I have processes that only
communicate through message passing this
message passing copies values I spawn
these processes they're lightweight
they're scheduled it minimizes the
amount of synchronization I have to use
in the system very nice all right so
let's talk about the state of the art in
distribution so when I talk about the
state of the art in distributed actor
systems for people to be successful in
building applications using these
systems we rely on these systems
providing three core pieces of
functionality okay the first piece is
membership so at any point in time I may
have millions of actors running across
millions of machines and I need to know
who those machines are and so usually
these frameworks have to provide some
notion of membership how I know what
nodes are in the system so i can contact
them and route messages to them ideally
you want a notion of failure detection
so you should know when nodes go down so
if you send somebody a message and
you're waiting for it you'll know if you
have to wait indefinitely for it so we
like our system to have failure
detection this keeps them live it keeps
our systems making progress under
partitions and things like this and
finally the third component is what in
our to sea world we call binding but is
more commonly known as name based
routing or process based routing and
this says that given I have some work to
give to some process on the network how
do I choose which process to select
and how do I know how to route the
message to that process okay and I'm
going to argue that when you're building
when you're building highly highly
scalable edge networks that none of
these systems that exist today the state
of the art are sufficient for providing
this functionality so the first I'll
talk about is scalable distributed
Erlang how many people know SC Erlang ok
a few okay so if you've been to Erlang
factory before you've probably heard
about s the Erlang its work if that's
coming out of Glasgow and cost us is
university i'm surprised calcis isn't
here or i haven't seen them yet but this
is largely a european funded project
called release it's mainly academic and
it tries to deal with the fact that they
observe that distributed Erlang folds
over once you get to around 60 to 65
notes now ericsson receipts that number
they say it gets the 200 within their
data centers that's fine it probably
does but 200 is still kind of not that
not that great if you're building a
large-scale edge network and what s the
Erlang you know proposes is the two
challenges and I've simplified some of
this from the original literature what
it proposes of the two challenges are
one what we refer to as this prohibitive
space growth because in a distributor
line cluster every node knows about
every other node and it pings every
other node along the heartbeat interval
right in addition to that every node
once it connects to every other node has
to replicate the process registry to
every other node and so quickly you run
into the situation of quadratic space
growth for every single know that exists
in the system to maintain to perform
connection maintenance and replication
the second problem that they identify
here is that is that if I want to if I
so if I break this 200 limit and I get
to a thousand nodes now when I go to
spawn a process i have to pick a node to
spawn it on and that chat and as you a
number of nodes increases that becomes
more challenging and so this is referred
to as explicit placement so espier Lane
attempts to solve this in two ways one
they have a this thing that's referred
to as s groups it's kind of a you can
think of it as super peering and it
basically says that I'm going to
establish smaller groups of nodes that
have full connectivity and then I'm
going to basically kind of have a
hierarchical structure where a
designated node performs the intergroup
communication so very typical and in
peer-to-peer networks this reduces the
transitive connection sharing and how
much data you have to replicate and then
the second solution that they propose as
they say well if we tag the nodes with
some attributes that we can filter on
then when you say I have a thousand
nodes and I'm going to spawn a process I
can filter that by saying pick me oh
noes that have you know eight cores to
pick the nodes that have the GPU or
whatever right but the challenge here is
that while I can grow this cluster and
now I can know where to spawn processes
I still run into the problem that I have
to explicitly pick who I'm sending a
message to and so I still need this
registry and in the worst case i still
have to know about everybody because
they need to replicate this whole
registry and now i have to find the
person by name to send the message to
and so they solve the problem of taking
the node but arguably since nodes are an
order of magnitude less than processes
selecting a process actually is a bigger
challenge in selecting one up future
distributed Erlang is the work so FC
Erlang is not being adopted a successful
academic project ended in 2012 none of
the 2013 not of the work that came out
of that project is is being picked up on
by ericsson for integration into Erlang
OTP so to distinguish that model of
distribution from their model I call
this future distributed our line because
since Eric's doing it it is definitely
coming in the future and will be
something you will see and distributed
our line future distributing is work
being driven internally from erickson to
improve the existing infrastructure
without changing any of the distribution
semantics whatsoever so erickson same
observation so this is funny that Eric
since you know so this is just me
nitpicking as an academic to say well
it's interesting that you've identified
a bottleneck at 45 to 60 nodes even
though you say you got to 200 but
whatever but uh but there you know the
problems here are the same the problems
they identify our for their effectively
fall into two classes one is that you
must maintain connectivity to all nodes
again we know this is a problem and the
second one is that every note of the
system has to replicate the global
registry again right and so this is
prohibitive in state same problems
identified just different scales
different order of magnitude
so Erickson's approach is that they want
to use the condemned Lea DHT this is the
technology that went to develop into
bittorrent this is a distributed hash
table that basically provides login
access on the global registry and then
they'll automatically disconnect nodes
when a connection is no longer needed
right so we have all week maybe we don't
automatically connect and we minimize
the amount of data we have to replicate
okay and so I you know your intuition
here as I'm telling you these existing
implementations what you should be
thinking about is okay the global
registry what is it well it's a
distributed database but it's a
distributed database where everybody has
the full replica and so what do we do
when we scale databases well we
partition data right and so we start
spreading it across a bunch of nodes and
then the challenge is how do we ensure
we have the lowest possible access time
to that data now that it's not running
on one note and so kadhim leah is
algorithm is designed to guarantee log
and access and so this is nice problem
again I'll argue that explicit naming is
problematic if you have you know so now
you can run you know thousands of nodes
you have millions of processes on
thousands of nodes how do I take the
name of who I want to send a message to
so again I think that's kind of a
problem that's not being addressed so
Oscar cluster is a clustering facility
that's been developed for the aqha actor
framework for Scala and Java on the JVM
and aqua club de basically looks very
similar to react or you ever seen react
oh you probably heard about that if you
have a renderer line factory before but
reactor is basically a DHT specifically
it's a one hot DHT that gossips around
the membership information so access
time is is constant rather than log in
in the worst case with some notable
exceptions under clustered sharing and
what acha does is well and use of static
assignment based on node attributes so
when I spawn processes I can spawn them
on particular nodes so it tries to you
know this is this is akin to semi
explicit placement in scalable
distributed Erlang's and finally the way
that we perform failure detection is
that we we basically kick our position
on the distributed hash table and we
pick F adjacent nodes and we heart beat
f adjacent nodes f is configurable by
the users so you have to guarantee that
you pick up
/ f that you can heartbeat all the nodes
within that time and so there's a little
bit of tweaking you have to do to get
the cluster running ideally but very
impressive number 2400 nodes that's a
lot more than Erickson's running on
distributed or like that's a hell of a
lot more you're going to run on react
core that's for sure and and the pret
end but but the notable piece here the
most important piece to notice is this
so because the because the DHT must
stabilize in between deployments it
takes four hours to deploy 2,400 notes
and this is because of the joint
mechanism and the way epochs are handled
and membership in the Aqua cluster
system okay and so Microsoft audience
it's a kind of another actor model at it
further extends the actor model with
what we refer to as virtual actors
virtual actors basically are if you take
after you remove lifecycle management
you automatically initialize on
receiving a message you automatic really
reclaim when the process is no longer
needed these have explicit placement of
randomly so this is automatically
assigned based on round robin or some
placement strategy within the system and
the process registry again is sharted
and it is it's a one hot DHT basically
for accessing the global registry of
where the processes are running so to
give you what this would sound like in
Erlang terms this is effectively if you
took global and you change global into
PG two but eliminated all the problems
that PG 2 has and then you basically
partition that that's effectively what
you get it's a 12 it's a name too many
process mapping where there may only be
one but can be many and you select based
on some selection strategy the process
you want to route to the actors come in
different flavors this is kind of an
extension of the model that shows that
you can have actors that will only be
guaranteed to be initialized once with
some minor caveat under partitioning or
actors that can be instant instantiated
a number of times and the public numbers
Microsoft talks about 200 knows it's
probably more than that given the scale
of the applications that they've run now
what I'll say is I don't like any of
these if I love all of these systems
have worked on all of these systems I
will continue to work on these systems
but in terms of edge computing I won't
use these systems for a variety of
reasons
so one I feel that structured overlay
networks are poor fit for networks that
have high churn hi Dinah Missa tee lots
of membership churn lots of network
partitioning mainly this is because the
DHT if it's partitioned and stores the
registry of where to find actors if you
don't have a proper replication factor
then you can't access an actor that may
have the routing information that you
need for an actor sorry a partition that
has the routing information you need to
find an actor second global registration
assumes that all processes in the worst
case is a pathological case we'll have
to talk or may need to talk to all other
processes in the system that's probably
not a proper way to design a system if
you if you need that and so global
registration is kind of a is kind of a
wart that I think is should be removed
and and mainly at least them in the
Ericsson erling side of things the
reason global registration is being kept
in the system is because of backwards
compatibility with existing applications
that are deployed and so there is a no
basis on whether that is a proper design
decision or not explicit binding which I
mentioned is the selection of given a
thousand processes or a million
processes I have to pick one to send a
message to this becomes problematic when
you start going into millions to you
know tens of millions or hundreds of
millions of actors which is the desired
scalability and finally explicit
placement and so explicit placement is
you know the idea of where do i spawned
the actors right and so how do I make
the choice given a lot a lot of notes
how do I build an application that
spawns things in the correct place all
right and I think they were supposed to
be some slide there that isn't so I will
just kind of move on so basically I
don't know what happened there but
basically all of the work that we've
done tries to eliminate these problems
and it's based in an approach that is
kind of the cornerstone of our EU
project the light Kong project the
horizon 2020 project on a technique
that's referred to as hybrid gossip and
so what hybrid gossip is is basically a
technique where you combine two
protocols to distributed algorithms one
that is highly efficient and one that is
highly resilient
to make a protocol that achieves high
efficiency in the stable case and then
can repair itself during the inefficient
network partitioning case so it's a
pairing of two protocols I'll give you
kind of an example that will dive into
in a moment here but the example is we
compare an efficient protocol such as a
spanning tree that's used for a message
dissemination this is uh this is this is
like a typo again this is this is
efficient but not reliable I feel like I
have the wrong slide deck and then we
pair it with a resilient protocol that
is efficient but reliable inefficient
but reliable and so this would be a
gossip protocol that's used for
repairing the tree when no join and
leaves the cluster okay um so if you're
going to take away one thing from this
talk I I want you to pay very very close
attention to the next section because
our entire research project is arguing
that when designing for the edge what we
want is this hybrid gossip approach we
want this in programming models we want
this in our runtime system and so what
we want is the only way somebody will
build applications for the edge and the
only way for edge computation to succeed
in the general case is that if we can
make edge computation as efficient as
computation within the data center when
the network is stable or close to stable
and so really it won't be the same it'll
be asymptotic if we look at it you know
from because of late differing Layton
sees and all the stuff but we want it to
be kind of asymptotic to the ideal
single data center performance and then
what we want is we want a system that
given the failures that occur given the
partition and given the churn we want a
system that will gracefully degrade when
failures are cure right and so ideally
in the steady state we have very nice
performance and then we have a system
that is kind of self-stabilizing under
failures and while the system is being
repaired so I'm going to give you two
examples of prior work from our
colleagues in nova nova university and a
listen one is the high power view
protocol and one is the plum tree
protocol okay so hyper view is a
protocol it stands for a hybrid partial
view protocol and what it tries to
achieve is it says given an unstructured
overlay Network there's some random
overlay of nodes I want to basically
provide resilient membership that's
tolerant to churn and I want to make it
as efficient
possible so scalable you know to a large
number of nodes efficient in terms of
the amount of state and messages that we
needed to maintain it now the
challenging part of doing this is is
that it's widely acknowledged that
partial views are the way to achieve
this and what a partial view means is
that any particular node in the system
does not know about every other note of
the system it only knows about some
subset appears this is extremely
scalable because no do not need to watch
for failures on every other note of the
system and they only need to maintain
active connections to nodes that they
know about but as you can imagine this
is extremely problematic when things
start to go bad and so if I only know
about a small group of people it's very
easy for me to become disconnected from
the rest of the group so what we need to
do is we need to have a protocol that
can repair the primary the efficient
view using some sort of resilient
algorithm this hybrid approach and so
the way we do this is we do this through
having two views when we refer to as
active and one we refer to as passive
active is maintained through this
efficient this is part of the efficient
protocol maintained through strategy one
and the password is maintained through a
secondary strategy so for the efficient
strategy the way this works is that we
every known as an active view this
active view is a fixed size so it's a
fan out pre statically assigned fan out
plus one to include itself and we
maintain active connections to other
nodes in the cluster now if I take all
of those active views and I build a
directed graph that directed graph it's
actually a if I build a directed graph
of who knows about who if I take just
the active views it should form a single
connected component that connected
component will have cycles but it will
form a single connected component with
some redundancy and then what I want to
do is if I want to gossip information to
the network I just flood my active view
with the data that i want to send and
that's very efficient very minimal
redundancy in that now the challenge is
that as soon as a node gets partitioned
part of that view could be I lose that
single connected component and so add
nodes join the system and my view size
becomes full I start migrating nodes
into a secondary view that's basically
the paths of the
that's used for repair and while I
perform and this is dynamically sites it
can have a maximum of log n which is the
diameter of the overlay and when I when
I incur a failure when I see that one of
the nodes in my active view is failed I
select a note from my path of you and I
swap it in and so I used this back up
view of nodes that I've seen but I'm no
longer talking to to populate that
primary view and so the system will kind
of churn until it stabilizes to ensure
that everybody has replacement
candidates available I need to basically
gossip around my backup view to the
other nodes so periodically I say to all
the other nodes here's the people in my
backup list let's swap and so I do a
random shuffling of this and so this
random shuffling happens on an interval
and this increases the probability that
you'll have a candidate for repair in
your view list so this is how I build
the director this is how I build my
directed graph cycles gives me my single
connected overlay so the second protocol
will talk about is plum tree is push
lazy push multicast tree protocol and
what poetry does is it take this
directed graph and it reduces this to
expanding tree so it will compute a
directed acyclic graph now spanning
trees are really nice for distributed
systems because spanning trees reduce
redundancy so it sends no redundant
messages and gives me the best possible
dissemination strategy for a given
object cuz usually these trees will
normalize on on latency and things like
this but spanning trees are really
fragile especially if a node gets
indefinitely partition that was an
interior note of your tree and so what
we need to do is have a strategy for
tree repair and so this is the secondary
this is the resilient component so in
terms of the efficient algorithm what do
we do we start off with our directed
graphs and we push messages to the
leaves of the tree so we flood this
network it may not be a tree yet and the
way the tree is computed is lazily by
saying that every time I observe a
message for the second time I can tell
that node to stop sending me messages
and this will in a decentralized manner
compute a spanning tree per node now
it's easy to get this tree to have the
street just connected like I said I
could take one no that's an interior
node of a tree and permanently partition
it and so what we need is a secondary
strategy for tree
and so the way this works is that we
will lazily push not messages but just
the message identifier that we've seen
to the / to the pairs that have been
pruned and then when those nodes observe
that they message identifier that they
haven't received on a channel that they
should have so if they think this is the
tree they receive an identifier of x
over here and there they didn't get X on
the tree within some bound of time then
they will basically use that to form a
replacement now you might say what
happens if I can currently repair the
tree in two different directions well
that's fine because the efficient
algorithm will prune it back and so
there's a very nice stabilizing protocol
that requires minimal coordination to
stabilize decentralized and so what
we're going to talk about here is the
loquat builds upon these two and most of
the work I'm going to talk about so the
programming model is still a prototype
today that we're that we're building but
all of this infrastructure and these
protocols that I've talked about plum
tree and hyper very we have these
implemented in Erlang and I'll talk
about how you can use them today and so
what we talk about is look lot look wat
is a generalization upwards compatible
they're laying actor model it basically
generalizes the notion of process
identifier to topics giving you kind of
a traditional pub sub system so signs
are two topics then you receive from
topix image in the pathological case you
would have a topic for process and
that's how you would perform a
simulation of the Erlang system in the
loquat model one notable difference is
that we don't provide slice o guarantees
we can do it it's expensive it requires
monotonic counters with exponential
space costs on the number of nodes in
the system but we can provide it if you
need it but what we say is that you
really should be building systems that
don't require a FIFO and microsoft
orleans also has gone the route of
saying that in most case you know don't
don't guarantee FIFO you know you might
get it but don't rely on it and which is
fine because you know the bachelor
school of engineering with jen fsm has
at least you know the stuff that's in
react is designed to avoid a lot of
these 50 things and a lot of this old
spiteful ordering dependence comes from
older code that is an OTP that was built
when it was a concurrent system and so
what i'll talk about is how we combine
this efficient membership broadcast
multiple broadcast trees to build our
backing protocol so our backing protocol
uses hype RV for membership it doesn't
use pipe our view is the papers
implemented we have a notable series of
extensions to adapt it to handle things
under high churn and under a connection
churn and things like this and according
to you know just a site to give you an
idea of where the protocol was designed
for the protocol is designed for almost
one hundred percent delivery under
ninety five percent failure rates this
is the textbook protocol we have not
evaluated our implementation to see if
these numbers align but we've known you
know to the long engineering tasks to
get it to just work correctly in many
ways what we take that membership we
layer a broadcast on top of this so we
use the plum tree algorithm for
optimizing the trees in a decentralized
manner but these are only reliable up to
seventy percent churn or failure rate
and so an anti entropy protocol that was
invented by batch o is used to ensure
reliable broadcast if that's required
for your application and so what we do
is we use the membership and the tree
the spanning tree algorithm to compute a
single metadata tree that is used for
metadata dissemination about who is
interested in what topics this metadata
tree can be seen as the membership
service for each topic so information
about who is interested in a topic is
disseminated on the tree and then we use
that information to seed the
construction of additional trees that
are per topic now we are not the first
people to ever do per topic trees this
has been explored in a variety of pub
sub systems that were worked on at epfl
on a variety of institutions in that
area and our system is optimized right
now mainly because these are the
patterns that we have observed in lat 44
basically power law distribution over
subscribers and publishers which
basically means that you know for the
majority of topics most people will be
publishing and subscribing to those and
then it will kind of Taylor off in that
manner and so none of the pictures are
there which is like really bad so I
don't know what I'll do about that but
we'll come back to that
and that's how the tree is constructed I
don't know why that's happening and so I
and so basically to give you an
intuition of how this works because my
grant my graphics are not there we we
compute we have our unoptimized overlay
and we optimize the overlay by using the
plum tree protocol and this gives us a
single tree for the entire network that
is used for dissemination of membership
information now if you know if pair and
me decide that we are both interested in
the topic of security teas which I think
we are what we do is we both broadcast a
message on that tree saying that we are
interested in CRT keys now on that tree
we will join the list of people who are
interested in the same topics into a
single set and then will disseminate
that information along this metadata
tree saying that for crd tease these are
the people that are interested
interested that single object is
monotonic so it always increases in
state we don't have to worry about
coordination we can do removals in a
monotonic way because of CR DTS and you
can I can refer you to literature about
how that's done and then what we do is
we use that membership information for
each node to take that overlay formed by
that membership information and compute
a per self rooted spanning tree based on
that so everybody basically has their
own tree that they compute based on this
membership information that they're
getting via this one tree that everybody
is part of a member of and so basically
this is very simple super positioning of
spanning trees on top of an unstructured
overlay so we use we use this tree
construction protocol and this
membership protocol to layer on top a
publish and subscribe interface it looks
roughly like this so this is similar to
your Erlang spawn link function this is
spawn topic and what this will do is it
will say for a given topic identifier
which is an atom and epoch which I'll
get back to in a minute create a topic
and so this is where a node says I am
now interested in this topic and we'll
start the construction of the topic
specific overlay now there's also the
ability to remove a topic since our
system does not have coordination this
is done this is done eventually so
eventually once this messages deliver to
all nodes they will unsubscribe doesn't
the reason for the epoch is kind of a
nuanced point here
so without coordination you can have
concurrent spawn and kill operations
happening for the same topic and so we
need a mechanism to totally order these
operations this epoch can be a variety
of things if you're running something
that provides you with strongly
consistent sequence identifier is you
can use that really in practice what we
imagined you would use is a you would
arbitrate on the node identifier just in
the same way that a bully algorithm
would work or something like this so we
would just basically lexa graphically
sort the nodes and if you ever had
concurrent operations you would pick one
so you can use whatever one match the
semantic that your application needed
the epoch is just an arbitrary value
that needs to have a total order to find
over it now in terms of messaging um
once you have a topic which is an atom
you can send a message to it just like
you would with normal or laying bang
syntax and then you have the ability to
receive from the topic and then perform
pattern matching on the values you
receive finally you can unsubscribe so
what's the difference between receive
and unsubscribe so receive basically
says if I have been delivered a message
in my mailbox I can pattern match on it
given a topic so if I receive a message
in the in the crdt mailbox because Paris
sent it to me that I can receive and do
normal pattern matching on it now
unsubscribe is different because
unsubscribed is going to say stop
delivering messages to my mailbox
because otherwise if you just call our
Steve you'll start subscribing to
messages so unsubscribe forces you to be
removed from that overlay no longer
deliver messages to the mailbox and you
can still match on messages in the
mailbox and so this is because you don't
want to have a node that it gets one
particular message that comes on the
network and then doesn't need anymore
and just continue getting deliveries for
messages that it will never ever receive
on effective leading to the unbounded q
problem in Erlang okay so in terms of
implementation so this is all work in
progress we're going to try to evaluate
this and see what it looks like this is
being done in the context of the sink
free fp7 project and the light-cone
horizon 2020 project which i mentioned
most of this work has come out of an
underlying infrastructure that we've
built for the last programming system
which is a crdt based programming
language that you
a bunch of these mechanisms to achieve
high scale and finally our
implementations of the hype our view and
plum tree protocols that we've built and
adapted for high turn environments have
been in the bypasses disco completely
have been demonstrated at ten twenty
four nodes 2048 almost and we have a
master's student in Laura Castro's group
if you know her that is working on
getting up to 4096 and so we simulate
this we don't simulate this these are
actual docker containers either running
in the Cooper Nettie's cluster computing
framework or maize oh this is all real
stuff we've done it all we got libraries
they're open source and I'll mention
where you can download those at the end
okay so in terms of hyper view what do
we have to do so hyper view was
implemented in a simulator called
Pearson this is an academic simulator
for building overlay networks it was
evaluated at 10,000 nodes okay very
impressive number differences real world
environment is a lot different than a
simulator that has a single kind of
control plane for a bunch of threads one
of the notable differences that in
compared to akka we could not wait for a
30 minute experiment we could not wait
four hours to bootstrap a 30 minute
experiment it just doesn't work we have
academics we have very little money and
so we wanted to bootstrap this cluster
as quickly as possible so are 1024
number there is a cluster that we can
construct in 15 minutes that's a
worst-case which is very very impressive
compared to the existing state of the
art and a bunch of our changes had to do
with that why did we do this well
because even with those real speeds I
spent nine nine thousand nine hundred
euros of the European governments money
so if any of you pay taxes to Europe
thank you and and so even with those
foods have prepared as well even with
those speeds we've worked very hard at
cutting down the amount of time it would
take so just to briefly highlight some
of the changes we had to do we have a
paper that outlines all of them the
first one is that given these nodes have
a fixed active size if all of a sudden
one node that says I will only maintain
connections to ten other nodes
experiences like 50 nodes try to join to
it or 500 nodes try to join to it very
quickly a lot of those nodes will get
disconnected
Victor's from the cluster very fast and
so what we needed to do is build
something into the protocol that would
retry connections this makes sense
because yeah and we try to ensure that
we have a minimum number of active
connections for each note this makes
sense because mobile devices are going
to get disconnected occasionally and
sometimes we'll have this clustering
behavior the second one is that the
paper the original paper assumes FIFO
delivery across all connections that any
two peers will ever have so this is
problematic this is problematic if for
instance we have a connection and a
message is being delivered very very
slowly on it and then I think that
connection is dropped but you don't
think that connection is dropped yet and
maybe your buffer is crossing that
message and then I start up another
connection you receive a new message on
that new connection fifo is easily
violated and this can cause clusters to
get into very weird behavior when you
deliver a message that says disconnect
before you deliver the join message
that's very weird clusters do not like
those kind of events happening and so a
a technique that we borrowed from apache
cassandra is that they they were the
first people to figure this out is that
you need to basically maintain a matrix
of all possible pure peer connections
and enforce a monotonic sequence
identifier across all of them with a
sequence identifier and an epoch
identifier both monotonic to ensure that
you can enforce the correct delivery of
messages cross connection this is a
non-trivial thing to implement and we
only found it once we get to 512 notes i
believe last one just to quickly mention
here is that this active view needs to
so we periodically shuffled as passive
view on a timer but when nodes are
changing their active view very quickly
so that primary view they have sometimes
it's more efficient if we piggyback
shuffle messages for the backups on top
of that and so this is just an
optimization we learned that would allow
us to increase how quickly replacement
nodes were available under churn and
then periodically if nodes had a full
active passive view that passive view
promotion happens on a timer and so we
kind of tweak some of these timers and
change some of the heuristics around
them so that we would periodically try
to bring nodes into the primary view
more often to increase connectivity so
this allowed us to maintain connections
over churn so I want to briefly kind of
mention who came before us and where
we're going so if you're familiar with
the ISIS system virtual synchrony from
ken Berman back old school Cornell work
this is the first system ever to
introduce publish-subscribe for
distributed systems for reliable
distributed systems in fact a
little-known piece of trivia this system
came out in the academic literature and
then the Erlang community built what was
referred to as PG which was supposed to
be processed groups that provided
reliable atomic broadcast so there's a
very nice paper that's written by a
bunch of people that might be in this
audience that show that reliable atomic
broadcast is really really difficult to
do in general nevermind in Erlang
because of the way failure detection
works and so the reason PG to exist is
because pt2 said yeah well we can't
provide atomic broadcast and so PG two
is the genesis so everybody's probably
wondering why why don't I use PG where's
the to come from and so virtual
synchrony basically was not adapted to
turn it required a view membership
changes that were strongly coordinated
but it was used in the New York Stock
Exchange and the French the French of
air traffic control system now when we
talk about scalable publish and
subscribe there's two general approaches
that you can take one is the gossip
based approach the gossip based approach
oh it says 41 I'm like I'm getting close
to you okay so i'll just wait to go say
the gossip based approach on the tree
based approach and we've shown that with
the hybrid approach we have very good
results so I'll skip over these and
which are related work and i'll just
briefly talk about the libraries we have
and then i'll be done so we have all
these available libraries to last is the
system that encapsulate slow clot that's
available partisan available here github
last flying partisan is our tcp based
distribution protocol that bypasses dis
girl it provides client server and
peer-to-peer topologies plum tree is an
adapted version of bachelors plum tree
that uses the overlay provided by
partisan to produce spanning trees
sprinkler is a system that you can go
download it's probably really hacked
right now but this is our system for
doing deployment of services across KU
benetti's n mezzos 4dr containers
then we have a bunch of data types that
support all of this right and so just to
finish I couldn't have done it without
any of these people because they they
wrote a lot of the code and I'm just you
know the academic figurehead behind all
of it so great thanks to them for all
the support okay thank you sure No so
what we're basically doing so we're
working on so we working we're working
on last which is a seer database
programming language so shared state
under high distribution and then we're
working on another language which is
called it's tentatively called spry that
you might have seen me tweet about which
is a language that's based on
probabilistic programming / immutable
data and so the general idea is that
these two languages would basically kind
of sit in particular points of this like
kind of cap spectrum of possible
distributed languages you could make and
so what we're imagining is that we can
create a language that compiles to beam
eventually that is just libraries for
now that will encapsulate kurt correct
by construction distributed programs so
so i am the guy who gets up here and
says you know eventually when you write
these large-scale distributed systems
you won't be writing in just Erlang
you'll be writing in these other dsl's
that are designed for use cases but yeah
that's that's the ultimate goal
yeah that's that's the major question oh
yeah so so the the academic answer that
allows me to punch is the answer that
says well you know iOS says that you
have secure code loading so we don't
care that's that's the answer that
doesn't get a reviewer to reject your
paper in terms of the research project
we have an entire group located in
Portugal that it's focusing on security
so that is not my area I'm the language
area but we have an entire networking
group with people that have done a
significant amount of work in
homomorphic encryption and all these
various things that they're well versed
in the security area that will be
working with us so i don't i don't have
an answer but we're aware of it yeah so
originally I consider doing this on GHC
because I wanted a type system but then
I realized that to write any distributed
program that I could actually run and
like actually get an evaluation that I
could put in the paper that would not be
possible and so as a former Erlang
programmer i selected Erlang because it
was the most efficient that being said
there is a group of students at epfl
where Scott you know it's a scholar
group the lamp group under Martin order
key there are people there that are
working on stuff that is related to our
previous work so hopefully some of this
stuff will appear in the Scala ecosystem
very soon I dunno about Pierre script
yes I did not know that but that's
that's a super cool thing yeah I mean I
just liked Isis's I just want to play
with it and see what we can we can prove
about these systems but like in terms of
building stuff that real people can run
like I mean that's it's almost a
non-starter for us because distribution
stuff in Haskell this is a nightmare
really the current state of it is a
nightmare hopefully that will get better
or JavaScript okay cool alright thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>