<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>MongooseIM - Messaging that Scales - Michal Slaski | Coder Coacher - Coaching Coders</title><meta content="MongooseIM - Messaging that Scales - Michal Slaski - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>MongooseIM - Messaging that Scales - Michal Slaski</b></h2><h5 class="post__date">2013-05-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/byOinfFk3HU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome again in the infrastructure and
messaging tract I am the next and last
speaker of this track
my name is Mahesh Lasky and I'll be
talking about man who's I am which is a
messaging server with focus on scaling
yeah so what is Mongoose I am this is an
instant messaging server that has been
used in social media gaming
telecommunications our focus is on high
volume let it it could be high volume
traffic a lot of lots of messages it
could be very many concurrent users
connected or a mix of the two and
focuses on making it scaled we need to
scale two really big numbers as I will
try to explain later
so we just try to make sure that we can
scale we can start with small deployment
to handle relatively small number of
users and over time hat boxes or scale
with the number of users growing another
thing here is that this is highly
customizable as we have seen over the
years every deployment of such server
has always to be customized for example
you need to integrate with custom
authentication mechanism with different
sort of databases of users so another
requirement here is actually to make it
reasonably easy to customize the server
so some of the domains that we target is
our telecoms all sort of messaging
applications that mix video audio
nowadays but also chat regular text chat
feature there also social media websites
where communication is key part of it
and again a chat feature text
communication could enable such social
interactions and then gaming where the
in-game chat system were many players of
the online games can chat while they
play or very chat so that they can
actually do discuss something before
joining a gameplay this is also
another use guys and it has been widely
used for such use case
okay now Mongoose I am
it supports XMPP which is a messaging
protocol it is used for example to
interface with facebook chat and Google
Talk chat feature so this is quite
widely adopted messaging server sorry
protocol and there are quite a few
libraries for all sort of different
operating systems including mobile
operating systems like Android iOS so
there is a selection of reasonably well
tested frameworks and libraries that
support XMPP which is good because this
enables some quite interesting
developments how many of you have not
heard about XMPP before one person okay
I will just quickly say a few words
about XMPP protocol properties it's an
open started standard so we can find
some rfcs as well as so-called accepts
XMPP extensions on the XMPP dot-org
website it's secured through channel
encryption stronger authentication you
can also isolate your network from other
if it is supposed to be for example and
charge system for your company only for
internal use you could isolate it from
external network it's flexible XMPP is
built on XML this is actually really
customizable protocol you can really
add-on add-on and make it really custom
if you need arises
decentralized meaning there is no like
central XMPP server for the whole world
each one of you can run your own and
they will federate with each other this
is how you can send a message from one
domain to another domain will federate a
little bit like emails not but more
reliably Oh in other words you will not
get that much spun as an email because
servers will have like direct connection
between between
and then they will authenticate each
other while connecting while federating
efficient meaning we don't Paul we
rather have persistent connections like
TCP or WebSockets so this is more
efficient than polling techniques and
proven it has been used for quite many
years now so there are really hundreds
of deployments of XMPP or sometimes
referred as jabbered servers out there
okay so enough about XMPP some of the
features that you can find in the
Mongoose am project support for
WebSockets XMPP over WebSockets is still
a draft but this is going to be now
standardized multi-user chat is one of
those extensions to the XMPP protocol so
XMPP defines some core building blocks
some core features of the protocol and
except will extend this with different
custom stanzas stanzas as in messages
packets of the protocol so multi-user
chat is one of such crêpes for example
now we are streaming this video and
underneath the streaming window on-air
length central org you can find a
web-based multi-user chat and people
actually some of you watching us now can
chat can give us some feedback can maybe
comment on the talks which are going
this is all based on WebSockets and I'm
going to use a chat feature I tried this
on my Mac obviously but also tried it on
my iPad and it worked pretty well so
WebSockets are current are nowadays
reasonably widely adopted and supported
by modern browsers and there are also
all sort of other extensions that you
can find on the XMPP dot or web site
there are tens if not hundreds of them
and I will try to tackle different
problems you can always propose your own
extensions here on my slide I also give
examples of some of the other extensions
that we currently support in Mongoose
but
we are not limited to this now this
slide actually I consider to be one of
the most important ones I know that we
came here for scalability ping I will be
focused on my second part of the
presentation but this I would like this
light to be one of the takeaways from it
this is really customizable platform not
only thanks to the XMPP protocol but
also to the modular architecture of of
the server in Ireland so you can add new
modules ad rank modules which provide
like plugins and they can hook into
so-called hooks so we can actually have
different sort of plugins coexisting you
can plug them in tag them off so there
are a number of custom modifications
that could be developed this way could
be some authentication plug-in or push
notification to mobile devices depending
on what you need or some other
extensions and now this is another
important thing I would like to bring to
your attention is that at airline
solutions we've been working also on
test suit it's published it's on github
it's open source it covers quite some
chunk of the Mongoose AIIMS
codebase and this has not been seen
previously before we forked the original
jeopardy project and rename it Mongoose
because now not only you get the open
source server but also an open source
test suit whenever you start to
customize your application add some new
new features you already have reasonably
good suit of regression tests that you
can run and verify and this is really
important because again as I said quite
often you will not take software as it
is out of the box you will want to
modify it slightly to cut a for your
needs only recently a few weeks ago I've
seen some contributions on the EJ Bodi
mailing list were to sort of prove that
this contribution can be accepted to
remain in jeopardy branch outer of the
patch was saying
and by the way it passes all those ESL
jeopardy tests so well point proven
this is exactly what those tests are for
and again I would like this slide to be
one of the takeaways from this
presentation ok so now to the other
point where we talk about scalability we
consider Mongoose I aim to be scalable
this can be done for instance with the
changing different database backends
depending on what is your target
capacity we can start with a simple
simple deployment were standard airline
monisha database is used for storing
passwords roster sessions when if we
need to grow we could actually use my
sequel instead of ninja for storing some
persistent data like passwords like
roster and and other persistent data we
could even try replacing mini Jie with
readies for the session storage I will
get into it so those are all different
kind of techniques on how you can make
this standard Mongoose configuration to
be more well to scale to some bigger
numbers now the reason why this default
version of your configuration actually
will use mini Jie and will use just
airline purely airline based application
is because when this way is actually
quite easy to just get it to work so
when you download the package with
Mongoose item or your clone source from
github you will just compile it and you
will create a release and there are no
other dependencies so you can easily set
up some local XMPP server instance now
so this is good again for starting for
some deployments actually might be also
good enough I'm talking about
deployments of thousands of users
connected and registered which for some
custom deployments of the XMPP server
this actually may be just enough
in those we would be more interested in
throughput of messages flying through
the system rather than the actual number
of connected or registered users now
this is a sketch of architecture so you
see that on the right hand side you have
different plugins that you can
enable/disable web sockets or mood user
shadows just some of those when we have
the session manager element this is the
black one which by default is running on
Airlink
database Monisha but we could also
replace it with Redis and make it like
an external dependency if and it arises
why this could be good it could be good
because of not only efficiency reasons
in some large scale are also for maybe
it's just easier to pull data from
readies for some applications so might
actually it might be a way of opening
internals of the XMPP server through the
world so to speak to your third-party
libraries and when we have this thing on
the Left bottle where you can decide
what authentication plug-in to use so
again there is a selection of options
here a default would be just to usually
use Anisha again but you could also use
some command line scripts you could use
some external sequel database
it could be anonymous so different
options here and this is usually the bit
that we need to customize for each
deployment because depending on what is
the environment you want to integrate
the XMPP with you will have different
authentication mechanism and well in the
very bottom you see also some client
handlers today it would be TCP which is
like regular default for XMPP you could
also have WebSockets we are working on
adding bosh which is XMPP over HTTP so
this could be add another way of
connecting your clients okay
so now on the next slides I have quite a
few different tests that we have
performed over the last weeks and months
they were tried on different
environments with different
configurations I know how it is with
benchmarks I mean one benchmark will
really fit your scenario some other will
not so I would say that this is just to
give you an idea of the throughput we
can have of what how would you scale but
as usual we need to execute some
benchmarks on your targets environments
actually get a better idea of what is
really through put of your particular
system so to properly plan capacity we
would always recommend to run some
stress tests anyway especially if you're
planning for high throughput for this we
use song a tool that has been mentioned
quite a few times over the last two days
in this conference so already has
support for XMPP so we do use it and
there is also other software stack for
some of those configurations which are
listed here for starters this is just
dedicated box an 8 core AMD with 32
gigabytes ram with all those persistent
data stored in external my sequel
database and this is just to get started
and to get an idea what a single
Mongoose I am box could handle on such
dedicated box so this is our
configuration this is roughly the
scenario we're gonna run so what I'm
going to do is I'm gonna first log in
all of those users with a rate of 150
per second so every second we get new
150 TCP connections established until we
get to the point where we have 400,000
users connected we don't send any
traffic those user are shy but this is
again this test is just to see what
would be the memory requirements for
that many users online so I have now two
slides that show resource consumption
first
tell us how much we saturated CPU the
red line is Mongoose I am the lower the
better and here we have like 70% of CPU
it's at the 8 core AMD CPU that we use
here so this is stable there are no peak
sauce it's quite quite stable this is
what we do is we log in users and this
is what the memory consumption looks
like so we again Mongoose is red and
it's linearly scaling the more users we
add the more memory it needs but the
good news is that this is actually
predictable so it's linearly scalable
thing and we ended up consuming
something like 22 gigabytes in total so
there is still like 10 gigabytes left
which will be used for messaging once we
start chatting we will of course we need
to have some extra memory allocated to
accommodate those messages being parsed
sent routed so I would say that you
never want to have many as many users as
you have run you always want to have a
good chunk of RAM left free in case
people start to send lots of traffic and
actually this is the the next test we
have performed on this box now instead
of trying to aim at the large number of
connected users we aim at a large number
of messages sent per second now there is
only 75 thousand users connected because
the throughput we were trying to get put
sent through the system was giving hard
time to some nodes it's actually quite
not that easy to kill the server but
still again the aim of this test is just
to show throughput of messages for the
system so we aim at roughly 20,000 per
second this is what we get this slide
again shows CPU utilization now you can
clearly see there are some two phases in
this test so on the left-hand side the
first phase of the test is just low
users we again we first need to log them
in then there is a short break and then
we start sending messages so now we
actually managed to saturate the CPU a
bit more it's something like 80%
saturation of CPU and we are handling
roughly twenty twenty one thousand
messages per second so this is the
second slide again mangoes red lower is
better memory footprint so you see that
again we linearly scale once we add
users when we do nothing so for some
time there is really no well the things
are stable and then at some point we
start sending messages which of course
requires some extra space to parse to
buffer to send but things are quite
stable I mean once you once we reached
our this target throughput of 20
something thousand messages per second
things are okay all right so this was
first set of tests load tests to give a
rough idea of what food we are talking
about again depending on the size of the
messages depending on size of rosters
all other things need to be taken into
account if you really want to plan
capacity of your system so we just pick
some values that we felt like could give
you an idea but we would need to be more
careful to actually plant capacity of a
target system rather than just take the
slides and plan based on those but still
it's those numbers are quite good okay
and this brings us to another set of
tests now that I want to show you how
about show you some things about the
running Mongoose in cluster mode where
we have more than one Mongoose I am in
an airline ring we need to switch to a
different environment so we decided to
go with Amazon ec2 it's not like it's
our desired environment for running
Mongoose I am
but it's very flexible in terms of
adding noise removing nodes so it was
fairly easy for us to set up some low
tests and machines to give it a try
surprisingly actually those Mongoose I
am knows were performing reasonably well
on Amazon ec2 but the experience so far
with Amazon ec2 is that things can get
wrong not easy I mean not going down
showing up again nodes ending up with
very distant locations I would say it's
good now for testing just to give you
some idea of what are the scaling steps
but again I would be careful deploying
this in production for Amazon ec2 I've
seen people doing this I've seen some
successful projects running on Amazon
ec2 but this has to be carefully
considered especially if we're talking
about really really really large traffic
large deployments that we had a chance
to work with rather deployed on
dedicated data centers okay so we are
trying extra large instances of Amazon
ec2 we try free Mongoose I am nodes
first and this is the test we will try
we are going to load 50,000 users this
is like the out of the box configuration
so no external dependencies on my sequel
Redis whatsoever this is just purely a
land-based deployment Monisha powering
both the persistent and transient data
and let's see what happens so now on
this slide I'm trying to show three
instances so mangoes one two and three
and again first we lock them in second
we send some traffic so this is CPU
utilization on those three nodes they
all behave pretty much the same because
the strategy of the Lotus was to balance
the load equally now memory here is
actually the opposite
this is free memory so the higher the
better though we run the test until one
of those machines almost run out of
memory and now the interesting finger is
that because we used standard monisha
configuration standard Mongoose in
configuration where all monisha tables
are loaded from disk to memory and
because we had 50,000 users registered
and each one of them had 100 bodies in
the roster we ended up with quite a lot
of memory stored Indonesia and when
loaded into memory at the start so you
see that actually the start point for a
Behrman goose' nodes even though those
instances support like 16 gigabytes of
RAM of 50 16 gigabyte of RAM actually
our starting point is around 4 because
the media table had to be loaded into
Ram so of course this is somewhat
limiting I mean especially if you want
to grow the user base so next step if
you want to scale to higher numbers
would be to move all the persistent data
from Lygia to an external database like
my sequel so this is our next step this
is what we do now we keep the session
data Indonesia such data being well TCP
some present some subscription lists and
other things and other well passwords
whatever it is needed
dante to authenticate users as well as
your list contacts this is all in my
sequel and now we try to load much
higher number of users over 300,000
this time around actually will be
sending presents updates or eleven
messages this is a limitation of the
Amazon ec2 and song environment on which
we were low testing so actually it is
less expensive for the Zune to generate
presence traffic which will actually
generate a lot of pressure on the on the
server because well each one of those
users has a hundred friends and every
time you change a presence you need to
update all your hundred users so one
small sensor can generate lots of
traffic so this was easier for the song
to put a lot of pressure on on the on
the server this way okay so let's see
how this one went so we have again CPU
utilization free Mongoose I am nodes
something roughly except 20 percent here
to lock them in that's fine there is the
presence phase which is less stressful
for servers but the fingers this is the
key point is that now we are handling
three hundred thirty thousand users on
those three machines as an opposite to
the previous one where we have stored a
lot of data in memory here actually we
start already at the very top almost 15
gigabytes of RAM free and then as we
login users it decreases and when as we
start sending traffic it decreases so
this is this is definitely good
configuration if you want to aim at
large large user base and now yet
another one having an opportunity to run
it on Amazon ec2 we tried another one to
see what happens if even the session
data which in a usual is actually as
long as this is kept in Asia its
replicated on all of the nodes
what if would this one be moved away
from
the cluster of Evelyn nose to some
external fast in memory storage like
Redis
so again by default as we configured
Nisha Nisha will be replicating all the
it across all nodes in a cluster and the
session table which stores all the
sessions let's say we have a hundred
thousand users log in on this node all
of those 100 thousand sessions will be
also replicated into the tables on other
two nodes and this is to facilitate
routing this is why actually Mongoose
will scale well with Nisha as long as
you have a lot of RAM because I'm
routing a message which came in here and
has to go out here it's a quick
operation you just have a look up the
local table you know now the pede of the
target session because this data has
been replicated and it's a reasonably
cheap operation to find out where to
route the message and when it's routed
using standard airline distribution and
when it's delivered to the process which
takes care of this session and then
transmitted back on the TCP socket of
the target user but now so this is why
amnesia may consume
possibly a bit of too much of memory and
you possibly could really need so we are
trying a configuration with Redis
slightly different scenario comparing to
the previous one we try to connect more
users very actually we connect on
quicker and we send messages and see
what happens now we have one more node
here which is ready this is the blue one
which is in the bottom you see that this
traffic we generated that didn't really
stress then red is node Mongoose nodes
are a bit stressed because we are
logging now quite a few users per second
it is 2000 users
second on the three notes so it's
roughly 700 per second on a note but
because now those Manisha Tunisia notes
don't need to replicate the data as
users log in this actually offloads
those notes so we can actually push
those harder we can have steeper curve
logging into the system which in some
cases might be actual requirement to be
able to quickly log in users and again
the second phase is more for sending
even though now we need to actually look
up this data and an external data
storage that's okay
it actually works reasonably well we see
here like 60% of CPU utilization that's
okay and in memory wise there is a
difference I mean this after logging the
many users we end up having something
like eight and a half nine gigabytes RAM
free previously we would end up with
having like eight so couple of hundreds
of megabyte have been saved which are an
environment like Amazon could be an
issue if you are running on the
dedicated box it's usually lots of RAM
anyway so Redis may be an option for
some deployments it's not a requirement
Nisha is actually doing reasonably well
if you want to deploy clusters of ten or
a thousand among these nodes so it's not
like Redis is a must but it's an option
in Mongoose a.m. for some specific
deployments okay
yet another step in scaling I would like
to share with you this one is borrowed
from Alexander Fogg System Architect at
Ubu I had the webinar with me a few
weeks ago so I borrowed some of his
slides here who is one of those modern
messengers that support video chats
audio charts
text chat he runs on all sort of
different platforms and in 2010 very had
traffic roughly around 600,000 users
connected with 10 million registered and
sending 500 messages per second so this
was roughly scale at the time but if I
had to scale to read to work bigger
numbers because there was constant rapid
usage grow so after 2 years since a new
platform has been deployed and airline
base platform has been deployed we are
now running well quite a few more
machines sorry bring support quite a few
more users online 2.2 million online
users 70 million registered and 10 times
more traffic is being pushed through the
system so now for such large deployment
like this one a single cluster of mangos
now it may not be enough it could be
because well traffic coming from
different continents and locations so
you may want to have actually
more than one data center where such
XMPP nodes are deployed it could be also
basically to escape to such big numbers
and this is what we do this is Rafa
sketch off of their current
configuration they have two data centers
new one being added and in those data
centers they deploy so called scaling
units a scaling unit is like a reason
small ten times the 10 node big cluster
augment of mangoes like nodes so we
deploy a cluster of nodes and then we
fed the rate between those clusters so
it's like yet one step further now we
have cluster of clusters so this is how
it works it's a one such scaling unit
would have something like ten airline
nodes some extra two for operational
maintenance so twelve in total in one
scaling unit data would be stored in my
sequel database there are some also
other nodes involved here that help
deliver some features for business logic
and you will have several such scaling
units federating between each other so
this is how its kind to even bigger
numbers
okay
so if you're considering some messaging
platform for a new project or you are
facing some scaling issues and you
weren't looking for a replacement
well takeaways from this presentation
XMPP helps with interoperability the
airline project we're talking about here
can be rapidly deployed and customized
thanks through some pluggable components
and it's quite efficient so if you're
planning even big numbers it's not like
you will need to buy thousands of boxes
project is open source you can find
source code on on github
we also prepackaged it for Mac OS Debian
Ubuntu but it's fairly easy to build the
project from sources more can be found
on the github account when its usual we
work on contributions so that's about it
I see some questions okay question
correct yes
now yes we did we did I mean here indeed
focus out that so just to repeat the
question the question is what about
offline storage so for users who are not
on online at the time of receiving a
message so that they can pick a message
with my login so there is an extension
or plugin which defines how we would
deliver offline messages in XMPP there
is a airline module which provides a
feature for mangoes
it's called mod offline by default it
would again use Malaysia which has
certain scalability limits for disk
storage successfully we have used my
sequel for such larger offline storages
as well as for archives it could be even
react if you like I don't think we had a
chance to integrate offline in
particular with ryuk database but this
could be also another option so far at
least the scale of offline messages that
we had a chance to work with fit my
sequel or cluster of my sequel notes
enough ok a question
so originally this was simply XMPP
Federation so the server to server
Federation protocol later as it turned
out it was actually it's a bit expensive
at the very beginning when you do
handshake and you establish the
connections we just remove for some
parts of it and just make it more
straightforward this is a trusted
environment we don't need to have some
heavy authentication routines it's still
running server to server so it's XML as
defined by RFC s and all this but it's
slightly what like it's lightweight at
least the initial phase so just to be
sure that if you're talking about this
this is over sir XMPP server Federation
so this is dedicated TCP connection this
is not airline distribution as far as
well as far as we are talking about
communication here between nodes within
one cluster yes this is standard airline
distribution so stanzas those XMPP
packets have some airline internal
representation and those airline terms
are being sent over between nodes until
they reach some process which will
transform them into XML and send on TCP
yes correct another question yes correct
yes so the question was if the video
part of the application is also somehow
supported by this airline based system
well the the chat feature is used to
signal certain events between endpoints
but the actual video streaming is a
separate feature it's not like in the
scope of this I am server that I had a
chance to discuss so it's a separate
media cluster okay another question
yes well goose
well started as a for curvy jeopardy so
we have to just remove some of the
bottlenecks that we have learned about
while working with it but it is the same
plug-in architecture as found in each
ability actually and as an example of
nice open source thing a jeopardy team
has today announced that they have
released a new community version of
jeopardy and they have migrated to
Mongoose I am like internal
representation of terms which was using
which now uses binaries airline binaries
rather than a blank strings which is
better
well memory wise so this I mean even
though this project has started as a
fork of jeopardy and for the last two
years
those were rather different as far as
the internal representation is concerned
as I have read only few hours ago
now this jeopardy project also has
migrated towards this internal
representation which is great which is
an excellent example of how open source
is powerful even though those are two
forks now we sort of again contribute to
each other so
so
okay so the question is what if user has
more than one active session how do we
cope of it okay as far as XMPP is
concerned
each user session can be identified with
a so called resource so your user ID
consists of like username its domain
name slash some resource name so user
could be connected over more than one
session and could be uniquely identified
by resource there are some extensions
proposed on the XMPP dot or website that
tried to tackle this problem
what if ya by default each a birdie just
has priorities and so does Mongoose so
basically the last session which was
active like sending messages is
considered to be highest priority and
traffic will be delivered only to this
highest priority session but this is
open source ever it is fairly easy to
modify this and make them all sort of
equal so that every time you send a
message all active sessions would
receive a copy of the same message this
could be an option but this is not a
default behavior okay another question
okay
okay so the question is in destructive
networks when in mobile clients for
instance would often go online and
offline and they would not necessarily
clean up what how do we deal with this
so turn well different issues here one
is indeed to check health of a
connection one of the extensions
discusses pink so there is mod pink for
those you can ping on regular basis
every few minutes your users and if I
don't respond you consider their
sessions to be dead and you simply
terminated on your end and this is one
idea it's I mean again it's okay it's as
hard as configuring it in the
configuration file so that instead of
running some standard plugins you would
also add in the list of plugins in the
configuration file this mod pink now
this could be a good thing or a bad
thing a good thing is because now we can
check help of those links the bad thing
is that we keep calling users and we
need to keep receiving this message and
keep sending them a spark which in some
situations is not something you want so
another approach to the problem would be
to to be less eager if I can say so
let's eat in finding out what's the
current health of this connection and
only when we send the message to the
user
sort of ask him to also acknowledge
receiving a message and so when we send
the message and the user does not
respond with an acknowledgement when we
consider him to be dead and actually I
think Skype was mentioned here I think
this is what Skype does sometimes people
appear online but as soon as you try to
send them a message by sudden you
disappear because we find out Oh active
we were online some time ago we never
really propagated presence updated we
are now going offline but we are
actually offline so this is less eager
but when it's less precise
so for some reasons it could not be the
way you want it but again so this is
another way of trying to address the
problem which actually also tackles
another problem which is how do we make
sure that messages are delivered in the
first place in such disruptive networks
because as we see those smartphones
being connected through a PA and some
sort of hand proxies those proxies are
not always honest with us mean the the
server side about whether the
connections really up or not so it can
take quite some time for the TCP socket
to come to a conclusion that this user
is actually not connected anymore
the question is we can auto detect some
of those scenarios well
Mongoose is based on the standard and
like gen TCP as long as this on the TCP
layer you would receive some sort of TCP
clause or other kind of participe
signaling yes we would close TCP socket
on the server side and this would
cascade to Mongoose to close the session
yes however again sometimes actually you
you send bytes on the socket you feel
like you sent to deliver the message but
actually never really reached the
endpoint so for this to happen there
again there have been some extensions to
example P protocol proposed you can find
them on XMPP dot-org one of them being
198 some other are 184 may be derived in
more and we had a chance to implement
some of those for various
implementations we can talk about it a
bit more if you like but yes this
problem has been tackled one way or
another it learned much depends again on
what is the application use case and
it's not like one solutions fits all all
right
okay do we have any more questions
no so we are good to go for the airlock
meet up later tonight thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>