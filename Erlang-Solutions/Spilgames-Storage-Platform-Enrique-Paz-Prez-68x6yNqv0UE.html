<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Spilgames Storage Platform - Enrique Paz Pérez | Coder Coacher - Coaching Coders</title><meta content="Spilgames Storage Platform - Enrique Paz Pérez - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Spilgames Storage Platform - Enrique Paz Pérez</b></h2><h5 class="post__date">2013-04-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/68x6yNqv0UE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right well welcome welcome to the
databases track here drilling 53 my name
is Scott lipstick Richie and I fasho
I'll be a track host for the entire day
welcome really excited about this you
know I sort of did the the database
business a weird database business and
so the well weird Nazi gold fish kind of
stuff anyhow so that some of you may be
relatively new to her Lang and wondering
why you would do database stuff in
Erlang and you'll get plenty of answers
throughout the day to day first speaker
is Enrique post and I should let him
have as much time as possible talk about
it guess work say thank you thank you
everybody for coming so okay let's
unblock this thing first oh yeah so
let's let me start by introducing myself
my name is Enrique I work as a senior
lon developer for spil games and I'm
very enthusiastic about Errol on
programming started with that several
years ago I really enjoy it a lot yeah
to say something personal something that
I really like is really knowledge people
code I like beautiful coat and I think I
find fascinating the amount of stuff you
can learn about someone just by reading
code our workforce bill games and spill
games is a company in the gaming
industry company from the Netherlands
we're mainly a game publisher which
means we enable our platforms to
distribute games developed by others but
we're also game developers as well we
serve data to more than 190 countries
allowing games this when I started
working for volumes this was something
that amazed me I mean receiving
connections from countries that I mean I
not know how to put in a map to be
honest and
we are more than 300 employees mainly in
the Netherlands we have the headquarters
in hilversum we also have a game studio
in China we serve more than 200 million
unique users a month and this is the
actual challenge for us I mean what we
do hosting the games is not really
rocket science but the volume is
critical and we make our money out of
advertising and also AM user
monetization so here you can see a
couple of pictures of our portals we
have portals for different target
audiences or families this for girls a
couple of games first one is in-house
developed games are cooking class very
famous very successful so far second one
is not one of ours games it's a sternal
game galaxy life a very nice strategy
game multiplayer but I'm here to talk
about storage platform so yeah the first
reaction that normally I find when I
tell people in the ER long world well
we're developing our storage platform is
sort of like what another one I mean
don't you have enough of that well let
me spend you a little bit on the
reasoning behind so spill games the
company comes from a scenario sort of
like this is familiar probably for most
of you classical lamp stack with my
sequel so our customers would enter or
HTTP interface through a load balancer
did we arrived to a PHP application that
will receive the code over there
actually detect what to do with it maybe
go to some cash retrieve some data then
talk to another service with parts of
that data to us recover build some more
information for requests that other
service might have need to go to storage
retrieve something from the storage take
it back return to the first one if
you're lucky not go to the third one and
then finally return an answer okay and
this is a very very simplified
version I mean I'm not talking about the
caching proxies the CD ends everything
this is a mess it works it made us grow
incredibly in the last five years but
it's really difficult to maintain and
it's really difficult to predict every
time you change one of these guys since
these guys are talking to a lot of other
guys and also talked into the car also
talking to the storage is really
difficult so main concerns your storage
model is all over the place each one of
these guys knows a little bit of the
storage and a little bit of the cash
since all of them are talking to the
databases using native drivers it's
actually quite difficult to sharp the
data of course not all your application
developers are or should be DB experts
so you have a lot of performance
concerns security concerns yeah and
again caching here I represented like
one big shared cache which is true each
but again Cashin proxies cdns everything
is cash in many places just for
performance you have a lot of
information cash in the client as well
is difficult so we really needed
something different and this is more or
less the idea we came up with so we what
we would really like is the load
balancer to address a request to a
public interface that is very strictly
defined and acts as an aggregator and
that public interfaces aggregates
information from the different
applications so these guys don't talk to
each other and each one of these
applications mainly Erlin now also some
Python most important monitoring tools
goes to this thing we call SSP this is P
stands for spil games storage platform
and this is the thing that we were
trying to build beginning last year so
it's like okay what we really want from
it is that the clients don't see
anything about the storage that point
they just ask the storage platform
this type of data and I don't care where
it is I don't know what technology is
used and I don't care to be honest I
just want the data and of course the
platform should take care of the caching
as well because again the client
application doesn't care if the
application is the information is coming
from a cash from a disk and and of
course a lots of Engineers talking will
always come up with very beautiful ideas
so like okay let's create a wish list
what type of things do we really want
from this abstraction layer over here
well we want to make it a transparent
chardon layer and we want to be able to
share the data based on the data
ownership so if we have data that
belongs to users we want to be able to
shard base and use ready for example of
course the platform should be highly
available it should centralize all the
cash in so it should be good enough
performing enough so all the caching is
there and we no longer need to spread
our cash all over the flow we want to
have one and only one data model defined
in one place and not as before several
applications knowing bits and pieces of
the data model very interesting one we
want to be able to change the storage in
a completely transparent way for the
client because again the client doesn't
really care how we store our data so if
we decide to switch from a my sequel
table to a swift storage for some type
of information the client shouldn't be
rebuilt or modify or adapted for that
the clines just requesting data and and
we want to be able to scale
geographically meaning bringing the data
closer to the user and I will get back
to that later in the talk so this is our
big wish list now in order to do that to
make it real
we want to have a clear mind set and the
platform again has to be always a battle
so we really need to distribute it and
make it nice make it work we want to
avoid global locks because in most
classical storage engines those are the
real problem and not only in storage
engines if you have any system using
locks if you look globally your chances
of designing something that doesn't
really perform according to your rocks
are really high so we want to avoid them
completely and accept the change is the
only constant and brains the possible
inconsistencies meaning your
requirements are always going to change
your hardware is always going to break
down when you perform an upgrade your
states between the different servers
will be inconsistent you have to prepare
your show software to live with that and
of course we were thinking a lot about
the dynamic paper from amazon well
looking at this list and ebay
architectural design principles lots of
things also already build systems for
storage like react we were looking at
the components that make react possible
and how it handles different things and
we basically took a lot of inspiration
and we came up with an idea yes ok let's
build a key value store with schema and
it's like what give a live store with
scheme well we like a lot the properties
for key value store too far to do just
have a key and retrieve a bunch of data
and makes it easy to distribute and do a
lot of things but we also liked our
schema because it's easy for validations
basically and this way we can actually
have the schema in the platform and
that's the only place where we have the
schema and since the clients are only
requesting by key they don't really need
to know that schema in detail strictly
so it could just work
so the main components for this system
are a bucket and this is a very similar
concept that you can find in react to
stuff we generate the buckets from the
definition there are mostly generated
OTP applications and each barker offers
a crowd like interface where you can
actually filter for example if you have
a bucket for user high scores and
actually give me all the high scores for
these user ID and there you have your
key value store but you can apply
filters later like that are more recent
than last week okay we have gads or
global identifiers and this is 64-bit
integers this is what we use a skeet and
they represent the data ownership so in
our company at the moment we have gads
meaning user IDs and game IDs those are
the two types of data owners that we
have and then the interesting bit
kemsing this is again not a storage
engine but a storage platform that
accessing abstraction so buckets can use
different storage engines so you can
have a bucket for user preferences that
uses just my sequel if you want a couple
of tables there and that's all the
combination all the magic is hidden
inside a platform you can have a bucket
that uses just a swift binary storage
you can have a bucket that combines the
mole basically you can have a bucket
with react you can have whatever you
want okay the storage engine is is
obstructed on top and the platform is
responsible for caching bucket gid
combinations so again user high scores
for some user ID can be cached and the
request can be atomic per package eid
needed yeah and please keep in mind that
we wanted to avoid to avoid using global
locks so this was hard one so how to
operate with such a system we have the
components in mind now what type of
operations are we
allowing our clients to do well we have
optimistic operations and optimistic
operations mean happy flow it means like
okay I want to execute something but I
really care a lot about speed if
something crushes something goes wrong
but I can afford it I mean this is not
critical information I just want it to
happen fast and these type of operations
of course will update first cash and
then the disk because you just care
about having the data updated quickly
consistency is not a priority in this
case you can afford a couple of crashes
for example I mean think about user's
activity feeds where you track what the
user has been doing you didn't really
want to block your system waiting to
update that the activity feed for the
user just in case a friend of the guy
just immediately comes to the system and
wants to check to the exact second where
the guy was doing one second ago I mean
if you miss one update it's not that bad
okay and the same thing for the popular
games list think about every time you
start a gameplay you really need to
reduce one more gameplay well you miss
one it's not critical okay the there's
one important thing that I want to
remark here there's no warranty of
aventyl consistency rick has a very nice
definition of eventual consistency with
gossiping protocol and stuff we don't
have such a thing this is that that you
can afford to lose and that's the reason
that you are operating optimistic if you
cannot then you operate pessimistic and
pessimistic is the classic operation
where you have all your warranties so
the consistency is the key it might take
slow but you're going to wait for
confirmation if you receive an error
message something like that you can
retry your operation you're dealing with
critical data here think about payments
data or personal user information and of
course in this case what you want is
persist the data on the storage and once
that has been successful than you update
the cash return to the client the job is
done you know all right so how do you
manage to put those components together
and allow those types of operations
these are more or less the details again
you can find some familiar things from
react here so the system in the
different notes all the notes will be
running a look at application and that
look up application will keep track of
replicated hashing ring yep and of
course a hash base is your GID space so
your key space now the bucket
applications will be started in the nose
knows don't have to run all the buckets
all the time it's not necessary you can
have some nose running some buckets and
some others with others right but when I
bakit start it registers components
which we call pipeline factories and
pipeline factories you can think about
them job managers something like that
okay are the responsible for a range of
gads to be operated successfully and
when the bucket application starts it
just goes and registers the pipeline
factories into the hashing ring so if
you get a hash let's say one then it
would be managed by pipeline factory
number one which is in node one yep okay
and the hash string we implemented a new
table is replicated we use run copies
dirty reads reading is of course most
common operation every time you have a
request you have to perform a quick read
and you only use a lock for the
transactional rights yeah all right so
in a pessimistic operation destructive
operation the way the system works is
the follow is the following any random
node in the system is this case the
first one receives in the bucket one
request it will try to look up the
pipeline factory through the lookup
application the pipeline factor is
responsible for the GED will come up in
this case with pipeline factor number
four bubbling factory number four will
encode the job in a generic way so it
transforms your API requests for
profiles insert whatever into a generic
job
that generic job well first the factory
ensures there is a pipeline which is the
actual worker for that GID yeah it makes
sure well either retrieves the one
that's existing or creates a new one and
it cues that January cooperation into
this worker for the GED and yes I said
we're queuing operations for one dat and
again normally when you rehearse this
you get some reactions from people and
this is what we normally get like decide
what you're queuing for one GID I mean
that means you're creating long queues
of operations I mean for what reason can
you spawn processes I mean can't you do
it any other way and yeah i mean the
criticism has a point there's an issue
this sequential access for hot spots as
a huge penalty think for example you
have a very popular game where you have
thousands of game plays in a second and
all those requests are actually going
for retrieving the basic game data a
name description and stuff for the same
game ID and are we queuing all of that
and that would be really bad and yeah
indeed it would be really bad we're not
doing it because there is a very simple
optimization i mean you read from the
cash out light of a pipeline now how
many pessimistic destructive operations
you get for the same GID in the second
well that's not that a high volume as
you can get for the reeds so you can
manage that and there are other growth
about serializing this thing sure well
you don't need your story engine to
support global locks so there we are we
avoided it why because we are just
making sure one operation finishes after
the next one gets executed and this one
also allowed us to combine several
storage engines for the same bucket
meaning that you can indeed combine
sequel tables with postgres tables with
a swift storage with something else
whatever you want and build your thing
and you operating although
one's for the same GED and until you're
done the next job cannot actually modify
the information in those places and of
course there's one more argument that is
very explicit that this most of our gads
are you sir jee IDs and users are always
evenly distributed I mean you will not
have the user that for himself execute
hundred thousand the operations
pessimistic the traffic operations in
one second I mean if you have such a
thing that's an attack that's not a real
user yeah okay in the wish list we also
wanted to always mentioned that we
desired to be able to change this
storage map this thing about the several
tables of different things in a
completely transparent manner to the
client and this is the concept that we
have of schema versions yeah this is not
gold version this is different thing
schema version so schema versions
determine what operations are allowed
and what's the exact storage map for
those operations client is never aware
of them client will just issue a request
and to retrieve some data or modify some
data completely agnostic on the version
and on the real storage and the system
allows you to have two schema versions
for a bucket running at the same time
and each bucket gid combination is
running one schema version this means
when you decide to migrate the schema
you don't have to migrate it for the
whole bucket just migrate 1g idea at the
time yeah or a wrench and very
interesting thing about this is that you
can actually do it under heavy load even
because guess what the migration of
schema gets cute in the pipeline
we also have a concept of sharks and we
wanted the system to be a transparent
sharding layer and i'm not talking about
sequel shardene strictly here this is
this generic shirt so some of your G IDs
for this bucket are in this class over
there and some other bunches in that
class over there basically in the system
takes care of it yeah it's of course
useful partitioning blocks of data and
the Chardon rules can be defined per
bucket by default you can do something
very generic like GID module the number
of shorts and of course bucket GID
combinations can be migrated between
shards and again you can do that and a
heavy load yep that means you can
actually have control or where your data
is that's something that I mentioned
before so how does this thing work well
let's zoom a little bit into what the
pipeline does so when it receives a
generic job from the pipeline factory
what it does is ok for each job that it
receives it goes to the bucket and the
bucket application will contact these
versions and shards thing which is
outside of the node itself it's shirt by
the notes as well yeah and it's used to
determine the version and the shard for
that bucket GED combination so once that
thing is retrieved the bucket knows ok
i'm working with a GED that is in
version 2 and it's actually stored in
the shower number one so it goes to the
specific code for the version 2 to build
the details of the operation meaning if
this is version 2 i know this is one
sequel table and one swift storage so
this is how I build an insert yeah so it
creates a closure something that it can
actually be executed and modify the
storage this also gives you the
advantage that you can put any type of
logic in your specific mapi mapping so
you can have I don't know commits and
roll box if you want for your bucket
they are healing in here in the code for
this specific version that uses that
type of storage to just implement it
yourself yeah and of course there are
some storage engines that will be more
helpful for some cases and some others
that will be more hateful sona for some
others so once the operation is exactly
build the pipeline has something to
queue and just queue it basically it's
just a closure that is cute and build
and run and then it goes the next one
okay so there was a lot of technical
explanation but this is also one thing
that we had a lot to expect yeah sure
you mention a lot of things versions
charts different story edges but that
probably means i need to know really a
lot of aps and a lot of stuff in order
to use the system and i really don't
care what you do I don't care about
pipelines I don't care about anything I
just want to give you a very simple
operation a very generic manner I no no
Earl at all and I want to be able to do
it to use your magical storage so how
can I do that well for that we use picky
and Vicki's an open source project very
nice we use it a lot currently
collaborate with anthem is the creative
Ella / of picky I strongly recommend you
check it out basically with picky we can
define our schemas in one place and
generate a lot of code out of it we can
generate the airline equivalents for
that definition we can generate
automatic translators from those
equivalents to protocol buffers to Jason
and it also provides us with an HTTP
server listening to it which is based on
web machine so it's a lot of code out of
the blue we don't write and it just
works and so to talk with our system you
can just easily use HTTP plus Jason or
HTTP plus protocol buffers or well okay
you know where line so you want to do it
perform a performant manner you just
include that definition that I mentioned
before in your client and use protocol
buffers to encode the payload
and why the protocol buffers to encode
the payload this is because something
that I mentioned before us well the
state there can be states mismatches
between your class right so when you
update that definition for your buckets
you don't update hitting all your
servers at the same time I mean that's
delusional it will never happen so might
be the case your client is using an old
version of the definition or a newer one
then the storage is actually using so
for that you use protocol buffers to
encode the payload you could use a prop
list just works the same way basically
we use protocol buffers because more
efficient that's it and you just encode
and decode when making the call and
we're receiving the result and as long
as you use some common sense we're
modifying basically and you don't create
mandatory fields out of the blue or
remove fields from one version to the
next you just create optional stuff and
make stuff optional then it always works
all right we also wanted to go well wide
with this so in our vision we
differentiate two types of things the
pink data centers and the blue data
centers meaning master and satellite
data centers so the master data centers
are the classic concept of data center
out there basically is this big
expensive building with lots of security
measures and big wires replicated and
everything really expensive and of
course they have a persistent storage
and they can own gads and with this I
mean each GID or bakit di D will
actually be owned only by one data
center which means it will be accessible
only in one data center this is a
distribution layer not a replication the
SSP doesn't take care of replication
this is distribution and of course you
can migrate the gads between data
centers right so classical case
in a very simple one you have a user
that lives in am sedan and suddenly you
detect that for then latest six months
he has been accessing from Seattle so
every time you retrieve the data from
the user you are crossing the ocean and
is really annoying okay so you just
migrate the data for the user to the
proper place you can also create your
URI sticks for the system to do that for
you yeah because of course we don't have
our DBA SS monitoring the users and
migrate and manually and the interesting
bit here are the satellite data centers
because this one's don't have persistent
storage with this I mean they are just a
huge cache that's what they are and they
are very easy to set up on the
Commission we use them on the cloud
basically so you can just easily use
amazon cloud or rack space or something
to just open a new satellite data center
in area if you want so this is where you
start basically you have one data center
all the clients accessing via dns to
that might master data center for all
the traffic you have everything there
you have huge disk security mechanisms
whatever and you have a cash but for us
this jump is very simple yeah so this
jump means you have your original thing
but also you open a satellite data
center with its cash and clients can
access that one via dns depending on
where they are right so we currently
have one master data center in amsterdam
now imagine we have a lot of traffic
from the states which is not a lot of
imagining and we decided to create a
satellite data center in the states to
speed up this region yeah so that which
can just do by opening in the cloud and
then the clients will start requesting
data of course this data center will
have absolutely nothing so all the read
operations and optimistic operations
will not found the gads on this cash so
they will have to go across the ocean to
retrieve the information from the master
data center and get back but once they
get back the information is cached and
then all the reads and all the
optimistic operations will be executed
locally
yeah and the beautiful of this thing the
beauty of this thing is that you didn't
need to do any migration you just
reroute the DNS traffic to your new data
center and suddenly huge crash starts to
be populated and you immediately realize
that you're speeding up the process a
lot yeah and okay you can do this
because there is a big event and you
expect a huge peak in traffic things
about olympic games this type of things
and then after three weeks after a month
you're done you don't expect that amount
of traffic for the area so you just
close or decommission your satellite
data center it is that easy but maybe
hopefully that traffic increase is
permanent so you want to go and make it
a master data center and the only thing
you have to do for this is of course
find the appropriate place and once you
have the upper plate place you just hook
the disc in there and slowly start
telling the system that now you're going
to own this GID that previously was
owned by this other guy yeah and again
you can use heuristics for population
alright very quickly over this just to
make clear the flow this is the flow
where the data center that receives the
operation is the one that owns the GED
for the operation and it's very simple
this operation is optimistic you updated
cash and then you press the data and as
soon as you receive the operation you
tell the client here whatever I'll take
care of it and if it's pessimistic you
first precise the data on the disk with
confirmation then update the cache and
then reply to the client is done if the
data center that receives the operation
is not the owner of the GID the flow is
basically the same but now there's one
extra data center and in the beginning
so if the operation was optimistic you
update the local cache do tell the club
afore even do you just accept the
operation you tell the client I will
take care of it you update your local
cache and then you create an operation
that is exactly this operation but you
just package it packet in a wide area
network
operation type that means we are also
building a while re nette will bust with
replication and and payload control and
checksums and stuff to actually
communicate between the data centers
right and the pessimistic operations are
just the same again I don't own the GID
I cannot do anything about a pessimistic
you have to wait I will cross the ocean
do it in the proper place but in the end
before returning to you i will update my
local cache yeah it says the same flow
yeah this is very beautiful but this is
what normally happens with beautiful
ideas so what happens if we lose a
satellite data center well basically
nothing happens if you lose this you're
losing the speed ups in your area
because your cash is gone right so
operations well as soon as you accept
that you have really lost it and it's
not like temporary down so you just
reroute the DNS to this guy and it's
done yeah well just reroute the dns and
of course you might be missing
optimistic operations that we were
actually executed on this cash over here
but we're on the way to be transmitted
to the master data center but then the
thing exploded so you have lost
optimistic updates but if you remember
the beginning of the presentation you
could afford loss not domestic
operations so this is not really
important now this is a disaster
scenario okay if you lose the data
center with a disc this pretty bad
because it means that you might actually
have this data center owns gads so the
information for this year this is only
accessible in this guy and this guy is
dead yeah so even if you reroute the
traffic and you realize pretty fast a
range of your g8 is the range that was
owned by this data center is no longer
accessible which is bad and
we also talked about critical data here
so does this mean that we lost critical
data well yes no luckily mostly no the
thing is I mentioned the it's not
responsibility of the storage platform
to take care of a replication but I
didn't say do not take care of the
replication ok so imagine for this is
very bucket dependent but you can happen
a synchronous replication mechanism
going on basically in your flow at the
system level imagine you're using
buckets with my sequel back end you can
use my secure application if you want or
you can implement your own replication
mechanisms again system level this means
of course that you are still losing
information because this is a
synchronous so the let the latency of
that connection will make you lose a
couple of seconds of operations and it's
bad but now compare that with losing a
real data center I mean is bad that is
not that bad alright so let's wrap it up
what have we learned in the process
where we are at the moment we're using
simple buckets on life with one data
center in Amsterdam we have added a real
app support for bucket only updates this
means when you just create a new bucket
or modify a bucket you don't have to
stop the nodes to just load the code for
the new bucket of course if you update
the core of the system you will need a
reboot it doesn't mean that you have to
reboot all the notes at the same time
again we're hammering SSP with property
based testing for validating all the
buckets interfaces and also for
generally generating complex scenarios
of lots of traffic doing pessimistic
operations at the same time and nodes
being shot at the same time and see how
the jobs are transferred from one
pipeline to another everything is still
working and consistent we're integrating
integrating restrictive search
capabilities which is something quite
nice for a key value storage thing
we're testing the one protocol for the
inter data center communication and we
will have more buckets going live this
first half of the year and a couple of
satellite data centers coming for the
second half year and in the process we
have used a lot of software of course no
one wants to reinvent the wheel all the
time so our old storage was my sequel
and we still use a lot of my sequel
through mi sequel which is a nice Erlin
library we have well the pluses here
mean actual contributions that have been
accepted in the upstream and the stars
are pull requests that we have issued to
the owners of the weapons so have
contributed multi database transactional
support to in my sequel and multi times
on support of course we have used GV n
EP for Jason encoding the coding in
stats d for hooking up graphite
monitoring which is really nice proper
for the property based testing so far
pool boy as a worker pool for keeping
the memcache connections and lager for
logging rebar as a building tool really
nice have contributed semantic
versioning for it to improve the
dependency versioning and share
dependency system to speed up the
process where you are on your
development machine and fixes for the
xref support we use picky and we also
modify a little bit basheh bench to add
support for point several graphs on the
same on the same picture basically so
you can actually compare the lines one
go
the slides will probably be on
SlideShare so so all these purple things
and also the stars in the places are
clickable feel free to go and take a
look that was it so now it's time for
you to ask questions yeah I was thinking
about doing that and it's the clear
light that is missing here you know the
numbers that everybody wants to see but
I did include them in the end and the
reasoning is the amount of operation
currently going through this platform
are not relevant compared to the amount
of operations that we are still handling
in the old architecture basically and
also we have more and more buckets going
live so probably we will make a slightly
modified version of this talk in their
annual conference in June and by them we
will have nice numbers what I can tell
you for now as the using basher bench
and property based testing to generate
sequences and laws shooting the system
in parallel the breakpoint was either my
sequel or the client submitting the
request never the software in the middle
yeah that's it yep
how do we do yeah well at the moment
we're mostly doing well of course where
I'd unit test for the basic scenarios
and more complex one for the stuff
related to a hashing ring and these type
of things but the interest in testing
comes out of two flavors the first one
is input and output validation and we do
that with proper with the very complex
generators of input and output and
checking for every operation that
basically if the operation is valid then
I should have modified storage and an
okay result and if not I shouldn't
basically it's very simple to say but
it's not very simple to test we use
proper for it and then for the load and
the interesting stuff what we do is
generate sequences to use a proper
finished state machine quickcheck has
the same thing as well and we basically
generate sequences of operations
destructive operations pessimistic ones
inserts for the same gid and these type
of things and then reach and we our only
post condition is after I read I know
what I inserted last basically so that's
what I should be seen and in the
meantime we dwell type of nasty things
like flashing the cash shutting one node
killing one node and yeah especially
with the parallel sequence testing is
very interesting because all those
comments and all those nasty things are
actually happening in parallel and the
sequins are interleaving so and an
actually proper can shoot pretty fast
and pretty sure we check as well and
yeah for more detailed load testing we
use some for the tea song or I don't
know how to pronounce that TSU and g
yes takings yes yes no maybe I explained
myself wrong each operation this is
still a key value storage so each
operation you do affects only one bucket
GID combination you cannot do one
operation that affects several gads
basically that's not the design the
design is more key value stores like but
a GID a bucket GID combination came up
two different type of storages down and
you're protected between different
operations with a pipeline mechanism yes
yeah exactly the exact implementation of
that schema version with that mapping
knows that ok I have to modify this
sequel table in this Swift store I will
do the first one then the second one
then check they worked you can go as
complex as you want their the specific
logic for one bucket schema version yeah
yes
yeah yes yep okay most of the monitoring
we do it with graphite graphite is
pretty nice too we hope into graphite
with east at ste this library by Richard
Jones is very simple a very very nice
and with that we can measure how long
typical operations takes the hot spot is
the amount of stuff some lines of code
are heat and these type of things and
real-time ish so a couple of seconds
delay but it allows you to explore those
graphs has very nice history support
that's what we mainly use now I
mentioned Python because this system is
fully built in Ireland but of course
unfortunately we cannot count with 100
or 200 airline developers in these 300
people that we have any company this
means most of our DBS know a lot of my
sequel because that's where we come from
we have huge volume is equal but if they
have to do something with this platform
they will not be able to do it in my
sequel it is yeah okay so how do we do
it we cannot use PHP my admin or
whatever so okay what can you go well we
can call Python okay then you can call
HTTP plus asian that's that's the thing
so we have Python applications for
hosting operations and DBS to be able to
manage the migration of the gads and
this type of thing the way they want
does that answer your question
yes yep this of course yes the question
was how do the filters work when you
operate via the interface of a bucket
and yeah the answer is the operation
work basically the pipeline operation
retrieves or operates on a list so
everything back GID combination access a
list of airline records internally okay
and then you apply the filters on that
list to actual with we fold basically or
at least comprehension or something like
that and that's how you do it and also
that means that when we cache we catch
the whole list of records we don't cache
filter results so each bucket GED entry
that is cached is either fully cash or
No non-cash at all basically and that's
that's how you make it and then in the
filters well when you define your your
bucket API and the fields that you have
your schema you also specify we don't
allow free filtering like just just
write whatever you want right now we
specify a couple of filters that you can
use per bucket
okay I'll be approachable later and also
tomorrow so any any other thing that you
want to ask it will be my pleasure and
just allow me to mention of course big
thanks to everybody and we're actually
hiring so someone is interesting just go
and take a look</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>