<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Cloud Scale Erlang - Richard Croucher - Erlang User Conference 2015 | Coder Coacher - Coaching Coders</title><meta content="Cloud Scale Erlang - Richard Croucher - Erlang User Conference 2015 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Cloud Scale Erlang - Richard Croucher - Erlang User Conference 2015</b></h2><h5 class="post__date">2015-07-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/te89NpsryTA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay thank thank you very much so first
of all something about myself so I
worked for a long time to sun
microsystems and was heavily involved in
the kind of calm boom there I worked
with two or three little startups at
that time ebay last minute calm we
helped design really what was the became
the standard blueprint which which was
deployed at a lot of the dot-com sites
at that time that kind of moved on to to
building large compute clusters and I
was the architect behind son's hat son
had a a dollar per per hour compute grid
which had several thousand servers in
that and I was the platform architect
who put that together I I got headhunted
into Microsoft to look after their their
windows live area so that was there
although internet facing side of the
business so that was passport hotmail
xbox live oh they called in properties
anybody who wanted internet-facing we
would stand up their servers what I was
a still relatively small it was around
200,000 servers we were adding four
thousand a month really heavily
automating everything in order to be
able to handle that that kind of scale
for the last ten years I've been mainly
working for their banks on high
frequency trading systems and that's
mainly because I get to play with lots
of good technology they invest heavily
in in high-tech there if we can shave a
micro second off the time it takes to
send a packet from one machine to
another that's a big win
so so that's been quite interesting
playing around with those technologies
and working with those so I really I've
kind of been helping a number of cloud
startups and using kind of my background
there too to help with those I code in
multiple languages sort of assembler
been using the line for a number years
now but fundamentally I'm a platform
architect and that's because I was
trained as an electronics engineer I
designed and built computers of design
storage systems of design networks I've
worked on the unix OS for about 30 years
and all those things combined into a
platform I part of part of what I'll be
talking about here is really some of the
challenges and around cloud scaling
other like so uh in terms of my or lying
history I came to the conclusion a
number of years ago that we were in a
dead end with object-oriented languages
in talent stop delivering as faster
processors and we were only going to get
more cause wow you know and we're still
on that road now and I was having
trouble first of all writing programs in
Java which could scale which had the
number threads to do that and more
importantly debugging those programs
when the way when they had problems when
they broke it was a nightmare and that
was really down to that to that to that
shared memory model I started the
research around to look what were the
alternatives functional languages would
look really good had I probably gone for
pure functional I probably would have
gone with Haskell but I liked all the
concurrency features and and the
communications the features within a
like and eventually it was really OTP
which sold it to me though as a platform
marketing I knew I was going to have to
build all those myself so having them
all there to begin with was was
a big win their number of years ago I
was commissioned by a cloud startup to
build them a multi-factor communication
system so this was a system where users
were expected to have to access it via
browsers have smartphones have iPads and
we needed to to keep all those
synchronized to keep the same content on
fa updated 1 the update needed to flow
automatic to the other needed to scale
to a million users came up with a a
dream based solution built around Erlang
an OTP used rabbitmq user name and
easier used yours as the main components
heavily using things things like the
rabbitmq is a message bus using a topic
based distribution they're using using a
persistent messages and to queue message
history so those devices making bacon
persist for 24 hours so that when when a
device connects it will receive all the
state changes which had occurred during
its disconnection so so so some good
features there now got that all built
and running functionally and then I
started to look out a Kate how do how do
i scale it now and it was really then
that I started to see it's actually not
as straightforward as I'd hoped you know
Oleg makes it really easy to develop
code which can run across multiple
system so I'm not going to dwell on
these to this audience you kind of know
all of this so it kind of does all that
for you and those are having come from a
distributed computing background those
are really difficult things to do so so
it's great that it that it does all of
them but when it when it when it comes
to scaling further it's it's it's a
challenge so as an architect rather than
just looking at okay well how do I so
over it to scale to a million people I
kind of it well wouldn't what would you
actually do if you wanted to build a
platform for a billion users if you
wanted to create the next Facebook you
know we'd want to do that with Erlang an
OTP won't we so or if you wanted to
support the Internet of Things they're
telling us there's going to be 50
billion I OTS by 2020 nobody's really
looking yet at what a platform to manage
those would look like wouldn't it be
great if that was Erlang an OTP when you
start to run through some of the metrics
so so I find the agent based approach is
really powerful nice quarterlings good
for that and I think it would be really
powerful for the internet of things as
well because it means from my
programming perspective I can talk to an
agent and the agent will eventually talk
to the Internet Oh to the IOT when it
gets the chance but if I want to know
it's last known stay I can get it from
the agent if i want to send commands i
can send it to the agent and the agent
will eventually send them across to the
IIT so it's a very powerful programming
model so if we look at this hypothetical
a billion if we start to look at okay
well how many servers would we need to
run that on so I can say well if I put a
million processes on every server and
I've got an agent and III Erlang
processes / asian i'm probably looking
at around three thousand servers if i
take that number now and i started
looking okay what would the memory
footprint look like for that so if i
assume an active agent requires two
megabytes that on Arthur idle agent 24k
I start to get and I distribute those
evenly across those servers I need 6 16
gigabytes for the active agents in point
8 gigabytes per server these are for for
my older ones but I've got Ed's to
handle I got message buffers I got also
so
things so 32 gigabytes kind of sounds
about right yes you could get servers
with a lot more i could put service with
ten times that in but then you're
starting to have to support 10 million
Erling Erlang processes on a server
perhaps 10 times that number of TCP
connections starts to kind of get scary
are those kind of numbers you you
probably want to stick down at the at
these numbers better so okay and when
you win it when I look at some of the
cloud providers out there people who are
actually supporting a billion users
today yeah they're in these kind of
ranges so so so it's it's the right
order of magnitude so really you know
for cloud scaling we want to be looking
at how do we support kind of a thousand
plus servers and you when we look in
what's in when i started with OTP I kind
of glanced at a lot of the components
there but it was only when I started to
investigate them further its run I
actually they don't do what I need them
to do since the display see it it allows
me to define nodes where my swear my
application will run but it doesn't
provide the scalability oh it doesn't
provide even basic load balance it it
provides reliability which is not a bad
thing but it but it but it but it
doesn't solve the problem process groups
that that helps me but it but it leaves
it to me to actually messaged them all
so it tells me who are members creates a
monitor fun it does easy things to be
honest it doesn't do the hard things
things it does are really easy and it
breaks the trouble is it doesn't stop
members joining more than once they stay
in that process group moniece they leave
the same number of times you end up
having to write a singleton to actually
manage membership
otherwise it breaks gen server cluster I
looked at that and I thought oh that's
great you know somebody's done this it's
it's similar to dis today see ya but it
managed to stay it allows a single
master Tootoo to manage stay and if at
master dies for somebody else to take
over so so again it does something
useful but it doesn't give us the
scalability so what are some of the
issues faced when we do this where
you've got it when you when you know
service joins that cluster it's got to
tell everybody that's there there's no
notification a node has has left that
and unless you actually set up monitors
one of the big things though is that
placement is explicit when you spawn
you've got to say where you want to
spawn it to and I think your OTP is
being pragmatically designed for for you
know the specialized computing
environment a key it gives you a
reliable environment but it is
documented as being constrained to the
50 100 nodes which you know isn't it
it's not going to help us on this on
these big sites so it's focuses on
reliability think that's really
important it's not focused on on
scalability for example supervisors are
constrained to only start processes on
the same node which you know under most
circumstances is probably the right
thing but not always in a cloud like
environment so I kind of what was surely
somebody must have solved this and lots
of people out there who have done this
and as a scientist as my background I
started to look around for real data our
on scalability this was work done by
amirah fiery as part of the the release
project and part of what they did there
was to benchmark react so tested that
with with bash Oh bench
and initial investigation there showed
that scaled out at about 60 or 70 nodes
what was interesting was that there was
plenty of resources there there's lots
of CPU spare lots of memory lots of
network bandwidth so so why was it
scaling out the conclusion or the
hypothesis i should say at that time was
it was that it was the global operations
so so they went on to investigate that
further and they created the benchmark
which allowed them to vary the
percentage of global operations and
measure throughput there so they created
a benchmark DB bench for good I had some
commonly occurring functions for
distributed program so aware is named
really just looking up in a house in a
net stable spawn but to real global
operations register a name and um
register a name and each of those has to
has to hit every node in that cluster
has to be it every one of them's got to
accept that and what they found was but
when they varied the amount of those the
0.5% it closely matched the performance
profile they saw from react so it was a
good proxy for for scalability
constraints on on react when they varied
those percentages which is what's shown
here so if they increase that percentage
to 1% the performance went right down to
what I've got 30 nodes scalability if
they took away the global operations it
went right up to 1600 notes so it showed
that that percentage of global
operations was was one of the biggest
constraint sir so people have solved
this seventh day I'm sure something some
of them are sitting here in this room
well certainly at this conference and I
looked around there I so whatsapp held
up very often as as
as solving this particularly and and I
have been very open about what they've
done and sharing the dissolution they've
done there so I spoke to rick read a
couple of weeks ago and got latest
update from him in terms of where they
are at the moment so they're handing
around about seven million messages a
second are using him leisure as their
metadata store so I've got about 36
billion records in there that's stored
in memory so four terabytes of ram
across their 75,000 writes the second
per node so they're running on over a
sales and servers they're fairly beefy
serve as though you don't get those
servers on Amazon 20 cause the server 64
gig to half a terabyte of memory there I
think that reflects that they've gone
gone a lot of it and easiest or stored
in memory there so and I think you know
Oh Lange runs really well on big servers
and that's you know that's one of the
great things here is that you can reduce
the number of servers by using bigger
servers so out of those 1,300 to chat
500 mms interestingly they don't run the
nexus freebsd but they do use various
OTP components so that that that's
interesting that they've stuck with some
of those but big advice from right with
decoupled d couple use asynchronous a
synchronicity and partition everything
they very heavily partitioned their own
easier very heavily partitioned that by
using separate tables we go back to some
of those other other folk there I spoke
to ad in row who designed the media
streaming service used at Nasdaq they've
been running that for a couple of years
now every time is a big announcement on
NASDAQ lots and lots of people want to
see that live they usually spin up 500 I
nodes on AWS to to handle that they test
it routinely on a sales and nodes an AWS
to do that but speaking to them
it's all hidden nodes they don't use any
OTP they've had to write a whole
boatload of stuff themselves in order to
ER to achieve that OVO business data
from a few years ago like many cloud
people they based display as multiple
scalable units so that they can manage
each one simply sweet scalable units are
full stack each stack has around six
react servers and i would estimate about
twenty service per cluster there a away
i expect from the area well guys that
was the good a lot of servers they are
actually deployed geographically so each
cluster is only around six 60 for
servers in each of those another
well-known add service I spoke to them
these were an official number so I
couldn't put his name or the ad servers
but I'm sure you can guess who it was
given it's not AOL so so they've got 79
nodes in a way it seems a strange number
79 Bennett's that's what he said and
they've got a cluster running there or
399 so it is much bigger but they don't
use any of the distributed services
again because they break at these points
so we only one there running his nose
alert logic I'm told they've got over a
thousand Erlang now it's but I couldn't
get real details on those the disco
cluster this better known that's a
hundred cause I think that's about 100
nodes in that cluster I was doing some
work with Stephen osky at bass show
before he left and so there was some
joint what we were doing there but
speaking to them more recently faith
their advices is it's the partition is
not to go above a hundred I stagevu had
been that it was the global operations
which were work the constrain so they're
either telling you to put a caching
layer in with radius which I think so
kind of a bit of a shame because part of
the reason why we went to to react in
the first place was because we wanted to
get away from em cash d in secret over
and not have a cash in there but now it
seems that yes if you want scalability
you
gotta put a caching layer in again right
sure yeah bit of a disappointment I feel
I spoke to ray Viet couchbase so his
largest deployed cluster was between
1800 nodes so they they test 150 but but
found that it struggled to get this
performance started to decline at around
a hunting 20 so once they got customers
with bigger you know more servers its
split is partitioned over multiple
clusters so kind of so what do we need
what do we need need to solve so so the
explicit placement I feel is a bigger
when you put together a large cluster or
servers you use low-cost servers if you
can save a hundred dollars on every
server there's a thousand open that
starts to be a big saving so so you cut
each one of those down to the minimum
which means they generally tend to be
less reliable than other servers and
that meet you if you're running a
thousand servers in a cluster it's
actually rare for all sales and to be up
and that means you've got to be able to
cater for the service coming and going
plus you need to be able to maintain and
to replace them and nowadays there's an
environmental cost if not a real cost
with keeping servers running you either
want the spin them down so you don't pay
AWS we just want to save the environment
impact and power them off we need some
kind of low distribution and
optimization so you can spawn a new
processor onto a optimal node and it's
not just standard load balancing really
you want to you want to move them to a
server which is say between twenty
percent and eighty percent utilized what
you're trying to do is is to be able to
keep servers busy and to be able to
remove servers from a cluster when you
don't need them again because of that
cost an invite
impact on there you need to solve nope
no discovery an announcement I if you
assume you've got a reliable Network and
I come on to that in a moment we can
make those reliable a distributed
supervisor which was able to start and
monitor a process on a different note
would be useful we need some kind of
distributed consensus and master
elections so that something can connect
and control what's going on there you
know all these things there are
solutions for we want to reduce the
chattiness of those global functions
they are useful but we just need to make
them scale a bit more we need a publish
and subscribe capability I found it
surprising the multi castling in Erlang
is actually uni casting to each
recipient I use multi costing a lot in
my high scale system fact I've written a
multicast debased login system I'm just
trying to get permission to open source
that works really effectively on these
large systems so you know some of the
some of these things are around a SD are
laying there's some really good stuff
came there but the trouble is the
program's finished what we've got is
what we've got nobody's going to look
after it it's it's they managed to to
port it across to 17 5 but you know it
if we want to use that again you can
have to pick it up and integrate it
yourself there's no finder so it uses
multicast to discover nodes in a cluster
jeep rockso global register bash up tons
of good stuff with react or you can use
it it does have a really strong react
bias so it's a so it's all some other
things but it's very orientated towards
that use case there is so I come on to
the networking
or in a moment but one of the things
I've used a lot in these high scale
systems is remote direct memory access
helps get scalability and improve
latency and too there is actually a
driver for our lane now done by James
Lee so lots of solutions available but
it's it's oh yes and and when you talk
to people like whatsapp if I don't well
it's not actually just using these
components or choosing not to use a lot
of OT pista but they actually make
fundamental changes they change the io
scheduler they make changes the gen
server and they even make changes to the
beam so suddenly this starts to get a
very complex project every time they
want to move to a new release they've
got of port across all those changes and
I think it would make a lot more sense
if we combined our efforts here and had
a single platform which actually deep
which actually did a lot of these things
so let's let's why not like moving
forward them oh I think I think it's
trying to go to a link silly mate let's
let's let's pull that back again that's
bad okay so networks and scalability so
what some of the things we found with
with with these large environments was
scalability is virtually always a
limited by network latency rather than
bandwidth it's true almost whatever
distributed application your onion on
those what we've also found is that the
local networks so if you're if you stick
to a layer to local network
we've actually found them more reliable
than the computers and nowadays they're
actually faster than the computers so
you're now actually designing for a
different paradigm you're actually
designed for a parallel where the
computers can't keep up the network and
the network is more reliable and that
actually changes how you design some of
the things higher bandwidth also lowers
late see just the packets here
realization as you go from 100 megabit
to 1 gigabit you shave off a 80
microseconds every time a packet gets
serialized as you go from from from
mungiki to 10 gig as eight microseconds
it doesn't seem a lot but as the packet
flows through the network it probably
goes it goes through that 10 * so so so
it adds up end to end the picture there
is is a hundred gigabit per second Nick
you can go out and buy now took dual
port what we found and is that tcp/ip
though bottlenecks as you start to get
above 10 gigabit no matter how fast you
make that make your NIC it really limits
at about 15 gigabit plus in doing that
it probably burns an entire core to send
data and another to cause to receive
data so it's quite costly in terms of
CPU to dry that I and you know this is a
known problem the the big computer
clusters all rely on remote direct
direct memory access to to actually
drive these and to lower latency and
reduce CPU overhead in my area in in
financial services as well over the
world's largest Stock Exchange's
actually use RDMA so nasdaq handling
five and a half million messages a
second noisy does the london stock
exchange one on intimately familiar with
i designed it the item a network there
singapore us I think know why does as
well but I wasn't sure so I didn't put
that up I think it I think it is I they
might based at the that the matching
engine sir very that database vendors
have traditional database vendors they
they've all turn turn to it for their
performance numbers so oracle DB to a
sequel server the big cluster file
systems source and performance data for
Lustra the other day they were i had an
aggregate data rate into their cluster
file system of 1.6 terabytes a second
that's moving data around i'm always
made that people don't know that it's
been in the linux kernel since 2 6 14
people used to be worried that you're
having funny man now it runs over
ethernet runs over i walk runs over what
internal cooling omni path so very very
latest Nick's for a lot of traffic we
actually gang the two ports together so
you gang those two ports the Gary at 195
gigabits with it these are data rates of
application to application through our
delay so it's not Colonel this is
actually one application on one node
talking to application on nervous
because I'd you may transfers through
from user space there's no memory copy
does it direct from user space on my
machine to use this place on your
hardware pulling it out plugging it in
so that 610 nanoseconds so that's the
time from the first byte leaving one
node to arriving in user space on the
other node so under a micro second so
150 million messages a second important
thing as well is at the moment you have
to buy a nic card the next generation
process of a min tail sky like so every
time you buy two sockets server you will
have it in there it will be embedded in
the processor so I feel it's something
we ought to be using now a lot of people
ask this is M cash d
this benchmark result with them without
RDMA so the first of a black lines there
are tcp/ip running over so the running
of the same physical interconnect 56 gig
interconnect of the black is tcp/ip over
it the red is Ida Mae over it so it
really just shows that the increase in
throughput very able to achieve achieve
by doing that simplest view of IDM a is
you register a bit of memory and that's
lets them back to pin it to make sure it
doesn't page out it also established
some keys you give somebody else those
keys so they can read all right into
your memory that establishes the cue
pair once you've got that cute pair
established you can send a command to
read or write that memory block that
command can be up to two gigabytes of
memory and then you wait while it goes
and does it and it gives you a
completion event when it's all done so
it so it Maps really well to a messaging
system particularly an asynchronous
messaging system like we have in our
like this is James Lee's put together a
very basic idea madest driver for for
relying on that he saw a thirty-three
percent increase in message rate but
it's it's a very basic driver it
requires a lot of tuning they're really
uh what we need is something actually
embedded into the beam so you know
nowadays a local message is a copy
binary is greater than 64 bytes passed
around by reference TCP will transfer
those cross and it will encode those in
an external format what I'm suggesting
is wouldnt it be great if we if the
system was able to detect I've actually
got an RD my connection between these
nodes and I'll use that instead so that
that's the kind of thing we can do there
now
so it fits solutions to this problem
what I I'm thinking this is should we
collaborate on doing this or should we
leave it to individuals as we are now to
go and solve it themselves and my
concern is that if we don't do this the
scholar people probably will Microsoft
is pushing people towards the Orleans
active framework running on running
running on on a zoo they've already
hosting a halo 3 on there and you know
those communities are huge compared with
us so so we can be very smug that we
know how to do it ourselves but we run
the risk of becoming a little backwater
the the the the Java and the.net
communities what why they go and do it
themselves so I we've got a lot of that
it's the hard bits to do already done in
her naked OTP but it requires us to to
integrate some resolver components
that's a platform architect I'd like to
see an another platform which wraps
around her to be nobody wants to break
out EP you know we depend on that but
but having a another layer of components
and options which SAT around that which
which people could use I think it would
advance the whole Erlang community it
would make a lengthy the premiere
platform for for these new generation of
cloud apps over Kirsten is who should do
it should we leave it to the industrial
Erlang user group it'd be nice to think
as I could but I don't have as anybody
hear from them but the annual membership
fee of ten thousand dollars seems to be
a bit of a hindrance I think to getting
a broad membership would be my view on
that I think creating a
she project it's actually a good way
when you look at what Hadoop have done
with what was a relatively simple piece
of code created this huge ecosystem
around that now I think that that that
kind of thing would be a way to go at
this stage I'm it's not something I'm
proposing to do myself but I am
collecting interest when people come and
see me at the table afterwards I'm
collecting names from people who want to
be involved in this and maybe we see if
we can get something started and and
collaborate together on that because
otherwise we'll just got to sit here and
be smug and do it ourselves and
disappear into a backwater so any
questions on that yeah true when
questioned about the library you
motioned yep did you do look at the
cloudy from talk so the ones I've given
their it's not it's not an exclusive
list yeah that's part of part of the
trouble is you have to go out and
discover things yourself if they were in
a platform and you know you could see
them you could find them easier at the
moment you have to go out and search
things find things out you start to
right thing and you find somebody else
has already written something to do that
it's yeah so there's lots of other good
stuff out there you know most of the
people at this conference know how to do
this lots of them have already done it
but we're just doing it ourselves a rule
reinventing the same set of wheels
Christian over there
yeah yeah yeah yeah that's a good idea i
mean i mean who's who would be
interested in this yeah yep so there's
there's a there's a fair share of hands
of people are interested I found lots of
people who who who are generally
interested in using this it's like and
i'm not i'm not meaning to give the
answer here you know these were just
examples the RDA my stuff is just
something I'm very familiar with nobody
saying yes you've got to use that to to
to lose what we want is a set of options
in the same way as you have a vote EP
you don't have to use everything from my
OTP to use OTP you select which ones you
want but what you want is them to be
integrated together and to have
confidence that that you can you can use
that module and it will work and you
haven't got to hunt through through
through the entire get to try to find
which which components are there and
which ones which ones could could work
for you ok thank you and come to see me
up</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>