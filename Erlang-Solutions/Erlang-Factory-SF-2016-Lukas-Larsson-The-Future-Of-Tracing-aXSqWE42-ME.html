<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Erlang Factory SF 2016 - Lukas Larsson - The Future Of Tracing | Coder Coacher - Coaching Coders</title><meta content="Erlang Factory SF 2016 - Lukas Larsson - The Future Of Tracing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Erlang Factory SF 2016 - Lukas Larsson - The Future Of Tracing</b></h2><h5 class="post__date">2016-03-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/aXSqWE42-ME" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">last talk of the day see how this goes
so I'm going to talk about about tracing
and to what's happening in the batch
machine the things there were the status
of it today and what's gonna happen in
the future
my name is Lukas I've been here a couple
of times before speaking I work at
darling solutions I've been at the Lego
TPT meterics and as a consultant for the
last one oh five six years I lost count
for a long time now I work with the
virtual machine so doing things there
for the lasts what is it like three
months something like that I've been
working on doing changes with regards to
tracing and the support of the tracing
and about machines so by now I think
I've changed every single line of code
that has something to do with tracing in
our line there and we today I'm going to
talk a bit first about an overview our
tracing and C go through a bit about the
different things about tracing that you
might know know when some of the things
that you might not know so I'm going to
go through things that I'm going to talk
about more about what's happening in
Elango TP 19 the different changes that
I'm introducing in there for different
reasons and also the work is continuing
going on in Allen 19.1 line 1902 and 20
and so on and so forth and a bit about
what I want to do with tracing then so
let's talk a bit about tracing today and
what the different things and what's
bundling to the whole tracing framework
that we have in the virtual machine so
the tracing stuff I pulled this core of
a Wikipedia it's a specialized to use of
logging so you have a dynamic logging to
try to debug your application as its
failing in a live environment or in a
testing environment to get information
out of what's happening there in a line
tracing support is built into the
virtual machine and it has been from the
beginning as far as I know you can trace
on state changes of the processes you
can trace on message processing things
happening in a system you can trace on
function calls you can trace on a lot of
different things inside the virtual
machine and you can decide where you
want these traces to be sent you can
send them to a fire
- tcp - standard out and - a process
lots of different things the general
setup is something that at least for me
it just sits and I just do it it happens
automatically you start tracing with dbg
you say Oh or processors call and then
you say I want to do it on this sequence
and then you call this sequence and then
you follow the different steps you say
this sequence 1a and then you see that
it's cold and it crashes and you get now
you understand what's going on and then
this is of course something you can do
any live system as you're monitoring and
so on so that's the basic idea of it you
kind of instrument your code lively to
see so you don't have to do i of formats
everywhere and i used personal i useless
when developing all the time so i io
format things in the code it's not
something that i do we have another part
of the tracing framework that's the
trace receiver so the things that are
receiving trace messages so they kind of
decide what happens when a probe is
triggered so when some come when we've
decided that we want to generate a trace
message for a function call for instance
we have a different options and this is
when we're talking about what's in the
virtual machine today so we have two
options that we can work with either we
send a message to a process and that
process can decide what it wants to do
with it so one of the most standard ways
of doing it is that the process princess
to send it out so that you get a message
in there the process could also do some
kind of analysis to see like okay so for
instance the tool f probe users tracing
on cold traces in order to kind of
simulate the stack of the process so it
when it receives a message it says oh
now you're here and then it's caused and
it builds a simulated stack for each of
the different processes running in the
Allen virtual machine and you do
profiling events because of this so you
can have them doing lots of different
things you can also send a message to a
port but now you get a message that's
converted to a the Erlang external term
format sent to the port and then you can
write it to a file you can write it to
TCP socket you can do whatever you want
to do is you can write your own ports
that
follows a very simple protocol and you
can output it to whatever you want to
output it to to syslog to be trace to
system tap to it yeah whatever you want
to do built-in to runtime tools there's
two things than there's file and TCP but
it's possible for anybody to write
anything and this port interface is much
more efficient than the process
interface to work with as it doesn't
have to do the same things so the
different kinds of tracing events that
we're talking about is for a process
there's a lot of meta State that move
around that I find this quite useful
when debugging what's happening with
processes and there's also intermixed
with the different calls so you have a
guaranteed order of the trace messages
as you pass through a system where you
can reason about them afterwards so you
can see that logically that these things
are happening so if we execute this
small function up here and we start
tracing out when we receive the goal we
will have a couple of trace messages
we're starting with receive and then we
get scheduled in so we receive the
message we get scheduled in we call the
list sequence function then we return
from the list sequence function we get
scheduled out and then we get scheduled
in again at the time later the timeout
is triggered and then we exit the
process and we can track all of these
different events using different probes
inside of the virtual machine and all of
these can then be sent of course to a
process or a port depending on how you
configure it and this is you can
configure these trace events on a perp
on a emulator level and then you can say
which processes you want to trigger
these events so you don't have to
trigger them for all processes in the
system you can say that okay I only want
to have this subset of processes
triggering these kinds of events and
tracing in a different way so it's the
intersection of all of the traced
processes and then the traced modules
when it comes to call tracing that
generate these events meta tracing so
meta tracing is another feature that's
not very well known
it's I think the original name comes
from being able to extract metadata out
of the runtime system in parallel to
normal tracing not really sure about it
but I think that's it what it does is
that it introduces a second tracer into
your system that's global instead of
being processed Pacific that you can
trace function calls on so it sends you
register a per function tracer that you
know that all of the trace message all
the calls to that function will generate
the trace message to another trace
process I'll give an example so we can
just start gbg and we get the tracer and
you enable it using the beef a long beef
so you do a line trace pattern gens over
costs and then you do the meta tracer so
you add an option to the trace pattern
saying this is a meta trace we see that
it matches one function in the gents of
the cost and then without having to turn
on any tracing on that process we get a
trace message from that call when we
call that process of when we send it to
our certain that gen so the cost
ourselves we'll get a trace message so
this is kind of orthogonal to the normal
tracing where you have per process and
it's normally used for tooling in order
to understand what's happening the
system so it's kind of used in order to
yeah figure one of the use cases I know
is that you put a trace on register and
unregister so that you know which
processes have which names so that you
can resolve that in the tooling and so
on and there's other use cases for it as
well and I will show some of them later
on so match specifications is also part
of tracing so much specifications are
used in ETS and in tracing and in
tracing it's used to filter different
events now match specification is kind
of an acquired taste and you need to do
a lot of acquiring before you get a
taste for them I think I've gotten there
possibly I'm not sure I'll go back and
forth on it so so it's there for to in
order to discard events early or change
the behavior of the trace in somehow so
you want to generate some extra events
create some extra date attached some
extra data to
the trace message or something but
normally the use cases in order to
decide not to do a trace event or when
to do a trace when depending on
different scenarios in order to not
overflow the trace system so it looks
something like this
so you have these smiley faces looking
up here so you have the function
argument match so that's basically
saying when a function call happens
these are the things that I want to
check and it's a list of the arguments
so in this case we have a tuple in the
first that should be a 2-tuple and then
we have a unknown and then we bind
something to the third argument of the
function and we check that the length of
the list that is in that argument is
equal to one and if that is the case we
generate a return trace and if this does
not match then we don't generate the
trace message at all so we kind of
filter down and say that we're only
interested in having trace messages when
this condition occurs because that's
when we know that something is wrong God
cross and then there's an implicit
message true action that always happens
when you kind of generate these match
specifications there and that says that
it should generate a trace message you
can say message false and then it will
execute this math specification for its
side-effects
we'll get a bit into what the side
effects might be of a match
specifications so combining meta tracing
and much specifications in some clever
ways you can work with very with systems
that have a lot of load on them and get
snapshots that are quite accurate and
low overhead of what the system is doing
at that time
and so in example here so we set a trace
pattern on gents at a cost we only want
to generate a trace message when it is
ourselves that's calling it that could
be another pet but that's what you want
to check there and we want to modify the
tracing state of the process that calls
it so this is a kind of function in the
match specification so it says enable
tracing
and put on scent racing and use the
tracer tea to ask the tracer for this
process so it will put on the scent
racing for that process and and we use
it as a meta trace so whenever this
function is called with anybody that's
what it's going to execute and then we
add this extra logic in here saying that
ok so now we're going to enable tracing
when this function is called for this
specific process that we're executing
him and you can have quite complex guard
saying that ok so if the second argument
is a list with integers biloba and so on
in order to do it and then when we call
Jen server cost we get a trace message
for the gents of a cost and we get a
trace message for the action send to
ourselves as well without having to set
up the send part and then we can add if
you want to the message Falls part of
the meta trace pattern and then we don't
get a trace message for the meta trace
we only get it for the actual trace that
we were interested in so we get only the
trace of the sending message let's see
yeah so there's a couple of functions in
much specifications that you can read
about in the documentation that I use a
lot when I'm trying to do complex match
match specs and trying to do complex
tracing scenarios and one thing is the
trace control word which is very nice
it's a one word 32-bit integer thing
that you can read and write from match
specifications and you can also read and
write to it from a line code so you can
use this to kind of guide your much
specifications to say that now I want to
do a snapshot they're not the match
specification you set it to 1 and then
you do a tracing and then it the match
specification immediately sets it to 0
again and then you kind of do sampling
of what you're tracing with very little
overhead and then you use enable and
disable trace in different ways to
enable the different trace flags in the
processes so a complex example would be
something like this so you can get the
let's see if I can get there so you get
the trace control word and you compare
it to
one and if that's true then we set the
send flag on the process and we don't do
a message if it's not equal to zero we
send it to false so the first time I
spawn this process that does list
sequence here it's at zero so we don't
get any tracing on it and the second
time we call it we set the trace control
word and then we do the see tracing and
then we get a trace message for it and
then the next time you call it we won't
get it again
oh actually we will get a Gideon get it
again here as we didn't reset it but we
can do that as well so different
combinations of this you can do
different useful things so much
specifications and meta tracing so
there's another part of tracing called
sequence tracing that you use in order
to trace events as they pause by
messages between processes so you kind
of follow the events as they pass so you
say that you set a token on one process
and then that process token will be
passed on and incremented as it gets
passed to the next process and then to
the next process and it's cleared when
the process replies so you can do these
things and you can interact with them in
match specifications again and you can
interact with them in met our match
specifications so all of these things
work together it also works over
distribution so you can send reverb
distribution now in order to say that
okay I just want to see the consequences
of this specific function call happening
in my system
we also have dtrace system tap support
in the virtual machine there's been a
lot of talks about it so I won't go into
much of details we have tracing on the
artists events and of our line use of
space events so we have there's probes
for almost all of them normal use trace
events that you can do in event racing
there's 4d trade system tab and you can
filter them using D scripts or s tab
scripts and you can correlate them to
external events and so on very useful so
this is kind of the scope of the things
that I was looking at
in I don't know room when it was in
November when I started looking at this
and trying to see okay so what are the
most important things they wanted to
change and I had a discussion with lots
of people from the mailing list and
trying to figure out what exactly should
I be focusing on and there's also the
requirements coming from within Eric's
and about what the different things are
in there so I'm going to go through the
changes that I've done and the different
things there and then I'm gonna talk a
bit about what's left to do
unfortunately there's still things to do
work is never done
so the first thing that I've been
focusing a lot about is performance and
especially like performance of tracing
as you add more course and work with
larger data sets and more load of the
different things so one of the main the
this is the architecture kind of what
the tracing system looks internally in
the yeah language machine before
nineteen so you have a bunch of
processes that are joining trace events
they are be that all of the different
trace events are being put into a global
queue which is protected by mutex and
then they are consumed by a thread
that's called the system message
dispatcher that consumes all of these
different things and routes them to the
correct process and port now this may
seem terribly inefficient because we
kind of have to route everything there
why don't the processors send it all the
way to the process or port from the
beginning and indeed that's it the
reason for that is something that's
called the global lock order inside be a
language machine so I'm gonna try to
explain what that is briefly and see how
briefly it is so in order to avoid
deadlocks within the a language machine
we have what's called a global lock
order which means that there's a defined
way for every mutex to be taken within
the virtual machine so that we know that
we can only take locks that are of lower
priority than the ones we already have
and there's a whole mechanism of how
this is implemented and verified within
the virtual machine so the general idea
is that the port lock is the highest
priority and then you have the process
mein and then you have the proc link and
then you have the process messages and
then a process status and then there's a
bunch of Earth's internal stuff and this
is not a complete truth there are some
other locks intermingled in between
there in total there's about 90 locks
that you have to keep track of in
different ways and in between processes
and ports so you have a main lock of a
process one and you have a main lock of
process two there's also an internal
ordering for these ones so that if you
have a PID one you're only allowed to
take or actually if you have PID 2
you're only allowed to take locks of
paly 3 4 5 6 you're not allowed to take
the main lock of PID 1 so then you have
to release the lock and take the other
lock and then take the original lock and
that screws up all kinds of guarantees
about what data structures you are
protecting with the different locks and
the problem here was that message
passing so sending a message to a
process is protected by one of these
locks and when we're doing tracing we
need that lock to be protected so we
couldn't take the message lock of the
process that we wanted to send a message
to while still keeping the process
message lock of the sending process of
the process that was being traced
because then we might have end up in the
deadlock and this simple solution that
we did when just starting up with the
SMP support was to introduce this thread
with a very low priority on that lock so
that we could just place that lock in in
the queue and then that other thread
could take the locks of the processes
and everything was fine
let's see should come back soon
I hope
it's coming
Hey
so we had a problem there and that was
solving that and it worked fine for the
Senora staff who were using them and
it's kind of been pushed along actually
we've been doing releases but now it was
time to do something about this so in
nineteen now that I've removed the need
for the state to slock to be taken so we
only have to really care about the
process message queue lock in order to
order events so the ordering of the
events is very important and you want to
be able to see that the trace events
arrive in the order that the code was
called so that they're not reordered for
some reason and some of the code needed
this status look to in order to order
like things like the process exiting and
exit signals being sent to other
processes and getting that order
correctly with other events that are
happening in the system they needed to
have the status log taken but I I
removed those part of that and we were
only left with the message queue lock
and I need to have that message queue
lock taken in order to order the events
so I had to work around that somehow and
the way you work around having to the
lock order is by using something called
the try lock so you try to take a look
and if it's already taken you have to do
an a synchronous operation of some sort
with the tracing you have to deal with
it and we cannot spin because it could
be taken forever or we could actually be
dead locking ourselves because we when
calling these trace events we don't
always always know which locks we have
so we might be trying to take a look
that we already have in the same thread
which causes all kinds of havoc so we
have to do a try lock in that way so in
order to achieve this there has been in
introduction of a per process trace
message queue that's in completely
internal you can't expect inspect it
from the outside but it kind of it uses
a lower priority lock and when ever you
get it failed the try lock you will try
to you will take the
process lock and then you put it into
the queue instead and start accumulating
trace messages in that queue and that
lock has to be taken every time you
generate the trace measure but this is
much better because now we have a
distributed system and we're taking
locks all over the place rather than
having one place where we're taking one
lock and having all of the different
schedulers contending on one lock
so the general algorithm is something
like this we taste we take the proc
trace look we check if the queue is
empty or not empty if it's not empty
then we just add it to the Tracie's
trace queue immediately we don't have to
and then we just have to queue things
because we need to order the events so
that a function return does not come
before a function call or a send come
doesn't become before a send that you
know is after so you have to order all
of these different events and keep
causality in the events intact and then
you unlock all the locks if it is empty
you try lock on the remote message queue
and if it succeeds you just put it in
the remote processes normal so the
tracers message queue box and if it's
busy you put it in the queue and you
schedule a flush event so you schedule
the process that is the tracer to flush
all of its trace queues as soon as
possible and that kind of makes it so
that it will generate these traits
messages almost immediately and the
normal scenario for this kind of
contention to happen is when that
process is actually running already so
we scheduled it to say that okay when
you're scheduled out the first thing
you're going to do is you're going to
flush your trace message queue box so
now we end up with something that looks
like this instead for the architecture
so we're sending directly from the
processes to the processes to the with
the different traces or to the ports
which turns out to be quite performant
so some of the functional impacts that
you will see when you're using this
tracing because this is kind of it
breaks a global ordering that was there
before that we didn't really guarantee
but it was always there so trace
messages are more much more likely to
become out of order with normal messages
so what that means is that if you send a
message to the tracer process and then
you expect the message for the send
trace and the actual message is sent to
that process to be in a certain order
that's not guaranteed anymore and has
never been guaranteed but it's been very
very likely that it will happen like
that it's very unlikely in in before 19
that it will happen it will be much more
likely in 19 and afterwards time stamps
will or will most likely arrive out of
order as well so you will get the events
that are generated by time stamps and
they will in the time stamps arrive out
of order because we're queuing things in
the different processes and the time
stamp is taken when the message is
generated and then it gets queued and
then it gets delivered to the tracer
later on so the time stamps will not be
in order but the events will be
logically in order so you can't like so
events from the same process will be
internally consistent and when you're
communicating in between processes they
will be internally consistent so if you
have a send from one process and then
they receive on the other one the
receive trace message will be consistent
with the different parts there but it
will be noticeable these things of
course not if and the port stuff kind of
still works the same way as it does
before so the ordering things works the
same there is just when you're tracing
to a process that we have these issues
they go so I did some benchmarks on this
let's just go ahead
I ran 10 processors on my machine I did
1258 and one encodes that generates
somewhere in the region of 7 million
trace messages and you measure time I
measured time until the benchmark
completed and I message measured time
until all of the trace messages have
been delivered to the tracer and they
also measure
the total memory usage during this time
so that's a baseline this is just a
benchmark running and you read this 18.2
two three and green one is my private
branch and we can see that nineteen is
faster than eighteen even without my
changes good stuff so I didn't have to
change anything to make it faster which
is always nice
I think this has mostly to do with some
optimizations in the compiler that's
doing this so the ACE and one encode has
actually gone faster which is good so
call tracing to processes so if you do a
this thing and you send messages to
decode tracing over different schedulers
on the x-axis we get a speed-up of
somewhere in the region of I don't know
two times three times faster so the this
is for the overhead of the call so this
is when the process is running how long
time does it take to complete so the
overhead of tracing it's about that much
so that's neat if we add how long it
takes for trace messages to deliver to
the receiving process it's about the
same there so four processes when you're
sending messages to processes it's about
the same we still have about a 2/3 time
speed-up everything if we do arity
tracing on processor so if we do call
arity so we don't care about the
argument so we have much much smaller
payloads that's there are being sent we
get a better increase a much better
increase I'm not sure how much that is
but it seems about five six times better
so we had were handling small trace
messages much better than we did before
but the larger ones kind of the overhead
of copying the entire payload of the
message kind of overshadows everything
else in there if we trace to port now
ports apparently was very bad in in
eighteen so we have this linear curve
where it just goes up and up and up and
up and for the new code it's kind of
like just flattens out and Rises
slightly
and it's like 400 and follow me up so
it's a quite decent speed up for that as
well and cÃ³lera t small messages not as
big a difference here apparently D it's
better when you're tracing small
messages to a port than you do if you're
doing it to a process memory usage has
gone down quite a bit as well so in 18
we're using about I would say 7
gigabytes when calling to the process
with the process and then we fall down
to about 4 in 19 and what am i doing
arity tracing we're about four gigabytes
of memory usage when we're running the
trace the old one and then we're about
seven hundred fifty megabytes so
improvements all around except for poor
tracing which is the opposite so for
poor tracing we're having the memory
usage is kind of growing it's not
growing a lot so it's at about three
gigabytes or two gigabytes but in the
old implementation it was so slow that
it didn't wasn't possible to generate
enough data for it to actually use any
memory so you kind of end up with no
memory usage so it's a good thing if
you're in a low memory system but I mean
the overhead kind of ruins the usage of
tracing at all so it's said yeah not the
good thing so that was the the
performance scalability things so one of
the other things that I that's coming in
nineteen is something called trace and
modules so right now you can have it as
a trace receiver you can have a process
or you can have a port in nineteen
you're going to be able to have
callbacks in a module as the trace
receiver so that you don't have to copy
messages onto other stuff in order to
make decisions about what's in the
different messages or you can plug in
different backends to your different
things so tracer modules are there in
order for you to be able to do earlier
filtering to kind of complement or slash
replace much specifications as the
earlier filtering mechanism you're
supposed to be able to have pluggable
backends that are much
more efficient and routing everything
through a process or reports so that you
can do it syslog after TNG you can build
a sampling profiler on top of this or
lots of different things
it's very performant as you are cold
directly in the context of the original
process when you're doing that and
hopefully it will be able you will be
able to write a line code to handle this
tracer so you enable it with something
like this so for all new processes I
want to do send tracing and instead of
doing tracer PID as is the normal one
you do tracer module and then a state
the implementation is quite simple so
you have an enable filter first that
says that okay so which trace points do
I want to care about what should I do
with them so there's three things you
can return you can do trace you can do
disabled or you can do remove so if you
trace then the trace callback will
eventually be called not immediately
because the enable check is done a lot
earlier in the call stack then the the
other stuff cause we have to do a lot of
fiddling around before that and then the
trace callback is called and then you
have the event type and you have your
tracer which is the state which is can
be any custom state that you put in and
you have who is being traced the message
and the two in this case because it's a
centrist and then some options that have
been supplied by the tracer like if you
want timestamps or if you don't want a
timestamps and all of the port and
process tracers that are that I showed
with the benchmarks they are written
using these callbacks so the performance
is still there with this new API which
is kind of nice one of the bad things
about this with the knighting release
unfortunately is that they have to be
nips all of them so you have to
implement your line code as C code in
the first release in later releases some
of the events the most significantly
call will be able to be run in a line
code but things like internal state
changes of
these schedulers and things will most
likely always have to be native C code
that you do things with but just having
it for call tracing I think would be
great to do lots of different cool stuff
favorite it handles all trace events so
trace events and sequence trace runs
through these modules in nineteen there
will be a process and a port tracer and
there will be ad tray system tap l2 TNG
back-end for these tracers that will run
through them in there and that's one of
the major reasons that we did this is in
order to have LT TMG support for these
parts because we we do a lot of stuff
with that in CND at the moment let's see
after nineteen yet so hopefully you will
be able to do more of it in Allen code
and you don't have to program C anymore
we will most likely move system profile
system monitor and other kind of events
to that to this system as well we want
to have specialized backends for
specialized purposes so if you're want
to build some kind of my ideas to build
some kind of sampling profiler and feed
it back into Scott's a flame thingy to
make it the overhead less so that you
can figure out things with all the
goodness of sampling and badness of
sampling and that you get or
loadshedding so you automatically
realize that you want to do things or
send stuff over UDP lots of different
backends that can be efficient in
manipulating this data in different ways
we're also adding more much
specifications more ways of doing much
specifications let's see so one of the
things that we're doing is that we're
putting in the ability to do much
specification filtering on send so
whenever you send a message somewhere or
and generate that trace message you can
do tracing on it I'm not sure if it's
visible but this specific match
specification filters so that it only
generates traits messages if you do a
send to a remote node so it kind of
keeps it in there and figures out which
processes are communicating over the
distribution channel everywhere and
we're also doing it from receive the
same way so in order to figure out
different ways and filtering on send and
receive tracing in different ways and
this specific one is that it's only
generates trace messages when Jen cost
arrives in the mailbox in the future
depending on how successful the trace
module thinks is we will either add more
match specifications or make the trace
module thing more powerful we'll see
what happens there so that you can
filter on other things like state events
of processes and all the things you want
to there's also been added support to do
better introspection of ports so poor
tracing so there will be when you do
enable tracing if you do a line trace
new ports for instance you will trace on
all of the different ports and all of
the different things that they do so you
can do send and receive tracing on ports
and the ports events themselves so if I
start something up like this and then I
open a file we can inspect under the
hood what's happening with that file the
different messages that get sent to and
forth the port and all of the different
things happening there and we can see
that okay we're not doing i/o format to
that port that what actually happens is
that it sends a command with a 4 and the
hello world and then it gets reply back
with a data which is kind of the status
code of that thing and when I close it I
send a rough command and then I do a
port close and then we get to all the
different link events and so on so it's
kind of completing the entire tracing
things for everywhere so my vision of
what I wanted to do before I started out
this a study I'm always I always want to
do more stuff than I actually have time
with
in different places so I wanted to focus
on these were the feedbacks I got from
the community and the things that I
wanted to do myself so I wanted to focus
on flexibility in order to get multiple
tracers working earlier filtering
overload protection of tracing events in
different ways
wanted to make the documentation better
so do a more I don't know basic TBG
interface that you can work with better
describe the dbg TTB interaction and
also make match specifications less
scary when you're using them integrate
better with tools so alta TNG d'etre
systemtap are all two's doing the same
job basically possibly get into the
comment race format groups and saying
integrating with the Eclipse trace
viewer thing that it has to view events
and look at performance so the overhead
of the actual program that's running and
the scalability on course as it's
running through and the throughput of
trace events as it's going there so what
I've done so far is that the early
filtering the match specification
documentation the ltte MGD tray system
tough stuff and the overhead and
scalability is kind of there and it
works it's great I've built the
framework to fix the overload protection
the commentaries format and the
throughput business
so hopefully that would be kind of
either the community let's hope builds
these things so I will have to do them
myself eventually in order to build on
top of the tracer modules and the
different trace events that we have and
the things that are to be done yet is
the multiple tracers or trace sessions
or whatever you call it and that's at
the moment highest up on my to-do list
to do I don't think I will have time for
19 to do it unfortunately but it's it's
gonna come question is if it's it's
probably going to be 20 than if it's not
19 will seem and then a bit more
documentation work
needs to be done in order to finish and
stuff and hopefully that would be in
nineteen we're continuously working with
improving documentation in different
parts are ya think that was it for me
many more questions yes for so the
details about exactly what I'm gonna do
there I'm not sure yet but the of course
the way like the what is it red bug or
all of the different debugging tools is
that they kind of measure how much
message input you get and so on and the
part of the overload thing I guess is
just turning it off more efficient and
earlier and having built-in support into
the libraries instead of having to rely
on third-party app things to do it it's
the general idea so I'm gonna steal
ideas from other people and put them
into it to be okay thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>