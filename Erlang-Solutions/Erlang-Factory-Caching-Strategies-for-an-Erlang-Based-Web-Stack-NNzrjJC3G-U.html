<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Erlang Factory -- Caching Strategies for an Erlang Based Web Stack | Coder Coacher - Coaching Coders</title><meta content="Erlang Factory -- Caching Strategies for an Erlang Based Web Stack - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Erlang Factory -- Caching Strategies for an Erlang Based Web Stack</b></h2><h5 class="post__date">2014-03-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/NNzrjJC3G-U" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Wow okay Wow that's going to be hard so
about me I work as a software architect
for spil games spil games you might have
heard of them where company that is one
of the main game publishers we do a
casual online gaming and also social
gaming we have web portals with strong
domains all over the world like a game
calm or games games com those are a
couple of English websites my role
involves basically overseen the
landscape technical landscape of the
company and also coding on daily basis
and prioritizing tasks so that's what I
do what I won't be talking about today
is client-side caching caching proxies
and see the ends and the reason is first
I'm not an expert on that and second
that doesn't play a relevant role in the
performance of our websites so one of
the key things in the portals that I'm
going to present today or the
architecture behind them is that we
generate full HTML pages in the back end
which means there is no HTML caching
involved there is no serving a template
of HTML and then letting JavaScript run
to generate the dynamic contact we serve
fully back in generated pages
personalized for every user based on
information we get from data warehouse
even if you are a guest user and you
come to our portal after five minutes of
play we can actually provide you with
specific recommended list based on what
you have done and this kind of things
and all of that is back-end generated so
this is one of the portals as you can
see there are some recommendations and
links to games everywhere well that's
basically what we do and how we do
business is yeah the text is not very
readable but so basically we have a
catalog of games which is in that purple
bubble over there this one we have our
own studio providing games but we also
get actually we get most of the games
from external game developers and we
have an advertisement solution that
allows us to hooks different ad network
to our system we package a game together
with in-game advertisement in a bowling
we can serve that bowl from all of our
portals and we can also put in the app
stores and serve it to external
publishers etc so we have a rounded
packed solution that we just serve to
the outside and i'm not going to present
today the web framework that we used to
build the portals i did that at the
airline DC in december i also did that
earlier conference last year so today's
about what's behind the web behind the
framework all those AP is and all that
business logic that provides us with
those recommendation list for example
and that's actually a three layers
architecture so the first one is the
interface layer on top is responsible
for load regulation and type validation
so making sure every request you make to
our back-end is actually correct and
every piece of information that we serve
is compliant with the api's that we
offer the border layer is the storage
layer it's a distributed storage is also
air along all the layers that you see
there are purely Ireland I presented
that one last year here in San Francisco
and in the middle we have blocks
providing the business logic to actually
gather the daytime formatted etc so for
example you can imagine a request to
interface game on the top right of the
picture would involve actually checking
what users making the request and
retrieving a list of game IDs that are
especially relevant for that person and
then go into the service game on the
right of the middle layer to retrieve
all the information for those IDs and
then the interface on top aggregates all
that information and pass it up ensuring
it has the valid format so that's
basically how we work and what's
actually the role of caching all these
well first of all we cash for
performance that's kind of an issue one
so memory is cheaper than network I oh
and memories cheaper than database
accesses and maintaining pools of
connections and these kind of things so
that's kind of the first reason to cash
right because you want to go fast the
second one is
can act as a safety net for you and this
is especially interesting i will be
showing in the end of the talk a nice
story about how cashing actually made us
through the weekend when we were
actually having issues there are certain
considerations that you really need to
have in mind when you do Cashin so on
the left side you can see the horrible
waves of traffic that are actually
getting to your data center in your
systems on the right side you can see
your systems so caching sits in the
middle which means if you have any
depend on it the moment you're caching
is gone your systems will be destroyed
and you can think like yeah it's never
going to happen you know I will
implement a very solid cash in solution
and and it will be fine but it will not
there are many scenarios that can
actually cause that barrier to disappear
starting with just the deployment and
gold star cache misses no you cannot
really handle the load you're done you
can have a developer just flashing a
cache to see what happens you can have
an architecture based on seven levels of
cache and then one buggering compatible
change and then you have to start
flashing them and pray yeah so there are
many scenarios that are actually very
prejudicial so caching is great to
optimize caching is good but do it once
do it right and do not depend on it yeah
so basically what we're trying to do
here is caching only in one place as
closest to the data source as possible
to improve the performance and to avoid
issues with updates and gold stars and
these kind of things considerations when
choosing a caching system are diverse
one of the most interesting ones is
distribution versus replication so
depending on the size of my data set I
might desperately need to have a
distributed caching system that use some
consistent hashing algorithm and then
having some part of the key space in
some servers some other parts in other
servers and see how to go from there
maybe actually your data sets smoke
and you can just cash it in each and
every one of the nose which is better
because then if you lose a note all the
information is catching all the rest so
it really depends what your can do the
other thing that you have to consider is
how long will the validity of your
entries be and what kind of granularity
you need an eviction so do you really
need very fine-grained memory control
like what happens if you have a couple
of entries that are no longer valid by
are still used in space in your memory
well if they are tiny and you have
plenty of memory it's not really a
problem you shouldn't probably hurt your
performance by making sure they are
actually victim on the other hand if you
have like I don't know huge entries and
they are not in use you're just wasting
your memory so it's a balance the other
consideration this particularly
interesting we just start playing with
caching business logic so or cash in to
avoid spensive calculations in your
business logic is what to do with an
error meaning I'm cashing this
particular piece of logic that creates
us calculate something that's really
really very heavy like think about a
cubicle or quadratic loop very long
input and it generates an error so what
do you do you cash it well then all your
users are seen that error even after
your system has recovered right okay
fine so we don't cash it well what
happens if it's actually an overload
error particularly on a web stack right
so user sees the error you don't cash it
or remove the entry from the cash use
your heat set five multiply that by 200
million users that we have nice yeah so
that's exactly what you don't want to do
so you have to think about it yes you
probably want to cash here somehow but
you need to find a proper balance for
your business so today I have two
different proposals for you and there
are two open source libraries that we
want to release the community and both
of them are tested in real fire
air so I didn't mention the numbers
before but so we have 200 million unique
users a month and we also have like 120
gigabytes of data being stored in our
data warehouse systems on a daily basis
so it's big is real real traffic ok so
both solutions has been tested have been
tested and I have graphs to show you the
stuff they're different so the first one
is actually a combination between
memcache d and a library that we're open
sourcing that's called air memcache so
memcache d I don't know how many of you
actually have worked with memcache dear
sir but the is very famous as a nice
performance implementation of a cache
server it's actually the reason behind
lots of lamp stack lamp stack
applications being able to support real
traffic so it's actually the secret that
everybody knows is needed yeah it's good
if you don't want to reinvent the wheel
creating a complex caching server it's
high performance rating you see it
allows you to do distribute caching
easily because you can just put an oxy
in front and then just Moxie takes care
of the consistent hashing and
distributes the keys which is simple and
nice you don't have to do really any
magic at all and of course your system
administration's will love you because
they have been you working with this for
a long long time so nothing new to learn
no new technologies it's not the airline
guy coming up with crazy ideas again you
know this we know this we can manage so
there are scenarios for it how does this
work well again it relies memcache days
so this is not a caching server it's
only the client yeah it implements a
pool of TCP connections and each one of
those implements a memcache a binary
protocol this is interesting because
it's fast that's good that's the key so
it's fast and it allows you very fine
grain memory management that's something
that you can do very well with mem
caching is a it's actually heavily
tested and the most beautiful
functionality of this system in my
opinion is that the well of course it's
revisor node restart because the cache
server is out of the node so if you have
to do a deployment maybe
your cash is still valid you don't have
to flash it or anything like that but if
you're running the memcache d server
locally together with your airline node
using the library airline can actually
supervise your mcats d server and we
started if needed and that's actually
quite interesting I won't call it
rudimentary but basically the system
works by inferring the health of the
memcache d server by the health of the
pool that's connected to it so if you
have many connections to the pool that
are dying because they cannot reach mem
Cassie mm cassie is running locally on a
port it will actually try to restart it
it is that simple so you lose the
information but your pool gets
regenerated everything is healthy and
you can keep using your system and it's
actually done with a supervisor so it's
actually quite cool you can more or less
see how it works on the green balls our
supervisor two men one is the
application supervisor and the other one
is a pool supervisor so the ball on top
is the user interface module which is
called memcache you just have get set
etc as as usual and the supervisor below
holds the different pools of connections
that might be pointing out anywhere so
one of the post can be pointing to a
local port when you have memcache d
server running the other pool can
actually be pointed into a different
machine to a moxie distributor layer you
can actually use the library however you
want so this is one of the options the
other option whoa yeah before I go get
to the other option the other thing is I
said this was testing real fire so this
is this is actually how we do cash in
for the spill games storage system that
I presented last year which is kind of
an abstraction on top of the storage you
can below it you can use react or you
can use my sequel or postgres or or
Swift or whatever kind of database that
you want so the storage is storage
engine independent but it performs data
validation and the distribution is
already taking care that's yeah so it's
quite quite a nice storage system
but of course the key is you don't
always want to use to access disk for
every entry or to go to the database
that you are running behind for every
entry so this application automatically
uses Aaron memcache to keep the cash in
separated and being able to have a
fine-grained control and it allows you
to do optimistic operations at operating
memory first just by actually using this
library so do you have a real use case
and that's the URL of the repeal the
case you want to take a look and now the
other option is airmailed cash l cash is
a very simple Cashin application that
works like server of caching servers so
it's great for in node caching of small
data sets so I don't know if there's
anybody from klarna on the room but
given the numbers that cost is showed in
the previous talk yet this is not for
those guys really not for you don't do
that but this is actually nice for
caching outside of the note inside the
note when you have small data set it
allows you lots of concurrency and your
data can live there for quite a long
time the caching strategies are not to
be set by server like validity and
eviction and these kind of things you
can set the folds / caching server but
you can actually within the same caching
server specify different parameters
perky which gives you a lot of control
the application is Error aware so it
will apply different strategies when
dealing with errors when finding an
error when trying to catch a value and
it allows you to do non intrusive
memorization of your code so i can know
if you're familiar with the concept
something that's very popular in the
closure nowadays so basically
transparently cash a result of your
function for future reference so this
library allows you to do that in their
lab the way it works each one of the
caching servers is a gen server which
holds a protected ets table with read
concurrency enabled so it's optimized
for massive
gabs that's how it works each one of the
cache servers also keeps track of the
stats the stats of course are calculated
asynchronously otherwise you do the head
performance pretty badly but still stats
are good enough to get a very nice
overview of your system when you hook
into your monitoring it implements
periodic eviction so careful with this
think about it if you have long entries
or so if you want fine grain memory
control and you want enters to be
evicted as soon as they expire this is
not good yeah and it allows you to auto
refresh overdue entries so what are
those basically the idea is that there
are three status for each one of the
entries that you have the answer can be
valid which means okay have just
calculated it is there you will request
that you will get it back everything
everything works everybody's happy the
entry can be overdue which means well
according to the validity terms that you
specified this has already expired but
also you specify that you didn't want it
to be evicted until it was this old so
there is this gray area in the middle
that is overdue and during that overview
entries are automatically refreshed and
you can actually whenever you hit with a
get request an overdue entry you can
specify or rely on the defaults that you
have specified for your server whether
you will accept the overdue entry and
the refresh will happen out of line or
you want to wait until the entries
refresh and get the most recent value
which gives you again a lot of
flexibility the API is very simple so
you can start a cache server by
providing a name and a set of default
options you can stop it of course you
can also configure a list of cache
servers with default options in your
config file and the application will
manage to do that for you and then you
have a get a set and evict and again
stats so the API is really simple now
the trick is the piece of code that you
see below so the last three lines is a
very simple description of what you can
do with this so you have something that
is an operation receives an input and
performs some extra complicated
calculation that involves database
access and this reading and TCP
communication and whatever you want and
return some output and just by adding
that tiny little line on top saying i
want you to cash this in my caching
server s1 and i want you to use these
specific options for it the macro will
actually take care of trying to get the
entry for the cash from the cash and if
the answer is no they go and calculate
the value set it in the cash give it
back to you and you can specify that
okay I'll take the not found or I'll
take the value I'll wait until you store
it all of that is normally specifiable
by the options so the idea is / cast
server you can configure stuff like the
validity time and the eviction time so
if you specify validities five minutes
and eviction is one hour that gives you
55 minutes of overdue which means the
entry will be auto refresh if its heat
you can specify a refresh call back in
the case of the macro do so before
obviously the Refresh Kovac is
auto-generated and is a call to the same
function with the same parameters that
you're actually called which is quite
nice so if you basically place that
micron top of add x y equals x plus y
when you add 1 and 2 and you try to add
1 and 2 again you will receive the value
from the cash if you add 1 and 3 the
value will be calculated and then cashed
yeah you can specify whether you want to
wait for refresh or not so whether you
will accept the overdue value assist or
you what you want to wait for the
overdue valley to be fully refreshed and
then only get it when it's really valid
yeah you can specify whether the
set and a wingtip evict operations
should happen synchronously or
asynchronously think about the case when
you do a set and immediately after you
do I get on the same key should you
actually get what you have the set or do
you don't care about that because if you
don't care then do it asynchronously and
the loading system really decreases you
can specify what's an error for you so
the application by default would
consider errors anything that is in a
tuple Aracoma something or the atom
error and all the rest will be valid
values but you can specify it yourself
you can pass a callback and then the
application will try to validate every
value before cashing it checking whether
it is an error with that callback of
yours and of course you can specify a
specific error validity meaning that ok
will cache entries for five hours unless
they're errors and if there's I want you
to catch them for only three minutes and
errors will never be able to refresh
just to avoid again overload situations
if you have an error the error will
never be able to refresh so errors are
cast for a specific error validity and
then they are out of the cash and then
if you actually try to get the same
value again then it will try to retrieve
it and if the generation error will only
cash it for again the error validity so
the application is very flexible and
again a little bit on the structure is
kind of the same so the blue ball on top
is Earl cash which is the user interface
module which is the one offering the API
that I showed you before so start cash
stop cash get set evict and get stats it
does not use responsible for providing
you with the API functions and also
storing the defaults for all the
different cache servers that you might
have and perform all the parameter
validation and that leaves us with Erica
server module and each one of them says
implements the logic of caching so it's
assuming cling input because the input
has already been cleaned it's one of
them
the earth casts server holds the cash
which is the ETS table and when using
the interface you read straight from the
ETS table and when right into it right
since RC realized through the gender so
what do we use this library for we use
it to avoid in line expensive
calculations we have some complex
operations to retrieve relevant
information for users and sometimes they
can take a while and we used to protect
the application running the business
logic of course and you can find a
repository in there and also is open
source so I encourage you to try it and
give us your feedback whether you like
it or not so the idea behind this
project was actually that at some point
we found that in our code base they were
like 13 or 14 different implementations
of different subsets of this and I bet
you also have in your code base
something like that it's so simple you
implement a tiny Jane server you know
that holds some state and maybe a time
or two to to change the values or
whatever and yeah it's all over the
place and it's one common pattern that
actually fits the bill you just have to
configure it properly it's easy to
maintain you can test it properly for us
to have really great value so it's the
line visible can you see the line more
or less okay so in this case I'm going
to tell you a story about the gate
related labels call and that call is
responsible for getting labels that are
relevant for games based on the labels
of that game so you have an action game
give me labels that are more often
related to action like racing or
shooting yeah so that's an expensive
calculation we're talking about
thousands of games with ten labels 20
labels each game and you have to
actually cross all that data so it's not
really cheap and since we have a lot of
games added every day it's not really
good to pre calculate its kind of
complex
yeah so the logic is implemented once we
do not depend on any other department of
the company recalculation this is pure
development yeah so on the third of
februari we were really having an issue
with this so I mentioned the pages are
pre-generated on the back end and the
response time for this goal was around
100 milliseconds so that's pretty bad
that means before you can even try to
get the HTML for your page you already
have waited a hundred milliseconds you
don't have the HTML and of course you
haven't even send it to the client so
this is really bad so first thing we did
was okay jump into that go and fix it
that's horrible cannot be that complex
yeah so we did and then we got to the 60
milliseconds which was pretty bad so at
some point we decided we really need to
do something with it you know users were
experiencing some pain and we're trying
to figure out on the way as so I
mentioned before the spill game you used
to be a huge lamp stack as well and
we're moving all of that towards the air
long stack stack that I showed in the
beginning so when you go increasing the
traffic and now we have like eighty
percent of our traffic on that stock you
figure something you figure things up
basically so you can stress tests you
can do whatever you want all is very
nice all is very useful you learn a lot
but when you actually put it to real
users you figure out things that you had
never thought about so in the middle of
this process we figure out that 60
milliseconds was really really bad and
we need to do something with it so we
implemented Earl cash and that was
around the 7th of februari and then the
latency went down to below 20
milliseconds which was ok not great but
ok better clearly better and then around
the 18th 19th or februari we finally
found out cool parameters for the
eviction type and the validity time we
stand it the validity or actually reduce
the validity to 15 minutes or so and we
extended the overdue to 24 hours
and that was actually pretty good so
during 24 hours the data auto refreshes
except if there is an error and then it
gets kicked out and gets real calculated
again and the latency went down as you
can see to two milliseconds three four
it was actually pretty good coming from
100 in the meantime this service game
application was actually responsible for
running that code and this graph can
show you the memory usage so what
happened here this is a very funny story
so the seventh of februari we were
having 100 milliseconds response times
and the service game was consuming a
little bit more than a gigabyte of
memory and then memory started going up
and if you remember the salento februari
was actually when we introduced the cash
and that was a wetness day and we
started measuring okay is this helping
our users are things going better and
then we want to do the eighth the ninth
and the night was Friday so okay yeah it
seems to work should we clean it up well
friday evening should we should we now
we shouldn't leave it like that so
nobody touched it and a monday the
twelfth of februari we fixed eviction
which was obviously not working nothing
was being evicted so you can see 11
gigabytes of data being consumed and
then we fix eviction and the problem was
gone but that's not the coolest thing of
this graph the coolest thing on this
graph that if you see the memory before
using the cash was actually twice as
much as after using the cash because we
really had the service running on heavy
resources starvation so resources were
running out rank use were going up
complex algorithms were running all the
time process couldn't finish your
processing because they were stuck in
the rhine cues so memory was not being
freed and suddenly we started cashing
the data and wow all that thing went
away and the overall that we introduced
by caching really wasn't that bad and
also you can infer already that the
eviction is periodic because you can see
the waves on the line but you can even
see that veteran here so
so this shows two different lines the
blue one is the amount of entries that
are cash for that get related labels
call and you can see that it oscillates
depending on the time of the day but the
interesting thing to see is that every
five hours it drops and that's a
periodic eviction yeah and the rest of
the time it goes up and goes around the
2,000 entries should be the average or
something like that that's the amount of
information that we have cash but the
cool thing is the green line that
follows is a heat rate here which after
the optimizations that I showed you the
last couple of days when the latency
dropped down to the three milliseconds
oscillates between the 96.5 and the 98.5
of heat radio which is really amazing
and that's just because we found this is
again not an universal solution but this
was great for small data set with these
variants of the keys we have other calls
for example for which we couldn't
increase the validity for all the
overdue period for 24 hours because the
variance of the keys was huge so it was
kind of useless it was just using more
and more memory and the heating the heat
radio was not going up but for this
particular call it really fit the bill
so to sum it up a little bit lessons if
you need fine grain memory control and a
cache that survives restart and you want
to do this beauty caching maybe our
memcache is a library it can seriously
help you if on the other hand you have a
small data set and you want to use auto
refresh features and you want to use
this library to also cash business logic
so it needs error awareness Earl cash
yeah it is not nice coming from me I'm
one of the main contributors to that
repository but I think it's a very cool
library honestly and it could be very
performance wise and it could be better
in many other aspects but is really cool
is very simple and
as you saw this was a real crisis for us
and we'll solve with that tiny little
piece of code so in the process we have
used several things from other people so
use a bull boy for the worker pools in
the air memcache library and to be
honest the binary protocol
implementation from memcache is not ours
either I can't remember the name but to
say in the readme in the wrapper you can
actually find out the reference the
original author we have used lager for
login and what we want to contribute to
the community today are these three
rebels so our memcache earth cash and
the last one is actually a piece of
functionality that implements Python
like decorators for functions for
airline so you can write a macros on top
of the functions like I showed you with
a cash for the memorization so go ahead
take a look feel free to use it
particularly the third one if there is
any macro guru in the room it could
definitely use some cleanup yeah that
was it so if you have any questions now
it's a great time to shop yes
yes the question was the third libraries
using a Bart's transform the answer is
yes it's use it used parts
transformation to figure out the name
the name of the function that needs to
cash in the arguments and basically
hooking in runtime to know what should
be used to auto-generate the key for
caching and what the Refresh callback
should be yeah what else sorry come
again ok I miss sorry yes what ok so the
latencies when we have a cache miss
there we go so late into will we are as
a cache miss is a run for this
particular function is around 60
milliseconds but again this is one of
the most expensive calculations that we
do in life for a complex more complex
that we run it through our data
warehouse department who actually pre
calculates offline and populates a table
we just have to read so basically the
idea is if you wonder about the response
times for HTTP requests in the gaming
portals yeah i can tell you those are
actually better than when we used to
generate the page for in front end in
the old stack like way better and
something around yeah I don't want to
lie to you it's probably anywhere in the
world around the three seconds tops for
a very complex page serving from
Amsterdam
more
well thanks a lot oh wait wait wait
there's one more when ya em so the worst
thing that can happen is that they all
are doing it asynchronously yeah so
basically they call the function and the
value is not found and then the other
appears and while the calculation is
being run the value is still not found
for it so you're doing the calculation
twice but basically the last calculation
will prevail and override so if you set
the same key twice it doesn't really
matter so after you set it the second
time the validity is the valid is
starting from that set operation and ETS
tables yes so it's ETS tables with a
read concurrency setting this is also
one of the reasons why this is better
for small not only small data sets but
particularly small entries so I heard
stories about a couple of gigabytes
entries neediest tables in certain
companies yeah this is not good for that
because every time you do a read you're
actually copying to the process space so
this really bad but yeah from my
professional experience that's really
not a common case you did you shouldn't
do that
no okay thank you know</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>