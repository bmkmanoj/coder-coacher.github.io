<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Building a Scalable Real Time Bidding Exchange  - Philip Clarke - Erlang User Conference 2015 | Coder Coacher - Coaching Coders</title><meta content="Building a Scalable Real Time Bidding Exchange  - Philip Clarke - Erlang User Conference 2015 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Building a Scalable Real Time Bidding Exchange  - Philip Clarke - Erlang User Conference 2015</b></h2><h5 class="post__date">2015-07-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/4PDInGV7bNw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi everyone thanks for coming um we're
very excited to be here very excited to
be here they're all platforms and share
what we've been doing with real-time
bidding with Erlang first few words
about me um I started my career back in
95 on this system here so I was at
Ericsson working on the EDX e10 was a
system that had message-passing Atria's
system and hot code loading so it sounds
familiar to most of you the fact that
racism is so good on this here that in
my department
if you're writing bugs on the software
you couldn't submit your bug report
until you it included the solution which
had also hot loaded on to the system
just to test it things change a little
bit 2004 we moved on to a new platform
for 3d radio access it was a nice step
forward for developers they using C++
but for our testers it was a different
story and I can remember the day
assistant pastor came up to me and said
this thing is just a big PC now and what
he meant by that was if it crashed or if
there was a some sort of bug all he
could do was take log files send them to
a developer and hope that maybe one or
two days he will get a fix 2009 then I
got to do a course on distributed
systems the practical exercises for
course were done in airline I hadn't
done any functional programming before I
hadn't seen Erlang before so it was
quite interesting for me and at the end
of that week I was very frustrated with
all the commas or food stops that I
couldn't get my head around but I kept
coming back to it because it just looked
like an interesting paradigm and event
see yes I really started to enjoy
programming it so this led me then in
2012 I applied for a job at AOL to work
on an airline system and the airline
system was a real-time bidding exchange
I'll talk about so this is the office in
Dublin and as you can see the airline
developers get the best view there so be
very welcome to come and look
the agenda today then I'll talk about
what is real-time bidding we'll have a
look at the real-time bidding exchange
architecture architecture we're gonna
look at some bottlenecks that we find in
our system and we also look at Harley
traced and debug this system so first of
all what is real-time bidding we're all
familiar we go on to the internet and we
enjoy reading lots of different articles
and this is a web page from Engadget and
there's a nice article about Legos new
game that they're going to launch and we
get all this content for free but it
does cause somebody to put content up
there and we're all familiar that this
gets paid for with advertising
so in this publishers web page they have
some HTML insert it's called the
placement and the placement it's just
part of the page where an advertiser can
display a banner such as this ad from
Verizon traditionally there's been
upfront agreements between publishers
and advertisers and what that means is
the publisher knows advance which
advertisers can place ads on their
system with real-time bidding real-time
bidding is the buying selling off
advertisements in real time when a page
is actually loading in your browser so
publishers webpage will connect connect
to a supply-side platform that's where
the real-time bidding exchange sits it's
called SSP in this diagram the SSP
they're in contacts a number of DSPs are
demand-side platforms the advertisers
bid in through demand-side platforms if
they bid in and hopefully when our
real-time bidding auction and the prize
for running the auction is that they get
their advertisement displayed on the
user's web browser well look at some of
the components involved in doing
real-time bidding in our system so as
I've mentioned the user is loading up a
web page in their browser
I get that the HTML downloaded from the
web server and that includes I need CTB
request to our out server our out
servers then going to decide is this
publisher enabled for real-time bidding
or not and if that's the case a request
is made out to the RT be exchanged the R
to be exchanged then decides which DSPs
should be allowed to take part in his
auction so the art be exchange sends a
kress off the DSPs and those DSPs bid
into the system as I mentioned earlier
the winner is chosen the response gets
sent back to the ad server and
eventually a JavaScript snippet is
inserted into the user's web browser
that JavaScript snippet then triggers a
request off to an advertiser's content
delivery network and we get the address
he displayed back into the users web
browser so I'm going to take a look at
the architecture of the real-time
bidding exchange so as I've said there's
a connection between our ad server which
is C++ components on the RTB exchange
which is done an airline the ad server
will send thrift encoded messages over a
0 mq interface and they're handled by
this 0 mq process in the RTP exchange
for every request we receive we will
then spawn off an auction process so
we'll have at least hundreds of these
auctions ongoing concurrently in the
system at any one time the auction
process there needs to look up campaign
data associated that request campaign
data contains things such as floor
prices for the DSPs and the support it's
banner sizes which can be displayed in
the publishers web page it'll also have
things like whitelists and blacklists of
advertisers that's counter cannot be
displayed in the publishers web page so
all this campaign information gets
loaded up into ETS tables when the
system starts up and it's kept dr. D at
the whole time systems running
let's take a look at the architecture
for actually loading up configuration
data Campion information is stored as
JSON documents and those JSON documents
get up handed into tables in a my sequel
database the my sequel database is
actually a cluster so we replicate this
data for redundancy and also for load
balancing so as you see in this picture
the idea is that each real-time bidding
exchange should connect to my sequel
server in such a way that their load
balance equally across the cluster we
don't have any load balancer there so
what actually happens is we look up the
hostname for the my sequel cluster in
DNS and that hostname is mapping to
multiple IP addresses so all the RTP
experience has to do is decide which IP
address is it that it should use to do
that
we use a function called hash neom what
you can see in hash Neum is that it
takes the name of the node that we're
running on on a list of those IP
addresses for the my sequel hosts in
line two we convert that no name from an
atom into a list and then we just sum up
that list to get an integer value called
sum line three we calculate an index by
just taking the modulus of some divided
by the length of hosts and then when we
index into that list of my sequel hosts
using index we get the correct my sequel
host to connect to we could also do this
if we just took the IP address of the
host that we're running on but we can't
guarantee our are to be exchanged nodes
IP addresses will increment sequentially
but we do know the node names contain 0
0 1 for the first one 0 0 2 for the
second one and so forth
so back to the the workflow here once we
have taken the campaign information we
then contact a card base server to get
user mapping information so when we get
a request in from a user's web browser
contains a cookie for our dimin but when
we send requests our to demand partners
we need to include the cookies for their
demyans
so card space is doing this mapping
between our cookies on their cookies
when we have all our campaign
information on our cookie information we
then decide which demand partners can
take part in an auction for each demand
partner we will spawn a new process
called the DSP process the DSP process
is there in charge of building a request
to send to the external DSP using a
protocol that a DSP understands in most
cases DSP supports open RTB they might
supports different versions and then in
other cases there are DSPs that don't do
it support open RTB but use their own
bespoke protocol the message is sent art
with a HTTP request over a long-lived
HTTP connection to handle the
connections we have an application
called le2 tpc and it contains a pool of
sockets for each demand partner so when
a DSP wants to send a request to takes a
socket from that pool
it handles one request from that socket
and then when it's finished the socket
goes back into the pool having having a
pool of sockets like this means each DSP
gets guaranteed a certain amount of band
bandwidth if one DSP happens to be very
slow then we will eventually run out of
sockets in that pool and they won't get
as many requests until they can recover
from that problem that they might have
so as well as performing all the
auctions that are going on we have a web
interface running on this that gives us
configuration information and gives us
our stats in real time we are exporting
stats finesse and SNMP file we have a
server taking in those SNMP values
zena's which will alert on the system if
it needs to we also use folsom right
there and for very fine grained
statistics so folsom is collecting stats
on the airline VM and also collecting
very detailed timer information on each
different part of the system and
recording a lot more controversies also
and then on top of that we are taking
the creatives that we receive back from
the demand partners we're storing those
in a database and we're sending them off
for a third party for a third party to
scan for things like malware and
information about the advertiser and all
that information gets fed back into the
auction process so how much did a system
like this scale so typically running a
peak a system like this can handle
around 8,000 HTTP requests to our demand
partners so building and sending it
files in HTTP requests per second there
are over 1 million players in the system
so that's over 1 million different
placements where ads can be displayed on
the internet and in a day and globally
we would typically send out around 50
billion TSP requests and that's growing
at a quite a substantial rate so I think
when we look next month we'll see these
figures are probably a little bit out I
did so what I think we can see from this
architecture is that it is a highly
concurrent system we're kind of lucky
that all the requests coming in are very
independent and they don't depend on
they're not influenced by other
Qwest coming in it means our auction
processes are all very independent from
each other if anything goes wrong with
one auction process it can't affect any
other auction going on in the system and
then for the DSP processes they can all
run independently as well and can't
affect each other however when we do
have a concurrent system like this there
are of course shared resources and where
we have shared resources those shared
resources become bottlenecks so what I'm
going to look at is what are the
bottlenecks that we find in the system
when we went into production I should've
mentioned earlier that this product went
officially live in October 2013 which is
less than two years ago but it was
handling some production traffic I have
some months before that but it's winning
a very fast growth and yes we find some
issues and hitters that we weren't
expecting so the first bottleneck I'll
talk about is the zero and queue
interface so as I've said earlier the ad
server is connecting to this RTB
exchange over zero and queue interface
in reality there's lots of our servers
collected to connected to the are to be
exchanged and then similarly these are
to be exchange nodes are connected back
to all these add servers zrm queue with
is read or dealer pattern gives gives us
this flexibility here and we can take
out an RTP exchange node and zmq you and
the ad servers will load balance all
those requests among the remaining are
to be exchange nodes which are there and
this was working very well for us for
the first few months until we had our
first issue and when we hit the issue
look like this the top line in this
chart represents our requests and then
you can see it was kind of handling them
okay but then it just suddenly dropped
off close to zero for about one and a
half hours so if the system
crashed or restarted for us that would
have been okay or if it had handled if
it dropped our in the handle zero
requests that also be okay because we
get alerts in those instances what
actually happened us here was a drop
down and handled very very little
traffic and at the time we didn't have
an alert set up for that when we looked
at the system when I got into this did
we find tens of thousand messages all
queued up in the 0 mq processes mailbox
so you have to remember that this is one
single process with an active socket so
any requests coming in from the ad
server the request immediately puts a
message into the mailbox of the 0 mq
process since the 0rq process is also
sending responses back the mailbox is
full of both requests coming into the
system and also responses which had to
be sent back are to the ad server out on
top of that then when we send response
back there was a selective receive
statement so we have to block for a
little bit of time to do that and yes we
had other applications running on the CM
host because we thought this system is
so great it doesn't matter how many
applications we have running on the
server so that looks pretty bad and why
did we fix it well this happened fairly
early on in the days for us and actually
fixing it it wasn't the first issue we
could think of the first thing we could
think of is before we understood what
the problem was and you have to remember
we were C++ developers trying to develop
an airline system before we could really
understand the problem we had to make
sure that if it happened again that we
didn't have an artist like for like one
and a half hours so we created an e
script and this e script is doing RPC
calls checking the message queue length
of the 0 mq process at ran at intervals
and then if we saw that message queue
was growing growing growing the e script
called airline halt that shut down our
system and the heart process restore
so it'd give us a little bit of
breathing space that we could restart
the note if it got into trouble
eventually as we got more information
about what was going on
we'd hang up with different strategies
to fix it what we did for quite a long
time then was run the 0nq process at
high priority so I've heard it mentioned
that it's maybe not a good idea to
change priorities and your processes but
this is a process right on the edge of
our system and whenever a request came
in we had to change that message in the
mailbox into an airline process as soon
as possible because there long as you
know can handle thousands of processes
but if your system has thousands of
messages queued up it's just going to
suffer from really badly at and see what
we've done lately then was change are
active socket into a passive socket
we're polling now for this year we're
polling 0nq driver and I to see whenever
a request there this also gives some
back pressure we can apply to the ad
server because we're not taking those
requests are to the queue fast enough
this year and Q will reaches high
watermark unavenged the next improvement
we made was adding multiple 0 and Q
endpoints so now it's not a single
process that's running in the system and
with configuration we can increase or
decrease the number of processes running
there another good lesson we learnt then
was to pool resources so as I've talked
about we have lots of these different
auction process in the system all making
requests the card space the connection
the card space on all the requests were
done by a single process running our LMC
this again actually worked quite well
for a certain amount of time but we're
not ratio of the one or LMC process to
hundreds of the saw action processes
grew to a certain threshold and the
system again
had a lot of latency on it and the
message queue kept building up in the
LMC process so to fix this we created a
pool of those our LMC processes and now
we've changed the reissue two-for-one
LMC process we have one auction process
so there's no starvation of a single RMC
process trying to run on the CPU the
only thing new in this system here now
is that the auction process has to
choose our emcee worker and we wanted to
choose the LMC worker that's been idle
the longest possible time to do that we
basically want to round robin through
the LMC processes what we've done for
this is store a reference for the
process identifier in an ETS table so
this is a representation of that table I
will see for each RMC process with an
index value so if we know the index then
we can simply find out what LNC process
is there in order to do this round-robin
we need to know the pool size which is a
value and in this case and we also need
a value for index so if index value is
one we look up the table at index 1 and
we get process 0 100 0 and this is the
code that we use to look up the table
from the auction process it first gets a
pool size at line 2 and then it uses
update counter to get that index and
then it's just a lookup element to get
the process identifier for the LMC
process so the line 3 we use update
counter because as well as getting a
value for the index it's it's also
updating the value in the ETS table
atomically
so we're guaranteed that when auction
processes go to lookup index they're
always getting an incremental value
that's up the account or works great and
I've seen it mentioned in a lot of
different places there are some times
though that you don't need to run rather
than through your resources
so another solution we use as well is
airline P hash two airline P has two
Tech's
and airline term is the first argument
and an integer as a second and what it
does is hash that airline term into an
integer value in the range of 0 to the
pool size so because in this case it's a
TSP process calling out the process
identifier of self is different each
time it's called and we can get a round
on value of index to compare the two
updates counter is useful whenever you
want to round-robin through your
resources P hash too can be used if a
random selection is good enough find P
hash 2 to be about 1/2 to 2 times faster
and since P hash 2 doesn't have to write
her up the add an index value there
can't be any locking in the system as
well so tracing and debugging is another
good thing we want to exploit on the
system just recently we had a spike in
one of our timers and this timer is
called a bid collection time and it
represents the time that the auction
process will wit to get all the
responses back from all the DSP
processes so what we found was every 20
to 30 minutes we would get this massive
spike and it's by about 5 times the
value of what we would expect this timer
to be and it did happen when the system
wasn't really under any load we looked
at the code we couldn't really figure
out what was happening there either and
this wasn't a prolonged our tidge so we
couldn't log on to the system and do
some printouts from the shell and check
what was going on so in fact we're
talking about a few for a few
milliseconds we got one particular spike
it's the only only possible way we could
think actually to figure out what was
happening is use the tray system and
airline
so what we did was decide okay let's
turn on tracing for all new processes
being spawned in the system so we use
DBT tracer for this we give it a
callback function called trace handler
so a tracer process gets started up and
it's got to use trace handler for a call
back trace handler then get some skit
which is a tupple and in that tupple
we're getting at i/o which is reference
to a file we're gonna write information
out to it's given an empty dictionary
and this dictionary is gonna be useful
for storing information about processes
it's been given threshold threshold
represents the value of the timer so if
we reach threshold we definitely want to
have information log to IO and current
value what was just for safety so if we
were if Kris handler was called more
than current number of times and we just
wanted to shut down the tree system and
in case anything was going wrong so like
line through we got the process down
fire of the tracer and then a line 3 we
enabled tracing on function calls with
timestamps enabled as well so the next
thing we are to do after we turned on
tracing was decide is the process going
to use for an auction or not in our
system we know every process is gonna be
an auction is going to call the function
bit processor process request so we
turned on this particular trace pattern
if bid processor process requests are
called
it meant we wanted to dynamically enable
more detailed tracing so this is one of
the function classes clauses for trace
handler where bid processor process
request is called in line 2 we turn on
in more detail tracing so we again
enable tracing with timestamp we trace
messages being received into the
betrayus function calls we also trace
where in the process gets scheduled in
an arc by the scheduler for each of
these events that we have enabled
tracing on we have to add another
function Clause to trace handler and all
the function Clause is doing is similar
to line 3 there which is put a key value
into that dictionary where the key is
the the process ID and the value is the
time stamp and all the information about
the event that has just happened so
another thing we did then was put on a
trace pattern for the function which is
logging the the value of the timer and
if we find the value of the timer was
greater than threshold we decided yes
it's time to dump all that information
about that process into the file so
conclusions from this are that it's a
real-time bidding exchange is a nice
concurrent system but in our experience
we find the hardware that single
processes are bottlenecks for your
airline system the way we approach those
bottlenecks was to set either the
process to high priority which might not
be suitable in every case but also a
more common way to handle a bottlenecks
is to have process pools I've also seen
as well that the airline VM is a great
platform for debugging issues for the
future for us we hope to continue to
scale a system by with more publishers
on more demand partners we're continue
the middle build a greater Erlang team
in in Dublin and elsewhere and so far
it's been a great team to work with it's
been a lot of knowledge sharing by Daryl
on and people are really enthusiastic to
to build a knowledge of airline as they
work with it AOL in Ireland is trying to
support the airline community as much as
we can so we offer internships to
graduates
we have a graduate with with us at the
moment who is contributing to our
airline platform we also do some
presentations going around universities
and we present undergraduates who are
just interested in learning new
technologies in general and then in
Dublin and September we will host our
second Erlang factory out of a well
Ireland's headquarters in Dublin so if
any of you guys are available or want to
come you'd be very welcome so I've left
some time for some questions at the end
and thanks for your attention when you
came into this problem with the active
one process
did you try passive active once yeah
with with with the driver if active once
had been available we would have used it
so it's it's a it's not jammed TCP we're
using there it's a 0nq driver
so yeah active wants to be something
that's we would probably like to move
towards because having the having the
passive socket there we are we're using
a lot of CPU even when the system is not
too busy just polling for that socket so
it's it's probably something that's
still going to evolve that at some point
even more something like active wants
this would be good for us in what's the
cause of it sorry yeah so the cause of
timing spike well it was actually to do
with a very bespoke integration with a
particular demand partner we had what we
were suspicious about was was was there
something special about this demand
partner that we were getting requests
back for Iliad and we thought well it's
impossible to happen because we have
timeouts waiting for responses back so
we thought either the timeouts aren't
working or the
the airline schedule is maybe not
working here are there's just something
wrong with messes passing that is very
slow the actual cause was hello the
timer was called a bid collection time
the problem was actually sending their
requests not waiting to get a response
when are you're sending a request we had
to make a request for another jam server
so another bottleneck and it just so
happened that this gen server from time
to time had to make requests to DNS and
in fact request the DNS was ongoing at
the same time the gen server blocked and
this this was actually giving us a spike
so yeah from your original diagram about
where the request comes from in terms of
that's me pressing something on the
keyboard that requests a page what I'm
sure about and perhaps others are is why
does it go to AOL there's these are
these just using AOL as an ISP no start
to do with the ISP as such it's to do
with publishers who want to use an ad
serving platform so publishers could
could go to a number of different ad
serving platforms so probably familiar
with Google and companies like ads and
so if they've decided to go with AOL
then in in there in the mark-up you
receive back from their web browser the
each TP request goes to ad server rather
than anybody else's ad server so it's
just it's a business arrangement between
the publisher and their well and and
that's why the request is coming to us
yeah so of course everybody wants to
know that answer it's it's okay
I'll just be very generic here typically
to get an ad served on a publishers
browser we could be talking about less
and a
than a sense actually to get that
displayed now it can vary it can depend
of that publishers using real-time
bidding and the platform also supports a
private marketplace or the direct deals
between publishers and advertisers as
well which I haven't really talked about
here those can be valued a bit more but
but typically yeah the publisher is only
getting less and sent for the ad and
then they're giving us a tiny piece of
the tiny piece of Dagon so yeah the only
way for the system to possibly make
money is to do as many concurrent
requests as possible you could you could
scale it quite easily one request per
server but yeah you have to do thousands
of them really for it for it to start
paying yeah it's good question we have
so so our latency requirements come
between the ad server and on the RTP
exchange so the ad server doesn't want
to wait too long to get our response
back however our TV exchange has to send
requests over the Internet to the DSPs
so we give DSPs SLS and typically you're
on the the value and we're talking
hundreds of milliseconds rather than
tens or single digits of milliseconds so
it's it's 100 milliseconds plus kernel
latency agreements we have for the ad
server and the actual the actual latency
we would have the agency requirements we
have at the moment could change tomorrow
it just depends from the business side
of of of the organization to do they
want to decrease that latency or can
they live with a longer latency if if
they can we had a little bit longer to
get a nod displayed it probably means
it's going to be more demand partners
who got the opportunity to respond back
into an auction so there's a balance
between
waiting a long time and making sure you
get all the bids back as well so but
yeah we're talking hundreds of
milliseconds but it can depend on a
publisher by publisher basis as well
okay I've got one question from an air
long beginners point of view what's the
one tip he can give to people who are
beginning with massively scalable
concurrent system what is one thing I
should do and then what's one thing I
should avoid doing at all cost
hmm good one um I I think I we we
totally rely you're building a system
like this from scratch
actually scalability looks like a
problem and another problem is
requirements which keep coming into the
system from product organization and
which can continually change depending
on how the business is going so
test-driven development is actually my
my my advice to anybody who wants to
build a system like this and then when I
come as a scalability well put it put it
into your load tests and see how it
scales you can also you could always
change it so long as it's a testable
system that you developed please give it
up for Philip cart</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>