<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Christopher Meiklejohn, Caitie McCaffrey - A Brief History of Distributed Programming: RPC | Coder Coacher - Coaching Coders</title><meta content="Christopher Meiklejohn, Caitie McCaffrey - A Brief History of Distributed Programming: RPC - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Christopher Meiklejohn, Caitie McCaffrey - A Brief History of Distributed Programming: RPC</b></h2><h5 class="post__date">2016-11-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/aDWZyYHj2XM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">or something these are so I'm Kenny
McCaffrey I'm a distributed systems
engineer I'm here to provide the
industry perspective on all of this and
keep it a little practical this is where
you can find me on the internet I work
at Twitter now I've been building this
tree systems for a while please give me
a shout out if you have questions or
anything after the talk ok so just
before we dive into RPC we're gonna talk
I'm gonna make a quick distinction
between what is actually a distributed
programming language versus a concurrent
programming language because in industry
you sort of hear the two terms conflated
and I think there's a very important
distinction
so the concurrent programming is this
idea that like computation can make
progress without the one before it you
know finishing so we can do things
multiple times it doesn't necessarily
mean in parallel it just means like
things can advance without waiting for
everything before it to finish in the
sequential manner or not in a sequential
manner and the distributed programming
is this idea that we can accomplish
things via networked computers so
they're gonna go out across the internet
you're gonna do a bunch of stuff and
then like somehow you get a result back
from that you need concurrency in order
for distributed programming to be
practical because otherwise you're just
sitting there and blocky and waiting for
all these machines to do something and
that would be super inefficient but
they're actually fundamentally different
like concurrency primitives are
fundamental to like running multiple
things at once and then there's the idea
of distributed primitives for a
distributed programming language so to
make this a little more concrete we're
gonna go through some examples that are
currently being used in industry today
so no js' is very popular it's this
framework written in JavaScript it is a
concurrent programming language and even
though it is only single threaded
because it has this event loop and so
you do things and then when they have to
go and do something that's like somewhat
asynchronous you have a callback that
can happen so when you're making like a
network halt like it has a scheduler
that can invoke something else happening
so you have this interleaving of
requests even though everything is
single threaded but when you have two
like
make that Network call to your database
or to another node process there is no
primitive in the language to help you do
this another super popular language
right now is going it's actually being
talked about very often as a distributed
systems programming language but I don't
actually think it meets our criteria
because really it's a concurrency
language it has concurrency primitives
in these go routines which are basically
very lightweight threads and it has its
own scheduler so it can schedule things
and it can actually take advantage of
multiple cores on a machine and then it
uses channels to do which is based on
top of CSP and that's how these
processes can communicate with each
other this is basically a concurrency
language or has concurrency primitives
but I wouldn't call it a distributed
language so the core concept with a lot
of these and there's a bunch of things
that have concurrency primitives baked
into the language is how do you
communicate with code running on other
machines and all of these languages that
we've seen have basically left that as
an exercise for the reader and so that's
why I'm saying they're not meeting our
bar as a distributed programming
language okay it's a concurrent language
designed for highly concurrent
telecommunication systems that was
designed in the 80s and kind of and so
it uses the actor model for concurrency
and so you spawned a bunch of language
processes which which represent actors
in the system and then these actors will
exchange messages you know
asynchronously and what's important
about this model is that there's no
shared memory so status kind of
explicitly sent through messages and
kind of a process isolation and then you
have a scheduler that will run a bunch
of these in parallel and kind of do
pre-emptive scheduling for those
processes and so with notable about
Erlang is that they you know initially
designed as a concurrent language was
later extended for distribution and so
in that extension which we refer to as
distributed rolling you were able to
send process the same messages
processes that could be located on other
machines and so the system would
transparently take care of forwarding
that message to the other machine and
attempting kind of a best-effort thought
that message would get delivered and so
the process identifier is encoded a
location on the network where that
process was the living and so you didn't
have to think about sending messages
like manually marshalling and sending
data like you know opening a socket
right and so you know there's there's
been a bunch of languages that have been
extended with actor/model for
distribution so one notably is cloud
Haskell which is which is an extension
of mousehole using the semantics that
came from a paper that was written about
what Erlang distribution semantics were
some of these couldn't be directly
implemented because you know there's a
little bit of a type difference between
these two languages and so there were
some things that had to be slightly
changed but so this is similar and in
the similar way you can look at
languages like termite scheme which is
another example of a language at
distended with cool so now that we have
that distinction so that we understand
that we are what we're talking about
with distributed programming languages
is that we want these primitives that
help us talk to other computers in a
network we're gonna talk to something
that came out early in the distributed
programming language sort of research
space called RPC or the remote procedure
call so the basic idea and the
fundamental principle here is we want to
make remote calls or ones that go across
address space basically look just as
simple as programming local calls so
that that's the goal of our PC so today
we're gonna here's a quick path of where
we're going we're gonna go through all
of these sort of like our FCS and points
in time where our FC was define and
there were different critiques and then
we're gonna come to present and like
what's being happening in industry today
and then we'll talk about you know is
RPC a good paradigm or is this one that
we should continue or be happy with and
then we'll talk about some future
research in the distributed programming
languages space that is maybe
challenging this notion that RPC is the
way to do distributed programming okay
first paper or paper RFC I guess
requests for comments that we'll talk
about is from 1974 which is which is
actually a series of RFC's about what we
about the procedure called paradigm or
PCP and basically this this this is kind
of a important paper and the lineage
because it establishes the idea that on
the internet we have a bunch of machines
and we'd like these machines to be able
to share resources and so we have to
find a way to do this and so at this
point a they they focus around procedure
calls so that you would want to remotely
invoke a procedure call that would do
something and you know this would be
done this could be you know accessing a
shared resource or it could be kind of
sharing information with another node
that's kind of left open and so this is
this is important to look at here it was
defined across all seventy nodes of the
internet so this is like no DNS this is
host files like this is you know I am
I'm in the NSF and you're at Stanford
research and like maybe I need to get
some file to you somehow or we need to
collaborate on something but what's
important about this is that kind of
sets the stage for for this entire
lineage of papers that that we'll
discuss and so you know the first so RPC
is interesting because it's kind of what
I it's it's kind of like a 30-year
40-year rap battle between like
academics and Industry people or vice
versa and so this is kind of the first
of three papers in that in that epic rap
battle and and so this is RFC 684 which
which appears the following year and so
RC 684 is basically just a critique of
RFC's 674 and it it talks about a lot of
things and these things that you're
gonna see are repeated across all of
these critiques but I'm gonna highlight
three of them so the first one is the
acknowledgement that a local call versus
a remote call is going to have a
different cost profile and so the
realization here is that if I make a
local call this is going to be a context
which you know something is going
you know execute and then control flow
is going to return to the invoker remote
calls don't always work like this right
systems could be partitioned I can make
a call that system could disappear it
could send me a message but that message
might not be delivered because you know
I don't know networks are unreliable and
you know local calls usually have some
sort of fixed latency profile right and
you know if you think about you know
having clocks week approximately can
guess how long the computational step
will take and so remote calls because of
latency can take you know this arbitrary
amount of time we don't know and so
having a system where a local call a
remote call community here a peers local
you may program some application we're
like I'm just going to do this thing and
then I'll do something else I'll wait
and your program may never finish right
if you never get that message and so
this is kind of the second point that's
highlighted here is that remote calls
can be delayed they can never return and
we have a variety of kind of
impossibility results and in the in the
academic space that kind of show that
you know failure detection is really
hard and in partially synchronous
systems and asynchronous systems it's
it's impossible to solve and so you know
this is a challenge in trying to make
something that has a whole nother domain
of failure kind of a peer trend like you
know it's just local it's fine let's
think of it that way and so this paper
is kind of the first to propose that
asynchronous message passing or assuming
that you know I'm going to send some
message and later I may get some
response but I'm not going to wait for
that response immediately and send it
kind of a message rather than building
an abstraction on a procedure call where
well I assume I'll invoke this and
immediately get a response and then
continue programming in a sequential
manner that asynchronous message passing
is probably a better way of building
these systems and so we see this in
Erlanger Liang as an example of language
the under distribution has a synchronous
message vessel now if we move a little
bit forward to RFC 707
in 1976 RFC 707 says a couple
interesting things and I'll highlight so
around this time we have protocols like
Taliban that runs on 423 we have FTP
that runs on a different port and the
realization that this RFC makes is that
well all of these protocols are really
the same thing we can implement them all
by our PC right so it says well tell me
is a call and response protocol you you
write something and get some response
back FTP you know you say user get it
okay say password getting okay SMTP ie
hello okay right
all of these protocols have this call
and response and why can't those
messages those specific grammars for
each of these protocols why can't those
just be a function call
and so this proposes kind of a
generalization to say that well what we
can do is we can just make this grammar
a grammar of functions and then we can
implement all these things in terms of
RPC you know regardless of outside of
they I give that a lot of these
protocols we wanted standardization that
was the whole point of the RFC reasons
IETF right and so and so what you can
you can kind of see a little trace that
this historically because if you look at
systems that implement some RPC you'll
see like things like bar log and an RSA
it's a lot of these things they all use
the RPC facility to run in the language
they all use a port mapper but you know
they are kind of different protocols and
you can kind of see this is certainly
and so this paper while admitting this
it still critiqued control flow and says
well we still have a problem right call
response is bad because I may
you know if I go to perform some action
I have to wait for that to complete
before I perform the next action so it
limits the ability for me to do
operations concurrently
it makes sequential composition
challenging or concurrent composition
and finally you know it doesn't allow
priority servicing as well in case you
know I want to you know for instance
handle some requests earlier because of
some quality of service metric or
something like this right so it doesn't
support priority service thing it
inherently is like a request response
cycle all right and so 1984 this is like
the big paper in the field Braille
Nelson 84 this is work that you know
this is work that's been the culmination
of the cedar RPC system at Parc this
becomes later the Sun Microsystems
implementation of Sun RPC 1988 with RFC
10:57 and so this is kind of the paper
that is the standard design of what you
would want from from an RPC system here
but now like project management comes in
and there has been some change where now
it has to run across multiple machines
and so really cool no problem this was
written using RPC so we should be able
to do this seamlessly and distribute the
code across multiple nodes in a network
and it should work right but the problem
is is basically what they're saying is
like the chances of this working is like
not good because RBC fundamentally
ignores a lot of things about how the
remote how remote procedure calls look
when it talks to another node like there
could be failure that the network is
asynchronous and that you have partial
failure they're relying on the fact that
like we can't actually know when
failures happen we don't have a reliable
failure detector and all of these things
whenever you actually run this stuff in
the real world doing this and having at
work is like not gonna fly and so that's
sort of like if we had a correct RPC
system all the tests would still pass in
this mode so they point out this
fundamental flaw where they're basically
saying there is no protocol that can
perfectly guarantee that both sides of
the RPC both machines in the remote
procedure call definitively and
unambiguously know that the RPC is over
in the face of a lossy network so this
is harkening back to something like the
two generals problem right unsolvable in
an asynchronous Network this is because
we don't have like perfect failure
detectors this is the FTP result so
they're basically pointing out that this
is a problem of distributed systems and
RPC is not even attempted to solve it
and so like it can't be the correct
model going forward okay all right great
so I was that I worked in telecom I was
an
NFS server so I guess you could say I've
been fighting RPC my whole life but and
so on 1989 RC 1094 defines you know what
we refer to as a network file system
protocol specification implemented by
Sun Microsystems Sun had the apartment
on implementation so later but extended
limitation and so the idea of NFS is
that I should be able to remount a mount
of a remote file system and using the
existing file system API I should be
able to interact with it and so anybody
who's actually used NFS in the real
world knows that there's lots of times
where you'll like CD into a directory
and type LS and it hangs because you
can't talk to the other node right and
so this is a problem and and what this
problem comes down to is this this
dichotomy between soft mounting and hard
mounting that that is critiqued by Waldo
many years later in 1994 who was also at
Sun Microsystems and the challenge here
is that NFS had two modes so the soft
mounting mode said that there's a bunch
of failures that can happen when you are
actually working with a distributed file
system and so what we can do is expose a
series of new error codes that will
represent like oh you know there was a
time out or I couldn't do this or I
couldn't do that because of various
reasons now the problem with soft
mounting and the reason that never got
adoption is because no tools like for
instance an LS or something like this
touch or whatever none of these tools
would have compatibility for this API we
just wouldn't be able to handle these
new error codes coming from the file
system API so stock mounting doesn't
really get adoption but top mounting is
the idea that we'll treat the resources
remote but we won't be able to leverage
our existing application right all right
existing UNIX applications I've worked
with the file system API oh and so and
so that the opposite side of this is
hard mounting right and so hard mounting
says well no we won't add any
Americans will just block right we'll
make every call synchronous will block
and we'll wait for things to return and
so this is the reason you have seating
into your directory and typing LS and
then your terminal hangs and you have to
kill it you can't even control see out
of it because you're in some IO you know
blocking mode that you can't interrupt
and so and so this is the challenge here
right and so we see that the developers
as a developer I want to leverage
existing infrastructure existing tools I
want the hard mounting solution I want
the transparent a what the unified model
I don't want to deal with soft matte
because that actually I have to make
everything handle kind of remote cases
or some subset of it right and so you
know a few years passed and they have
this thing called core but how many
people here remember Corvo and so 1991
we have korva and the common object
request broker architecture back when
like acronyms like this were super cool
which was probably never but and so the
goal for BA is this idea that we want to
support calls cross that address space
so multiple machines know shared memory
we want to do this across different
languages of Java C++ C whatever and
it's gonna be for object-oriented
programming because you know it's 1990s
object-oriented programming is the thing
right and so kind of like microservices
is the thing now and so and so the idea
here is that with the first challenge we
have to deal with is this idea of the
interface definition language right so
if I'm gonna transfer an object like an
n64 from seed and you know if C++ to
Java I have to deal with like some sort
of like type conversion right I have to
know how to serialize this correctly
deserialize it correctly and so I have
to have a mapping between these types
across the machines cross languages
there's needs to be four objects in
primitive types and so this is this is
kind of a challenge and so actually
Stephen ASCII who usually comes to these
conferences but not anymore
wrote an article called it's just a
mapping problem and who actually wrote
the Corbett book so this is kind of a
great you know moment of changing your
mind I suppose but I and says that you
know the challenge here is that can we
map things that are remote to local can
we treat can we map these exceptions
when we map exception handling can we
map types and do all this stuff and so
the answer sort of ended up being know
even though korva I believe is still
going today and one of the major
articles that challenged this which is
probably one of the papers that has been
very influential to my own career is
this paper for 94 by Sun Microsystems
laboratories by a by Waldo and in his
group here called a note on distributed
computing right what a great title it's
just so I don't know so perfect and so
this this paper challenge is an offense
it challenges korva a challenges
everything and so at the time core was
remote method invocation basically the
same thing as our PC in many ways and so
it applied to objects and so this paper
this paper makes an observation that the
dream of CORBA and as stated by this
paper is that what we'll do is we're
using object-oriented programming so
we'll write an application and what
we'll do is the natural interface for
the application will fall out because
that's the promise of object-oriented
programming which we realized is it's
not right and so they say first the
natural interfaces will emerge from the
application domain because objects talk
an application boundaries it's nothing
to do with technical implementation
details another thing that we know is
not true and then they say well what you
do is you take that application and then
you just move the objects around the
network to find the best performance so
to think that we can apply you know an
np-hard optimization problem to figure
out the best distribution of objects on
a network that's rapidly changing an
asynchronous you can fail all the time
that's kind of a thing we know is also
not true and so kind of all the bases on
this application design is kind of
fundamentally flawed and the reason that
they want to do all this is because they
want to achieve what's referred to as
the unified model and so we see here
there's quote although I mean just drops
the mic right here and while
is the thesis of this note that the
unified view of objects is mistaken
straight up you're wrong and he says
this for a few reasons and so these are
new you should know this this is a
familiar theme this is paper three of
the new epic rap battle here and so we
can see like you know latency well we
know that latency is not going to be the
same we know that latency is a problem
we know that operations are going to
take a lot of time and we kind of need
this to be explicit to build a system
that performs well the second is memory
access so how do we deal what pointers
and references so if I have an object
and I want to send that I want to send
it to another object like through a
method invocation well what do I do well
I can serialize the object then
serialize the class of deserialize
around later side and build an
appropriate class on the other side but
then what if that object references
memory so then I can serialize all of
the objects and I can serialize all the
references and set them over the wire
but then what if I'm using relative
pointers well now I have to deserialize
it make sure that all the memory lines
up so that all the relative pointers
stay the same geez I mean this sounds
really hard to write so I mean maybe we
just copy everything implicitly right I
mean why not write in the Minette works
free and so and so finally kind of
elastic here is that partial failure I
mean I could send a message to somebody
and they might process it and send me a
response but maybe they died right
before they send me the response so do I
wait forever like does that method
called wait forever does a timeout if it
times out does it throw an exception
what's that exception right can i retry
it so that's an interesting question
that we'll come back to and so what do
identifies that there are two paths
forward since you treat everything as a
but as local you get your unified model
where you treat everything as remote and
this is or about just or works with some
exceptions and so to kind of wrap up
this you know this paper I think this oh
yeah I'm just gonna read this directly
because I think it's just so important
here and so clear and I couldn't say it
better this approach would defeat the
overall purpose of unifying the object
models who treat everything as remote
the real reason for attempting such a
unification is to make distributed
computing more like local computation
and thus make distributed computing
easier the second approach to unifying
the models makes local computing as
complex as distributed
so he's kind of challenges it's like
what did we get what did we do
everything is remote it's called clear
but did we really gain anything okay so
we left in the 90s and our the present
about you know more than 20 years later
so I'm gonna start with a little story
about some of my development history so
I used to be in videogames and I worked
on the Halo franchise for the Xbox and
the way that our Xbox is communicated
with our services I worked mostly on the
services side was they would send C++
trucks that happened to be ripped out of
memory with padding compiler padding in
big onion and they sent it over the wire
to a c-sharp server with a little endian
architecture and then we had to
deserialize it and write code to do that
so that was obviously awful and never
broke so you know like we'll just come
back to that story in a second but like
we you know there was a there was a gap
where basically our PC had kind of died
and like we were doing this crap also
because like there's only turned 46
Meg's of RAM on the Xbox me weren't
allowed to use it for the network cuz
the graphics but like you know whatever
that was the thing and then you see all
like a rest and everything had gone Oh
sort of away from our PC for a long time
so now we're in this world you have
micro services or like whatever buzzword
you want to call it now but there's a
bunch of things like there's a lot of
complexity to our applications today
there's we're all building distributed
systems today essentially and in
addition you know they all basically are
written in different languages because
reasons sometimes it's happiness
sometimes it's performance sometimes
it's like newfangled shiny thing but
this is actually like not crazy like
this is what a lot of like stacks look
like I would I would think today and so
because of this we've seen the
re-emergence of what we're gonna call
I'm gonna call our PC frameworks and
quotes and we'll get to that so one of
the big problems these things are now
trying to solve is that all of these are
generally running within you know when
you deploy software in a data center
making HTTP calls is expensive because
it strings and like doing all that it's
like doing the network negotiation is
expensive so we'd like a more compressed
framework we need to figure out how to
find and talk to all of these micro
services that are running and we need to
do this translation between different
language spaces right so we're gonna go
through a couple I'm gonna spend the
most time on finagle because I think a I
come from Twitter and
this is pretty core to how I build
services today and I obviously have the
most experience with it but there's a
lot that are very similar so finagle
came out and it was an RPC system for
the JVM it has an interface definition
language which is thrift and so what it
does is that helps us map between
different data types this is an idea
that came from korba it basically takes
the space of it punts on this idea of
like how do we deal with like memory
access and pointers by just copying
everything so it sort of punts in that
field it then also solves a problem of
service discovery so how do we which was
not addressed in a lot of what we went
through today cuz it wasn't as big of a
problem I think at the time when you
only have like 70 computers on the
internet but now like actually knowing
where this machine is running and
wanting to do continuous deployment that
thing's not static a static DNS is not
probably gonna fly if you want to bring
the machines up and down on and like
have tens to hundreds and scale
elastically and all those nice buzzwords
so we need some way to solve this so
finagle solve this by plugging it with
service discovery it does a lot of
really nice engineering to sort of
figure out if there are bad machines and
siphon off requests for them all of that
makes my life as a developer a lot
easier because I don't have to write
this for every single service we deploy
and they also solved or they applied
promises which was not a new area of
research is another area of distributed
systems programming that chris has a
great blog post on if you want to read
more in the history but they applied
this to like RPC so that you could solve
this control flow problem so that we
don't have to sit there and just like
wait we can actually fire off a bunch of
requests have it promise for the result
and then like execute on the promise and
compose them and do all of these nice
things so this is super nice this is
actually really useful this is a much
better world than having to like convert
C++ strokes in c-sharp in different
endianness this solves that problem for
me it also solves the problem of like
where do these things live it doesn't
solve all of the problems right I would
say that like you know as a developer
when I'm writing an IDL already a
finagle service I'm not actually like
writing finagle as like every functional
interaction I know that when I like have
a finagle request and I'm making them
it's going across the wire so implicitly
like we have now decoupled sort of this
model of what I program
locally from what is remote it looks
very different right it also like will
give you like things like timeouts and
retries and all of that you've just
specify on your own
and so like another good story sort of
from my career to highlight this as I
was working on observability at Twitter
and we had a problem where our service
grew by an order of magnitude and so all
of your performance calculations and
things are now wrong and so we basically
we're doing a lot of work on the
services to make it more robust and
we're reliable and one of the things we
notice is that whenever our data store
happened - hiccup briefly or like go
even close to its SLA and start feeling
a few requests we'd have this enormous
backup in the Metro congestion service
and you would sort of see the JVM get
really sad because there's all this
memory now and they would sort of die
and it was it was bad times you get
paged and I didn't like that so what we
did is we wanted we spent a good like
about a month tuning the retry policy
and actually reducing the consistency
level because we didn't need a strong a
consistency guarantee as we were using
and this actually made it much smoother
so that the network could hiccup or like
our data store could hiccup and it
wouldn't cause a huge problem and in
fact our service but this was like a
month worth of my time basically
retuning something that in the in the
theory of like what we tried to set out
for our PC it should have been solved so
finagle does not solve a unified model G
RPC is the other super popular one right
now it comes from Google it uses an IDL
which is protobufs it has more language
support because it's not just based on
the JVM and it also supports
bi-directional streaming which is pretty
different and it also I believe has a
pluggable infrastructure for plugging
and service discovery and how you know
where machines live but they're pretty
similar it's just like choose your
flavor of like what language and
environment you like I'm in worked with
this extensively so if there's something
that I totally missed sorry but you know
what if this all comes down to and what
I'm trying to get at is that martin RPC
frameworks don't provide a unified model
and they never tried to so are they
really like our pc frameworks in the
sense of what we started with you know
you're going over the wire and so this
gets back to this fundamental quote from
a note on distributed computing where
waldo says the hard problem and
distributed system eating are not the
problem of getting things on and off the
wire and I would like to bother these
modern RPC frameworks are only solving
the problem
things on and off the wire not that this
is bad or I would like to not live in a
world without this right let me go back
to that world where I have to serialize
all this this is an excellent piece of
engineering and a fundamental building
block of how we do communication but
that's the problem that is solving for
us is getting things on and off the wire
it isn't solving the problem of like
what if it fails is this request
idempotent can i retry it how long
should I wait for it to fail right like
what do I do it's not so made any of
those problems for me as a developer
still have to understand all of this and
it's definitely not even trying to
provide a unified model so it's just a
it's just a building block to make you
know an API or a library to make my life
better so I don't have to like open
sockets and do all that on my own so
realistically right now we have a bunch
of concurrent programming languages and
a bunch of RPC frameworks and we're
gluing them together and we're making
distributed systems and life kind of
sucks so I would like to posit that
maybe we need something better and to
close us out the idea is here is like
the point of RPC was to make remote
calls just as simple as local calls and
if we treated everything as remote then
we haven't really simplified distributed
computation at all which is why you
don't see G RPC and finagle trying to
like unify the model it doesn't make it
any simpler it actually makes life a lot
harder so that's why they stay at the
boundaries and then if we can't treat
all calls as local if this premise is
fundamentally wrong is the procedure
call actually the right model that we
want to keep going with in the future
for distributed computation do we need
something else and what might that be
talk about I'm going to briefly talk
about three things now focus on the last
one a little bit more because it's a
little bit closer to practicality at
this point but and so the challenge
the challenges that Katie highlighted
with with finagle is that if we if we
need to be able to retry operations we
need operations that can be safe to be
safely be retried right we need
idempotent operations if we we have
fewer Network guarantees we need
operations that are commutative and so
there's been there's been a bunch of
work in the area I I have some work that
I've been doing as part of my PhD called
on a programming model called glass that
only allows you to write programs that
are safe where operations can be
arbitrarily reordered rematched or
replayed so you lose a lot of expressive
power in your language you can do fewer
things but all of the things that you'll
do are are basically safe by
constructing correct by construction as
we call it and so that's kind of one
area that's being researched now
somewhat related to that is the work on
bloom by by Joe hello science group at
UC Berkeley and now Peter Alvarez group
that at UC Santa Cruz and what blumen
and subsequent version bloom L try to do
is build languages that minimize
coordination so what they try to do is
say well this operation is commutative
and this is idempotent and so we can do
these operations without any
coordination at all these languages
allow you to express more because you
can express things that aren't that
don't have those properties and then
what the language will attempt to do is
is draw draw a nice little graph that
shows you here here you need su keeper
for instance because this program will
not be safe under distribution without
an order or something like this and so
Peters approach has the ability so it
took contrast these approaches the last
book approach makes is is reduced
expressive reduced expressivity
but everything's correct by construction
where Peters approach is making an
arbitrary program and then having an
analysis tool to tell you where you can
weaken things and so these are kind of
complementary approaches the final piece
of work that I'll talk about is work by
a friend of mine Heather Miller she
spoke here last year but she spoke
directly about this topic possibly and
she spoke about her work on a library
called spores which is around cereal
closures and so Heather's work is is
kind of a she likes to call it a dual to
the actor model so a dual to something
like a relay and the idea here is that
rather than have actors or processes
send it use asynchronous message passing
to move data around the network what
you'll do is you'll move functions
around the network instead there's
challenges around this as we solve it
something like Corbin right and so you
can't get a closure that closes over
some global memory reference or
reference to a global object in the
global namespace singleton object in the
global namespace there's challenges and
so what you do is you use a type system
such as the type system that Scala
provides to ensure that everything that
you attempt to capture with the closure
can be safely serialized and then you
know that that closure can be
arbitrarily sent to any node on the
network and executes fine and so to kind
of see where something like spores is is
apply it could be applied practically
and was being researching in the context
of we can consider a system like spark
and spark as a system for doing
large-scale data analytics it's kind of
the natural successor of Hadoop because
it actually addresses failure and
performance and so the idea where that
is that you perform function
compositions so I have some data set and
I say I'm gonna map it filter it fold it
and then what will happen is when you go
to read that value it'll be realized by
moving the functions around computing a
new piece of data that represents the
result of this computation so data in
this model is immutable persistent data
structures and so this is kind of an
alternative way that's trying to say can
we can we take a step back and look at
how we're trying to build distributed
computing systems and maybe look at
different paths and so all three of the
work I demonstrated kind of try to
address those problems cool so that's
our talk we hope you learned a bunch
about RPC and maybe challenge your
notion that this is the one and true way
going forward and hopefully gave you
some hope at the end so you're not just
stuck all of the resources for in the
papers and things mentioned for this
talk including slide links are on my
github repo so feel free to check that
out also tweet it out after the talk if
you can't read
and thank you and we'll take questions
now
thank you for the talk
do you have any examples of people using
any of the three sort of future
technologies right now if that work
actually made it into art but it was
being researched jointly by EPFL and
data bricks there was an intention to
put it in there I don't know if it made
it but very much that model can be used
in that context it was designed
specifically for that I don't know if
somebody's adopted a research library
because research code is usually
terrible in many ways in terms of the
bloom work the analysis code that bloom
uses to identify those coordination
points that analysis has been used in
another piece of work they're not about
programs but about system composition in
a system called lineage driven fault
injection which actually is being talked
about somebody at this conference is
talking about it I don't know if that
happened already or not but Shawn
keyattr was talking about it and so the
analysis could have been repurposed is
it's a general logic for identifying
places that are unsafe without ordering
and so you can use this in other
contexts and then my work on last phase
is built on top of conflict replicating
data types which Martin talks about in
this talk earlier today and there was a
workshop yesterday on it and so those
CRT teas have been tremendously
successful I guess for for you know for
research that's fairly new in industry
so it's used by bet365 and it's used by
Bachelor react and it's used by Rovio
and riot games and League of Legends so
the data structure is that forgiving
properties are used independently but as
of now the research that we're doing is
purely a research prototype that does
not have yeah I think it's also worth
mentioning like we didn't put a sink
message-passing on this slide but I also
still thing and you can contradict me if
you disagree but a sickness is passing
is probably a better thing release is
interesting because it it sort of merges
well this is an actor framework I used
to work on it when I was at halo but it
actually emerges this idea of the actor
framework an ethnic message aid with a
little bit of our PC because it does
have this idea to do a call-and-response
or you can just do a fire-and-forget
message so to simplify some of the
program model but you still very much
know you're making an asynchronous call
that may go over in the network can I
ask a question since I have mic and
everything and so what are your thoughts
on AWS lamda and how does it fit on what
you were talking about today so I
haven't used it AWS land how do we think
of what do we think about AWS lamda and
in serverless I guess it may be the
server getting at yeah yeah I'll give
you a viper started from industry I
think it's really good that we're
simplifying the operational cost of
having to do and deploy something like
iterating really fast is like what I
want right like this is where I'm saying
like finagle and G RPC are like
foundational building blocks I do not
want those to go away they make my life
easier until we get something better and
lambda sort of like that lambda doesn't
address anything with distributed
computation though I actually have a
tweet conversation where I was talking
people at the server less calm because I
didn't realize it was getting P tweeted
on the stage but it was being it was
basically this idea this idea I was like
what do you do because I just heard of
it I was like what is this
and I'm like what do you what do you do
with your state right like what how do
you serve your state because they
basically had this slide that was like
distributed systems or solves it was
like that sounds like magic and ponies
and I'm all about those things and they
were just like will you store it and you
know you stored enough three restore net
and int it and with dynamo I'm just like
why am i blanket like listening every
other key value store in my mind and
that's hella disturbing system right you
can make
call that could fail or not and so you
still have to understand some of it so I
think you know it's making its I think
lambda is great because it reduces the
operational burden that it takes to
deploy and run code I do not think it
solves anything with distributed systems
stateless infrastructure with AWS lambda
and DynamoDB and so how is it working
they said well you're done lambda gets
an input from the database and then
writes an alpha to the database and then
the next input is read from the database
and render the out they've written to
the database I'm like god that sounds
like the state monad it sounds very fun
yeah but I lambda is challenging because
I think that once you start putting all
of your data everywhere now consistency
is a huge problem and there's only been
about like three researchers I think
probably historically have ever
considered consistency in programming
languages but as we start putting stuff
everywhere I think that's a problem that
we really need to start considering so
hopefully they'll be a revival of that
thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>