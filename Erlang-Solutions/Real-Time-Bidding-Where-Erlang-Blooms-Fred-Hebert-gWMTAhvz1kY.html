<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Real Time Bidding - Where Erlang Blooms: Fred Hebert | Coder Coacher - Coaching Coders</title><meta content="Real Time Bidding - Where Erlang Blooms: Fred Hebert - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Real Time Bidding - Where Erlang Blooms: Fred Hebert</b></h2><h5 class="post__date">2012-06-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/gWMTAhvz1kY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right so yeah who am i I'm Freddy
Bert working on learning some airline
has James I've told you i'm currently
working from bloom digital platforms
doing real-time bidding which is
something i'm going to explain pretty
much right away I've been working before
that for one year doing training for
Erlang solutions and I've done a few
open source libraries like what it is
it's a hairline history to a history to
the airline chill and pretty obviously
the guy in that animation is not me so
when we first started on internet doing
advertisement we had something called
display advertising in fact it's still
there is the traditional way to do
things and usually the ads are being
pushed on the server by a single kind of
resource so when you have your own
website one to you advertisement your
Colleen a server a guy resource who just
has a bunch of clients and
advertisements they display it for you
on a website i used to work for we had
Microsoft approached us and it just told
us you change your background to the
windows vista background and we're going
to pay you 80 thousand dollars a week so
that was one way of doing things it just
deal with contracts you have different
people or it's some kind of automated
process so usually if the ads are not
annoying or just a pain in the ass of
the users you're able to get actually
meaningful clicks meaningful conversions
for that and everybody wins oftentimes
it doesn't work like that the ads are
not always relevant so they're get
something called a real time bidding
right real-time bidding is kind of an
evolution based on that so instead of
adding a single resource on a single
site just pushing all kinds of ads it
think are good for it you instead deal
with some kind of major network so it
might be a Google Network appnexus
Castle you've got a bunch of them and
what they do is that they're tracking
users and somewhere for you in what
usually I thought was Carrie but it's
not really scared because completely
anonymous and what that does is that
that gives an uneven sense though to a
bunch of different advertisers that can
tenth right to give you the most
relevant advertisements possible and
that usually is a good thing because if
the advertisements are more relevant to
you well you're more likely to find
products you like if you just have
advertisement that you don't care about
their just a waste of
with most of the time right so this has
the potential for better targeting in
general so to give a more practical
example let's say I'm a car dealer and I
have my own website and usually what I
will want to do is contact a company
like say google to display ads on my
website so this here will be your google
guy or whatever Network we're dealing
with so any time a user makes a query on
my page there's a snippet of code or
HTML code JavaScript or whatever and
that one just sends a request to the the
person taking care of sending
advertising for me so what that does
then just sense the where's the country
oh yeah since the advertisement back in
my page and with that what do you do is
that they usually drop a cookie with it
and that cookie will tell them what kind
of advertisement they've displayed to me
so they don't prepare too often that
kind of stuff right so this is the basic
thing that we see as a user browsing a
website so yeah I like money I wanted or
text me display them on my side and
that's about it so in practice it's a
bit more complex than that when you deal
with a nice change so the first step is
still pretty much the same right I've
got my website right then I sign my
request but instead of just returning
the advertisement straight to me what
happens is usually following thing the
exchange is going to forward the
advertisement DD offer to a bunch of
different bitters and a gia trader which
is the one we're developing at loon
digital is one of these and what the
trigger does is that it value AIT's a
request knowing that maybe that user has
a given H is currently visiting a car
website and we can decide well this is
someone I really really want to display
an advertisement to and bid more money
in it or bit less money if it's
something we don't feel is important or
all ghetto or algorithms feel is not
important so once we know what we want
to do with that request we reply back to
it and then the exchange Network what
you're going to do is just put a snippet
of code in the page and that's nibud of
code will also set their own cookie and
that snippet of code then is calling our
server in that way and then we're going
to display our advertisement on the page
and put our own cookie on there so that
way we're able to match different
users even if we're switching or working
with different exchanges that kind of
stuff so now let's say that the car
dealer here that we've got on a tough
left is actually deciding to deal with
advertisement outside of the song side
so now they have advertisements on their
outside but they want to advertise
themselves to other people so there's a
different let's a different car dealer
website right this one is for used car
and something it's my competition as the
car dealer on top left so when the user
goes to that website they it will
usually carry its some cookies with him
so now what we do is that when the
request goes to the exchange to display
an advertisement we have an idea that
this advertisement was actually
something somebody that we had in our
system already and we can know by the
history of that that that guy probably
just went browsing cars in our own site
so that guy's asked a lot of value to us
so what we do is that we're going to bid
a bit more and then all the cookies are
going to be refreshed on the client side
there and so do the same process takes
place on the other side so the advantage
of that is that we hopefully get the
user back to our dealer and this is the
other dealer right so this is what we're
trying to do in general getting better
advertisement by knowing a little bit
more what the user is doing across
different websites so the part that
we've been working on in erlang is the
ad gear gateway the ad your gateway is
based in the trader and it's basically
this piece of software that's sitting
right between a bunch of bidders that
can be external or internal and a bunch
of exchanges like the Google looga Homer
Simpson doing that so there on the left
side and we have the bidders trying to
just put money on the advertisements and
we've got a couple of databases there so
most of the stock was set up before I
got there I can't no I don't know all
the details of everything but I can tell
you this about the hardware the software
using by default when it comes to Erlang
and that kinda stuff we're using
currently six servers 1620 courses each
a couple of gigabytes of RAM it's a bit
different from that I just quickly
checked when making the slides and we
could easily scarce you're way way more
scale to weigh more than that the
problem is that right now
it doesn't make an economical sense to
us to do so all the OS is running in
their origin too we're running Cassandra
and for the database you've got the
links to Cassandra library that's the
one we're using to deal with it there is
also a reddish database going in place
with some of the bitter and other
operations we're doing and all the front
end of the way under the web service
receiving the requests is actually
handled by cowboy we also have other
data centers dealing with just doing the
cookie matching the thing exchanging the
cookie this is actually done in see in a
night in an Apache module but will focus
on the Gateway so the basic idea of it
is that when we get a request about the
advertisement we're usually getting
something like six to seven to nine
thousand requests per second on each of
the note that we have in the system and
we can easily scale it up too much more
than that so we have thousands of these
of these queries per second and we have
many bidders so if you've got 7,000
incoming queries we might have three
24,000 outgoing queries or something
like that depending on how we filter
them so in the system as a whole we're
getting in general four thousand four
times the number of inbound queries
going in a system right now with all the
sockets that kind of stuff and usually
for each of the bid we had a limit of 35
milliseconds to answer to with whether
we bid or not and if we bid what's the
price what's the advertisement all the
information available it can be higher
than that but in then in general that
will be around what we have and ideally
we have no downtime because down time is
money going missing so yeah we've got to
scale yeah that's a basic point we have
to do so first thing we need to do
actually is to measure it's a bit
useless to optimize without knowing
what's going on there was a talk
yesterday I can't remember which one it
was but someone just assuming that
something will be faster than another
one and ending up being wrong which was
a big loss of time so what we're using
to measure stuff is first of all the
graphite from 10 for all the statistics
that we have in a browser this is done
using stats d which is ya know Jas app
just piping in for me
the graphite system and we have stats
barrel with the link there that's been
done by my coworker that lets us
basically the single calls to the
statistics system based on sampling
rates that kind of stuff so that if we
have a million requests we don't
actually send 1 million messages over
UDP that will be useless and we have an
application called vm stats that pipes
into that and just gives us general
virtual machine stats so this is the
kind of graphics we can get you probably
don't see it extremely well on the
bottom you view on a bunch of different
lines the little legends and this is a
graph of what was going on with eyebrows
as we had it in the system I don't know
if you can see then there's a bunch of
little red lines at the bottom squiggly
lines just going up and down every
minute or so they're just at the bottom
right here and these squiggly red lines
our calls that are being missed there
there are errors over what we're sending
over the network and then the green
lines are in fact when the load balancer
that eyebrows has gets locked up and at
regular intervals it gets too can be a
huge peak the load balancer just
completely Lux up no longer able to
process any requests and actually behind
that there's a little process that every
five minutes or so will restart eyebrows
which was a pretty bad solution but we
didn't really have the time to do
something else what we've got there's a
huge dip at the right of the graph and
then it gets better right this is what
the stats showed when we change the
library for a custom version of HTTP see
so while HTTP see is actually a very
very nice little library made by some
guys that are lying solution and we
picked it mostly because it's supported
doing a synchronous requests because as
the and dolls law will tell you your
system can only go as fast as its
conquer as its as its sequential parts
right so if you're if we're using a
synchronous HTTP client to forward our
requests to our client bidders then
we're never going to be able to have
really really good times because it's
just doing them one after the other so
using HTTP see we're able to do a
synchronous do it concurrently and what
we did was just write a new low balance
alert for HTTP see and this
is where we get into a bit of the
architecture part so the way it works is
that this is a model of the eyebrows
kind of hat if you have a single load
balancer in the center that just opens a
connection you're going to have if you
have in that case the client on the
right is having 400 sockets to a given
domain it's going to slow down the one
on the left that only has a hundred of
them right so what we wanted to do is
split all the load balancers work one
load balancer by domain and this is
something that's not very very frequent
out there because a lot of people want a
general-purpose client library for HTTP
see what we needed to do is actually
keep the socket open as much as possible
on very very few hosts the reason for
that is that if you do a TCP handshake
on H connection you're making you're
losing 10 milliseconds out of 35 so
you're losing a third of your processing
time just trying to contact the person
that might not even be it on your stuff
so this is something we definitely
wanted to avoid so what we've got is a
big connection pool per domain and it's
a message-passing base system what it
does is that it basically receives all
the requests dispatches their message
per message finding the other the first
free socket that's available monitoring
the process that it dies it restarts it
a synchronously and by doing this we
were able to have that big difference
here and if you can see the red lines
they completely disappear after the big
dip and the green lines also mostly
disappeared you see there is no longer a
big peak meaning that it gets completely
locked up different systems that we got
a few errors here and there and the
system is generally more stable the
purple lines that you see the the big
one on top was actually how much queries
were able to send to the customer so
just send just switching to a different
model of load balancing we're able to
actually multiple not multiplicative
it's almost a third more traffic that
we're able to go through customers just
by sending changing the model of load
balancing and dispatching right the
central point is to avoid all single
points of communication and we're used
to all single points of failure but what
we want to do is reduce all single
points of communication so you don't
have all the messages you're sending
going to a single process i'll come back
and that later but the thing is if we
have nine thousand requests
second continuously day and night you
can't expect a single process to be able
to churn through nine thousand messages
per second day and night and do some
processing on top of it so we need to
split it as much as possible to reduce
that kind of stuff so one of the problem
we have with that is that it's still
kind of sensitive to overload right we
have nine thousand requests and we can
be sending nine thousand requests to
each of them if the split is not done
correctly and so in case of extreme
overloads when we have a special traffic
peach which might be coming either from
something really weird under on the
exchanges or just us configuring it
round we might be crashing all the
exchanges because they're no longer able
to do anything and the message queue is
build up and everything falls which is
kind of something of something we saw
with the error lager coming with her
line right we have this er lager and I'm
pretty sure we're not the only people to
have seen that kind of pattern you just
see a little error lager q going up and
then the whole system locks up goes out
of memory and you don't know what was
going on and this is a problem that is
endemic to using the default arrillaga
in airline it doesn't truncate error
messages so if what you're having is two
megabytes of requests that are crashing
the process then your turn going to try
to write two megabytes on the disk and
this is going to take a lot of time
right the big problem that we had with
that is that because the error log RQ
will crash and go out of memory it was
worse than not logging it out because we
were making the system extremely
unstable and we were never able to look
up why it was crashing was just
happening so fast that we lost all data
and turning the logger off with a better
option in logging at all but we do want
to know why is the system scratch it
happens at all times of the day we
didn't know why so we went with Andrew
Thompson's lagger from Basle and we saw
improvements all the way right there so
the build that the big purple peach that
we have here where what was going to
happen when we had errors if we look at
this one you can see the legend on the
right so I can just read it but at about
10,000 messages in the queue it crashes
with that one is going to well over
twenty thousand messages in the queue
and is still churning around without a
single problem alright so just using lag
rahe the system with staying alive and
going
the only problem we have is that we had
so many error messages at once that the
logs will rotate before we're able to
find what was the cause of it so during
a week or so we were just letting it
trash like that and just accumulating
the messages until one of us was sitting
at the logs watching them when the crash
did happen and it turns out that it was
a single exchange sending badly
formatted TCP packets that were too
large for the content size and we're
using an old version of cowboy and that
was causing cascading failure is killing
all the requests or when you have ten
thousands of them you have been
thousands error messages coming in at
the same time and it's very important
because we never really know what's the
scale of the failure we're going to get
when it starts cascading something like
layer was able to handle all of that so
I do recommend using lagger instead of
the default error lager in Erlang
although we do keep it working through
the error lager API that already exists
the big advantage is that lagger rights
cleverly to the disc and truncates error
messages if your air lager doesn't
truncate error messages you're likely to
have the same kind of problems we had
and so we're lucky we had steadily
gained in the system right now the
system is simpler so what we decided to
do once that our stable is running our
own little bitter though I just spend so
much time on that gift I was just going
to let it play out yeah alright so the
next step to be able to run our own
little bitter was to be able to
distribute configuration and this is
something important because when you've
got a new campaign going on you're
saying that well I'm ready to spend
fifteen thousand dollars on printing a
given number of advertisements and it
happens that people handling the
administration panel something just
enter the wrong number and suddenly
you're spending a trillion-dollar
instead of sixteen thousand dollar and
you don't want to go out in bankruptcy
so it's important you're able to
distribute the configuration and refresh
it really really fast or disable stuff
so this fancy drawing on the slide is a
bit what we have we have six bitter in
teal and purple that kind of stuff and
in the center what we want is a central
controller and we want to send to our
controller not as a sin
point of failure but at a single point
to distribute all the configuration to
every place right and we never want a
single point of failure so this bush
design idea to use bird file which is
just you take in our Lange term you put
it to binary shove it in a file and then
you put in the system and what we do
with that binary file is that we load it
in memory and turn it into an ETS table
depending on the structure it had the
reason for that is twofold first of all
is that if the central configuration
server goes down you don't want to be
unable to bid so having a static copy of
the configuration on disk is always
useful the second one is that we want to
go within ETS tables the reason is that
if it's going over the network it's kind
of annoying to know that you're going to
do 10,000 requests a second per server
on an external configuration machine
that's going to change maybe every hour
or so or it mean maybe even only
questions of days so we're doing a lot
of network roundabout and traveling for
the reason which reduces the time and
the latency that we have available so by
using that and putting it out in ETS
we're cutting a lot of requests we're
cutting a lot of replies and it's
sometimes a kind of suboptimal solution
when it comes to flexibility but in
terms of speed that's pretty much as
good as it can get so we went for that
using the central server we were able to
direct all the campaigns that kind of
stuff and it's currently working pretty
well we do intend on maybe changing even
that application in a year or so when
the configuration is getting too big to
rewrite frequently the principle between
the birth files does it just loads the
file in memory makes it into a table and
garbage collects the table every time
the file changes we're just monitoring
on disk if you've used mochi web
reloader loading new modules in Erlang
it's basically the same concept but with
burt files turning them into database so
we're just refreshing the configuration
whenever we want so we're lucky the
configuration ended up working fine so
what we did is we decided to add more
features we're adding more exchanges so
more traffic coming in that's a bigger
assistant load we were wiring lower
latency so we want more system loads we
want to go faster and to be more
consistent in our time so we weren't
pedal to the metal right so we just
increased everything and what had
happened happened we nearly crashed
everything
happen is that database kind of started
to suffer all the data the live state is
something that we push to the database
and as many of you might know when you
just push it out to the database the
database can only do so much until it
starts having a hard time dealing with
it so the problem we had with cassandra
is that the traffic went up the load
balancer we had was a message-based a
bit like the one we're using we ll HTTP
see but because it's slower the impact
was much bigger so what we had with this
kind of graph in our monitoring stuff
you see the lines at the bottom the big
Peaks rather the blue line those are the
problem and things those are timeouts
when we're doing a request in that we're
bypassing the low with time because
we're locking on waiting for reply
telling us whether we're busy or not
trying to get a database connection or
whether we actually get an answer or not
so just trying to contact the database
was sometimes enough to make crash the
whole query that was allowed 35
milliseconds so this was a serious
problem and the reason for that is that
ques suck for that kind of stuff right
so when i get a queue usually it's
pretty simple if I just get one element
inside of it am i available to treat it
it's all right I'm never going to get
some kind of build up and queues are
fine message queues are always all right
the problem is that when i get a bit
more load in the system i can deal with
a few of them if you look at the red
curry right now the wet query just came
in the first batch so i might be able to
help that one but then just builds up so
at the time when I like the red query
the red one there it's possible that
it's already timed out and so every
message in the queue has been too old
because of the processing time we're
taking and this is very very bad in
general for us because it means that at
some point you get behind the time for
all the requests and basically
everything you do is just trying to
catch up churning messages and dropping
everything you're not able to do any
single thing with your system this is a
very very big problem so what we decided
to do is pretty much just give up right
we just stop we no longer took traffic
and we just made different systems so
yeah no that's not really the truth and
you kind of expected that right the
thing is that stacks are much much nicer
to actually handle that kind of stuff
here I've got something that's the full
message queue or full message stacked
right on the request so if I to if I
take this one there I add a new one then
the next one I'm going to consume is
that one and what's good is that the the
query i just took is completely fresh if
i have 35 milliseconds to handle it i'm
sure i'm going to have the 35
milliseconds because it's entirely new
all the other ones are too old so it's a
best effort I don't mind dropping the
older queries because it means I'm going
to get much much better results of the
new one and this is a kind of pattern
that we don't really see often in the
applications erlang is being used for
and we often ask a bit jokingly to have
a message stack instead of message queue
in the airline processes because that
will help us right the problem with
stacks is that you need to basically
clean them when you're done with it so
at some point let's say the red request
is timing out so now I know that
everything at the end of the stack is
timing out so what I can do is just drop
the whole stack but it's annoying
because i need to do manual garbage
collection right so the thing that's
actually nicer is nothing I don't mean
nothing is better I mean that using
nothing no stack is actually better than
using a stack or using a queue because
the situation we're in is that no matter
what we do we're always in an overload
situation we're always going to get more
requests then we can handle with the
database that's a constant even if
reduced traffic the database is not
vital to the whole process but it might
be slowing down the whole process so
we're fine with generate dropping
queries so we've got a single request
here and while it's being processed I
get a new one the only thing I do is I
just turn it down I don't actually need
that request I reply I'm not going to
bid on that one I'm taking my sweet time
to process the red one and so another
one comes I just drop it and at some
point I'm done with my I read request
there and I get a new one and that one
I'm fine with it I can handle it and I
just have to turn down all the rest and
this is a central principle that we're
trying to do to develop in a lot of our
operations right we have the right to
drop queries and we can basically handle
being in constant overload if we're not
taking the time to analyze the request
or trying to get permission to know
whether we have the right or not to
process it
so any pool system that is going to use
a cue to base a message is not going to
work for the kind of application we're
doing we need to use absolutely nothing
and that was the central principle
behind discount which is a little
pooling library that we developed for
the kind of work we're doing the real
central idea is that we don't want to
use messages it's her language we don't
want message passing the features that
we really want a ver lang are the soft
real time constraints having good timer
being able to do stuff in the country
and see but we don't really care about
the messages there because if we use
messages we use mail boxes which are
excused which means our assistant dies
right so we're just basing it on that
and we've used it as gas central pool to
try to make it better and we're afraid
of central point of failure center
points of communication central point of
communication was a big problem so this
is a general structure that discount
tries to have and that we try to use in
general in the Gateway to be able to
handle the load we need to handle so its
base under the idea that we've got to
ets table the first one is right only
and it's the one that's name ID
encounter the second one is read-only
and at the ID in the pit and the thing
is that when I want to write a given
resource I'm doing a get resource
request what I'm going to do is randomly
hash it and the house should give me an
even distribution between all the
different IDs I have so here in the
circle I have five different resources
that i can access so if that resources
get hashed to the orange part of it that
maps down to the fifth part of that one
so what I do is that I want to use only
right only and get all the resources so
this is the trick by using the ETS
increased counter atomic operation I'm
able to write to the table and read at
the same time but as a single right on
the operation so we're completely
cheating the system but the thing is if
I'm not using a system like that and
writing and reading from the same table
the way a bench market on a laptop like
this was that you get at most something
like 500 requests per by tenth of a
second which is too slow for us if we
use only read only table right only
table I have not been able to hit the
limit before running out of memory for
processes
so what's costly with ETS table is
actually switching between writing and
reading the way we're doing it with that
exactly is that way so here I'm
incrementing the counter from 74 to 75
on the orange one and I know that
because the value is 75 it's not my turn
the trick with the protocol is that the
counter needs to be 20 so if I get a new
resources that one is purple increase it
here and i get the value one then I know
that because the value is 1 i'm the
first one to try to get that resource
and i'm able i'm giving myself the
permission to read and go message a
process to access a resource so this is
basically implementing new texts or
rather few textures in Erlang
efficiently using ets table so that way
we're not doing any kind of messaging we
never have any kind of cute build up and
we only deal with requests that can
actually acquire the resource and we
don't care about queueing anything
because as soon as we're done with a
resource we reset the counter to zero
and instantly there the next restores
holds on it and we're sure it's going
always going to happen because we have
maybe I don't know 200 of these
resources but we've gone nine on nine
thousand people trying to get hold of it
so it's always going to be a full system
and we're always going to handle only
the freshest queries no stack nuke you
know garbage collection to join the data
all we have is one query at a time so
that gives a simple a very very simple
way of writing resources which is just
you want the request I give it to you
take it back and there's nothing really
complex on it it's just completely
distributed distributed within a node of
course so this is a result of running
discount as soon as we deployed it there
was a bit of painful deployment we had
something like 10 20 minutes without
requests but what we can see is that you
have all these all these little red
lines on the left side of the graph and
what these red lines show is all the
time outs that we had and those are
requests that we're taking too long with
the database that we had problems with
and that basically meant that we were
losing money trying to bid on something
that made no sense on the right side of
the graph you see a bit of jumble of
blue lines or red lines are lower if you
do running average of them we pretty
much got rid of all the time outs and
instead we got
I'm busy messages basically telling me I
don't have the time to handle database
go do something else and this gave us on
a larger view something like that so
what we see is that the green bumps are
all time outs when you zoom out on it
after a day you get what's on the right
so you see there's no green bump at all
but there's also no busy bump that came
up there's just all brown line that we
can barely see and what's to remember
about that is that a single time out has
a big ripple effect that affects all the
queries for minutes at a time because as
soon as you start going late then it
just gets worse and worse and worse and
worse until you're just churning in the
vm until it gets down again it's very
very hard by having maybe a tenth not
even a tenth of the requests are getting
busy or actually keeping the steel the
system more stable and overall we're
handling way away more queries just
because we have more time to do
everything right so this is really the
central point because how we're getting
overloaded we want to be able to drop
anything thats unnecessary focus on what
has this time we're reversing the idea
of the queuing of messages over lying
around and doing it that way and what's
actually nicer is that when we look at
the run queue so this is drunk you how
many processes are waiting to run right
now on the left side you have these huge
peaks and this is because Erlang was but
it was busy when the mailbox gets really
really high then that process usually
gets a higher priority on the scheduler
and it's just churning to get rid of
messages as fast as possible and makes
the whole system unstable and all
processes are waiting when we don't have
these process skills we get what's on
the right and a very very smooth curve
at the end so in general the whole
system is more responsive we answer to
more requests and all we had to do is
use a bit of tricks with ETS table and
discount library rather than using any
other pool system now I will certainly
advise against using discount if what
you do is something where you cannot
afford to drop request right we are able
to use it only if my only because we
have the right to drop requests and just
say I'm not going to bid on that one if
you don't have that normal pool system
like pool boy that's used by the bashful
tea it's probably going the best thing
that you can
and even in many many times like we're
doing with L HTTP see just using message
passing if you can split all your
requests enough we'll be able to handle
a heavy load now we do plan to at some
point maybe replace the L HTTP
subversion we have to use discount but
this is a generally good idea so yeah
our system is now as stable as a
mountain goat and what we have left to
do with further optimization some of the
things we decided to do was to replace
outgoing Jason by I lists the general
idea is that because our Jason is
frequently the same format it's a common
protocol we have with our customers we
don't lead and we don't need a general
library and when we were profiling our
application Jason was actually where all
the CPU was going and it was slowly
changing our application for something
network bound to cpu Balan because
decoding and encoding your request was
taking so much of the CPU power it would
create problems by going with airline io
list which are just lists of bytes and
binaries that we assemble ourselves and
do maybe a little escaping of ourselves
on the data we don't trust we're able to
make sure that the whole gateway remains
a network bound application and never
gets really in trouble with cpu-bound
stuff so we're shaving stuff as we can
there's still a bit of stuff that's
really risky on the cpu but we're
reducing it all the time so if you've
got really really flat JSON structures
or really really flat structures of
string in many kind writing yourself a
library that just wraps it into a
nihilist and avoids all the kind of
parsing that's usually necessary in
strings it's something that is very very
good to do a basic optimization we're
also trying to get rid of nips for two
reasons mainly nips are generally very
very fast and that's good something we
found out though is that in some cases
if the NIF is taking too much time to
process something it's going to lock up
a scheduler right so if you've got four
scheduler and your virtual machine you
got for long requests your might be
risking the stability of the system as a
whole just to try to get something
faster sequential but in peril and
concurrent system you're going to hurt
your system in the long run so we're
getting rid of this or that if we're
using only pure Erlang then the
reduction system gets in and we
sure that no request will ever lock up
one of the schedulers this is a bit
counterintuitive because if you do any
kind of sequential benchmark the nips
are clearly better and we did that even
with the iOS when we're just comparing
iOS with the EGS an encoder what we were
getting is pretty much a speed up
something like five percent with the IO
Jason solution which seemed like a bad
idea when we just compare it like that
because it's more complex more risky we
have to change a working system for a
really really little game however when
we did conquer and benchmarks using even
just a hundred processes then the IO
decent solution quickly got better to a
factor of two or three times faster than
the NIF solution that was used so when
you benchmark your stuff always be
careful because benching marking it
sequentially is not always
representative of how it's running in
production and if you do more advanced
benchmark you might find out that using
the niche is not always the right choice
now the second reason why we switched to
trying to remove all this is that when
we're doing our deployments we're now
just hot reloading the code we're not
using relapse and releases for that
because they tend to lock the system
we're just reloading the module flat
like that and the requests are going so
fast that we generally don't have the
time to provoke any crash because of
that we're fine running in the old
version for a fraction of a second and
all the new versions automatically gets
in the problem is that nips were
blocking that we're not quite sure why
but at some point when reloading the
module and an if and the NIF is busy we
will sec fold the vm in crash entirely
so it was getting actually worse then
just restarting the vm right now we
didn't get rid of all that if some of
them are really useful so what we're
doing is just keeping reloading them and
when we will need to reload them I will
probably just do a rowing upgrade by
playing with a sets of up a set of
options but in general nifs are not
something we won't in the vm except when
it's absolutely necessary there's also a
question of maintenance of course if we
all have good C programmers on the team
that that's kind of easy when you have
good C programmers that also know how
your line via more
that's kind of more rare and it's easier
to either just find a SI programmer or
just find an airline programmer so it's
a definite advantage when you have to
work with the library to keep it in
there lang most of the time and yeah the
other idea is decentralized as much as
possible other future challenge that we
have is that as the demand grows will
need to transparently scale to multiple
data centers right now we are easily
able to do that the problem is that when
we're handling budgets for ad campaigns
it's harder to do with transparently you
will need to have different instance of
all the budgets because as many you have
known many of you may know the cap
theorem if something crashes we don't
want to be locking up all the other VMS
or to be overspending on the budget
because of network problems between data
centers so this will be the next
challenge this might make the latency go
up because the databases will now need
to do request cross data center between
acceptant instead of just within a
single data center and we will want to
decentralize as much as possible in
general so I think that's most of it for
the content I I don't even know when I
was supposed to be done maybe I have
been a bit too fast five minutes so I'm
rather tired good thank you so any
questions on that yes
the new process there are two reasons
why we can do that the first one is that
if the resources that your load balancer
is holding currently sockets then you
need to do the transfer of 200 circuits
or rather open 200 new sockets which is
not practical the other problem is that
because we don't have stacks as relying
message queues is that you have a cute
in a stack one after the other which
might worried some circumstances but
it's way more shaky than using a custom
solution that uses nothing so we're
trying with that but the problem was
that yes as soon as you get in Erlang
you need to put stuff into the stack and
if you're in constant overload situation
you're not able to actually inspect the
older elements of the Statler timing out
so it might be possible you're just
constantly getting you or newer requests
and you never see that you're building a
huge stack that will eventually kill the
vm the same way a queue will be doing it
we could do that if we had destructive
updates being able from some time to
just look deeper in the stack but it was
just simpler conceptually and in
implementation to do the stuff with
discount in the end any other questions
yes
okay yeah so the question is that when
I'm loading in configuration how do we
look for the table results the thing is
we pretty much copied the system that
OTP currently uses which is the
application get environment you've got a
key you've got a value so we're using
configurations a key value store we just
wrote a little wrapper library that
looks into a table of tables that's our
main index reads that table and go fetch
the information so when we load a new
table we swap the table of tables to get
the new versions keeping old one in
garbage collection so we don't trash all
the requests to working with the other
table and just generally make it it's a
bit of a mix in the way airline users
code upgrades right you have two
versions you purge after a while and we
have the version of the application
system where you have a main key and
then it's a key value store that we are
maintaining based on bird files yes yeah
right yeah yeah that's a default
instruction that well the thing is that
relapse are generally useful when you
need to you need to do the locking
because what you want to do is be able
to call the code change function right
so what we do is that we decided to just
give up on that feature entirely and if
you don't need that feature then you
don't really need to relapse in the
first place it's just load a module
again because the requests are so
short-lived that we don't even have the
time to take the time to update them we
just let them finish and die and unless
we reload twice in 10 milliseconds we
won't have any problem with that right
so it's completely fine the only
problems we have is when we need to
change the configuration of the
supervisor because a supervisor updates
as configuration by using the same cause
so we so far we didn't have that problem
we were doing rolling upgrades but it
might be interesting to either mate
relapse only for supervisors or only
make use of the SIS module manually
enforce it when we actually need it but
so far we haven't had the need for that
yes
yeah yes absolutely when we do when
we're doing the request the basic system
is that the cowboy server has the
handler with the current request we
instantly spawned a new process that
handles it and we have a timer in that
one if that process takes too long to
reply we kill it and we send a message
saying we're not going to be it on that
one so that's what we say about saying
we can't ignore requests so as soon as
we can we just kill that process then
the parent form that sexuality cowboy
handler will just look at it and say
okay I'm just returning a no-bid
response which tells the exchange that
guy is not going to bid on it and we're
staying within the delay is to say that
we're refusing to do it so that's where
the soft real-time parts of rank are
being able to handle all these timers
concurrently without breaking the
stability of the system no on that one
that depends on how you do the acceptor
pool on the tcp system when you're doing
tcp acceptor is you can start let's say
100 x scepters which is what cowboy does
by default and so you have a central
main circuit but you have 100 accepting
sockets so when that happens all the
accepting suck is the first one that's
free is able to do is you're able to
receive a hundred 2,200 soryu boost it
to 800 you can have a hate and rage
request concurrently being accepted so
this is a question of how the sockets
are implemented there's no central point
dictating where the request starts they
just start don't really because you have
many accepting sockets that each live
within their own processes
yeah the simple reason is that it's
easier to handle the timeouts it that
way if you're doing the timeouts within
the same process then you need to either
frequently lock yourself in a receive
that looks at the time left or just look
your seventh timer and kill yourself
which is not very flexible and you need
to basically do it manually if you have
an external process to do it then in the
pro in the one that's actually handling
the request you just think about the
happy case i always have all the time i
need and the other one on the right is
just taking care of making sure that you
don't go too far so it's a much much
simpler programming model to use two
processes at the risk of using a little
bit more memory than using a single
process and then having to think about
the timeouts yourself all the time while
you're doing the clean code so it gives
a better code base in general three or
four something I thought well I mean it
works the way it is right now so maybe
we could look into pounding more
processes but we don't have a need for
that so we're not doing it
no no there's no relationship there's
always just new stuff coming in the way
cowboy works is that where the accepting
socket accepts a connection it's
pummeled a new handler process so we've
got a constant pool of 200 400 except
errs and as soon as you get something
this pound it and then start a new
accepts and you're accepting so you've
got this part of the code that just
dealing and accepting and this part of
the code that is dealing on processing
the request and that part of the code
that just watching the process or the
request and it's all independent and we
never had any problem with that the way
it works right now yeah you keep you can
do it that way the problem with that one
is that your kind of realizing
everything the wit works in cowboy is
that the definition of the main circuit
is shared by the supervisor which is
simple one for one supervisor and so the
definition doesn't have to be
transmitted all the time the process
spawns knowing the definition of the
socket it has to listen to so it's more
efficient on that side in how it's
handled there it's the way cowboy with
built which is pretty efficient and
we're just following along with that
other questions well thank you for
listening to me I hope you enjoyed the
talk
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>