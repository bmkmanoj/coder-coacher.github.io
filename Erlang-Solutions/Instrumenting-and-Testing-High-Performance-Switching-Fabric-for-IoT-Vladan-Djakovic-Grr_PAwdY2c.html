<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Instrumenting and Testing High Performance Switching Fabric for IoT - Vladan Djakovic | Coder Coacher - Coaching Coders</title><meta content="Instrumenting and Testing High Performance Switching Fabric for IoT - Vladan Djakovic - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Instrumenting and Testing High Performance Switching Fabric for IoT - Vladan Djakovic</b></h2><h5 class="post__date">2017-03-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Grr_PAwdY2c" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so my name is bottom jaw bridge
I'm with iDevices it's a company that
makes everything from things apps around
them and infrastructure to connect these
things before joining I devices about
years ago I was doing different kind of
infrastructures from Silicon to game
distribution high performance computing
p2p cameras for consumers so I was kind
of all always were lots of servers are
and there is a lot of infrastructure to
be done this was very interesting
because this was opportunity for me to
do something really exciting and very
very interesting the opportunity was
that this could have been done for the
scratch and I'll explain why we did it
from scratch and how it went
so essentially I'll be talking about
what is this high-performance searching
for PRI IOT what problems we encountered
and what did we do about that so what is
Internet of Things it's really millions
of devices small computers on most of
the time and they sit and they wait for
something to happen it happens very
rarely they send the messages out about
environmental things temperature
humidity and so on
this thing's talked to back-end servers
and they talk to handsets that's that's
what they do
and what you really do now
infrastructure is making this connection
between handsets and things and back in
service work as you know everything is
behind net these days ipv6 never
happened so this infrastructure is
really taking care of things that should
have been taken care of by ipv6 they're
not be interfered with that so it's a
custom back-end
the main goal main rationale was to cut
down the cost today if you want to have
a thing and you want to connect it that
somebody's going to run back in for you
even though you're you know your local
commercial services for that is going to
cost you between dollar fifty and 450 or
five very device here so if your lender
the cost of the device is twenty bucks
and you're making ten bucks profit you
sell into the thirty the question come
who's gonna pay another 10 15 bucks for
the live demo device and this is totally
an answered question I mean this is the
song so obvious but this is totally
unanswered question when you have 20 30
50 devices per human people not going to
sign up for 50 accounts there is not
enough data coming from this to be
monetized to keep up with these costs it
just doesn't exist so the things have to
become as simple as plugging in the wall
electricity plug-in putting plug in the
wall outlet if you had to have the
relationship with your PG&amp;amp;E for
everything you want to plug in you know
the electricity would cost like hundred
times more you know you know I'm going
to plug in my hairdryer on from this you
know how are you today it won't buy
something and so on right so that's
that's non-starter these things and not
hands they're not computers there is no
money in eyeballs there so the cost had
to go down and the goal here was is to
cut it down by essentially up all its
magnitude which we did so the also this
whole infrastructure is pretty much
invisible the devices are primed and
enabled during manufacturing there is
nothing for user to do to collect of
course the application Todd can do
whatever they want but for basic
connectivity there's no user
interactions there no onboard there is
none port it is your own party tank and
to get this cost down hundreds of times
we had to cut down on electricity cut
down on the server requirements and cut
down benefit so this is how we did it
we went through all udp-based particles
we call it
P the name held for couple years it's
for distributed server mesh they drawing
the thing by themselves the the system
by themselves there is no control panel
there is there is nothing basically you
cannot even login to the service
there are appliances and they are made
to join the existing system or existing
systems and one of the drawing there is
nothing you can do to that I mean there
is absolutely no command and control
center nothing like that
the mesh is fully distributed so you can
just keep adding service and the only
the Russian help the service when you
provision it there are no other
interactions there is whatsoever it's
very efficient because we're running on
bare metal we pay for per CPU cycle far
less than we would have when you go to
rental services their rent VMS UDP is
far more efficient than TCP in the
requirements are relaxed you know it's
not important you get every message you
get most of them the based on UDP loss
is run point 4 percent so it pretty box
well and you spend a couple of times
less than bad with that we buy a bandit
directly form tier 1 segment providers
so we pay many many times less then you
would pay too when you go to service
providers like Amazon and others so to
give you idea what we're doing here
essentially executing close to hundred
thousand session per second per pizza
box and we can host about million
concurrent clients of that if you
compare it to others you see it's far
bill beyond anybody else is doing as far
as I'm aware it is secure so there is
the kind to serve encryption there is
end-to-end encryption in SDK
they both have forward secrecy the
server's do not understand the namespace
of hinds so they don't know who they are
so there is nothing server can snitch on
anybody there and service doesn't
understand data files do so there's no
honey pots that's the point that the
third party have security so what
happens from each of these pizza boxes
they're hungry so UDP sockets on
multiple interfaces we always have
multiple backbone providers terminated
in our cabinets each packet has
predictable world which allows us
essentially to shape load on the network
interface all traffic is encrypted all
krypton CMS I would say that 50 60 70
percent or CPU is cryptography the
message switching service dispatches
that packet out in less than a
millisecond and we have a ephemeral
storage service which is mapped to RAM
and disks and we have a custom sheet
read pool and you had to write our
custom database for this when with the
point notes you bring the box provision
from bare metal and then it's done there
is no repair if something breaks it
breaks there's no software repair the
software just runs basically you cannot
log in these boxes the typical test
cycle for no distribution is cut moments
because they had to run for a long time
and the typical lifetime of of release
is between 7 months in almost a year
so we have beams running life for a year
continuously and again remember that our
users are machines they up 34-7 the load
is pretty much uniform and sometimes you
get surprises which are going to talk
about so these things are actually less
forgiving machines we have to support
forever all the devices ever deployed
even early on when we didn't have all
the things right so we have ability to
handle multiple protocols because once
you sell something you have to support
forever
if you drop support it's really bad for
business the load matters a lot because
we are extracting the maximum out of
each culet and each space and it bit
the load is quite higher with unstressed
load is quite high in service so it's
very hard to replicate that on tests
Pat's be when whenever you slow it down
it behaves differently it's not the same
thing
that's one big problem we had that how
do we test is has a high load the input
traffic is possible to replicate it's
you know a couple gigabits per second
per cabinet and UDP is different from
TCP you are going to see things that you
didn't even know existed or internet so
we tried to do of course we can
traditionally we tried to do logging if
you log things our service fall down
like ten times
so the scenery's changes and you don't
have problems you had before he started
wobbling even if you just send messages
to a single process and not write that a
new disk it slows down couple of times
this one big should the version here and
mention Ireland and why we using Ellen
there was nothing else that could do
this it's a talent
LT is mostly one-to-one communication
they have millions of that they are not
connected to each other right so along
this micro trestle I don't have it just
made for this thing there's nothing else
that can do it and this this capacity is
what that's why we went for for Eric
however again the problem is how you do
debug this this system we cannot reply
the traffic it's unpredictable
it's just too much of it so basically we
have to go back to the basic code
walkthrough here something goes wrong
you see there and spend hours looking at
the code and trying to figure out what
happened to help people do this we had
to figure out how to narrow down the
problem space and this is what this talk
is about so we need to visualize the
system when we look at things
it's always the whole system of huge
number of devices we never look at
individual trends because that's
impossible
the devices are unpredictable that
they've talked simple things and
actually all these things that happen
between one device and the it spear have
been debugged during development that's
not the problem the problem is what to
do at the high load so the picture had
to include everything from performance
and service performance of backbone
pipes and the group performance of
clients if you have problem SDK a group
of hyunsung exhibit different behaviors
and all this had to be done without
hurting the runs so we designed this
thing that we call poles it's a custom
instrument C system it uses our own UDP
based protocol that we use for other
clients for real-time communication it
has a subscription model so essentially
the the aberration runs on
infrastructure nodes themselves but the
actual subscribers are outside usually
put them on ec2 it's a queue because we
didn't have any additional ports we
didn't have to open anything else it
uses the same security as our basic
protocol and by the way we have our
protocols audited and we do it regularly
in my head I mean we pay a lot attention
to security it collects data from
multiple sources some data is collected
from the beam itself and I'll talk a bit
about that and the other stuff is
collected from the operating system this
is not business analytics this tells you
nothing about about that's your bit
about files but it is really not IT
analytics as it's mostly understood
today you know what coins do how many
times they're switched on whatever they
switch on or off and so on this is about
system performance so again it's a
subscription model
subscribe this run on usually on Amazon
on ec2 or multiple zones and subscribe
is decide which nodes they wanna monitor
and what they want to monitor on this
node the publisher is daemon on a node
and it pushes data to subscribers and
typical rate is essentially one packet
per se
and barometric so it's very very well
bandaged on the note publisher separate
even watching it and see the
communication to the rest of the system
is ruining the main targets the
publisher binds to it so if he dies
that's fine others just can't try to the
sake they just skip that packet the data
comes for the beam and we have dependent
demons that collect eyes statistics such
as CPU usage disk performance network
performance and so on so the important
thing with them send is the publisher
and has no effect of system performance
it's been dying away it has can have
problems it does not affect the natural
performance the process is in beam that
send data publish our well lightweight
the coaxial counters and so on there is
pretty much nothing per client these
aggregate things and they essentially
wake up every second and send the number
down the UNIX domain socket so what do
we collect further being himself some of
the data we get a number of clients
number of concurrent sessions from the
OS what is the beam means current memory
beam CPU which is going to be very
important actually how many packets per
service we have in and out what storage
system doing and we also pong the node
from the external subscriber itself to
get the sense of efficiency as we have
standard battery tests and we measured
lightness and the subscriber represent
data there are separate instances we run
in situ we are currently using the
graphite in the database and the graph
are as rendering engine
we keep lot of history the history is
very important because sometimes you're
gonna realize that there are things that
you should have pay attention to or
didn't and you go back a month to and
say AHA this is what happened that so
it's very important to be part of
history
if you're not familiar with the graph
Anna you saw some you'll see some
screenshots we it's a web interface for
display you can configure it in
different ways and users which are
basically system people in house they
had to learn how to use prefer not to
see different stuff
the system is main such that we can
interface it to other display hundreds
and we are to consider some commercial
displayed ndreds hope we'd only I mean
if we find them more useful than your
graph are so I'm gonna show you a couple
of the examples important thing to
understand here is that because we had
to develop this phone scratch who didn't
really know what to look for and again
going back to the to the whole project
when we started nobody knew Ireland I
mean so it's not like anybody was
experienced our own programmer whatever
else was addressed the only tool that
could do the job so we had to learn as
we as we move on and the important thing
we understood is that you have to
display as much as you can you don't
know what you're looking for you want
original data and non derivatives and
you you're gonna get surprised how good
our eyes are how good the visual system
is in humans that you're going to spot
things which you never thought you can
spot but you have to give it some some
data to work on so I start from a very
basic stuff this is observing the beam
memory in normal operation so when you
boot machine you're gonna see how our
couple of weeks the B memory goes up and
then in stabilize its in point
especially about once a day there's a
small jumps and then it stays there and
then when it gets to some maximum it's a
it hangs on there
and if you blow up this maximum you're
going to also see so the bottom one is
the week work week week worth of
observation you see that about it does
garbage Russian proximately on daily
cycles we never went deep into the
garbage collectors to feel
what's really very well we try different
couple different ones but once you see
this and say this is normal then you can
spot the abnormal stuff and to give you
some ideas our machines have usually 100
gigabytes of memory so this is you know
it's got a lot there so then this is one
example how what to look for when you do
the bugging so but there's a problem and
the note goes down which happens very
rarely so actually had inaudible down
not due to heart failure in once so it's
like once in maybe you know tens of
several years so people especially
looking for straws and in this
particular case we have some testing was
going on with the third-party gateway
and we isolated that traffic there and
with no notice is that not went down
after one of these tests right so just
give you just example that you don't you
don't know what you're looking for but
you have to go to a bunch of these
diagrams and graphs and you know then
try to make inferences what could be the
reason that things went bad
again this is observing the normal
situation and this is how we track that
when a node fails that clients are
redistributed on other nodes in this
particular case the grass the grass is
tact and the show number of clients per
machine this this case it was a four
machine setup and one went down it was
actually this
the story trailer so when falls down
when goes down the plants so you can see
here that number clients went to zero
basically right from some number there
and then you can see the clients on the
other machines within basically minute
and a half the price for this machine
migrated to the available machines so
something what happens you look at this
and say okay it were closed so
everything's okay there when things are
not okay you're gonna see these things
longer organs have this kind of clients
having problems migrating and so on
again you're watching here huge number
of clients it's not one client right
this is brute behavior then you see
things that you didn't think you're
gonna see in this particular case we are
observing at all times essentially the
latency for the standard battery of
tests that we run on brain stop service
and we have back to these test machines
and there are different costs and as we
have servers on both coasts what is
normal situation is that this tester a
certain 99% of the latency is the travel
time so this is pretty monotone use
curves and you can see here that this
one is from the same host where the
actual test server was and this one put
the opposite cost so have difference in
child speed of speed of internet not
speed of light and what happened is that
we just saw that the near closed thing
went all the way up and we can know who
was going to know I mean it was totally
totally and we didn't know what was
going on so then we isolated the
backbone provider that was involved in
this and we called start to trace routes
so what happened is as we found out that
the fiber was cut between I think it was
between Seattle and Bay Area and
they had to reroute the rerouted through
Kansas City so we're quite surprised
being solar packets are going through
Kansas City these two machines but that
was the cause and actually provided
didn't think we're gonna noticed so they
apologize later on and I think from now
on they are going to tell us ahead of
time and things like this happen one of
the things that we do regularly is do
stress tests where we basically load
service to capacity where we expect some
packet loss which is how we deal with
with overload and here you see a basic
trace from couple dose stress tests you
can down there in CPU which you never
want to hit saturation so it's kind of
close to hundred percent there and you
see input and output packet for
particular servicing as you can see how
in normal operation the packets are kind
of the same numbering out but when you
stressed as the discrepancy there people
there's back loss this is not very
accurate but this really confirms what
our stress harness is telling us so this
is kind of sanity track when you look at
these things and then there are client
box clients of course all kinds that we
have the we deliver SDK and SDK
essentially is presenting kind of
coherent interface to our protocols and
the basic expectations is in the
protocols most of the time is that a
number of input and output packets are
should pretty much in sync unless there
is overload so here we observed
something interesting this is not a real
situation you just blow up a particular
place and occurred that we saw that
there are more impacts than two decades
and these clients had some problems so
we this is one indication that you can
order using the group behavior you know
many many clients you can still make
inferences that there is
along with the SDK out there and this is
kind of business data that actually the
company really like to see this is
number of clients over couple of weeks
so although this is system test this is
actually tracking sales so you can see
how the clients are going up and the
glitches are glitches with poles so
they're not going up and down every day
but with problems instrument in there
that's one thing you have to keep in
mind that your instrument system also
has problems so you should always take
whatever it tells you with the grain of
salt and the things we don't know there
so for example what we are observing
sometimes we have this increase in being
memory in CPU usage and we have no idea
what's going on absolutely no idea
what's going on it doesn't seem to be a
problem but it's kind of disturbing that
there's something going on that you
don't know so I will keep an eye that
and we get some bandwidth somebody's
gonna go and try to figure out what is
this about
so that's this pretty much it again this
is this is dealing with high number of
clients you want to be very fast and
very cheap in your operation very very
low cost you always see the large group
and you want to kind make inferences
what this large group is doing when you
start you don't know what you're looking
for
so you need to display a lot of things
and then try to figure out what's normal
what's not normal and trust your eyes
that you are going to see the difference
sometimes you can automate so if you see
things going wrong specific way you can
of course automate data create alerts so
you don't have to count on your eyes
anymore because you have observed
particular patterns
and you have automated them later on
this rusty system is always gonna have
bugs so you have to be ready for that
sometimes you know we get excited all
for nothing it was not a problem it was
the instrumenting thing and in IOT sdk
is what determines the pretty much
everything so this is not web browsing
we have browsers and well defined
protocol so terror everything uses SDK
to connect this and one one thing that
we do is that we use the system to
evaluate and you know SDKs as we do new
releases to make sure that there are no
differences or they're known your
problems introduced with that so that's
pretty much it and questions
so so the devices the actual firm refine
scarlet lines they are primed in the
factory and we give them a bureau
certificate which we called coin we salt
to that right so that coin has certain
performance characteristics you can do
so many packet so you can do our tenth
episode a or can do video streaming
right
the point is going to be different of
course it'll cost different and that
point is all that our system is asking
for hot so it's correct yes
yeah and it's installing manufacturing
it becomes essential part of building
materials so you don't have to think
about that it is the errands when the
work and usual its lifetime of the
device oh no
so our service our FreeBSD we will spray
Beasley we have a couple of eater
gigabit interfaces per box they'll go to
different many providers so we use
Phipps
different routing tables for different
providers and inside each service and we
have like three services designed three
kinds of services each service has wide
range of ports which are randomly chosen
by clients and on each of these ports
there's a process whose only job is to
get a packet and figure out who to send
it to inside so per client processes are
they doing the crip Shem and the rest of
our then they come up as clients come up
and go away but the processes that
actually try out traffic from the from
the port their run to port and they're
very fast because they just get the
packet and figure out does it do they
need a new process to
to process that packet or is existing
one and that's better shall be done on
on the source basis so it's very fast so
we have but per server we have say 500
ports so this helps yeah go ahead
question we have a dispatcher pair input
socket so on freebsd is really about the
networking it's very stable so again we
have hundreds of ports open which are
basically randomly chosen by clients so
we have a kind side if you will all
balancing and when the packet comes
there is one process pair port so many
many processes per interface that is
just looks at the back and decides based
on the original packet who to send it to
does not decrypt does not know that it
was sends it to the process that is
going to handle that
this is all errand processes these are
all error I mean the this Patras it's
all in their own yes everyone opens the
sockets yes Alan Costin sockets okay
what is all just right so we operate our
own service and we use among all TP and
FreeBSD on the server side it's pretty
much all our code there is nothing else
so it's pretty straightforward the
protocol itself has a pole mechanism so
the clients essentially send the hard
detect it's every couple minutes if the
if we find out that the net window is
less than that which sometimes days then
we calibrate that and they said send
packets more often which means that we
keep the net open with very low overhead
traffic and so the clients can receive
traffic at any time because we do keep
the network I'm sorry go ahead
okay so you mean what is the target
we're going to try and send packets
right the load balancing is on hand side
so clients have service directory and
they calculate which one to use and we
have the thing we called words so we can
limit the clients to this particular
geography if you want if that's
important we never use any DNS no
generous is totally out of picture DNS
is we don't have any fingers in our in
our protocol correct yes correct correct
so so what what happens actual insight
is that servers maintained by their own
service directory and they kind of have
consensus on that and there is thing we
call directory service right and it sir
has that and then clients when they're
born they know a couple of known names
and they learn more as the move on so
basically once a day or so on they pick
up in your listing for clients and the
listings are one persistent so the
Machine goes down it's usually up you
know within 24 hours that doesn't affect
service directory and so the service
directly main thing is when you add
service that's the most dramatic thing
right and fines have to find a new
server to go out there
correct
yes it depends it depends so there is a
teen difficult world and they overlap
between between data centers and servers
so essentially when you add the new
server which wasn't there before
clients will eventually redistribute to
to use the new one yes I don't have a
good answer to that so the things that
do not bother the service we put on a
back burner and we kind of aware that
they're happening
we don't care these hiccups that I show
you the bumps there are kind of CPU goes
up you know not much but you can see
that right so we didn't do anything so
far we just didn't answer we just kind
of keep an eye on that and it's been
running for a while it could be totally
benign so we we don't really spend time
on things which don't affect users
yes but what is this if you input to
that yes yes we could so this could be
one thing you decide appliances so
they're not demons the non listen or
anything so I mean questions how you get
there in first place to do that so we
have really good stability again these
things run for almost a year so we keep
a list of things that your potential
problems but you know you have to
prioritize and see what what you're
going to solve first omen so next and
here yeah
okay so sure sure sure sure sure so
there's a top on youtube if you look my
name you're gonna see the talk about
that which I gave a developer conference
last year and very in a short is we
provide the two basic services other
than directory service we provide
conduit which is real-time communication
you pick your address is having 28 bit
things so it's like a pin we don't know
what it is and you talk to the other guy
the happy stone or the same thing right
and we didn't conduit we have a Datagram
mode obviously and we have reliable mode
as well so you can send messages the
retries and so on and so on we use
conduit for our own devices to piggyback
other protocol so that so we pick back
in other protocols which excel aspect
you know tcp/ip or so on we can
piggyback them on on this product on
providing more connectivity and the
other service we provide is ephemeral
storage
it's either short term four hours and
then just disappears it's all of the
database so every record no more than 30
hundred bytes it has been a bit key
space and we have a long term which is
seven days and what you use this for you
use this for a few client base logging
it's great for video broadcast because
one guy writes and many of them can read
and you don't have to bother this guy
here so doesn't the two basic services
provide so the both kind of
communication just one is kind of
delayed can be delayed for a week and
stays hangs on there before it's been
picked up and so far because we have
many different classes of devices we
didn't find need for any other service
really this real-time and this delayed
communication pretty much covers
everything
I don't want so close to the</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>