<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Michael Sperber - Concurrent ML  - The One That Got Away - Code Mesh 2017 | Coder Coacher - Coaching Coders</title><meta content="Michael Sperber - Concurrent ML  - The One That Got Away - Code Mesh 2017 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Michael Sperber - Concurrent ML  - The One That Got Away - Code Mesh 2017</b></h2><h5 class="post__date">2018-02-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/pf4VbP5q3P0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">as you get older well as I get older
anyway I i noticed more and more that
there seem to be a lot of things in
software engineering and programming
languages and how we write software that
we knew in the 80s and that we somehow
forgot and are now rediscovering and
concurrent ml is one of those things
that we should be rediscovering anyway
so
well the Liggett Aires slide about what
I do is I'm CEO of a company in of a
software company in Germany called
active group we do software in things
from social pedagogy to semiconductor
fabrication but we do it all using
functional programming and well we also
run a German language blog on functional
programming we also run a conference
that is sort of a little bit like this
one and it's almost even better than
this one but it's in Berlin Germany
so if you're interested it's February so
that promises to be pretty good anyway
so so a long time ago in a former life I
was a lecturer and researcher at the
University of Tuebingen and I taught a
course on concurrent programming and I
really read it I really designed that
course from scratch and I was looking
for good textbooks and I came across
this thing conker called concurrent ml
which seemed to me really the the
paradigm that was best suited for doing
for writing concurrent applications and
I ended up then implementing concurrent
ml for a schema plantation called scheme
48 I later designed a concurrent ml
implementation for an obscure language
called star that you can still find if
you look for it and some people might
know that I wrote the sixth edition of
the scheme standard so even though this
talk is called concurrent ml most of the
talk today is going to be in scheme and
I just got praised earlier this week for
doing a talk that cost the audience to
do mental gymnastics so that might be
one of those talks too anyway so so but
really so this is the best book on
concurrent programming and genre Pete
keeps promising me that there'll be a
new edition any day soon but even if
there isn't it's still it still is the
best and you should totally get that
and I do not have a single example
that's better than
in this book so that's what I'll run
with today you know see a few other
things so fast forward though so I
taught this course in 2002 when I was
doing a tutorial at the workshop for
commercial users of functional
programming in 2011 and I kind of
remembered that remember concurrent ml
was all about message based concurrency
and you might work today in an
environment that gives you message based
or concurrency such as alcohol or Erlang
or something like that but I was doing
an f-sharp tutorial now discovered that
it actually has a little framework for
doing message based concurrency called
oddly enough mailbox processors anyway
so I figured I would just go and do a
little example one of the examples from
the this great book that I remember from
the concurrent ml book and just rewrite
it an f-sharp to show people how things
are done in F sharp and you can see here
here's a little network here's well it's
kind of a an academic example is but you
express the Fibonacci numbers as a
network of concurrent streams and so you
have data flow going going around yet
and I think you recognize it so you kind
of so you'll always add sort of the
previous two numbers and so but so you
always sort of have one number going in
there and you need to delay one by one
at them and then things go because goes
around and around in this network right
and so I thought well it's just gonna
take me like a half hour to do it in F
sharp I couldn't do it I couldn't do it
I couldn't do it ended up Julian doing a
different example but the reason was
that really the F sharp mailbox
processor is something that employee v'n
though it's based on message based
concurrency it really implements
something that we know is the actor
model and the actor model here's a
picture of the actor model that's it
right you have things you sent messages
to processes or two actors right and so
if you're if you're into functional
programming you always want to do things
that compose and and sort of looking at
that picture you realize this does not
compose right specifically does not
compose in the way that this requires
composition right and so I ended up
trying view really hard so never mind
that code
so trying really hard to to implement
another example mergesort but but I
really ran against that fundamental
restriction and it just messy and
unsatisfying
so abstractions really matter
abstractions that you use for whatever
it is in this case function of
concurrent programming and I would argue
that composition matters as well so I'm
gonna tell you a little bit today about
the facilities that exist in concurrent
ml that an able abstraction and
composition and that have since mostly
not completely been forgotten so not
completely I mean concurrent ml even
though so was originally developed for
something also long forgotten called
standard ml of new jersey a standard ml
implementation well you see some people
you might remember that yeah but pretty
pretty dead these days um anyway and
but it still exists with a couple of
other almost equally obscure programming
language implementations a couple of ml
implementations but the majority of
these are sort of descendants of scheme
like scheme 48 and racket and guile
guile has a pretty new and shiny
implementation of conqueror in ml and
sour and there's also a haskell library
that appoints concurrent ml but that
seems to be pretty dead as well all the
other ones are
well maybe the environments that they
exist in are kind of dead but within
those are well maintained so I'll show
you a little bit why a couple of things
are not concurrent ml and I I can't even
remember why I had forgotten that in
2011 so Erlang and goal in closure core
acing and akka streams and whatever or a
how generally are not are not concurrent
ml so here is so I hope we can make out
all the letters so here's again the
picture of that people not cheap Network
if you sort of look at the original
figure again you can see that the edges
have been labeled as have as are these
boxes in the middle and here is a
concurrent ml solution written and
scheme so I don't know how many how many
of you people have done scheme doesn't
really doesn't really matter scheme is a
very simple language so we're defining
just a function that creates a frequent
not she network we're saying we're
creating so these edges here we're
implementing them with a concurrent
mechanism called a channel that connects
to things or that connects various
things in this network and so we're
creating channels for each of the edges
here and they have the same name so
here's the out channel here's C 1 C 2 3
C c3 c4 c5 we just create channels and
then we hook up the components to those
channels so for example here's the delay
component which is sits between c4 and
c5 here's the copy component which sits
between c2 c3 and c4 here's the add
component and you can hopefully see that
the program corresponds directly to this
picture is that clear okay good and so
then what remains is to actually
implement the abstractions that occur in
that program for example here is the add
thing so add is this component it has
two input channels and has one output
Channel so it has premieres inch in
channel one in channel two and one
output Channel and really well you need
to focus for a moment here on this part
it just says well - the output Channel
we're gonna send the some of things that
we receive from input channel 1 and
input Channel 2 so that seems pretty
intuitive well later see that there's
something wrong here but we'll get to
that and you just need to keep doing
that right as message as numbers come in
you need to spit them out at the other
end which is why it says forever here so
there's 4 everything here's here's the
engine that keeps all of these
components going it's a function called
forever it takes two arguments one is
called Anette which is the initial value
that you feed around this caruso and
this function f and what you always do
in concurrent ml is you spawn a thread
that's what that spawn thing does and
you feed it a thunk and here there's
just a loop that just says well we're
gonna go around and round and round and
each time we're gonna apply that
function f right and in this particular
case you might have seen it says forever
false which is because we're not
interested in the value here we're just
implement we're just interested in the
effects but will later see an example
where we're interested we're actually
interested in the value
so there's the forever thing so here's
the delay here we're actually interested
in the value delay is sitting here so we
need an initial value that we can that
we can use for this for this loop that's
going on and we have one input channel
and one output channel and so we
alternate sort of between receiving
something and sending something out
right and this value will either be
false when we're sending something out
or it will be the value that we received
which is something that we'll be sending
out on the next round okay
so you can see here it says if and it
alternates between sending something to
the out channel or receiving something
from the in channel right and the return
value of this becomes the next V and
then gets sent sent out here okay so we
start with an it and you might remember
that there was this one one number that
we fed in here's the copy here's the
copy function it has well read it from
the right so it has one input Channel
two output channels ouch one-and ouch -
again it goes on forever and it receives
something from the input Channel and it
just sends it on to the output channel
one and the output Channel two so as
with that there's something wrong here
but we'll get to that later this program
is actually gonna work but you might you
might notice that there's something
oddly disturbing about this right so I
should note yeah so so what we've used
here are two functions called send and
receive and for you ml and Haskell and
we're and our programmers I put in type
signatures so you have a channel of type
that carries values of type a so these
percent things are type variables and
sending some things just gonna not
return anything you feed it a channel
and a value to put on that channel and
receive you just give it that channel
and it will give you the value that
somebody else put in there and these are
synchronous operations so a send and a
receive have to meet for both of them to
succeed okay
that's important if you're used to a
framework where these things are
asynchronous and corcoran ml there's
synchronous they have to they have to
act
we meet simultaneously so here's what
might worry you about that copy function
is in that we receive a value and
there's an ordering here at first sent
something to out channel one and then it
sends something that I'll channel to
right and and if you have this kind of
ordering and concurrent programs
well you might remember there might be
dead log issues aren't in this
particular case but they might occur in
this case so what you really want to do
is you would like to not say which of
these has to happen first right one of
them will have to have to happen first
but which one is right might be
determined by the other side of those
channels and you can't see that on this
side right so what we need is something
that's called selective communication if
you will where you say well you know
here's a number of things that might
happen and I'm gonna wait for any of
those to happen and then well I'll do
something based on that and so a big
question and the design of concurrent
programming frameworks is what are the
things that you can do selective
communication on right so so for example
so now we're going to jump to a few
other languages so for example closure
right closure comes with a message based
programming framework called core async
and you can write a copy function here
I'm gonna gloss some of the for any
details to get back to them later and
this thing here is like this this bang
thing is receive and this other bang
thing is send right and as you can see
here we get a value we receive a value
here from the n-channel and then we do a
selection between those two out channels
right so there's all things as alternate
or something like that says one of those
two things might happen and in this case
also the communication is synchronous so
we're gonna see which one goes through
first either sending V on out channel 1
or an out Channel 2 and then we're just
going to send it then depending on
whether we actually succeed in all
channel 1 then after that we're gonna do
our channel 2 or vice-versa right and
this does not have an ordering issue
anymore so that's pretty neat so closure
Corey thinks comes pretty close but
we'll see you know pretty close is
sometimes not enough
so anyway so so of course go is the
go-to language these days for concurrent
programming there's different so the you
might have noticed this thing alt here
is syntax if you know closure it's a
piece so it's implemented in a library
but it's still a piece of custom syntax
and in the case of go there's actually
called something so there's something
called select and it's not part of a
library it's baked into the language
right and you but you can basically do
the same thing you can send v2 out
channel one or out channel two and you
can see which one of those succeeds
first and then you just send it to the
other one respectively but if you think
about it you already noticed well how is
this you know in this case it's fixed
right we have one in two always right
but what if you have a list of those how
does how does that work out right so an
Erlang of course and and so early is
also based on message based concurrency
but it has this actor model so when you
do this in Erlang you realize that there
aren't any channels you don't send
something to a channel you send
something to a specific process right
whereas to a channel multiple process
might attach to a single channel so this
really is just sort of a translation in
notation but not really in spirit in
well this copy function does copy one
forever
it receives one value V and then feeds
that to output process one or output
process two and well bang is often
asynchronous so there's maybe the
ordering issue doesn't occur so much in
practice and there's some issues with
this so but we can't even really talk
about the issues that we talked about
before right the Selective communication
it's something that we can only do
between receives in Erlang and a timeout
right f-sharp same same thing
essentially we're really the
communication between these mailbox
processors often that's shortened to
agents is always asynchronous so there's
no way to have to even talk about
selective communication between
different send operations that you
that makes sense okay so anyway so how
does how does that work in concur in ml
well in concurrent so you you you
certainly Dan eight you need a name for
the things that you can do selectively
communication on and if you look at the
concur now book it will call that an
event I find that a little bit ambiguous
but I kind of like the idea of a send in
a receive meeting so I like to call that
a rendezvous so that's what that RV
thing is its truth it's true or some
parties meet on something so in this
case there is a select function you feed
it several rendezvous and it will try to
it will try to make one of them succeed
and will give you the payload value that
comes from that and then there's two
versions of send and receive that don't
just do it that don't just perform the
sent or send or receive but that instead
give you the rendezvous that if you try
to select on it will will perform the
actual send or receive okay so so this
these are constructors for that for
those rendevouz and here's a common
here's something that you can do with
them it's make sense so far so now you
have these things and you can
immediately think about well if your
functional programmer you think about
Combinator's for these right and the
simplest company Combinator that you can
imagine is that well if you have a
rendezvous that produces an a you might
want to do something with a value that
comes out and so well you express that
in the form of a function that accepts
an A and maybe gives you back a B and
you can wrap those two things together
and give you a rendezvous where that
produces a B right it's like map
essentially even though but we often use
that with an effect in that function so
so and you need this for for the it's
for for this so so you can even use this
for a for version of copy that uses
select with concurrent ml and here's how
that works as well you make rendevouz
for out channel for sins on out channel
one and our channel two right and then
you use wraps on each of those
individually to then do the send on the
other channel respectively right
then you do a select around that that
works okay and again you know this is
the version that's the right thing and
you can do the same thing for ad and you
can see the same set pattern here is in
that well for ad you need two numbers
not just one as in the copy so the code
gets a little bit more complicated but
in this case so you have a select on
receive RV on inch one a wrap on RC VAR
v on inch two and you wrap things around
those where you see well here's the
receive on the other one can you see
that not everybody not okay good good
doing well and we cons those two values
together pass that out of the Select and
then at them and put that and put that
on the output Channel okay things work
out so again so but select again is this
thing it goes from a bunch of rendevouz
of a to a right be nice if we could do
that so that's not what we usually try
to do with in functional programming
we're trying to do composition we really
want to stay within rendevouz so there's
a more primitive construct in concurrent
ml which takes a bunch of rendezvous is
called choose and it doesn't just
immediately give you back the a it gives
you back a rendevouz of a and then you
can combine that with other rendevouz
and do something with that and then
later when you really have built up your
entire description of your rendezvous
then you can sync on it and select if
you look at the type signature you can
see that it's essentially just a
composition of those two right you
choose you create a rendezvous that does
selective communication then you sync on
that and so that's how that works
and so you already have have a nice
algebra here and of course so in your
typical you know modern concurrent
language usually you're selective
communication is going to be restricted
to certain kinds of rendezvous usually
it's like send on something receive well
if you're often it's just receive on
something or a time out but sometimes it
might be sends and receives but there's
of course all kinds of other rendezvous
losers time outs you might have
different kinds of channels we'll see
one more kind
you might have access to shared memory
i/o the sockets
iota files and things like that and of
course in your concurrent program you
would like to be able to combine all of
those in selective communication and
it's always a bummer when you have to go
to an API that only allows you to do
some of them I mean if anybody has ever
written like a runtime system for a
programming language ready to have to go
to like UNIX select you know how
frustrating that is that it can only do
file this guy yeah well you know right
you only do file descriptors and time
out and you have to somehow get
everything into that world right so this
is much much nicer so not just that you
can build your own right so for example
something that you might sometimes one
is swap channels where you don't just
send a value to somewhere else but where
you exchange as part of the raw and even
with value and here so here's there are
six are as record definition which just
says well a swap is something that has a
single field which is a channel and
you're the comment says what the channel
carries the channel carries a pair so so
you swap things it it is a pair of the
value that you're sending to the other
side and a channel for the value in the
other direction right so and we just
stick a new chant whenever we make one
of those swap things we always stick a
fresh channel inside and that works out
just fine but so so again if you look
sort of in the center here you can see
the same pattern again is if you're
swapping you don't know right which one
of which one of the two parties to the
swap is going to be able to execute
first right the scent or the receive so
you just do a selective communication on
the receive and the send and depending
what that is you either so so for
example of receive succeeds you will get
you will get a pair out of the message
that's coming in and the channel I can
use for sending my message back to the
other side right and then I just well I
sent the message to the other side and I
produce the value that
I got send and here's the other
direction if the Sen succeeds first then
well I sent the channel on which I will
receive the value from the other side to
the other side the other side will then
send something on that channel and this
way things will just cross over so that
uses the choose Combinator there's
another Combinator up there which says
guard which is there to make sure that
we always allocate so guard just takes a
thunk that produces around a room and
converts that into a rendezvous because
each time the rendezvous will execute if
you will you need to create a fresh
channel up here so each time it executes
that thunk gets run so here's guard
takes a thunk produces a rendezvous and
coerces that into a rendezvous ok so
there's also if you read on concurrent
ml I'm not going to fully expand on that
but there's also something very powerful
called a negative acknowledgement but it
really goes to the power of the system
if you imagine writing a lock server
right in this world right you want to
acquire a lock so if you want to acquire
lock that means you need to send a
message you need to express your desire
to acquire a lock to the lock server and
the log server is going to look up is
that log going to be available and it's
gonna make that available to you but by
that time you might have lost interest
because acquiring that lock might be
part of a selective communication right
and so the log server would like to know
about this that you have lost interest
right one more reason I like this word
rendezvous right because I think it's a
nice metaphor what's going on so so
you're I don't have a complete lock
server but the way it works is the
client says creates so with knack is
sort of like guard but it creates this
neck rendezvous which you're able to
synchronize on if this party loses
interest and the pattern is always this
the client creates there's a nak
rendezvous here and sends it over to the
other side and the other side can then
use that to determine that we've lost
interest okay so here's that pattern so
we have that nag rendezvous we create a
reply channel that the log server can
send us an acknowledgement we spawn a
thread that will send our request to
server and then we wait for the for the
reply and then the server essentially
well it receives a request the request
contains a reply channel and this NAT
rendezvous and it just selects between
those two things right either we're able
to acknowledge oh yeah and again a
rendezvous was two people two parties
getting together right so in this case
there really is agreement on not only
did I give you the lock but you also
took it and so here you can then commit
to the lock having been acquired or you
have this knack in which case you can
abort and give the lock to somebody else
so that's the most complicated bit but
it you know your concurrent frame
comic-con programming framework try to
do this nicely right you will find out
that it's it's pretty difficult so so
all this you know I'm getting ready for
this talk and just thinking generally
about concurrent programming you know
I've been trying to think about what it
is that really is nice about concurrent
ml erling is nice in many ways but it's
not great on composition right and one
of the reasons it's not great on
composition is that it's fundamental
under mental map model that's built into
the language is a couple of things that
are put together right you have that
process a process has to run on this
receive and con's construct that we've
seen the process is wedded at the hip
with a queue of things that go into the
process and so it's it's several things
well the closer people would say a
couple a couple of things complex it
together and we know it's a very useful
thing for building systems but it's not
the only thing that you could build to
make your concurrent system run and so
channels sort of give you a nicer
picture or a more primitive picture and
that you have this channel here in every
process or you and you can have you can
share that channel between different
processes and everybody can write and
read and that makes it much nicer to
build composable frameworks if you're a
closer fan you might have had high hopes
for this new well it's not new anymore
but it just gone out of alpha I think a
couple months ago something called
casing which gives you a channel based
message based concurrence
so you might think this is gonna give me
or I thought anyway it was gonna give me
the power that I was used to from
concurrent ml specifically because its
core API does have culminate errs right
so well so here's the official
documentation for the merge Combinator
and and you can see well channels and
has channels that go in and you kind of
think oh that's kind of nice but then
you realize well it's a common Eider on
the read actions on those channels and
it's you know those are the only things
that you can combine this way right none
of the other actions work that way you
can define you you can write your own
Combinator's in in-court async but the
way that always works is that you need
to have what's called a go block they
thought they would they were doing
themselves a favor by making it
attractive to the goal people and a goal
blog will then return a channel which
will receive the return value of the
code inside that goal block so you kind
of can kind of go from channels or
whatever it is to a goal blog back to a
channel and you can use that for
composition but it doesn't very well
work very well in practice in particular
this sort of cycle here has like a 10x
performance overhead so that's that's
how that works right it's you have a
goal block you have a bunch of channels
going in you write your Combinator code
imperative lis you go through the
execution image machinery of the goal
block and then you get a final channel
for the result and so it kind of looks
like it might work but in practice it
doesn't really go there and the reason
for that is also that core async complex
a couple of things namely lightweight
concurrency and message based
concurrency right and I think that's one
of the things that we've forgotten from
the 80s is in the 80s we knew how to
build a programming language runtimes
that had extremely cheap threads right
well Erlang still knows that right but
most everybody else seems to have
forgotten and so that created the need
to somehow put lightweight concurrency
on top of the heavy weight threads of
the run of the language runtime right
and so in particular if you're on the
JVM then the native threads on the JVM
are pretty expensive
you know and and that led people to
believe well it's because there are
threads right it's because there are
threats and they're not actors and
actors are much cheaper but of course it
has nothing to with any of those things
it just has to do with the fact that we
that the community at large knew about
more efficient programming and
implementation machinery and so that's
why core a think I know I'm rambling a
little bit but maybe it'll all gel and
make sense in the end
well why Corre Singh has this go
construct because it creates sort of a
special execution machinery for the coat
that's in here coat in here doesn't
really matter but what matters is the
mackerel will compile that into what
they call a state machine and if you
know c-sharp they also talk about a
state machine that they're going to
compile it into but really what it's
about
it's it compiles into a bunch of
continuations that have a shared
representation and all that vocabulary
has also been lost since the 80s so so
there's that so I'm trying to tell you
where things have been conflated so that
maybe in your mind
you know but by the time that you go to
sleep overnight they will they would
clear themselves up um if you if you
talk to people who do like modern
message based concurrency frameworks
such as akka they're all they're all
obsessed about buffering and and back
pressure specifically the core icing
people are obsessed about back pressure
right and unfortunately it's not that
easy in that kind of framework because
you have threads and so does everybody
know what back pressure what that is
about right essential so what happens is
that well you might have it specifically
if you doing asynchronous communication
you're gonna have a queue somewhere some
kind of buffer for your math messages
right and if you have a big system with
many queues if those queues grow without
bounds because your system more stuff is
going in than is coming out then your
system might become unstable right but
if you have a concurrent programming
framework that allows you to spawn
threads that will create queues anyplace
that there's synchronization going on so
in particular you know if you do incur
asing you have three cues attached to
you
we channel right there's the buffer of
the values they're sitting in the queue
then there's a queue of the threads that
are trying to send and the queue of the
threads that are trying to receive and
so you don't I mean even though you want
back pressure and flow control in your
program you don't get it with the
mechanisms that the API gives you so and
I think the closure people should have
spent more time on D complexing their
framework so that I could actually plug
in the right mechanisms for back
pressure here
so getting backs getting back from the
rambling so you might have seen that
it's a little known computer scientists
of ages ago set this right you know I
you know in 2011 I've served discovered
the hard way this is is that really the
language of concurrent ml had shaped my
thinking right and that it would and so
and if you have that at your disposal
everything else that comes after that is
slightly disappointing but also the fact
that a lot of people have forgotten
about concurrent ml specifically the
people who implemented concurrent ml
frameworks since then really means since
they didn't have that language at their
disposal as their API is turn out less
powerful than they could be so you know
if you want to treat this as a hierarchy
then there's concurrent ml here there's
a new well it's not quite new but a
couple years back Aaron Chiron wrote a
great paper on reagents which and that's
a mechanism that's and faces a danger of
extinction but the ocala people are
picking that up which is slightly more
primitive than concurrent ml but is
based on on very similar ideas and that
is also performant enough to not just
implement concurrent systems but also
parallel algorithms Cori things slightly
less powerful a lot of people refer to
wor CSP which is nice but concurrent ml
is nicer and/or Erlang then gives you
that specific combination of elements
that give you all the advantages of OT p
but i really wonder whether you should
see that as a straight line and whether
there shouldn't be more branching points
here that explore alternative designs
and I'm not I'm not currently seeing
those and I would I would like to see
them in the future
so
as everywhere in programming even though
so so some people don't believe that but
I believe you should really compose all
the things whatever you're doing and
conquer in ml gives you that and you
know even if you don't get to do this an
industrial setting concurrent program
writing concurrent programming
concurrent ml or any other languages
that implement concurrent ml is
tremendous fun so I invite you go out
and do that and just it will put a smile
on your face especially if you've
written concurrent programs that have
not put a smile on your face in the
future in the past but there's large
open questions associated with this
entire field right is how do you combine
composability with things like back
pressure and distribution and I I don't
know how to do that
I'm thinking about that and hopefully at
some point some results will come out
but it seems not as many people are
working on that as there should be
that's all I have for you
is it possible to model channels with a
process in the line I mean couldn't you
have that us the channel that you do the
run
yeah but the fundamental problem is yeah
the so the question was yeah so that was
my thinking in 2011 right because f
sharp has the same model right okay I
just keep the can I treat the process as
a channel right but but the problem is
that you can only attach one receiving
process to a channel right and to say
after f-sharp is the same way so you can
have a lot of writers but you can only
have one reader and so that makes that
picture asymmetric and and and and you
know I I thought the same thing but I
invite you to try it right and that's
gonna be a not so pleasant experience
yeah yeah yeah that's from that it did
exactly that and I can assure you it's
it's it's it works but it's not pleasant
it kind of works right thank you very
much okay thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>