<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Megacore: Megafast, Megacool? - Kevin Hammond - Erlang User Conference 2015 | Coder Coacher - Coaching Coders</title><meta content="Megacore: Megafast, Megacool? - Kevin Hammond - Erlang User Conference 2015 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Megacore: Megafast, Megacool? - Kevin Hammond - Erlang User Conference 2015</b></h2><h5 class="post__date">2015-07-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/NNTUmDWgHtc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">great thank you very much good good
morning everybody Saul i'm going to show
you about today are some exciting
developments in parallel architecture
some exciting developments that mean
that Erlang is going to help us to be
able to use are these architectures and
this is work that's been funded over the
last three years by the european union
we're very grateful to them for doing
that the project has involved various
people including particularly Erlang
solutions partners from agh who've been
working on Erlang this is University in
Poland also elta and outer soft are from
hungry they can tell you more about some
of the work that afternoon Melinda tooth
will tell you about that later on so
first of all to motivate where we are
and today everything is going multi-core
even this is a laptop from which I
during the presentation is a multi-core
device how many calls does my laptop
have audience question any any guesses
816 to okay you guys think I'm rich
right this this this little device
that's from 2011 um internally it has to
x86 cores ah these things they're hyper
threaded so maybe there are really four
in there but in addition to those four
possible cause it also has 12 GPU units
I it has a device which is just a parish
management it has a Bluetooth controller
it has all kinds of other processes
sitting it if you count them all up all
you discover is that this little device
i'm using here just to do my
presentation has approximately 20 or 30
calls in it and this is something very
small scale at this point in time we can
only take advantage probably the cpu
cores maybe the GPU cores are but in
future are what we're hoping to do is to
make our full capability of these
systems are available to you as the lang
programmer
so today intel has stretched this last
year they released the Intel Xeon Phi in
2013 this is a many core device it has
60 course integration into a single
package but what you'll probably notice
is these cores are incredibly slow and
overall the package uses still 300 watts
of power this thing okay it's got a lot
of course but they're slow cause and
it's still consuming far too much power
to go into a little device like this but
this Intel believe probably is the
future for our processor chips this is
today the fastest computer in the world
in a month's time it might not be
because they're going to release a new
list but since 2013 this has been the
fastest computer in the world does
anyone know what it is so I can't hear
I'll tell you it's the T on to our
supercomputer at the Chinese national
university of defense technology I have
no idea what they're doing with that I'm
not sure I want to know this device has
16,000 nodes each of them has two Ivy
Bridge multicores so there's either
bridge multi-core maybe 16 cause each
and each of the nodes has three of those
Xeon Phi chips each of which has 60
cause built into it if you chat it up
what you'll discover is it has three
million 120,000 x86 cores not these tiny
parallel ecause the Costas was telling
us out yesterday these are real
honest-to-god x86 cores this is probably
the world's first mega core computer so
what are these future keeps going to
look like this is where we're scaling to
well I can tell you for a fact that
they're probably not going to be shared
memory okay normal memories gage costs
the same to address maybe you'll have
some way of distributing the memory
using
not where maybe if Intel can get it to
work you'll have hardware transactional
memory so you ship stuff off but the
idea that you just go and access memory
location and get some data back from it
is not going to reflect the hardware
reality all these myths that
conventional programs that built up
about cheap and easy access to all the
memory at the same time just doesn't
have a new device like that so assuming
assuming fully shared memory is not
going to work but most of the models
that we have today to address this for
example partition global address spaces
divide the memory area inch lots of
different spaces they make the
programmer to all the work now as Erlang
programmers you guys know this is a
stupid model right shared memory is not
the sensible thing our particular for
large-scale machine what you want is to
have shared nothing model that airline
has is going to be a nice fit to these
future machines in particular side
effects across the system like that will
not work efficiently and effectively
this games have big shock to most
programmers it's okay to us we know that
you don't need side-effects so where are
we going in the long term what people
want to build and when I say the long
term I mean 2020 what they want to build
our exascale computers which will have a
quintillion operations per second the
power of something like 50 million
laptops okay it's almost inconceivable
how much computing power these devices
will have or possible applications are
going to run apart from cracking my
messages to in what's apt my wife or
something like that but very importantly
are what you'll notice over here is that
the power consumption of these devices
is anticipated to be something like 100
megawatts so these supercomputers are
they will consume enormous amounts of
power your knees build almost your own
power station just to run one of these
devices this tension between energy and
power
it's permeating all the way through the
computing device that we're using at
this point in time it's most obvious
with these really high-end devices but
ultimately what we're talking about
these days is how much computing
performance do you get for each Joule of
energy that you put in and various
people mentioned this in the talks for
Costas has mentioned it earlier for
example so I think this is make it cool
but I can't see anything or maybe it's
really mega hot energy usage scales
linearly with the number of course this
is why we're going for multi-core as you
add more cause the energy usage goes up
with a number of calls but if you
increase the clock frequency the energy
usage scales with the cube of the clock
frequency this little graph over here
from a computer science textbook called
Patterson and Hennessy on computer
architecture and what it shows is that
in roughly 2005 we were the fastest
chips pentium falls internally were
producing as much energy as a hot plate
so you could cook eggs if you could get
inside the chip if we had continued to
scale them are by 2010 or there abouts
we would be up to the same heat
internally as a nuclear reactor and by
today we would be sitting around with
rocket nozzles on the back of our
laptops okay clearly that's a problem
and this is the fundamental issue which
is called Cisco into the multi-core and
then the many core and then ultimately
the megacorps world I'm gonna try to
help you do that but of course does it
scale
well if you're going to go to a megacorp
machine what you ain't need is millions
of threads so here's a little
application I knocked up earlier and you
can this this application it's just a
test bed it runs for about 20 seconds of
execution and what you can probably see
down here is a number all I won't get
too close because I won't be able to be
on the microphone but it says 331 and
then something like 587 to etc this is
not my phone number this is the number
of threads that this application creates
in 20 seconds of execution that's a lot
331 million threads in 20 seconds the
application is running on something like
16 core machine so how do we do that
we've got 331 million threads we're
running on a 16-core machine how do we
deal with that well what you've got to
do quite here in Java I'm afraid you're
stuck Java is not going to cope probably
with 331 million threads but because
we're running Haskell what we can do is
to take our 331 million threads and
figure out well actually because you've
got a much smaller machine where only we
only need 20,000 of them to keep the
Machine occupied now which 20,000 is a
pretty hard task you're certainly not
going to go round as a programmer and
say okay I need that one on that one and
that one and that one this has to be
fully automatic automation has to be in
there as we go to these large scales so
why do I want 3331 million threads if
I'm only going to run twenty thousands
of them well the short answer is I want
to write an application that could run
on a much bigger machine if I don't
write my applications to have the scale
today when these machines come along
tomorrow what I've got will not scale it
will fall over I will spend a huge
amount of effort trying to increase the
scalability it's much better to have
sufficient scalability already in your
application from the outset
then to squeeze it down but not have
enough and try to get more threads are
to increase the amount of work later
that never works have to having too much
cutting it down much more sensible
approach so wanting actually is to
reprise how to build a wall for you some
of you may have seen this before I think
it's a good little example that
illustrates the problems of how you are
paralyzed programs so I gave that
presentation ah and in Watson who is a
professor from the University of
Manchester said to me afterwards well
that's all very well Kevin but not all
applications are parallel right they're
gonna be things that aren't parallel and
I making his scientist I like a
challenge so I said well what in and he
said well say you're building a wall
right you put a breakdown you put
another brick and so on and now you've
built a row of bricks and then you can
put the next brick down and now we've
built the second row of bricks and we do
the third row and now we have a wall of
bricks and that has to be a sequential
process right
Wow I'm going I'm going to try to make
it parallel so let's imagine that we
have for our bricklayers here these
probably have to be Swedish bricklayers
rather than British bricklayers all this
won't work but what we can do for
example is bricklayers can each layer
brick and then we lay another row
another set of bricks now we've got the
first row exactly as we had before and
we could do the same on the second row
and we can do the same on the third row
now we've got a wall so clearly this is
done we've done this in parallel we use
speed at the top so great that's
fantastic how much faster was that twice
three times four times more than four
times while getting your audience
reaction here go on stand out on a limb
how much faster is that yours you're
smart five times I've only got four
workers here say it's theoretically
that's impossible four times less than
four less than four but that's safe you
could be more precise let's let's work
it out so three bricks so can we be
three times faster at this point and
another three also three times faster ah
now I've got three now we've got four so
it's a bit more than three and another
three okay so it's still a bit more than
three and another three and a final
three so it's a bit more than three but
not quite up to four if we use the same
algorithm even if we put a million
workers on to this problem it will still
only go at the same speed so to make
this go faster what we'd have to do is
to break the problem down there are
fundamental limits on how fast an
application can go based on the number
of tasks that we have
okay so number of workers is a limit
Burt is not the only limit there are
limits which are fundamental to the
algorithm as well and that's the
important point we have to find the
algorithmic limits as well as some
theoretical number of worker limits so
here's how not to build a wall brick
brick brick brick brick and so on and
eventually we get the wall okay nobody's
laughing it's ridiculous right very
Haskell programmers in the audience who
keeping quiet well for the benefit of
any Haskell programmers are this version
is functionally equivalent to the one
that I showed before it's exactly the
same wall I've just ignored a few little
side effects like gravity okay so the
reason this is to this is idiotic is
because if we build a wall like this the
what the bricks higher up well I do this
are not supported the fundamental
support they need is not available what
matters is that I have the right support
for each of the bricks that I'm placing
that is the right data the right task
dependencies for the tasks I'm
constructing once i got that then I can
run later tasks so can anyone think of a
better way to build a wall than the one
I showed you on previous slide
yeah it's what each worker can move
vertically not exactly because when you
put you can't put the brick you can put
this brick here once you put that one
there but then this one depends on to
others okay so it's a little more
complex than just going vertically once
we got to this stage you can't place the
next one but having placed that one and
that one you can place two and then
place these ones and so on there's an
ordering perfect you have to identify
the ordering so that's one way to do it
what people sometimes say I want to give
this presentation is well you're being a
complete idiot what you do is you
preassemble each of the rows and then
just bring them in but then of course
you're ignoring the dependencies so you
have to fix up the defense's when you
make when you put the wall together okay
so you can do that if you're very very
smart boy your tooth then is to
basically leave a hole where your
dependencies are and fill that hole in
later and in a lazy language a skull its
may not be too hard to do that in a
strict language like I lang it can be a
problem because you don't have the data
to fill the hole good so how not to
build a wall so we need structure so
tossed identification is not the only
problem you also need to consider lots
of other issues coordination who does
what you need consider communication
because if these brick layers are not
communicating with each other they are
going to trap their fingers etc you need
to consider placement which of the
workers deals with it which brick you
need to consider the scheduling precise
order in which the bricks are placed all
kinds of complex issues now if you're
doing this using a typical concurrency
approach then the programmer has to deal
with all of these issues and that can
limit the scalability it also takes a
lot of time and is potentially a huge
source of coding errors what we want is
to have structure and abstraction and
not just another brick in the wall
so what I'm trying to do is to teach
people how to think in parallel and this
is a slightly different mindset you may
think that thinking concurrently is
enough but it's not quite enough I'm
afraid you have to start to think not
just about the dependencies you have to
start thinking about breaking the
dependencies thinking outside the box
what is necessary and what is not
necessary in your in your code so in
particular might need to deal with
hundreds of millions of threads if I'm
writing a concurrent application
typically are what I will have will be
10 or 20 or 30 very similar threads in a
parallel program of the kind I've
described I've got hundreds and millions
of them I cannot possibly know and
control what each of those threads is
doing it's like having a hive of bees
you can't know each bee in the hive
individually fewer than you might wear
as concurrent applications typically are
like getting your children or your
colleagues to work on something you can
know what each limb that's doing and if
there are only five or ten of them well
that's a lot of children the very five
or ten of them you can probably keep a
track of what they're actually doing but
much more Matt it's impossible you need
to have models that don't allow you to
introduce deadlox okay so as soon as
you're in a situation where you could
introduce deadlox and you have millions
of threads kicking around are you
against central life debunking these
weird deadlox and sure is great to have
tools that can analyze your program and
detect deadlox this like bit like having
a very good cancer doctor who will who
all cure cancer for you it's great to do
that but personally I'd rather not have
it in the first place it's much better
not to have the disease than to have a
really good cure for it because cure is
never one hundred percent and it takes
time and it's painful
and you can't program effectively whilst
you're fiddling around with
communication worrying about how each of
these million threads is going to
interact with the others it's far too
low level design are you will never be
able to understand the interactions
between millions of threads it's
literally impossible to do that that way
lies insanity so the approach we're
taking in paraphrase are the party
approach is to start bottom up from your
program which could be an existing one
or or a new program doesn't really
matter if you're starting from an
existing program you might have to go
through some additional steps identify
bricks in your program components and we
use a semi-automated refactoring tool
using Wrangler and/or reef actual to
help us identify the components of the
program so using well-known existing
technology to do that think about the
pattern of parallelism so what kind of
passion do you have what do what
structures do you have in the problems
that we've spoken a lot about patterns
I'll reiterate that into the second
photos have not seen any previous talks
these can be things like Map Reduce task
forms a parallel search where you're
looking across a lot of different things
try to find the one or two things that
are interesting that's a pattern that we
might have in our program if you're a
mathematician maybe you've got something
called a completion algorithm what
you're doing is just I trying to work
out all the values that correspond to
some qualifier so find all the values
that comply with some restriction and
collect them and you can do that in
parallel but there are very complex
dependencies between the things you've
found so far and new things you might
create and so on so forth and then
structure the components into parallel
program so turn them into concrete
parallel skeleton code I'm also using
refactoring and of course then you're
going to have to go through the process
a few times until you've got a good
parallel program
so here's the general approach start
with the original Erlang source code
apply program shaping to extract the put
the programming to the right form so
that it so it can be casual eyes and
then if you're an expert you can just
put that through the refactoring tool
which shouldn't reduce the patterns for
you but if you're a novice a beginner or
a professor you don't know what you're
doing or you'd like some help then we've
got very clever pattern discovery tool
that Melinda will tell you about this
afternoon which will basically look
through the program kind of google it
find what patterns there might be and
say to you this the best one perhaps we
think this was the best one why you try
that and see how it works out but it's
really fracturing tool and out pops
parallel code without any dependent well
with only the necessary dependencies are
without any deadlock with all the
communication abstracted away for you so
you don't have to worry about it great
so common patterns that we're going to
discuss include things like a form where
you've got lots of workers doing the
same thing on different inputs or a
pipeline where you do one thing after
the other people familiar with this are
from computer architecture this is kind
of like a function composition so it's
very easy to turn a function composition
into a pipeline lift the arguments are
independent you can then run these
things in parallel so it's a nice
pattern particularly the top level of a
program or a map are a data parallel
operation where you play an operation
across all the ants for data structure
at the same time so breezy to get a lot
of peril of them that way because then
the problem is the parallelism is often
too small to be used effectively so you
have to worry about how do you group
things together to make that work so the
bricks these are functional these are
these are functions what I call the
brick is just an expression that we can
evaluate in parallel I've done this in
pseudo Haskell pseudo Erlang so please
bear with me
what we can now do having got a brick is
to write it in parallel so if you've got
a construct and language called par what
the solicitors just say okay spark that
off it's be like spawning off the are
brick in parallel with doing the rest of
the computation where the result of the
brick is going to be plugged back in to
the pattern that's as we need you and if
you've got a language like Earl angle
scholar this is in Haskell if you're
like like Eleanor scholar what you can
do is just pass in a parameter to
trigger the computation because
otherwise the computation will happen
instantly you don't want to do that you
wanted to happen you want to create a
future that you can then trigger when
you need to as part of the execution
higher-order functions I can be used to
capture parallel patterns so what we can
say for example is our that we're going
to evaluate the function f in parallel
with applying the function to the rest
of the list and this will then basically
apply the operation that you want I'd
said it in parallel across all the ends
of a list structure so you can write
that just as a higher or simple
higher-order function in something like
Haskell or in Erlang and then to build
bricks all we have to do is just apply
the operation that makes the break to
some input and bingo what we've got is a
parallel implementation of our program
very very simple very very
straightforward and this idea is the
idea of a skeleton so you can push
library like this are for Erlang it's
called scale Chris Brown I did this he
told about this he gave us tutorial on
this on Wednesday cities include Maps
forms reduce pipelines also feedback
which I'll come back to in a bit in the
talk when you create a skeleton what you
have to do is construct an abstraction
of the skeleton and then instantiate it
using an ER lying operation scale do and
what this will do is to take your
skeleton abstraction and your bricks and
basically
construct the task structure the process
tree that corresponds are to be scouting
you've decided on plus all the breaks
put in in the right place to
automatically do that for you very very
neat it's fully nestable so you can have
skeletons within skeletons to whatever
depth you like ok so you're not just
restricted to having a pipeline or a
farm you can have a farm with pipelines
inside it and so on and so forth very
flexible mechanism the way this works is
you've got a skeleton description you've
got some input items you apply the scale
do operations to the skeleton
abstraction and the inputs and that goes
way produce the press s tree for you
distributes it and then collects the
output like so so for a pipeline what we
might have is our an a we use the Aten
pipe we pass in a number of skeleton
structures scale one scale tues however
you like what's going to do is create a
pipeline linking all these some
skeletons together passing the inputs
that we've got are applying scale due to
that puts together a pipeline process
tree and the result of that is going to
be the result of applying the pipeline
to each of the inputs in turn where the
skeletons the operations scale one scale
to these are execution and parallel or
you can do something similar for a form
if you've got em instances of the
skeleton wow what you can do is to apply
these in parallel to some input set of
tasks and we get out the corresponding
outputs from the fonts this just
distributes are the inputs according to
the to this structure or from a map it
is very very similar except with map so
map looks a bit like a form except this
decomp and recon phase well that's doing
splitting the list or other data
structure down into component into
smaller structures that it sends off
to each of the tasks in parallel they're
going this problem of overloading to
small granularity with the tasks so you
can split and recombine your input and
output great so little example and
colony optimization what an ant colony
optimization does is to try to find a
good solution to particular scheduling
problem and so there are n factorial
possible schedules for n jobs and
solving this is np-hard which is
computer science speak for my god that's
difficult or worse that affect when I
see something like this I leave it to my
colleagues who know better than I do
know how to deal with it it's a complex
problem the basic idea that we're doing
is that we're going to model a number of
ants moving through some set of tracks
and the ants basically are going to go
down different parts in the anthill as
they go down they lay farrah mean trails
as we discover good solutions what we do
is we mark that trail with the the
unmarked trail the Farrah mean the more
ants that go down a particular trail the
stronger the track and eventually we
find the best solution we hope so it's
basically some sort of heuristic search
process that we're doing with this
example not a classic Erlang example but
what I'm trying to say to you is there
are things you can now do in Erlang you
might not thought of doing okay so we
can do some really interesting stuff so
ants can be paralyzed using parallel map
that's pretty obvious we've got some
input coming in what we do is distribute
the ants across the data that's coming
in so here's our ant colony and having
done that what we need to do is to pick
the best of them this is basically to
work out the strongest trail we do some
sort of update process and we feed that
back in for the next stage okay so this
is a rather more complicated structure
than we've seen so far
the code for that in Erlang using our
scale library looks like this the
interesting bit is the red stuff here
we're creating a pipeline and what
pipeline is doing is basically this bit
here of the task and picking the best
and updating it and then a feedback what
the feedback is doing is iterating over
the pipeline structure so the feedback
is basically doing that part so a couple
of relatively simple structures what
have you been able to do is to strive
this really rather complicated parallel
algorithm and hopefully get a good
implementation for it now we passed that
in scale do with the inputs and off it
goes in the ant colony just does its
staff spits out answers to us that's one
example we're going to now is show you a
second example very very similar
slightly more complicated also met
heuristic approach for optimization
that's been done by our colleagues at
agh and Poland they're experts on search
strategies and what they are doing is to
use the similar approach someone I
described except that we have a number
of agents each of which is located on
some evolutionary island where the
islands as with jarvinia and evolution
with Darwin's finches are bare
disconnected from each other but there's
some way of apps and exchanging
information so the agents can do things
like die reproduce my great fight etc in
some order more trying to do is to use
this to basically find the best solution
to some heuristic problem so basic
structure you gently show the detailed
this some sort of tagging process we
might want to migrate some of the agents
between islands then group them are the
agents then do their stuff we apply some
shuffling operation to that and we just
go round the process again so you see
this is very similar in structure to be
ant colony how do we do with that first
stage is to discover some
options more we might discover is well
these agents over here we've got quite a
few of them let's just form them so
pattern discovery might say that's a
good thing to do now unfortunately these
three things at the top it's not easy to
do those at once there may be
dependencies between them so what we
might then have to do is some sort of
program shaping where we're simplifying
the code structure at the top level to
put into the right shape for the
parallelization and having done that
what we can then do is three fatrick
gain and now we can put in a pipeline
between here here and here between the
initial phase the agents phase shuffle
phase link it all together with a
feedback skeleton and that involves
manual refactoring unfortunately patent
discovery tool can't discover patterns
there is complex as this yet so you'll
have to do this by hand so we ended up
with a much more complex pattern
structure so we've implemented this at
agh in language got four different
implementations sequential one where we
started a concurrent one which is the
obvious way to do things using Erlang
but then we've applied skel just to show
how the scale approach works so with
this feedback it's actually a pattern
discovery etc and finally we did a bit
of manual work on the scale version to
tune it a bit and we introduced some of
the sequential parts to do with the to
deal with the islands because he found
from running program that was a good
thing to do here's how this looks in
terms of speed up what you can see is
the concurrent application here this is
a low ratio computation communication
what's happening here is that the
concurrent one is bottle is topping out
fairly quickly there's not much
scalability on that lot of data being
shared
if we just use scale we get much better
scalability it's going up to 40 or so
this on a 64 core machine and who then
do some hybrid ization you can make this
you can get a much even better scaling
figure we're getting performance of
about factor of 60 on 64 course which is
pretty good that's key going to keep
going up to quite a high level so we
thought this is a good result but that's
not the entire story be here are some
figures that tell you how much this took
to how much Chinese took to implement
social code it's 85 lines of code it
took about 10 hour 10 days of coding
effort to get that right enrolling by
experts the concurrent code on 353 lines
and total lines including the 85 took a
further seven days just to do that work
just to make this version concurrent to
fix all the bugs etc using scaling
contrast we've added a hundred lines of
code sets simpler than the concurrent
version because you've got the library
code sitting around there to do the
complex stuff and that only took a day
of effort and the hybrid scale version
took another two days of effort to tune
it so by using scale not only do get
more efficient code more scalable code
but you also get the code much faster
takes less effort to do that it's being
used as within campagna it's being used
within an ad word planner so it's being
used to sell adverts to us on an asst
available through the google play store
so you can get this you can run your app
campaigns using our software using the
evolutionary algorithm that I guess
showed to you so it's really it's a real
example its commercial semi commercial
very pleased with that so just to
conclude we've successfully paralyzed
applications from a lot of different
areas not just once I've shown you but
several other places too including
classical algorithms Castle high forms
algorithms programmer productivity has
been significantly increased by the
availability of both existing patterns
and new generic ones doing specific ones
speed ups for getting their clothes in
some cases to the theoretical value the
automated system produces the false
results that are practically as good as
an expert parallel program I with almost
no effort that's a nice result if you
have the automated pattern discovery
then it can indeed discover some useful
patterns they can really help you so you
don't have sort of scratch your head
poke around in the code and try to think
where's the best place to put the
patterns used in production code and
what Chris showed is a villip ido
framework also lets us to map some of
these things on two GPUs as well as CPUs
so we can now accelerate our parts of
our line code provided they fit the GPU
framework we can get real performance
boosts there at sensible energy usage so
we're pleased with that ok now it's not
just about big systems of course the
same principles hold for small systems
and even mobile phones are becoming
multi-core so don't think this is just
about big systems these techniques apply
equally world small systems even mobile
phones have GPUs etc and if you don't
believe me and you're right to be quite
skeptical because I'm giving you a sales
pitch here right please go and talk to
one of our happy customers Ramsey from
sheffield university or anything
Ramsey's in the audience but what Ramsey
said to me and he's been telling other
people is scale is great is fantastic
all I had to do was just pop it in my
program went much faster and I was able
to exploit some high force computing
resources that were available to me and
it would've been a lot work for hatchet
by hand so he's going to chat to Ramsey
to get further endorsement lots of
industry interested in this this is my
new project so we've got IBM interested
in exploiting these ideas are this
product is just starting on res are the
companies
leave it up for a second so this is
gaining commercial interest thanks very
much full contact each other we have
time for some questions
I looked at your code it wasn't
beautiful well so so to chance that one
is it doesn't have to be because it's
automatically generated are you
complaining about our library code or
the code that's been inserted 0 will be
the example we've just inserted we've
done this basic for illustrative
purposes to show you what's going on you
could obviously have cleaner code there
so yes we could improve the code our
that's being written we can make it tidy
if we wanted to it's only possible have
better refactoring tools where
researchers right so we we just want to
get the job done and that was quick and
easy quick and dirty yep you can
certainly tidy it up you could have
libraries you can have abstractions but
we want to make everything visible well
I think one of the important things
about what we're doing is we don't have
anything up our sleeve right and you can
just go in you can see the code you can
see exactly what's going on but yes if
you have any ideas for improving the
code quality please tell Chris will be
very happy the questions about
scalability so the highest we've
currently got with our line is about the
64 here so we're quite pleased for
getting 60 for scalability there are
weevil I didn't say we've also done the
same thing in C++ and we've used
slightly different okies distribution
mechanism MPI in C++ we've run the code
on 1500 calls on a 33,000 core machine
and we're getting good linear
scalability on on that okay so the nice
thing about pat approach is because it's
decomposing then things will scale so
because you've broken the dependencies
you can have much more parallelism
so inns in the C code yes in the Erlang
code we haven't yet but there's no
technical reason why we can't such as a
matter of practicality we need to do it
good question I hope to see airline code
running on a megacorp machine sometime
very soon someone just has to give me
access to it I have a question also does
the tool give you any hints on when to
use this because I guess there's a lot
of overhead to actually have a process
and send a message than just having a
doing a quick loop in line so you're
saying well what you need you is to you
need feed-in performance information so
we party the party tool the patent
discovery tool indeed will say to you
yes this is worth doing these are the
what it will do is to profile parts of
the code it will try to work out some
it's got cost model tells you about the
costs of execution under the different
models models say is well I think if you
choose this particular passion at this
point then the performance is going to
be the following it's sometimes wrong
because it's just it's academic right
it's theoretical but it's a predictor
and and it and it helps oh yes you can
do that I would certainly recommend
using the party refactoring tool the
pattern discovery tool is a very cool
piece of technology which you could
always measure things by hand if you
wanted to it makes a point to parallel
line something which is tiny most of the
time any more questions
okay one question regarding the dynamic
scalability so Emma cracks that
basically your pattern recognition
allows you to basically suggests a
multiple you know different pardons to
improve almost secure program but what
about you know dynamically adding more
nose or descaling verification at some
point so did you consider something
doing something similar to that so for
instance not just running on the
multi-core machine that has some
specific configurate number of course
but basically running something in
clouds that you can insert more cores
when you need it and like just gay all
the number of processes when you don't
need them right so correct question is
can we basically scale the system
dynamically to include more calls and
system so you're running low on effort
and you just add another node in you go
gear down to your hardware shop and you
pull out another server right and plop
it into the system so we haven't we
haven't at this stage done that because
we have got the distributed system in
there we've done something similar in
other settings the complication is
usually not adding in and not adding in
stuff is what happens when you decide to
throw it away okay so what you've got
then got to do is to basically say okay
I've got these running computations on
two different chains now what I have to
do is to grab them off this machine
migrate them back onto one server and
then keep going and that can be quite
slow the nice thing with the approach we
have is that because the tasks are
decomposed like this because we have the
bricks you can send smooth the bricks
back I'm plying the code structure stays
the same you can just keep executing
okay so we haven't done it but that's
definitely possible with the model that
we've described mobile disguise very
flexible okay thank you Kevin thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>