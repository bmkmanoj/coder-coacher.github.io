<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Everything Will Flow - Zach Tellman | Coder Coacher - Coaching Coders</title><meta content="Everything Will Flow - Zach Tellman - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Everything Will Flow - Zach Tellman</b></h2><h5 class="post__date">2015-11-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Yj5HnSXWbJ8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">oh hey we're here to talk about Q's and
many of you may be wondering or at least
wondered when you saw the abstract for
this why because you know Q's are pretty
simple right there they sit there in
between the producers which end cue
messages the consumers which DQ messages
usually but not always in the order in
which they're added and the reason is
that Q's are kind of everywhere right if
we are trying to do anything
simultaneously like for instance handle
incoming HTTP requests we handle that in
Java for instance by creating a thread
pool and a thread pool is just the act
of putting a Q and B come in between the
incoming requests and the threads that
are all simultaneously reading off of
that same queue and under the covers
this blocking queue actually has not one
but three queue like data structures
inside of it we have the buffer which
has the FIFO sort of semantics that
we're familiar with but it also has
these things called conditions or
condition queues and these condition
queues are called
not full and not empty respectively and
these were used when the buffer is
either full or empty if we try to add a
message to a full buffer then we are
forced to park ourselves at the not full
condition which will get triggered
whenever the buffer loses one message
and then one of the threads which is
waiting at that condition is woken up
and can proceed and under the covers the
conditions are implemented somewhere in
deep in the JVM I'm not entirely sure
how but you know this is built on top of
a bunch of queues that run within the
JVM and the threads themselves are built
atop queues that are within the OS for
scheduling and even those when we get
down onto the hardware run on cores
which have their own sort of ability to
do scheduled for hyper threading and
other purposes and so between us and
everything that we want a computer to do
there about half a dozen queues at least
and so you know this is a ubiquitous
structure right this is something that
we use everywhere and the reason is
because queues allow us to separate the
question of what we want to have happen
from when it happens and this is really
important because outside of some very
controlled environments we as
programmers do not know
the context winter code is going to run
right we know that for a particular
request we want the sequence of events
to happen we don't know when we're gonna
get a request we don't know how many
requests we're going to get we don't
know what the relative priority is and
so the only sort of option that we have
is to talk about this in the abstract to
say we want this event to happen at some
point and then allow the semantics of
the queue and this sort of execution
modeled our system to determine exactly
how that plays out and so given the
ubiquity of this I think you know it's
important for us to understand queues
how they work how they fail and
fortunately there's a sink called
queueing theory unfortunately it is
mostly a branch of Statistics and it's
very little to do with software and you
know this is a slightly snarky slide I'm
by no means a you know leading expert on
queueing theory but I am someone who's
taken you know two or three pretty solid
attempts to understand it I'm someone
who's written a number of systems and
what this talk is is the things that
resonated for me the things that I read
and I wish I had already known with that
said you know there are more sort of
formal references for this if this is by
far the best book that I've found it's
about 90% pure math but it does try to
ground out into sort of more real-world
examples and if you find this talk
interesting I highly encourage you to
read this because it's very likely the
things that resonate for you will be at
least slightly different from the things
that resonated for me with that said one
of the core distinctions in queueing
theory is between closed and open
systems in a closed system there's only
one thing happening at a time the
producer will give some sort of task to
the consumer the consumer will work on
it it will come back and only then is
the producer allowed to introduce
something new a very common real-world
example this is a shell prompt right we
type something in it goes away it works
and delivers another prompt another
example of this is a single person
browsing a website if someone clicks on
a link they have to wait for the next
page to be delivered before they can
click on a new link right these
certainly exist
and we do build them and we do interact
with them but by and large they're
fairly easy to reason about and so the
real sort of scary thing right the thing
that we struggle with are what are
called open systems where there's none
of this sort of coordination there
none of the seating of control from
producer to consumer this is what
happens when we open up a web server to
the world right requests come in and
while we're really trying to like finish
one another one shows up before already
this is you know inevitable and so this
talk is mostly going to concern itself
with open systems and I spend some time
trying to figure out how to best
introduce the semantics of open systems
and it turns out that using real-world
examples is not a very good way to do
this because real world systems have not
one cue but hundreds of cues all sort of
interacting and very difficult to
understand ways and so what I'm gonna do
first is show you a simulation right
this is going to be one cue behaving in
a way which sort of tries to mimic how
the real world works but you know has
all the caveats to come with a
simulation in the simulation producers
are exponentially distributed and what
this means is that the time between each
event showing up follows an exponential
distribution and an exponential
distribution has a very particular
property which is that no matter how
much time elapsed is we never become
more likely to get an event so if we
expect to have people show up once every
minute and then an hour elapsed is it
doesn't mean that we're suddenly do
there is a very particular sort of
fall-off to this curve and this is a
good model for sort of events coming in
from people who are not coordinating
with respect to each other people who
are not aware of what other people are
doing so this is a good model for how
often people show up to banks or to food
trucks or make requests to web servers
right these are sort of many independent
actors when we talk about the complexity
of the tasks that show up though
exponential distributions are not a
particularly good model because they
have this very particular shape which
maybe doesn't model the complexity of
our tasks and so a more more common
distribution to use when we're talking
about the complexity of the incoming
tasks is what's called the Pareto
distribution I'm assuming most of you
heard at one point about sort of the
8020 rule right this is a particular
sort of ratio a particular type of
Pareto distribution that has a ratio
between the head of the distribution and
the tail of the distribution right but
for different sorts of systems we're
going to have different sorts of
relationships between these sort of more
common smaller tasks and the less common
larger tasks and so for the purpose
of our simulation we're going to have a
bunch of arrival times which are
determined by an exponential
distribution and what this means is that
even if events are showing up at for
instance 1,000 recorders per second that
doesn't mean that they're showing up at
regular 1 millisecond intervals right
they kind of pile up on top of each
other and then their long periods of
time or nothing happens and the
complexity of these tasks over time is
going to turn from something which is
relatively thin tails right has a lot of
very simple tasks and very few complex
tasks to something which has a fatter
tail or is on average more complex and
also has more of these sorts of outliers
showing up and if you're looking for a
real-world example of why this might
happen there are tons right maybe our
cash got flushed
maybe we're now reading from disk at the
same time somebody else is maybe there's
a network issue maybe one of our
downstream services is in the middle of
GC right there are countless reasons why
this might occur and when we're looking
at the behavior of the system I'm going
to use something called the latency
spectrogram and I don't expect that many
of you have seen this before so I'm
going to take some time to explain it
the vertical axis here represents
latency or a logarithmic latency and
latency is defined by the time that
elapses from an event showing up at
traversing the queue getting consumed
and being completed the horizontal axis
here represents the time and
specifically these sort of transition
from this thin tail to a thicker tail
sort of distribution the color
represents the density how likely it is
that something sort of falls within that
particular layer and it starts with red
as the most likely and falls off to sort
of the cooler colors and so we can see
here at the far left that we have this
very thin red strip right this is a
system which is behaving normally there
are some things which are taking longer
but by and large we're being able to
have a very consistent fast sort of
handling of this but as the distribution
begins to change over time we see about
two-thirds of the way through it leaps
up and they start to sort of get
scattered around and this is still
stable or largely stable right but we're
responding now maybe one or two orders
of magnitude more slowly than we were
before
right we're really starting to kind of
get under load but this continues and
eventually we just sort of completely
lose our ability to keep up and we take
off right and notice that this sort of
has
this very large arcing trajectory and
it's suddenly what was very diffused
became very very tight right we have now
very reliably slow response and this is
because the time spent on the cue now
dominates right everything is on this
sort of constantly growing queue and so
whereas before we were relatively
sensitive to the variations the queue is
average it all out right we we are very
very reliable but you know growing ever
more slow and of course this wouldn't
continue forever eventually we would run
out of memory or something some sort of
fixed resource that would cause our
process to fall over this is a system in
crisis and we know what we do when we
have too much load coming in right we
just add more machines and so this works
exactly like you would expect if we add
one more machine we can see that it
remains stable and there's the fact able
to respond much more quickly and much
more reliably even at these sort of
far-left but as it sort of continues we
see it start to flare up maybe if this
trend continued over time it would also
fall over when you have four machines
were barely phased right and this is
intuitive this is something that we
would you know expect this is sort of
how we respond to these sorts of outages
in the real world a less obvious thing
though is what happens when as we
increase the number of consumers we
increase the rate of incoming messages
by a commensurate amount so here we see
that we have one producer in one
consumer and for producers and for
consumers in 16 and 16 and notice that
all of these fail at exactly the same
time which makes sense right that is the
point at which we are sort of
outstripped no matter what these sort of
coefficient here what I think is more
interesting though is how they behave
leading up to that point of failure
right we can see that with only one
producer and consumer it's very sort of
diffused and then begins to kind of grow
more chaotic and before it finally falls
over in the case of the four it you know
has a little bit of a lurch closer to
the point of failure and then falls over
in the case of the 16 it's like almost
unexpected it comes from nowhere right
it's behaving very very reliably up
until the point where it falls over and
sort of the intuition here that we need
to have when we reason about this is
that when we have an unusually complex
task that comes in things and the queue
will begin to sort of grow behind it
right if we have one consumer and a
the unusually complex task comes in
things begin to back up behind it and
now the time that they're spending the
latency begins to be defined not just by
their complexity but by the complexity
of everything in front of it which means
that even the really simple tasks
sitting behind a complex task become
very slow to computer but if we have
more than one consumer were less hobbled
by the fact that we had this thing come
in right maybe we only have three out of
the four consumers to sort of make
forward progress but that's something
right in the case of the 16 consumers we
have to have this very unusual
confluence of events to actually be
stuck wholly right and so this is
important because the variance in our
response is even under sort of more
typical loads determines the stability
of services which are upstream of us
right this sort of change in the
distribution that we saw that caused us
to fall over could easily be the thing
that causes somebody else to fall over
even if we're still behaving in a
largely correct way and I want to point
out that the 16 consumer case is by far
the closest to the systems that we build
right we have multiple machines we have
multiple threads and so this sort of
sudden failure is the norm so what we've
learned so far is that your system is
probably an open one at least it is if
you're worried about it and it's the
long tail the unusually complex task
that makes the queues fill up and that
means that when we're looking at the
complexity of tasks looking at the mean
or the median is absolutely the wrong
thing to be looking at right we need to
be looking at the upper sort of tiers of
these distributions looking at the 99.9
Plus sort of tasks because these are the
things that dictate the behavior of our
system and more consumers will make us
robust to these sorts of outliers but in
either case when it falls over it falls
over hard right because there really is
no substitute for just being able to
keep up and so the one thing that we
have learned from this and if you take
nothing else away from this talk take
away this it's that unbounded queues are
fundamentally broken because they put
the correctness of your system in other
people's hands right we have in having
unbounded queue into sort of leaving it
there we make certain assumptions about
how our users are going to behave but we
don't tell them that right and even if
we do tell them that they're not
necessarily going to respect that and so
this is just a failure mode that we have
left wide open and it'll happen
eventually
it's a given and so it's not sufficient
to just
hold onto all the data we have to have
some bigger better plan for what we do
when too much data is sort of shows up
at our door side door yes doorstep and
so the three things we can do are we can
drop the data or simply get rid of at
the moment it shows up and we can't
handle it we can reject the data which
is to say drop the data and respond back
to the producer that we dropped it and
we can pause the data which is to say to
the producer we can't handle this right
now I'll let you know when we can so
dropping is something that we can do if
the newer data showing up makes the
older data obsolete this is true for
certain forms of telemetry this is true
for you know a very sort of narrow set
of cases it's also true if we just don't
care right it's great if we get the data
if not then so be it
I think that we tend to say that this is
a case way too readily one really common
example of this is that that's D which
is a monitoring tool uses UDP as its
typical sort of transport mechanism and
UDP is a best effort delivery system and
it gets there great if not Soviet and
the problem here is that we are trying
to understand the state of our system
and when we are most in need of
understanding our system and high
fidelity we are least likely to get it
because if the system is in crisis then
you know the data is not necessarily
getting where it needs to go and I think
a lot of people tend to say oh it's fine
we'll just drop the data if we can't
handle it because they assume that that
will never happen right the assumptions
like you know I guess in in theory we
might have to drop the data but in
practice I've never seen it happen like
and you know in some cases might be
invisible them if it's happening all the
time but if you hear that distrust that
don't you know assume that it's true
that this will never happen in practice
so rejecting is oftentimes the only
thing that we can do this is what
happens when a server returns a 503 this
is what happened when you know Twitter
returned the fail well back in the day
this is a system saying I got your
request I don't even know what it means
all I know is that I can't handle any
more right we do not even have the
resources to more gracefully like do
anything than just say go away and come
back later and often this is the only
thing that we can do in an application
and the sort of semantics of that right
exactly how we do this what sort of page
we return how we messages to the
customer is an per application sort of
decision right this is something that
needs to be sort of discussed and
designed and everything like that but
oftentimes when we build these sorts of
tools we're not building it with respect
to a particular application we're
building something we want to be able to
use in a variety of applications and in
that case we cannot make the choice to
either drop or reject the data because
that's only something that someone with
full knowledge of what we're trying to
accomplish could do so the only thing
that we can do is we can defer the
choice and this means we can posit a
tour or apply back pressure right we can
tell whoever's upstream of us I don't
know what this means to you but I
certainly can't handle that and so that
tends to sort of propagate upstream
until it hits some point where someone's
like you can't handle that I know
precisely what to do right I can drop
the data I can reject the data I can you
know do the thing that is sort of
application specific and this is why in
the Java implementation of the queue we
had not one but three different sort of
queue like things because the puts right
the et not full sort of condition here
is where things that have had back
pressure apply to them are waiting right
that is where they sit until we tell
them that they can continue and so we've
talked about queues and what I've said
is that queues are useful for separating
what from when right we can also say
that it's good for addressing
potentially unbounded demand with
bounded resources we can say that it's a
way of putting interfaces between
systems which may have very different
understandings of like how much work
needs to be done and these are all
saying pretty much the same thing and
the point that I would like to make is
that the buffer which are the thing that
we kind of think of as a queue right
this thing with the FIFO sort of
semantics it's actually totally
irrelevant to any of these properties
right we don't need it to be able to
have this sort of separation of control
and so we can make a queue that has no
buffer right that is an unbuckle and
this has all of the nice sort of
semantics that we have and the reason
that this is nice is because unbuffered
queues are a closed system right this
thing that I said that we don't always
get but it's really nice when we do
right because now we have a perfect
ceding of control right the producer
cannot make for
progress until there is a consumer
waiting for it and so in practice we
want our cues to be on buffered wherever
possible especially within a certain
process because it's much much easier
for us to reason about we don't have
these sorts of amplification effects
that we get when one buffer makes
everything a little bit slower and that
makes things a lot slower upstream and
that can happen sort of you know as many
times as you put these cues together and
so you may be wondering why I have
buffers right and the answer is that
back pressure sort of enacting back
pressure and retracting back pressure
takes time it takes effort right if we
want to restart a thread that we set
aside we have to copy the stack back
into memory if we want to restart TCP
traffic we have to send a message across
the network and wait for it to sort of
spin back up we have to you know read
from disk or from a database to get this
stuff because the time that elapses
between us being ready to do work and
actually having work to do is wasted
time right it hurts our throughput and
so a buffer is useful in that it allows
us to maintain and higher throughput
have a more consistent throughput but
that is at the sort of cost of having
potentially a larger latency because
these things have been sitting on the
queue waiting to be handled so what
we've learned so far is that you should
always plan for too much data this is
going to happen you should know what
you're going to do we should use back
pressure wherever possible because that
is the most neutral choice right it
allows somewhere else in the system make
that decision and in practice that
decision should be made in one place we
don't want to have a lot of different
sort of dropping or rejection strategies
interacting with each other we should
have it defined in one place which makes
it easy to change easy to understand
buffers can give us throughput at the
cost of latency and this is something
that like the sort of choice of how to
respond to this buffer should exist in
as many as few places as possible right
maybe you have one big buffer at the
sort of periphery of your system but
having many different buffers makes it
hard to think about it tends to amplify
these sort of latency effects if you
want some more information on this
search for TCP buffer bloat this is a
very good real-world example of what
having many composed buffers together
will give you and so in practice most
queues should not have buffers
a concrete example of this is a system
that I worked at at my previous job
where I wrote a service called ma and ma
was sort of an omnivorous data service
where people could post data to us and
we would write it to a disk for later
batch processing it was very simple did
very little just had to kind of get the
data to a place where it was safe and at
rest and replicated and so all we really
need to have here is some sort of thing
where all the data comes in it gets
balanced across a bunch of machines they
do some minimal processing on the
incoming data and then write it to some
sort of store and because I was trying
to make my life as easy as possible we
decided to build this on top of Amazon
which gives us the elastic load balancer
to spread this across all there and can
generally handle a lot of volume if you
ask Amazon nicely and s3 which is a
perfectly you know good replicated data
store and so this solves a lot of the
harder problems that might fall out of
this and all was left to me is to sort
of build the thing in the middle and so
some key properties about this service
was that it had to be horizontally
scalable over time the sort of volume on
the service grew from about 40,000
requests per second to about half a
million queries per second and this was
something which just sort of had to be
able to deal with it had to be low
maintenance this was not the only thing
I was working on if it took more than a
day out of every month and then
something had gone seriously wrong it
was not real time we were trying to do
batch processing the data had to get
into some sort of persistent store
eventually but immediately was not a
requirement and the most important thing
was that the loss of data was directly
proportional with lost utility this is
something where we're trying to do sort
of high-level pattern analysis and so
losing 1% of the data would lose us at
most
1% of the utility of this data this is
very different than if we were doing
payment processing we're losing 1% of
the data would lose us the entire thing
and so this makes the job much easier
which is exactly what we want and so
previously I'd sort of thrown together a
little background process that would
consume the Twitter garden hose which is
10% of the Twitter firehose and this was
consuming about tuna rather 600 tweets
per second at peak
and so what I did is I took an HTTP
client I piped it into the Hadoop s3
client because that's what we had on
hand and wrote it to s3 and this worked
perfectly well and so for my initial
sort of v-0 of this I took a web server
built on top of the Java nettie library
I popped in they do best three client
and you know we're off to the races the
problem was that I was previously
unaware of the fact that s3 would
sometimes stop accepting rights for a
while
we don't know I don't know why but this
is a perfectly reasonable thing for a
downstream service to do what I also
didn't know is that the Hadoop s3 client
in response to this would simply
accumulate messages in the hopes that s3
would become available again who'd be
able to flush and catch up and
everything would be great and you know
we wouldn't have lost any data but in
practice what would happen is that the
queue would grow until we ran out of
memory we would start you know oh oh I
mean the process would fall over the
traffic would be load balanced on to the
other machines which were equally unable
to talk to s3 and the entire service
would just stop and so clearly this was
not a good approach and so when I
started thinking about it from you know
more first principles I realized that
I'd been making this job a little bit
harder on myself and I had to because we
wanted to eventually get to a3
but what we're trying to accomplish over
the course of accepting a request and
coming back and saying ok we handled
your little new chunk of data was it had
to go somewhere that wasn't memory so we
could write it to the local disk and
then in the background take from local
disk and upload it to s3 right we just
had to get it there eventually and the
fact that if it's on disk and the entire
sort of host dies then we might lose a
data but that's ok we're okay with
losing a little bit of data we don't
want to make a habit of it but so be it
and so we for this I wrote a couple of
libraries that were open sourced
these are closure libraries so you know
use that if that's what you use but
they're pretty simple one of which is a
thing called the durable queue which is
what it sounds like it's a append-only
disk back thing that uses you know slab
allocated files and memory mapping and
other things that make it reasonably
fast and on top of that something that
journals data to s3 by first writing to
the durable queue and then pull
the chunks of work as they come through
and so this system as it was was much
more stable because all we had to do is
write it to disk and disk gave us a lot
more space than memory but just because
I'm solving an easier problem doesn't
mean that it was somehow guaranteed to
work and so arguably one of the flaws of
the Hadoop s3 client was that it didn't
you know ever let go of data right it
just you know kept on accumulating had
this unbounded queue that was
fundamentally broken but another father
had is that it never told me that right
there were no metrics I the only way
that I could actually infer that was
what was going on was by waiting for the
failure and then having to go and pick
up pieces and figure out what happened
right there was potentially a way that
it could have sort of broadcast said he
was in distress
not enough distress to fall over but
still something that would have you know
told me that this is what was going to
happen
and so for these libraries I was made
sure that there were reasonably rich
metrics for this right and the case of
the s3 journal there are some fields
they'll tell me how many you know tasks
have been in queued how many of them
have you successfully uploaded for the
underlying queue it has some more
in-depth stuff like how many slab files
are there on disk how many are loaded
into memory and so this is great right
we have metrics now that tell us about
the part of the process which is taking
stuff off of disk and uploading to s3
but there's still the other component
right where we accept the data and we
respond back as quickly as you can okay
we got it we handled it and this is
important that we're responding quickly
because if we're not doing that then our
partners were going to sort of ramp down
the volume of data they were sending us
because they would assume that we
couldn't handle it and so we needed to
make sure that what we were measuring we
were measuring both sides of this system
and one of the sort of interesting
complexities here is that nettie the
underlying library has a model where
connections as they are opened our
signed it to a particular thread and
this is very useful because it allows us
to not have to worry about
multi-threading it allows us to use non
thread safe operations and the sort of
context of a particular connection but
it also means that if any one operation
blocks it blocks a bunch of other
connections
have nothing to do with what it's trying
to do and because we are interacting
with the disc then things can block
right this is something that we should
you know plan for and so the solution
for this is that you go and you use
another cue you put a thread pool behind
this and as requests come in off the
wire you for them in where they can be
handled by a separate thread pool which
decouples them allows you to block on a
particular request without causing a
bunch of other connections to you know
be similarly affected and when we talk
about metrics typically the beginning of
the request lifecycle from our
perspective is when it finally comes in
off the cue right where we begin to
actually act on this and a lot of you
know requests response latency metrics
will measure only this part but this is
a problem right because again cues
introduced latency we know this and we
need to be sort of aware of what's going
on
but thread pools in Java are not very
well instrumented and so I ended up
writing another library which is a Java
implementation of instrumented object
pools and thread pools and has some
mechanisms that allow you to take the
telemetry coming in off of that and use
it in a feedback loop to resize the
pools and this helps a lot because it
means that whenever we ran up on these
servers we could have a nice little job
JSON endpoint that tells us a whole
bunch of things about the functioning of
the system for instance it tells us the
rate at which tests are coming in
they're being completed and being
rejected and notice that these are not
single scalar values these are
distributions because again these things
are exponentially distributed sometimes
events show up all at once and so if we
sample this at a small enough time slice
we'll notice that for some time slices
at the 99.9 percent tile we're getting a
hundred thousand requests per second
within this sum this little time period
but the median is that we're getting
30,000 and this is important right again
we're trying to understand sort of where
what is the maximum load that our system
is having to deal with likewise we can
look at the task latency in the queue
latency where the task latency is the
overall time it takes for something to
clear the queue and get read and the
queue latency is just a time that's
spent on the queue and you'll notice
that the upper end of both of these
distributions are different the ickey
latency is actually a lot
even though that's supposed to be a
subset and this is because this is
sampled and both of these are looking at
slightly different samples but the other
sort of interesting thing here is that
the queue length is zero because we are
using an unbuffered queue and yet
sometimes apparently it takes us two
milliseconds to traverse that empty
queue and that's not because we're
waiting right if the queue actually had
nothing on the far side of it we would
just reject the request outright and so
this means that somewhere even though
we're not on buffered somewhere in the
bowels of the JVM there is a buffer
we're traversing it we can't help it and
so you should always be clear that like
just because we've decided to make
something a buffer that doesn't mean
that there's some sort of instantaneous
you know execution semantics here and so
now we have better metrics we don't have
complete metrics we don't have something
that actually measures the entire system
because the system's not just us it's
also the client that's talking to us and
maybe someone makes a request and drives
into a tunnel and it takes us 60 seconds
to actually get to them and this is
something that we could measure it's not
clear what we would do with that
information if we knew that people were
doing this very often but you know
there's always more for us to be able to
measure here and so what we've learned
so far is that we should be very clear
about what it means to be complete in a
particular task because if we can move
the goalposts closer that just makes our
job a lot easier
we should be very picky about our tools
we should make sure that they don't for
instance have unbounded cues lurking
somewhere in them but we should also
make sure that they tell us what they're
doing because if you're looking at a
system and you're trying to make it fast
right you're building a system from
these pieces building it on top of
pieces which are individually fast
because you found a benchmark that said
they are individually fast it doesn't
necessarily turn into some sort of
Gestalt which is itself fast because the
assumptions made of the benchmarks even
if they very closely resembles what
you're doing it's not exactly what
you're doing right and so you should
always prefer something that tells you
what's going on as opposed to something
where you know you have been assured
that it'll be fast don't worry about it
and of course you're never measuring all
your all of your system but you should
measure as much as you can easily and
you should always had in mind sort of
the next tier of things that you could
be measuring if the system begins to
behave oddly so one last example here
a lot of networking frameworks when they
are trying to give sort of an
introduction and we'll use this example
which is a chat server something which
is a small subset of IRC or something
like this you saw this a lot with node
when it was first being rolled out and
what I'm gonna use for this is a
language called closure you probably are
not all deeply familiar with this I
don't think it's important as long as
the parentheses don't completely you
know cause you to roll your eyes back
into your skull but we're going to be
using a library here called manifold
which is has a collection of sort of
async primitives one of these is a thing
called an event bus which is an
in-memory publish/subscribe model and
that's really all you need to know so
our chat room that we're building here
has a connection this connection can be
TCP it can be a WebSocket it's some sort
of duplex communication socket and we
assume that the first message that is
sent to us by the client is the room
that it wants to join once we have that
we can set up to persistent flows we can
take all the messages coming off the
connection and publish it to the room we
can then take all the messages being
published of the room and forward them
back to the connection right very simple
but this actually gets us a chatroom
right it's fully functional but there
are some problems here right one of
which is that when we're publishing the
active publishing is defined as
successfully sending it to all the
people who might be downstream of us
everyone who's listening and so what
happens if one of our members of a room
drives into a tunnel or closes their
laptop or does something else that makes
them unable to receive messages they are
going to hold back everybody right
because we're only have as fast as sort
of our our slowest link here and so when
we are thinking about this we have to
consider like how do we go and protect
the majority of our users from these
sort of misbehaving minority and so it's
not enough for us to just say blindly
let's go and connect it such that
everything that comes out of a chatroom
is sent along we have to potentially
buffer write the buffer will give us
some amount of control over this because
it allows people to not be able to
accept a message now under the
assumption that they will be able to
eventually right this gives us sort of
increased throughput at the cost of
latency for the
individual user and we've decided here
arbitrarily that we're willing to borrow
42 messages and this might not be the
best idea because who knows what a
message is it might be a word it might
be a novel so a better approach would be
to create a metric that says how large a
message is and say what the maximum
aggregate size of messages are willing
to deal with here and here we say we are
willing to deal with a thousand units a
character a byte whatever but even this
doesn't really fix the fundamental
problem the fundamental problem is that
we will eventually run out of space the
buffer will be full and then we're
blocked again right and so we have to
have a hard stop something that goes and
just shuts us down and so we can
additionally say there's a timeout here
right this connect mechanism is taking
messages from some source of messages
and forwarding it into a sink and we can
define a timeout we say if we've been
waiting 10 seconds for this to actually
you know complete then we're done we're
just going to close a connection and
call it and so that finally puts sort of
an outermost boundary right it means
that this person can only misbehave for
a maximum amount of time before we're
just going to go and boot them but this
actually creates a different problem
which is what happens when someone
starts to send a lot of messages they
begin to spam the channel either by
accident or on purpose and this means
that now since everybody is sensitive
this now right if they can't go and
accept the messages at the rate that
they're coming into the channel
everyone gets kicked right we've just
set up a new sort of misbehavior and so
again it's not enough for us to just
simply take everything that someone
sends to us and publish it to the room
we need to have some sort of limit on
what they're able to send us and so we
can throttle it we can say we're only
going to accept one message per second
and this also leaves sort of a undefined
kind of semantics here because if
someone is allowed to send one message a
second and they're quiet for an hour
have they built up a credit of 3600
messages is that like something that
we're willing to let them send all at
once and by default this takes the most
conservative approach which is that
someone can send one a second if they're
quiet for a while that doesn't give them
any sort of you know credits but we can
define that we can say that you know
someone gets 10 seconds worth of
messages if they're quiet for 10 seconds
but no more
and so this is slightly more code than
what we had before and it has a lot more
magic constants in it right what we had
before was this sort of Platonic ideal
of a chat server but the other sort of
problem with this is that you know we
can imagine this chat server sitting in
the middle of this problem space and
there are all these different dimensions
here how many messages are showing up a
second how many messages are being sent
per client how many clients are sort of
misbehaving by sending five orders of
magnitude more messages than the rest
and there are points of failure on that
right obviously our system will fall
over as we get to some point on that
dimension but we haven't defined what it
is we've left it sort of up to the
faith's we've said you know some
combination of the contexts in the
execution and the hardware that's
running on and you know whether the
planets are in alignment will cause us
to fail at this point though tomorrow
will fail at a completely different
point and so again we've left the
correctness of our system up to other
people and by drawing these arbitrary
but hard lines in the sand we've defined
the problem that we're trying to solve
we've actually sort of carved out a
space that we feel to be the problem
that we want to you know have a good
correct solution for and this is not to
say that there aren't sort of other
implicit failure modes that were not
handling for instance we're saying that
we can handle one message per second
from every person but what if we have a
billion people connect all of a sudden
right we haven't defined that that is
still sort of a failure mode that we're
leaving up to the fates but we should
always try to be as specific as we can
we should always try to demarcate as
many boundaries as we can because this
gives us a better chance of having a
system that we can reason about and so
we talked about queues and one thing
that we learned is that unbounded queues
aren't unbounded they're just bounded by
something that we have failed to define
and when we're building an application
we should always plan for too much data
we should never allow our queues to grow
without bound but where we're not
defining a particular application we
should try to be as generic as we can
and that means applying back pressure
leaving the choice up for someone else
and we should always have metrics from
every part of our system every you know
component sub component because that's
what allows us to understand whether
not the assumptions that we've made are
valid because they're not they're never
going to be completely right and even if
they are then the problem sort of
definition will change out from
underneath us and so the only way that
we can cope with that is to actually
measure things not just limit things but
sort of measure and make sure that the
limits make sense still and so that's it
thanks questions it was a great talk and
I guess one one choice you've got is
about what your defaults are where we go
for sensible defaults so the experience
of using like a proxy same way it makes
you think about literally for everything
how many connections are you going to
take in total how many is that front end
gonna take how many is that back-end
gonna take at what rate are they gonna
come and the defaults are actually quite
low and people get frustrated and a
similar thing happens with Hadoop where
if you start using it suddenly you reach
all these limits and there's all these
knobs you've got it tweak but is it is
it I think it might be the case they're
forcing people to make the choice early
on it's better than having a sensible
default that you usually will work and I
wasn't sure from your examples whether
you've done that or whether or whether
you actually allow people to choose
rather than force people to choose it's
just the latter I mean so in many of
these cases there are some defaults but
generally speaking and you know when
you're dealing with the networking layer
where you're sort of not responding
quickly enough there will be back
pressure applied and everything like
that but you're still very much able to
exhaust memory or something like that
there's there's no automaticall way to
kind of detect that that's what you're
doing and save you from yourself and so
I think that it is better to have a sort
of dollop of documentation and education
about what these knobs are and what they
mean and why you should be sort of
tweaking them I think that Hadoop and
some others probably hit the extreme
point on this and sometimes it ends up
being a like make it work being set to
true instead of false and like that's
the thing yeah and and I think that
that's that's obviously very frustrating
and
having you know non-pathological
defaults is good but I think that having
people assume that whatever you set is
sort of this like one you know size fits
all kind of parameter is is bound to
disappoint and so yeah yeah it was
really great I've two questions the
first one is with the logo latency
graphs that you showed to provide the
example for the buffered queues is that
useful as a visibility kind of tool for
general systems that we might might work
with day to day that's my intuition I
have played around this a little bit I
mean the problem is that you know most
people the end the day they use graphite
and graphite is taking a time series of
scalar values and so doing distributions
is hard like you can have a bunch of
different scalars that represent
different percentiles but that's not
quite the same thing
and so yes but there's a lot of sort of
complexity and kind of getting there and
it's not clear to me that all that
effort is merited like I don't know how
much better it is right I've only ever
used it as sort of a retrospective where
you're going over a sort of log data or
something like that
having it as a real-time view seems
pretty neat to me but I don't know if
it's like genuinely like twice as useful
or something like that and my second
question was around if we follow on your
argument for using unbuffered cues and
to kind of annotate and use metrics to
guide a lot of that thinking to
understand what's going on with the
subsystems that we use were there any
common metrics that you found whether it
was runtime specific or otherwise that
you felt we should be definitely
tracking and that shows some of these
things the example being the kind of
latency stuff where for the 99.9 percent
all things showed up I mean I think
they're looking at the top
top end of your distribution whatever
you sort of define that to be and I
think that you know three 9s is a pretty
common one that people use that's useful
another one that I didn't show on this
visualization but I find very useful as
a utilization so if you have a fixed
number of resources kind of
understanding how much contention there
is how many of those are sort of laying
fallow and there's some math that you
know is a little bit idealized but like
in queueing theory will show that once
you get above like sixty or seventy
percent utilization the chance of
collisions where you know sometimes
temporarily like everything will be
exhausted becomes quite likely and so
you can kind of see that as a point that
gets approached and like understand that
is sort of the point at which you see
that sort of drop-off you you sort of
hit that other equilibrium where things
are you know stable but slower and so
that's a useful one but I mean I think
that that's very sort of case-by-case
and I think it's better to understand
your system and understand your metrics
then sort of assume that like what
worked for one will necessarily work for
the other so you talked about queueing
theory and what I've seen of queueing
theory before kind of models the world
as a fixed number of executors we live
in a sort of cloud world now where when
one of your metrics goes over a certain
threshold you can start spawning new
consumers just queueing theory have much
to tell us about those more dynamic
scenarios so yes I mean so unfortunately
because it is just like an idealized
sort of thing you find that like a lot
of times it's modeled as like you have
one consumer or you have an infinite
number of consumers and you don't have
hot in between there and you know maybe
the cloud services more closely
resembles the infinite number of
consumers that not but it's I mean if I
could like maybe rephrase your question
like can you actually do formal analysis
of all of these systems and predict what
the behavior is my answer to that would
be no and I think that it's much better
to take the system as it is stress it
with some sort of benchmarking tool
or something that will go and sort of
put it under load and simply observe its
behavior and try to kind of Intuit it
from that like that that at least is
what's been what's worked for me if
someone were able to come up with
something that like could accurately
simulate as I said like the hundreds of
cues that are all sort of working in
tandem that would be awesome that would
be very very helpful but I've not seen
any such thing thank you if you have any
more questions please take them offline
with Zach I think thank you for a very
interesting and stimulating talk</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>