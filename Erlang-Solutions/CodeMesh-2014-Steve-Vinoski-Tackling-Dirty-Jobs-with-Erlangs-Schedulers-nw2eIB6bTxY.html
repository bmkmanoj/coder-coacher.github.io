<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CodeMesh 2014 -  Steve Vinoski - Tackling Dirty Jobs with Erlang's Schedulers | Coder Coacher - Coaching Coders</title><meta content="CodeMesh 2014 -  Steve Vinoski - Tackling Dirty Jobs with Erlang's Schedulers - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>CodeMesh 2014 -  Steve Vinoski - Tackling Dirty Jobs with Erlang's Schedulers</b></h2><h5 class="post__date">2014-11-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/nw2eIB6bTxY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I'm going to hear our line programs
or aspiring Erlang programs how many are
C programmers okay there's going to be C
so don't be frightened how many you're
here because there were no chairs in the
other session so let's jump right in we
all know that Erlang is good for massive
concurrency fault tolerance that's the
kinds of things that usually hear about
early soft real-time of course the term
real time has been co-opted to mean
something other than real time but we'll
ignore that for now but it's also a
really great integration platform so for
years I worked for an Irish company
called Iona technologies back in the
corba days and all that stuff and that
was all about integration that's how I
founder language my job is doing
integration and always looking for ways
to integration I find ever lying and its
really good at connecting things
together and has a variety of interfaces
to let you do that so if you want to
talk to an external process some other
process in the operating system you
could use what's called a port I know
that term is overloaded I'm not talking
about a port like an important number
which is called a poor it's kind of like
a pipe pipe connects to the other
process and you talk to it through the
pipe something called c-note I think I
know rowing has a distributed bits of
distribute systems got a distribution
protocol built in so the sea know what
you can do is write your own code well
in see I guess for C note but it could
be any language like j interface lets
you do the same thing in java but lets
you write an early note or a note that
follows the Erlang distribution protocol
in some other language and
you taking third lane happen and that
I've done that in the past for some
integration Erlang also has built into
it tcp UDP and sctp networking so that
just comes part of the package for some
of the internals if you want to
integrate things into our life so not a
separate operating system process but
something that is right in the same
operating system process as airline
itself there's these bits built-in
functions that let you manipulate things
like quartz and pork drivers are things
that attach to the ports I mentioned on
previous slide we're talking about pork
drivers later right now we're going to
talk about something called a native
implemented function anybody ever
written for an if an if okay whew some
of examples of integration there's a
build tool in your line hold rebar rebar
uses ports for external commands it has
to run get you know to clone a
dependency or something it calls get
using a port with inner leg as a port
driver called inet drb and that is the
thing that gives you all the networking
that i mentioned earlier that's written
in C the Erlang vm is written in C so it
links right into the vm and that's where
all the networking is done if you ever
want to see some really interesting seco
go look at the inet driver at sea it's
huge it's like over 10,000 lines long i
believe and then another example 3 ops
ii leveldb which is a persistence back
end for react that's our in c++ leveldb
is a library that comes from google he
leveldb is the airline rapper
that turns it into an if and lets us
plug it into the virtual machine so
let's go through you know exactly what
an if is so you start with a regular
Erlang module imma show example of this
but first I'll go through it bullet by
bullet and within the module you have
functions just regular airline functions
and they can either have a default
implementation or they can just be
stubbed out throw an error and the nifs
the native implemented functions that
correspond to these airline functions
live in a shared library or diello the
module typically uses an attribute
called onload to tell to run a function
when that module is loaded running this
function and what that function
typically does is just low as the module
loads up the shared library and when
that happens the runtime goes through
the nymph which has this entry block
goes through the block and says for
every function in the module I'll find
the corresponding c function and i will
basically dr. up the bytecode to jump
into the sea code instead of running any
function that's in the original Erlang
module it's that make sense so if I
start diving into code and you do have a
question about any particular piece of
it please ask now it's kind of hard to
ask that at the end you go back to slide
2 we don't want to do that sure do you
recommend an alt mixing
okay so the question is do you recommend
mixing Erlang with nips so some of the
functions in the module are Erlang
functions and some are gifts and I think
that's perfectly fine I have an if that
i wrote years ago that's it provides
shot two functions for early and it's
not really needed anymore because now
it's built in but the way I did it was a
rope the airline functions they're all
capable but slow and then if you love
the NIF overrides all those with C code
that's fast so that's not a badly and do
it if you might have an environment
where you don't want to have the C code
or you can't compile it or you don't
want ship it or something okay so for an
example of an if if you go if you want
you can go clone this repo it's called
bitwise basically there's one function
in there you can imagine having a bunch
of functions for operating on bytes but
bitwise has one function exclusive or so
it takes a binary which is just a big
memory buffer in Erlang and a value
which is a bite and it exclusive or is
that bite into every bite of binary
returns a new binary now you could make
an argument saying but allow you to do
it that way you could say do eight bytes
at a time it would be much faster but
this is an example i just want to show
so bear with me here like i said you can
go clone that if you want these slides
are also in henrico if you want to
follow along that way so let's look at
an example so here's some Erlang our
bitwise module and we're exporting the
exclusive or function and we have our
onload attributes saying run this
function when this module is loaded
we'll look
function right now and it kind of looks
like oh that's way too much stuff but
basically all it's doing is finding
where the shared library is and loading
it so that the important is at the end
there just load our nif you have the
name of our shared library very simple
you'll find this kind of code and pretty
much every new if you look at and here's
our exclusive or function so we're just
saying X or ticks have been a binary and
a bite we check the arguments make sure
they're within ranges and all this does
is returns an error if not loaded
there's actually an erlang in the Erlang
module there's a specific nif error call
you can make there as well I didn't do
that but so that's that now let's jump
into the sea code so if we want to look
at the X or two nif so the first thing
we do every nif has the same signature
it returns an if term and Erlang term it
takes an environment and it takes our
CRV where r b is a bunch of terms RFC is
the count of how many terms you're
getting and here all we're doing is
checking our arguments we want to make
sure we're getting two arguments the
binary in the light we want to make sure
the first argument is a binary that's
what can spec binary knows the the rest
of it is just checking to make sure we
got an integer that's within range for
our white value if none of that's true
then we throw a bad argument exception
but it assuming we're being good and we
passed all the right stuff we first
check our binary if its size 0 way I
work to do we just return the original
binary if we do that work to do happens
kind of right here we allocate a binary
and we're just looped through it
on every bite doing an expensive or he's
hear a funny story about that let me
digress a little I think you'll
appreciate this I was working on swear
somebody was writing some code in Java
actually and they're trying to allocate
the buffer so they did something or was
like they put to this symbol 12 saying I
want a buffer of size 2 to the 12th
power well it's very slow as I said now
must be the memory application just be
too slow or too small they make it
bigger sin I'm to to the 14th well
that's actually exclusive or it's a no
power operator so 2x 414 i believe is 12
so they're allocating a buffer of 12
bikes when they really wanted something
of like 16 cares I just always remember
that being kind of funny anyway that's
our exclusive or and here we're
returning our value so what we're
returning is at uppal our binary our new
binary and then this integer which is
just set to zero and I'm gonna explain
what that is later but that's that's
what we return so any questions about
this fairly straight for all right who
loves big data everybody loves
right this is big data right let's read
in some big data so were you using our
read file and reading in this huge
binary which happens to be 2 billion
bytes to a billion bugs as big data and
we're going to time our nif doing this
exclusive or across our binary so
there's a timer TC function that takes a
module function and list of arguments
runs that function that those arguments
times how long it takes and returns at
uppal which is the time it took in
microseconds and the return value of the
function so that's what we have here we
have the time and have the return value
and this is really bad because it took
almost six seconds to do this now if
you're going to do this and pure early
it would take much much much longer than
six seconds so that part's not the bad
part the bad part is what we're doing to
Erlang schedulers when we do this so I'm
going to dive into how Erlang hand is
processing but before I do that any
questions about noose there all done if
experts now by the way you may or may
not realize all right so early process
architecture we start out we have multi
core system pretty much everybody has
multi-core these days so we have cpu
core one up to cpu core and wherein is
whatever value for your system on top of
that and have the operating system and
Colonel kernel threads and the vm runs
above that or like vm within the vm you
have a bunch of schedulers schedulers
our threads that executes the emulator
basically and what the emulator does it
runs all your bike code
the instructions that you get from your
compiler girl and modules but scheduler
threads by default you get one perk or
so if you had an eight-core system you'd
get bate scheduler threads by default
you can change that with the startup
parameters you can say I want to reserve
to my force for some other things so
only have six for example you can also
take schedulers offline and online at
runtime you can actually take them on
down and put them to sleep and then
bring them back online with calls from
the airline module you can actually
start you know for scheduler threads per
core if you want to whatever you like
each scheduler has a run queue and the
run queue holds processes okay
so what happens to schedule process is
scheduler will go take process out of
its run queue and it will start running
that process whatever the instructions
are it's running that that set of
instructions function calls and it's
counting function calls those are called
reductions basically roughly and it will
run that until you get two thousand
reductions once you get two thousand the
scheduler thread pauses that process and
sticks it back in queue takes another
one and that's just to get you know
balance that's so one process doesn't
run all the time on a scheduler and all
the other ones are sitting there waiting
if if your code doesn't receive so it
waits for a message on its internal
message queue if it doesn't receive that
will also cause it to get scheduled out
because you're going to be waiting for a
message or it's an emulator track which
happens when you call a built-in
function you can get an emulator track
and then you know the trap basically
means trap back to the emulator and the
emulator says hello I should schedule
this out right now so it does all that
stuff and that's again to give this
illusion just like an operating system
does when it swaps across multiple
processes Erlang p.m. is sort of like a
mini operating system that swaps across
its own internal processes um so again
your scheduling it out choosing another
one if you want to see a lot of details
yes for those anderson wrote a really
nice blog post that goes through a lot
more detail but i've given you the gist
of it all right now thread progress
there's something that it's a technique
that the vm uses to try to scale better
and if you've ever written a system like
this you would traditionally use blocks
mutex locks you know and maybe ref
counting
kind of thing so the scheduler threads
are sharing some data structures and
what they found is that this don't scale
really well if you're using those
traditional approaches of locking and
stuff so they do something called thread
progress instead now think about what
schedulers are doing they're basically
running a big loop they're looking at
process running some of its code and you
know the opcodes that are in the
bytecode tell it what to do and
eventually it's going to hit a spot
where it's going to schedule out process
it's going to keep kind of looping
through this whole emulator code so all
the schedulers are all doing this all
the time that's all they do is just run
like this they all know in detail what
the others are doing because they're all
doing the same thing just at different
times so what they do is they use that
knowledge to know where other schedulers
are in their progress through this big
loop kind of thing and knowing that they
can know when schedule if they're
sharing something they need to delete or
whatever they can wait till all the
other schedulers get to a certain point
and report the fact that they're at that
point and it can say oh I know every
other scheduler is done with this now
and I'll just delete it and they don't
have to sit and do ref counting and
walks and stuff like that so there's a
in the OTP repository on github there's
a bunch of papers under internal docs
under the the Earth's runtime Erlang
runtime system directory Earth's there's
a bunch of papers in there one of them
is thread progress but there's a lot of
really good information detailed
information
if you're interested in details the
virtual machine yes yeah so so the
question is driving threads and not
schedulers but i'm thinking schedulers
and threads are basically synonymous
it's a thread running holding the
emulator code so i refer them all the
schedulers but yes it is multiple
threads all cooperating to report
progress to each other so that they can
know or the other schedulers are in
their progress and act accordingly yep
okay now if you block a scheduler it
can't really report read progress it
can't do anything the other schedulers
start to wait and when you want the
scheduler will talk more about what that
means but when you block scheduler it's
not available to run other things an if
if you read through the documentation an
if should never run for more than one to
two milliseconds there's another part of
the docs that say well you actually
could run for 10 milliseconds don't tell
anybody but try to keep it down to one
or two milliseconds next and there's
some you know we talked about the
reductions earlier and nifs act as one
function call so the emulator it's one
reduction but that's not really what's
going on because an if can be doing a
whole bunch of stuff and I wanted to
seconds and really needs to count its
own reductions and report that correctly
so i'll show how that's done before I do
that though there's this problem called
scheduler collapse and we found this
with react we're not the only people to
have seen this others have reported it
as well but basically what happened the
first time we saw was when you had a
system that's running at a high load
that then kind of drops off so imagine
like a web property that has a lot of
traffic during the day at night it sort
of goes kind of quiescent or whatever so
the traffic drops off now furlings
designed at that point to once there's
no work to do so the schedulers will
just kind of put themselves to sleep
there's nothing to do they'll just go to
sleep and then in the morning the
traffic picks up again but those
schedulers double wake up so kind of
like schedulers that have gone with the
conference party had too many beers and
didn't get up from keynote next and not
naming any names or anything but uh yeah
so these things weren't waking up and
what was happening was one scheduler is
trying to do all the work of all the
schedulers is one that stays awake does
any work that comes in so that one
schedule showing do everything and the
others are sleeping and we've written
some tests when I colleague Scott
Ritchie knows quite a bit about the vm
as well and excuse me he and another one
named Joe blonde stood is also very
knowledgeable about
wrote this thing called nif wait to kind
of show that you could induce this
scheduler collapse with a very fairly
simple myth there's also what Scott does
is show that you can also do it just
calling Erlang script up functions or
its own built-in function so you don't
have to be writing poor nips cause this
to happen but this is due to not
respecting the time limits that you're
supposed to stay on schedule or supposed
to be one to two milliseconds max on
scheduler thread there's a call that you
can make from the Erlang logical system
monitor and you Cassie a kid which is
process ID and this argument long
schedule and time in milliseconds if you
do that what happens is if something
runs along over the time you specify say
you said time is 10 so 10 milliseconds
there's something ran longer than that
you'd get a message sent to that kid
saying hey something just ran over time
it ran for this many milliseconds and it
sometimes can give you stack traces I've
seen cases where it can't give you much
information but it can give you
information about the process that was
doing it and what they were doing at the
time so this is a good thing to log if
you're running early and productions
have a process just sitting there
getting these messages and logging them
if you start see lots of them you need
to address this long scheduling problem
okay so let's count reductions I
mentioned that we have to get our
reduction count right so let's go into
our line module and I know this looks
quite busy but we're basically getting
our kid that's what parent is up there
top we're going to spawn a function
we're going to create a new process for
this function and what that function is
going to do is just ask how many
reductions it has done to this point
then it's going to run our exclusive or
function and then it's going to
how many reductions have happened after
that so reductions before reductions
after it's also timing itself how long
did it take we're getting the operating
system timestamp and we're going to
return send all that back up to our
parent process so we're going to send
our process ID the amount of time it
took how many yields of the scheduler
thread we did and how many reductions
before and after if we do that with our
exclusive or function which is now
renamed to X or bad is the bad function
if we do this we get in execution time
of 5.8 six seconds so we've blocked the
scheduler for that long only for
reductions have happened and the 0 there
is just that hard coded 0 that was in
the code that i showed earlier in the
make couple call at the end of the nip
we'll get to that later so this is you
know this is why this is a bad function
it's taking way to online scheduler
supposed to be one to two milliseconds
max it's almost six seconds and only for
reductions for that amount of time is
just way off so what can you do well one
thing you do is let's say take our huge
binary that big data two billion bytes
and break it into chunks so let me get
breaking into chunks call this bad
function once for every chunk and then
unite all the results at the end so
let's look at how we do that so we have
this Erlang function has the same
signature as X or what it's doing
choking all this one does it sets up our
initial parameters we're going to set
our trim size 24 neg just arbitrarily
take a guess and our empty binary here
at the end is our accumulator that's
where all the results are going to go
the claws of this function that does all
the work again I know it's kind of
the code but basically whoops let's go
back and what we're doing here is we're
taking our binary taking chunk size
binary off the front of it running it
through our X or bad function and then
recursively calling ourselves with that
result accumulated here here's our
accumulator we're tacking this on to the
end we're counting how many times we've
gone through this loop and once we get
to where our chunk size we have just one
chunk remaining we get the result for
that and return our new X or binary
along with the count of how many times
we've gone through this loop so that
count with zero is our X or bad function
because we were just in the NIF that
whole time this time Rendon if a whole
bunch of times one for each chunk any
questions about this one of the problems
with it is how do you determine chunk
size I just guessed for Mac you may
guess differently if we run this through
our our counter reductions we get a
better result we get 476 chumps process
we get a better reduction reduction
count of fourteen hundred plus we
probably didn't block the scheduler but
not sure maybe we should have enabled
that long scheduled monitor to make sure
we weren't blocking it but it took
longer which makes sense because we're
doing we're building this big binary in
Erlang there's garbage collection going
on so so our memory application
specifically so it takes longer so a
better approach is to use this new
function called een if schedule nifs
I've been doing for about the past year
I work for that show but I've been
working with Ericsson the makers of
early no TP to add a new set of
schedulers basically that I'll talk
about shortly but I had this idea and
came up with this function recently
called edith schedule if this is a seed
function basically what you can do is
pass a name of a function and function
pointer into it and it takes an arc cr-v
array just like a normal lift does and
what it does it tells the system to
schedule that for future call and it
basically allows the nymph to yield the
scheduler so we can look at how we might
do this here's a c function called XOR
yields this is going to take a yielding
approach we're still doing the same
checking arguments and stuff that's
really the same as before but we're
going to do now is create a new RV and
it's going to be like the yielding
function we just did in Erlang but we're
going to pass in our original arguments
we're going to take that format guess
again for Meg chunks that's our counter
this is a scholar resource it's a way of
wrapping a C or C++ thing and kind of
turning it into an airline term so it
could be a pointer could be a struct of
some sort whatever we're going to use
that for our accumulators to keep our
data that we XOR and then we rescheduled
function so there's our guests things
before and we're going to schedule a
function called X or two with our new
are crb for some future execution at
that point we're out of this nif so this
takes very little time well within one
to two rows
range but we've now set ourselves up to
run something in the future okay so xr2
you could think of it as an internal nif
it's not visible at the Erlang level
there's no corresponding airline
function that gets redirected into the
sniff it's just visible in the C code
but it has the same signature as an if
and what X or two does is it works
through as much of the binary as it can
given the time that it's allowed to do
that and it extractive its reduction
account using a function called time
consumed time slice and once it uses of
its time slice it rescheduled itself so
it's almost like recursion anyway but
but the other thing does it looks at how
much work it does within a time slice
and that original for mega chunk that's
just a starting guess it will then
evaluate say well that's too small I
want to go to a bigger jump size and it
will adapt that chump size for the next
call now this is going to be real busy
code so I'm not going to try to explain
every line but basically in the
highlighted area we're just setting up
what region of the binary we're going to
operate on in here we're just getting
the time of day running through the
exclusive-or that we're doing to
generate our new binary and then getting
the time day we're just timing how long
it takes given the chunk size that was
passed to us we then do some fancy math
that trust me it's correct now I got it
wrong when I first
this in Chicago her life but it's close
enough but this basically says how much
of our time slice that we just use and
we call this function consume time slice
if our percentage of the time slice that
we've used that we've accumulated is
such that we've used up the time slice
completely we have to get out so what we
do to get out is adjust our chunk size
based on how much we get through set up
our new arguments many of these are the
same arguments that were passed in and
we reschedule our self to run again in
the future if we haven't researched I'm
slice we just say oh we have an easier
time slice the chump size is too small
so we're going to go do another chunk
and see how much we can get through once
we've completed the whole thing we make
our resource into a binary and return
our result which is that same truffle
the binary and the number of yields of
the scheduler we did it by using een if
schedule knit all makes sense right
again these slides are online with the
code so you can look at it clone that
repo if you have any questions just
email me so our yielding if gives us
much better results it's faster 5.41
settings only 7.8 million reductions
which is completely accurate given how
much time it took and we've yielded the
scheduler almost 4,000 times so we're
never walking that scheduling so in the
future once you start using this this
stuff is kind of experimental right now
you have to comply lang a certain way to
turn it on but there's
no more excuses for walking scheduler
with this call now another approach I
mentioned earlier that I've been working
with Eric sinful past year so and it's
on this idea called dirty schedulers
it's not my idea Erickson came up with
it or carb green specifically came up
with this idea and if we go back to this
picture we had earlier the vm and
scheduler threads and stuff let's just
sort of make some room in there and
let's put some more threads in because
who doesn't like more threat honestly DC
here stands for 30 CPU scheduler and
there's one of these perform as well
there's never more dirty CPU schedulers
than there are regular schedulers
because we don't want to see you dirty
CPU schedulers to kind of deny service
to the regular scheduler threads so if
you in your runtime were to take down
some of your scheduler so that you can
do with calls that I mentioned earlier
the dirty schedulers also get reduced
these are unlike regular schedulers each
regular scheduler has its own run queue
dirty schedulers all share one common
run queue and not only are their CPU
schedulers of the dirty variety but
there's also io schedulers which is a
whole different set of threads and
there's an of those it's not tied to
cores and this is much like the async
pool that's in earlier now the async
pool by default you get 10 threads so
there's ten of these by default but you
can change that with parameters that
start up and everything they to also
share a common run queue so that's the
30 io scheduler and the way you enable
that way you enable the dirty scheduler
stuff is with this enabled Ernie
schedulers config thing
building your life experimental feature
in your life 17 you'll know you have it
if you start your shell and you see this
thing reported dirty schedulers 8
available aight online in 10 30 i go
schedule your numbers may vary but how
do you use these things well it's also
tied through univ schedule if so you
just pass a flag to need of schedule if
to say hey stick this on a dirty
scheduler and you can tell which variety
it is by the flag pass or you can in
your block rien de claro your if
function you can say run this nif always
on a dirty scheduler this is all brand
new as of September so you need 73 or
newer to use it and this is the block
i'm talking about all nips have this
where they declare the name of the
function the parody of the function and
then a pointer to the c function that
implements it and we've extended that to
add this flag so here this last one is a
cpu scheduler dirty cpu scheduler you'll
notice three of these things all run the
same c function this one happens to run
it on a dirty scheduler the other two
that are basically the same they're that
bad function we started with but they're
all doing the same thing so if you're
run on a dirty scheduler these are the
results we get five point nine five
seconds a production 10 yields you might
say whoa hold on now you just went
through a whole
production should be close to eight
million not eight but we're never on
regular scheduler these schedulers are
not they don't have the same rules you
can stay on a dirty scheduler as long as
you want to that's why it's dirty it's
not clean code it's a code that doesn't
behave well you can't collapse that
scheduler like we can with a regular
schedule because it's not doing threat
of progress or any of that other stuff
and all the other regular schedulers
were all busy doing the things they
normally do when this was running on a
dirty scheduler thread so we like to say
schedule and dirty right oh there's no
chunky or yielding needed you don't have
to write you know your code to be kind
of friendly to the scheduler thread but
that doesn't mean you can just do
whatever you want right here's you have
a set number of those threads you could
certainly write code that just sat on
one and used it all the time and never
yielded it so that's bad especially if
you're writing part of a nap and someone
else is writing part of now and they're
both trying to do next day so you can
also use schedule with to cause a dirty
if to reschedule itself same as a
regular than if and that way it can
yield the scheduler and not have to
occupy it all the time and also an if
can use this call to flip itself from
dirty back to regular back to dirty just
rescheduling itself across different
threads any questions on all that this
so did you schedule service plus there
are nice
scheduled a regular can you schedule a
regular lunch Berlin watch on the right
so the question is can you schedule a
regular girl in function using this call
and you can't it's really tied to nifs
just the way the arguments are passed
and everything like that it's it's all
tied in the nips I thought about that
when I got this done is like leaning
maybe you can schedule a regular Erlang
function and decided that would be too
much to try to squeeze into this thing
and I'm going to talk about something at
the end very end that would let you do
that scooters and where exactly both
so the question is about how many giri's
schedulers have started by default and
the recommended number to run the
recommended number is kind of hard to
say because it depends on the
application but by default you get the
same number of dirty CPU schedulers as
you do regular schedule so that number
is never greater than the regular
scheduler count io schedulers are
different you get 10 of those by default
and you could make that much larger so
if you're doing a I owe heavy
application you might bump that up to 60
or something and never use the CPU dirty
CPU scheduler so it kind of depends on
the application but we do restrict the
number of dirty CPU schedulers to be no
greater than the number of regular
schedulers my mission no should it be
lower it's hard to say I mean the idea
that we had was to just keep it the same
if you want it to be lower you can make
it lower if you are as I mentioned
before the idea was to not overload
regular schedulers not freak undone from
doing their work using was the most kid
the system monitoring sometimes related
to some other processes are do is make
the enemy of our ESO by allowing
actually a there is Hitler's DC they
could
so the point twenties making is that
sometimes other events in the system
cause the regular schedulers to have
long scheduled events and that's exactly
why we've limited if you really if you
see those events and you know it's
because of dirty scheduler work you can
drop the number yourself but we just
thought it was more straightforward to
keep them identical but it is something
to watch and I still recommend using
long scheduled events even if you say oh
I only work on dirty schedulers I don't
have to worry that much anymore it's
painless to set up a process to receive
those events alright I'm going to talk a
bit about pork drivers and what a poor
driver is is it's really just a way of
associating a set of native code
callbacks native code in C C++ callbacks
with an erlang port so that's a pork
just like I mentioned the very beginning
of talk basically what happens is things
happen to the port and when that stuff
happens the vm calls your callbacks that
are associated with the court so if
you've ever written a driver in an
operating system it's very much the same
that's why it's called the driver you
supply a set of callbacks and when
things happen certain callbacks are
called a file handling all the TCP UDP
sctp and some other stuff is all done
through drivers inner life so this
thing's been around this interface is
older than been around a long time
people like this because it's easier to
get the arguments but there are things
nifs can't do that drivers can do such
as dealing with file descriptor events
so if you're waiting on Io on file
descriptor can't do that and if unless
you do it yourself it's all built in the
vm with a driver you can just call the
vm to do that
for you and there's also that basic
thread pool that I mentioned before
that's available to drivers if you want
that an if you kind of have to write
your own I just wrote a driver
independent of this called enm it's a
wrapper around nano message so if your
fan of now message might want to check
out that driver but I'm just using it
here as an example it doesn't really do
any dirty scheduling or anything it's
just a straight-up networking driver but
drivers are native code and they have
the same kind of limits on execution
time and everything that nifs do though
that we've described this whole time so
right now what I'm working on is trying
to get dirty scheduling and drivers to
play together dirty moves were real
simple compared to drivers it's driving
me insane this is what an entry block
looks like i'm not going to go through
all the functions but every one of those
E&amp;amp;M underscore things are C functions
that are callbacks that are invoked by
the arm by the early bien when certain
things happen and then here's some flags
we won't go through those but basically
a bunch of callbacks the nulls are where
there are callbacks that I'm not
interested in so I just don't even
supply a call back in that case what I
might do for the driver dirty driver API
is there's two callbacks that return
values and those are numeric values what
I might do is for those you can return
special values so if you want to
reschedule that call back onto a dirty
scheduler we would return the
appropriate value and it would go to
that thread get rescheduled on to that
thread and you can also just reschedule
on to a regular scheduler which is much
like using you know schedule nif to
reschedule yourself you can just say
that and that's a way of yielding
there's also a new call than
experimenting with called scheduled call
back so it takes this function pointer
which is you know takes up this data
comes from your driver it's specific to
your driver it's just like a whatever
you pass in and some argument that you
created earlier so you would call this
function passing in the pointer to the
function you want to schedule in the
future the port is you know the pork
that you're in the driver is attached to
that poor flags is where you would pass
in you know which type of scheduler you
want and this argument is what gets
passed to your call back when it's run
so it's fairly straightforward there's
one other call that's in the API which
is am I on a dirty scheduler so you you
pass in the port and this will tell you
true or false whether you're honest a
dirty scheduler or not so the way you
would use that is saying this is a
control call back these are the
arguments it gets by default what you
can do is get your driver data which is
again specific to your driver and in the
driver datings stored away report that
you're attached you and you can say hey
I'm a dirty scheduler if I'm not
reschedule me to a dirty io scheduler
next time I get scheduled come in here
and it's on a dirty io scheduler so it's
yes you are on a dirty scheduler so then
you would run the rest of the code that
one so that's kind of what I'm
experimenting with that's the important
bits right there another way of doing
this would be to use that schedule
callback function i mentioned earlier so
this would be for like the output V call
back has a void return and actually like
i said before only two of these
functions have returned values all the
rest are point returns so in that case
you can't return special value there's
no place for it to go so you would use
this schedule callback function and say
here's report i want to be on io
scheduler here's the function i want you
to call and i want you to
this argument which I've built here from
the incoming arguments again this is all
experimental might look this way might
not don't go off the right code against
this I might get rid of that special
return guy there's only two functions
that can use it it might not be worth
the extra code so the next steps for all
this work is to finish the dirty driver
so if I would really like to do that
this month we shall see if you want to
know a lot more about all this stuff you
should see Ricard greens original
presentation from 2011 where he came up
with native processes and dirty
schedulers now we go to your question I
just mentioned native process we're not
quite sure what a native process is if
you look at this 2011 presentation it
has a certain shape a native process you
can think of as an airline process
that's running native code which is kind
of what initiatives right to native
process sort of but what the goal of
data processes to me anyway others
differ would be too and I think Ricard
mention system is talk in this
presentation would be like can you
combine ifs and drivers so that you only
have one of them and they can it does
all the stuff that they both do today
why have two interfaces collapse 21
allow file i/o events you know all that
stuff and so there's sort of an API
presented here that your car came up
with it's worth looking at undoubtedly
will not look like that at the end to
probably but that would be able to in
both
an erlang function from native code so
long answer their shared question okay I
do want to thank Ricard green he's
guided me through this work and you know
my work with Erickson he's been very
patient explaining a whole bunch of
stuff and pointing out flaws smoker
Erickson also the same is helping quite
a bit in this work and there's an early
user name Anthony ravine he was actually
early on user of the year this year
quite prolific with patches to Berlin
and we're in IRC channel one day and he
mentioned nip traps and there's like
traps that would be cool oh my how to do
that and that's where I got the idea for
schedule nib so good thank Anthony for
that francesco and i are working on a
book designing for scalability with her
lying OTP you can get an early release
form it's not done yet but who's this
code in fifty percent off and with that
shameful or shameless advertising I'm
finished thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>