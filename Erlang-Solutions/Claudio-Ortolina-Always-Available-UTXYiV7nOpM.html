<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Claudio Ortolina | Always Available | Coder Coacher - Coaching Coders</title><meta content="Claudio Ortolina | Always Available - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Claudio Ortolina | Always Available</b></h2><h5 class="post__date">2017-06-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/UTXYiV7nOpM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">we're going to talk about today about
availability so I had some trouble
fitting the subtitles in fly but what
happens when resilience is your primary
requirement so we're going to talk about
applications that have to resist outages
so as it was mentioned so that's me I
get paid by hour-long solutions to do
different things most of the thing most
of my time is spent consulting and doing
training which means that I get to talk
to a lot of people that's the the
primary advantage so the topic of this
talk comes from just looking at what
people do in addiction at different
stages and trying to get at least one
basic idea that usually takes some time
to sink in when you learn the language
and when you move on with your in your
journey to write the legacy applications
you can always reach me at those two
coordinates there at the end of the
presentation as well so you can always
find me there now let's just dive into
it so who seen these things before now
it's a bit of a trick question because
that's the Italian branded version of
that object so that's a pager in Italy
they were called tell a dream if you
remember them like I crudest to you so
my my father is a doctor he does it does
emergency room and things are there so
when I was a when I was a kid and like
mobile phones were my thing there were
times when he had to be available and
that would be like a 12 hour time period
and what he had to be but essentially it
would be it would need to be in the
hospital in less than half an hour if
anything happened so to guarantee that
they would give him one of this because
it I mean it's a pager so it just buds
it it shows the phone number and then
you just like run to the hospital and
then worry about everything else later
but the good thing about it is that it
would allow him to spend you know time
home or like we could we could go out
and do things together and then if
anything comes up it can always run to
more but it wouldn't need to be confined
to like
just like being near the phone now I
never heard them say in this in the
sense that and this is something else
off to developers we I think we have it
slightly different acquire most most of
the time
he never said our sorry I can come to
the hospital could the data bit is down
so sorry you kind of have to deal with
it yourself and then possibly do self
surgery or simpler that never never
happened so a point is there are things
that you do and the things you take for
granted in your daily life where
availability is the most important
concern but that doesn't seem to be the
case in a lot of things we build and I
wish that could change so backtracking
in a second for that things you may have
heard I may have heard as you learn
elixir or Ireland like bear in mind that
I will I will try to mention things like
both both things at the same time but
realistically ninety-nine percent of
what we're talking about applies to eat
the language so it doesn't matter really
what what you use is just principles and
some ideas so who has heard this elixir
is full tutorial on just like give me
some hands just to you know yes ok
multiple sources I I definitely am
definitely showed that talking with
people I said that myself and then this
one you have to use OTP or you need to
use a DP or if you use OTP is going to
be amazing like variations of sentences
that contain the word OTP okay he
recovers to a predictable state this is
like you've got a process he crashes he
recovers like this like we like we heard
these things so in 2015 or beginning of
2014 when I started learning the
language when I read these things I was
particularly excited about this because
it felt like it would solve some of the
problems I had with the software I was
writing so I was particularly excited
except that it didn't like for the first
year a year and a half I was using some
of these tools
but in hindsight availability wasn't
better like not at all I was just like
building similar things with different
different tools and it wasn't available
and it took me some time to realize and
then I I hope that like people in the
audience just come here that's obvious
like xqe what it is but it took me some
time to think about is that availability
somebody you have to design for because
that doesn't just happen or at least if
it with the tools we use now in elixir
line you don't necessarily have more
availability because you have to design
for it
now what does that mean it means that
you we have a lot of lower-level
tools that work well together and we
have a lot of good primitives and
patterns that we can use but the biggest
problem we need to solve if you really
want a good availability is to design a
system that makes that a primary concern
and if you don't do that it's not going
to emerge so there has to be something
that you really in the same way you
worry about security or other concerns
it has to be something that you really
push forward so what we're going to do
from now and until like pretty much the
end of the talk is we're going to work
on an example application and it's going
to be like diagrams and color blocks
very little code this application is
called Libra is an like because it
weighs webpages so you you use it to see
how heavy is a web page is web app and
that's just because that's the zodiac
sign for a scale and it's a very short
name so it does the job so let's look at
like for example like a browser version
of it so main interface you put a URL
there press the button and what you get
is this page where you see well if you
can you see that there are there's a
page body sizes and there's a total size
in kilobytes for the page and there's a
list of resources that that page links
each one of them
with a weight and status their
representative if the crawler was able
to fetch it or not now that this page
once you see it because the inspection
can take some time it updates in real
time so you're going to see the page
receiving SQL data and then rerender so
the implementation we're gonna we're
going to talk we're going to talk about
is an implementation based on on Phoenix
so that's the main underlying framework
that we're using and if we see it we got
like basically three separate worlds
that we have to get to work with each
other one is the internet on the Left
where all the pages we call our this is
our for example in this specific end
point where we create a page inspection
it's just the post route that that gets
invoked when you press the button and
then there's a database and the database
in our case is PostScript but it doesn't
really matter like it is that
respectively an external data store now
what happens so the basic implementation
that we start from is this one we fetch
the page when we start posting that's a
blocking call so while the application
basically is waiting to respond and
you're waiting and seen like a beach
ball or a spinning wheel you is patching
a page when it finds it it saves it to
the database so we go from the page it
parses it gets how the the length of the
page body size saves it and then
broadcasts because remember that we have
we're gonna have we we are concerned
about updating in real time so we broke
up the page creation now this note
doesn't necessarily is not necessarily
processed at at this point in time but
we do that and to fetch the assets of
the page we spawn a task fetch the
assets possibly in parallel weigh them
and update the page in the database okay
now let's make it a tiny bit interactive
is it clear so far anyone has any
question about this
all good okay now this button raise your
hand if you've ever spawned the task in
a controller okay nice not not that many
people okay this is legit
like you can definitely do it that's
fine there's nothing wrong with this so
it is a it is a pattern that you can see
you can see it use it does the job it
gives you a bit of a extra capacity
because you don't have to wait for an
expensive operation to to finish and it
does whatever it needs to do oh but yeah
and at the end you broadcast that sorry
there you have updated the page now
let's go to the other route you want to
see an inspection now each inspection is
unique so you can expect to at the same
URL twice you just get back a different
UID so you get a page with that your ID
and you read it from the database like
boiler plate like this could be
scaffolded as far as we know that's
that's extremely straightforward boiler
plate control implementation as the page
is real-time there's a channel for it
same routing scheme you get the page
from the database and then you process
any broadcast on that topic that comes
in so that if the page gets updated and
the inspection finishes you can rerender
with the extra numbers in the table okay
so we're going to use a recurring user
specific metric to judge this this
application in terms of availability
which is a Telugu score based on the
pager that we saw before feel free to
use it as a trademark you may need to
pay a royalty but my my contacts are at
the end of the presentation so we can
sort that out but in general the score
is not great like it could be better
because effectively we have a very very
very specific problem which is this we
have a hard dependency on the database
if at any point the database goes down
we can provide 0% of our service 0
because effectively just to paraphrase a
like thought
for fox mulder the throat is out there
so it's not anon in our app is in the
database and in the end that means that
we are confined to whatever availability
strategy we have on the database side
now our goal is to get this application
to work during a database outage and
this means completely work not a subset
of the functionality how can we do that
so do we have a master plan for this in
the sense that we're going to see at a
high level what's our strategy and they
were going to look at implementation for
each one of the steps so if we don't
want to depend on the database we're
going to keep that data somewhere else
now
if we use a different database that
pretty much repeats the problem right so
it does not help so the thing we can do
is keep page and access data in memory
in other words we perform the inspection
process and keep the data in memory the
important thing is that we use the same
data structures now in the case of using
active example and Postgres you get back
effectively structs so as long as we
keep those trucks consistent we can't we
may have we may work on a replacement
that has the same type signatures as the
same interfaces and that's important
because it makes our life much easier
when we do this so one one little thing
to keep in mind is that it's important
to think about these data structures and
try to map them in the same way the
other thing we can do is as we we have
each inspection process which is which
is unique and keyed by a UUID we can
have one process per page now the thing
to keep in mind is that it's we can
think about this each inspection as a
life cycle process one thing that the
said they I forgot to mention five
minutes ago is that one requirement that
the application has is that these these
inspections expire after 24 hours so if
you use other services like that we just
like you use them and 24 hours later
that data is gone
so in that case we can design a system
where each process manages its own life
cycle by each process get started does
whatever it needs to do to get the data
then basically sets itself an expiry
date like in the future and then when
that happens is going to self-destruct
and at that point writing data to the
database becomes something that the
process tries to do but we shift the
perspective writing that data does not
mean working on our primary source of
truth it means having a back-up plan
when our database goes down sorry when
our application node goes down so the
main strategy is that if the database
goes down the data is in memory if our
app goes down we have a guarantee that
we replicated data to the database
up to a reasonable amount of the
reasonable extent possibly very very
very close to the point in time we are
so this process this worker that inspect
pages we're going to call it inspector
worker and we're going to build it with
a gen server so what are the
responsibilities of this inspector
worker well if you look at time and
that's the 24 hour life time for one of
these inspections we know that this
process is going to start and is going
to terminate now what does it do during
the life cycle so he starts with a URL
and it fetches the page for that URL
once you have that and you know when
assets are you fresh the asset and then
you say that to the database and the
important thing is that throughout this
process the throat sorry let's make it
less confusing throughout this flow the
process can respond to questions so if
you just fetch the page and you ask the
process ok give me the page was the
state it's going to give you a page
without assets if you fetch the asset is
going to give you the page with assets
so it gives you that
from its own state from its own memory
so it never goes anywhere else to find
it and then once it's done is its fetch
operations you can save it to the
database then after a bit after 24 hours
it will terminate when it terminates it
deletes the data from the database so we
have a clean life cycle that get the
data where it needs to be and removes it
where it doesn't need to be there
anymore
now let's implement that with gen server
2 so again same time so we start with it
in it where we receive a URL now we can
choose to fetch at this point which is a
bit expensive and it's a blocking
operation or we can do it as
synchronously the thing that we need to
decide for this is if we care about the
existence of that URL as a result of our
inspection for example if we want to say
hey you try to expect this URL and it's
a 404 then we can do this a
synchronously because the inspection is
still valid even if you pass a URL that
doesn't exist however if you possible
you are that it's badly formatted then
the process cannot just cannot do
anything and it could just by self
terminate at that point so in the init
assuming that we care about 404s
we cast the process cast to itself so
fresh the page then it does that then
after that it casts a fetch asset and it
does that and after that it cut a save
so it does that so we do that in chunks
you can try to compress some of these
steps together Delta becomes like a
mostly a choice but effectively we are
just trying to make this like you we
just do like small atomic operations to
keep the process as unblocked as
possible you can even make it more
complicated at this palette let's just
stop at separate callbacks and after
that we sell we set up a message like
send up the message that will expire the
process now expiring means we handle an
expert message and then we stop
that point and when that happens we
delete so this way we are we make sure
that any situation where there's an
abnormal termination the data in the
database would be deleted now it doesn't
happen very often because of the life
cycle but it gives us a sort of like a
safety net to make sure that the data
remains as consistent as possible now
what are the benefits each step is a
callback definition which means that
it's a public function which means that
you can test it independently and each
step is schedules the next one so if you
squint a bit this is like a state
machine in these guys that goes only one
direction and doesn't fork it's a very
if it's a very simple state machine but
it does mean that you always know what's
going to happen next in the process life
cycle and the other thing is that self
destruction is a doesn't depend on an
external schedule or polling mechanism
you don't have to ever process that or
something that every five minutes says
okay what do I have to the lead now
which process do I have to kill each
process manages itself independently and
finally persistent is an implementation
detail because it is in one function is
in a callback and then so far because of
the nature of the feature we are
basically using Postgres as a key value
store so you could potentially swap this
for something else that has different
properties in case you have other
concerns for example around a data store
that is maybe more available or
something that gives you different
properties in terms of speed of query
and things that whatever whatever is
your concern but it means that your bet
your primary interface is not going to
be the database anymore to get this data
now we looked at a process in isolation
like I have a URL I want to start
respecting me your this URL I'm gonna
spawn a process and that could be like a
floating process in your system
completely unlike without any connection
any link no monitoring in in your system
your application that's
not ideal because what if that process
gets into a weird state where the
network becomes unreliable and southern
you have zombies everywhere and
apparently that's a bad thing so we
needed to provide them and in this case
we can start with a simple one for one a
simple one for one supervisor is a
supervisor that is one of their
different types of supervisor 0tp this
one particularly takes a boilerplate
definition so it knows how to spawn one
specific type of worker and just that
one but it can have as many of them as
it needs so it can spawn one two or
three and it keeps tabs on them it knows
which ones are alive and which ones are
not so what we can do is we can have an
inspector supervisor that each time we
want to expect URL we ask the supervisor
hey give me a new worker that does this
and by doing this we have a few
important properties at this point then
we have an API to spawn workers and an
API is an API so start hiding a bit of
the implementation detail we know that
we're spawning your worker because we
know that we are interacting the
supervisor but we can hide some of the
details for this the workers are
monitored and we start with in case of
abnormal termination so for example if
404 is an acceptable error that children
cause our worker to go down because it's
an acceptable result of an inspection
process however if you have for example
an internal network error because the
machine that is doing the inspection
ends up being dropping off the network
and you get for example like an NX
domain I don't know how to go to the
network error you may want to let that
just bubble up as a critical error and
just let the process crash because that
way the supervisor will restart to the
same URL and that happens up to a
certain up to certain number of am of
attempt so you know that this way you
can distinguish and you can start to
discriminate between acceptable failures
that are semantics that have to do with
your domain and failures that are
absolutely exceptional events that you
just don't want to deal with in an
explicit way
and the supervisors always know we
children are alive so you can you avoid
the problem or the zombie process now
next problem are we good
is it is everyone okay okay so route
into a worker now the problem is this
one I wish I should have used this as a
screen shot that one of the other talks
from yesterday about visualization but
today once we we have to fetch a page in
our new version of the system what we
need to do is basically we have to go
from an ID from a user ID to a process
so we have to resolve that in some sort
of ways so the problem we had before was
get the page from the database now we
have a problem of finding a worker for
this now that means that each worker
needs to have a UUID so before when we
are writing to the database we were
relying on Postgres to generate the you
ready send it back to us now we need to
handle now we need to handle that
ourselves
so we start an inspection we assign a
UUID to that process and that process
registers itself with that new ideas a
name to do this we need to use the
process registry because by default
process names are atoms and you don't
want to cast a user ID string to an atom
and use that as a name because that
introduces a memory leak that cannot be
resolved without rebooting the node so
it's important to keep in mind that when
you have to map types that are not atoms
to processes a registry is always your
best bet so what are we going to do
about this we get pages we get we have
to find a worker we wrap a call to the
registry that we have in our application
so we can we can use this registry wrap
it result to a worker and use that to
get the data we need
so at this point the benefits under each
worker manages its own name in other
words it's a responsibility of the
worker to register itself with the name
we don't have to do that from the
outside world which means that we don't
have to manage that synchronization each
worker starts declares its own name
registers itself and the registry will
get updated when by automatically
because it's it's monitoring this this
process is when the workers die so we
have we had a very good situation where
our registry knows exactly which
processes are alive in the same way the
supervisor does widget if that just
happens automatically so we don't have
to do that ourselves so once we declare
this flow then it will work and because
you're working on a single node alexey
1.4 ships a built-in local registry that
does exactly this that is by far my
favorite feature of their release
together with enforcing parenthesis for
function without arguments there really
does the job so what we've seen so far
if you exclude the fact that we use in
Phoenix we are only leveraging standard
library so we're not putting in any
dependency so all of this is possible
just by using tools that the language
provides by default now it is a quite a
bit of machinery though that we put in
place like we have workers we've got a
supervisor we got a registry in place no
one else needs to know about this from
the outside world so what we need we
need a public API and the public API
purpose is let's make sure that all of
these implementation details do not leak
through the callers at the consumers of
this API so exactly because of that in
this life which is like that that's just
a bit of code about how this public API
could look on purpose there's no
implementation in the sense that the
perspective we have from the point of
view of these model is that we care
about providing a consistent interface
our web layer will consume all
lay this file because you only this API
and we give really strong guarantees
about the types that we return in the
sense that when we process URL we get
back okay and the UUID for that
inspection we don't know that there's a
process behind it we don't care at this
at this stage it just doesn't matter in
the same way once we have a UUID and we
ask to get the data about that page we
get just the page data we don't know we
don't know that that comes from the
process we just know that you've got a
page that may have some data in it or a
very thin complete or complete we just
don't know but we don't really care at
the same time we return we return errors
that make sense so get page if you look
if you look at it it is a a annotation
in a specification that it could be
something that comes from a database but
you just okay and the type or error not
found and not meal because a personal I
prefer explicit errors instead of meals
but in general we just keep just keep a
predictable API the height determination
detail so the consumer of the API is
unaware of this implementation detail we
can refactor the entire thing without
affecting anywhere else in our
application now this particular this
particular property is also one of the
things that I think it's important thing
about especially with a with the next
releases of Phoenix but in general in in
election language that you have in a
public API with a very focused API
usually means that you can easily work
on the internals without affecting the
other parts of the system so it's it's
important to just like make a bit of
extra effort to do that now we started
with one specific requirement which was
surviving the DB outages and let's I
will appreciate if anyone last to the
part at the bottom just because I think
it's pretty good try to the rescue even
out of pity
yeah now we want to survive with the
outages different strategies for this
when you when you deal with the with
resources that can fail there are
certainly different patterns you can use
and like some people introduce circuit
breakers and other like patterns to
survive outages in our case because we
have a very focused usage and we we
basically know what we want to protect
against we can use a try
we can use a very focused try rescue in
the sense that we try to say to the
database we rescue a specific exception
and not all of them just one and
possibly retry same as when we try to
delete the page from the database
now try rescue should be used sparingly
in the sense that you are it may hide
errors that would just be better you
will just be better off letting the
process that cause that function crash
so you may end up rescuing if you don't
if you're not very specific things that
you would want to bubble up because you
would want the process to restart to a
predictable state so I mentioned one of
the sentences that everyone says and you
wanna rescue relevant errors in our case
if we care about being able to connect
to the database and I mean that database
online best error we have we want to
rescue because they represent the
inability of our system of a vector in
this case to open a connection through
the B connection with the database we
also have the opportunity because we
have two specific points in our code
that talks to the database and they
encode you have the possibility of a
failure to think about what we want to
do when the failure happens we can retry
so we can schedule the process to retry
and we can do that with a back off so
maybe if the process if the database is
down chances are there it may be down
for another few seconds so why don't we
wait five seconds to try to save again
and retry so all in all there are
different things you can do even to
improve on that but the point is that
you have like a specific power
the only one in your codebase where that
concern can be expressed and you can
work on the code for that so what's the
road from here so the road for me is
that we haven't touched on two specific
topics first one is with story state so
the note goes down Postgres is alive and
well we bring the note back up how do we
restore the processes that are supposed
to take care of the inspection option
one we read or restream the relevant
entries from the database meaning pages
that are not supposed to be expired and
we spawn a process for each one at the
right stage so if if the inspection was
over so dispassion because the page was
saved in the database it was over so we
use that page record from the database
as a state and then we calculate how
much time we have left for the expiry
that's option one one thing to keep in
mind is that first of all better to do
this in batches and the second thing is
when you do this is in general when you
do like long-running operations they
have to deal with the database sometimes
if you if you just write that code
without like a bit a little bit of extra
care you may end up hogging your main
repo because you keep checking out
connections for example like that does
not necessarily what we're doing here
but that happens so in these cases may
be a separate repo and a separate pool
of connections could be good because
that way you can guarantee that the
other repo that in the meantime can be
used to serve requests Israel is
available the option to is lazily spawn
workers in other words we start let's
say that I want to show ID one two three
I look for a process with that without a
URI ID if it doesn't exist I check in
the database if that you already exist
and if that page is still it's supposed
to be alive if it shouldn't be deleted
then I spawn a worker for that now that
is a lazy that could work better in case
you have like a you a huge amount of
data how
ever really limits availability because
you're doing equator to the database at
that point so you may have it if you are
if you are not available at that point
then you're still back to square one the
other the other elephant in the room is
the going to single node to multi node
because we built a stateful service by
now and that means that the memory of
that service becomes a form of state
that in the case of a multi node setup
we have to manage now that means how do
we do with that there are so many ways
you could approach this you could for
example replicate that State on each
other node that you have and because we
are working with something that only
moves forward it has very little
conflict possibility then this could be
something that you handle with a pub
subsystem for example and something that
could just give you the ability to
replicate each inspection on every other
node now that problem the problem with
that becomes memory effectively because
even if you have five nodes of activity
of the same amount of memory no matter
how many nodes you have the other option
is to shut this but then you have to
think about what happens if one node
specifically not goes down and you need
to rebalance and maybe that data goes
lost so certainly increasing or moving
towards like a strategy that wants to
increase availability comes with other
comes with other challenges that you
have to think about the single no
registry should become a distributed
registry there are different options for
this nothing built-in in the legacy but
you can you have to think about that as
well
and finally this is something that is a
side-effect of thinking about a multi
node architecture is that you can deploy
think about these components as deployed
independently the likelihood in this
application is that you may going to
have more work in inspections compared
to the work that the web stack needs to
do because the web stack is very minimal
so if you find yourself that you need a
couple of web nodes and five inspector
nodes you may need to think about how to
split that code so that you can pack a
node that is just a web node and inorder
is just an inspector
an umbrella could help you with this
spoil aside the ability to build nodes
independently and pack your applications
together there is by far the biggest
advantage of using nubrella by
segregation in terms of code
organization you can do that in the lib
folder you don't need an umbrella for
that the broad applications really shine
when you can pack independent
applications together in different load
layouts without having to restructure
your entire application every time
so in general designing for ability
requires thinking it is worth the effort
because you you can be available and and
that's important maybe you have a
service that maybe it's mission-critical
maybe not but in reality because outages
happens always when you don't want them
to happen if there's any time you want
them to happen but in general if you
have like a spike in traffic for a
particular promotion that you want it to
be available so any questions I'm
definitely willing to answer like
compared to this is just to give you
expectations I can do a better job than
that
yeah I've been racing myself you know
hey I said should I be bracing myself
here yeah no right
well thank you this is my this my inking
in case you need to reach to reach to me
again these are my coordinates feel free
to ask questions or anything like that
if we have time I can take questions now
otherwise yeah I think we have a bit of
time does anyone have a question or a
pop back there in a moment as well so
thank you very interesting speak so you
say only rescue well-defined errors and
you'll notice the first to say that but
how would you actually do that because
we're talking about what you want to
secure ourselves against is the database
going down yeah it's like well all
database being unavailable and can be
that in so many ways like all of a
sudden well your machine says yo I have
no Freeport to talk to that machine or I
dropped off the internet or I have a
pile
or whatever the segment how do you
enumerate there really is dependent on
the on the library that manages that
connection part like in this case like
that
DB connection library which if you have
questions about the author is in I don't
know if he's in the audience but but I
don't see I don't see him I don't see
you in the audience but he has a you
know seriously speaking is just that
that specific library there may be other
exceptions that you want to ask you
about it's a matter of seeing how that
library is constructed and if it does if
it does that if he throws a if he throws
exception or ages exception for this
those particular events different
database may behave differently may even
just like not even button throwing
exceptions we may even just get like for
example I'm working with CouchDB and
CouchDB as an HTTP interface so I may
get a 500 or I may get a 404 or 410
it really depends on that specific
driver in this specific example
it was octo and Postgres so that would
be the thing you you match for although
there might be other instances I just I
normally just worry about that because
that's the most frequent one that I see
there is another question over here hi
thanks for the talk you elaborate on the
last part when you talk about how you
would deploy different kind of nodes to
to handle the the memory issue because
I'm not sure okay
you mean you mean like capacity issue so
just being able to serve more serve more
expections or you're talking about the
data well the data the data about the
fact that if if you saw under everything
in memory has become complicated so the
the simplest way to tackle that problem
is to switch to a high available data
store and pretty much this car whatever
we talked about for the last 45 minutes
there's a sip there's a simplest option
that requires less less work on your
whatever you basically need at that
point is a assuming you have a ten
inspections in note a now if you can
afford to lose that data when the note
goes down lose it completely and the
note come another note comes back up and
you're able to define how that node
needs to restore so for example that
node can knows how to grab only the
subset of the data in both gross that
pertains to its own State for example
you could use like consistent hashing on
the URL so that you partition the data
in a predictable way at that point you
somewhat replicate the pattern of
restoring the data on a subset of the
data for the node you're that's coming
back up that's one possible way but that
gives you a window of unavailability
because you're doing that job the other
option would be to have note that the
data is replicated on for example over
five seven nodes on to other nodes and
then once a note goes down the other one
takes over now no no none of these
things are just just come out of the box
you can do it in libraries for them but
in some ways that's the principle behind
like the the react as a database kind of
works in a lot of scenarios like this so
you can but the basic point is that is
not super straightforward and there are
definitely different trade-offs that
depend on how much data you have what
kind of load you have and what kind of
consistency requirements you have in the
sense that it may be acceptable to have
like 10 seconds of unavailability for
that specific page and that's the
trade-off that you will to make over
total availability in case of an outage
does that answer your question okay
thank you does anyone else have another
question
thank you for the great talk I'm
interested in as well it seems like
you're interested in having stateful den
servers right that seems to be kind of
the what you have created and is there
any other research that erling has done
early is going around a long time this
can't be the first time that someone has
tried to make a stateful gem server that
like if it crashes right it comes back
with the state ahead do you know of any
other research are you looking for I
mean the simplest thing you can do is
just move the state out so one thing
that is possible to do is you use the
gem server just for the lifecycle but
even in terms of QT data memory you move
that data in another process that owns
that memory so that when the gen server
goes down you don't you you don't lose
that data you could use like a new TS
table that has a there is like fairly
protected me mainly it's easier it's
owned by your application main
application process or it's got like a
good strategy for recovery and then you
move that data into the table and at
that point you could also read from the
table directly or not you're not tied to
that but at that it makes it it makes it
that the chance of a worker doesn't
doesn't die the old so it doesn't affect
the existence of the data that doesn't
protect you from the node itself going
down and also the other thing is that
then you have to worry about
synchronization because and in terms of
like which so basically you have to do
the same work twice they leap from the
table and the leaf from Postgres when
it's time to expiry so that's the other
thing does anyone else have another
question all right all right cool thank
you okay cool
thank you very much
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>