<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>A Case Study in LASP and Distribution at Scale | London Erlang User Group | Coder Coacher - Coaching Coders</title><meta content="A Case Study in LASP and Distribution at Scale | London Erlang User Group - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>A Case Study in LASP and Distribution at Scale | London Erlang User Group</b></h2><h5 class="post__date">2017-11-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/lhHtiBpDa54" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I thought that was a sticker oops do I
just start yeah good I'm going to show
you the code mash there's still a
discount to the end of the week we use
this special code you can get 10% off
crazy Winkeljohn it's going to talk
about last as he pretty enough to read
under the information but I'm gonna get
started okay let me so let me double
check this Mike to make sure I wasn't
lying okay
hi everybody alright so I'm gonna give a
talk this talk isn't actually gonna have
a lot of erlang in it so you're not
going to see a lot of her line codes
we're gonna talk about everything we've
done is on Erlang
so all of this the numbers you're going
to see in the graphs and everything is
all built on early but I'm just not
gonna show you any erlan code so this is
a talk that I am so rich ESCO has to ask
me to give a talk and I was busy
preparing my velocity thing this morning
so I said I have this talk it should be
interesting and so hopefully you'll find
some interest out of it as well okay so
I'm Krista I'm a PhD student at the
university couple into the pond and
instituto superior technical and lisbon
and this is joint work with Mineo and
kaiser Swanson and Oxford and for a
variety of other places and all of this
work has been funded by the EU under
this project than its predecessor so
this is a grant that we have under the
horizon 2020 program which funds us for
three years doing edge beginning okay
and if you know a parrot cinder who's a
big Erlang name he's also in this
project for this is what's our lang
going on and these projects right so
where we come from so where all of our
research is in it's kind of in this area
about distributed applications and kind
of the way that we're approaching the
problem here is that distributed
applications are the reality of how
you're building things today this is
kind of the standard thing that
everybody is doing and basically what a
lot of people but many people say is
completely obvious but what a lot of
people don't realize
is that even if you're writing these
applications are partially in the
browser or they're on a mobile phone
like this you're dealing with usually
operating on local state that state has
been retrieved from the server that
state is most likely style especially if
you have a popular app that's probably
not stale if your application is not
popular and typically we have this
problem where if I have two copies in
that state and I can currently modify it
I have to deal with conflict resolution
if those changes conflict okay and so
typically what systems do a lot of
systems systems like parse that was a
mobile platform systems like Apaches
because sounds really what they'll
typically do is they'll type Sansa
rights and then they'll just drop one on
the floor they'll throw it away and
they'll say I'll take the last one based
on the clock and we know this is really
bad from a systems perspective from a
design perspective from semantic side
because one you're throwing away an
update that most likely really meant
something otherwise the user wouldn't
have done the update and two it's really
difficult to synchronize clocks and just
say you know even with things like NTP
PTP this is really hard at least in the
Microsoft Azure environment where
haloruns they have seen clock skew I
think up until something like 80 seconds
or something like this so this is a this
is a thing that can happen when you have
computers that are running a lot of
services okay and so finally the last
kind of point here is that a lot of the
applications today can operate while
they're offline because the client code
is all running on the server itself so
if I'm it's the summarize kind of the
problems for audiences that might not be
familiar with the large-scale
distributed applications we have
concurrent updates to shared state we
have conflict resolution so how we
figure out how to deal with these things
we have to deal with consistencies of
replicas so a lot of times state will be
State will be stale or out-of-date
because you haven't contacted the server
recently and finally ordering events and
update visibility so given I have an
event that 10 people are going to see do
they all see it in the same order do
they see it in different orders and
finally update
in that if I perform some updates how do
I know how long it's going to take for
everybody to observe that updated the
system okay so the traditional
architecture that we're usually building
things with today roughly looks like
this we have three tier we have clients
we have servers that are on business
logic and we have a database now in this
model communication typically goes
through the data center application
servers are where we write our code and
finally clients are running clients must
be online so the clients that cannot
communicate with the data center and the
application server cannot make progress
so we analyze this architecture there's
some interesting points so first that
this is a very easy application to
program we don't have to think about a
lot of currency problems because we're
synchronizing through the database
effectively using it as shared memory we
have application servers that are
roughly going to be sequential or
concurrent so these are very easy to
program but the downsides of this
architecture are that the arkin is
extremely high latency because updates
must occur at the clients and be routed
into the application service to the
database and back and it has low
availability because if these clients
are offline they can't make progress and
so in the application that we focus on
these applications typically run a
geographic scale so continent-continent
wide and large get like multiple
continents and so these round trips are
really problematic because the fastest
you can do this is 152 milliseconds if
you're traveling at the speed of light
which we know we're not that our
applications are not so if we imagine
what an ideal architecture might look
like what I really want to do here is I
want to replicate an application code on
the client devices I'd like to store
some cache of the state on the devices
as well
I'd like to connect my clients in a
peer-to-peer manner so they can
communicate with one another low latency
interactions and so we observe is we
have state replicated at the client from
the database we have clients that are
can remain online under partitions and
we have our caveat line
who can keep doing things if they get
disconnected so if we an that we want
the analysis on this this app this is
extremely low latency operations can
occur locally I get that near native
experience its high availability its
fault tolerance this client gets
disconnected and so this client
communicate to the servers and stuff and
the other clients on the app others
rights and the system exhibits weak
consistency so this is what makes the
application very difficult to program
and the challenges here are because we
have to reason about multiple
communication paths we have to deal with
messages peering on multiple paths so if
this client sends a message to this
client it may be observed on two
possible paths because of the redundancy
we're adding to the system so we're
adding redundancy to get high
availability but we're making this
system more difficult to program so I a
highlight kind of some historical
approaches that have been designed to
try to solve this problem so I'll
highlight a few so one is the Bayou
system by Doug Terry this came out at
Xerox PARC this was a database and
firming environment for mobile devices
that would be periodically connected and
this system is kind of one of the early
is not that first because it first
appears much earlier than this by 10
years or more but this is one of the
first systems that says conflict
resolution can be supplied by the user
as piece of code and then the database
will resolve the conflicts based on that
code so this kind of starts the genesis
of the work on automatic conflict
resolution that you see in systems like
react I there's - all highly - languages
from the systems community of bloom and
bloom L these are languages for
programming lattices and weekly
consistent systems building protocols
where you don't need to coordinate it
very redundant and finally two systems
you
might not be familiar with cloud types
and the whole sequence protocol which
are kind of languages that try to bring
CRT T's into the language which is which
is what lasts does now what's
interesting here is that all of these
systems have been theoretically designed
in the lab they've been designed on
paper by researchers and research
reports that have gone on to be in very
big conferences but none of these things
have actually run in the real world no
one has actually done a real world
environment test of these things and
kind of the argument you know the kind
of argument this talk is going to be
that building super large scale systems
and actually making them work on real
cloud environments is really difficult
and so that's the story that I'm gonna
tell so a story I want to tell is about
a paper and a programming language that
I've been working on called glass as the
paradigm PPTP e15 and this is when I was
at bachelor technologies working on
react with my advisor peter van roy in
Belgium so our last news is it's a
programming language that's implemented
as a library because it's a prototype
and it allows for safe distributed
large-scale distributed programming is
very very very very restricted
programming language because you only
can do things that are safe under these
like pervasively like pathologically bad
at deployments okay
basically what it does is it takes CRT
T's which you've probably heard a lot
about because they're used by that 365
various by reactor they used by all
these people and what you can think of
these aren't just abstract data types
that contain a merge function that says
for any two copies of the state I can
merge them together and come to some
subsequent state ok and this is really
nice because when you have these large
scale distributed system react doesn't
even run it that big of a scale spoken
as a batch of employee former fashion
employee you know even a small scale you
can run into conflicting updates if
you're updating rate is high enough and
your
synchronization between nodes is not
fast is not quick enough so this value
convergence is very important it says
under concurrency buy diverge I always
have a merge point and so what the seer
duties do is try to provide a meaningful
merge so obviously the most trivial
merge is to throw one away and pick the
other one
these trying to be a bit more meaningful
about it so they modeled sets and
dictionaries and counters and things
like this
okay and basically the programming model
tries to provide a functional like data
abstraction sorry a functional
programming model on top of this here DT
data abstraction so looks like it what
any example looks like so this is kind
of pseudocode e it's like simplified
Erlang
apparently my comments are still in your
hurl and even though I took all the
Erlang out of the syntax like some of
the comments
so here I'm going to create a collection
a set that's called a and I want maybe I
wanted to Rive and you want to drive a
new set B and that new B will be derived
by taking a computing the Cartesian
product with a filtered copy of a
something kind of trivial like that and
then and so what's interesting to note
here first is that a is going to be a
memory location that's replicated to
everybody that's participating in the
system so it's fully replicated when I
great B everybody will also have V so
this is a fully replicated database in
effect and then the language provides a
process that concurrent process
abstraction that allows me to fork off
things that keep running no no no now
the thing to note here is that is that
when this says doing sort of random
memory so imagine this is a process
that's continuously inserting random
numbers into a what happens is every
time you change is a it will join that
value with the current one so or produce
a change so will modify it locally and
then it will take everybody's copy of
together so everybody is slowly
approaching the final value and then we
even think of this is how databases work
so they kind of get a little bit more
technical about how this works all of
our variables in this language have nice
little abstractions like this is a set
however it's actually a joint
semi-lattice
this is what gives us this merge
property and then these functions that
convert these semi lattices into other
semi lattices so B is a set it looks
like a set that looks like an earlier
set when you interact with it this is
actually a homomorphism through these
functions this is kind of lifted in kind
of the haskell sense so you don't
actually see any of the seminars bits it
happens automatically and like I said
these concurrent editions always join
with the current value so if I have a
register and I change the value of the
register I will use whatever the merge
property is to advance that okay so we
had this project called sync free erlang
solutions was a partner in the end try
fork was a partner bachelor was a
partner a lot of people were partners
and and basically on Romeo entertainment
was another partner and so what we
wanted to do is see if we could build a
application for Romeo that specified by
them that was design from the Angry
Birds use case of one that takes in the
other's real world and this was all done
in the context of the sync free European
Union 7th Framework Programme project
and so the idea here was that I wanted
to extend my mobile app to have the
ability to display advertisements and
then track how many times these
advertisements were clicked now once
those advertisements were clicked a
particular number of times I'm well
shown sorry not clicked impression so
showing once it hit a particular number
of times I want to disable the app so
it's not displayed anymore and so
there's interesting application
requirements here so the first is that
these counters that track how many times
the ads are being displayed are being
modified by every client so it's
tenshun shared data the second is that
they wanted the desire to scale the
systems and millions of clients of
mobile devices and the trick is that
they wanted to do this all while the
system was offline so you know as a
thought experiment you can say well can
I just use a number for this right and
so if I have the number one and I say
I've shown the everyone 10 that my kids
Brian and and he also has one and then
we both shown that now is this - a - so
how do I know you know kind of core
problem here is that if I use a regular
number how do I know if we saw the same
- like we share one right you're - as
part of my one in a my - as part one in
yours right and so really when I merge
those together they count to be three
but the count is two if I just naively
pick the maximum if I add them together
I get for it that's wrong as well so
even if something as seemingly easy is
just trying to track how many times
somebody has seen something gets very
difficult when you do this offline and
your clients do not talk to one another
hence the motivation for the work so I'm
going to show what this application
roughly looks like implemented and last
so it's I'll do it with just diagrams
not code so what we need to do is first
initialize a counter for each add then
what we're going to do is we're going to
select a bunch of adds to be displayed
and those would be put on the clients
and then we're going to write some code
that says counter hits a particular
threshold I'm going to remove something
from the set up ads that should be
displayed now this is also a tricky
thing how do I remove something without
coordination right so if I have a set
that's what has one two three trying to
set that's empty I add one to it I give
it to Brian Brian adds two to it and
then gives it back to me and I removed
one and then I send here the empty set
does he think is that the way message
was that the original message when the
set didn't have one in it yet with that
delayed we don't know
so even
removals is very very difficult and we
have to do this in a very particular way
and that's why we try to hire all this
with these data types from the user so
we have some different types of CRT
teeth here we're gonna have objects
we're gonna have collections and we're
gonna have processes and so basically
the way the application starts is that
we're going to create a bunch of ads and
insert them into a collection we're
going to take a bunch of contracts and
insert them into a collection now when
we want to determine the selection of
ads that should be displayed we're going
to take ads so contracts are basically
just the thing to say yes we should
display this out and so what we want to
do is we want to take the join of these
two so we compute the Cartesian product
of ads and contracts which gives us ads
content ads cross contracts we want to
filter them based on only the ads that
have contracts and then we want to just
project this to a list of ads that can
be displayed so this is actually how the
code executes sandblasted executes as a
data flow there's two collections we
produce some intermediary collections
then we produce a final collection so
this is how it actually looks if you
write it with the functional
Combinator's and last but we'll look
this way but to make things easier for
the programmer we can really write it
like this so we can say I just wanna
select the ads join the contracts and I
assured that I have an ad I have a
contract for every ad that should be
displayed so what happens is when you
type this into last peer it compiles to
this execution and these are early
processes that drive the entire thing so
we have kind of a programming we do a
little bit of coaching on top of this
now finally we have to disable ads once
they've been displayed too many times
and so what we do is we start a bunch of
processes running on the note
and these processes basically are
waiting for a threshold condition that
would come true
and when this threshold condition
becomes true they'll perform the action
like removing the after the set there's
a couple things to note here like I said
where you can't remove from the set we
actually have to model a in a way that
shows logical time advancing for the
remove min and finally these conditions
have to be very special in the system
they have to be conditions that once
they become true stay true forever and
so these have to be monotonic conditions
so on a counter that I'm incrementing I
could safely say that when something is
this greater than 50 it will stay
greater than 50
it becomes more tricky if I can
decrement that counter okay so let's
talk about this scaling and the
implementation so we wrote our original
prototype in Erlang all of the code
today is an Erlang we have it has been
trying to think of how long I think the
initial comment was January of 2014 or
something so we've been working on it
for a while and it's mainly good me and
two other people so if we're going out
for a very long time and basically the
way it's implemented is that it's a
programming model that's layered on top
of a database that stores all of the
states for your programs and that is
layered on top of a network topology
that is pluggable so our initial
prototype in 2015 this was running on
top of react core we had a version that
ran on top of react they used react for
the states and then layered the
programming abstraction on top then we
replaced it with react core no we built
a react core 1 then we built a react 1
as part of a prototype with part of
Basco and now we have entirely custom
system that we've written called
partisan that replaces all of this which
is where we get most of our scalability
wins from and we'll talk about
participle so the way this works is that
last code basically empowered the
experiment there's servers that control
the advertisement creations roughly 300
lines of code
this is
we'll see in a second that that's a lot
of instrumentation and we have 276 lines
of code for the clients finally 50% of
this code is instrumentation because we
had to run these experiments in Amazon
and Google cloud platform and so we
needed a lot of instrumentation to know
that things were going right because
things go wrong frequently enough that
we needed it so what's that so the
architecture so again the architecture
roughly here is a shared state database
and so every one of those variables I
showed you for the programming language
of mapped to a location in a key value
store so actually and laughs you can
layer the programming model on top of
any key value store as long as you have
a key value interface for it and you
have a call back whenever the object
changes and so we have a database called
antidote that is a kind of a competitor
to react I guess those researchers yes
but and so sort of a different type of
database but a certainty base looks a
lot like yeah it's built on react core
and basically all these variables are in
just underlying database and that
database synchronizes with other other
databases so every client runs a
database with the programming language
and all of those databases synchronize
using anti entropy techniques so class
comes in two varieties actually comes in
more but these are the two all
highlighted because these are the two
that we experimented with so data center
last is models that traditional
three-tier architecture I showed you it
is a run hop DHT which is what griot
core basically is it just
we didn't ask people where the cluster
ownership is is gossip to everybody so
they know based on a particular token
exactly what no to go to to get to it so
this is a one hop distributed hash table
which is us instance of a structured
overlay network and basically what
happens is the client we build a cluster
of last nodes and then we have a bunch
of clients and they communicate via the
server so very much that diagram I
showed you before for the traditional
architecture okay and then the second
piece here is the system that we built
later we build this in 2016
called hybrid gossip glass and so the
way hybrid gossip glass works is it's a
modification of a protocol called
high-power view and this builds an
unstructured overlay and there's an
unstructured overlay works using partial
membership so if you think about the way
distributed Erlang works today every
node knows about every other node and
communicates directly with every other
node now to achieve a scale that hybrid
this type of awesome system uses it
places a restriction on what knows how
many nodes a node can know about so for
instance if I have a cluster of a
thousand people I may only know about
five of those people and then when I
rely on is transitive message delivery
so I can disseminate my update to all
the members in the cluster by handing it
off to my five peers who handed off to
their five peers and I'd gossip out that
way so this means that I don't need to
know about everybody in the network but
it does mean that the transitive closure
of all of those members must have a sync
form a single connected component every
node must be able to reach every other
node for me to ensure that everybody can
talk to everybody and so this is the
trick on how all this works and so the
system that we implemented in Erlang
called partisan basically does this it
completely bypasses distributed or link
so there's no distributor Li don't even
we connect the nodes for some
back-channel communication
debugging but but they're all hidden and
none of the communication our system
goes over distributed Erlang it all
bypasses this completely and goes
through this membership system and I'll
show you that I don't really have any
slides that I've been on I'll show you
the library I'll give them and then
finally I once I once I have a way for
the members to talk to other all the
other members in the cluster I still
need a mechanism for sending the
information to everybody and I need a
way to do this very cheaply so C or D
T's are very expensive because even to
model a counter so to model the counters
the CRT I need to source set that size
end of the number of participants that
will never increment that counter so
normally a number would be a you know
byte or whatever and now it's a lot
better that's the challenge here is that
I have to be able to synchronize these
things efficiently so in our original
implementation we used state basic
Rinna's ation this basically means that
periodically you gossip out the values
periodically I just contact all of my
neighbors and I say here's all the
latest values for things that have
changed this is very expensive but this
was the state-of-the-art in CRT t
dissemination as of you know five years
ago or whatever so this is kind of a
standard and there's no react worked
actually with some tricks it would
gossip around vectors to try to minimize
transferring state but in the worst case
if it was a concurrent update it would
have to send the whole thing now what we
did was we modified our system to use
this Delta based strategy since Nelson
Bay strategy basically says every time I
mutate an optic locally I will buffer a
differential of that change off of my
current state I will contact my peers
and I will send those differentials to
just my peers now the trick here is that
to get these differentials to be as
small as possible and to actually match
the equivalent to have an equivalence to
this statement once
ensure that it works correctly these
differentials have to be sent in causal
order so in the order they occur so I
basically have to have a FIFO buffer for
every one of the people I know about
where I buffer these changes and then
when they come online I send them to
them I'm getting so the trick here is
that if I might in one of these muffins
what I have to know about everybody in
the cluster
I can't buffer the updates for every
customer and so the trick here is that
to to benefit from Delta Base to benefit
from Delta base dissemination I need to
minimize the amount of peers that I know
and so I need the hybrid gossip
technique so this kind of relationship
here and we actually had our first paper
on this rejected because we couldn't
actually show that the combination of
that using this traditional kind of
infrastructure or standard dist are all
with this Delta thing we couldn't
actually show that it's scaled as well
because we would just build up these
buffers and as soon as a communication
crashed so it was very it was very
important that we got these two things
working together then we saw the biggest
benefit in doing the evaluation we
actually learned something else in that
since peers can learn about updates via
other peers we also found out that when
in doing the Delta if I sent it to Brian
and Brian was going to send it to
somebody else who received something for
me he can further minimize that so along
the path based on knowledge that we
learn we can compact the object and make
it smaller and smaller and smaller this
is another trick that we learn that we
would have never known unless we
actually implemented all of this stuff
so for the experiment so what did we do
so we ran experiments in Amazon we have
been running two experiments for four
different topologies at 30 minutes each
we worked on this for nine
to actually achieve the numbers and run
the stuff at scale it cost us nine
thousand nine hundred euros and we only
got about 16 experiments or something so
that's it's pretty bad
but we have a lot of negative results
that's for sure so that's good so how
did we do this
so what we did was we we deployed 70 m3
to excel instances on Amazon ec2 these
instances were configured with zookeeper
and mezzos we packaged up the last
application into docker containers EPMD
does not play well on maze of us at all
and because of that we bypassed all of
that stuff we had a customer service
discovery system we have our custom TCP
membership layer though they made those
works is that Makos is going to run a
bunch of tasks across those machines so
you're gonna have 70 machines and it's
gonna run a bunch of tasks on when we
run 500 over clustered subdivided each
of the servers gets allocated for gigs
of memory two virtual CPU units so the
thing when we talk to the make those
people were like what the hell is to
like what the hell is 0.5 v cpu like
what does that mean so it's some like
weird time division thing that's not
actually even enforced by container
groups you are all this one when things
start getting wrong the memory limit
however is enforced by business so you
will definitely see the killer take
your cluster if you happen to back up
some messages and crash the Erlang p.m.
so this is another thing that you see in
terms of the containers
we have maces will map one task to a
UNIX process which runs the docker
container Iser inside the docker
container either you run a full colonel
and a full that's inside it that's
virtualized
inside of that we run an instance of the
Erlang VM and then inside of that we
want an instance of the last runtime
okay now just so I mean that's that's
already a lot but then if we think about
how how so now our experiments on Google
so like we have all this virtualization
but then in Google's environment you
have googors user level kubernetes
underneath that that runs on Google's
kubernetes which then runs on top of
Borg and Omega which is on top of
Hardware of machine virtualization which
is then run on hardware so you know when
you see a latency spy you literally have
no clue where it's coming from
it could come from any of these layers
and these layers these places we observe
can be anywhere between for a single
operation can be anywhere between ten
milliseconds to four hundred
milliseconds so tale agencies are a
reality of this world so now obviously
if I run 500 notes on 70 notes there's
going to be some colocation so notes
communicate through the TCP stack they
have no awareness of what is co-located
completely random distribution here
this means that nodes in the system at
varying communicational agencies and
they are very difficult to control so we
just have to deal with the reality that
way
scientific experiment these things might
not be running with same blades these
are even close to having the same
latency and then finally sometimes you
can get like you know
so before we allocated the machines all
to ourselves and we were using we were
using the virtualization layer that was
not on a machine basis you obviously see
the effects of noisy neighbors so you
might have somebody that's like I don't
know a mining Bitcoin or something like
this and why or the crimping of Vidia
and you know encoding a video and so
like you see these effects in your
graphs because noisy neighbors are
reality as well and so finally I hear it
should go without saying that this is a
conservative approximation because you
know I mean my you know if I bought the
new iPhone I mean it's obviously gonna
have a lot more CPU than 0.5 virtual CPU
nuts whatever that is and so so this is
a obviously these numbers are all a
conservative under approximation of
reality now what's interesting is when
you go to run a principled scientific
experiment on a real cloud there is a
shitload of non-determinism things go
crazy all the time and because of this
we had to be very principled with the
way we ran our experiments so what we
would do first is that we would
bootstrap some of your bootstrap we
would create the clusters we allocate
the notes we'd launch all the tasks we'd
wait for that to happen then all of our
nodes try to communicate with one
another and so we have to have a system
that doesn't start the experiment until
all the nodes can see each other because
if you happen to before you have this to
do and we learn this the hard way before
before the nodes are connected you know
so let's say that you say as soon as I
launch all the nodes run the experiment
what happens is that you launch all the
nodes none of them see each other they
over the experiment nobody does anything
and then they experience them so you
actually have to build the system that
says wait till everybody
to each other which means you actually
have to write software that will go and
ask everybody if they know everybody and
since not everybody knows everybody
because that's the design of the system
you have to build a system that
aggregates all the system and then you
know whatever you implement Dijkstra's
algorithm in your check for reach
ability within a map and you know and
that tricks for a graphic view built and
analyze it so if this is there's a lot
of work just to verify that like your
notes can talk to one another now then
we create our task now once all the
notes that see each other we begin the
experiment simulation so what we wanted
to do is just play a benchmarking
workload generator we just wanted to
generate like a million updates
see the system operate and then shut the
system down
and so there's way this versus each move
generates its own workflow and begins
executing the experiment we'll talk
about why that's important in a moment
and then they'll periodically share
their state via those mechanisms and
torture but then once everybody once all
of once the benchmarking suite has been
run so once I fire all my updates into
the system I want all the nodes to see
all the updates oh I need another
implementation that then waits until I
know that everybody's seen every update
because if you just you know if you
benchmark a single Cassandra code and
you say my Cassandra performances a
million requests a second but it never
talked to any of other knows because
it's doing that any synchronously you
don't wait for it you haven't actually
demonstrated anything and then finally
we built the system for automatically
aggregating all the metrics it generates
Canoe plots and those things with R and
does all of this really nice stuff
because we were grad students and we
would run these experiments and then go
to the bar and then come back and get
the results we actually had a system
that would automatically post the
results actually into github which would
then trigger a notification that I would
get on my phone and then I can actually
read the results of the bar actually
didn't even have to come back to the lab
so yeah I mean grass suits are lazy when
it comes to code
alright so the experimental
infrastructure so what did we do okay so
we built on mazes we ran a thousand
24 tasks for the experiment we are at a
much higher number now and I think we're
close over 2,000 now and we have a
student Lavinia who is getting up to
4,000 hopefully if he does his master's
thesis and basically what we needed to
do was slowly scale up so my trick about
this is it so another another problem
here is that we wanted to save money
because we're academics and so what we
wanted to do was build clusters as fast
as possible so we wanted to be able to
build a 10,000 what 1,024 another
cluster which ran on 140 physical
machines we wanted to do this in 15
minutes or less and that 15 minutes is
the worst case and so when this means is
that you have to deploy basis clusters
940 goes so the trick here is that I
made those if you say deploy a cluster
of 140 notes it deploys all of them and
then they all try communicating back
with the central zookeeper at the same
time now this approximately happens at
the same time that you've provisioned in
Amazon's days that are all machines of
exactly the same type in the same
network in the city availability zone
and so when they all communicate that to
zookeeper and maze versus internal
multi-access implementation they
overload the system and then heartbeats
are dropped
now when the heartbeats are dropped
Mazal doesn't know what's going on and
so mazes cause the notes abandoned or
orphaned now when the notes are marked
as abandoned or orphaned mixes will not
immediately evict them mesas will late
27 minutes and then it will evict them
so it's very difficult if you deploy the
experiment and then all of a sudden and
then all of a sudden the system makes no
progress because you have to wait 27
minutes because it known was abandoned
so what we determined is what you have
to do is you have to deploy the 20 node
cluster and then scan auto-scaling using
these CTOs calendars 240 260 280
and sometimes you would still fail in
the autoscaler sometimes a lot of times
we would build this cluster we would
finally there's one time where we waited
22 minutes for the cluster to get built
scale up to 140 notes we had 140 nodes
and I started running the experiment and
remember I told you had that checked and
we have that check and this check says
make sure everybody can see each other
so checks family so I'm sitting there
what's going on in my dungeon office in
the middle of the night I'm like what's
going on and I basically adopt floating
the Amazon page and it's like sorry
capacity overload and they took you know
40 machines back because we were bidding
up machines at little prices so we are
also trying to gain spot instances and
you know get things cheaper or than
you'd normally get them because the
instances when we ended up running our
CC to 4 XL instances these things these
wishing me a hundred and forty of them
and these machines are like 249 an hour
or something per machine and you know
these are high memory high memory
instances so I mean this is it's a pain
in the ass
ok and so make those we abandoned maze
O's we actually switch our entire
infrastructure over to Google cloud
platform and now we have a version that
works on now we have a version via Kumud
IDs and now we have a person that runs
on Azure Google and Amazon via the
kubernetes api and that has been more
predictable but a lot more expensive in
some case
now once you launch once you launch a
thousand tasks if you don't have dist or
you just have a thousand Erlang games
right so you need a way for these things
to talk to one another if they have to
know where these nodes are so we built a
system called sprinter and this is a
Earling based service discovery
mechanism so it's a system that will and
will periodically pull the kubernetes
api and looking for machines that will
figure out who should connect to those
machines it will initiate all of the
connections this is what allowed us to
perform experiment orchestration and
control now
what this system would do so the trick
of ensuring everybody can see each other
the best thing we come up with is that
what sprinter will do is they will
periodically talk to every or line mode
and it will say tell me who you don't
know and then it will periodically go
and collect this information that once
it collects that information it will
what's a collective information it will
then take that information and build a
graph for every member so it will slowly
start building a graph now once our
context everybody it will have a full
graph and then a one-on-one reach
ability algorithm to verify that it's a
single connected component and that
everybody can see each other
transitively the experiment will be
delayed until we had all the nodes
connected and then finally some of the
caveats of our protocol is that our
membership protocol is probabilistic
which means that game nodes are
disconnecting and reconnecting very fast
because of an error
not only it means that you can leave
notes bandaged isolated and so we had
one though that was an anchor point like
a connection point by a DNS name and
what knows we do is they were
periodically see if they were
disconnected and they were reconnected
this is the reality of how this thing
actually works so it did this isolation
reconnection this more finally I will
show this in a moment we had a visual
cluster debugger that allowed us to
visualize the cluster and see things and
do fancy stuff so probably the thing
that we're most proud of probably the
thing that we're most proud of here is
is partisan which is our is our
workaround for the issues that we
experience with distributed for length
it has pluggable backends for multiple
cluster topologies and so whether it
basically means is I can write my
application once and I can actually
change what the networking underneath it
looks like so this is really clever so
we have static topology which means I
can manually specify who you're going to
connect with and it's static we have a
client-server topology which will ensure
that nodes that are marked as servers
communicate with one another and clients
will connect to one of those servers so
you build with star topology then we
have another topology which is our
hyper-v topology which is a hybrid
classic implementation we have an
extension of that that uses a fork of
bachelors Plumtree
so he we took their version and we
improved it quite a bit and so we have a
version that will actually taken out of
overlay and then compute spanning tree
so you get people more efficiency
without a major loss of redundancy and
then finally we have lemons called
default and default basically connects
everybody in everybody so this gives you
a baseline when you're doing this stuff
that you can use so there's a hospital
distance is darling you know I don't
have splice this is the other time I
didn't talk about it since we're only
meetup I'll talk a bit more about how
this works so most of our engineering
work lately has been on the default
manager so the default manager is nice
because it provides so it provides TLS
so it's got a full TLS support it can do
multiple connections per node these
multiple connections per node
be classified so I can send up my
channels and so that basically means
that I can put one type of traffic on
one channel another type of traffic on a
different channel which alleviates
head-of-line blocking problems it's got
a bunch of fancy stuff around this on
managing connections and how it does all
of this stuff and we're using it to
slowly replace so some of the work I'm
doing is replacing reacts underlying
infrastructure with this to see what
kind of performance gains we get we have
core specifically so we're very very
proud of this we've had industry
adoption of this so there's at least
three company I don't know what names I
can say so there's at least three
companies that are using this which is
extremely cool on the Microsoft Orleans
version in Erlang called early ins which
is being worked on by Germany is uses
this and actually uses last to store all
the state for the cluster actually so
he's actually he runs a whole lot
actually which is very cool but it uses
this as well and I believe a bunch of
expats were people who ended up at
Comcast communications in the United
States are actually using this as well
so it's pretty neat and it allows you
like I said to swap out this topology
without actually rewriting any of your
applications
now the tricks are that there's a lot of
things that you can't do with this
mechanism so to achieve the scalability
and the performance that we get out of
this we have to basically make
everything asynchronous so we believe it
only asynchronous programming so there's
no such thing as like a gen server call
anymore
you have cast only and explicit receives
so there are some things that change
because because you can't have a call
when your transitively delivering a
message across the network it doesn't
you can do it but it doesn't really make
any sense because you don't know where
the reply is coming from and so there's
a bunch of changes there all I should
finally say that um and strict enger' is
is taking a bunch of the changes that we
did for
the things that are important and he is
plan over the next year is to port some
of them back to EPMD so the fully
Berlanga version of EPMD so it's nice
we've had impact it's nice to have
impacts when you're an academic we like
that and so this is what I the custom
visualizer looks like so this is like
you know it's static which sucks because
I don't have a video of it because I
didn't want to spend like 200 euro to
deploy the thing just for this talk so
so this is our custom ology zoom in zoom
out all this stuff it's real time so the
coolest part is seeing the cluster
assemble because you see the notes pop
up and then you see the links appear
between them and then the whole thing so
it's cool and we were criticized on the
Internet as saying like it's for show
like what do you do with it it actually
is very interesting because rather than
watching a black screen one of my
experiments from I can actually see the
clusters get set up the data sent and I
see the trip down and then form again so
it was actually a nicer thing to watch
than watching watching black text on a
screen when I stay up to like 4 a.m. on
these experiments with we have a
deadline and so this is this is this has
been kind of nice one of the things that
was half implemented that didn't
actually get there is that these links
will these links when we inject
partitions which can be colorized so we
have protectors we colorized these
things because partisan also has the
ability to do fault injection so if
you're using the hyper view back-end you
actually there's an API call you can
make that says partition is noted for
the network and then reconnect it and it
actually has two modes it can operate in
it has one mode where it will drop all
the messages so we'll make the partition
look like it's happening so that is good
for testing when you want to test
message omission now if you want to test
the resiliency of the protocol under
partitions you actually really need to
disconnect things because you have to
disconnect them to see the network kind
of reconnect so it has two ways it kind
of has
you know a pseudo partitioning where we
could just basically dark out the link
message omission and it has a real one
where the link would actually go away
take it early alright okay so so that
the next challenge was that our first
paper that we wrote that was rejected
basically used the centralized a
centralized process for running the
experiment so I launched a thousand
notes and I had one known that was going
to tell everybody what to do
so obviously the problem with that is
that you only run as fast as that one
note and that one note doesn't run very
fast even if you make that machine way
beefier with way more CPU and all this
stuff it's still bound by the amount of
time it takes to serialize and send a
message on the network and that that
takes time it is one of the most costly
operations that you can do so we needed
to have a mechanism for doing a
synchronous workflows and so to achieve
this basically the stages that we wanted
to do was we wanted to add a particular
point say all the notes start doing this
wait to do this until this time and then
wait to do this next thing until this
time and so it would do it would block
for generation of the workload the
benchmark that would then block to
ensure that everybody saw the effects of
the benchmark then it would aggregate
the logs and then it would shut down so
what we made was a custom CR DT called
we're closed
C or D T and the way it works roughly is
that it's a data structure and that kind
of enforces a like a linear sequential
protocol and so it's a data structure
that basically says it's a guarantee and
it says ok I have X amount of people in
the network and everybody starts doing
their own work so they do their work and
they mark these flags as true and then
you compose this so that when all the
flags are true please start doing the
next piece of work then everybody marks
their flags and then you move to the
next step and so this is modeled as a
it's a kind of functional it's pairs
that you compose like kind of like a
cons list lattices from node 2 I've
completed the work so you have a
dictionary
you can post those dictionaries and the
tasks and this allows us to do basically
barrier synchronization without actually
coordinating without actually having the
notes have a coordination protocol
asynchronous so roughly looks like this
all the notes move to the first stage
they start doing something they'll
continue to do this thing and then spin
until all of the notes have marked their
position with you and then move on to
the next one and so we get this with
that event generation and waiting for
things to happen then we want to edit
before you shut down the cluster we want
to ensure everybody pushes their logs up
so we can analyze the lines if you
really had to run the experiment and not
aggregate the logs before you shut the
cluster down yeah and so there was one
thing before I talk about the numbers
and kind of wrap up there was one more
thing that I wanted to talk about here
which is uh which is this decentralized
orchestration thing and so really the
importance of really one of the really
one of the very important things of
doing these high scale experiments is
that we found that to really drive the
system to get the highest through but we
needed notes to generate their own
workload to basically benchmark
themselves so they could dry or have a
one-to-one mapping so that those notes
could drive the other nodes as fast as
possible because you can't have these
single benchmark products even in like
the bachelor venture design the problem
with bachelor bench or MZ bench or any
of these other measurements is that you
you basically have this you know a notes
that sit out here that are driving the
database and so you know what you really
want to do is you want to co-locate
those little generators with them and
have
parody on one-to-one so you can drive
those as fast as possible and then
correlate those results back so it's a
much better strategy for getting
fairness in benchmarking as we saw
because centralizing the benchmarking
are not having a uniform kind of
distribution of nodes to one another can
definitely skew the numbers and you
actually end up seeing a lot of defects
of the central location of the bench
marker all right so some numbers so here
are so I have like two slides of numbers
so there's some interesting things to
note here so okay so this is the amount
of data that's transmitted for this
30-minute experiment these are the
cluster sizes and these are the
topologies so this is the data center
last running on react Cora this is the
hybrid gossip blast around a state
dissemination versus Delta dissemination
so again we don't evaluate the Delta
dissemination for the data center blast
because you can't buffer all the updates
and so the things to know are so you
know in terms of data center last it
does very well right this is kind of the
standard bind it does very well because
that has no redundancy and is the
central sort of component goes down this
is to fails to kind of move forward in
terms of hybrid gossip blasts because of
the additional resins this is the exact
same experiment the exact same portland
we see that you know this is a pretty
big increase in state and that's because
the network introduces redundancy so we
send redundant messages so since you
know in the data center one everybody
goes to the server the server goes to
everybody everybody only sees each
message once in the hybrid gossip last
because we have the resilience to
networking the network partitions
everybody can see the message more than
once and so the system is going to
transmit more state to the same
operation and then finally the deltas we
see that a lot of these messages
actually
being redundant containing redundant
states and so just by switching to a
protocol that uses the differential
dissemination we get a pretty
significant performance improvement so
this object this the strategy is
obviously the best which makes sense
because it's the newest technique that's
that has been invented okay in terms of
scale so here are numbers of running
Erlang fully replicated correlating
applications
10:24 this is the number as of as of
December of 2016 so that's when these
numbers are generated so in that
December the first or something 2016 and
so we see you know the performance here
so data center last so in our
configuration so in our particular
experiment so you can't look at these
numbers as definitive numbers here
because it's based on the hardware
running on and all of the various things
related that but we see that as we hit
this limit here the data center last
system just stops functioning we just it
does not it does not operate the server
overloads it crashes now yeah I could
make that number go higher if I threw
more hardware at it and gave it more ram
or whatever but you know we're showing
that relative comparison here so we see
that the data center one you're sending
you're sending through the server
server's orchestrating everything it
falls off our digital number is actually
put the same experiments are much much
lower where we see the fall off
completely and then I do have some notes
here yep so it fails to get us above is
256 no limit we see that in terms of the
hybrid gossip last one
this is sending the full full state and
it's not sending it say to everybody
with the redundancy and the data
structure is growing on the order of
numbers of the number of peers in the
system so this is actually quadratic on
the number of
years that is a problem with the data
structure that's being used to model the
counter it's not a problem with the
infrastructure so we know that from
doing further investigations and then
obviously we see the same growth pattern
here with the hybrid Gaza Delta one the
same kind of quadratic growth pattern
based on the notes and again this is a
data structure problem that we can
address but we see that you know
obviously the hybrid strategy is the
best so this is what you should be using
we actually be using Delta's okay so
that's kind of the end of the talk here
so I'll just give some takeaways about
what we learned so this work took the
first year of my PhD it took nine months
to generate these numbers again we spent
nine thousand nine hundred euros and
then we presented a live demo of this
work at the European Commission of
Brussels which was frightening for me to
do as a first year PhD student but we
did learn a lot so some of the things
that we immediately learned here are
that the existing frameworks and tooling
that you're going to build on top of a
c2 mais o--'s Google cloud platform
kubernetes all this stuff the existing
frameworks you're building on are going
to impact your results if you're
building something that's high scale and
you have to know how those things are
going to impact it you have to do a
thorough analysis of this otherwise your
results will be skewed we really like
visualizations we found that having
visualizations and being able to see the
cluster and stuff like this was very
nice and understanding debugging and
where it's continuing to be it's
continuing to be very valuable for us
this is especially during these
experiments we had one student from
University of Oxford that a gentleman
who was working on our high part view
protocol extensions because actually if
you can implement the protocol in the
paper which was actually
implemented only implemented in the
simulator that protocol does not work on
the real internet and the reason is is
because they make all these are really
bad assumptions like like that you can
guarantee five-foot between any two
nodes across connections that's that's a
non-trivial thing to do and the fix is
highly complex because you have to keep
these vectors around you're summarizing
information where you're getting it from
and you have to have these epoch numbers
that are durable on connections that you
have to know what messages are coming up
which connections in which epochs this
is a non-trivial thing that we end Apple
so Apple was extending Apache Cassandra
with the algorithm and we were building
it in early at the exact same time we
ran into the same problems like why does
this happen it's like ah because there's
this weird little assumption that the
paper says where it says it's gonna
fight for across all connections you
ever have
turns out you dropped connections on the
internet sometimes and you know that was
one thing we had another thing that was
around we had another thing that was
around joining leaves so sometimes a
node would join a cluster and then the
node would leave the cluster and rejoin
and sometimes because of connection
delays so it's because of connection
delays what you would see is join join
leave or sometimes you would see leave
join join so the protocol gets really
confused when sees things that border
and so we basically needed to write an
entire thing I'm dealing with these fast
joints another thing is that you know
you can observe things in weird artists
because you're getting them in the VM
off the socket and like you're going off
different sockets or something with
connections it's very difficult and so
what we found is that getting these
protocols to actually work and verify
they're working is very difficult to do
and so basically basically we had a PhD
student working full time on that
we had a PhD student who's working
full-time just on the membership we had
a PhD student who was working full time
just not getting the datatypes optimized
and we had other PhD students writing
experiments there were about three of us
working on this full-time and having all
of this visual stuff was really
important the other thing that I don't
highlight in his talk but that I've
talked about publicly before is
automating automating your experiments
and we during this time because we were
in a rush because we had a deadline to
present this at the EU we had a policy
where we never accepted a pull request
without the canoe plot so we basically
said like if you're changing the
behavior of a data structure or this
networking protocol or whatever like
we've automated canoe plot generation
like it's literally these four commands
you run these four commands and if your
PR doesn't have a graph showing that
you've actually made performance better
your change does not get merged and so
this actually worked really well for us
because we got extremely high quality
code because well extremely highly
performing code because I did not accept
anything that did not meet this criteria
but also the thing to that's very nice
is you know in these experiments it's
nice to have a hypothesis that the graph
confirms rather than generating the
graph and explaining the graph later
that's usually a valuable technique for
science the plank is to you know is that
scientific method so we also try to
explain that as well what we found is
that running all of this stuff running
all these experiments totally
non-trivial all these abstractions
provided by cloud providers is really
difficult performance can vary widely
across deployments across machines I
mean we were probably committing a
scientific crime by comparing numbers
that were run on different AWS
deployments but we did it anyway and the
reason really is because things just you
get the point wherever right
control over this you actually do get a
lot of control if you pay Amazon a
shitload of money so when you get to the
level where you're paying them so much
money that you get to pick where your
VMs go like that's the Netflix level
okay I'm a researcher who lives in a
small town in Belgium I do not have
Netflix money like that this is probably
Netflix for like a minute 9900 euros so
you know just in conclusion I guess the
final point here is that these these
things take time
and it's it's very hard but we were very
happy because as far as we know
so when Ericsson told us they ran their
biggest Erlang cluster at 200 notes we
were very happy to show 1,024 with our
efforts of over nine minutes so we were
very proud of it
so that's kind of the end but I want to
show you the libraries just so you can
check it out and so basically under the
last playing organization we have all of
our lovely contributors here worked on
these things alright so these are all of
our lovely contributors so there's some
expat show people in here you might
recognize Ishaan Maccabi Scott Richie
there's a bunch of co-routine experts
these these are Portuguese and then me
and some other people we have a bunch of
interesting projects up here most
notably we have last this is the actual
implementation of the programming
language in datastore we have us we have
this thing called gen flow which was a
based on paper that we wrote four years
ago which was a basic data flow
computations which went on to influence
so the data flow stuff an elixir we have
our scalable peer service medicine you
can use this is a version of plug is a
spanning tree protocol that runs on top
of partisan that's a highly forked from
the batch operation it's much more
optimized and then we have two others we
have a
we bring and we have implementation of
CEO duties so this is now actually the
referent this is one of two reference
implementations so if you want CEO
duties this is the place to go these are
the latest stuff the most optimized is
state of the art this is one of two
reference implementations and so there's
partisan stuff if you'd like to check it
out there's some documentation right
here
so as it looks that's probably booting a
VM and Amazon so we have documentation
on all of this stuff so last PG last
important it's all there and we have a
slack channel you can ask us questions
we're in there working all the time and
like I said you know a lot of this stuff
has been adopted some guests and people
from which is thanks no no so yeah so
it's interesting right so we have done a
significant amount of work on yeah so
basically yes there's a lot of
interesting that we've thought a lot
about this and we don't really have any
definitive solution just there's a lot
of really interesting things here one is
that for these large clusters so
distributor like one of the problems
distributor like has
his heart beating right on connections
so what protocols do not you start
beating what we use is basically we keep
kind of active TCP connections but keep
lives that are being managed by TCP and
we don't actually do explicit part
feeding on the channel now we leave
these connections open and basically we
use the connection disconnecting as a
family now in the in the hypercube
protocol so in our everybody talks to
everybody system the failure detection
gets a bit more complicated because now
we're maintaining multiple connections
to the node and so what actually
constitutes a failure and then the hyper
view one I may have a connection to
Brian through two of my peers and if I
receive a Down message from one how do I
know that implies that he hasn't been
partitioned from that period and so
Bezier detection obviously is a hard
problem you know that it's actually not
solvable in the general case on any
secret summary and so monitors and links
and all the stuff relied on failure
detection
I mean basically the way it works at
Erlang is that if you drop on these
heartbeats enough times you can fire
this message so that's a problem so what
we what we did so someone's basically
when we found out is that if you can
offload a lot of traffic off the distal
channels so if you can imagine so we go
you know you have to start all over here
a fully connected everything's over one
channel and on this side you have ours
which is like partially connected and we
do everything on TCP right there is an
interesting middle space that we're
exploring which is why don't you use
just or for channel control messages
only and we open additional data
channels and so we have a prototype of
something that I've been working on the
past two weeks or whatever actually know
it when we're going for like three
months but not the past two weeks where
we basically built these 200 up clusters
using this drone just for channeling and
then we'll do data on that additional
will do it on these backplane
basically and what's nice about this is
you actually can do calls this way I
just figured this out last week and so
what we do is you you'll send the call
of a monitor over the Asaro and you'll
receive the responses back on TCP it's
very very nice and so this is the thing
I want to give a talk about it looks
like done all this work and I need to I
need to give a talk about it so maybe
I'll do another leg factories but we
found that you can do cause you can do
some loggers so I think there's an
interesting space where we can retrofit
existing applications but in this kind
of large in the super large scale one
it's probably not going to be possible
to do that now there is one exception in
that the work that we're exploring with
this master student is basically
building hierarchical topologies where
we think that we can maintain full
connectivity between the different
levels of the hierarchy so we can
basically kind of like what SD or link
does a little bit but but improved on
top of that because basically what we
want to do is do like a hierarchical
hyper meter so rather than there's this
a bit more naive which is fully
connected just so trying to be smarter
than that because we think that there's
a lot of area so I may be getting it so
hopefully that if nothing interesting
yeah yeah going through your oh we even
wonder sound like the issues yeah
yeah yeah okay but like it's weird
because like there's no way for me to
respond to you so like I click the
button like it's a issue result but I
was like the page is still up so
actually it's probably from strange
therefore something this yes yeah I just
what I am you one thing that caught my
attention is you talked about the single
system illusion yeah now I come from a
company that is strongly ERP based sure
and the focus of our attention at the
moment is to take the monolithic on
premise software and expose it through
the cloud which is turning out to be
quite a challenge but we are still based
we still have this underlying assumption
that there will be one database
somewhere in some back-end that is your
single source of truth and what caught
my attention here was the single system
illusion where can you see more how
about how you think this will work okay
you're talking about the Angry Birds
advert counter which is fine but yes I
ain't coming from a full-blown of ERP
system where people are running sales
and distribution material management
finance HR Payroll the whole works
sure so I need the point at the point of
the common the point of not that I make
is that our kind of research agenda is
is trying to is trying to look at that
model differently what we're trying to
say is
that shows a few things right you want
to put all things and there's a few
things that you put all the things put
all of your information on one day this
right so we don't have it we don't have
an answer to the security story right
we're not interested in that we're not
security researchers now if we put the
security and data if we put the security
kind of constraints aside there is
something interesting about the notion
that that the data is being generated by
clients clients are actually generating
that data right users of the system are
generating that data and they coordinate
with a centralized point to story so we
think that's a very interesting thing
because we think that's an artifact of
von Neumann von Neumann architecture
that is an artifact of fun -
architecture it's the idea that you have
a centrally located memory that people
are coordinating on to change right
there's a concurrency problem and so we
think the the interesting idea that we
think is interesting from a research
perspective is saying that given that
this is an artifact of a programming
model what happens if we build a
programming language that assumes the
opposite assumes that if I generates a
piece of data and you generates a piece
of data rather than on storing it in a
common place how do we just have a way
of saying where the information is right
that's another way you could build
systems now the problem with that is
that you authenticate an optimization
all these things become really complex
right so essentially locating it is good
to solve those other problems but from a
research agenda when we don't have to
think about security as a researcher to
think about security I publish it one
area of my areas and a very particular
thing and so we like to think at it we
like to think about it different from a
philosophical point of view and so the
comment that I make about the single
system evolution is because in fact it
in fact it is true that the clients are
generating their day right like this
idea that we're going to
located is not where the data is
actually being generated it's just a
convenient source okay from my context
that's only partially true
because in our software you have Japan
well general broadly speaking of two
categories of data you have master data
which is stored centrally and everyone
references this but from that
transactional data so who's making the
master dinner
ultimately it's made by the users but
it's only made once it's reversible
yeah okay so what I'm saying is that so
from my point of view I mean I mean from
my point of view I'm just looking at it
from a different perspective which is
like I mean the perspective of saying
that the user who wrote that first thing
so he asked the question here like so
you say one user makes one user makes
this thing that most people read right
and you put it in a central place now
why couldn't that sent to a place B that
users computer where it was created it
could be right exactly so that's the
point I'm thinking is that it would be
for historical reasons but the point I'm
making is that if I'm making here is
that locating the data where it's being
generated that is really the source of
truth right where it's actually
generated is really the origin point if
I if I actually wanted to build so if
you look at these languages and these
distributed systems I have comments
these systems that are exciting there's
entire languages we haven't actually a
variant of are stuff that uses problems
and if you actually look at these
systems they'll track the entire lineage
of a document back to the original
person right and so in that case it's
kind of saying like for me to like go
put it in this data
I mean why couldn't it just be on my
computer right central location is
important for security it's important
for authorization it's important for
access control and it's important
because it serves as a directory because
it is extremely difficult to solve the
directory problem in a decentralized
manner efficiently and because of that
we put things in central locations
yeah that doesn't mean I'm just saying
that I come from accompanies like a 40
plus year heritage of doing things I
mean you know that's like I mean my is I
mean yeah I mean I I completely
understand I I mean I understand I
understand completely
any from my perspective I mean I am the
guy who got up on stage at velocity
today in front of like a thousand some
odd people or something and I'm the
person who told them every program in
your language that you're using is wrong
we need a new set of languages so I mean
I completely understand or sympathize
with the uphill you know the uphill
battle in the changing paradigm because
as much as I say why would you program
it go it's a 1970s language that has
nothing interesting and actually
implements Tony Moore monitors incorrect
and not one of those implements channels
incorrectly when I get up there and see
that I don't really make very many
friends it doesn't change the fact that
everybody yeah it's true because they're
essentially I've also gotten very very
mean comments from a lot of people as
well this thing it eat you no way you
just say
is the same time for you you could find
when you took the difference between the
animal model of concurrency versus what
people will do with software collection
with memory connections and how we think
about people see recently it makes sense
for people who sing in demo blocks and
that's what you do with your productions
just how people sing so you can point
you around yeah I mean yeah even when
they're laying if you look at her like
you know the the concurrency model there
still baffles people I mean because
they're used they're used to shared
memory with a lot like take out a lock
or whatever right and so I it's a very
different way of thinking about things I
you know I wear my thesis is is
basically on large-scale edge networks
where large scale mobile edge networks
where it is impossible to send all the
data back to a central point it's just
not possible there's too much of it and
so the whole point is to move the
computing out to the edge and to enable
processing at the edge so data can be
reduces there and we absolutely send the
meaningful results of computations
because we'll only get into a place
where it's gonna become harder and
harder and harder to store to same every
piece of information in a centralized
database and processing just it's not
tenable with the amount of data that's
being generated long term now you know
in the 70s whatever I could run one
instance in db2 and be fine because
every piece of data was a scam yeah yeah
and you know data was generated by
people so
they do have duplicates by design yes
they have duplicates by design so the
trick here is that okay so yeah here's
the trick is so there's a variety of
these algorithms type our view scamp all
of this stuff right thinking all these
things the trick is that you start with
everybody knowing about everybody and
you want to minimize the redundancy as
much as possible right where you still
stay connected so you do need some you
want some redundancy because you want to
be able to withstand partitions but you
don't want to have enough that the
protocol becomes inefficient so it's a
game that you have to play to get in
here
and so the predominant technique that is
winning today in this game is what you
do is you build the unstructured overlay
so random or dos running a kind of
overlay and you accept that don't be
some percentage of duplication
redundancy and then usually what you do
is you on top of that network build a
spanning tree to reduce the redundancy
so you rely on there being redundancy so
if you think about it you wanted to
build your random overlay there's a
bunch of redundancy high part of you
does this so our software does this
hyper-v does this and then what you do
is you want the most efficient message
transmission possible so you construct a
spanning tree and you eliminate all
redundancy now the problem is is that
when there's a network issue and you
drop a connection the tree remain noble
are going to be a tree and what you do
is you use the redundancy on the overlay
network to flood the network messages to
rebuild the tree so you have this
two-tiered approach which is a an
optimized transmission level this is why
the protocol is called hybrid gossip
because it's basically an optimized
transition protocol with no redundancy
and then underneath it you have a highly
redundant overlay that that runs on top
of that provides the ability to repair
the other one right sees I know that was
a lot but if you email me or tweet me or
something I can send you more
information but basically it's a so all
of the work that we're doing I'm very a
design tenant for all of the work that
we're doing follows this hybrid mode it
basically says that you want to have a
less efficient redundant system for
maintenance and repairing of the
optimized protocol that runs on top of
it so yes there is redundancy so most of
our configuration that we hit these
numbers with so to hit these numbers
here the configuration was so hyper view
also works all huh there is redundancy
but then hyper view does an additional
thing which basically says that it will
only talk to a fix-up appears but if it
learns about more peers it will sort
backup list that it won't talk to so its
peers that it knows are in the network
but it won't talk to them and then when
a node that it is talking to that might
be redundant gets disconnected then it
will select a backup here and swap it in
so that's the way that for the car pairs
and then the tree protocol repairs by
falling back to a flood to rebuild the
tree
since update the email of the met date I
like that saying is like I was told to
if he muscles beat this one or this one
yeah exactly
so yeah one of the communication levels
you have your actual application data on
the optimize one and then on the backup
one you have metadata about managing the
yeah it's it's it's very complicated
it's very nuanced there's actually a
further optimization so the Tremont is
interesting but it relies on every node
computing a spanning tree from itself
and so you have to do that in a
decentralized manner so how do you build
a treaty centrally and so you actually
play this trick where you flood the
network with messages and as you're
flooding the network and messages as
soon as you start seeing duplicates you
know you've detected a cyclone on the
graph and once you have second on the
graph you prune and then you prune those
links and slowly the pruning will
converge on a tree which is very clever
trick now the trick with then the
further trick that you have to do is
that as you're sending messages you
don't know when the tree becomes no
longer a tree and so the trick you do
there is that you are sending on the
data Channel and then periodically
you'll send metadata messages on the
backup links so the peers you don't talk
to and those backup links you'll just
basically say the last message I sent
was identifiers whatever and as soon as
there's no notices that it received via
the redundant backup channel a
notification that it should have
received a message it hasn't received
then you know the tree is broken and
then you run a protocol to repair the
tree these protocols obviously have non
are not super optimal performance here
because you're relying on all this
transitive delivery you have to send all
this metadata so you know you got you
don't get like you know if you connected
to everybody and have a single TCP
channel would be faster obviously than
detecting this stuff but these protocols
operate fully decentralized
just normally I you talk to
when happening when you try to come into
my cookie dough agent paper yeah yeah do
you know anyone else up try to implement
it says to it yeah yeah there's three
implementations so somebody else try to
implement it in our line they called it
Jen hyper it was like a behavior or
something I looked at it the
implementation is not sound so do not
use that word
I absolutely rather it is like a very
faithful implementation of the paper but
it's clear that they never ran it at
scale because you can't have a
Greenfield it from the paper and expect
it to work I we are probably the primary
implementation at this point Apache
Cassandra has an implementation of it
but they don't use the backup views so
in the backup user used to repair the
primary views this backup these backup
lists of nodes are used to repair the
primary is under failure and Apache
Cassandra or already needs to know about
every node of the system because it has
to do routing for the database and so
their backup list is just every node in
the system that actually makes solving
the problem a lot easier so it's
actually interesting it makes solving
the implementation a lot easier but it
actually causes you to prepare slower
because you have more peers to pick from
and those peers are not optimized based
on so normally in the regular protocol
that peer list will be peers that
connected you within some number of
joints so you know that they probably
have lower latency tier than the other
peers and so in the case where you can
possibly select from any node it may
take you longer to find a note that
actually has the best performance so
there are some interesting challenges
there but we have a we have a full
write-up in a paper is we have a full
write-up on a paper on all of the
changes that we made to their paper yeah
yeah but I mean it's been fun I don't
know I mean how many papers end up not
surviving and yes truffle yeah after you
deal with a couple of papers how many
does not survive implementation yeah
yeah that's a
it's a really good question yeah I'm
sure a lot I'm sure a lot but I mean at
least in this work we have I mean is
this implementation it's probably at
least ten papers worth of other people's
work - two of ours - no not me I think
our work produced two conference papers
and two workshop papers and then we
probably give up my techniques from five
other papers so soon to see any papers
hyper-v do Plumtree and one more than
I'm forgetting whatever yeah but yeah so
it's a significant amount work yeah so
you know the conflict resolution
strategy in a in the system is I would
say fundamentally the main problem like
you have to understand what there's your
Mulligan what the clients require other
or me is not an increase I so the
datatypes attempt to provide a general
collection of data types that can be
used as building blocks so that conflict
resolution is based on trying to model
trying to model as a mostly to the
sequential semantics as possible okay so
for instance with a set to give a set
and if the set has the current editions
and removals it basically assumes that
you won't be issuing ye removals so in
that case the additional wins because it
assumes that the removal must've come
after in addition so it's like in most
cases it makes sense unless you write
applications that blindly remove from
sets without observing the element in
them
which most people don't do so it tries
to model semantics and it tries to model
a general conflict resolution
they decide as sanely as possible but
yeah but but this system actually the
last system only expects from the data
types an API that provides a mutator
that is guaranteed to be inflated and a
joint function that is guaranteed to
form a guarantee to provide at least
upper bound over semi-lattice and so you
can literally implement any Erlang code
with this behavior and then plug it into
our system and I would just worry
because we totally at the abstraction
boundary other main results at all areas
on what it takes to design system we're
not falling into the single system I
mean change essentially what it is to
the outside I mean for someone I don't
know that's what we're figuring out I
mean our point right now has to assume
that clients own all their data that's
partially replicated you need to figure
out how to find it and we want to
provide a programming language that is
completely safe under under distribution
that's kind of that's basically my
thesis and so you know the challenge
that we're working on right now is
saying like okay we have a system that's
all C or D T's everything has to be
commutative given that if I have single
system semantics and I have this
distributed semantics what's the
differential basically what are the
programs that I absolutely cannot write
with this language and then we figure
out how to get those things and so most
of our work most of all of our group's
work is on that so basically from the
database side of things we have a
database that we work on that is under
the same design principles and you do
yes and the way internal works is that
it provides causal consistent so it
doesn't implement asset transactions yet
but it will shortly it provides causal
consistency week transactions so Adam is
the only week isolation it's nice
everything's to see Rd T which means you
never roll back a transaction ever and
then it provides asset transactions it
will provide asset transactions and what
colleagues of mine have done is they
built an analysis technique that's based
on this first-order predicate logic that
can be extracted in applications and it
will basically say rather than use asset
transactions everywhere
I actually can tell you just the lines
of code that need asset transactions and
it's interesting because you know from
analysis point of view this is really
only on things that involve
preconditions so if you're actually
doing if you want to do referent if you
want to do referential integrity in the
database without cascading deletes then
basically you have to do is write in the
objects before you read their references
and your phone if you want to write a
group of references together or support
cascading deletes you actually can do
weak isolation atomic transactions under
snapshot isolation and you could do that
fully available and then if you want to
check a condition and do something well
since that condition can be violated in
the interleaving you need asset
transactions and so basically all of our
all of our so our goal as researchers at
least in my group is you know rather
than start from the single system
guarantees we want to start from really
weak ones and build up to it only where
we need it so we start by doing dirty
stuff and having the coordination
and so in database design this is a much
better design than the spanner design so
this vendor designers use thousands of
transactions for everything the system
will run really slow will rely on your
two phase locking two-phase commit and
will reduce the window it takes for
commits with atomic clocks and GPS right
that's a great way to scale asset
transactions if you actually need asset
transactions and for some of Google
stuff they do need geo distributed as to
transactions otherwise they wouldn't
build this thing but for an average Joe
schmo's you know whatever making a web
app to sell widgets you know probably
doesn't need asset transactions forever
let's be honest was he used today
probably doula provides a message
collection you may even usability dos
what's that what they used today to the
reject yeah I mean by default like
whatever like post crap notebook got by
default post dresses like snapshot
isolation anyway it's not serializable
anyway or something where they might
have just changed it or something but
like an Oracle was calling I mean
everything's you know like if it uses
crazy enough to quickly switch they're
constantly to yeah we can interrupt and
discussion but once it goes it's not
yeah but even in your case you actually
don't need asset transactions causal
causal consistency would be fine yeah
and I say we just guarantee user
operations happen in order yeah yeah
quite fun actually if there was only one
application server you could talk to
fight but yeah assuming a system yeah
yeah well simpleton aunty Menace yeah
yeah yeah yeah yep yeah yeah
concurrency currencies Harden yeah
Kenzie said you had a question yeah I
was curious at you evaluated and the
other either structured or unstructured
overlay networks we did not so one of
the things is that so everybody has been
asking for this and they've all been
asking me I don't know why but everybody
has been asking for a DHT that is not
react or not that they have a problem
with react or is DHT what they want is
like cord or like a corn style DHT
without all of the other react course
stuff they want just the DHT algorithm
so a lot of people I think at least four
people have asked me to extract it from
an accordion because what's amazing is
that for some reason so now that you
know fashion is no more which is very
sad but now with the demise of bass show
and nobody working on react core anymore
really I've somehow gone from like the
guy who like was the junior developer on
it to now like the expert I'm like one
of the expert react or people now for
some reason and and so everybody has
been asking me early ins especially
wants a corn style DHT and they
found at once there is an implementation
of cord I buy what Tristan told me is
it's not very good like there's some
sort of problem with it where I forget
what he's skeptical of it being correct
or working or something like this so but
they're using it for now so no because
it's really hard to find like really
high quality or like libraries for for
some of these algorithms it's very
difficult so no supposedly Erickson is
building academia DHT into beam it was a
I saw I saw the talk yeah
yeah I was I was how they could it go of
it yeah I said I was I was highly
critical of it and they did not
appreciate that very much so
cadonia that's a BitTorrent be hdhd yeah
so supposedly distributed rolling is
being replaced with us and it will
provide a sharded global global bond to
like global database global what's that
yeah yeah it's the basically replacing
that infrastructure of global and so
they're doing it with this DHT so
they're they're building a version of it
yeah I was not a critical in it because
I said that they should not have global
anymore because then well I think the
real I said that sounds like it's a
symptom
the real problem is we should get rid of
global but because I think it's I think
it's absurd to me building applications
in 2017 assuming that you can have a
globally geo replicated like namespace
for storing object it just seems like is
like you know 1819
his idea I in christ-like in 1980
digital buildin system I was like
fifteen hundred and one machines or
something fully replicated like geo
distributed for managing like payroll or
something like this and like even then
they were like don't do that so I think
it's I think that programming with
global is very bad and it should people
should not use it anymore but the
problem is is that distributor really
relies on global it relies on global for
global names and all of this stuff and
so that basically would have to change
you would have to have another way of
doing that and so I think probably a
better way is to have some sort of
gossip protocol that maintains the
metadata direct dictionary
maybe that's partially replicated or
geographically scoped so that you can
you know identify I identify like you
know still be able to route messages to
a name but not assume that that
directory is geo replicated but who
knows so also a challenge for Microsoft
domains as well because they have some
design decisions still on the core at
least when I was at MSR there's still a
lot of design decisions around how
things we discuss they do a little bit
better because they so that basically
the the thing that Erlang is doing is
extremely sort of the way or means does
it which is Orleans assumes that there
is a set of server nodes and a partition
the database of names across the server
nodes and then the hash names they go
find where these things are so Orleans
is a one hop DHT for the global registry
where which is required because of the
virtual actor abstraction where the
condemned Liam one is going to be a blog
hop DHT multi-hop DHT without a finger
yeah who knows we'll see what design
wins
they uses the pastry DST yep it's it's
one huh okay pastry is no no pastry is
well no it depends on the implementation
on the finger tree to a finger table
right well for some reason I didn't
think the Orleans one was before it was
multi-hop anymore I know and I don't
know oh yeah yeah yeah I know it's hard
to connect the paper to the
implementation because they also I
pretty sure I remember in one
publication they referenced using pastry
and then its subsequent publications
they reference music okay yeah I don't I
am so I don't think changed the other
trick is that if you notice one of the
papers says at most once messaging and
the other paper says at least once
messaging so that's another MP arity but
actually that was a typo because
somebody submitted the wrong version at
the camera ready
little known fact so yeah I don't
remember buy it yeah I thought yeah your
problem I mean you probably remember it
better than me and therefore I worked on
the implementation but I I i only did
transactions so I don't really remember
how I don't really remember
so that actually might be correct
and I don't know if I can go we have to
I may have to say everybody person man I
got I got a paper done by men and I have
to have a phone call I probably go to
the puppet I probably need like 30
minutes</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>