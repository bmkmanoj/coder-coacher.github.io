<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Erlang Factory 2014 -- Step By Step Guide to Building an Application Analytics System | Coder Coacher - Coaching Coders</title><meta content="Erlang Factory 2014 -- Step By Step Guide to Building an Application Analytics System - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Erlang Factory 2014 -- Step By Step Guide to Building an Application Analytics System</b></h2><h5 class="post__date">2014-03-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/XBuQg1ZElao" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Thank You Fred hmm hi I'm Anton I work
on analytic set what's up and I'm going
to be talking about a system that we've
been working on and hopefully by the end
of this talk you will have enough
information and knowledge to be able to
build something like that in your own
environment so that's why it's called
step by step guide to build an
application analytic system so a little
bit about our problem so we have quite a
few users and they're deep and what's up
is a messaging mobile messaging app and
the app is deployed to extremely diverse
environments so we have seven different
platforms tens of thousands of devices
and the users used the app in a like
very unpredictable way so it's very hard
to judge how the app performs in the
field without getting some data from the
app itself and then analyzing it to see
if we can make some improvements find
some bugs and overall improve the app
and user experience so so as I said
we've been working on a system that
internally it's called wham whatsapp
application metrics and what makes this
problem challenging is that with half a
billion active users give or take each
sending about a thousand data points
we're talking about half a trillion data
points a day and with each taken 10
bytes conservative estimate it tolls at
five terabytes per day what on the other
hand what makes this problem sort of
easier is that it's an internal tool
it's not an enterprise project it's not
it's not even open source of this yet so
and our users are engineers so we can
take a lot of shortcuts here
and before we i'ma forward and explain
how it works and what it does I'd like
to show you a quick demo so the the
thing that you're looking at here is
called the ipython notebook it's an
interactive computing environment
written in Python and it's used by lots
of people all over the world as a
interactive computing environment
basically you can write Python code in
what's called code cells and then get
results of your execution displayed in
various formats and it can be text can
be in an image can be a set of images
and be even a video or formatted XHTML
so I'm gonna run through a couple of
examples to show about a couple of real
use cases so what you're looking at here
is a and and the over overall system is
divided in two parts there is an
analytic database that stores data and
then there is a query interface on top
of it that allows us to source data in
this ipython environment and the results
of the query are cached in memory and we
can run all sorts of post-processing on
it including visualization transforming
data and basically lots of stuff so and
what you're looking at is a query it
kind of looks like sequel but it has a
slightly different syntax but what we're
doing here is we're selecting some data
from the table named field stats and the
actual data that we're requesting is a
to build a histogram of application
launch time and we ask for
thousand bins and the range of the
histogram is from zero to 300 330
thousand milliseconds which is 30
seconds and we do it and we ask it for
the s 40 platform and the the the match
expression here basically means the full
day of the day before yesterday so the
equal sign is corresponds to truncating
the they the timestamp by the closest
d-unit
and then we the you sample means get
this data for at least 10 percent of
users and UTC is basically a timezone
were interested in UTC days so let's run
this query it it returns almost
instantaneously it and it prints a
tabular representation of histogram
which is basically you can't look at
histogram and the tabular representation
when it's returned but the thing to note
here is it basically the histogram was
built in by the query using over four
million data points so another thing
that we can do with that is we can plot
it so we can ask and let me
let me expand the area a little bit so
we can plot a histogram and this is a
high-resolution histogram if you
remember I request that the thousand
bins and we can do a lot of interesting
stuff with it for example we can and
generally what we want to see here is we
want this time application start time to
be small so we want the histogram to be
skewed and like towards this toward zero
and we're also interested in like not
seeing this long tail over here on the
logarithmic scale you can see like
there's a lot more prominent so you can
actually see something so another thing
we can do sometimes it's useful kind of
to look at see if I can get it under
control at the histogram at the lower
resolution so I can ask the system to
lower the resolution of the histogram
and do just basically do some styling so
that it looks like a real histogram and
it's also possible to zoom in in and
zoom out to a specific range or sub
range of the histogram so but it still
doesn't explain why we're seeing so one
thing one interesting thing think here
is that they're their humps right so the
distribution is multimodal now that's
interesting so let's try to look at why
is it happening and I'm going to right
I'm going to run the second query which
is similar to the first one but I'm
going to group the the histograms by a
client version by application version so
I just did that and I got a bunch of
histogram returned essentially for every
s40 client version we have out there and
on the next line I'm going to just pick
the top three by absolute count so
there's some good default when the for
sorting and selecting top elements
and then I'm gonna just print a
histogram again and you see a great plot
and it kind of fuels that distribution
for the top three mostly used whatsapp
version on us 40 are kind of similar but
so we don't get much but in order to
confirm that similarity we can move them
to a single plot and then recalculate
counts and there to turn the absolute
counts and the histogram bins into
relative proportions and indeed the
distributions are fairly similar so
nothing stands out here so let's run a
similar query but this time we segment
or group by histograms by device name
okay so the query returns and we will
plot the top five device names by the
number of users by usage so in what we
can see here is it's pretty interesting
so now it's pretty clear that where that
multimodal distribution comes from and
if you ask us 40 engineers they will
tell you well yeah those devices
actually vary a lot in terms of how fast
they are like in terms of CPU and
resources available so that's not a
surprise and and we can do even this and
kind of see an overlay the view of of
that and so that's that that's a very
small example that shows like very small
portion of the capabilities of the post
processing and the Korean capabilities
that we have so let's go back to to the
presentation so in our experience of
building and user net it it turned out
to be a pretty powerful to
for doing application analytics but it
also a very simple in a way how it's
built so and I really believe like any
competent engineer should be able to do
something similar in reasonable time
even even at scale so in the rest of the
talk I'm going to talk I'm gonna go over
these basic components and describe how
they work and and I won't be able to
talk about collection because it's a
it's a very like company specific and
it's messy and it's not nice generic
thing to talk about so where does one
fit if we compare it with popular tools
have probably everybody uses growth
systems like graphite and printf style
text logs and we know that graph is
extremely graphite is extremely powerful
yeah it's actually very easy to analyze
data because once you get data in you
immediately get visualized and you can
use all sorts of functions to to work
with it text logs on the other hand but
graphite is kind of limited in the sense
that it's only it only works for time
serious you cannot do histograms for
that you can do things that I've just
demoed so text on the other hand the
printf style text logs they're like you
can log anything like they're extremely
flexible in terms what they can
represent but getting data out of text
logs is a major pain it's it's it's it
really doesn't scale using tools like
grep and pearl doesn't kill scale past
certain one point yet it's still
valuable to look at text logs sometimes
with bare eye so one is not a
replacement for those tools
WAM is a but it's an attempt
to get something more flexible than a
strictly time serious type of tools and
a lot more usable than just raw text
logs or raw tax law is passed through
ETL and and kind of mean available in
some other structured form so to be more
specific in terms of comparison with
graphite so data that can be stored in
one may be looked at time series or may
not be looked at time serious in a story
that full resolution
unlike in graphite there is a arbitrary
number the dimensions you cannot use
many dimensions with graphite the system
will just blow up it's not designed to
do to do anything like that and then the
interface to the analytic part of who I
am is essentially sequel and then you
can do you can leverage all the power of
PI data stack to do analysis on
aggregates and samples so the analytic
part the analytic database part of one
can be represented as a described as a
fast parallel cyclic queries of
distributed and sparse log and that
brings me to the second comparison with
this time with data warehouses data
warehouses are traditional traditionally
used for fasten'd oolitic queries in
warren data warehouse a scale extremely
well and things like that so it's kind
of limited compared to them in the sense
that it doesn't have full sequel like no
joint sub queries it has it doesn't have
many tables like you can join across
tables you can have relationship between
them and it's not as optimized as data
warehouses multiple query plans CPU
intelligence CPU scheduling it's super
optimized data format it's not it's not
what Ram is currently has but it has two
amazing properties first of all in order
to use that you don't need it ETL there
is no loading step
involved the data that comes through the
collection system goes straight into one
gets stored and immediately available
for querying now they and then the
dynamic what I called it in dynamic
schema evolution adding fields adding
columns is a no op essentially it's a
logical operation that gets picked up by
the system but it has no actual like
management schema management involved
there's no no database commands to run
sort of so the data model and this is
the data model when it comes to
consuming events from the application
basically one can take in any event that
can be modeled as a set of key value
pairs and the key part of the key value
pair represents some kind of field ID
symbolic field ID and the field value is
a is essentially a sum value of one of
the primitive types with support so
here's an example of a typical event
that will log through the system so we
have fields like event ID which in this
example corresponds to media upload then
media load type which could be one of
photo video audio then media upload
result which can be okay duplicate or
error and then we have two measurements
the media plot size and immediate plot
time in milliseconds so pretty
straightforward right and it's useful to
think and it's conventional to divide
the set of fields that you log into
analytic system into two subsets one are
called dimensions and they're usually
categorical fields that are used to
describe the actual measurements and
then the measurements themselves they're
called matters so but that's not the
full story so what makes our problem a
lot harder is that we have other
dimensions and everyone comes from the
mobile app has those dimensions
reported as well so we have timestamp of
each event we have user ID which is a
very high cardinality dimension we have
seven platforms we have application
version operating system version device
name countries and information about
networks and this list is not complete
so so what do we do next
we actually define it we actually define
a schema and we did it in a very simple
and straightforward way we have three
files dimensions dot JSON measures that
JSON an enum JSON and they have very
simple format they're essentially a list
of small JSON object and each object
describes a field in case of dimensions
and managers or an enum type and here
you can see an example of the defining
definitions of media upload type and the
media upload result dimensions for
example media flow type has an enum type
media type then it has a unique ID and
and some other properties one of which
is just describing what it is so the
media flow type is a field ID it must be
unique across all fields in the system
and the color codes are also unique
across all fields in the system so in
the supported primitive types a string
enum timestamp integer double both
pretty pretty straightforward limited
said no nesting flat namespace so how do
we manage the schema so we put those
three files in a git repository when
somebody needs to make a change they
make a change add a field for example or
duplicate a field then they push changes
to the central repo and then they ask
the client to tell the system to update
they call the client to update the
schema in the system so and that's
pretty much it
and then the system pulls the latest
version of the schema from the git repo
and does the right thing on the server
side okay so we talked we mentioned
about sequel and sequel works on top of
relational day
so how does this kima translates to a
relational interpretation it's very
simple so we have a table created
implicitly for each application and that
table has all the measures and
dimensions defined in the schema just
like that so there's no actual create
table or alter table it's but it's
implicitly there okay
so moving on to the storage format there
are two traditional approaches for
storing structured data in relational
databases one is row based another one
columnar I'm not going to go deep into
details but columnar is the one that is
actually used in analytic databases so
if you imagine the table and vertically
divided it in columns and then take each
column compress it and serialize it into
a file and they take another and then do
it with any every other column so you
can linearly serialize them in the file
then add some indexing structure store
some metadata about like Dictionary
encoded categorical Val categorical
variables and things like that you get a
columnar format it's highly compressible
it allows you to fetch only necessary
columns when you actually do the query
so if your query takes two measures and
and group bys the aggregates to two
metrics and group bys by some other
third one so you only need two columns
and if your database stories I don't
know hundred columns which is not
uncommon even and typical data
warehouses like they're not touched so
overall it's an ideal upload approach
and you should do definitely look at
that and probably do that if you're
serious about building an analytic
database the problem is that this data
format is not not exactly trivial like
it may take a while to get it right and
perform well so I'm simplified a bit
there is a lot more complexity that goes
into columnar a representation
so instead
we didn't have all that time at least up
to this moment so we may look into
columnar later so what we do is we store
data in so called long format and it's
essentially mirrors the format that the
data comes in it has a each event
represented as a sequence of non null
field and each fields encoded as a
combination of field code and field
value this means the you know it's row
by row but you don't the get you
basically can compress the sparse
representation and to dance longer
presentation and the so primitives and
and then there's a pseudo code that
describes you can see a pseudo code that
describes how it's actually done for
primitive types it's a basically attack
that says well this is a primitive field
of a primitive type then the field code
then the primitive value itself and for
the dictionary encoded strings we do
like the first entry corresponds to the
actual dictionary entry and where we
store the field code the the unique
string ID in the stream then string hash
for optimization purposes and the actual
string payload and the second one
actually references it so once you have
one string in your system in your stream
you don't have to repeat it anymore
standard dictionary encoding practice
just and of course you need a end of
record marker somehow represented
because you sterilize record one by one
and you need to basically know when one
record ends and another one starts so
typical practice used in culinary
databases it's called run length
encoding so if you have a column value
that spans several consecutive rows
that's called a that's called a run
basically it can be compressed by taking
the the value itself and then specifying
how many time it repeats
times it's repeated that's called the
round length and you can represent it in
that format and further compress it down
so it's fairly compact is a simple and
very easy to implement
it's definitely slower to read than
columnar format now now on top of it we
use general purpose LZ family
compression algorithms called LZ for
it's generally faster we found it to be
generally faster than snappy and much
faster than l0 and it has a high
compression mode which gives us six to
one compression ratio and while our case
also very nice has a very nice tool in
it comes with a default framing format
basically it lets you not care about
checksums it does it for you and and
other options as well so how do we store
that data we we write records in the
described format to log upto a certain
threshold
it's either by determined by either by
size when the size of size of the log
registered in the rest fault or time
reaches certain they're a saw when we
then we rotate it so when we rotate it
we compress it name it with the sha-1 of
the contents of the log and compress it
with lz4 so and then the next step is to
move it to a permanent storage so onto
the query language I mentioned a couple
of things about it I won't go too much
into details basically it's much you can
see a SQL query corresponding to the one
query at the bottom and generally it's a
lot longer so the main motivation for us
is to make it shorter it's actually for
interactive use we type it a lot in the
shell we use it in the ipython
environment and it helps with typing and
general compresses the syntax so there's
shouldn't be like really it's pretty
obvious that it's
mirrors equal semantics just says a
couple of things on top of it and a
couple of short hands one of the things
that we specifically added was sampling
including random sampling and histograms
and the language is overall extensible
very extensible so we had features and
small tweaks there all that time and
it's the tooling makes it really easy to
do so so finally the query system so
there is a query compiler that takes the
schema and the query string and turns it
into a C program then the C program is
compiled with a C compiler and linked
with a small C runtime you get a program
that you run in parallel on data stored
in files in the local file system then
you each each instance produces
temporary results then at the end you
just merge results and write output in
in JSON format into the output file so
that's that's what the the system the
query system does in a nutshell the
query compiler is written in Erlang it
so the whole parsing if in order to
parse that language we use a tool called
P key and it allows us to describe the
core language syntax in a high-level
declarative way so and the only thing
that we need to do after that in order
to parse it is to call this string so if
it saves a lot of typing and it
validates the a lot of syntax for us and
you don't have to think about it when
you're reading extensions and things
like that then the analyzer and code
generation is actually its it generates
a tiny C program so the runtime really
quickly nothing not nothing super
interesting if you think of it a little
bit you will be able to get the same
result first time you try so you have to
read data write output do JSON
formatting for the results reduce hash
table you can do standard sequel you
represent a histogram is a fixed size c
ra because you know the
the range and the the the histogram been
with then you you do a bunch of you can
do standard stuff for order by and of
course you use standard SS DL stuff or
random just wrap it and then the string
table symbol table is represented as a
hash table and the group is also as a
hash table so the for for the the only
interesting thing is for account
distinct we use something that's called
a hyper log log as a probabilistic
algorithm ears of research went to it
it's super nice if you want to count
efficiently with a small configurable
error up to like billion unique elements
and it allows you to do that with a
really small stage just a couple of
kilobytes hmm so with that with the with
that tricks you should be able to get
your query the process input data at the
rate of one or two gigabytes per second
so and this is actually how it works
like the single thread is not actually
used but just an example so you send
pipe file names to to the reader which
is just a wrapper for lz4 and it pipes
the uncompressed output that one query
which byte stores results in the file
for the paralysed version we actually
used new parallel it's written in Perl
but this is all you have to do to to get
the parallel query working and basically
this spawns and workers and then
distributes the load across them then
all those workers story intermediate
results and the same program being
called an intermediate results produces
the JSON output the combined JSON output
so the protocol if your so the API or
the database protocol basically there is
tool called peaky RPC that allows you to
create JSON over HTTP per define
services really quickly in Erlang again
you use a high-level definition language
you and everything thrown at you as h
HTTP POST will get validated and turned
into Erlang term and this is all you
have to do inside to probably you have
to process it and then you turn it into
a record and the system does it the the
transformation the other way around
returns JSON to the user so for example
so if you want to do a shell you go with
SSH with key-based authentication and
have a program sitting behind SSH that
reads a line includes that line into
HTTP POST call principal to standard
output and that gives you a pretty
powerful shell especially if you use
something like a red line wrap which
gives you line editing history of
commands dump tab completion and
everything else you spend a couple of
hours in it and you're done fiddling
with parameters so so like if you think
of it
suppose you have log files and that's
basically mirrors what you usually do on
the server right so you just have a
bunch of log files and they look for a
little VAR log director or something
like that so if you have them written by
wom and just keep them there put the api
in front of it and the query system in
front of it you can actually do a pretty
sophisticated analysis using the
analytic queries the sequelae queries on
those logs just just like that and
storing those files and files is in the
local file system maybe a
enough even if you were to store a
terabyte with each file taken for 64
megabytes you will end up with only
32,000 files give or take so any more
room file systems should be able to
store that pretty easily so so that's
that's basically those components alone
get you're already pretty far ahead like
it's and it's really fast and it's a
scale as well and it's really powerful
so I'm gonna have to go have time for
questions okay so I'm gonna go really
quickly what else you can do with the
local file system would that go and
distribute that file system so you can
add use file system as an index you can
basically partition all your data into
one day intervals or whatever intervals
you decide and add that under some extra
directory you can do application
indexing as well so for example we Sigma
we kind of index everything by platform
but this is very applications specific
and it's not automatic you have to do
something in the first query system in
order to support that and the worst part
of it if like you you have to decide
about it up front and you're not gonna
be able to change that so and then if
you have multiple applications multiple
data streams you just add another
directory for it for four tables so so
I'm not gonna have time to talk about
the distributed storage actually there
is a very simple way how you can provide
the distributed storage on top of all
this and have this data processed
locally and then aggregate query results
and
and such so I'm going to talk really
really briefly about the UI and
post-processing so the web the ipython
notebook is a fantastic tool and if
you're not using it today pretty sure
you're going to be using it in a couple
of years the pro the project is super
active and it's amazingly useful it's an
open source project there are also great
Python libraries that we use we didn't
have to have plotting didn't have to
write our own plotting so and one of
them is pandas it basically provides a
fairly efficient in-memory
representation for further like pivoting
slicing and dicing manipulating running
a bunch of aggregate functions time
series and manipulation and so on then
the matplotlib is a de facto standard in
the python worth for doing plots and
then there is a seaborne that we
currently use for a little bit more
advanced stuff so one point about the
plot in libraries I actually looked at
various JavaScript charting libraries
and they all pale in comparison compared
to matplotlib so my third tip is a
scientific plotting library and I'm so
happy that I decided to go with it
instead of like various charting stuff
because you learn so much how things can
be done and it has so many features that
you wouldn't be able to find anywhere
else the problems that it generates PNG
is essentially and they're not they're
not interactive and so we would like to
have more interactivity in the UI you
sure I've showed you some some of the
JavaScript widgets and sliders these are
something that's built in in the ipython
environment you just have to call
interfaces in order to use them but it
would be also cool to work directly with
the graphs and have brush and tooltips
panning and zooming and things like that
we're also going to be looking at more
powerful visual is a visualization
techniques
the there are many many tools that we
can use in the Python world alone and so
so there are some references I'm not
going to keep them on the screen but
there will be available in the
presentation it's basically the tools
that we use and some of the references
to the similar systems and some of the
inspirations we took when when we're
building that system so with that I have
to wrap up thank you for listening
yeah sure yes absolutely everything that
comes in has a time stamp you see the
first thing so that's every event is
time-stamped not always useful but it's
it's great to have yes it's a it's a
it's a fairly simple code generation if
you think of it like if you never
thought of it I encourage you to think
and it's actually a pretty simple so you
actually go row by row you might realize
a row and then you decide what to do
with it some of the fields are going to
be aggregated some of the fields are
going to be used as group by key and the
overall the whole program with all bells
and whistles like the the compiler stuff
is probably the most complicated part of
the system but it's still like 5,000
lines of fair line code and it's really
really straightforward I must say it's
just for data manipulation all the stuff
that takes takes some you know code and
the overall system I have to mention it
it's it's well under 15,000 lines of
code
so excellent question well first of all
we're not quite there yet that's that
was an estimate so we're about like five
to ten percent of the five terabytes and
the event volume right now and then if
you notice there are lots of pipes
involved right
so basically on the thirty-two virtual
cores machine we can process data at
anywhere from 6 to 10 gigabytes per
second and I think that's like really
really low so I just haven't had a
chance to spend time on finding those
bottlenecks and some of them are like
really obvious and and it should it
should be way way higher so we're going
to scale the number of hosts obviously
and also one interesting thing so so we
have several sampling capabilities so we
actually don't have to go over all data
to get some interest to get some useful
information so if you want your queries
to return something fast you do
something if well we may start looking
eventually at columnar formats and
things like that but so far performance
has been sufficient so it's like we do
best effort at optimization and I've
been more focusing on the front end part
of it because honestly it's been the
hardest part it's like it's a it's a
human interface problem but it's a
difficult one like it's it's open-ended
how to scale an analytic database you
can read about it but how to provide a
nice friendly interface to working with
data it's not obvious like data
scientists use there's low level Python
libraries all the time but they're not
really that accessible to people who
don't want to do like very low level
like our style analysis</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>