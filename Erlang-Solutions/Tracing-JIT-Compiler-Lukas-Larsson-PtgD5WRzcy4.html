<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Tracing JIT Compiler - Lukas Larsson | Coder Coacher - Coaching Coders</title><meta content="Tracing JIT Compiler - Lukas Larsson - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Tracing JIT Compiler - Lukas Larsson</b></h2><h5 class="post__date">2017-03-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/PtgD5WRzcy4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi everybody so I'm going to talk a bit
about a research project that Erickson
is doing together with the Swedish
Institute of computer science which is a
just-in-time compiler for the language
machine yeah so my name is Lucas I've
been working as a consultant at Ericsson
for about six seven years working with
the a language machine various aspects
of different things for the last six
months nine months something like that
I've been involved in this research
project to work with the just-in-time
compiler on and off depending on
whatever else needs to be done so I'm
going to talk a bit about the
implementation details and different
things about it so some background first
to see so beam is the name of the
interpreter within the a language
machine that's Bogdan a long abstract
machine or beyond depending on we want
to talk to that so it's a register based
voting machine using direct threading
for its dispatch loops it's written in
pure well C ca9 and it uses the GCC
extension extension labels as values in
order to do its direct threading
dispatching if we don't have that
extension if you're running on something
like for instance the windows that
doesn't have this we get some we on
windows we compile it with GCC and use
efficient studio studio if we can or
otherwise we use a big switch station to
do the dispatching not really important
and some of the optimizations that this
virtual machine does in runtime is that
it does some pepole optimizations for
the when loading code so it does the
instruction combining and instruction
specialization when loading code there's
a bunch of other optimizations that is
done in compile time and if you want to
know about those you need to talk to
somebody that knows something about the
add-on compiler this is not me there so
this is the example that I'm going to be
working through most of the presentation
today so it's I hope this is a familiar
function for most people calculating a
Fibonacci number eventual
in the series of some sort so if we take
this code and we compile it using the
Allan compiler we get what's called the
beam format so the beam format is what's
inside the dot beam files that are is
generated when you compile a file the
press printed version of that looks
something like this so we have in this
function we have first we test this it's
an integer that's the thing that's
coming into our function we do a select
value which is a dispatch table to see
depending on the value of what's
happening so if we have a 0 or 1 we're
going to jump to the label 3 which is
you can see here so late label 3 this is
what we're going to jump to and if it's
not a 0 or a 1
we're going to jump to label 4 which is
to do the rest of the code so basic
control flow through this select file
and select all is done for when you have
multiple targets for the same thing so
if you have three or two or more targets
if you just have two you do a compare
and you do other kinds of operations so
this is what's in a beam file when you
compile something in there and when you
load it you get something different so
we do these optimizations that are we're
talking about that we do at load time so
you get some kind of people and
instruction combining operations so the
is integer test that we had before and
the Select Val have been merged into
this I jump on Val 0 instruction so both
this combines the test of is this an
integer together with B dispatch logic
of the Select Val into one instruction
and it also does specialization which we
can see at the end which is the excess I
over there so we know that the input
values here come in what's called the X
registers of our virtual machine so
we've specialized this instruction so
we've done both instruction combining
and specialization for this instruction
and depending on the control flow we can
see that we get a pointer so if we are
not within 0 1 we jump to the red value
or if we are we're going to jump to the
blue value and just return the value as
it is
there's in beams or the beam code that
compiled them to this there's about 180
different instructions when we load it
that's about triples or around 500 600
instructions when we're loading it and
it's on these instructions that the
just-in-time compiler it's working so
that's why introducing these
instructions to you if you want to see
what's being done the debug command e
RCS debug that's mentioned in the slide
title is how you can dump this
information to disk and you can see what
optimizations are done in load time and
figuring out the case of why isn't this
being done the way I think could it be
faster and different varieties and you
can also of course go into the C source
code and read what the specific
instructions actually do in there most
of them are fairly simple except for the
binary syntax which is a mess and very
hard to follow but most of the other
ones are very simple in there so a
single instruction if you're going to
look at it so this is the I jump on
valve instruction written in C code with
a lot of macros to do various different
things it's not really important that
you understand know what it means but
everything boils down to this C code
that we are using just GCC to compile
everything go in there so the beam
interpreter is nice it's very small it's
easy to manage it has a relatively small
memory footprint when it's running since
it's been basically the same interpreter
since the middle of the 90s
it hasn't changed a lot and at that time
the memory constraints were a lot harder
than they are today and it hasn't grown
a lot since then
so it's matured quite nicely we since
we're running an interpreter we don't
have to cross compile any modules we
have very predictable performance we
know that on different platforms it will
behave relatively the same
and many implementation relative to a
just-in-time compiler is very simple and
all the commodes of the complexity comes
from the allen compiler and not in the
runtime
there unfortunately rather than it
comparison to a ahead of time compiler
like if you're compiling C code or C++
something it's relatively slow factor
1020 so err than those if we're
generously on and because of the way
that the airline module model works we
cannot do any cross module inlining in
different places in the Allan compiler
because you're compiling one single
entity of an airline modules we can't do
any inlining across these things and
because of the way that along tracing
works we can hardly do any inlining
within the modulator so we have a lot of
constraints on the compiler and what it
can and cannot do at compile time which
doesn't allow us to produce as good code
as we would like
enter just-in-time compilation so in the
just-in-time compiler we're trying at
the runtime to remove these constraints
that have disallowed us to make some of
the optimizations that we want to do and
instead doing them in runtime when we
know more about the system that we're
running at so what is just-in-time
compilation so just-in-time compression
is you basically translates the
interpreted code at runtime to machine
code so that you can run it as machine
code rather than doing this interpreter
loop and dispatch taluk all the time and
we can do specific runtime optimizations
for your applications that when we for
instance know that's the lists module
that you're running in it's the one that
we expect and it has the fold with the
implementation that we expect it to be
we can optimize for that specific phone
and so on so we have a lot of
optimizations that can be done and this
is done also a lot of other languages do
these things I'm sure I've missed some
of these like the JVM is the most famous
example they've been using it for a very
long time this via lars also been using
it for a long time and they all have
different trade of different platform
characteristics that they use them
exploit depending on what the languages
and what different things are running
there we have the VA so that's the
Chrome JavaScript mostly large
JavaScript engine that changes name more
often and I change boots and the blue
edges and the pipe idea is some trace
phase just in time compilations and
there so some of the requirements that
we have on a just-in-time compiler for
this is that it should be easy ish to
maintain it shouldn't add a lot of
maintenance burden we're about three
four or five people roughly as Theo
t-t-timmy correction that knows how to
kind of do these things and spending
more than one person maintaining it it's
not really viable as we need to have
something that's relatively low cost and
maintenance and gives us quite a lot of
bang for that person we need something
that is easily can maintain the up
economical semantics of the virtual
machine we don't have to worry about
it's behaving differently when you're
running just-in-time compiled code
versus running just interpreted code we
want it to be fast obviously and we want
it to be devoid of any user interaction
so you when you're using it you
shouldn't have to say oh well I'm
running this scenario then I need to
tune it in this way or something it
should be as seamlessly as possible for
everybody there we've chosen to use a
tracing approach to this so which is
different from doing a what might be
called a message base yet so we're
looking for longer traces so longer
execution execution paths that's running
in your code to try to figure out and
optimize those the general pipeline of
the way that we do these things is that
we as most gist try to locate a hot
function something that's been called a
lot so in a normal system this would be
list map phrases that function gets
called all the time by a lot of
and function most just try to define
some hot place and when you find out
what place we try a process that's
running that code and we follow along
the path that is executes and records
all the different instruction that are
instructions that it records until it
either comes back to a loop or is very
soft in some direction called some other
code there are some different heuristics
and I'll talk more about exactly what
those heuristics are later on from that
trace we generate the code for doing
graphs of all the different instructions
so we get something from there from the
cold following graph we generate LLVM
intermediate representation code so we
have another beam ir that we generate
from there then we feed that IR into the
LLVM compiler that compiles the code and
then we plug that code into the
interpreter and then we get to run the
machine code rather than running the
interpreted code I'll go through all all
of these different steps in more details
now
so profiling hotspots so when looking
for where things are hot it's very
important that this is a very very
lightweight mechanism so what we do is
that we insert instructions at
functional ingress and when you return
from a function so when every time you
call a function there will be a special
instruction at the top of the function
that's basically increment a counter
increment the counter increment the
counter for that and we do it for entry
so that we can get like ten recursive
functions work very nicely so that when
a function gets called a lot in order to
work with body recursive functions so
when you're popping the stack as well a
lot so we want to optimize stack popping
we also insert these kinds of population
counters when you're doing returns in
your code so when you're doing jumping
back and in order to optimize the kind
of code that's in djenne service for
instance or when you receive a message
and then go back to a received loop and
i'm the you receive another message you
go back in to receive loop we also
insert these profile counts
when a message is matched out of a
process so that we can get that in there
so that we can optimize those loops as
well and it's a very simple counter as
you will see we've tried various more
advanced heuristics for this but there
always is too much overhead and you
don't get enough data to weigh up the
overhead so the inside the virtual
machine this is implemented as a small
32-bit counter that's embedded into the
code so if you're running a honestok
this is the gist only works on 64-bit
systems at the moment and 64-bit Linux
with special memory models for how you
compile the emulator loop so you can
guarantee when you're compiling the code
that the code is in the lower strata two
gigabit memory span and this means that
pointers are only going to be 32 bits
wide which means that we have 32 bits
empty which is up here so this is the
normal memory layout of a khole function
which is basically the address of where
we want to dispatch to and the pointer
where the new code is when we're doing
the call there and the memory layout of
the place were recalling looks something
like this we have some method 8 of the
function first which is basically the
mfa of the function so that we know
where we are then we have this counter
that counts how many times this has been
called we have an anchor which is a
pointer that says ok so this is where a
trace information is stored when
something is being traced and then we
have the actual code which is just down
here and in the calls we don't actually
dispatch into the profiling instruction
but instead we're doing that inline into
the call instruction so every time you
do a call you execute something like
this C code I think this is directly
copied from the sea from the
implementation
if the anchor is not null we increment
count and if it is above the threshold
we start tracing and this is done by all
schedulers at all times so this is very
very racing to do this so you have a lot
you have potentially 64 threads banging
on this counter at the same time and one
of them is going to one or more is going
to find out that this counter has gone
above the threshold so when we before we
start tracing there is a mutex that need
to be taken inside this anchor pointer
that's in there so that's the way we
synchronize multiple threads trying to
do tracing and for return the same thing
is done so we have a profile instruction
with the counter in the return so the
return jumps up here to this pointy
place and increments the counter and
it's realizes that it's been again then
it does a put a trace on that process
and traces that away so that's how them
yes there they just I think we have
added like 10 20 different bits to read
this information out so yes you can use
various to inspect to see which ones are
hot which ones are not hot how long ago
they were encountered and these things
and we have various ways of signal these
things out so yes there are ways to
extract information so if we now I'm
going to talk a bit about the tracing
and how that works so if we remember
this I jump on Val the arrow instruction
it looks like something like this so and
in the C code we had like this so we
have a couple of different choices to
make within this instruction and those
choices that we make within the
instruction is also part of the trace
pattern so not only which airline
instructions are one is recorded but
also which part it takes within an
hour-long instruction it's recorded so
when we do this we create a Co folding
graph of the entire interpreter
and generate that that looks something
like this and if you'll notice this is
the scroll bar so it kind of it's big it
didn't really fit on one screen it fits
on something like a 1022 screens if you
have that there so in this as I said
it's about a 180 beam instructions that
boils down to about 500 loading
instructions and that goes up to
somewhere in the region of 4000
different blocks that we trace when
we're doing the tracing of these
different things most of them are very
simple but some of them are very complex
like for instance binary handling is on
a very very complex instructions so this
I jump on Val 0 would in this graph be
translated to something like this and
I've used like in this uses the LLVM IR
to do this analysis so you can compile
and finally emulate the loop to LLVM ir
and then you can analyze that to create
these different diagrams and also create
understand the call forwarding graph of
your system so we use those tools very
heavily to do this analysis and
basically what we do there after we've
done the analysis is that we generate
two different emulators so we generate
one that does the profiling and we
generate one that does the tracing so we
have two parallel and interpreters in
the system and when we encounter
something that needs tracing we switch
to the tracing emulator and run the
until we decide assign the trace or we
fall off the hot pot and then we just
ask executing in a tracing interpreter
we record which of the basic blocks
within the emulator we are executing and
how many times we've executed that look
within this specific trace and then at
the end of that we get a call for ring
graph so a trace for us is right now
this changes about every week when we
try to fine tune different things but
right now a trace is constantly complete
if we
loop on it we return back to the
original original instruction a streak
is detected so we have the concept
called streaks there so it's a longer
path of execution they get executed a
lot that might end up in a different
native call so with a function that we
have already compiled to native code or
it ends up in some other place that's
very very hot but it's not a loop yet so
some kind of dispatch logic is normally
okay so we get these kinds of streaks a
lot when we're running like the compiler
or dialyzer kinds of program and a trace
is canceled if it grows too long is too
wide or is it's too unpredictable for
anything so if we have a lot of tracers
going in different directions we cancel
that race because we realize that this
is not going to be with rather we found
that something that's very very
unpredictable in nature it's not a good
fit for the tracing JIT compiler yes
there and we do multiple iterations of a
trace so if a trace gets run just
because it's run once we don't compile
it we run it I think at the moment 16
times and so when a trace has been run
sixteen times without any new paths
being discovered that's when we compile
it into native code so let's have a look
again at the Fibonacci trace so this
function traces into something that
looks like this it's a quite a nice
trace we have a distinct right side and
a distinct left side and this seems
quite optimal for our tracing to
optimize so if we look at the first part
with it which is the head of the trace
it's basically the decision tree and
it's the code that we've seen before the
is jump on valve zero thing and this is
a picture of the top there so we have
the top decision which is where the
trace starts so it starts at that node
and then it always goes down one step so
this instruction would always be
executed and then it makes a decision
are going either right or left in this
decision tree and that the decision is
zero or one it goes to the left if it's
not it goes to the right and decision
tree yeah and you can see also to the
right there you have the original
instruction so these numbers here is the
basic block marker there so we can see
that it's taken this is basic block 764
which is the past that it has here does
766 which is this block here and then it
makes a decision either is going to go
to 768 which is the 4th through top or
767 which is the other post so the
entire instruction is part of this but
what comes next is integrated into that
decision let's see Ostia pictures some
explanation both different numbers so
the basic block number the amount of
times we've traced the block so we've
called this function 41 times before
decided that now it's time to do a
just-in-time compilation we have an
instruction pointer so where in the code
we are running and we also have a
breakout count so the mum the number of
times that this check failed when we're
running the trace now this becomes
important when trying to find exit paths
that are hot when code changes its
behavior and then on the right on the
right side of everything we have the
code that just allocates something on
the heap and then runs the body
recursive part of the Fibonacci function
and if you'll notice down on the right
side here we have a breakout counts of
684 so this is the reduction counter so
when we've been running this trace it's
run out of reductions has been
preemptively rescheduled 684 times at
this point and that's nothing we can do
anything about so that's just a fine
number and that's needed to be there in
order to guarantee soft and real
to find properties we have the return
instruction at the top of everything
which is basically what happens so you
get a zero one and then we get the in
lining of the popping of the stack now
first we get obviously the call of this
Fibonacci which is going on the right
side of this trace so we get that part
and then we also get in lining of the
topping of the stack so the actual
testing of everything and this might be
one loop or it could be when it's
popping the stack of a lot of things it
could be a lot of computations so the
entire function is part of one trace in
this case which is really nice so
another example the more complex example
is when you try to for instance get a
trace of object so this is the or deck
function which basically searches
through a list to find a specific
element in that list quite simple
when I when I run just start up the
system I get a trace on this and the
trace looks like this when it's traced
so this is a lot more complex and it's
not run as much and it's a lot less
predictable the reason for this is that
the caller is part of the trace so
always when you're doing this kind of
tracing in the most internal loop will
be the first one that will be detected
as hot in this case since the or defined
is looping on itself to find the value
in a list that will be considered hot
very quickly but or defined is called
from a lot of different places and the
actual return to these different places
is what's causing this explosion of
trace places so for instance it's being
called by or lint in two different
places so when we're running the shell
the Erlendur is there so it's been run
in a couple of places there and they
were like I don't know 50 60 different
places that this function was called
from when he was being traced
so what do we do about this so that's
where we have this counter that counts
how many times the trace has been
executed in one place so by pruning all
the different nodes in this tree that
have been executed less than five
percent of the times the five percent
seems to be a good enough value we prune
almost the entire tree down to two two
cases left out of these 50 different
ones we just get this little part here
that seems to be these are the functions
that call this function the most so they
will be part of the trace that we
generate when we explicitly encounter
this looper and if we look at the code
that gets colder it's not surprising
that this is one of the hot places
because we have basically a iteration
going on around that or they found find
so what's this trace here is really
seeing is that it it's starting running
here and it goes all the way into the
fold and back again around here so it's
found the outer loop and just in time
compiled that outer loop for everything
and which is really neat and for all of
the other cases eventually if something
is hot there will be a trace being
started in that place and that will
encounter this native code and just
inline that native code into the calling
trace so we still have this small kernel
and if we work on just this code and
this code is very very hot we have a
very excellent trade shortly for that
but if in the future something changes
in in other places then different traces
will go into this specific case and in
line those traces into other places I
hope that makes sense
so by now with so this tracing logic and
how that works and how we split the
emulator into two different versions
have changed a lot during the years we
started working on this or rather
frame my colleague as the Swedish
Institute
the sign story working on this 2013
maybe so three four years ago and the
first version of it try to use a
slightly tried to do the analysis on sea
level to try to figure out the different
parts in different areas the second
attempt also dated on the sea level but
using something called lib climb that's
an LLVM tool a twelve that works on the
CAF see that clang works on in there and
the current version uses something
called LLVM stack Maps or patch points
which is a way to instrument your code
to make the compiler tell you where
things are located in memory or in
registers so we can know for instance
that the instruction pointer was
allocated by the compiler to be in the
register or nine on an x86 for instance
or that the stack pointer is in a
specific register and as values move in
different instructions in different
places but that's helped us a lot that
was originally conclude contributed by
the Mozilla team as they were working on
their just-in-time compiler and
integration with LLVM I don't think
they're using it anymore but they did at
some point there so once this is done we
have a trace we compile it into native
code and the native code does a lot of
optimizations as well so this is the CFG
of a compiled trace so if you'll notice
here there's only left four different
parts over here rather than being one
two three ten or something like that so
the LLVM compiler together with our
knowledge of what different beam
instructions does and what you can
accomplish you can make as many students
reduce these instructions down to being
four instead of ten that helps a lot
with performance in different regards so
one of the good things and one of the
bad things about our approach at the
moment is that we store all our info
about the process in the same way as the
interpreter does which means that the
airline registers for instance for
argument handling and so on is being
stored in an array rather than being
stored in physical registers this has an
impact on performance obviously but it
also helps but it helps us a lot when we
jump back off a hot path so all the time
we have to assert that the conditions
that were met when we were doing the
tracing are still there and if not we
have to jump back into the interpreter
mode and by keeping both the stack and
the register array as it would have been
otherwise we can do those jumps very
seamlessly and very efficiently back
there some of the optimizations that we
do on these traces that are very very
helpful is that we consider like all of
the code that's running is considered to
be constant so we can do things like
constant folding and these things on
constants that are running in the code
so if we have a trace that is if we have
a function that inserts a constant into
another function we can know that that
is a constant and do optimizations
because of that so if we have if we
start and create a new dictionary and
then call that into another function we
know that it would be a new dictionary
and it works very nicely and we do a lot
of redundant restore so when switching
back from the trace emulation is placed
in the native code back into the
interpreter emulator this we do a lot of
and the voices to see what's actually
live and what we need to restore when
jumping back so that we can do a lot of
we don't have to do as much reshuffling
of things when we're running the trace
this takes a bit of time when we're
actually compiling it but it normally
worth the effort to not they have to
restore all of these different pointers
and all this with different memory
locations
yes oh the checks that are left there on
the right side of the trace it's
basically to check if we have memory on
the stack we check if our argument still
is a small and we do ever done then
check that our optimization hasn't
figured out yet we check it X 0 minus 1
is still a small and then we also have
to check if we're supposed to be
preempted so those are the only checks
that were left and that's when when
those conditions occur that's when we
fall off the hot pot in them so the
first two are the first 1 2 &amp;amp; 4 are kind
of have to be there
the third one some optimization paths
might be able to figure out that it's
not needed all right then and the code
that's being run is basically the code
up there that's the small code yes so
when installing one of these things into
the code now we reuse some of the
profiling instructions and such that
we've seen before so the basic function
ingress so if you want to install a
native code in this we the first thing
we do is that we set the anchor to be
null so that we don't care about the
anchor any more
this causes our profiling logic to not
trigger anymore if you remember the if
statement that I showed you before with
the profiling logic it checks first if
the count if the anchor was actually a
pointer and if it's not then we don't do
anything so this is the reason why doing
this check is because we want to do is
efficient install of these natives
counters we just put a null in the
anchor and after that we install the
native code pointer and we write it
forcefully over this data here so we
just put the native code pointer in that
place now doing proper S&amp;amp;P
synchronization and so on there and then
at the end of it we overwrite the return
instruction and put the gist native
instruction in its place so when the
code any process runs this code now
it will instead run a JIT native
instruction which we'll just call the
function pointer that we just installed
here and it will jump there and then we
will execute the native code so
benchmark always fun so this works which
is kind of amazing we was a long time
ago since we had any bugs on it and most
of the bugs are is when we're trying to
do new features and so on we end up
breaking things but I think kind of
working since I don't know October
November something like that then since
then we've only been doing performance
optimizations for various degrees and
trying to figure out how our line
programs actually behave and trying to
figure out what hotspots and how to do
the tracing and what good optimizations
and what not now in different areas so
running this function beam takes on my
stationery computer about five seconds
to run a fifth above 40 you get a big
integer running it with beam chips
so the just-in-time compiler you get
about 2.3 seconds so that's 50 percent
decrease in run time which is nice if
you run it with hype you're getting them
better so we're not 100% there yet but
we're getting closer to it every day and
the main reason why hype is started on
us is this thing that we're keeping
registers in the array rather than
putting into physical registers for the
arguments that's the main reason why
height is covered and also this
otherwise when so when or if we do an
optimization with that we should be
dropping down in the in the same region
so running the benchmarks that the hype
team usually used to evaluate different
speed ups and different things when they
are doing things beam just gives about
an 80% speed increase on those
benchmarks when running them hype gives
about
the speed of the beam jet so we we are
we are faster than being by quite a
margin but height is still faster than
us but they wrote the benchmarks not us
so we are better at other things and
there's maybe three benchmarks that
contributes about 60% of heights speed
up there and they all have to do with
this register handling things most most
most more functions there and there's
also some places were we're better for
instance if you want to calculate the
length of a list beam justice you're
saying hype is relatively slow in those
benchmarks but in the three benchmarks
there is basically I think doing the
Mandelbrot set calculations hype is very
good at and various other things
so in general beam just is best at about
six of these benchmarks hype is better
at about 15 of these benchmarks and four
of them are about equal then so we're
not there yet but we have some ways to
go a fun thing where you can do with
benchmarks is that you can artificially
induce something where you can see that
you're on the winning side so this is
what I've done here so we have a module
that we've compiled with hype but
unfortunately the list module in this
case is not compiled with heiped which
will create kind of a very bad scenario
for hype so for beam it takes about 2 to
5 seconds to run this with beam gist
about one second and for hype it takes
about five seconds to run this so do you
have you can do a lot of things with
benchmarks and so here we have clear
proof that beam just is the best thing
that you can do but you don't have to
read the lower part it's not this okay
so just compile your entire system with
hype on your route you will be happy in
there so that's the how it works
simple enough what we're going to do in
the future so we have some ideas about
what we want to do
so we want to do some cross trace
optimizations so of them at the moment
we're basically only doing inlining of
traces into each other but we want to
take a step back and allow something
like an airline process to run and look
at multiple traces that are in the same
area and do more aggressive
optimizations across different traces in
different areas after the system has
kind of calmed down and reached a steady
state and we want to push this the array
of registers into physical registers
because in in small computational
kernels which is where we really want
this to join this is where really what
we need to get good performance we also
want to do more of a lightweight
compilation strategy at the moment we
have to be very very sure that we're
going to gain something in order to
compile it if we're not sure it's it's a
big cost for us to compile it so we want
to do some kind of multi tired
compilation things and I think this will
be our next step to try to achieve
something that doesn't take as much time
to compile and we want to be able to do
more optimizations on the call
forwarding graph so doing more early
optimizations of different kinds so that
we can do reduce the amount of code that
we need to compile and analyze when
you're actually doing code generation
and take advantage of the way that the
adding semantics work and that we can
infer things across module boundaries in
a way that the compiler cannot yeah that
was it
cool any questions
no they are on they they are they are
blacklisted and disabled by the system
if you we detect that something is there
but they are enabled in load time so if
you start your system and you want to
not run the just-in-time compiler then
you can put flags in there you can
disable them while the system is running
as well yes you should be able to yeah
yeah they're not perfect the values
because multiple threads updating a same
value a good question we did this is a
politics money thing is did simple
answer
I'm not they we didn't feel that we
wanted to use the hype back-end to do
these things and we wanted to explore
something with the LLVM tool chain in
order to do this compilation and pray
the guy had some ideas about how to do
this and that's why we're taking this
approach you know it's moving in there I
don't have a very good answer for that
no later yeah no I don't know when it
were if it will ever be merged in
upstream I obviously hope I had hopes
when I started on this in what is it
September that we would have some kind
of prototype in 20 that isn't going to
happen
but we're still trying to figure out
exactly where this fits into or things
or if we want to have a fully fledged
kit or if you're going to take small
parts of it and put it upstream or what
we're going to do there so we don't know
yet this is research and we don't know
how it's going to come into the product
yet</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>