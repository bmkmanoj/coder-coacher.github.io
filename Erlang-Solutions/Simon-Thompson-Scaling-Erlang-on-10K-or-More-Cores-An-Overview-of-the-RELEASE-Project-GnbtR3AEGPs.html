<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Simon Thompson - Scaling Erlang on 10K or More Cores: An Overview of the RELEASE Project | Coder Coacher - Coaching Coders</title><meta content="Simon Thompson - Scaling Erlang on 10K or More Cores: An Overview of the RELEASE Project - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Simon Thompson - Scaling Erlang on 10K or More Cores: An Overview of the RELEASE Project</b></h2><h5 class="post__date">2014-07-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/GnbtR3AEGPs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome back after after lunch and I'm
very pleased to introduce myself as the
first speaker of the afternoon I get to
host myself so I can talk for as long as
I want and not take any difficult
questions okay so welcome back arm what
I'm going to talk about is the release
project which is a European funded
project that's been running since 2011
and will be terminating early next year
and our goal in this was with to try and
scale Erlang and we've said 10,000 cause
that and I'll talk about how far we've
got in trying to do that the project
involves a number of partners number of
universities companies that begin with e
like solutions EDF and Ericsson and
university partners in Sweden and the UK
Sweden Greece in the UK so very much
like this is chris said pinching words
from chris and and indeed from from some
of the talks earlier on I think we it's
become a commonplace you know I can
explain to to anyone they say I've got a
dual core machine that multiple multiple
core machines are the norm rather than
the exception and the number of course
is growing for reasons of energy and
power consumption and and heat
generation and so on so we're familiar
with that and we're familiar with the
fact that that they're being used in all
sorts of different contexts therefore
inside embedded systems for doing things
like graphic graphical processing but
also they're becoming the norm in we see
that the one of the rationale of the
rationale for this project is that we
feel that they're going to become the
norm for general-purpose systems as well
and so we're asking the question in this
project what are the right programming
models and tools for using to program
general purpose you know server software
general purpose software on this sort of
platform
and I think it's interesting the the
leader of the project is Phil Trinder
arm and in the sense i'm giving the talk
in his on his behalf um it's interesting
to reflect on a project like this once
it's been running for a while and what's
intriguing is that it we've not come up
with a big bang we've not come up with a
project at a product we can give you at
the end of the session which will
immediately allow you to to go off and
write things at massive scale but what
we've done is attack that problem on a
number of different fronts and hopefully
made some progress on all of those i
think the things that we were looking
for in in doing this was a realization
that if you are running on a massively
multi-core system then call failures are
going to become a su nhu you will have
if you have a system which will uses a
lot of cause look at the mean time
between failures you're going to get
failures within a system and you need to
but you need to build something that
will allow you to to handle that in a
graceful way and also we knew things
that are going to be scalable so I don't
think there's anything sometimes I get
asked to comment the University writes a
plan and it produces 20 pages and you
think you can cross out 19 and a half
pages of it because they're just the
obvious thing that you would always say
we all want scalability we all want
robustness um so know that I don't think
I'm saying nothing particularly
contentious here so the aim that we had
and the name of the project was to use
the actor model concurrency oriented
paradigm to build these reliable
general-purpose systems and to aim it at
we said 10 to the 5 there we've said
10,000 sometimes to aim at large numbers
of cause and the crucial thing I think
the crucial insight we had was let's
build on Erlang if you look at the
Erlang infrastructure I've discovered a
good way of building slides if you type
Erlang into google images and you see
what it it comes up with and you can see
it's weird it's simple its has some
useful
it has some interesting people there it
has it shows that the Erlang the Erlang
ecosystem is a strong one it's one way
people are used to building scalable
reliable systems and what they're
building on is is this effectively and
I'm sure this is familiar to most of you
but just there are one or two points
that we should make about the approach
we've taken that it's worth just just
running over so we live in a world where
a typical host you know physical machine
will have will have a number of cores
and as you know you can run you can run
Erlang nodes on multiple cores and
indeed airline comes out of the box now
as a multi-core system what it's worth
being aware of is that within those
yellow boxes they're not black boxes
within those yellow volumes we don't
really have much control of what goes on
the OTP team are our suppliers with a
system where you have a run cube per
core and that's all dealt with we are
not expected as Erlang programmers to
worry about run queues we're not
expected to worry about which processes
sit on which q that's dealt with by the
system out of the box so we the way that
we can organize as programmers on top of
this is to is to deploy those those
yellow volumes we can deploy Erlang
nodes and you can see in that picture
that you can one way of using Erlang
distribution is to is to deploy a number
of nodes on the same on the same host a
number of sorry yeah a number of nodes
on the same host so each of them
involves a few cause we can do that the
other thing to say is that model the
Erlang distribution model where you're
connecting different Erlang nodes
together is one where out of the box
everything is connected to everything
else and I think that's how its
illustrated there so each node there is
connected to two
reload so that's the sort of model we're
playing with and we think well how does
that map onto onto a much larger system
one thing we had to think about is um
here's a picture of think of each of
these groups of of boxes as a host so
we've got a number of cores on each got
to think about what happens with core
failure the model at the moment is if
one of your cause fails you effectively
take out the whole machine so if we're
thinking of at the moment of using
Erlang distribution as a way of scaling
then what we have to think of is each
node being mapped onto her or each host
be mapped onto a single Erlang node now
in the future i think that would that
potentially could change and we could
use Erlang distribution as a way of
assembling as a way of marshalling
together different cause on the same
host but we need to be able to deal with
that sort of failure with the failure of
a core perhaps on the same piece of
silicon perhaps on a different piece of
silicon but the way we've looked at it
at the moment in the way that I handles
this is if a single core isn't working
you take the whole host now so that's
that's a design decision we take so I
think I've just reiterated that that la
multi car is black box while it's the
sort of yellow box in terms of the
diagrams we don't change that but one
thing we've had to do is if you're
looking at building systems like this
we've had to think about observing
behavior or multi-core systems we want
to be able to understand because one of
the lessons from this is you need to
measure you maybe think you know how
systems are going to behave but you
don't know until you until you measure
and as I said that core failure leads to
to host failure here and that may change
but but for the moment the way we've
made this with their design choice we've
made is is this one okay so we said we'd
build on Erlang but
um the scalability that Erlang has is in
practice is constrained and these have
been the things that we've been we've
been attacking as part of this project
so there are things and there are other
things that perhaps we've not attacked
yet there are aspects of the virtual
machine for example synchronization on
on internal data structures when you
move from a model which is concurrent
but single single processor to a model
where you have true things going on at
the same time then some of the
assumptions they're built right into the
the implementation the virtual machine
like the model for tracing need to
change so for example synchronization
locking on internal data structures has
been something we've looked at and
obviously having the OTP team as part of
the project has been very important for
that there are language aspect arm so
the issue about the out-of-the-box
behavior of everything being connected
to everything else a fully connected
network of nodes that's something we
wanted to look at and about what happens
when you spawn processes are there do we
need to think about what we do about
placing processes when we spawn them and
we thought about tool support about
deployment of these sorts of systems
about observing them about visualizing
and so on so we tried to and here's a
got a diagram that summarizes i hope the
sort of approach we've taken so we've
we've looked at the virtual machine and
try to optimize measure and optimize
things there on top of that we've built
a library called scalable distributed
earl angle s dr liang which handles some
of the some of the questions about what
you get in terms of out-of-the-box
behavior we've tried to build on top of
that infrastructure that allows us more
easily to to scale and observe larger
systems and cross-cutting we've looked
at tools variety of tools to support
this and looked at some case studies so
what I'm going to do in the rest of the
this talk is to give a brief description
of each of those areas this is partly a
plug for a series of tutorials tomorrow
and there's also a talk there in the
last slot today which tells you more
about SD Erlang so this is if you like a
taster of what of what we've been doing
a high level hovercraft overview of the
of the material so if we start off with
a virtual machine arm howdy are we there
yet the cry of your children when you've
set off on a journey what we wanted to
do right at the start of the project was
think or can we benchmark can we look at
how things scale so if we don't know how
things in practice behave it's going to
be difficult for us to to change the
behavior to improve the behavior of real
systems and it's interesting i was
talking somebody over lunch about this
one of our problems has been there
aren't a huge number of Erlang
distributed systems out there in the
wild there are people there are a number
of very high profile Erlang systems
distributed systems but it's not but the
majority of our language systems are not
typically not distributed all there they
are working on a very small number of
hosts and so are not are not aiming for
scalability in the way that we're
looking so one of the first things we
did was build this bench oil system this
is the people at Upsala and I CCS in
Greece this provides a nice benchmarking
set of scripts to do benchmarking of
other Lang systems so this the is the
URL up there if you want to get hold of
this and this shows for example I think
this is showing scalability of dialyzer
on arm a it says it has all the
information there intel xeon blahdy blah
eight gigabytes of ram and it's showing
you a number on a particular release of
Erlang so it allows you to to look at
scalability behavior on different Erlang
releases and there are different there
is different performance on different
releases it allows you to it turns on
different options and
so you have a variety of things that you
can do using using these scripts and you
get this night you get this
visualization what else do you see we've
also done some looks at the
infrastructure so this has been work
done by the OTP team and others looking
at the way the TTS storage behaves and
I've got some slides about that in a
second looking at the way that memory is
allocated and de-allocated and there
again it's a map it's a matter of doing
detailed stuff within the within the
virtual machine about looking at the
locking of data structures I mentioned
the earlier this you've tracing and
that's another know that all these
things get get billed get hardwired into
you make a number of assumptions when
you build a virtual machine and some of
those have to be suspended when you look
at a truly multi-core organizing process
tables and port tables modeling
processes in port signals so there's a
lot of detail here again you'll find
find more about this in in Erlang
release notes as well as the upsala
website and a particular piece of work
that hasn't gone into into a line
because it may have it may have
consequences for particular particular
like existing airline programs this is
to do when you do a copy of a of a term
in doing message passing if you have if
you have sharing in that in that term
what happens at the moment is that that
sharing gets expanded out so if you have
two pointers to a large sub term what
happens when you do the arm when you
send a message with that term as a
payload the sub term gets copied so you
can your messages concurrent message
send can behave very badly in that sort
of context so what what this part of the
project has done is deliver a mechanism
for sharing preserving sharing
within terms when they copied as
messages and there are particular
situations where that's a real value if
you are sending around pieces of code or
whatever arm then that can be that can
be really important but because its
behavior may not always improve things
it's not been it's not gone into the
standard deep down into the standard
release and here's some results about
scaling you can see as the releases have
gone on there's been gradual
improvements small is is good in these
tables you can see that the progress
from our 11 to our 16 has been one of
producing much better scaling behavior
arm for ets set tables whereas there's
been slightly less good behavior for for
ordered sets and so that was a clearly a
place where it's it's more difficult to
achieve optimal behavior and so I think
there's there's there's clearly more
work needs to be done in dealing with
ordered sets and this this shows the
scalar but what happens in our 16 with
the various concurrency options for ets
tables you can see that's that they are
again they're giving very good
performance particularly for sets if you
look at you can see the reading and
reading read/write options they're
giving you good good performance so the
lessons for lesson to learn there is the
ordered sets or not not not working as
they might be locking is a problem still
but is getting improved talking to
Kenneth about this before before lunch
numa is a problem in general in that
what is happening is that that the
virtual machine make some decisions
about what architecture it's targeting
and as things stand it's not there
aren't the options for for tuning it for
different architectures but of course
that's possible but that's I guess
that's a general issue that we're where
that level of the system is being
provided centrally and then some general
advice
about good behavior on for read and
write the places in which to use the
read and write concurrency options okay
so that covers what's gone on in the
virtual machine and still there's still
more work going on there another thing
to say that we have been we've been
eating our own dog food as it were that
we've been looking at our own systems
and trying to to paralyze them to quite
a lot of the work for instance of work
on term sharing came out of
parallelizing dialyzer and we did we've
done our own work on a paralyzing aspect
of wrangler which i'll talk about a bit
later on okay so what have we done in SD
Erlang what we've looked to do in stl
lang was to build to provide some
patterns for interconnection so we
looked at various options and the one we
came up with was to allow you to put
nodes into groups and have full current
connectivity within those groups but arm
to make those groups not have a single
group where everything is connected to
everything else which tends to be the
default what we've also allowed is for
overlapping between groups so the sort
of architecture you see here is one
where for example you have each of these
subgroups is performing a partition of a
task and then then the nodes in the
overlap are they the bridge between that
sub task and the central node which is
collecting all the information together
so that sort of topology allows that
that kind of distribution arm to be
performed so you have have part of the
problem solved their part of a problem
solved their part salt there and then
collecting the information in the center
so the distribution out of the box sorry
this suggested cheesy animation but
obviously if you have the model like
that and you have everything connected
everything else you got in quadratic
complexity and and you begin to you see
this in arm in looking at scalability
this is the D bench benchmark from from
from Bosch oh and looking at their at
different different levels of global
operations you get you get straight line
scalability with the red but the more
global operations you add the more your
scalability gets throttled at at certain
point and that's simply because your
your global operations are growing
inside and this is a scalability of
react this is work done by date the
group at heriot-watt Glasgow who moved
to Glasgow plus people at John Meredith
at basho looking at scalability 44 react
and now you begin to see there with that
you're scaling Falls offered at about 70
nodes so as I say distribution out of
the box is looking like this arm and
what we're providing is this this model
where you can in principle have smaller
smaller groups and that's that
connectivity so it's based on the global
group concept that is in Erlang but in
with global groups those were a
partition there was no overlap and
having the overlap here allow it
supports a number of different
topologies for example what you're
getting is some sort of hierarchy here
with the bridge nodes being the bridge
between the top level and the in the
next level down and of course you can
build ad-hoc models and this is where we
begin to see we're able to run sdl lang
on this is up to 1400 nodes so this is
this is the closest we get to 10,000 but
as you can see we're not at the end of
the project yet so we have a we have
that goal we have that goal out there
and you're seeing there that we're
getting with we're able to push with SD
airline up to up to that limit where as
we were we had to stop with Erlang it
it's 1200 nodes so we are getting that
sort of scalability but let's talk a bit
about what
groups what we can do with s groups the
operations you have on these you can
create and delete them you can add and
remove nodes and you can get back
information about s groups in their
contents and you can register a nudge it
unregister names and this is where the
global operations of the operations
which belong to a group come in that
naming names are shared within an S
group not globally so you're having for
example in you might share this group
will share that name this group will
share probably the name of the of the
central process so there you're getting
a natural hierarchy is mapped onto those
on much on to that topology and that
allows us to build these and you can
remind you can scale this out
hierarchically as far as you wish to go
so that's the the anisa p is a library
in Erlang which is it's relatively easy
to use what we've also done is um the
other thing that goes along with having
this sort of the sort of model is the
ability to to have slightly more freedom
in the way that you spawn new processes
so instead of saying I want to spawn it
there we give you the ability to choose
where you're going to spawn it according
to attributes how low did a pro how low
did a note is the proximity of an ode to
the note that you're on so you can
within the same group or perhaps one or
two groups away so we're able to allow
you to spawn things according to the
this high level view of the system
rather than saying I want it on that
particular node so we're giving you that
level of abstraction there we did a nice
I think it was nice because we do we did
it at Kent we did a nice piece of work
arm of using quick check so we had our
implementation and the guys that Glasgow
said let's build operational semantics
and I thought well it's not really much
point in having an operational semantics
unless we can execute it and then the
obvious thing one when you've got
executable semantics is you try it out
you try out random combinations of the
operations so we built a nice quick chek
model of the of how this group should
work and we were able to do that and
that we had a nice balance between
finding errors in this in the
specification there is in the
implementation and some some
inconsistencies which were we couldn't
we couldn't quite a portion blame but we
found that that was a useful way of
doing some debugging so that was a nice
nice application right how am i doing
for time okay nobody is telling me what
I have two stops and that's fine at the
third level up we're thinking about
operations and maintenance and this is
where the lank solutions contribution
has come in they're concerned they have
customers who are concerned with running
large lank deployments so what they
wanted to do was build a tool that will
support that and this will also also
support the work we're doing here on SDR
lang so what this does is give you
visibility about what's going on in our
line clusters either standalone or it
will integrate with other oh NM so here
you see you can see for instance it will
gather metrics so you're getting metrics
of data of total memory all sorts of
memory matrix total memory at a memory
and so on so you can gather those
metrics across your across your system
and you can classify that according to
different airline releases different
aspects of the system and so on what one
back does is it's an airline process
that sits on every monitored node and it
sits there gathering metrics doing some
logging raising alarms and so on so it's
a relatively small piece of code that's
injected into each to each airline node
and for one example you can see here
some alarms that have been raised these
are the sort of alarms you're seeing are
no down got a disk almost full so you
can you can describe events
you want to be notified and this gives
you a dashboard of that sort of that
sort of problem and you can you can see
that the ones that the toff have been
cleared so the node is no longer down
it's been brought back up but there are
other major major minor alerts there
that need to be dealt with okay so the
final aspect of what we've been doing is
doing various things with tools so you
heard me talk about Wrangler arm
yesterday just to say that some of the
work we've been doing there has been
supporting supporting this project as
well as supporting other things so we've
got refactorings for introducing s
groups and other parallel some other
parallel refactorings complementary to
the stuff that Chris is doing
refactoring groups to s groups and so on
and we've done some parallelization as I
said you maybe saw stavros's talked
yesterday about conqueror that's also
been developed as part of this project
so it allows you to explore into
leavings between processes focusing on
race conditions and they've done a
number of case studies mochi web and and
pull boy lot lots more information there
on the conqueror website we've also done
some work and is it a tutorial tomorrow
about this about taking the percept tool
which is part of the standard Erlang
distribution what that does is profiles
profile systems you can then analyze
them and display the results in a
browser and what we've done is extend
that so that it supports a number of
features which are particularly to do
with running on multi-core so for
example what you can see up there is a
picture what what percept out-of-the-box
does is give you a picture of runnable
process when the process is runnable
each line there represents behavior of a
process when the process is runnable and
when it's not what we've done is is
refine that so that you can see the
orange there is indicates runnable the
green indicates it's actually running so
you get a more refined picture
of the behavior of all your processes so
i should say this is an offline tool you
use your profile and then afterwards you
do the do the analysis we've added to it
information about scheduler migration
for example so you see even in quite
even in quite small systems you might
have a process will migrate between
schedulers perhaps a dozen times it's
part of the built-in as i said earlier
on this is stuff that you want to even
though you don't have a handle on it
it's useful to know that that's going on
as part of the standard Erlang
multi-floor multi-core implementation so
what happens there is that there's work
stealing between different different run
queues and you can see even in a
relatively modest application you might
have you might have quite substantial
movement of processes between run queues
we built a dynamic call graph we allow
you to link from particular particular
processes in this sort of model to the
source code for that process and so on
so what we tried to do there is give you
as much information as we can about the
behavior post-hoc of running systems
we've also to address scalability tried
to present information about processes
in a in a scalable way so you can start
off with a process root and then only
expand the processes that has spawned
and so on as far as you wish to go and
you can profile in a selective way and
we also do we use parallelization to
perform the analysis in parallel so that
if you have a number if you're profiling
a long-lived system you put your
analysis data into a number of files and
those can all be processed in parallel
so more information about that in a
tutorial tomorrow but also online as
indicated there and just a picture here
of some work we used some screenshots of
what we did in parallelizing wrangler
here you see the number of active
processes at any time during the during
the
execution so you see there's a whole lot
of stuff going on in parallel there but
then in this section you've only got one
or two processes running at any one time
and that you can see the the picture of
those two processes that are running at
that point and for example this was the
sort of parallelization that we saw so
evade again this is its hint back to
what Chris was saying earlier what we
had here was a list comprehension and
that turns quite naturally into parallel
map what we did find and I think this
was this is clearly true in the in the
dialyzer case as well is that in
practice unfortunately we don't tend to
write things in a format that's that's
easily or always easily parallelized so
for example here was a case we were
examining examining clone candidates and
that should be you'd think should be
something where we things are done in
parallel there was a tiny little bit of
information threaded through there you
can see that the number there in the
written the tail call is incremented by
one what we're doing effectively there
is just each of the each of those
instances is given a unique identifier
which is a number and we're doing that
by by using a parameter to the function
now of course that that sequential
eise's code that doesn't need to be
sequential so what we have to do instead
is turn first of all turn that into a
zip of the the tasks that we have and
the numbers were going to give them and
then we can arm we can turn that into a
parallel for each so I think you know
what are the things that we we felt in
practice was you need to you need
something which will give you this sort
of analysis so that we could see that
something was going on here where we
could this was this was causing a
bottleneck but we had to examine it
ourselves and do some refactoring by
hand before we were able to to enable
that final that final parallelization so
I guess we had to replace
what was going on there via for each and
then we could do the tournament for each
into a parallel for each but you know
that's life right the other thing that
we did was built and tools for
visualizing what's going on are on the
right you see a tool that's showing you
how s groups are behaving arm and what
you see there is the intensity of a line
shows the intensity of communication
between between those two nodes and each
of the circles represents an S group now
you think why are they come why there
are other lines that join things that
are not in the same s group and that's a
result of some initial setup code
sending some messages around on the left
is perhaps more interesting this is a
picture of this is an online
visualization so a real-time
visualization you can get of the
behavior of a program running now this
is a 2 processor so the top half is one
processor the bottom half is another
processor each arm processor has six
cores so six that those six ellipse is
there and on each arm core are running
to hyper threads so what you're seeing
here so there's a scheduler / I to
thread so there are 24 schedulers going
on there what you're seeing here is arm
size of the run queue so we're coloring
for the length and the color both
indicate the size so you've seen the
size of the queues and also you're
seeing process migrations there so it's
showing you an animation either either
post hot or in real time of how your
program is behaving on a particular a
particular host in and that's in the
small and that's showing it rather more
in the large I think we've I think in
our final our final version of this
we've it was interesting to look at
modeling the S groups as overlapping
circles but i think it's not that's not
going to be a visualization is going to
we've involved some visualization people
at Kent in doing this and click there
are lots of different ways you can do it
and this it's an interesting experiment
but we'll find another way of doing it
finally I think you you you color things
for proximity ok but this we found
interesting you see there a phenomena
going on you see quite a lot of cross
processor migrations for example those
are lines going from north to south you
also see that size of run cues is
different on the on the to hyper threats
on the same process and on the same cool
so interesting no you just get to see
stuff you get to to understand how your
system is behaving by having this sort
of visualization we've done some work on
on supporting tracing so the guys iccs
have done a lot of work on enhancing
what you can do with DTrace and system
trapped and added a backend for percept
to which uses system chat that gives us
a better scalability now it is involved
it's less invasive than using system
tracing and also with system chap it's
an operating system level tracing
facility so you can trace things in the
vm as well as in Erlang at the same time
we've also done some work on enhancing
enhancing Erlang tracing again to try
and help with scalability so logging
only nodes between messages one of the
problems with tracing is that you can
generate too much information so what
you want to do is only generate the
stuff you want rather than generate it
and then filter it out at a later stage
so we've built in stuff for logging only
internode messages and also filtering
log messages as they're generated so you
only generate messages of a particular
kind right I'm drawing to a close and
just to mention a couple of case studies
that we've we're looking at on this
Cindy Oscar this is coming from EDF in
France they have a large arm simulation
framework built in Erlang way in fact an
object-oriented layer on top of Erlang
which they use for simulating things
like the French electricity network
simulating smart meters across across
very large
very large areas large numbers so they
can involve millions of interacting
interacting entities and also we're in
the process of porting Erlang to the
blue jean q which is again a I BM
supercomputer that's not complete so
this time next year we'll have more to
tell you about that so to sum up here's
the years in consortium alang solutions
ericsson and electricity data forms the
University of Glasgow Kent upsilon and I
CCS we continue until next februari so I
think this time next year we'll be able
to give you a final overview of the
project project website is there and we
acknowledge the support of the European
Union so questions so they're going with
the questions if you any questions
there's a microphone
this one to the back
in the in the devotional you showed in
sort of gave the thickness of the lines
between for message passing does that
count mainly the number of messages past
or does it count other properties such
as memory we can do either the way it
was showing there was just a number of
message or the frequency of message is
being passed but easy we can configure
that either for size or total volume or
average size so that's that's
configurable this question here
have you had a chance to compare our
lines concurrency with the go Google
because there are addressing concurrency
as well claiming it's going to be really
good at it so I mean we haven't no I
mean I think concurrency is not well I
don't know there are you there are a
number of different levels you can look
at this what we've been looking at arm
there's bear almost bare metal
concurrency how much can you achieve in
terms of concurrent processes that's one
question which really we haven't been
addressing but the question is what can
you do at the the distributed system
level and that's what we're looking at
okay well thank you very much good</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>