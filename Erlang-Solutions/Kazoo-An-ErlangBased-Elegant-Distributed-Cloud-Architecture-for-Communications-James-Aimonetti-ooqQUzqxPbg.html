<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Kazoo: An Erlang-Based Elegant, Distributed Cloud Architecture for Communications - James Aimonetti | Coder Coacher - Coaching Coders</title><meta content="Kazoo: An Erlang-Based Elegant, Distributed Cloud Architecture for Communications - James Aimonetti - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Kazoo: An Erlang-Based Elegant, Distributed Cloud Architecture for Communications - James Aimonetti</b></h2><h5 class="post__date">2013-05-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ooqQUzqxPbg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">great so my name is james i'm andy i
work for 20
and we make kazoo which is a distributed
telephony platform built in early so
what is kazoo let's distributed
telephony so we come from the open
source world a lot of work was free
switch an asterisk if you've heard a
blue box I was one of our projects from
a while ago but they're all single box
single server concepts and so we wanted
to build the distributed in the cloud if
you will but you don't have to version
of briefs which essentially and so to do
that we've had to create a layered
approach to the different parts of
building telephony systems and then
applications on top of that platform and
to do that we have a very event driven
design where a lot of our code is
reactive to things happening like a
phone call coming into the system so
some of our requirements we needed
redundancy and fault tolerance both at
the code level with handling individual
calls and things like that but also at
the server and at the data center level
because as we've seen with when
Hurricane sandy hit New York when amazon
has their outages entire data centers
can and will go down so you if you're
not distributed geographically as well
and your service goes down and you
provide 911 services and somebody needs
to call 911 and they can't that's bad so
those are some of the design concepts
that we've had to think about as we've
got forward with this we wanted to make
it easier for developers to create
telephony applications so we wanted to
push concepts higher and higher level as
we move up the layers so you'll see that
in some of our work and then we needed
to be able to scale horizontally we can
obviously scale vertically by getting
bigger and bigger
servers but we also need for the
redundancy and fault tolerance to be
able to scale horizontally and have it
be easy it should be trivial to spin up
a new server as in a certain layer of
the architecture so under the hood real
fast we use opensips and camileo as our
SVC if you're not familiar it's kind of
like H a proxy for web requests but for
sip but there's also lots of nice
modules in it that do lots of good
telephony things for us as well our
primary sip engine is free switch it's
where we do all of our media handling so
if there's codec negotiation transcoding
recording voice mails things like that
that's all done by free switch if any of
you attended the lager talk this morning
Andrew Thompson wrote a plug-in for that
that creates a c-note out of free switch
so then we are able to talk to free
switch like another Erlang node so we
don't have to do all the read off a
socket and you know take text and turn
it into Erlang terms we get all the
Erlang terms automatically we use
RabbitMQ for our messaging layer so this
helps us bridge the gap of the low level
part of kazoo which sits on top of all
of the free switch servers and then our
application layer which is where things
like voicemail and ringing a device and
things like that happen and then we use
big couch for our long-term data store
most of the time you set up a user you
set up their device you set up the call
flow and then they don't touch it for a
year big couch is wonderful for this
we've we've had some challenges with
doing more real-time stuff with it and
so we've adapted to that but it for
long-term storage in a distributed
manner and having it be operationally
very easy be couch has been a blessing
for us we have a lot of sad but funny
stories about customers deleting entire
databases off the disk and big couch
doesn't blink recovers them fine and
we move right along so it's we can't
speak highly enough about it and then
obviously kazoo and we have two pieces
of kazoo we've got the low-level layer
that sits on top of free switch called
eco manager and then we have the
application layer where things actually
happen so here's our layered approach at
the border we have sip coming in from
our carriers and from our devices so
when you pick up your phone and you type
in a number it actually sends a sip
request which if you're familiar with
HTTP and a get with the URL and then
headers and then a body sip is almost
identical except that it's invite phone
number at some realm and then some
headers and then potentially body and
then we send responses like 180 183 200
ok sounds familiar right so that's
that's up there but sip is kind of a
pain to deal with so our next layer is
taking sip and turning it into our
internal format Joe talk today about
having contracts and taking the outside
formats and converting it into an
internally consistent format that's
exactly what we've done we have an
internal format for these sip messages
for our commands to do things and so
that's what II call manager does besides
the clustering of free switch it takes
our internal format and gives it as
instructions to free switch which then
become sip messages and things like that
amqp does the marshalling of our data
around because distributed Erlang is not
great across the land we use RabbitMQ to
do that by just connecting TCP sockets
across the land right now we're still
playing with a lot of the h.a and things
like Federation and such for rabbit but
right now we kind of have engineered it
so that we can have a rabbit in this
data center and a rabbit in this data
center and everybody still can work
because kind of the binding glue is the
free switch server
distributed Erlang across the land is
it's not great across across the public
internet the LAN wa n yeah it was as far
as I know it was designed to be run in a
trusted environment a trusted Network
and so making connections between data
centers not ideal it's doable but not
ideal and then we have our HTTP which is
where we interact with most of our
developers our GUI we have breast api's
for doing all that stuff and then
obviously communicating with big couches
all over HTTP so that's kind of a
high-level what we're working with and
here you can kind of see the a potential
layout so you could have your sip
proxies we try to use like DNS SRV
records and things like that to do round
robin across that and then they round
robin to the free switch servers which
then we have multiple ecall manager
servers connected to each free switch
server so if one goes down 1e call
manager goes down there's somebody's
still there listening hopefully
potentially multiple RabbitMQ servers
within a data center multiple
application servers and obviously
multiple big couch if you're going to
yeah well so our business model is more
on helping you set this up and running
it we have a hosted offering but it's
not our primary focus so we don't have
we haven't scaled it for real traffic
Karl's done a lot of likes it p testing
and things like that and he would have
better numbers to tell you the whole
idea is that you can take this and run
it on very small commodity hardware you
don't need to buy huge servers to get
started so we run on rackspace one gig
instances for all of these servers
so they're very minimal so it's
obviously going to depend on the size of
the hardware you want to throw at it
because in general the limiting factor
is going to be your free switch servers
so it depends what you want to do if
you're doing a lot of transcoding it
doesn't make sense to run on small
instances you're going to want beefy
hardware but if all you're doing is you
know bypass media mode and all you're
doing is the sip signaling you can get
away with virtualized no problem yeah so
our event driven design lots of things
happen to come into our system and cause
things to happen and then within the
system itself with timers and things
like that firing and so most of our code
is reactive to that and we do have
people that like to manipulate the DV
directly which is naughty but it happens
so we need to react to it as well all
right so that I've given talks about
kazoo at previous Erlang factories
that's why I'm kind of just blitzing
through it all because we wanted to talk
a little bit more about sort of the
lessons that we've learned with using
Erlang to build this platform we're
coming up on three years that we've been
working on it so I want talk about some
of the utilities that we've built and
kind of how they've evolved a behavior
that has become the cornerstone of most
of what we do how are cashing has
evolved as our needs have evolved how we
process a call just to give you an idea
of how we structure the application and
then the latest that we're playing with
is this tight coupling of both the gin
listener and the gin fsm behaviors and
hopefully you guys will learn something
and also have feedback for us about
where we can improve so the first thing
that we decided was everybody knows JSON
so we're going to use JSON so that the
ease of getting new developers and
invested in our platform would be low
but at the time moji JSON to
was kind of the only real JSON encoder
and decoder module and they tagged their
JSON objects with the struct right so
you had a 2-tuple with a struct and a
propolis to represent an object okay
fine and then all these other json
parsers came out and now they're just
doing the single tuple with the propolis
inside of it okay fine we knew that we
wanted we didn't want to do a lot of
pattern matching on the JSON object we
wanted to treat them like a dict you
know some sort of opaque type so that's
why we created wh JSON so that we could
interact with these decoded JSON objects
in an opaque way and then as parsers
came and went and we're better or faster
or met our needs at you know different
needs then we could swap them out but
none of the application code would have
to change so now so that's how that's
how we started it was just very simple
things like get value set value delete
key things like that but now that we've
had a concerted effort to integrate
dialyzer into our workflow we have types
defined for use in specs and we've
aliased a lot of casting from a data
type to a data type rather than doing a
get value which is then wrapped in a two
binary or something and then as I said
we're able to switch out the encoder as
needed and we actually did the
transition from OG json to e json i
think we had like three lines of code
that we had to change because Carl was
pattern matching on the struct format
that was naughty so when we first
started we had very simple JSON
validation where we would iterate you
know we you would set up a list of
required keys a set of optional keys and
then maybe some optional value value
date validation or type validation but
then we decided with this great JSON
schema stuff that we wanted to maybe do
that instead so we wrote a validator
that would
use that spec and try and validate JSON
for us so like any good team we gave it
to our intern to try so he wrote the
first version and it was bad I guess
I'll pause here for a second because we
were talking about it at lunch but we
found we have a rule of three that's
kind of emerged as a pattern and I don't
know if there's anything formal to this
but we find that on the third rewrite of
a component we get it that's the time we
got it right so the first time it's
usually very exploratory very much a
prototype to the point now where we
expect to throw away the first version
and it's just kind of to wrap our heads
around the problem space the second
version is usually like if I write it
the first time Carl usually goes in and
adjusts and does it better and then the
third time we actually like meet and
talk and then we get our heads around it
really well and we write a really good
version of it and so that's kind of
where we are now we're both Carl and I
have taken stabs at writing this
validator but we may there's I saw that
there was another repo that was doing
this so we may just swap it out and
contribute to that instead but it's a
very simple API you pass in the JSON
object and the schema which is also a
JSON object and you get back the fixed
JSON object so like if their defaults
that you didn't define it'll fill those
in things like that or else you'll get a
prop list of the key path and the
failure message so that you can report
that back to your client and then this
is probably fairly typical but wh you
till was our dumping ground for random
functions like type conversion timer
offsets we had to do some stuff with
couchdb that involved encoding and
decoding the account IDs that we use so
that got stuck in there there's like
Gregorian to unix timestamp conversion
there's there's all sorts of random
stuff in there right it's the total
drawer so now we're slowly starting to
break those out into modules that are
more easily discovered but if you ever
wonder where we're doing something
that's probably in WH you too so as we
saw earlier amqp is a very big part of
our platform and so we wrote a behavior
eventually to help us in that endeavor
at the beginning every process that was
going to interact with amqp use the low
level primitives like creating a new
queue consuming from that cue binding it
setting up the bindings and all that
stuff all of that work was done on a per
cubasis so each application if I wrote
it I did it one way if Karl wrote it he
did it a different way there was a lot
of inefficient use of channels which are
fairly expensive to create and destroy
and so if you had a process that came up
received a message and then went down
that was fairly taxing between us and
rabbit so there was just a lot of
duplication of effort going on a lot of
weirdness where yeah anyway it was bad
we were hurting cats with it and we were
trying to figure out why we were having
performance problems why there was
stability issues all sorts of great
things so we said you know what we've
got Jen server and we're receiving our
messages and handle info and then we're
unpacking the aim QP packaging and then
you know decoding the JSON and then
we're sending it off to the worker maybe
we could standardize this so that
everybody can make use of that so that's
where Jen listener came from it's
essentially Jen server with amqp
specific handling on it so in your start
link you send in the bindings things you
know q.q settings consumption settings
all sorts of amqp specific stuff and
then what we decided was each of those
messages that came into the handle info
of the gen server part of the code would
then be spawned into a handler process
with
information provided to it so that if
you needed to serialize back to your
code you could do that but initially
each request would come in and be
spawned into a new process because
processes are cheap this was amazing for
our code it was amazing for performance
suddenly our rabbit you desolation
dropped significantly things were much
cleaner and most importantly we were
able to hide amqp specific stuff from
the application code so now you're not
having to know how to reconnect to the
broker when the broker goes down or how
to connect to a different broker that's
higher priority things like that all of
that is done for you in Gen listener and
so with our interns you know getting
into telephony nobody knows telephony
that knows are relying surprise weird
and nobody that knows are lying those
telephony for the most part I mean it's
really hard to find that intersection of
skill sets right now and so what we've
been having to do is teach people Erlang
and then slowly teach them the lefty
side but then to add in amqp on top of
that has just been a nightmare so this
has been great because now they don't
have to worry about it and eventually I
would like to add XMPP and actually you
know be able to use other messaging
protocols as well but Jen listener is
everywhere it's very pervasive in our
code and it's great so if you have
patterns i recommend creating behaviors
because it does amazing things for your
code for consistency it's just great
Jimmy that's pretty good WH cash this is
another module of ours that has become
quite prolific in our code base in the
beginning going to couch obviously if we
can not with stuff not changing very
often we didn't feel the need to pay the
penalty of the network so i just created
a dick trapped by jen server that had
per entry TTL and we basically just
cashed the json objects that we were
storing in the database
and that way we didn't have to go there
if it was already in the cache it was
great except that it serialized access
everybody was dumping their stuff all
the applications were dumping their
stuff into the same dicked and hopefully
if anybody has done anything like this
they know how bad it gets on high
performance code but we did have a very
nice simple API and it was easy to use
but it didn't scale so the next step was
to take it out of the dict and put it
into a net stable and now the gin server
managed yet stable and that was much
better we grew from the prop list into
having records that slowly grew because
Carl got tired of adding extra
parameters to the functions and so we
just wept it all up into a prop list and
put that into a record and so now it's
we can do much more complex fun things
with the cache entries but still a very
simple key value set and find and peek
and whatnot still worked great we took
it out of the registered process and we
started starting them on a per
application so now my application has my
cached objects and none other so I can
control it better and we started to do
more complex work so we have this
ability to have a user defined in the
system and that user can have multiple
devices you can have your desk phone at
your office you can have your cell phone
that your call forwarded to you can have
a sip phone on your laptop you can have
a home phone you know other devices all
of these things you know when you ring a
user all of those things could
potentially ring and be answerable so a
lot of that work to figure out which
devices go to a user that kind of stuff
that we can pre-compute and cash well
now that's the cash has been extended to
use that as well we had a callback so
that when an entity was expelled or
deleted from the cash we could run
arbitrary code so things like
registrations if a rate normally a phone
would read
sir for say 60 seconds and then before
that expiration occurs they would re
register and so now you always know
where the phone is if they don't then
maybe they just forgot and they'll
register again but if you suddenly get a
whole mass of mists of expired
registrations and they're all from the
same domain maybe that site's down maybe
the network connectivity is down there's
a lot of information to be gleaned from
some cache entries being expelled and so
this ability to run arbitrary code we
can email ops we can say hey you know it
looks like you're having trouble with
your network because 10 of your devices
have failed to reregister so we can help
it helps us be proactive in that regard
oh yeah but our biggest issue was still
that invalidating entries was still
problematic a user would change their
caller ID and the GUI and they would
complain that it hadn't updated and we
would tell them well wait 15 minutes
because that was the default TTL I can
then it'll update so we wanted we wanted
to fix that so the current version if
you're doing things that involve the
rest api is the database entries things
that users can change outside of the
system that they expect to be reflected
internally you can use the gin listener
now every time something gets changed
within the rest api's we broadcast out
on today mqp that this document has
changed and so you can actually
subscribe for those document changes and
immediately invalidate the cache entry
so that the next time the caller makes a
call their new caller ID is pulled down
and you know then things are good that's
been nice
alright so call process call flow
processing so we have this idea of
somebody dials a number what do you do
most typical use case is dial a ring a
user and if the user doesn't answer go
to voicemail right you could also go to
a menu press 1 press 2 plus 3 and based
on those choices you have a tree okay
pretty simple when we first started
creating the call flow application we
had the CF executor process get started
up for the call and then it would go
into these sub modules like CF user CF
voicemail and then we would basically
have try catches everywhere just in case
the user module crashed or the voicemail
module crashed and so obviously very bad
for Erlang not idiomatic at all so now
see if executor is a gin listener and
it's ponds the CF user process and hands
off the call information and it proxies
all of the amqp events related to the
call so say you're in a menu and you're
waiting for somebody to push a
touch-tone see if executors will
actually receive the a.m QV message
unmarked shall it get the JSON object
out and then send it to the CF menu
process and then the CF menu just has a
small received loop where they can grab
it and then make decisions based on that
so call flow executors monitors those
processes if it receives it down then it
moves on to the next module or these sub
processes communicate back to the CF
executors saying continue or stop like
if you've reached the end of the call
flow tells it to stop and so that's
given us if we've got rid of a lot of
exception handling code and it's just a
much cleaner interface much easier to
work with now so for an example on the
right you can see this is what our GUI
looks like so if somebody dials
extension 101 it's going to dial Darren
if he doesn't answer then it'll go to
Darrin's voice mail
this is the representation in JSON so we
have the first module to be run the data
that gets passed along to the see it in
this case of the CF user module and then
the children for what should be done
next if anything we use the underscore
don't know where we got that from to be
the catch-all child so if CF user say it
rings in this case for 20 seconds if
Darren fails to answer his call then the
CF user will get that event the timeout
event and it will execute CF continue
see if exe continue which will tell the
call flow executor I'm done but you
should keep going in which case see if
executor will come to the children find
the catch-all and in this case it's
going to spawn a CF voicemail process
with the ID of the voicemail box and
then there's nothing left to do so once
see a voicemail says it's finished then
the cough flow is done and it it
terminates as well bless you whoever so
it's pretty straightforward the JSON is
easy relatively easy to read so this is
typical for most of our applications I'm
just using the call flow application in
this case so we have the top level
application supervisor that gets started
when you do application start call flow
this is the supervisor will get started
it'll create a CF listener so most
anything that's going to be interacting
with amqp has a top-level listener for
the most part that handles application
specific requests so in this case our
listener listens for routing requests
which you can't read but we also have a
second listener called the CF shared
listener that listens for things like
presents MWI and other non routing call
flow related events which without
getting into the telephony stuff it's
just other stuff that goes on related to
bones and then we have the CF executors
to
visor which is a simple one for one so
very quickly call comes in CF listener
receives the routing request spot and
that JSON gets put into a spawned
process call flow figures out whether it
can handle this call or not because we
have other things that handle routing as
well so it may or may not actually know
what to do with this call if it does it
sends a response back down to e call
manager and it gets back a win if it if
he call manager determines you're the
guy that won congratulations when that
win is received that's when see if
executors gets spawned and the call gets
processed and so obviously you know you
could have potentially hundreds or
thousands of CF executors under this
supervisor so I imagine eventually we'll
get to a point where we start charting
the supervisors and we're not there yet
so this is the latest and greatest that
we're still kind of playing with but
it's the tightly coupling of a gin
listener and a gin fsm where it makes
sense so in this case we have call
queues which are you know call our calls
in gets put on hold until an agent is
available so you could you know the
whole idea is that you can handle em
customers with in client or agents where
n is less than M right everybody's
familiar with call keys at this point I
hope but agents and call queues
themselves Len are very easily modeled
with a finite state machine but we
didn't want to throw away all the aim QP
work we adjust done with Jen listener so
we've I said rather than trying to
integrate amqp into Jen fsm we'll just
tightly couple and that a listener
process and an FSM process and let's see
what happens
so a lot of what happens initially is
that I think for somebody was talking
about this but you end up with a status
or a state or some sort of internal
variable within a gen server that
becomes your state and then you receive
events and you do things based on that
state well that's an FSM so let's
actually use an FSM so I've kind of
leave these out differently okay so we
have a top level supervisor that
supervises all of our agents in this
case and then we have an agent
supervisor which is started as a one for
all which means if any one of the
subprocesses dies all of them will be
restarted and the agents is a simple one
for one of agent supervisors that's
always fun to talk about under the agent
supervisor we have the fsm and the
listener processes and so you can see
the main states of an agent are it's
ready I want to call it's connecting
meaning we're ringing the agent trying
to connect with the customer answered
the agent has answered and been bridged
to the caller a lot of call queues once
the agent is finished with a call they
are put into a waiting period so that
they can enter notes into a CRM or do
whatever paused if I'm going on lunch
break rather than logging out I can just
pause myself and I won't receive phone
calls or outbound where I'm making a
phone call on my phone unrelated to call
queue stuff so the code becomes very
concise the way you deal with DTMF when
connecting versus when answered versus
when waiting or paused changes routing
requests so if we're in the ready state
and we receive a router mood quest we
know that the agent is trying to make an
outbound call so we go to the outbound
state instead but rather than having to
do all the amqp related stuff in the fsm
we offload that to the listener
so all the receiving of call events
routing events things related to our
agent and the calls that we're working
on get routed through the gin listener
which then spawns into the gin fsm
module and then all the when the fsm
needs to do things it sends requests
like bridge to caller to the gin
listener who then sends the actual amqp
commands to do so our synchronous in
synchrony so when the fsm sends a
command to the gin listener process it's
a cast and so the gin listener has
almost no state other than like agent ID
account ID things like that but it just
receives something from the fsm process
and does it there's no decision-making
really other than you know inserting my
agent ID or something like that it's
very dumb it just does what it's told
it's very obedient so the fsm holds all
the state about the agent so it's the
serializing point these guys are not
well it's like a gent server right or
you know it's just hanging out it's not
it's not blocking in a received loop or
anything like that the low level stuff
might be yeah so at so as amqp events
come in the gen listener process
receives them and then spawns a handler
for it which then immediately sends it
feeds it back into the fsm so when I
want to ring an agent so the callers on
hold here I spin up I originate a
channel to the agent and then I sit and
listen until I get either a channel
answered event or a timeout or reject
maybe the agent hit reject on the phone
so all of those become events back into
amqp which the gen listener receives and
feeds into the fsm so if I'm if i go
from ready to connecting which is when
i'm trying to ring the agent and i
receive channel a channel answered event
then i move to the answered state if i
receive a cancel or a timeout or
something like that I'll increment how
many failed attempts there were and go
back to the ready state and then if they
fail like three times or some
configurable amount a log the agent out
automatically
yeah in the state of the fsm yeah but
things things related to call like
reporting and calls and all that stuff
go into the database but things specific
to the agent process are stored in the
fsm and it's been really nice I've
really enjoyed this pattern I've applied
it to the call queues as well but it the
call queues I've made a little more
complicated we still have the queues sup
and then they queue sup and this is
really fun when you were talking about
amqp queues vs call queues versus
message queues within the Erlang process
there's a lot of overloading of q and
this it's great we have a queue manager
which was necessary because we wanted to
be able to process more than one call
into a call queue at a time and so we
for now I have a workers sup and a
worker sup and then the fsm within this
case to listeners and there are reasons
for it but this works really nicely
because call flows will receive a call
for the call queue if it'll send a name
QP message which the queue manager is
listening for which will then do some
reporting and metrics and other things
and then it will republish this customer
call request which will then get a
round-robin between these cue shared
processes so they all bind to the same
amqp q so you can have multiple
consumers and then these causal round
robin so that's how we do the parallel
processing so if there are five calls
and you have five listeners all five
will try to connect to agents at the
same time and then you have the cue
listener which handles so we receive the
request it gets fed into the fsm the fsm
then asks the queue listener to send
events out to the agents saying who can
take this call the agents respond to my
cue listener process via amqp which then
feeds it into the fsm and
fsm has right either round robin or most
idol or some other strategy to figure
out which agent that responded should
get the call things like that so anyway
it's really cool it really cleaned up
the code from you know having just gin
servers with a variable in their state
and its I don't know it's been really
cool i really like it so you should try
it oh and that's the last slide how are
we doing on time we're good ok so the
project is at github it's completely
open source the whole platform we have
chef scripts to help with installation
but you can obviously manually install
it all the components like grab it and
big couch and free switch and opensips
camileo they're all open source so you
can build your own cluster play with
this we're twenty six hundred Hertz org
we do telephony stuff that's me James
and we're always unofficially hiring
because it's really hard to find people
that want to do telephony and Erlang so
you don't necessarily have to do
telephony either we do a lot of other
stuff on top of the telephony so if
you're interested let us know we're
really fun any questions comments snide
remarks
yeah the question was besides gentlest
or any lessons learned with rabbit yes
and that's a talk in and of itself we so
rabbit is wonderful we like big couch we
can't speak highly enough about it all
the issues have been on our end with
abusing rabbit like creating channels
and Q's willy-nilly and really having no
it was just kind of Wild West shoot-out
I want a channel i can have a channel
you get a channel you get a channel
everyone gets channels and creating
those channels you know once they're
created it's fine but that whole
creation process really is expensive
relatively and so if you have a lot of
calls and they're all spinning up
channels at the same time to try and it
gets to be quite a bit of load and so
obviously that impacts your scaling so
as we learned better strategies for
where we could where we could have a
little bit of serialization on creating
cues consuming from those cues and then
you know multiplexing out into Erlang we
saw a significant cpu utilization drop
memory drop all sorts of stuff to code
it our code itself is cleaner Karl's
done a lot of work on sharing
connections doing automatic failover we
have clients that are using stuff
they're using kazoo in very tumultuous
environments where links go down all the
time and so it's not to pat ourselves on
the back too much because there was a
lot of work involved to make this work
but the ideas that you know the theories
that we had about how the platform would
work held up from those part and so
we're we're able to go from three nodes
completely connected to three nodes
completely not connected and then have
them come back hours later and
everything's fine so yeah it was mostly
just us abusing rabbit and then figuring
out better ways not to
use it I don't know what's a yeah that
was another thing unbinding a binding
that didn't exist would take the whole
connection down which would then you
have to reconnect everybody and you have
the stampeding herd and then it just
became a complete mess yeah so we've
we've done a lot of work on our side to
insulate ourselves from accidentally
doing that things like that
okay at the moment yet we haven't had it
hasn't been the limiting factor for us
the question was have we found latency
issues with using rabbitmq for the most
part we don't the requests don't go over
the land or the LAN so we're operating
within Rackspace's Network internally
and so it's usually pretty fast well we
do go over the land as well but that's
kind of like a failover so we sent with
like we send out a route request so
it'll go from E callmanager to the Lapps
via rabbit but it'll also go to the
other data center you know Chicago and
Dallas say so we expect if the requests
originates in Dallas that the Dallas
application server will respond first
just because it doesn't have to deal
with the network latency as much but if
for some reason the Dallas application
server is down or doesn't respond
properly or whatever then we still have
Chicago as a fallback and maybe we have
a hundred milliseconds 200 milliseconds
on top of that but in general it's we
haven't seen significant issues from it
not not to the point that it's
noticeable to users most users are used
to you know even up to a second of pause
before ringing starts
so we don't we've actually had issues
with users with their own installations
modifying the XML to configure free
switch because we we try to manage
everything in kazoo eat including the
free switch XML configuration so there's
we don't use standard like the normal
dial plan or directory XML structures or
anything like that we've patched a few
things for free switch but I mean we
don't really do a lot of bizarre tuning
or anything like that we've been doing a
lot with it Sofia recovery stuff if
you're familiar with that so Anthony the
main free switch developer they wrote
the ability to have free switch running
in an H a so basically you have to free
switch servers and if this guy is
processing calls they it logs it to a
shared database and then their heart
beating with each other so that if this
one crashes you can actually rebuild the
channel on this server and send a
reinvite and then all the calls should
stay up assuming the carrier and all
that stuff is involved so we've taken
that and we've applied it to individual
channels so now we can if say you call
say you're in New York and your call
happens to get round robin to LA and
then you're calling somebody in New York
so rather than making the sip and
potentially the RTP go all the way
across the country twice we could
actually move your channel from the LA
box over to a new york-based box and
then you get much lower latency much
better quality audio things like that
and we don't use the registration module
and free switch we store all that in
kazoo so if you register to free switch
one and a call comes in for you on free
switch to we don't have to somehow take
the registrations from one and put them
onto we store them in kazoo so then we
know how to get to your phone from free
switch to even though it never got your
registration things like that but yeah I
mean as far as like doing weird hacks to
free switch not really it's pretty
vanilla still
well so when II call manager starts up
it has a list of free switch servers
that it connects to and it registers
itself for dialplan requests directory
requests things like that so if you're
familiar with like XML xml-rpc stuff
that free switch can do like XML CDR and
stuff so it's essentially like that
except with moderate laying event we are
able to do that all with native Erlang
rather than an HTTP request or something
like that but then from there all the
dialplan and directory is done by kazoo
using either send message or the API
commands like EU IDK land that kind of
stuff
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>