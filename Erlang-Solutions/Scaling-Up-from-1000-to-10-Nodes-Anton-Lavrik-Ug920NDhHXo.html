<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Scaling Up from 1000 to 10 Nodes - Anton Lavrik | Coder Coacher - Coaching Coders</title><meta content="Scaling Up from 1000 to 10 Nodes - Anton Lavrik - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Scaling Up from 1000 to 10 Nodes - Anton Lavrik</b></h2><h5 class="post__date">2013-05-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Ug920NDhHXo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so my talk is about a data collection
system that we built at alert logic and
the lessons there we learned along the
way so before I describe what our system
does and by the way how many of you have
written applications in there lank
almost yeah almost everyone so but for
those who have do not have experienced
with their lank and the rest of actually
the audience if you have any questions
feel free to interrupt me at any time so
before I describe the actual and my talk
comes in two parts the first in the
first part I will describe the system
whether it does and our architectural
approach and in the second part I will
go over the list of problems that we
encounter it while building testing and
am testing the system so and so let me
start with some numbers so these are
numbers from last year and so in the
first row you can see the number of logs
alert logic collects every day from
various servers and on the and the
number of tweets Twitter receives so the
second row corresponds to the actual
data volume so what the first row means
an actual data volume and in the third
row and I thought this number is really
really interested in Twitter like their
servers actually generate hundred
terabytes of logs a day so they shared
this number last year in one of the
talks at strange loop and so and the
number of logs we generate our servers
generate is not that interesting so
which means that our operational
footprint is low and to be honest it's
not like a fair comparison like these
companies are completely different so
the things that Twitter does with its
server logs is really really interesting
they have like a completely separate
team that all it does it supports like
this massive Hadoop cluster they do lots
of analysis there so what essentially
Twitter does is that it takes some fair
number of human generated messages then
distribute it to massive audience and
does a lot of analysis and that's
probably contributes to that hundred
terabytes a day of server logs alert
logic on the other hand clicks machine
generated data and then it runs then we
run some analysis and present the
results and the collected logs to users
to the users and the security analysts
in summarized filtered or other
meaningful ways and the type of analysis
that we run is mostly related to network
security and compliance so this is what
our system looks like as of last year so
alert logic has a product called log
manager log manager is a hosted log
management solution at the top you see a
100,000 hosts from which we collect data
today well last year and we do it via
appliances and it works this way
customer signs up we send them an
appliance here X it up configures lock
collection through our web UI and then
appliances started started collecting
data from remote hosts and once they
collect they send data to our back-end
to our data center then we do a bunch of
stuff in the data center and we finally
expose the collected logs and some
analysis results reports and other stuff
to the user through the web interface
so the problem with that system is
actually here in appliance is the
problem with appliances is that
appliance is a physical server it's a
small server but there's still some
costs associated with deploying it and
managing it and they're deployed all
over the place we have customers like
everywhere and this piece of hardware
can break down and the in order to
account for that you need to either keep
a provision spare or send a replacement
really quickly or start thinking about
doing highly available appliances which
is doable it's not a rocket science but
so we had to do something about that and
then Rex by Rackspace or one of our
largest partners start telling us hey
your appliances take too much of our
rack space can you do better so at this
point and of course we had to account
for product and company growth so in the
new system we're looking at collecting
data from over a million hosts different
hosts and going back a little bit just
to describe the kind of logs we collect
is for the most part these are server
logs so we collect six logs we collect
logs from windows boxes and also logs
but what we call flat file logs such as
Apache my sequel I is and everything
that's basically written to a file and
can be like or and rotated and not
rotated in some way and there's who some
of them go ahead
that's great it's it's a daily number i
think the slide says that gets daily
numbers oh that's fine so if anyone has
any questions about the numbers i'll be
happy to clarify so and some of these
hosts are real servers like web servers
or database service or domain controller
some of them are and they send a lot of
the logs and some of them are like
point-of-sale terminals running some
ancient Windows version behind like
really slow links so it's a kind of its
it's not a typical data center
environment so collecting collecting
logs for amendments and some hosts go up
and down like they don't have permanent
internet connection they are located
somewhere in like oil rigs so collecting
data from this collecting logs from them
is actually a fairly challenging problem
and doing it at scale of course is even
more challenging and reliably so in the
new system we're looking at collecting
logs from over a million of hosts and we
also had to solve the appliance problem
and it came down to two things so we
introduced agents as a primary
deployment product deployment method and
an agent as a agent as a program that
you install as a software package that
you install on your host and it collects
logs just from that host and we also had
to simplify our appliances a lot to make
them more manageable so the host at the
top they do not go away so we collect
them today as we were collecting from
them yesterday so remote collection is
still very like important use case but
we wanted to offer more more flexible
deployments and so some of the
management problems that we had so those
are marked new appliances are marked as
a prime and and agents are in the yellow
oval marked as
h prime but the rest of the system is
pretty much the same so since the the
remote collection stays pretty much the
same i'm going to focus on the new way
dealing with agents and our new
appliances so zooming in I need to tell
a little bit about what we do once we
receive the logs well the first of all
we need to collect them and after we
collect them we store them run some
analysis run reports make the log
searchable generate alerts if they
configure it and finally expose all of
that stuff through the web UI so if we
look inside the new appliances in host
on which we install new agents they the
software that they actually structured
as a set of programs written in C++ and
one program for example handle syslog
collection another program handles
Windows Event log collection the third
one flat file collection and the fourth
program supervises the third the first
three so and we call them clients and
the reason we call them clients is that
each program regardless of where it runs
communicates with the backend directly
so they don't actually coordinate
between each other all the end we try to
keep them as simple as possible so all
the actual coordination happens in the
back end so they just perform the log
collection function another interesting
thing is that those clients that run on
a host on which we install an agent they
are inside that point is essentially the
same programs so and it's it's almost
the same it's it's the same code in case
of appliances they perform remote
collection function which means in terms
if you want to collect from our most
Windows host those programs reach out to
a remote Windows host and collect live
Storm em if it's an agent
they collect from the local host and
they speak exactly the same remote
collectors and local collectors speak
exactly the same protocol to the backend
and and and they're also like very very
simple they're statically linked and and
we also built in some built some
management functions to simplify our job
for example we can upgrade them remotely
so we have a function built in in
special clients that allows us to
upgrade software versions or downgrade
individually or sets sets of agents and
sets of appliances at once with a push
of a button so if we look at the back
end it's a it is structured in three
layers and the back end and talking
about collection system mainly here and
I'll focus on the collection system
specifically so the collection system
includes a load balancer and clients
talk through that load balancer to the
what we call collection controller those
are actual nodes and load balancing and
persistent QR functions and and finally
the final goal is to offload all the
collected logs to the persistent queue
so this is what our system essentially
does now knowing that all those clients
are actual small programs that talk to
the backend independently we can like
simplify this diagram and for the
purpose of this presentation and
architectural you know over you treat
those clients independently so and the
original so the original program then
transforms into having hundreds of
thousand clients running somewhere all
over the internet that talk through the
load balancer to the collection nodes
collect collector collector controllers
and finally their logs end up in the
persistent queue and of course this
picture wouldn't be full
complete if I didn't specify the
protocol so the clients communicate over
HTTPS with client-side ssl certificates
and we heavily rely on sticky sessions
that's a pretty conventional term for
specifying that all those clients
maintain association with a specific
back end node specific controller nodes
and if you forget for a second that
these are called collection controllers
and there is some like weirdness
persistent queue here this looks like a
typical web application architecture so
what's different though is the fact that
unlike in a typical web application our
client sending us lots of data like like
and then the typical web application
clients would send small requests and
receive data back so it's kind of the
opposite picture another thing that
makes it different is that the
collection controllers they're basically
receive a portion of collection logs
this is the brain of the system by the
way the collector we keep clients very
simple and the collection controllers
they decide what to collect when from
which hosts at each interval and what to
do with collected data so and normally
when they receive a new portion of
collected logs apply a little bit of
transformation mostly attribute
expansion and get the and call a couple
of services and finally write it to
persistent queue so while they're doing
that the data the collected portion sits
in memory so and and it stays there
until the this final step of data
processing finishes
and that's that that's also and not only
collection controllers but some of the
stuff in the load balancer does the same
and we optimize our data collection
system for collecting as less often as
possible and we try to collect data with
in chunks that helps us to so we buffer
logs a little bit on the clients and we
did specially so that we know that like
larger buffers or logs compress better
and we transfer data compressed and it
also helps us to minimize the request
rate so essentially that mini my help
stew keep the volume the request
straight here in here lower and it helps
with some of the transactional stuff
that we do here when processing logs so
and I would claim that this problem is
actually is much harder than a problem a
typical web application needs to solve
basically you're dealing with lots of
data that's coming your way and you need
to do it to handle it in certain time
and do it very efficiently so we try to
do as less operations that data
processing here as possible to to keep
that efficient so and if you if you look
notice the the number three next to the
collection it's the actual number that
we have today in production so and so
these are literally four core virtual
machines and if you look at the original
diagram we had thousand appliances and
the collector controller function used
to run on appliances in the old system
and now we brought it down to three
collect collectors node and the
collectors controllers they're
implemented in their length and the same
similar type of relying program used to
run on our appliances so that's the idea
behind the name of the talk
so how we took the program that ran on a
thousand nodes and got it to run on only
three or handful of nodes and we can do
it very very efficiently we learn to do
how to do it very efficiently so we if
you look inside the collector controller
nodes and it's a very so that's an
airlink application and inside that
there lank application for each
thousands of clients we have a matching
airline process running and it's a it's
a long-running airline process and the
HTTPS requests routed through the load
balancer from the clients eventually get
delivered to this airline process in the
form of gin server call and then they do
all the processes and and all those like
twenty forty thousand airline processes
basically do their work simultaneously
although all at the same time and this
is exactly the use case that our lank
was built for it's a fairly typical
application architecture scenario in in
telecom so when you have lots of
processes small processes that doing a
little bit of work and they all do it
all the same concurrently and they're
like really shines at that so basically
you don't have to think how to map all
this like thousands tens of thousands of
concurrent things happening to your
handful of CPU cores the airlink vm does
it really well and it does it with soft
real-time guarantees so basically
guarantees that they all keep moving
slowly if they're bottleneck on some
like cpu resource go ahead
that's a good question I actually forgot
to mention that so the load balancer one
of we went back and forth on various
load balancer options and we quickly
found out that Erlang is not allowing
SSL implementations not very efficient
so we basically decided not to use not
determinate SSL in there lank it's a we
were seriously bottleneck on CPU so and
we right now and it's going to be
changing i'm pretty sure but right now
we're using a combination of h a proxy
for SSL termination and we have a
custom-built HTTP 11 reverse proxy
written in Erlang for doing some of the
stuff that we need so one particular
thing that we need in a load balancer is
maintaining those sticky sessions and
and also dynamic reconfiguration so we
need to be able to rebalance load
basically take some of the clients from
one node and move them to another node
we need to have dynamic blocking like if
some client starts misbehaving or
something else like happening that we
want to prevent like down the stack we
can quickly block it at the load
balancer level without you no change in
h a proxy config or anything like that
so we have a separate system built for
that so for course p.m. running on
vmware vsphere whatever so when we have
three of them and we know that we can so
remember I was talking that hosts like
those clients and generate load a
different rate so some of them will be
like constantly spinning and sending new
portions of logs and some of them we
will reach out to them like every five
minutes or so so and so it's it's it's
it's a little bit hard to predict the
exact
load on those controller nodes so
because right now we're in the process
of migrating production customers from
all to the new system so this system is
not fully loaded yet but you're in load
testing we after we spend a couple of
months in the bidding our system we
gettin pretty confident about that set
up and this collector knows the layer of
collector nodes they provide basically
horizontal scalability level so we can
add any number of them as we grow they
don't talk to each other they are
bottlenecked only on some of the
external services and on the persistent
queue and also on cpu and memory if it
fills up so we all right so these are
the some of the services that those
controller processes need to call while
the process portions of collected logs
we update collection statistics and we
have kind of like commit event because
we need to our product is used for
security and compliance and we need to
collect logs reliably so we can at
lulu's logs and after we finish the
collection cycles we write special
indication about like the position at
which we finished collecting them and we
store it permanently so this is one of
the operations and of course we have we
have about 10 different services that
those collection controllers call like
configuration database and other stuff
some operational tools but we are we
know that in real life scenario we can
support about thousand requests going
between clients and collection
controllers per core so as I said where
we are optimizing for doing as a very
little work in the collection
controllers and we keep hitting
bottlenecks of course for the use case
that i described committing the
collection state we use react and that's
a purely key value type of operation so
we store it by key and we'll load it
back by key for example if a collection
control a collection controller node
gets restarted and it actually works
really well so if you know react and
what it can do in terms of key value
it's like the best no and the persistent
Q is also custom-built it's not react
react is used and the bunch of airlink
is used in all our other systems like
storage or flying data processing
interactive query and interactive
searches but I'm not going to be able to
talk about that today so react is used
in some other parts of the system but
this case is very very simple this use
case so and it's a it's interesting so
and we couldn't see that well while we
were like ranging you're in the system
we couldn't really answer that question
so what changes when you actually bring
a program that turns on their thousand
nodes and get to read and run it on a
like three or five well it turns out
like everything changes and and some of
the things are obvious like it's just a
lot more work in terms of requests trade
data volume / / your server / your node
number elope and connections of course
the number of processors per vm goes
back and also you get start seeing some
things that you don't see normally see
in not so loaded environment like your
failure rate and all those side effects
that hang off of it goes up and some
bugs get exposed race conditions
and even memory leaks that you would
otherwise wouldn't have noticed and
another kind of probe problem it's more
of a operational problem arises from the
fact that now all the customers and all
types of clients generating that that
have different load patterns they
basically connected to the same set of
nodes and that means a misbehaving
client can affect others you cannot
easily say what's going inside of your
airline vm because as I said the load is
not uniform like one host can generate a
gigabyte logs an hour and another one
can hardly generate any and they create
completely they have completely
different like performance footprint
it's a it's becoming harder to
troubleshoot things in like what's going
on during like processing of collection
response and with appliances running
once one or a couple of appliances
random per customer well if if if
something happens like your appliance
goes down or some network connectivity
issue comes up like other customers will
notice if anything happens to that
environment well everybody gets affected
so you need to account for that and it
wasn't and you kind of think about all
these problems in the beginning but then
it takes a lot more time than you think
to build systems to work with them and
then of course you need so in order to
deal with some of these problems you
need you absolutely need application
monitoring with several thousand
requests per second you can't get much
value from logs from looking at logs and
monitoring your server logs you need to
if something goes bad you need to react
fast
this and for this reason you need
interactive dashboards and monitoring
and you need to be able to visualize
your application behavior from different
angles and I'm going to show you some
examples and how it was very helpful
like it's super critical like you cannot
do anything without that we use graphite
and then the airlink library to offload
many many application metrics that we
collect to graphite and then plot them
so now in the as i said in the second
part of the talk i will go through the
list of problems that we actually
experienced during the load testing and
and explain what the problem was and the
resolution step and typical resolution
scenario and some of these problems are
I there for the most part they're going
to be specific to airline so the first
problem once we started loading the
system we implemented kind of ran
through a little bit of integration
testing making sure that the system more
or less works and then we turn on load
and we saw like pretty much very fast
death of the airline vm because it ran
out of memory and if you just to remind
you so those who collect collect
controller processes running an airline
vm they handle quite a bit of data so
data comes in they need to transform it
right it to persistent queue and this
means that they generate a lot of
garbage and it turns out in a loaded
environment airline garbage collector
which is / heap basically when they
generate garbage they generate it on
their own heap and also part of it in
the binary heap but most of the messages
that we collect we represented as Erlang
binary star lang binaries is a
essentially a mutable array of bytes
and basically what we've learned is that
by default garbage collection in a
loaded system does not may not keep up
with the data generation rate so you
create more garbage and don't collect it
fast enough and the solution our
solution to that was very simple so
after each interaction between the
client and the collector process we just
call garbage color we force garbage
collection on that controller process
and it like it immediately solved the
problem and we measured the overhead it
was yes so and we could have applied it
selectively but it worked so well so we
never slide one line change red we've
never gotten to like optimizing it we
measured the overhead on a not so loaded
system it was like within single digit
percent so we never had to do anything
about that and it's probably due to the
fact that we do so little try to do so
little when handling requests so we
don't generate a lot of garbage and most
however garbage as binary data that
comes from collected logs and it can be
apparently collected very easily so
there's heaps process heaps can be
rolled up very quickly that was easy so
the second one well that was actually
tricky and you need to know moreover
lankan to solve it so the problem was
that you turn on the load you wait for a
while you remove the load memory so you
turn on the load memory usage goes up
you remove the load memory usage goes
down but not all the way down which
means that during that time something
got stuck somewhere and as we're running
tens of thousands of clients it kind of
build up so you start with simple things
right you just try to garbage collect
all the processes in the system and
if you remember we already do that for
collection controllers but we have a
whole lot of other the airline processes
running like we have connection poles we
have caches and everything else to help
to speed up our application so you do
this it helps a little bit but doesn't
really solve the problem so the data is
still there and then you start typing
those crazy queries trying to find where
your sub binaries are does anyone not
know what a sub binary in Erlang means
okay so as I said there is a type
built-in type called binary which is an
immutable array of bytes and you can
take a slice of it basically a slice and
it's called the sub binary its
represented as a starting position and
the length of the slice and the slice is
treated this binary as well it's handled
by the language and the runtime
transparently you don't normally you
don't have to care about that and it's
much cheaper to pass it around the small
slice rather than do copying on larger
binaries for example like create more
objects and create more garbage and
airline offers a lot of troubleshooting
capabilities and that sensor link is
fantastic and I'll quickly go over what
this crazy things and those crazy things
you type in a airline shell remote shell
so you log into the node and you start
typing all those commands and it's not
that you type it all at once usually you
try to do simple things and then you
kind of it gets more and more
complicated because you're trying to
narrow down the problem and this
function goes over the list of processes
and it extracts information of binary
references by calling this process info
function and doing some actual work to
extract that information about that and
then it takes
it's got a sort it somewhere by takes a
sub list of 50 there's the sort so it it
fetches basically sorts the results
trying to find the top users of binary
top holders of binary references out of
all processes and that takes five five
of them here's list sub list so and as a
result you see things like that and for
this who are not familiar with our lank
that's a typical notation for
supervisors so you see that some binary
like 50 500 megabytes is sitting in some
supervisor supervisor is never meant to
like hold any data so it's absolutely
crazy and there's a popular open source
package called G proc that doesn't also
doesn't do much and yet it references
like significant volume of data so and
and you kind of keep thinking about that
and start and then start killing parts
of your supervisor tree because after
you kill this it didn't help like the
memory went down usage but it's still
like a higher than it used to be that it
was before the test then you start start
killing parts of your supervision tree
like parts of your application and
finally like you n so all the memory
goes back to when it was and it turns
out the process with the the problem was
related to sub binaries so one thing
that I didn't tell you is that we don't
use row HTTP when we're communicating
with between clients and the back end we
have an RPC layer there and RPC layer
allow us to basically tell clients to
execute some commands and then come back
and then they come back with responses
and we use a
Cole buffers it's a it's a popular
package popular serialization format
open source by Google Google protocol
buffers to represent our like RPC
protocol headers and the library which I
happen to be the author of when it
parses those HTTP responses RPC
responses it doesn't it creates a lot of
small sub binaries because it's a it's a
binary blob you try to turn it in a
structured airline representation and it
creates a lot of sub binaries and while
doing that it doesn't copy them and
parts of the sub binary is used as like
process identification so we use them in
process identifier is registered with G
proc parts of supervision tree child
specs child parameter start parameters
and that leads to they also ends up and
that leads to the sub binaries like
sitting all over the place in our
application and referencing fairly large
binaries that correspond to their
original blobs that we received from the
client and it turns out those sub
binaries can end up anywhere they can be
in purchase heaps then they can be in
ETS tables yes we use ETS tables heavily
for out of Prague cashing in in message
queues in Erlang processes communicate
by messaging sending messages
asynchronous messages to each other and
each process has a mailbox where
incoming messages can be queued up so
sub binaries can sit there and
references larger portion of data and
also it turns out they can be also in
driver cues we didn't see that these two
problems in our system but it's good to
know so keep that in mind when building
your system so if you have high traffic
application you may actually and some
slow like downstream
you can be bottleneck get bottlenecked
on memory really quickly so then the
solution to that problem is to call
binary copy this function was introduced
I think I believe in our 14 and Drix
works really well it can copy any binary
but also a sub Byner if you have a sub
binary it creates a takes it that slice
and creates a new object out of it now
for the more complicated terms you can
do this and and and it feels so if you
have like a complicated airline term
that you know hold some sub binaries you
can do this transformation this is an
native serialization basically convert
in any airline term to a binary and then
back and it actually really really fast
so you of course need to measure and it
also depends on the complexity of you
turn but this trick works so if you
don't if you have a complicated Erlang
term like nesting lots of nesting and
binary and sub binaries are actually
somewhere like leaves in that nested
airline terms and you don't want to like
iterate over that term and meticulously
copy sub binary fields it's it's doable
you can write a easy cycle it's probably
would be like 15 lines of code in there
like but I was lazy i did that and it
the end of processing of the the final
term it's actually so what we pass here
as input as a serialization dcl
deserialized protocol buffers so because
the library doesn't support copying some
binary is natively so we can we kind of
wrap d serialization in that
transformation and it works
automatically so that took us maybe a
couple of days to chase the the other
problem and this one is very typical
this one is very typical in and very
well known it's a frequently asked
questions so if you follow go ahead
oh it's very simple so when you create a
copy the larger binary that a sub binary
ref used to reference it it goes away it
gets immediately garbage collecting
because the sub binary is no longer
references it because it's actually now
a new binary it's a new object remember
sub binary is a reference to a binary
and binaries are reference counter the
reference counted yes that's right but
from our systems from our system
standpoint it's it's a leak I mean the
data is was licking somewhere it's not
strictly speaking a league because
they're like is a managed environment
and in theory you shouldn't get any like
memory leaks like I'm not talking about
that if you'd never go to drivers and
state your application stays fully in
their length that's our case then you
shouldn't have that problem you strictly
speaking you're right I was just
describing it from the point of like
behavior of our application it's it's
consuming more memory and actually in
the end after you know running it for a
while you will just run out of memory it
will consume all of it if you don't take
care of it so and that's one of the
things that I said if we had very few
clients running we wouldn't we wouldn't
have noticed that problem at all because
yeah there is a high chance that would
have ever noticed that because it
wouldn't be considerable how much time
do you have six minutes okay great so
this is actually a frequently asked
question if you if you use supervision
trees and you supervisor
the components called pattern called
supervisor there is a chance your system
will go down very quickly so basically
what you're using like your OTP textbook
that's great you expect it to work
reliably handle failures but it turns
out there's a very frequent pattern and
server and server environment for
example and I'll explain that how it
looks like basically you start getting a
supervisor has a parameter that
specifies the maximum frequency failure
of one of its child if one of its child
dies at a higher frequency keeps down at
a higher frequency and supervisory
starts it immediately so if it goes into
this restart loop any frequency that you
specify like reasonable like a thousand
deaths in the second can be exceeded for
example if you if you get in four
thousand incoming requests and you will
die less than a second and you specify
frequency like thousand deaths a second
you will die really quickly so and there
is a typical solution to that problem
unfortunately it's you need to do
something extra and and even if you know
about that I knew about that you kind of
can forget it some to do that in some
place and then your application will die
and of course if your top level
supervisor dies your application dies
and when your top level application as
you know you know it dies and then good
thing it will it is restarted by the
heartbeat but it's still it's it's not a
good thing because you lose all your
caches and everything so one solution is
supervisor to is a is a modified version
of OTP supervisor written by RabbitMQ
guys so you can just rip it out from
their implementation and start using it
heads a and it has an additional
parameter which is a delay and an
additional delay between restarts
that's as simple as that or you can
introduce delay in your gen server
function handlers explicitly like that
but it can get very tedious and you need
to remember to do it all the time sorry
you can you can add a startup delay
that's that would be another way to do
that but it would be great to have that
as a part of OTP so a very simple
problem that's very straightforward
solution not many people who start using
airline know that and you even
experienced users can forget and get
problems so this problem is really
really serious so we use genu DP so and
we hit that problem twice once with
logging we offloaded or server logs by
UDP using UDP from airline km and once
with the application matrix as i said we
use graphite and studs d so we also send
block and if you generate and the you
apply your typical you know OTP scenario
and what you do you create a gen server
so first you send your logs for example
or or application metrics to Jen serve
and then Jen server sends them over a
UDP and if your system is loaded enough
you will die so he is TCP and Jen server
it's and the same problem exists with
jen tcp on tcp as well so you will hit
that with jen tcp and it looks like this
so request rate goes up i'm sorry you
can't see that but this is request rate
going up and all of sudden it drops so
no requests coming out at all this actor
actually response rate not request rate
request may still come in but we measure
response rate and it just stops and when
you then you look at from the other
angle at what's going on in your
application and you see that in the
error logger q stays it about like over
15,000 tell em
in there which means it cannot afloat
all that logs fast enough over UDP and
you think what the hell is going on UDP
is meant to be like really fast they're
like there is little shouldn't be any
bottlenecks there well it turns out and
it comes down to how things are
implemented in our lank so Jen server
that sense logs over UDP has a incoming
message queue and so basically logs keep
coming and then it tries to send it
asynchronously over UDP but that the
asynchronous call has a selective
receive underneath and that's the this
is a selective receive and normally it's
a linear operation linear to the number
of messages that sitting in your mailbox
in income in message queue so it
actually goes over each element of the
message queue until it finds a match in
message and it does that on every send
attempt and the longer your message
queue becomes the longer this operation
takes and eventually you get to the
point that you just completely blocked
and the reason that why the whole system
stalls is because every single process
sends log messages and every single
process gets blocked by sending message
to another two years on server which
just cannot do like anything and airline
vm has a property that the sending
processes get penalized and get
preempted and not scheduled if they
attempt to send a message to a process
with a really long message queue so your
whole system stops and what you see on
your cpu usage graph is that there's
only one core that's that that's
basically airline vm consumes only one
core and nothing else happening that's
that was really funny and i said we hit
that problem twice so the solution is
one solution is today's gen server to
that's again written by RabbitMQ guys
and it helps to end it basically what it
does it drains your gen server message
queue as soon as it receives a message
it just puts in a man into a like
separate cube managed by itself that's a
simple solution the but we ended up
doing this so we ended up removing that
synchronous call and removing that
internal receive statement and basically
instead of going through that receive
cycle we ignore the response from the
driver from the UDP driver and just keep
sending them commands but that's not
trivial right so and I didn't know about
this solution as well so but fortunately
one of our colleagues one of my
colleagues did and of course you can fix
jen-jen UDP so there was a fix that
turns the linear look up into a constant
time lookup when you do a selective
receive and it's also it's already fixed
in gen server gen server call but I
don't see any reason like off the top of
my head why it cannot be sold for Jen
TCP and Jen UDP so that users don't have
to go through this headache so last
problem client-side latencies and
remember i told you in the beginning
that our lank is really nice because you
don't have to limit concurrency in your
system by default and it's true with one
caveat and it is when your system
becomes overloaded and bottlenecked on
cpu and that can happen in our system
because we do some data processing we
process we can come in logs and stuff
when things get really tight in your
real-time environment with lots of
process is trying to do things
concurrently your communication with
other services you start experiencing
client-side really high client-side
Layton sees when communicating to other
services in this example
Union to look here the blue line and you
can't you can barely see that it's the
server-side latency but this is
client-side latency and you can see it
on this graph but request rate was
really high at this time like four
thousand requests per second and the
airline's node was consuming all cpu and
what happens and we use HTTP client and
not a standard one that just doesn't
work at all so but still so what happens
is that that and they're like airline
tries really hard to give software time
you know her characteristic and means it
starts pre-empting your HTTP client and
it keeps like doing that and delay in
schedule in your HTTP client and maybe
some asynchronous stuff inside the
Erlang vm for for like you get terrible
toys you can see it here but this is 100
milliseconds so the server actually
responds in two milliseconds but the
client all in all gets response back and
100 milliseconds and that's a 50-percent
percentile which is just crazy so that's
this is the same graph at nineteen
percent percentile and this is
ninety-nine percent Thailand this is
maximum and it's interesting that when
you get to measuring maximum latencies
those two grav server side latencies and
client-side they converge I cannot
really explain that but so that's one
things to watch out so when people try
to think like whether we should limit
concurrency like by implementing workout
pool worker pools you should consider
that so all in all like airline has been
a great tool for us for alert logic and
for this project in particular and it's
been very efficient provided that you
know this tools and some of the things
that I told you about they're well known
but not so well known and not so well
explained enough especially when you're
lying users so it's a blank I'll go
quickly over this list
so we have thousands of concurrent
processes running and overall we're
doing fine with that we rank is a simple
and very powerful language that helps
you to focus on your logic instead of
like fighting with some low-level stuff
it offers really a good fault tolerance
model if you do it correctly it actually
works so process crashes is something
that you need to look at but you can
safely like ignore it won't like wake
you up in the middle of the night the
runtime and the vm is really really
stable like you can one hundred percent
rely on that it's been around for over a
decade tophers reach troubleshooting
capabilities you can log into remote
shell and run some commands on a
production node for troubleshooting and
stuff you can also use tracing which is
also nice and if you are smart and you
know what you're doing you can even
write run profiling other environments
also for that capabilities but Erlang
does as well and the the community is
growing some of its it's constantly
getting better one meaningful one thing
I want to mention is rebar so we use
rebar rebar releases and we even started
using rebar for other stuff notnot
airline for tracking non airline
dependencies and finally there's also
room for for improvement and I have a
dream after using their line for five
years so I know how to deal with this
problems that I talked about some of
them were new for me but we solved all
of them really quickly so and the dream
has a name of open server platform as
opposed to open telecom platform so some
of the things that I talked about it
wouldn't have happened if they were
addressed either as a part of OTP
principles or slightly modified OTP
principles more specific better suited
for server environments packaging and
deployment like rebar packages work
however there's some legacy stuff
like real tool config it's it's just
it's a mouse like ideally you don't want
to like touch it for you want to
standardize on some parameters and never
see it again so there's some legacy
tools that made a lot of sense in the
90s for embedded environment and telecom
it doesn't make any sense and server my
logging it's getting better there are
some good open source packages but
especially for new users it's it's a
mess so the ideal setup would be an
option to looks log to syslog or
standard they're never log to files and
you need to include sasal logs properly
formatted that don't blow up your vm by
default so HTTP client that works the
standard HTTP clients never use it it's
buggy and it's slow and we ended up
using el HTTP see but it's not really
good as well so we'll be migrating off
of that as well so it's actually I don't
know any HTTP climb that may work for us
and help to bring this Layton sees now
hmm I have not I looked at it I think
they're pretty much all the same it
comes down that their pool a connection
polls are implemented in efficiently and
there's a philosophic questions how many
like cores and how many load do you need
in terms of like trying to make mini
requests before your connection pool
implementation becomes a bottleneck so
the more kind of things that ball neck
on that single connection pull that so
it's a it's it's another complete story
but you need to watch out for those
bottlenecks and try not to overload your
connection pools or implement them in a
more efficient way so this is a this
would be really nice i don't know it
like this is totally achievable right
this one I don't know somebody just
needs to care about that so every time I
it's there is a fork hell there's an
in-house maintained like Forks for those
day
a basis but it's not really manageable
it's somebody needs to just fix that and
this is a stretch goal of course we
don't use distributed Erlang and we very
consciously not use it don't use it
because there are so many bottlenecks
associated and the trickiness associated
with that like we avoided half of the
boundary problems boundary the Cleveland
gave a talk yesterday we didn't have
half of these problems because we don't
use distributed Erlang we don't we have
a natural flow control across our system
and to end so we don't have to limit
anything and so distributed airline case
the default one I think we have
applications for that we use it in other
parts of the system but we decided not
to use it here and structure our
application differently so that we don't
have to use it but it would be really
nice to have reliable cluster management
leader left and distributed login and
things like that without dealing with
some of the limitations of the existent
implementation that's that's all I had
any questions do we have time for a
question
so you should have listened it's it's
not bad but you should have listened
about lots of limitation that it has you
can go and watch the recording of
Cleveland's talk yesterday's and he and
there were also a bunch of other
speakers like from russ last year
Cloudant Adam chokoloskee I think his
name he also give they use a distributed
airline big time and they hit a lot of
limitations with it and he spoke I'm not
very like I'm not the best person to
answer that I just I know some things
and I know some other things that make
me not use it like if if possible</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>