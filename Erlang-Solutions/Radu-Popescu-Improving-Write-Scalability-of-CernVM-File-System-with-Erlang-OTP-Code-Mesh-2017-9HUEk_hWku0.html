<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Radu Popescu - Improving Write Scalability of CernVM File System with Erlang OTP - Code Mesh 2017 | Coder Coacher - Coaching Coders</title><meta content="Radu Popescu - Improving Write Scalability of CernVM File System with Erlang OTP - Code Mesh 2017 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Radu Popescu - Improving Write Scalability of CernVM File System with Erlang OTP - Code Mesh 2017</b></h2><h5 class="post__date">2017-12-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/9HUEk_hWku0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">really happy to be here first I would
like to thank all the organizers of the
conference we're having a really good
time they've been doing a really good
job so I'm Radu
I work at the European Organization for
particle physics research also called
the CERN and which is located outside of
Geneva Switzerland
and it's the home of the Large Hadron
Collider which as you know is the
particle accelerator that helped confirm
the existence of the Higgs boson
so this accelerator is indicated here in
the image by the yellow circle and just
to give you a sense of scale you could
almost fit the entire city of Geneva
inside the circle so Geneva is this
basically here with a bit of a few
neighborhoods here on the other side so
you can see you can actually put put
them inside so this accelerator is 27
kilometers long and inside it looks like
this this tunnel which is about a
hundred meters underground and the power
used is about a hundred and eighty
megawatts which is actually equivalent
to all the households in Geneva combined
so definitely where we're waking up the
biggest power bill there and the power
the energy developed by the accelerator
is about seven tera electron volts so an
analogy is let's say this accelerate
each particle to the energy of a
mosquito buzzing around but putting all
the particles together in a coherent
stream gives you the energy of a
speeding train crashing forward so at
four different points in the along the
length of the accelerator are the houses
of four main experiments doing physics
on the Large Hadron Collider and these
are independent entities which are there
looking at different questions such as
why is it that we can only account for
5% of
the mass in the universe or how did the
Big Bang happen and for instance this
experiment is called Alice this is CMS
if I'm not mistaken this is Atlas and
this is their HC B these two are the
largest experiments and they're actually
involved in more general physics
research and they were the ones that
were able to confirm the existence of
the Higgs boson Alice and htb are doing
more specialized physics and
unfortunately I'm not a physicist so I
cannot go into more detail here but you
should look into it if you're interested
and at the center of each one of these
detectors you can find you can find a
silicon tracker so this is a device like
this big and actually it's it's more or
less equivalent in function to digital
camera sensor that you have in your in
your smartphone's and this one is the
inner barrel of the CMS experiment so if
you're familiar with this picture or it
seems familiar with you it's maybe
because you have this CD in your
collection so good good work I can I can
warm you recommend it but now as I said
so these are these are the more or less
digital camera sensors there are about a
hundred million mega pixels in
resolution but they they have to be able
to trigger once every 25 nanoseconds so
they're much more complicated than
performant and the data rate we are
talking about is so high that you
actually need specialized hardware
around these things to be able to
capture it and actually do a triage and
and a filtering there on the spot of the
acquisition to bring it down to a
manageable size that can be stored so at
this point we are storing about five
petabytes of data per year which is
including the various derived data set
but in the next phases of the LHC
experiment which are expected to happen
by 2025 we are looking at 100 petabytes
of data per year per year which is a
20-fold increase
this is data then on the on the software
stacks which which is actually the focus
of of my project things are not so so so
critical we're just looking at five
million lines of code per experiment now
also the computing infrastructure is
quite particular at the CERN since not
everything is located at the Geneva
Geneva site you have a main data center
there but the CERN is also a
collaboration of around 170 compute
centers in 42 countries which are
pooling together that their computer
resources to be used for analysis and
simulations so this makes the the
worldwide LHC compute grid which is
running about two million jobs every day
and this is a this is a plugin for
google maps that is it is available it
shows the the like the real-time usage
of the grid and when I took this
screenshot there were around 290 mm
running jobs over four hundred and sixty
thousand cores which amounted to around
17 gigabytes of transfer per data
transfer per second so this is this is
the computer architecture we're looking
at on the side of the LHC software
stacks we have hundreds of developers
working on these projects and at this
point we have something like a hundred
million binaries that have to be stored
and distributed so we are producing
around one terabyte new per day of stuff
to software so it is it has a high high
rate of change and of course it's the
daily production releases that have to
be stored and used on a longer term
while the nightly builds they are pruned
on a regular basis still all the
software has to make its way to the
almost like a hundred thousand machines
that make the the worldwide compute grid
so it's a it's a problem to think about
and to solve so this gives you a bit of
context about the certain VM file system
so the origin of this this file system
is actually the the so called certain VM
project and this was an idea
to create a reconfigurable vm appliance
so you had the certain vm that could be
used on a in a certain configuration on
a computer on the grid or it can could
also be configured for interactive use
by physicists on on their laptops but
this this project in its initial
iteration was not very practical because
of the large size of each one of these
variations or each one of these
configurations of vm and then also to
the time spent actually creating all
these configurations so for desk
we created the turn vm file system which
meant that the the base size of this
appliance was 20 megabytes was just the
minimum amount of files and stuff needed
to actually boot it and and bootstrap it
and then all the contents of the even of
the root partition are loaded through
this file system so this file system is
is actually a module for file system in
the user space I don't know if you're
familiar with this technology you have
various implementations such as SH FS
and stuff like this so it's a pass to
main user space that gives you a POSIX
file system interface access to various
remote repositories of software and
files which are with our standard HTTP
servers or or s3 instances and these
these repositories are made available
through a global cache hierarchy HTTP
cache arched hierarchy for performance
so one thing that is very important to
note is that the system is lazy so when
you mount one of these file systems all
that happens that you have to download a
bit of metadata to see basically the
list of files currently available but
the content of the files will only be
downloaded on demand when needed so
these things makes things very very
efficient now it's not a general-purpose
file system it's a it's a system where
clients have a read
view well special designated writers
have the only right access to the
repository so in a way it's a
publish/subscribe pattern and HTTP is
used as a transport exclusively which
simplifies the interaction of it with
with firewalls and many things so the
main components of this file system are
the client fuse module which is
complemented by complemented by
complemented by different cache plugins
so these are these are plugins that can
actually improve the performance for
specific use cases so you can imagine
that you're using it on a supercomputer
where you have for instance many nodes
that do not have local storage but they
are connected together by a very very
fast network so there you can imagine
some sort of peer-to-peer mesh plug-in
that that could actually make
performance much better for that case so
you have this flexibility and you have a
standard set of default plugins that
that come with the software then the
server tools which are supposed to run
on this designated writer machine these
are command line tools and for serving
the files we're using standard HTTP
server so you it works with apache or
nginx and all these things HTTP caches
is designed to use squid so we don't
have to maintain these components and we
use industry standard well well-tested
tools so about the design of the file
system probably the most important
design choice is the use of immutable
content addressed storage blobs so all
files are are transformed into one or
more of these content address blobs and
things like the deduplication are coming
for free basically from this
implementation
we're also compressing all the blobs to
store things more efficiently and then
for the metadata we use SQLite catalog
to store to store all the metadata of
the
repository so this is these are
basically tables which are tying the the
content addressed storage blocks to
actual file names in the repositories
and all the extra POSIX metadata so file
size permission and all these things
here you should note that we actually
gave a lot of attention and a lot of
effort to the POSIX compliance of of the
file system so this system is supposed
to run software so on the grid
repositories are mounted and all the
software is run from the certain VMFS
mount points so of course in this use
case you're gonna be hitting many more
corner cases then if you're just storing
data so put put get so actually a lot of
effort went into making this as POSIX
compliant as possible now these metadata
catalogs you can see them basically as a
miracle tree encoding of the state
entire state of your repository at a
given point in time and of course things
like versioning and snapshots are also
easy to implement and are available in
in this file system and everything is
poor based and this is important so
there will be no communication initiated
from the server from the centralized
servers towards the client everything
starts in the client the client requests
a file it gets a copy of it it's cached
it has a time to live and this means
this system works as well for instance
on on a supercomputer or on a grid as on
your laptop in hotel Wi-Fi because you
don't you don't need to a special
configuration for the firewall to to be
able to access the repositories ok so I
mentioned that this is not a
general-purpose file system but updating
these repos follows a publish/subscribe
pattern so to publish you have this this
writer entity let's say implemented as a
set of command-line tools
and they're running on a special
designated machine which actually has
write access direct write access to the
storage of your repository and this can
be either a local mount or s3 and this
writer is able to construct a read/write
view of your repository Pastore with a
union mouth such as overlay FS or au FS
and these tools are doing the job of
compressing hashing the files and
writing them to this repository storage
then after this step is done new
metadata catalogs are constructed and
published and finally the the repository
manifest which is the little text file
with that displays the current revision
number the current route hash all these
things
this is swapped atomically at the end
and it's at this point only that the new
state of a repository becomes available
to all the clients so the this
summarizes the existing workflow you
have a centralized release manager
machine with which you interact directly
so you have to log into that machine and
then run a few steps and you have
published to the repository so this
makes it quite straightforward to use
it's good for scripting and it somewhat
hides the fact that you're dealing with
multiple machines with the distributed
system but as you saw you're taking
these locks exclusive locks on the
entire repository so there's no support
for concurrent writing into it with this
scheme and also since you are basically
expecting somebody to log into this
machine which actually has the
repository storage mounted this can be
unsafe first-first certain installations
it depends on your on the clients
installation and then performance for
the publication is enough for most cases
but we do already have some clients
which have some some more specific need
- to publish more files or a larger
payload and we're also looking at more
use cases for this system so we would
also like to improve the performance a
bit and be able to to let's say scale
out to multiple release manager machines
per repository some properties and
constraints under which we are working
so considering the system as the
repository and the cache and the clients
it's an eventually consistent system
different clients can can find
themselves at the same time on different
revisions once the publication stops
they will eventually converge to the
same consistent view of the repository
and then from point of view of
concurrency there's a lot of concurrency
to exploit here so the immutability of
the content addressed storage blocks
means that you never have a case where
more than one entity actually is writing
to to these blocks the and also pushing
these objects into the storage is
idempotent then the directory tree
structure of our of our data makes it
easy to identify parallel branches where
you should be able to operate almost
completely independent and of course
there is a critical section in the
publication process which involves
updating the metadata catalogs and
swapping the manifest but luckily this
part represents a small fraction of the
of the publication process so this is in
our advantage if we want to do to
paralyze this so the the the starting
point can be can be described as this
you have the user machine and the user
will connect to the tow machine serving
as release manager over SSH there you
have the client and the server tools
installed you're making the publication
and this machine is able to access the
authoritative storage of the
repositories to NFS or s3 and this
repository is then replicated and made
available to clients and where we went
is here
so here you have multiple users each
accessing their own their own instance
of the release manager machine so here
basically we improved the architecture
by splitting the release manager machine
and to multiple machines with different
roles so in this new architecture the
release manager still does the so-called
heavy lifting so compressing compressing
hashing the files and then putting
together payloads to send to the to the
storage gateway and this storage gateway
is the new component it's responsibility
is receiving these payloads and packing
them of course check checking that
everything is alright it impacts them
writes them to storage and it also
rebuilds the metadata catalogs as needed
now the two part are communicating
through a well-defined service API it's
good this is currently an HTTP API and
in this new architecture as I said
multiple release managers can be writing
to the repositories concurrently but to
maintain things consistent the gateway
has an extra responsibility which is
actually handing out exclusive leases to
different release manager machines only
two distinct sub parts of the repository
so the same directory is not modified at
the same time by to release managers and
this of course simplifies things greatly
so we maintain the same workflow but now
we add an extra parameter to the to the
CBM FS server transaction command we
specify which surpassed we want to
acquire lease for so things are still
familiar for for the client so this new
storage gateway component as I said it
serves as a distributed lock manager for
our system checks the rights of
different clients to modify the
repositories and is responsible for
assigning exclusive leases to client and
also it does the work of receiving the
so called object packs which are our our
archives with with multiple files
and multiple content and rest blobs and
write them to authoritative storage and
for implementing this new component so
just as a parent Isis everything up to
this point is implemented in C++ so the
the the client and the server tools are
C++ but for this new new component we
decided to go with with Erlang and an
OTP service distributed group or for for
this part of the system and we went for
this choice because we knew that the
language and the framework have
excellent support for concurrency and
also for for distributed applications so
we were attracted by the actor model of
concurrency and also the different
properties of the language such such as
working with immutable values and
functional constructs the unique error
handling approach of Erlang and also the
fact that you're dealing with with let's
a battle-tested proven technology so you
can can benefit from all this all this
experience and of course since most of
our software is C++ you have good ways
of interacting with C++ in Erlang and
this was important to us so this
basically describes the the architecture
of this gateway we are you start with an
HTTP front-end which we implemented with
the cowboy HTTP server it spawns
different handler processes that hit our
our our back-end so here the back-end of
leaves and receiver are all all o TP gen
servers that implement their own own
part of the functionality of the
business logic the backend acts as a
multiplayer multiplexer forwarding these
requests to different subcomponents we
went with amnesia to persist the the
session data and this was very
convenient because in any choice of
database that comes with OTP so you
don't need to install anything else
and also it has nice support for for
distributing for a distributed
configuration or replicated
configuration and this could be of
interest for us in the future if we want
to move to a high availability
configuration for the for the gateway
and then our existing code base is used
through these worker processes which are
C++ and they are connected to the Erlang
application through the ports driver
interface so basically there are sub
processes that are you have they have a
set of pipes connecting to and from the
Erlang application and since there
they're supposed to be doing the more
computationally heavy or more intensive
parts they are usually run one per core
and you have a pool boy worker pool that
that manages access to them so since we
are C++ developers mostly this was a new
experience for us and there were some
some really good good parts about using
Erlang so OTP is is an excellent
value-added it's really difficult to
understand under state so having these
reusable and tested components really
allows you to easy easily build a larger
application that is that is already
coming from C++ it's really great to see
this focus on on tracing and the live
inspection of a system so if if you're
used to gdb as the like the state of the
art of inspecting the state of a system
opening up observer where you see the
entire process tree in real time is
something else entirely
so the mutability and functional
constructs of the language really helped
when it comes to the to the debugging so
there's much less time spent asking who
who did what where why how so it's much
simpler to understand what's what's
what's happening and then the approach
to concurrent
concurrency is really makes our line
feel like like a domain-specific
language for concurrency it really makes
it makes things simple you know good
tools for automated testing dialyzer is
a tool that really is is actually is
essential so Erlang is dynamically is in
a dynamically typed language and
dialyzer allows you to specify type
annotations for your functions and then
does the so-called success typing so
checks that that things are logically
composed together and this is this is a
very important safety net when you're
developing the application and of course
you can you can interact with C++ code
either by writing functions implementing
functions in c++ that are then linked in
or as i said with these port drivers
where you have sub processes and i think
for certain case you can even implement
entire nodes in c++ or other languages
and they will expose the the node API
the Erlang know the API and of course
like not everything is great there are a
few things that are that could be
improved so dynamic typing is strange if
you're coming from C++ of course this is
a subjective thing but in my experience
developing things in a dynamically typed
language I feel less confident when it
comes to refactor things then if I if I
have a more advanced and more more more
strict compiler so what this means is
that I really have to be thorough write
these these type annotations for
dialyzer to help me out and to ensure
that I'm doing things correctly and also
be disciplined and maintain these these
annotations as I go forward then
deciphering our language can be quite
confusing at the beginning then you get
used to it but there are things like
logger which can make at least error
logging much much much simpler and
it's very easy to use then OTP has a lot
of functionality but it also has quite
large api's and there are many
parameters and switches that are very
easy to miss for a certain time outs
that can then then you have you have
various processes dying on you while
while you're testing and developing and
you don't know what's happening and it
could be to a hidden time out so so
definitely you need a you need a book or
a reference if you're new to the
language while you're developing but you
know the overall impression is very
positive so we would definitely use
Erlang for new use cases in our
architecture as we go forward and I
think one thing we are looking forward
to is getting more operational
experience because pretty soon these
these developments will will will make
it into a release a public release right
now they are available but in the master
branch and then when it's time to deploy
these things on a larger scale more
interesting advance for sure well that
will happen on the operations part and I
just like to mention an Associated
project so we actually have a darker
graph driver plug-in based on certain
VMFS so and you know if you use darker
you know that the darker images are are
composed of multiple image layers which
are superimposed and when you're doing
docker run for instance you have to wait
for all the all the layers to be
downloaded until you can actually use
the the container and doesn't matter
what you're actually planning to use out
of all these layers they have to be
completely downloaded before going
forward and we implemented support for
actually storing the contents of these
images in certain VMFS repositories so
what this means is when you do docker
run this will complete instantly because
all it has to do is mount these
repositories and then the files will be
downloaded lazily on-demand as they are
needed so we add the CERN we're using
this already in the in the CI and
relation where we are using a lot of
docker images there and this has really
improved the quality of life in this
part of the code so it's it's also
available on github give it a try
also the the CERN VM in certain VMFS
projects are turning ten next year and
we are organizing a certain vm workshop
at this turn at the end of January this
will be users and developers of certain
VM and different other project projects
which will be giving talks and it's open
to anyone so you are in normally invited
to join us like to give a shout out to
my team
Jakob bloomer actually certain VMFS
arose from his PhD thesis there's a
jargon is our team leader I thought
he'll out and Nicola Hardy Nicola Hardy
is actually the the technical student
who worked on the docker graph driver
plugin so I thank you for your attention
there these are the links on how to get
the software and documentation and if
you have questions I'd be more than
happy to answer POSIX speed right I mean
it's simple it's it they're not you're
not doing multiplexing there you're also
because you have multiple instances of
these of these workers and you can do
this higher up actually it's so it's
read and write and actually we're using
JSON 2 for the messages and then after
the JSON message if there's there's a
lot of data to transfer that a separate
binary yep can you elaborate a bit on
the lease
fencing mechanism that you use to
protect certain subsets of the
filesystem and why you did not choose to
give each of your gateways a distinct
subset of the filesystem in the first
place so to speak so that you divide
your your namespace equally that's a
good point and we didn't think of it
that way and the leading how is done is
basically if you're if you're asking for
a lease on a sub path it looks if
there's a common prefix in the in the
path and a if there's no conflict the
lease is given and there's a time to
live for for each lease so it's in fact
it's not very complicated and if you
look at it at the catalogs which
basically have to have to be rebuilt at
the end of each publication together
with this merge operation that happens
they're actually forming some sort of
conflict-free replicated data type
because you always have a summation of
files of the changes that happens and
can happen in any in any order also you
don't you don't care
so the this reconciliation process is
very very simple there's not because we
can limit the use they use a usage of
this I mean we we say we do not allow
release managers to write in the same
place and working under under this
condition simplifies our work and is
enough for the use case that we are
looking at yes do we have any more
questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>