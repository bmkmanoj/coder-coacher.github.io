<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Lambda Days 2015 - Kevin Hammond - Megacore, Megafast, Megacool | Coder Coacher - Coaching Coders</title><meta content="Lambda Days 2015 - Kevin Hammond - Megacore, Megafast, Megacool - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Lambda Days 2015 - Kevin Hammond - Megacore, Megafast, Megacool</b></h2><h5 class="post__date">2015-03-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/gLXrHRFa-wY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so it's great to be in Cracow again
thank you for the invitation and the
work I'm going to tell you about is
follows on from Garrett's talk in some
way so it's very excited to see Gerrit
talking this morning about patterns and
bouncing squirrels I think what I'm
saying this talk is going to follow on a
bit from that sadly I don't have any
bouncing squirrels to show you that's
fine if you want to know more about our
research please go to our twitter feed
here are my details I pass them on at
the end of the talk as well and there
are web pages where you can learn more
about the work we're doing on the
project so the work here is based on two
European projects one called paraphrase
that involves various partners from agh
and other places are throughout Europe
we have 13 partners are from age
countries and what we're looking at is
parallel patterns for hash genius
multi-core systems so I say garance talk
was appropriate I really meant it and I
am coordinating this project for my
sense it comes to an end are in about
two months time also a new project
rephrase and this is a slightly
different it's refactoring parallel have
seen yet software and what we're
proposing to do is look at all software
engineering issues are the people have
said are important but which has
basically been neglected I'm explain to
you a bit are what i mean by refactoring
chris brown this afternoon will give you
some technical details about the
refactoring process that we're using we
think it's very powerful mechanism in
combination are with pattern based
approaches to dealing with parallel
software i'll show you our bats later
very pleased to have taken functional
programming out into the camp of the
enemy in the form of sorry i shouldn't
say enemy and friends like IBM and other
large software companies i'm i'm afraid
that we're looking at c++ in this
project that's pragmatic i'll take a bit
more about that later on there's a
reason for doing this but bear in mind
that i am a haskell guy so whatever I
have to do
whatever I'm doing ultimately has to do
with functional programming and we're
working with a lot of industry so here
are a few of our industrial connections
so this is very hot from an industrial
perspective so let me give you some
basic motivation well multi-core is now
ubiquitous even this little device that
I'm presenting on at the front here is a
multi core system how many calls does my
laptop have or the audience question 24
h 16 oh right sorry seven okay I like
your thinking 4242 let me tell you yeah
so it's got to x86 CPU cores I prayed
Apple don't put quad cores into these
devices but maybe that's really for
because we have have hyper threading
okay so it's sort of four or maybe two
depending how you count them all I have
12 GPUs nobody thought about the GPUs so
I have 12 GPU execution units these are
all cause they're very specialized their
specialized cause but they are
definitely cause i have two HD video
encoders okay these i think i made by
all their calls their presence of course
they only do one thing very specialized
very dedicated i have a bluetooth
controller if you look inside that
you'll discover there's a presser which
is just dedicated are dealing with
bluetooth I have a disk controller for
my SSD I have a power management unit
which is more powerful than the very
first computer that I programmed in all
it's doing is figuring out should we
turn on or off this device and so on and
so forth it's all together this little
device sitting down here has about 20
calls in it of various types okay this
trend is going to increase so today you
can buy one of these devices an intel
xeon phi this was released in 2013 it
has 60 cause um you'll notice that
they're not running 60 x86 cores sadly
they're only running at about 1.2
gigahertz
one of the big trades at the moment is
between press the speed and the number
of course and with the Xeon Phi until
have gone down d let's have lots of
course but they wrote we won't run very
fast then we'll know why temperature how
to synchronize ation train cause power
consumption yeah these are all good
answers these are all good answers
fundamentally I think the issue is power
or energy you'll see down here I put a
little number 300 watts okay now that's
quite a lot of power quite a lot of
energy to draw if I had that in my
laptop I wouldn't go very far with it I
certainly wouldn't have it on my lap I
can tell you far too power hungry party
energy intensive if these things were
running at three gigahertz it wouldn't
be consuming 300 watts it will be
consuming 1.2 kilowatts maybe two
kilowatts of energy now that is
basically the same as a heating system
but I definitely don't want that to be
carrying around as a portable device so
energy fundamentally limits the speed
which caused to run lower speed chips I
can run at lower energy consumption this
is the future the future is keeping the
clock speed down whilst increasing
probably the number of course it's all
out future well what I confidently
predict based on drawing the strength
line between my two core laptop and the
six core xeon phi is that in a very few
number of years we will have devices
which have hundreds of thousands or
millions of course probably very low
power and i'm calling this mega call
does anyone here have Wikipedia editing
rights good could you credit me with
that please
I want my don't go down in history so
what do mega call systems look like well
they're going to be knows the Lincoln
system each node is going to perhaps
have several large CPU cores plus
specialist many core accelerators things
like the Xeon finds a highly
heterogenous press the structure some of
which we can access like the GPUs CPUs
on this device some which is going to be
dedicated that we weren't really able to
program very easily but they'll all be
capabilities that we have available to
us and high-performance network link
nodes probably not much memory per core
and XL systems in particular who's
really large-scale systems are going to
be highly abstemious now that's very
exciting but I'm now showing you is
going to be incredibly frightening
particularly if you're not functional
program because these devices will
probably not have shared memory okay
remember those big arrays that we used
to have well they're not going to work
very well on this class and machine be a
myth that we have access uniformly at
the same speed are two memory across a
high-performance computing system
probably even a low performance
computing system is just going to go
away so anyone who is today thinking
well the best way to program is by
setting up a huge great array and using
shared memory to access that I'm afraid
is going to come up against brick wall
very very shortly let me show you this
this is the fastest computer in the
world is anyone know what this is no
sorry dude you now know there's good
quite good guess ah yes
this is the tm2 and by the way that's
changes so from day to day what is
actually the fastest does change but
when I googled it this was what is the
fastest so tyann to from chinese
national university of defense
technology ah any idea what they're
doing with that no don't don't tell me
33.86 petaflops per second so in the
high performance well people think in
terms of floating-point operations per
second 33.86 petaflops what's more
interesting to me is that it has 16,000
knows each limb has to 16 core ivybridge
multi-port and three of those xeon finds
that i showed you it's a very common
pattern if you do the numbers if you do
math as the Americans say are you work
out which has 3.3 million 120,000 x86
cores in total ok so megacorp is not
that far-fetched okay today if you've
got a machine like this you need to be
the Chinese government and you need to
build small nuclear power station
perhaps to power it in the future I
confidently project but this kind of
technology probably not quite on that
scale will be coming to a laptop or
desktop near you and exascale computers
take this out from these kind of figures
so what people projecting is going to be
the future in high-performance computing
is that we may be 33 times faster than
what I've just shown you and that's what
people are confidently predicting will
be in place by about twenty twenty about
maybe 20 25 so 33 times the power of a
machine like the monitor showed you
available to do whatever you want really
if you can afford it at the energy
budget so mega cool I think or maybe
mega hot it's a better word you guys
quite cheap yeah that's right so Reba
see we sing an onslaught from both
directions at the top end you're saying
these really really powerful machines
lots and lots of calls and bottom end
you're saying things like relatively
pine other cheap commodity presses i'll
show you want two of those essentially
using same technology so at the low end
what were you saying is multiple cores
used to keep the energy usage down and
in the best cases are this can be maybe
less than five watts of power okay
that's very exciting one of my
colleagues also from Poland is working
on devices which are printed onto paper
wait so you print the circuitry onto the
paper are you print a battery onto the
paper this is solar-powered these
devices run for a few years on that
battery charge before you have to
reprint them that's seriously cool so
energy usage scales linearly with a
number of cores so more calls you add
are the more power you get pretty
obvious but as you increase the clock
frequency so the power usage scared
squares with cube of the clock frequency
and this is really what's driving the
move towards are more cores are but
lower speed so his of graph just to show
that and if you carry this out ok this
was Donnie I think two thousand and
something what they predicted was well
by 2006 you'd be basically have the
energy density of a nuclear reactor in
your laptop and by 2010 or thereabouts
we'd be having the energy density of a
rocket model in a laptop if we just kept
scaling the energy usage and that's why
we've got more to go okay luckily I
don't have a rocket in this fuel cells
great it's not in my laptop so it's not
just about lower assistance even mobile
phones are multi-core even devices like
the one I've got here a multi-core and
and in particular the Samsung Samsung
Exynos 5 option this has eight cores and
file at a point in time for from
dark they're not powered this another
trench will have lots of capability but
we just don't think you have to fall to
power it up all the time we gonna have
to be selective so we need to use calls
when we need the boost and not all the
time if we don't solve this challenge
then we're just not going to be able to
keep on making progress in computer
science so it's problem everyone every
program at every developing here is
going to have to face at some point
probably pretty soon if you don't start
to think multi-core and then beyond that
many call mega call then your software
is just going to stop working it's going
to hit brick wall there's only so far
that the whole point architects can
stretch existing technology and that
point I'm afraid looks like it's going
to happen pretty soon from all the
current trends so all future programming
is going to be parallel so this is the
future guys it's White's precise so
welcome to the future here's another
little advice just show you are what's
happening are at the moment ah this is
something called it's an NVIDIA Tegra k1
just been announced I it has for fast
cause big.little architecture once lower
low power a 15 Corps Base you can switch
between them so you can go between the
high power ones or the lower power ones
as unique so as with the Samsung you can
use the for low power devices when you
don't need much energy no you're reading
email you're doing a spreadsheet
whatever and then when you need the
performance boost when you're playing
angry birds something like that then you
can switch for higher power higher
energy devices brittle period of time
it's 192 core Kepler GPU two gigs of ram
and this is an exciting development this
is shared between the CPU and the GPU
one big problems at the moment is if we
have code running on the cpu we have to
offload the data from the cpu to the GPU
get do the computations on the GPU get
the results back
sharing between CPU and GPU on one of
these chips dramatically shortens that
path makes the exploitation of GPU much
easier I predict these things are going
to be the near-term future they're
starting to emerge so called ap use and
so you can buy these now in various
devices but I think these are going to
come very very common in the very near
future so you will have to program using
gpus how many people have done that phew
how many people are still saying but
those who've done it one or two good GPU
programming at the moment is a test of
anyone's patients and and General Sir
machine it is very very hard Vladimir
will tell us this afternoon exactly how
hard when he tells us about all the pedo
head genius system and the thing
concerns 125 peak power watt watt peak
power usage very very impressive costs
about one hundred dollars you can buy a
development kit there's something like
100 dollars for one of these things so
affordable keep affordable high
performance dark silicon I've mentioned
important trend go over to the dark side
don't power all the cenacle in one point
and turn off CPUs turn off gps whatever
you're not using for a computation and
power them up when you need to so when
you need to deal with large computation
and in some designs you can project but
actually what's going to happen in the
future is that we won't be moving data
through to the pressing units what we're
doing is part lighting up pressing units
that are near the data so we're crossing
computation perhaps between data items
rather than pass some data between
computer idols so you may be thinking
completely differently at the
architectural level just to keep the
total energy usage heat dissipation down
to sensible levels so it's actually it's
really crazy designs come up in the near
term hopefully mostly hidden to the
programmer but say I've gotta make a
call well doesn't that mean millions of
threads on such a computer well
yes here's a little example it's real
program test program if you read the
number down there let me read it out to
you ah 33 11 6 15 22 don't try phone
that it's not fair number this is a
number of threats but this program has
created in about four and a half seconds
of execution I'll let that sink in for
second that's a lot this has been done
deliberately as a test I'm actually
running this program on an age call
machine you can't see all the course it
each of these is a graph of one of
course you can see they're pretty active
we're getting an hour factor of eight
utilization of course it's like a good
allocation okay so how have I done that
is this magic but of course it's magic
if you're trying to do this using
something like a conventional java or
pthreads approach I'm afraid this
probably wouldn't work particularly well
I've seen systems with a few thousand
threads they tend grind to a standstill
on the very latest equipment just
doesn't work the trick is this is
written in Haskell parallel huh SCOE
it's a testbed our program something
we've been working on or I've been
working on now for about 20 years in
fact so we're quite proud of this
technology but basically the trick is to
have this parallelism available so if we
have a megacorp machine we can soak it
up but then to actually utilize our much
smaller set of the available parallelism
to Betsy match the powers and it's
available to the architecture so we've
actually created only 20,000 threats
it's still a lot but we have incredibly
lightweight threading and therefore we
can cope with that and that is a trick
minimize the overhead get the perils and
overhead down as little as possible then
you can deal with a lot of threads but
then do match the work to the system
don't just take
program has given you because if you've
got too much you're going to be swamped
alternatively if you have too little
you're not going to keep the device
active you've gotta get your gonna have
more than enough to keep an arbitrary
system active to get good scalability
but you then need to match it to your
system resources and that's the trick
written lots of papers on this ask me
later if you're interested in some of
those papers so what are we trying to
achieve well this is just show you what
parallelism and concurrency is all about
here is the theory we have lots of cute
little puppies lining up at their balls
to eat here's the practice and what
we're trying to do is to make the
practice more like the theory so the
Cuban puppies are actually lined up by
the right balls and we go to get good
efficient schedule so let me digress and
tell you about how to build a wall okay
very therapeutic by the way you happen
to have a few of moments wall building
is a great exercise Winston Churchill
used to it apparently so I was at a
conference a few years back with ian
watson who's a dataflow guy or the dough
to go Flo guy if you like and in said
well that's all very well having these
millions of threads but you know some
problems just aren't like that some
things are necessary sequential and
being an academic I like a challenge so
said well okay and yeah like what and he
said well building a wall no you have to
place a brick and another one and
another one and so on getting a little
air bricks and then we can do the same
with the second row of bricks noble
second row and now we do the third one
and we've built a wall and that is
purely sequential you have to place
first brick and then the second one and
so on everyone agree no good some people
programmed in parallel before
so let's imagine we have several
bricklayers okay so you have got 4 brick
layers are these are going to be highly
efficient polish bricklayers no laughing
please so how can we build a wall faster
using off all bricklayers well you late
and bricks at the same time it's more
bricks and same shy now I've got the
first row more bricks more bricks on set
and we built the wall wall is exactly
the same but now I built it in parallel
by breaking the problem down into these
bricks how much faster was that cynic
let's let's assume that we live in a
perfect world where I'm an academic this
is research I can do that there is no
overhead everything is perfect if
everything is perfect how much faster is
bad very obvious practice a bit harder I
show you the practice as well so 4 times
everyone think it's four times who
thinks four times raise your hand have
the courage of your convictions in a
perfect world more more than four times
good people paying attention any other
number 2 times 3 times bit more than 3
few of you ok let's work it out theory
is always easy to work out so I place
three bricks so I'm not going to get it
back for four and another three bricks
still three four so that's bit more than
three love the string still a bit more
than three number three and the final
three so using for bricklayers I I still
can't get a speed-up of a factor of four
because the problem decomposition
doesn't let me do that
okay so even if you put more resources
onto a problem it doesn't necessarily
run faster you have to match the amount
of work to the availability of the
workers so you're correct answer is
about 3.1 in a perfect world or in a
real world hopefully more than one let
me show you how not to build a wall
place a brick it's another break it
place another brick and you keep going
and now we have a wall okay now
theoreticians please note this wall is
isomorphic to the original one these
aren't exactly the same or you can see
that just fine inspecting the diagram
but it doesn't work you know it's total
nonsense in practice why is that gravity
and tastic lucky newton is not in the
audience I've ignored a fundamental
dependency this brick here relies on the
bricks underneath its is supported and
so on throughout the structure okay if I
know that dependency then this is not
going to work and this again is true for
parallel programming I can't place these
bricks I context cubes parallel task
until I have executed these ones so
turning out what are the necessary
dependencies what's going to make my
wall standoff is absolutely
fundamentally essential to getting good
parallel execution luckily function
programming helps us do that so task
identification the bricks not the only
problem we also have to worry of course
about things like coordination who does
what our communication making sure that
the brickies speak to each other so you
don't get their fingers in the wrong
place placement which brick is put where
our scheduling which order do the Brits
get Laden and so on and so forth ok ok
let's see a better way of doing that
clear and get a more than a factor of
three speed up from this problem yeah so
if I flip the beds
I left on ground yep that's a good
answer what I said to you is are there
are some fundamental dependencies okay
the ultimate problem has to respect
those dependencies but as you're
building it up maybe you can break some
of those dependencies so there are real
dependencies and there are accidental
dependencies getting a good parallel
solution involves respecting the
dependencies that are necessary but
breaking the ones that are not so for
example that brick has to rest on these
two but it doesn't have to rest on boats
so since i placed these two i can place
that one I don't have to build a
complete wall complete their as I showed
you and you can also do things like
prefabrication if the dependencies are
accidental and guys if you using
concurrency approaches I'm afraid you
have to solve all this stuff I don't
think it's a good move for a megacorp
system we need structure we need
abstraction you don't need another brick
in the world what we need to do is think
in parallel new high-level programming
constructs allowing still with hundreds
of millions of threats as I showed you
so we should be capable as as programs
is dealing with that we shouldn't be
mucking around with things like deadlox
okay it's very very clever people trying
to solve our problems of deadlock
detection it's a bit like curing cancer
okay it's great to cure cancer the
person I'd rather not have it in the
first place and that's what debt markets
you cannot program effectively while
fiddling with communication etc if
you're playing around the communication
structures you're thinking it fall to
lower level you should be thinking at
the level of the bricklayer how do you
place things 11th choice not thinking oh
I've got communicate between this thread
on that thread and sew and this is a bit
frightening for the modern generation
you have to think about performance okay
for the last 20 years people have
basically been ignoring real performance
information they've had abstract models
that help them with the programs this is
ok because all architectures have
basically been the same in the future
they won't be we will
stop thinking again about performance
information integrating this into our
design big shift back to the past okay
and hopefully we can do this in a way
which is high level and abstract so
solution well according to Bob Harper
the only solution anything works for
parallelism is functional programming
I'm academic you can verify this the
citation is Bob Harper's facebook page
so nice thing about parallel functional
programming purity means no side effects
lots of advantages easy to find
parallelism impossible for parallel
processes to interfere with each other
you get a nice break down clean simple
bricks we can debug sequentially but run
in parallel and that gives us an
enormous saving in programmer effort am
I going to tell you but about that later
on you can concentrate on solving the
problem not on putting some sequential
algorithm into some vaguely parallel
system there are no locks deadlocks or
race conditions okay which means there
are huge productivity gains you can
actually deal with these parallel
programs with huge numbers of threads so
I Brett definitely recommend our
parallel functional programming I'm
going to skip this one it's a bit short
of trying as best say I've used to
concurrency guys parallelism is kind of
the opposite so parallelism is the
reality concurrency is an illusion so
the bricks of course a functional a
brick is a function in some sense also a
also known as closure so parallel
Haskell what I can say is let's define a
brick to be some expression that i want
to calculate and then what i can say is
let s kubrick in parallel with the main
computation well that's going to do is
set up a future a little marker which is
not a real threat or something that i
could execute in the future if i need
more work and if you've got a strict
language ok i sorry guys you're a bit
behind the times but if you do have a
strict language like Earl angle scala
then you can mimic the effect
using conditionals or function
applications so you can or you can get
some of the benefits of things like
futures etc in strict languages as well
but you may have to program by setting
up function instead and then passing in
some dummy argument it's a bit clumsy I
know but you get some of the above just
laziness that way i'll put my slides
online later see where to go so the
paraphrase approach is to start
bottom-up identify strongly hygienic
components bricks so by strongly
hygienic I mean things that may not be
completely pure but which don't have
unwanted interactions with each other
think about pattern of parallelism and
these three things like Matt's we're all
familiar with maps folds reduces in our
functional programs these are good
patterns and they often lead to good
parallel implementations structure them
into a parallel program using concrete
skeleton to do that taking into account
things like performance energy etc usage
and then do it again until you get it
right and the putting it into the right
form and the doing it again is going to
involve refactoring the code okay so
Chris Brown's afternoon will tell us
about that but both legacy and new
programs so here's our approach start
with our sequential code are lying
Haskell Scala F sharp insert your
favorite language here put it through a
sausage factory sausage chimpo sausage
machine crank the handle so I'm going to
take my patterns in on one side I'm
going to have my pattern machine my
sausage machine cranking out parallel
programs at bottom and taking into
account costing a profile information
then I get a parallel program in terms
of Erlang house scoffs caller etc ok so
I'm afraid if you give me Erlang you'll
still get Erlang out if you give me
Haskell you'll see that Haskell out
we're not quite that clever it's
semi-automatic systematic yes Chris
Chris on will tell you about
mechanism later this afternoon
essentially what we have is a system
which is tool based which is programmer
directed which aids the programmer to
introduce the powers so it guides the
program and how much I think will shows
some more automatic ways of directing
the programmer towards the patterns that
are best okay so what we think is the
programmer should be helped but if you
want to get maximum impact and
completely replacing the programmer
isn't effective if you use our tool and
introduce the perils and that's great
but if things change in future how do
you deal with yourself again so you want
to help the program that's that's right
that's that's right so the idea is to
keep programming in the loop to take
advantage of programs that ease when
they're available to guide the
programmer to train the program to do
the right thing in the long term we
might be able to do things fully
automatic for it fully automatically but
I think we're still a little way off
that at this point of time and then we
go down to our hash genius hardware so
common patterns can include things like
our farms Maps pipelines divide and
conquer reduces etc you've probably come
across these terms before I of settings
let explain some of them Sir pipeline
you messaging one thing after the other
very common pattern that we discover in
our programs are particularly a function
program at the top level you in the
program's i write in haskell i tend to
do something like read input and then do
something and then do something else
series of phases that's a pipeline if
you have laziness you can make that
parallel or if you can break the
dependencies you can make that parallel
a map applying the same operation over a
list or rather collection structure are
basically all the operations can go in
parallel reducer a fold basically a tree
cascade are applying the operation
between the elements data at a time and
the farm is interesting because what a
farm is it looks very similar to a map
if you
my structure here you'll think these are
the same pattern some level of
abstraction they are the differences
with the farm what I'm going to have is
a fixed number of workers going to work
out how to allocate the data items to
those workers maybe there's one worker
per call in my system whereas with Matt
there'll be one worker per data item so
a lot more change for parallelism and
it's me a very effective way of taking
something interesting which is highly
parallel today and putting it on to a
system which has a relatively few number
of calls because the number of workers
the number of agents actually doing the
work is quite small it can be quite
small 8 16 32 something of that order
great and of course these can be
combined you've heard all heard of
Google's MapReduce presumably just
combines two of these patterns in effect
so think about patterns these things are
functional so higher-order functions can
capture parallel patterns so for example
we can find a parallel map in Haskell
basically to be a map operation where we
spark off the computation in parallel
with the main body at each point and
then to build bricks all I have to do is
to apply the parallel map of the
operation which is creating the brick to
all my inputs now I've got a parallel
program wasn't that easy that is
basically what I did with that 331
million example if you know the
literature these are often called
skeletons arm re kool invented the term
in 1989 so please go and read much okay
I've got a few slides just show you so
Chris is going to come back to this
afternoon we've built a skeleton library
for Earl Angus part of the parallel as
our paraphrase project this is fully
nestable and it's available from the
site essentially what we say are things
like run this skeleton with these inputs
and they've got some abstract
description of the skeleton structure am
a perform whatever it happens to be
you'll see concrete examples it's
afternoon
also as I hint it and functions to deal
with hedge generating essentially what
we can say is well let's have two
different types of processor say CPUs
and GPUs again is Haskell and now I say
as well I want the brick on the cpu to
be say a power map and if I'm running on
GPU what I want to do is to use the GPU
library data to accelerate map to do the
same operation in parallel brick is the
same the parameter is different so I've
used higher-order functions basically to
choose between different types of
operations same API same functionality
great use of function programming my
mind and then you can automatically
choose in the runtime system between CPU
or the GPU you can use cost information
of run time just to choose join run this
on CPU or the GP simples okay still some
research to do that of course Vladimir
will tell you about this this afternoon
he'll tell you about the lo pido
framework are for hybrid services that
extends ask elf Erlang with these kind
of alternative choices for CPUs and GPUs
ok so to conclude the many core
revolution is here computer hardware is
changing more rapidly now than at any
point in the last 50 years roughly
speaking the last 50 years almost
nothing happened in computer hardware
things has got a bit faster we had
caches better memories and so on now
huge changes in the landscape of
programming megacorp is already here
also known as exascale programming for
high-performance computing or big data
if you're into that so if you look at
data side megacorp is used for really
really large scale data manipulation
it's very exciting lots of money but
Petra narrating and energy are both
important most of the models
unfortunately we have are far too low
level they can currency based and they
don't expose the mass parallelism if you
have patterns and functional programming
that's greatly going to aid the
abstraction means of threads easily
controlled easily scalable and can deal
with hatch genetic
so thank you everyone you may think this
is a bit of wishful thinking this is
Phil Warbler and Hank barendrecht and
Sandra's discussing this offensively
rampant lambda men instant androids my
colleagues say but no C++ and Java and
other languages are bringing functional
programming to a language near you
slowly but we're getting there so we now
have closures we have abstractions etc
Swift has first class first class
functions all of these are exploitable
and we are as I hinted start actually
taking these ideas about patterns etc
and applying them in the c++ concept
context we have an eclipse refactor but
uses these ideas we do need you please
join us more of you the better thank you
very much questions questions I missus
peptic when it comes to the exascale
really because you can't say that every
problem is for Eliza becuz every
programs for hundreds of thousands of
course and if the kracken is not simple
innocence requires a lot of
communication you end up with all of
your time spent on communication not on
the actual computation so there are
still our problems and probably many
real-life problems are not paralyzed ok
let me on / um so you're correct there
will be some problems that cannot be
paralyzed but remember what I said about
real dependencies an accidental
dependencies so if you're familiar so
there's a law called Amdahl's law which
english governs performance and this
photo says is when you've broken things
down to the smallest possible unit as I
did with the with the bricklayers then
fundamentally adding any more workers
isn't going to get any faster so you've
already seen an illustration that on a
small scale so quite right there are
some
problems that won't break down the
question then is can we find ways to
break them down can we use tricks to
eliminate some of these dependencies
which appear to be essential would
actually in practice or not and one of
tricks that we can use especially in the
functional programming context is
immutability replication so one way to
reduce communication is to for example
replicate data to do things multiple
times you may pay more computation costs
but you reduce communication costs when
you keep that in balance gives you
correctly identified but in these
large-scale systems what matters partly
is how much data you're shifting around
the system also if you're in a
functional world are these closures are
great because they are tiny bits of
computation okay so if you're trying to
shift a big thread around the system no
big Java lightweight stretch so
so-called lightweight to thread then
you're in trouble because there's huge
amounts estate these block transfer
that's going to take a lot of time with
a functional language you can send a
closure the closure basically has a
small amount of data associated with it
you hope it has computation associated
with it if you're lucky is just a few
bites and have to be transmitted it you
can send that much much faster mohnish
much less cost CSO exascale is going to
be a challenge it's something I've been
thinking about particularly in the
hostel and scholar contacts something I
would like to deal with but I think it
has some techniques look out there oh
great all by the way because um watch
your dog not easily be and you use the
word yourself be generalized in the
sense that you're distinguishing is had
a situation
independent composition because the this
equation and by opposition kind of us
with latency but we all sort of all
aspects like failures and my opinion I
think that yes dependent and independent
composition end up nihilism is just one
aspect of it and your systems will also
have to cope with trailer and how do we
deal with those things and i guess thats
basically the same trick can also say
like i'm not going to fail fast am I
going to fail once your comment yeah
absolutely independence is the is to
keep breaking dependencies is absolutely
key there but you also know the question
about failure modes about recovery etc
be nice thing for you have a purely
functional computation is that its
status whenever I do it it delivers the
same result so if my computations failed
result because the machine thingy then i
can just redo it anywhere anytime
anyplace so you have an automatic
mechanism for dealing with hardware
failure and recovery i can also
replicate computations so if I think
something's going to fail I can have
many instances and I just pick the ones
you come back they all look the same
itself doesn't matter if I'm wasting but
hablar okay if you have staged involved
then things are a bit harder generally
speaking I recommend that you don't if
you do then you have to do you need some
other failure mechanisms its
computations deliver different results
are you may need something like the
Erlang approach let things fail our
restart computations re-inject systems
they have alternative ways of computing
same result or even sometimes just drop
the finger to j because at the end of
the day not every computation action
lashes something we get a bit worried
about our in computer science but in the
Earl and community telecoms community
they know they're going to drop a few
calls it doesn't matter if they're drop
k tional calls you saw don't drop all of
the calls all the time that's what
happens if you're too focused on dealing
with all the computations so sometimes
cranson should
is to drop some of the computations if
you can get away with that to
reliability metric at best a wave lat
okay steps upset if you cheat customers
than a lot of extensive customer
hi thank you for your talk I'm wondering
what's your take on logical program and
how can solve ultimately parallelism
Ramona Nerra mercury which entities with
that but i'm wondering photo what's you
know redundancy and so logic programming
is is also great quite i like the way
that logic deals with the problem of
program decomposition if you have the
right decomposition structure you can
easily do many things in parallel at the
same time unfortunately don't have the
same abstraction capability that you
have in functional programming so way
back when i started my PhD I actually
started looking at logic programming
initially and I spent some time thinking
about parallel programming in logic
programming I came to the conclusion
well actually function programming had a
number of benefits are both in terms of
performance and in terms of structuring
capabilities so yes for the right
applications particularly for building a
search problem our logic is great our
perch for more general type problems i
think functional programming has an
advantage so I hope that's good answer
30 if you can build higher order
functions into logic then that's also
fantastic yes the unfortunately still
the sourcing issues state with
performance so they still don't have the
same love of performance that you get
with a functional program so that's
something that has to be tackled this is
much harder in a logit set weaving or
will the Buffaloes be more important
than done it already
yes would you like me to lab rat but
 punters fundamentally it's all
about breaking the program down if you
need to keep a million cause active you
need to have a million threat okay
concurrency doesn't really help with
that because it gives the other you have
the illusion of multiple threads
happening what you're really doing is to
build in some dependency structure into
your program it's the way that
concurrency mechanisms typically deal
with multiple threads is to add in
artificial dependencies to make sure the
bad things don't happen okay what you
need to keeping me in cause active these
things really happen in the same time
these dependency structures getting away
get box etc
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>