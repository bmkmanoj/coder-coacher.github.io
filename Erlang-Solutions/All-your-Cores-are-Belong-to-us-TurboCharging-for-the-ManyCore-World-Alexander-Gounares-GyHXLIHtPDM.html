<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>All your Cores are Belong to us: Turbo-Charging for the Many-Core World - Alexander Gounares | Coder Coacher - Coaching Coders</title><meta content="All your Cores are Belong to us: Turbo-Charging for the Many-Core World - Alexander Gounares - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>All your Cores are Belong to us: Turbo-Charging for the Many-Core World - Alexander Gounares</b></h2><h5 class="post__date">2013-04-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/GyHXLIHtPDM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good morning everybody and thanks for
coming out so a few people ask me about
this as those getting the talk prepped
so for those of you that don't
understand the reference to all your
cores or belong to us comes from a video
game called 0 wing 1991 where there was
a very bad translation and it turned
into this internet meme of all your base
are belong to us those of you that get
it have already laughed if you don't get
it google it look for youtube video on
it trust me it's pretty cool and that's
all I have to say about that so the
start of this story begins actually by a
year and a half ago at AOL this is the
AO headquarters in Virginia my office is
right up there this was the AOL
datacenter and it was a September of
2011 and you know I had this big
management job blah blah blah so I'm
doing my budgets for 2012 and we're
looking at all the machines were putting
into our data center we've publicly
disclosed or they not we anymore that a
wall has 55,000 servers and their data
centers so as you might imagine no just
with a yearly refresh rate we're buying
a lot of hardware and all the hardware
we're buying is no 12 core machine
16-core machines that's kind of your
standard you know when you rack in a
rackmount machine these days but there
was a very funny thing going on for all
that new equipment we were putting in my
budget was the same my traffic was the
same you've got a well numbers traffic's
been flat revenue is basically flat so
those in this sort of bizarre you know
twilight zone where I'm putting in you
know many multiples of capability you
know just way faster machines but we're
getting nothing out of it and that's
really got me thinking is like what's
going on and as you all well no you know
the hardware advances are phenomenal
this is one of my favorite slides about
just how good the hardware has gotten in
1997 a one teraflop computer was
literally an entire data center as 72
cabinets will follow computers
and today you can buy that on one chip
is newsy on fee you can get them on
newegg or CDW they're about 4 grand US
dollars so to go from you know basically
an entire data center room down to one
chip is pretty phenomenal and even if
you don't want to go to one chip there's
64 core machines that are about 4,000
bucks you put for AMD chips in there and
nothing fancy just the one you blade
commodity and they're about five
thousand dollars you can put 512 gigs of
ram in these things the number i quoted
you the 4.7 k includes 128 gigs and so
we're just phenomenal how good these
machines are we had a problem though and
the problem is this guy anybody know who
that is what's that I know I know more
more is the good guy in this case he
gives he gives to us but but and all
takes away this is Jean M Dahl and and
all came up with this formulation called
Amdahl's law which assume everybody is
familiar with and the basic concept of
and all and alls law says the amount of
speed up you're going to get in a
parallel system is proportional or
inversely proportional to the to the
amount of serialization in the code and
hopefully folks can see this but if you
look at the number of processors you
know if you are you know fifty percent
parallel you can have 65,000 processors
and you're only ever going to go twice
as fast it just doesn't matter how many
processors you throw at it and it's it's
a particularly brutal formulation
because even if you're 95% parallel you
can have two thousand processors and
you're still going twenty twenty times
as fast so if you sort of flip back to
this world where you know the chips are
going more and more cores and Moore's
Law is continuing there's a company I'm
investing in that's got the ultra
extreme ultraviolet lithography so
there's like another 10 15 years of more
and more transistor density easy and
what are you going to do with all those
all those transistors
basically going to make cores out of
them you do system-on-a-chip that'll
slow down a little bit but there's going
to be more course but we can't use them
and you can see study after study after
study on the web where you know
everybody's programs did you get two
more and more cores they start slowing
down which is where this bizarre thing
you throw more capability at it and
stuff go slower so you know naturally
being kind of an entrepreneurial guy
that's done hmm all right well if
there's a big gap between what software
can do and what hardware is doing you
know there's a business opportunity
there so we started concur X about a
year ago actually to basically explore
the opportunity to figure out what
software we could build to take
advantage of the new hardware and I'm
not sure how folks in the room came came
across her Lane but we started very very
methodically more of a top-down way we
said here's an opportunity what
technologies out there would fit the
bill and help us address it my apologies
for the cheesy cheesy photo shopping I'm
not pretty good at it but videos
relating to the rescue I don't need to
give the pitch of wire Lange is great to
this room it's think hopefully everybody
is figure that out but there's a lot of
tremendously good properties about
Erling that lend itself well to the mini
corps era but there's a problem
even on simple benchmarks stock Erlang
the stuff we did last summer such as
ar-15 be 0 1 star curling doesn't scale
very well in this particular example
this is amanda brought set so if you
want to think about you know the most
trivial thing two parallel eyes there's
no nothing goofy about it even amanda
brought set when you split up the
computation it starts slowing down after
about twenty twenty four cores and you
start scratching head singing what the
heck you know why would Mandelbrot go
slow he's pure is it's one hundred
percent parallel and so we did a bunch
of work on this and throughout the
summer last summer we we got it to it we
got to scale and these were all fixes to
the virtual machine which I'll cover
cover in a second so really sort of
reinforced our faith in the bed on
Erlang she said hey all we got to do is
fix up a few bugs here and there now add
a little bit new technology we're going
to get this thing to scale and so we
started trying a lot of different stuff
my favorite example which is actually up
here in this number we were doing some
bidding for a customer and this
particular customer they're confidential
so I can't say exactly what they're
doing but generalizing a little bit they
were trying to take a photo photo
streams from you know Facebook and
Twitter and whatever and they wanted to
analyze the images you know in real time
put them into a geotag database and do
some other stuff and blah blah blah so
we built this out in Erlang using our
system where we would take an image
coming off of a facebook or what have
you decode it all in our line just user
mikono video processor and we put it
onto those 64 core machines that i
showed you and with 17 of these machines
we were able to do 300 million photos a
day which is facebook volume so you just
pause and think about it and think about
the power of multi-core in the power of
Erlang here we wrote just regular
airline code to do image processing
which normally you do at GPU for and we
could do facebook volume on roughly a
hundred grand of hardware
so you know we got super super excited
by that and me being the guy I am I'm
like okay well this is awesome let's go
productize it oh wait sorry let me first
explain what we did for the curious so
to get all those numbers nothing super
super exotic the garbage collector and
the standard Erlang actually has some
locks in there and we got rid of them
and that actually gives a pretty good
speed up and just for clarity all of
these numbers or the best cases that we
got I don't think I need to explain to
this room performance is extremely
subjective the numbers are good for the
exact run at the exact time that we did
it and there's no promises made beyond
that because a gazillion variables that
go into performance that said I do like
to share numbers because you can kind of
give you you know a sense for what's
possible you know in if every fall the
stars are lying so in the mend abroad
case for example the thing that really
crushed you was garbage collector
because all your floating point
operations every add and subtract and
all that triggers the garbage collection
garbage collection triggers locks and so
even on something you think is trivially
parallelizable like amanda rod set BAM
you get hosed with the garbage collector
because the garbage collector is sitting
there taking locks oops so we go fix
that stuff we get it faster the memory
the memory allocator it folks know when
you create a process you create 233
words by default if you're doing this
facebook style image processing example
you know first thing you do is take a 1
megabyte image so 233 words is not
really the optimal starting point so we
both all the stuff we could
automatically detect in tune using spawn
ops you know how much memory to to
allocate that was a huge win
particularly in that facebook scenario
the folks know what memoization is so
one of the things that we were for those
of you that don't know it's basically
caching
cash dior Akamai on the inside of your
program if you have a function with no
side effects and it's called a bunch and
it's expensive function then you can
cash the results you don't actually have
to save it and we found that for some
customer scenarios that worked extremely
well but memoization got us up to 40 x
11 scenario but it's one of those things
were you know if you have any side
effects at all on your code it breaks
down and one of the things that we found
later on is we got two more sort of
complex customer examples how many folks
use lager or arrow lager or any of them
anybody that was using lager air lager
or any of those things I mean lager is
side effects so we had we had a real
tough time because everybody put in
logging or something like that and that
really goofed up the automatic
memoization because we couldn't tell the
difference between what was a you know I
know but a benign side effect and what
was that you could ignore and what was a
side effect that you really had to pay
attention to when the need are things we
did which ended up not being that
impactful as we change the scheduler to
be aware of messages right now the
scheduler in Erlang is is very naive it
just sort of goes around and and grabs
work and it's sort of a greedy algorithm
but it doesn't pay attention to who
actually needs to get work done so we
change the scheduler to actually pay
attention to the message flow and
prioritize jobs that needed to be done
and then the last thing we did which
worked really well on the big hardware
was an ooma where a memory allocator and
in the interpretive mode it totally
totally crushes because in the internal
of being in interpretive mode you
basically have these giant register
arrays that basically hold all the state
for your interpreter but all those
register arrays are allocated in domain
0 so if you throw more and more and more
cores at it every core has to go over
the hyper transport bus 22 to the
basically CPU 0
to go grab that memory and you just get
crushed because you know every every
every core is bottlenecked on the first
first one so flipping the numa aware for
the for the interpreter was a huge speed
up as well so it needs to say we were we
were very very excited by all these
numbers so then we figured okay well
let's go try something really hard and
then go go sell this and make lots of
money and buy boats and retire and all
that kind of good stuff so we figured
we'd use Chicago boss and I saw Evan
earlier there we are a huge shout out to
Evan if you haven't used Chicago boss
and your rails guy you will love it I
used to be my previous company was rails
evan has done a brilliant job and
there's a huge community around it and
they're going at light speed so much so
that you probably want to fork it and
then keep up to date every now and then
because every day there's more features
coming in what's that features features
any but it's a but it's a brilliant
system and we actually took our website
is all written in Chicago boss it is so
close to ruby rails that we actually
converted one day our site to ruby rails
just to do some apples to apples
comparisons it took two hours to port
from chicago boss to ruby rails so
absolute piece of cake and the only
thing that took time was the evan bless
his heart chose de django templates
versus Ruby templates so you had to go
do all the crazy syntax change between
de django templates and Ruby templates
but otherwise if you like Ruby you'll
love this stuff so we ran our website on
this which by this point have become a
fairly sophisticated website logins and
sessions and cookie management and all
kinds of crazy stuff and it might be
hard to see in the back so it's it's not
quite fair because actually there is a
25-percent win so our stuff did work but
it wasn't giving us the fifty percent
wins or the 40 X wins all the stuff that
we were very excited by and so this this
hit in December when we started doing
these numbers and for any Lord of the
Rings fans this is a bear a juror and
Mount Doom and that's kind of how we
felt in December it's like oh my gosh
what are we doing we had all this cool
stuff and just doesn't work in real life
oh man what the hell and of course we'd
raised money for the company we had all
these promises and excitement and like
whoops so but we didn't give up we kept
working on it try to figure out what was
going wrong and here's where we are now
we got a 45 times speedup improvement on
you no meaningful complex apps and
basically we got it scaling up to about
eight cores eight well eight cores work
scaling really really well and up to
twenty four cores scaling decently but
we do flatline out after that which I'll
talk about in a second so how did we do
it the trick to this and what it comes
down to an all kitchen set all cases but
we need when you think about mini corps
and multi-core it all comes down to the
locks that by far and away is the is the
dominant vector and it turns out Erlang
has a hidden lock in our favorite
feature the gin server and the issue is
the code is not very very readable maybe
this is just the standard gin server gen
server template the issue is a very very
common coding pattern where you know
you'll have your gin server you have
that very very nice little state
variable which is kept super efficiently
on the stack and then the you know
excellent job implementing it but it
effectively a shared state and so when
you have lots and lots of processes
coming in using gin server call which
the bad guy in here you basically have
all of these processes bottlenecking on
the gen server and this can be really
subtle and really hard to spot with
Evans permission I stole a bit of code
out of Chicago boss one of the nice
things about Chicago boss is the API in
the in the framework is so easy to use
but on the inside it's got a couple of
gin servers and you know you'll see this
code all over the place in in Erlang
whether it be some nice AP in fact you
know every Erlang book I've read
recommends this practice you know
instead of doing Jen server calls all
the tuples you know make a nice little
wrapper API for convenience that's more
descriptive and then do the gin server
call on the inside so as a result unless
you go and study all of the code that
you have all the code that you're
depending on in all the libraries that
you're calling you might have
serialization bottlenecks in your code
that you don't even know about you'll be
calling some know what you might think
of harmless API called you know get all
models or get all routes but in fact
it's going to a singleton gen server
instance and becoming a bottleneck and
without studying the entire stack of
everything that you're using how do you
find these things so in hindsight I
might regret putting this light up here
right before lunch so one of the
colleagues on our team a guy named a
phony Charlie Garrett he made a bet with
me he'd buy me a steak dinner at the
fanciest restaurant in town depending on
who could find the most bottlenecks in a
big piece of code that we that we were
working with and he was going to it the
old-fashioned way he was going to look
at the code and study it and I was going
to use our new fangled tools because one
of the things that we did once we
started understanding where these
bottlenecks were as we built a set of
tools that I'll show you in a second
needs to say I won the bet with the
tools i'll show you we found four
bottlenecks and he found two by hand and
it took me a lot less time so if the
demo God's permit I'm going to try to do
a demo here
okay this is our website everything is
available and it's free to use so folks
should feel free to check it out and it
does look much better on a big marker so
one things that we've done is we've
built a tool to visualize and understand
what's going on and all the different in
your system there we go
okay so one of things that you can see
here is these are all the processes in
the Chicago boss app and you can see all
the message flow the size of the node is
proportional to the number of oh and I
said a load test going some of the
things that you'll see very very quickly
here I didn't do my talk fast enough so
I put some load on this Chicago boss app
and what you're seeing here is
everything everything everything is
bottlenecking I run that one process and
so very very quickly you take your
program put in our system throw some
load at it and boom you can spot the
bottlenecks immediately there's a
bottleneck there there's a bottleneck
there one actually that one's okay and
then there's two more bottlenecks right
here and right here and they just show
up immediately in the in the
visualization anybody guess what that
bottleneck is what's that I it is a gen
server and and in this case it's the I
nets HTTP gen server because all the all
the initial HTTP handling comes through
the inet server and what you're seeing
here is that I netserver and now it's
done with the load tests we just
finished it what you're seeing those you
know it's spitting off a bunch of work
that work is returning at state but it's
becoming the choke point this particular
example is running on an eight-core
machine so they got pretty excited by
this because now you didn't have to go
study all of the all the code to
understand you know where did you have
these singleton instances you can simply
just start your code running and look at
it and the way we wrote this it's a
tracer that plugs in it hooks into trace
port or actually the Erlang trace not
trace port so we get a bunch of data
coming out we send it over WebSocket so
it visualizes real time using d3 on on
your browser we went ahead and built a
lot of other views this one is showing
messages between processes by volume so
you can see that there's a lot of
underlying volume in
I net stack an HTTP stock as you might
imagine then another one that we wrote
this looks pretty lame in this in this
example but it's actually intentional I
left a bug in that we have since fixed
so this shows it's a tree map of memory
usage by by process so one of the things
that can get you in Erlang is sort of
the hidden memory leak so here we have
one process that is eating up it's been
running now for about a week so it's
eating up what what is that about a
gigabyte of memory oops and everything
else is is is microscopic you can't see
it by comparison and this bug was a very
very simple bug it was a bug in our code
where you know Evan again to his credit
in the Chicago boss framework they've
got a really nice session management
system so you can handle all your
cookies and know what users are logged
in and all that kind of stuff and it's
all turned on by default super easy to
use very very elegant but I left it on
for my API calls so the particular
customer code we're using here has got a
bunch of a JSON calls in it so all of
the JSON calls which are going you know
mile a minute are building up state as
if everyone is a new user so the Chicago
boss framework very correctly is
thinking oh my gosh you got like a
million users can you kick ass let me go
let me go store all the all the session
state for them and it's dutifully doing
exactly what we asked it to do which was
store all the session state so of course
the fix was you know four lines of code
just say you know session enabled false
and then away you go and it fixes it and
then my favorite graph which is not very
very actionable by anybody here and now
the system is more quiet and so it's not
as interesting as it was before here
we're looking at the amount of work
happening per per cpu and one of the
tricks you want to do is
do plus SBT and NTS if you're doing
anything on mini corps lock your
schedulers to the core so that they take
most advantage of the cpu there but one
of the things that we found that we've
not yet addressed is that the schedulers
go crazy you can see these are all the
different processes in the system and
they're just moving moving from quarter
quarter core like raising we found this
a nap after app after appt yeah the
colors are grouped by application so and
in every diagram you've seen seen so far
thanks for the clarification we
basically take all the processes but
then we go grovel the system we figure
out what OTP app you belong to if we
can't figure that out we go look at the
the bean file we try to figure out the
name a reasonable app name based on
looking at the directory hierarchy so we
do a whole bunch of stuff to try to make
the data that you're getting more useful
because if anybody's used the trace API
lots and lots of awesome raw data but
you know unless you want to deal with
pigs all day long it's not very useful
so a lot of the stuff we've done and all
this codes available is to do the
decoding so that you can look at it and
see and on a regular size monitor that
doesn't have the goofy 640 by 480 you
can see the legend up here you know tell
you exactly you know whether you're in
the Chicago boss code your own your own
code the AI Nets code something like
that so it makes it really quick to
decode what's going on so one of the
areas for future work which would be you
know somewhere between the vm and the
operating system scheduler is to try to
quiet this down because one of the other
things you get into mini core work this
is just going to kill us on Neumann
machines because as we move process from
you know processor one processor 822
process for 16 to 64 you know you're
going to blow out your cash you have to
send all that data across the
hypertransport I mean it just is just
going to kill you so we've got a you
know
collectively figure out how to get the
scheduler to be more stable if you will
but still not start of things and so
forth so that's a bunch of work to be
done still so with that this is my
backup slides in case the demo gods did
not were not favorable this is where
when I made this slide it was this
weekend you can see this was only at 17
million words versus 600 million words
so that that hidden memory leak was
actually pretty pretty bad and normally
you're when you're under load the
schedule the schedulers looked like this
so all this is super easy to get like I
said it's just a you know an early
module that plugs in hooks into Erlang
trace we have two flavors of it one
flavor works on standard ar-15 there's
another flavor that works on the
concurrence enhanced Erlang virtual
machine which can get off of amazon so
it's all pre-configured ready to go on
the concurring Erlang am I we've got
additional data coming out and we have
all the performance improvements that
have previously talked about so all
those are also available but it works
just on your Mac it works on we have it
running on Azure people tried it a lot
of different places so it works just
fine anywhere in sort of the base model
and you just put it in your rebar and
start her up and go to OTP fashion and
everything works hopefully if not send
email I'm also a customer support it's a
start up and I think the punch line is
we couldn't be more enthused about
Erlang i mean the the power is
absolutely there and you know we've seen
just incredible numbers to give you a
sense of where this compares against a
ruby rails version our production can
coax calm is serving
about 1,200 users a second on a single
amazon instance a ruby rails version
does 26 so it is night and day different
you know being in the Erlang world and
oh by the way thanks to them and I get
to go do everything in Ruby real style I
didn't I didn't give up my model view
controller or methodology I didn't give
up any of the stuff that I'd grown too
grown to love in the rails world but
we're getting just tremendously better
performance and maybe maybe one day
we'll have actually everybody goes and
logs on maybe we'll push our server to
the limit will have to get a second
server but for right now 1,200 1,200 a
second we can handle a lot of load on 11
server and this is doing the full
enchilada of session management States
Jason calls all the all the crazy Ajax
tov the WebSockets I me everything you
saw there we can handle on the load no
problem so that's where we're very
excited and we're looking forward we're
not happy with peeking out at eight
course because already for five grand
you can buy a 64 core machine so if you
just project you know where this goes
there's a lot of headroom so last but
not least before we open up for some
questions there is an awful lot of
science behind what we've done I had the
good fortune of showing what we like to
call the bait all of the pretty visuals
dr. Lee oh no where's your hand she's
our chief scientist she's really thought
through a lot of the math behind what
you saw today and there's a bunch more
stuff we have in the pipe so her talk is
today at two o'clock so oh no sorry four
clock so please stop by and check it out
and with that let me open up for
questions and comments
the last one that we've measured
systematically was our 15 b 0 3 we have
not tried our 16 yet because there was a
small parametrized module change that um
him pass easily take some water here and
not only did that hit Chicago boss that
also hit Moki web and mochi web is how
we do all of our JSON so and we haven't
picked it up yet
I'd recommend two things one where we
got the winds that I showed you we
actually did exactly that strategy we
changed some of the stateful gen servers
to ETS tables because what you know the
typical model here was near particularly
a website we weren't changing our state
with every request these were things
like understanding the dynamic routing
so it would only change everywhere
rarely so we do an ETS table with read
concurrency true and it goes a lot
faster we don't really have anything
more sophisticated to recommend at this
time I think one of the work items maybe
for all of us collectively be to sort of
understand what would a you know
concurrent gen server look like such
that there was a very clear description
of what state changes had to be you know
asset compliant in a sense and which
state changes could be you know lazy
evaluated like transactional memory so I
think if we somehow figured out a you
know sort of pseudo transactional memory
flavor of a gen server we could probably
do a lot of a lot of work here because
not everything has to be you know
exactly in sync all the time for a lot
of the common shared state cases but
that's all work to be done Kenneth
yes we have not yet sent them in a
formal patch request
yeah so other stuff we've done it's by
three-quarters general the Numa stuff is
extremely tied to the operating system
does the mate Numa work we actually
modify the Erlang vm and we modified the
memory manager in freebsd so we could
have exact control of which physical
page which physical memory chip the
virtual page got mapped to then we
locked it down and then we told the
Erlang vm exactly where to go so that
piece you can have if you want that's
not very general it's cool code but so
we are basically the process of cleaning
that up and we will be submitting the
going through the change process shortly
here great
right it makes a huge difference so we
will get those changes to you sooner
than later than maybe avoid some
duplicate work yep exactly yeah and i
think the the a half ross which in
hindsight is kind of obvious but but now
candidly we didn't anticipate it as
strongly at the beginning was you can
get locks in all different kinds of ways
it's not just you know pthread mutex
lock right you know you can get locks
just three even a message passing
paradigm so that's sort of the hot and
just the prevalence of these things and
the ease of which you can get caught by
them so that's where we ended up
building these tools right now we have
the visualizations for free the base
level if you want the fancy
visualizations you go to amazon get our
am I it's all pre-configured five cents
a core away you go excuse me
I think somatically we don't have
everything figured out in our paper
product over time but I think
thematically what we will continue to do
is have a set of functionality that's
free for everybody and in particular
changes to the Erlang system itself
because of the open-source nature of
those continue to be free and then we'll
have a set of value add functionality on
top and you know it's very very typical
you look at New Relic and other folks
that are kind of in the performance
space say in the Ruby area that's a very
very sort of tried and true business
model but feedback is welcome on what
people are looking for I'll come back to
you Kenneth no it doesn't it was 17
machines and it was no 99 whatever
percent was the math yeah
I think we can get it down to about four
because well you look at these sorry we
were to spend more effort on that
particular example we'd probably go to
try to vectorize it there's two two bits
here and a couple of business issues one
you know the the is not just the
vectorization all the floating-point
math you know does garbage collection
and allocation for every operation so
the first step is we'd have to change
the runtime to basically have some
notion of you know type inference saying
say okay well we know this is a float
and it's going to be a small float so
don't worry about it just just go do the
math and don't bother garbage collecting
then we would have to do the loop
unrolling to do the vectorization all
the modern ships have sse4 so you can do
for way math I had to chance the program
that Z on fee that is a wicked wicked
fast ship if you can vectorize but a lot
of that work requires changes basically
to the vm and to the compiler and one of
the things we kind of Lee struggle with
is the business model around it because
that would be you know 10 to 20 person
year effort and if you have to give it
all away then I have great investors but
they're not that generous Kenneth you
another question
we did both so the trick to the
visualization we use the the base data
coming out of the vm and we use trace
process info system info code the code
server there's about 10 different points
in the code we can go grab information
and synthesize it so there's a you know
I couldn't be happier with the work and
so thank you very very much I heard some
folks and you'd put in all the right
hooks we did enhance it we put in
additional hooks which again will be
getting back to you guys that share more
about what's going on with schedulers
one of the area that the current vm is
weak on is basically describing what's
going on with course so we put an
additional data for that and then the
visualizations this is an erlang
conference javascript conference but
there are 50 million javascript evil
tricks about hooking into the animation
frame buffer and all that to get it to
visualize right because if you just take
a standard you know d three sample it
can't do what I just showed you so
there's some wicked crazy stuff in the
in the in the JavaScript that I don't
don't try this at home kind of thing
which means it doesn't work on ie just
just for the record it varies with most
folks we've seen around thirty to fifty
percent perf impact it's what it means
if anybody noticed really when I was
doing it I took our our load test one
things that we include with concur X is
a free load tester we just used song and
we've got it spun up in a bunch of
Amazon machine so you can just hammer
the heck out of whatever your system is
because it turns out if you use blaze
meter or Lodi Oh any of these guys they
can't generate enough load to hit in
Erlang server so you got to go do your
own
well I take that back if you play play
blaze meaner a lot of money they will
generate enough load but your standard
thing won't generate enough load so we
find that basically thirty to fifty
percent hit so all that we do to really
kind of understand what the bottlenecks
are is we take the load scenario that
we're using the test our what's our
what's our high watermark and we take it
down about half and then that tends to
work pretty well but we did find one
customer on Wednesday that saw a 5x slow
down and we're digging into that but we
think it's with the where they did they
do a lot a lot of message passing so
absolutely yeah everything all the data
is still there it stays live and just to
give you a sense for how bold or crazy
or choose your favorite word we are
about this our homepage which gets
thousands and thousands of hits this is
live visualization coming off of a
WebSocket and we've let this thing just
run 24 7 for you know basically two
months now every now and then we'll
update it but just runs 24 7 and
thousands of people hitting it we can
create multiple sockets to it all
feeding like data I mean it so it just
holds up really really well yeah because
what's happening here when you click
into this these are the the handlers for
all the mean because we've had a lot of
people hit it so it's the kind of thing
it thank you so much does a nice huh yep
and that's exactly the power of these
visualizations you can just look at it
and say oh what the heck's going on here
then you can go look at it and say oh
it's fine that's what it should be or
you look and say oops there's a bug but
just in a glance you can see that kind
of thing yeah they just pop right out so
that's why we're just so excited by this
and
I don't think I'm going to get more than
one steak dinner out of it because to
the hole I wasn't one too easily it's
anyway but the stuff is super reliable
so please try it out yes sir yeah just
an HTTP GET sung gives you a
million-in-one options it be trivial for
us to you know expose that out just ping
me afterwards we can add that in four
piece of cake is just forgetting going
we weren't trying to be some guys
because they did a brilliant job so we
just put the simplest UI on top it'd be
piece of cake to add that for you
so in that case it would be what's a 22
reactions to it when the automatic
memoization is turned off by default so
there's nothing really to to stress
about their second reaction is we use a
tool called out purity which was a
hostess I think wrote that right and
there's a there was an open source tool
in Erlang called purity which basically
goes through and figures out which
things have side effects so if you were
making a call to memcache d it would
basically trigger as a as a side effect
and so it wouldn't it wouldn't hit and
then our experience with it was again it
was very mixed some for some apps it's
genius for some maps it's kind of
pointless but in all cases we found that
the right way to do memoization was to
first instrument and you know study your
code build a data-driven profile of what
to memorize and then have that in
production because what you don't want
to do at least with the level of
maturity of memoization you don't want
to be dynamically memorizing on the fly
in a production system that would be
exactly so basically we went through we
get all the data we would crunch it we'd
say okay do these ten functions memorize
these ten functions we run a Porsche
transform and then rebuild it to be
memorized so just kind of like lager
it's very very very nice integration but
it's you know you can test and deploy
and have confidence that what you
deployed is you know exactly kind of
what you expect
yes indeed that genes talk yeah so what
we found was and this we did I forgot
how many different code bases we looked
at a lot of different customer code for
this we found that on average about
fifty percent of an erlang program was
mem Eliza ball but the trouble was
almost all those memo memo I zabal
functions were effectively at the leaf
and then sort of the interesting worker
functions where you would get you know a
lot of activity happening they
invariably were also interesting to the
programmer and so there was a logger
call in there and so that was the real
you know sort of the downer if you will
on the memoization because it just in
practice it's a beautiful theoretical
concept it just in practice it didn't
work well and so
yep
yep no I think that would be what we end
up having to do but it's the kind of
thing where the the programmers will
need to be greatly involved because let
me pick on lager again loggers are very
very nice piece of work there's a new
relic adapter for it so if you're
counting on New Relic giving you some
you know runtime metrics of uptime based
on the data coming out of lager and all
of a sudden it got you know memorized
away you know New Relic is going to give
you all the pager alerts and you're
going to go oh my god and it's actually
the right thing so you got to be you
know the programmer has to be involved
so that that declarative way of doing I
think is is right right excuse me ah so
Numa I don't have a slide on it but let
me just describe it this way so Numa is
non-uniform memory access and on any I
shouldn't say any I don't know one
hundred percent for sure on most
multi-core mini-course systems what ends
up happening is imagine that these three
signs are your are your big chips it's
like in that blade I was showing you we
have for AMD 6272 chips each one of
those chips is directly connected to a
memory bank with l1 l2 l3 cache and so
forth and then in between the chips
would be a bus you know think like a
network in an AM DS case they call it
the hypertransport and so what the the
system itself will do is if you access
memory that is physically if this core
accesses memory on this memory bank it
first we'll go over to the depending on
the topology you know go over to a
middle exchange point go to the memory
manager there get routed over here fetch
the memory come back and then finally
get delivered and we look at the math on
it again with a whole thing's
performance it varies but typically
speaking on the chips we're looking at
you'd see a 10 cycle lag to hit main
memory on the same domain and a hundred
cycle lag to hit memory across the main
so it's a factor of ten performance
penalty and that's why that that
scheduler jumping around problem is as
these machines get better it's gonna
crush us because we're going to hit a
factor of ten performance hit on it
happy to explain offline a little bit
more if you need I think on that time so</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>