<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Margo Seltzer - Automatically Scalable Computation - Code Mesh 2017 | Coder Coacher - Coaching Coders</title><meta content="Margo Seltzer - Automatically Scalable Computation - Code Mesh 2017 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Margo Seltzer - Automatically Scalable Computation - Code Mesh 2017</b></h2><h5 class="post__date">2017-12-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/MHZDXC4zJ0c" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm gonna talk about a somewhat insane
project that we've been working on for
several years and just so you know I
know it's an insane project so don't
interrupt me and tell me it's insane but
join me in the insanity because I think
we have a new way that we're gonna take
advantage of these thousands of cores
that you're all gonna run on your cell
phones so I want to give a call out to
my graduate student amos Waterland
because this was his idea and so you
know the great thing about you being a
professor is you get smart students and
sometimes they come up with these like
stupid ideas but but you want to be
encouraging so amos came to me his first
year was essentially this project in
mind and i thought oh how cute and I
thought you know I'll let him work on
that this year and at the end of the
year he'll realize that this is totally
insane and we can't do it and he will
have learned a lot and then we'll go on
and we'll do a real project and instead
at the end of that year I was completely
convinced that we could actually make
this real and since then we've brought a
gazillion people on this is a
collaboration with some folks at Boston
University and so we've had both a pile
of graduate students a bunch of faculty
also a bunch of undergraduates and even
a few high school students working on
this project so I've managed to convince
at least a few people that it's not
totally insane if you have questions in
the middle you should feel free to
interrupt me I will repeat the questions
for those who are viewing this on the
line so the people running the a/v you
don't have to panic so I want you to you
know remember back in the day when we
first put more than one processor in a
box and we had this fantasy that what we
could do is take a sequential program in
wave a magic wand and all of a sudden we
could run it on thousands and thousands
of machines and do something other than
MapReduce we've never quite realized
that vision so I want to suggest a
different approach so I have a colleague
who took up running a few years ago and
the more he ran the better he got so you
know if you look at poor James he
started out with these like you know
almost 11 minute miles and over the
course of a few weeks running every day
he got his time
down to like sub eight-minute miles and
now he runs like sub sevens and he
spends all his life on the road
so the question I'd like to ask you is
like people get better the more they do
things but what about programs do
programs get better so here's a very
simple program which if you give it a
big big big number you know can you
figure out the prime factors and so
we're gonna do it stupidly by just
iterating over all the integers and
seeing if we can like find anything that
multiplies so what happens when you run
this program it takes some amount of
time to run and then you run it again
and it takes exactly the same amount of
time to run it and so you tried a third
time and and every time you run this
program it pretty much takes the same
amount of time so the question is why is
it that people can get faster and
computers can't and part of the reason
is that most of the time the way we run
programs they don't learn anything but I
want to pause it a different world where
now we're gonna run this program ten
times but we're gonna run it under this
monitor program called asked for now you
can just assume it's magic and the
question is what happens ok these are
actually real numbers so now the first
time you run it it takes some amount of
time but but as we keep running it
you'll notice that we actually get
faster so that's kind of cool now what's
happening here is that every time the
program runs we're learning something
about it and we're trying to leverage
what we learned the next time now you
should all be a little skeptical here
and going well duh if you run it on the
same input every time you get the same
answer that should be pretty easy we're
running on different inputs every time
here ok so I'm not really trying to pull
a fast one so we're giving it a whole
set of different really big numbers and
yet we're getting smarter so now you
should be going ok oh that's kind of
cool so tell me more all right we can
get even faster than that so that was a
technique using online training which is
we get just a little bit smarter every
time we run it you can also take the
accumulated wisdom of having run it a
bunch of times and do offline training
and then we can do a little bit faster
now you might not be terribly impressed
because you don't know how many cores
I'm running on here and so the answer is
I max
running on two and a half cores is the
way to think about it and this is about
the best you can do given the details of
this system so what I'm trying to
convince you is that the reason you
should listen to the rest of this talk
is because we actually have a way to
learn something about programs and make
them faster every time you run them and
how many people think that would be like
really cool good alright so you're gonna
listen to the rest of the talk so now
what I'm gonna ask you to do is join me
in this thought experiment so imagine
that I take a regular single threaded
processor and I take the memory and the
registers of that processor and I think
of that as a state space now memory and
registers registers would be a big state
space memory and registers is a freaking
huge state space don't worry about that
for a moment if we think about this as a
state space then what does it mean to
execute a program well executing a
program means first you load a bunch of
things to create your state you know to
initialize the state and then every time
you execute an instruction like putting
0 and a register you sort of change the
state just a little bit so the
instructions haven't changed but the
value of the instruction pointer has and
the contents of the register have so
every time I execute an instruction I
take a teeny little step in the state
space okay so if executing instruction
is a step in the state space then what
does it mean to run a whole program well
what that means is that I'm carving out
some path in this very high dimensional
space so I'm showing you a path in two
dimensions but in reality this is taking
place in this enormous state space oh
good I'm seeing some skeptical
expressions I like that that's really
good so you should you know at some
point you'll start wanting to ask
questions and that's totally fine so if
I told you that execution was this path
and then I said how would you paralyze
it you would of course all come up with
the same answer which is oh this would
be great you just pick some points you
know evenly spaced in the path you put
each of those on a core and then you run
the cores in parallel okay it took me a
long time to get that
right so I just want you all to
appreciate it okay so you run these all
in parallel and then what you've done is
you've run in one over end the time so
if I have n processors I run in one over
N to the time does anybody see any
problems with this yes yeah why run them
all you could just run the last step and
now you would get infinite speed up okay
so that's really not gonna quite work we
need to get a little bit smarter what if
instead I sort of watched the program
run for a little bit and based on how I
watch it run and things I learn about it
I make some predictions about things I
might do in the future and so each of
these blobs represents you know a
prediction I might make about where this
program might end up in the future and I
could make a bunch of those predictions
and you'll notice that some of them are
really bad they're far away from the
line but some of them are really pretty
good so using the same idea what would I
do if I had these predictions well I
would take each of these predictions and
I would allocate a core to them and then
while our regular program is just
chugging along I would start executing
from each of those predictions and some
of those executions might fail and like
you know go off into never-never land
others of them might run successfully
then as I run my regular program every
time I get to one of these places I
tried to predict I might say anybody
been here and the answer here is like no
so we keep running and we're like oh so
close but no no one's been there and I
keep running but all of a sudden I get
here they say has anybody been here and
the answer is yes and so now what I can
do is just sort of fast forward in time
and then I can do this every time I get
to a point and say you know where do I
go next and so every time I get to use
one of these speculative executions
I've actually sped up performance okay
does this actually like have any prayer
of working the answer is yes so here is
a real live program what it is it's an
Ising model and that's not important
except for two facts one it's
computationally really expensive and two
this is actually a program that was
handed to us by some colleagues in
physics and the way their program worked
was that they had put each of these
configurations in a linked list and then
they had to iterate over the list to
find the lowest energy state so you
might say well that was stupid why did
they put it in a linked list if they put
it in array it would have been more
efficient and the reality is there are
real people out there in the world
trying to get real work done and they
are not computer scientists and all they
care about is getting real work done and
so they want to write their programs in
as simple a way as possible and I know
I'm preaching to the choir to some of
you but they really don't care about
what language they want to write in
whatever language they're comfortable
with and they want to get their job done
and they would like us all the computer
people in the room to make it run faster
and so that was sort of the challenge we
took so they had written this is a
linked list now one of the things that's
interesting about that is that like
compilers and automatically
parallelizing systems typically do not
work well on dynamic data structures
because like who knows where those
pointers are going but it turns out that
malloc is kind of predictable and so
what we're actually able to do is sort
of guess likely values that might have
been now act in this list and so work on
multiple places in the list
simultaneously so what I'm showing you
here is in fact the number of cores on
which we ran this we have a 44 core
machine and this is the speed-up we got
so the black line is what we would get
if the speed-up were perfect we used
every core to its max we don't get that
the red line is the speed-up we would
get if there were no overhead in the
system and it was just a matter of how
good our predictions were so that's not
bad that's like 2/3 is the best we can
do and then the blue line is what we
actually get given the system that we've
built so what I'm claiming here is that
again you should stay and listen to the
rest of the talk because that's pretty
good so how do we do that so let me give
you some intuition before I go into the
details so imagine that I told you that
the trajectory of a program looked like
how many of you think you could make
predictions for it yeah that would be
pretty good on the other hand if
programs look like that that would be a
little scary and in fact some programs
should look like that your encryption
programs better look like that and an
interesting question would be to look at
the trajectories of things that are
supposed to be random and secure in
cryptographic and make sure that they
are because if we can predict those we
have a problem the reality is that most
programs look something like this or not
most but many programs are at least
somewhat predictable and you kind of
look at that and say yeah you know maybe
we could make predictions there so what
I'm gonna do is present a general
architecture for how to build systems
that do this sort of learning and then
I'll talk about two different
realizations that we've made the first
one was sort of a proof of concept that
was not to put too fine a point on it
pig-dog slow but demonstrated the
potential and then the second one is
actually a real live system that
actually speeds up programs so what are
the hard problems you need to solve in
order to make this work so you need some
sort of engine that that captures this
notion of walking along a trajectory and
then what you need is somebody who will
say where should I try to make
predictions in your program because it
turns out that some places in your
program are predictable and other places
are not so we need to be able to figure
out what the good places in the program
are and so we call that a recognizer and
once we have the recognizer then we want
to feed that back into the execution
engine so the execution engine can
periodically stop at those places as
they hey has anybody made a prediction
here then what we need to do is we need
to actually build some sort of model to
make predictions so that's our predictor
once we have those predictions we have
to decide which ones should we actually
use because we can make lots of
predictions and once we've decided which
ones to use then we're going to
speculate on those threads and every
time we do one of these speculations
we're going to enter it in a cache and
so the model of execution is that
periodically the trajectory based engine
says hey here's the state I'm in do you
have a cache entry for it and the cache
will reply ideally yes
and then the engine can jump forward in
time okay questions so far yes
great so the question is so you're
trying to take every program and map it
into a side-effect free execution sort
of we're trying to find pieces of the
program that we can transform that are
in fact side-effect free so for now for
example we ignore things like i/o
although we we sort of have techniques
for how we think we would deal with i/o
but right now we're not doing that so
we're looking for pieces of the program
and if you think about you know there
are hardware architectures that will do
sort of tiny speculation they'll execute
you know they'll speculate ten or twenty
instructions to get rid of things like
branch misses we're doing hundreds of
thousands of instructions so we're
looking for big beefy chunks of programs
that are side-effect free that we can
predict where they start so hold that
thought by the end of the talk it should
be more clear and if it's not we'll come
back to it okay all right so let's look
at the two implementations I'm gonna
call one of these scaly ask and the
other ones speedy asked so scaly asked
is going to demonstrate the potential
scalability of this approach now the
good news is that the scalability is
going to look really awesome and the bad
news is that we're going to take
advantage of a software based virtual
machine to do it and that software based
virtual machine is really slow right
we're interpreting every single
instruction so it doesn't actually speed
anything up but it scales well based on
what we learned from this then I'm going
to show you a real implementation that
runs at native hardware speed and
actually achieve speed-up so that's
where we're going and where we'll start
is by mapping this architecture into the
interesting pieces that we do for the
scaleable implementation we'll go
through that and then we'll do the the
speedy implementation so I talked about
this software based virtual machine
think of this as an x86 emulator that
looks at you know x86 instructions and
figures out how to update a state space
the recognizer is actually we've got
this sort of circuitous logic where the
recognizer is going to use the
predictions and make many many
predictions and just pick the one that
seems to work the best so we'll get to
that in more detail
the predictors are tantalisingly stupid
okay like you know so I talked about
learning and machine learning and you're
all thinking Oh deep neural networks
these are so dumb you will be amazed and
yet we get results that actually work
all right the allocator is not very
interesting and we won't talk about it
and then the speculators are actually
just copies of the software virtual
machine and and when we get to that you
like to understand like why we did the
software virtual machine and then the
trajectory cache was really stupid and
has slow lookup but again speed was not
our goal here all right so let's go back
to the state space so if you recall the
way we want to think about execution is
that we transform this really big state
space into a bit vector so yes we're
dealing with bit vectors with hundreds
of thousands and millions and billions
of bits seems kind of dumb and then what
we do is we execute an instruction and
the bit vector changes to reflect that
instruction and that's the the VM is
really built that way so it really does
manipulate these huge bit vectors all
right lather rinse repeat so what we do
with the recognizer is we want to take
advantage of the fact that we've got
this model building and these predictors
and we're going to make many predictions
at once and we're just going to watch to
see which ones seem to work so we'll
find some value of the instruction
pointer and we'll say okay I'm gonna try
to build a model and in order to build a
model I need at least two points so I'm
gonna wait a while and see if I see that
point again and if I don't see this
coming back ie I'm not in the loop right
so we're trying to predict things that
are in loops if I never see that again I
say yeah that's not very good let me try
a different place in the program so I'll
try a different place in the program and
I'll wait a while and it's like oh I saw
it again I have two points from two
points I can build a line I can build a
model so you try to build a model and
then you ask you know how good is its
accuracy so let's imagine that we have a
bunch of potential values of the
instruction pointer that we think we can
build models for we go ahead and we
build models and so every time we hit
that place in the program we say here's
another instance and then we ask who's
got the
accuracy and if we actually get a
prediction that comes true we say ooh I
like that model and we throw away the
other guys okay so it's just build
models for every instruction point or
value you see and pick the one that
seems to make the best predictions not
terribly sophisticated but if you think
that's not sophisticated let me
introduce our predictors okay so this
was where these were sort of done about
four or five years ago but but machine
learning was still way more
sophisticated and in some sense we
decided let's let's do the dumbest thing
imaginable and see how that works so
whether man is in fact the dumbest thing
available which is whatever the bit
value was in the last cycle that's what
the bit value will be now and that you
like why would you do that because the
vast majority of bits don't actually
change right like the whole program
state doesn't change so that actually
works pretty well our next one is a mean
predictor which says okay I'm gonna just
look at the values I've seen over time
and for each bit I will predict the
average value turns out that works
pretty well for bits and like your Intel
status register that works pretty well
and then then we get a little smarter
right we have a one layer neural network
otherwise known as a logistic regress
sir and then so so little story here so
my graduate student and I had this
ongoing debate because he was really
fixated that we shall only use this
entire bit vector representation I'm
like well you know we we actually know a
little bit more about the state like we
know that it's grouped into registers
and in at a minimum we know that like
fights are access together so like could
we maybe leverage a little bit of that
information and so this was our
compromise we also do a linear
regression 32-bit features so if you
have like counter variables this one
works really really well okay so what we
do is we take these four different
predictors so for every single bit in
the vector we are going to try to
predict the value of that bit at the
next time we hit this instruction
pointer but now we have four different
predictions how do we put them together
and what we do is we basically do a mean
weighted averaging so how does that work
so we've got this fit vector and we've
got the four models and what we're gonna
try to do for each bit is pick weights
of those models that will tell us how to
compute the final value so we start out
and we set all their weights to 1 and
then we ask each of the predictors to
make predictions for a bit and so each
of them will give some prediction so the
top two are going to give you ones in
the bottom two we're gonna actually got
three ones in a zero so based on that
we're gonna make a prediction for the
bit and say okay it looks like it should
be a one and then we look at the value
that we actually get and it's a one and
so then we say okay everybody who voted
one gets to keep their weight but the
one who voted zero we're gonna cut in
half lather rinse repeat so now we have
some new weights we asked for another
prediction they all give us a value let
me say okay based on that it looks like
I'm gonna predict one again we say what
did we get it's like oh we got a zero so
this time we leave the zeroes in place
and we're gonna update the other ones
and shrink those lather rinse repeat so
we just keep doing this and the result
is that what you're guaranteed at the
end of the day is that the weights you
get will give you a prediction that is
at least as good as if you had picked
the best predictor each time so we you
know we're doing pretty well here but
we're doing this based on really really
stupid predictors so that's sort of
interesting all right so now we get to
speculation and this is going to be the
one piece that that is sort of the whole
reason why we wanted to do this using a
software emulator and then when we get
to the speedy implementation the
challenge will be how do you do that in
hardware so the way I've described this
so far is that the only way you could
ever use one of these predictions is if
you ended up in exactly the same state
but it turns out so let's think about
memoization right let's imagine that you
had a function call and it's a pure
function then
we all know that you don't need the
entire state to be the same you just
need the parameters to that function to
be the same so if you're building a
memorizing compiler you don't look at
the whole state you just look at the
parameters so we know that in fact
pieces of computation don't depend on
the entire state
they only depend on the state that you
read during that computation so can we
leverage that observation to allow our
predictions to be used in multiple
places in the program in the same way
that we can use the same function to be
used in multiple places in the program
and so the answer is yes so what we want
to do is we want to figure out while
we're running these speculations we want
to figure out exactly which pieces of
the state do those speculations depend
on so here's what we're gonna do every
time we get an instruction we're gonna
try to maintain we're gonna update these
four values which have a read master
write mask and then some actual bit
values that get read and written so when
we process the when we start out the
trace it depends on nothing every time
we get an instruction we say okay which
bits of the state vector are used in
this instruction so in this case there's
sort of the instruction pointer that
gets used
there's whatever the value of a is and
there's this constant eight or no
there's not the constant age I'm sorry
there's the register and there's yeah no
there's the eight okay so these are the
bits that get looked at in this
computation and now we have to say well
what values do they take when we start
so if we're gonna look at those values
they'd better have these exact values
for us to reuse the computation so then
what we do is we start executing and we
say okay which bits are we going to
write as part of this instruction we set
the write mask and then we say okay and
what values are we gonna end up writing
there and we put the writes in there now
I've shown you a one instruction
speculation what we do is we instead do
hundreds of thousands of instructions
but continue building up the same read
mask and the same write mask and so at
the end of the day what we have in the
read mask says these are the only bits
you care about and what the read value
says and this is the
you you need them to be in order to use
this as a speculation all right so what
does that mean we take these things we
call that a cache entry so now what's
happened is that those speculations that
I showed you before can get shifted sort
of up and down anywhere so you know the
little speculation that that might have
taken place somewhere in the graph can
actually move around and happen anywhere
as long as it has the same dependent
bits and so we get these sort of
parallel trajectories that you can
follow so instead of only having to hit
one place you've now expanded the number
of places where you get to reuse your
computations that makes sense
great all right so now as we run along
what we do is we say I'm in this state
is this a cache hit or a cache miss and
so we look at each entry and we say well
there's the read mask so those are the
bits we'll look at and now we'll compare
those to the bits here and in this case
the values don't match so we say sorry
that's a miss let's look at the next one
we have to match on those bits okay ah
this time the values do match so now I
need to compute the end state and so
what I do is I look at the bits that are
in the right mask and I bring those down
from the cache entry and then how do I
fill in the rest of the bits in the
output of this function where do they
come from what was it the input State
great so they just drop down directly
and that's how I get to reuse these
speculations and it turns out that
reusing those is a huge win and the
tricky part of getting a real scalable
and being implementation is figuring out
how to make that work all right
so how well does this work in practice
the answer is pretty well so I'm showing
you these are now log log scales so in
this implementation we're running on
sort of like a big blue gene machine
with thousands of cores so we're going
out to for 4k cores and so again the
Green Line now is the ideal speed up the
red line is the best
we can do given some of the overheads in
the system and the blue line is what we
get okay and this program is this Ising
model again so what you see is that we
actually scale pretty darn well almost
linearly up to about a hundred cores and
we continue to get speed-up up to about
two thousand cores and then we tail off
anybody want to speculate why we tail
off after two thousand this is the one
that's going over this linked list yes
size of the linked list exactly
so this linked list has about two
thousand elements in it so the best
parallelism you expect to ever get is
two thousand and so that's exactly where
we peak so that's actually really
encouraging we haven't gotten linear
speed-up there but we've gotten almost
50 percent right that's kind of cool so
this was what convinced us to go further
and if that isn't good enough here's
another fun result so this is sort of a
toy program there's this thing called
the Collatz conjecture which says that
if you start with an integer and if it's
even you divide it by two and if it's
odd you multiply it by three and add one
and then if you do this repeatedly you
will eventually get to one so no one is
ever proven that this is true but it
appears to be true so it's the Collatz
conjecture so you might want to write a
stupid program that just iterates over
lots of integers and says does this hold
true so that's what this program is so
um the Green Line is sort of the
baseline this is what it takes to run on
a single core the blue line is the
speed-up we get still running on a
single core but using this ask system so
now you should be going wait a minute so
you're telling me that this system
that's supposed to achieve parallelism
actually improves performance on a
single core like you're lying to me I'm
not lying to you what's happening here
well if you think about what the Collatz
conjecture does right they're a fixed
number of paths that ultimately lead to
one right there was two four eight you
know that went eight four two one and
then there was like maybe sixteen eight
four two one
or maybe there was whatever it takes to
get to 16 right so there's only a fixed
number of paths that go to one this is
learning those so as you run it more and
more times what happens is it says oh
I've seen what happens when you start at
the number 4327
you end up at 1 so I can predict it
that's exactly what will happen so even
on a single core we're actually getting
speed up here because this Auto
memoization is actually working and so
this was not a result we actually
expected this was kind of one of those
like Oh the system must be broken it's
giving us numbers that are wrong and
then you dig into it you go oh and we've
had several of those moments in this
project so we used to use some what my
student calls grad student gradient and
descent which is how do you find the
right place in the program to make
predictions and he thought he was really
good at that and then we threw our
predictors at it and we found better
places it was like oh okay so the system
is now smarter than we are
okay that's kind of cool and then here's
another one that doesn't work quite as
well so this is that same Collatz
program with that auto memoization
turned off so just trying to look at the
parallelism and what we see is that
again Green is perfect red is what we
could do blue is what we do do and the
reason that this doesn't work quite as
well as the other case is that the
Collatz loop is really really tight and
so in order to make predictions you get
burned by how much overhead we're
introducing in the system because
there's not all that much computation
going on so this technique works when
you can find sort of big long chunks of
computation all right all right so
that's the scalable implementation so
the question you know once you get those
kind of results you go yeah all right
this looks sort of attractive let's see
if we can build it for real and make it
run and so that's what this next system
does so we run on bear you know regular
x86 platform under Linux what we do for
the recognizer is we had two different
ways of doing it one was this grad
student gradient descent and then we
tried to get really smart and we used
these fancy Gaussian processes and the
good news was they worked really well
the bad news is there
really slow and so you have to do them
offline and then I had a student who
discovered that once you get the
dependency tracking in place turns out
anywhere in a loop
you know toward the top of the loop
really works pretty well and so that
became much simpler so now what we do is
we pre run the program a couple of times
and we pick the right value and I'll
show you exactly how that works now for
the predictors we got much more clever
we started out with a deep neural net
because like that's what everybody does
and then we discovered that that had
some problems and so we actually went to
decision trees which are much simpler
models but it turns out that they work
quite well now our allocator is again
pretty stupid what we do is if you're we
divide computation into time steps and
we just predict the next end time steps
where n is the number of cores we have
available and then our speculators also
run on the same hardware and then our
trajectory cache is basically a clever
hash table which I will tell you more
about so that's where we're going so
here's the critical technology that
we're gonna use we're we're kind of
peeking into execution we need to be
pretty sophisticated about that so we
can use a lot of the you know P trace
light capabilities and that's not quite
good enough so we also use pin which is
a dynamic instrumentation tool so the
idea is you take a program and you run
it through pin and you can tell pin that
what you want to do is you want to
instrument certain instructions so in
our case sometimes we instrument
branches and sometimes my instrument
loads and stores and then associated
with each of those instructions you can
build what are called pin tools and so
pin tools are little pieces of analysis
code that will run every time one of
those instructions gets executed so it
you know you basically you run the
program you get to one of these
instructions you go you execute some pin
code and then you jump back and so the
overhead of running with this tool is
going to be a function of a how many
such instructions do you execute and B
how many extra instructions do you
execute to do the instrument
so the name of the game is we would like
to instrument as few points as possible
and that's not always easy and we'd like
to do it in as few instructions as
possible and so when I start to show you
the overheads the overheads are really
all a function of those two features all
right so now what do we do for the
recognizer what we do in the recognizer
is we take the code and we pin all the
jump instructions and then what we do is
we run the program and the first time we
just count how many times we hit each of
the branches or the jumps and then what
we're gonna do is we're gonna pick the
instruction that is above some threshold
so we want to make sure that the place
we're building a model we see it enough
times to build an accurate model so we
got to see it enough times to be above a
threshold but we don't want too many
times because that means the computation
between each time is really small so
once we've exceeded the threshold that
we're looking for we pick the one that
we've seen the least and that is going
to be the one we do so we do kind of
cheat a little bit it's not entirely an
online process any more offline what
we're doing is doing some analysis to
decide where to do this instruction
pointer so you could imagine that if
what you're going to do is run a
gazillion things in the cloud on lots of
different cores you know running a
program twice probably isn't too painful
if you're only ever gonna run a program
once then you don't care about speeding
it up anyway all right so that's the
recognizer now we have this problem of
you know actually having asked stop at
the right instruction point or make the
predictions build the model etc so what
we're gonna do is basically use P trace
liked techniques so we load the user
program onto the processor and then we
run it under P trace and then we take
pin so our speculations are gonna run
under pin so our normal program that's
sort of trucking on ahead is mostly
gonna run pretty much at main at the
speed of the processor with a breakpoint
every now and again the speculators are
gonna run slower because they're gonna
have to run under control of pin and so
we spawn
one pinned process for each core that we
have to play with and then what we do is
once we hit the recognized construction
pointers so the place in our program
where we want to build a model we take a
snapshot of the state and we throw it
down onto one of our cores and then we
go off and we run the speculation in
parallel with the user program that make
sense okay let's look at our predictors
so we want to build a decision tree so
in theory every single bit depends on
every single bit in the previous state
and so you get a fully connected
decision tree which would be a nightmare
and awful in practice if we can do that
dependency tracking that we did in
software then we can reduce we can get
rid of a bunch of those lines so the
question then becomes how do you do the
dependency tracking in software and the
answer is use pin so now okay the first
time we used pin to get the right
instruction pointer we instrumented all
the jump instructions to do the
dependency tracking we've got an
instrument every load and store in your
program okay yeah I heard a groan
somewhere that can get really expensive
and that is exactly where the overheads
gonna come from but if you instrument
every load in store you can do exactly
the same tracking we did in the software
virtual machine in hardware at closer to
full speed that make sense so we track
every load in the store and then you
know we wrote really really streamlined
hacky assembler to just you know update
our bitmaps as necessary all right so
now let's talk about the trajectory
cache so the problem we're trying to
solve here is actually a fairly
complicated lookup problem right hash
tables work great if you know what to
hash right so if I just have a bunch of
keys and I have to hash them than all as
well but in this case that's not really
what I have what I have is keys are a
these enormous bit vectors okay fine but
worse than
at the only bits I care about in these
bit vectors are the ones that were
actually read during the computation so
the particular bits that I care about
might differ in every single cache entry
so which bits do i hash right if I hash
the whole thing that's a lose because
now I'll only match if I match the
entire state and we want to be able to
match on these other states so here is a
great example of answering the wrong
question so we literally spent a good
year with two really smart students
trying to say how do I build a data
structure that lets me do efficient
lookup and some random subset of the
bits and we developed all sorts of fancy
tries and trees and all sorts of stuff
like that and the truth of the matter is
none of them worked very well so then
another student came along and said
actually um let me ask a different
question given one of these state
vectors is there something about them
that's relatively unique that would let
me use it as a hash key that would
mostly kind of work and it turns out the
answer to that question is yes so it
turns out that at any point in a program
if you want to get a pretty unique key
for it you figure out what registers are
currently alive and you hash the
contents of those registers so now
instead of having to build a key out of
this enormous state space we're building
a key out of a way smaller state space
so you pick the place near the top of
your loop where most registers aren't
alive you have a handful of alive
registers each of those has like 32 bits
in them and now all of a sudden you're
hashing on a relatively small number of
bits and it turns out that's relatively
unique now so if you get a hit on that
the hash of those then you have to
search the whole vector and make sure it
really is a hit but it turns out that a
vast majority of the time if you hit on
the contents of those live registers you
get a hit across the world so this was a
great example of if you
the right question it's way easier to
answer it than if you keep asking the
wrong question over and over again so
this I thought this was one of the best
insights that when my students had so
now that we can get these dependent bits
we can actually build much smaller
decision trees because we now know
exactly which bits a computation effects
and so we know where the dependencies
are and so we get much sparser decision
trees yeah question right so so what you
do is you first hash that gives you the
entry in the entry you have the whole
bit vector and the right map and the
read masks and then you check those bits
that are under the read mask so once you
get a hit you do have to check and what
we find is that you know 99 point many
nines percent of the time if you get a
hit it does in fact check out and so
it's it's a little bit of wasted
overhead but not a lot
any other questions while we're here
okay we're almost done now with the
speedy implementation so how does the
allocator work so we take the
computation we divided up into these
steps we're a step is basically the time
of execution between two instances of
the place we're building a model now if
in fact the speculators ran at the exact
same speed as the main program then what
would happen is in the first time step
you would you know run run the program
normally and then while you were running
time step one each of the other cores
would be running the next time steps so
that when this guy finished he would say
okay I'm at step two has anybody done
that and this Court would say yep I did
and then he'd say oh I'm at time step
three now has anybody done that and
you'd get a yes and so he'd be able to
go right on to five and if he was
running five then you would give all the
speculators the next so in this case you
get perfect speed-up at every instant in
time everybody's running the same speed
and you would get linear speed-up now we
saw that that's not what we're getting
so what happens well it turns out that
depending on the program these
speculators which are running underpin
with instrumentation every load in store
runs some amount slower than the main
thread so
this case when we finished I'm step one
there's nobody who's finished time step
two and so we would have to do two so
that's not gonna work
oops alright so what happens instead is
that you figure out how long these
pinned threads take and you speculate
enough into the future so that you will
be able to use them but this overhead
that the pinned threads take is
precisely what's going to define the
limits of what you can possibly achieve
so the bigger the pinned overhead is the
less we can ever accomplish that make
sense good alright so now let's look at
some results here all right so this is
that same length list program that I
showed you at the very beginning but now
you know how we get these results so
let's dig into it a little more so 44
core machine we can't run on the big
blue gene yet so we're running on this
Intel box so black is perfect speed up
the redline is given the amount of
overhead that pin introduces if we were
perfect how well would we do and so
that's what the redline is and we get
the blue line so the question now that
you ask is okay where's the problem in
the blue line why is it perfect why
isn't it perfect and there's two
possible explanations one is our
predictions aren't always perfect and
the second is there's additional
overhead so it turns out the predictions
are actually perfect but there is
additional overhead so remember that
fast snapshot thing I told you about
okay it's fast it's not infinitely fast
so there's a little bit of overhead that
gets introduced every time I need to
dispatch speculations to the other cores
and that's that gap okay let's look at
another program that gets not quite such
stunning results so this is a matrix
multiply and you'll see that the big gap
here is not so much between red and blue
but between black and red right so why
is this program so much more expensive
to pin well it's a matrix multiply and
the matrix is about 16 million entries
so that's a lot of load and store
instructions okay so this is a case
where we've just got a ton
of overhead in the instrumentation tool
and we get about half the speed up that
we might get because it's also a really
big state that you're copying so I'm
gonna claim that that this doesn't bum
me out too much and you should be going
wait a minute you're getting like you
know f4x feet up on 40 cores that should
bum you out a lot so why doesn't it
depress me too much the reason is that
you know we're kind of making do with
today's hardware to do this but imagine
so so if you've ever looked at
transactional Hardware right everybody
was like really excited let's make
transactional memory transactional
memory constructs a read set and a write
set in hardware at clock speed that is
exactly the functionality I need to do
this so my hypothesis is that with a
teeny little bit of hardware support ie
pretty similar to what that hardware
guys already did when they built
transactional memory we should be able
to get rid of that big gap between red
and black and if we get rid of that big
gap between red and black now suddenly
we're back in a really exciting place so
that's why that doesn't bum me out too
much and every time I meet engineers
from like AMD or Intel I like have
little conversations with them so what
do you think it so I'm actually my game
plan for the near future is to get one
of my students who can do some hardware
modeling to see if we can build a gem 5
model of transactional memory chip and
see if we can get it to work and do
better so that's where we're going with
that so this is the explanation that I
just gave you we've got that big
overhead and so here's a bunch of
benchmarks that sort of are different
points in that same space so three some
you have a big again thousands of
integers and your goal is to find any
three that sum up to zero and again so
we do a little bit better than we did
with the matrix multiply but we still
have this really big list of numbers and
so we have a big state space that we're
manipulating and then the covariance is
basically a lot like matrix multiply and
then read map is is sort of interesting
because we do have the big gap between
black and red because that's the pin
overhead but we actually get pretty good
results relative to
the best we can do because the read map
is infinitely parallelizable and so we
would expect that we should be able to
get good parallelization there and then
here's our friend colapse again and you
should look at that and go huh
like like what's that how can you do
better than the best you can do but you
all know the answer to that right it's
the auto memoization so remember we
talked about the program that's what
happens at the very low fact because we
actually do get an advantage of that
Auto memoization and that's kind of cool
so that's where we currently are as I
said where I really would like to go is
to figure out how we can leverage
hardware to get rid of that pin overhead
because that really bugs me
there's also we're when we copy the
state we're not doing a very nifty
copy-on-write yet and so that will also
get rid of a bunch of overhead and then
there are some more fundamental
questions so what is the class of
programs that we can actually speed up
this way
that's a really good question so this is
where Theory meets practice so we have
some colleagues at BU who our
theoreticians and they're like oh we're
done with that part of the project I'm
like really tell me more
it's like well you know this will work
on any program for which there is a low
entropy point in it okay explain to me
how I explain that to my scientist
friends who are like writing programs
and he's like that's your problem you're
the systems person right so we
theoretically understand which programs
we can speed up and in practice we don't
know how to translate that so we've done
a little bit of work in that space where
we use these same state vectors and try
to predict what the loss function is
like how bad will our predictions be and
we do that based on like a lot of the
static characteristics that you can
extract from compiler tools and that
looks a little promising so you could
imagine a technique where it's like here
run this tool on your program and we'll
tell you what kind of speed up we think
maybe we can get for you and that might
be attractive every time we talk to
people in the sort of the real parallel
community who like right parallelizing
compilers they're like
we can do everything you can do and it's
clear that they can't do everything we
can do but the question is what what are
the differences between the class of
programs the compilers can just do
automatically in the class of programs
we can do now there is a fundamental
difference by compilers work on a
particular language and sometimes rely
on user annotations we're working on
binaries right you could apply this to
byte codes as well we have not done that
but that would be a really interesting
thing to do
we are language agnostic because like we
run on binaries so it's clearly a
different set of programs and a
different set of constraints but we
don't have a great answer yet - will
this work on my program and then there's
the you know we can mine the machine
learning problem forever like ooh we've
built three different learning
algorithms like which one's better or
what else should we do is there you know
an uber learner that would work in all
cases who knows so those are the kind of
things that we are working on and the
project has mostly been founded by NSF
and Google and on that note I will wrap
up and be happy to take more questions
yes oh oh now you know you get the mic
did you explore the compiler side of
things where you actually generate
different programs that are easier to
run through your system or easier to
paralyze so the answer is not yet I do
have I think that there's actually a
very nice symbiotic relationship that we
can achieve between our runtime and what
the compiler can do and we've not
explored that at all but I think it's a
great area for research one of the
things about the architecture so you
know we're happy to let anybody use it
is it's really plug and playable like
you can plug in all these different
pieces and it's nicely modular so if
somebody who was a compiler person
intent like wanted to play around with
that I think it would be a fabulous and
rich area for figuring out what is the
compiler know and also what could the
compiler communicate to the runtime that
would let the runtime do better so okay
small disclosure here we cheat a little
bit we do use a pass in LLVM that we
wrote to identify the live registers so
we can do the hash fingerprint stuff so
I am convinced there are ways that we
can actually play nicely but we haven't
explored them yet other questions
yeah front Chris is gonna get his work
out today how do you know that your
program provides correct result so
obviously you could run the original one
but then you need to wait the original
time right great so I'm so the question
is like how do you know that we're
producing the right results so so you
know we can sort of prove that the way
we're reusing speculations are correct
now this does depend that your program
is deterministic so it turns us so
there's a bunch of different answers to
your question so so there's like three
different answers at least one is I have
a really paranoid grad student he wrote
this whole framework to like make sure
that everything we were doing produced
the same results so we've checked
everything B we've sort of got the
proofs that what we're doing with the
bit manipulation is accurate but the
more interesting answer your question is
how accurate do you need to be right so
in some computations you need to be
perfect but we've taken this idea and
we've actually applied it in
places where you don't need to be
perfect so the way these physicists
think or at least the way I understand
the physicists think right is that they
have this energy landscape and what
they're trying to do is locate either
energy wells or energy Peaks so it turns
out that if you're in an energy well
they don't really care where you are in
the well all they care is that you're in
the well so what that means is that you
you can take this ask like technique of
speculation and then have a very fuzzy
function that says am I in this space
and basically if you're in the space you
can reuse whatever you've done and
they've actually done that so so funny
story we actually compute these Ising
models faster than like anybody out
there and so we were able to get a
publication in sort of a lower tier
journal and the higher this is when
we're using the neural networks and the
higher tier journal is like this is
great we would love the paper just
explain to us what your neural network
is doing and we're like excuse me you
know people don't do that but of course
I'm not a machine learning person so I'm
really intrigued by that problem and so
we actually through an intern over the
summer at trying to figure out if we
could build an interpretable model on
what the neural network was doing so we
could explain what it was doing we
failed but that means it's research
right because you're allowed to fail but
I'm still I'm still really intrigued by
that idea because one of the things we
observed when we were using the neural
network learner is that we could build a
model for like icing and if we use that
model on a different program we got some
speed-up like we need some right
predictions it sounds like how does that
work
and so my hypothesis is that what the
neural network is actually learning is
something about your program and also
something about the compiler and how it
generates code and we don't really
understand what that is but it's really
tantalizing it's like to figure out so
that's a third answer and then there's a
fourth answer as we've used again a
similar technique to speed up Markov
chain Monte Carlo so if you think about
the space that you're going to explore
in a Monte Carlo Markov chain Monte
Carlo it's totally predictable like
we're
you might do and instead of running
every point on exactly the same data the
whole data set you can run it on a
subset and that gives you sort of a hint
as to what your path might take and so
it turns out that during burnin we get a
lot of speed up doing that so summing
that all up the short answer is yes we
are getting accurate outputs on
deterministic programs and there's a
huge amount of potential for things we
could do on programs that are where
fuzzy
comparisons might actually give us some
benefit so longer answer than you wanted
but really interesting stuff any other
questions
yeah we're just about out of time I know
but but I started late I just let you
know I got you back on schedule this
thing might go again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>